brand,comment_id,text,subreddit,created_utc,score
Intel,o5wjywr,it's about time they brought back the Turbo button,hardware,2026-02-17 18:02:20,37
Intel,o5v8czz,"It does intrigue me when Nvidia does it, theres lots of pushback, when Intel do it it's fine   Intel have also used it for marketing slides against AMD without FG",hardware,2026-02-17 14:04:10,46
Intel,o5vy0fq,"And lsfg can do 20x   It isn't the same as regular FPS, and I dislike how they frame it like that in the title",hardware,2026-02-17 16:14:20,39
Intel,o60pkco,"call it what it is, its a smoothing technique not a frame multiplier",hardware,2026-02-18 08:19:48,8
Intel,o5w3bx1,Can't even read it because it requires either accepting the tracking cookies or paying for the subscription so basically it's behind the paywall.,hardware,2026-02-17 16:41:38,14
Intel,o5vn973,"I donâ€™t understand frame generation, itâ€™s always feels sloppy and laggy. I have never encountered a game where it was beneficial. I have 144hz monitor, so if generate from 60, or 30, itâ€™s all shit. If generate from 144 fps, to 300, itâ€™s looks better, but feels much worse.   â€œ300 000 000 fps by the press if a button, but it still feels like 30â€",hardware,2026-02-17 15:21:33,2
Intel,o61v4hw,The downside is a game has to support XeSS 2. That's an extremely short list!,hardware,2026-02-18 13:42:32,1
Intel,o66xdg9,people spend hundreds on a high end 1ms response monitor then turn on framegen and add back like 100ms of delay like gg,hardware,2026-02-19 05:11:00,1
Intel,o60dwo2,"downvoted due to referring as simply ""3x FPS"".",hardware,2026-02-18 06:35:18,-4
Intel,o5yy1it,3x the frames but they all suck!,hardware,2026-02-18 01:09:21,-5
Intel,o5vae9e,"""internet you will disagree you will piss and moan in the comments all day you will cry about it scream about it go on Twitter and write it no no no and then you go to buy your Nvidia card..."" Gordon  There are several camps, ""Nvidia is marketing fg/mfg as ""performance"" which upsets people""  ""Those features are going to cause devs to stop optimizing (because somehow although nvidia/amd recommend \~60fps first), devs are going to push 35fps to 140"" Average r/pcmasterrace  or ""I hated because its hardware locked, I deserve access to it because I bought nvidia all my life, why dont they offer lower version (Its 100% possible)?",hardware,2026-02-17 14:15:18,37
Intel,o5vehr1,"It's because it's supported on the older a  and b series as well, and not locked to pather lake only. If nvidia supported the full mfg on the 4000 series I think it wouldn't have been much push back, shoot maybe even the 3000 series.",hardware,2026-02-17 14:37:28,23
Intel,o5vfjt1,"The limitations of this tech are generally understood now. People felt lied to when Nvidia debuted this feature, because they said that it would double your framerate, and you can feel that that's not quite what's happening (input latency likely goes up instead of down). And, to their credit, I think Intel has even described it as ""frame smoothing,"" which is way closer to the truth than Nvidia has gotten.",hardware,2026-02-17 14:42:59,34
Intel,o5vuc3x,"Two things are important here.  1. Intel giving mfg to older gen cards as well when amd and nvidia didn't.  2. Intel never marketed mfg as ""fps gainer"" thing, only a frame smoother thing. I watched intel engineer interview with digital foundry guy he clearly say this.",hardware,2026-02-17 15:56:18,16
Intel,o5vgs7b,"The way they advertise it is super different, way less scummy than Nvidia. Intel is pretty transparent in their marketing whenever itâ€™s used, they always provide performance numbers without frame generation and treat FG as an optional extra, versus Nvidia whoâ€™ll always use it if they can and obfuscate its usage when it is.",hardware,2026-02-17 14:49:19,11
Intel,o5vlm6x,"most probably people are still mad about ""5070 = 4090""",hardware,2026-02-17 15:13:32,14
Intel,o5vt8la,Please find the slide where Intel says that a 5060 = 4090...,hardware,2026-02-17 15:50:59,8
Intel,o5vmvhr,Has Intel really â€œused it for marketing slides against AMD without FrameGenâ€? I thought those slides were FrameGen vs FrameGen and Native vs Native.,hardware,2026-02-17 15:19:41,5
Intel,o5wg43n,most people have actually used it by now and understand it's applications.,hardware,2026-02-17 17:44:29,2
Intel,o5zvb97,"Because they claimed 5070 was hitting 4090 performance due to MFG, which just invites ridicule.",hardware,2026-02-18 04:15:54,0
Intel,o5wv4fe,"Is there pushback? I find Nvidia's framegen to be way better than anything else. And it sucks because handhelds are the ones that would benefit the most from it, FSR4 is really meh compared.",hardware,2026-02-17 18:53:17,-2
Intel,o5wks3o,Its way better then lsfg though. Xell helps with latency a TON. Makes it actually usable imo,hardware,2026-02-17 18:06:03,44
Intel,o64vyx2,"LSFG is software. It is not a hardware accelerated technique, like shader models and tesselation. It is not slated to be incorporated at API level in DirectX. Stop pushing this crap. Gd cant you guys get it yet?   This software is absolutely no replacement for hardware FG, yet it is being shoved down your throat again!",hardware,2026-02-18 22:06:58,9
Intel,o5w3m12,Those blur the UI which is awful,hardware,2026-02-17 16:43:06,24
Intel,o5ygtcq,"You don't have to pay anything, just accept and have fun.",hardware,2026-02-17 23:34:10,7
Intel,o64u7yq,Okay. thats fine.,hardware,2026-02-18 21:59:10,1
Intel,o5wki51,Just open it up in an incognito window and accept? That's what I do with most random links.,hardware,2026-02-17 18:04:46,-5
Intel,o5w7q0w,"It's very much not obvious, but with frame gen both the starting framerate and game can massively affect the result.   Worst case turning on FGx2 can almost double your latency, though around 30-50% is typical. Not an issue if you're running closer to 100 FPS to start with, but if the game has relatively high latency and you're limited to 144 Hz it can feel flat out worse. 3x or 4x, wouldn't bother without a 240 Hz+ monitor.",hardware,2026-02-17 17:03:56,6
Intel,o5w14sl,"The difference in latency between 144 and 300 fps is extremely small. It might not make sense in competitive fps games. But in a lot of games, it just makes sense.   And the difference in latency between games and controllers is already much bigger.",hardware,2026-02-17 16:30:03,7
Intel,o5vnprj,Something's wrong with your setup. FG and MFG works great on my 360Hz OLED monitor.,hardware,2026-02-17 15:23:48,7
Intel,o5vvh0s,"Physics be dammed, marketing will plow through with laggy frames and claim a victory.  Meanwhile, if you actually use it... ðŸ¤®[](https://emojipedia.org/face-vomiting)  [](https://emojipedia.org/face-vomiting)",hardware,2026-02-17 16:01:51,2
Intel,o5x5ozl,"> If generate from 144 fps, to 300, itâ€™s looks better, but feels much worse.  I'm willing to bet my whole life savings you would never be able to tell in a million years and are just being an elitist hipster.",hardware,2026-02-17 19:42:56,1
Intel,o5w55vz,"Using an Intel B580 with XeSS and MFG and there's no noticeable input lag or degrading of the image quality. I've used FSR and have been able to use DLSS on friends PCs, and I know what you're referring to, and I agree that it happens. It just isn't an issue with Intels MFG the same way.",hardware,2026-02-17 16:51:08,0
Intel,o695jh2,Broadly exaggerated. :D,hardware,2026-02-19 15:20:40,1
Intel,o5vs3mx,What would anyone gain from this?,hardware,2026-02-17 15:45:24,7
Intel,o5vwr1v,Less competition is always better for everyone. /s,hardware,2026-02-17 16:08:09,4
Intel,o5vuymv,"But 2x fg is already good enough for most people. If Intel wants a real win, increase supply of their gpus",hardware,2026-02-17 15:59:20,3
Intel,o62czcf,Downvoted due to ignoring the content. :P,hardware,2026-02-18 15:13:24,7
Intel,o61v90e,"I'm using it, and it's very good!",hardware,2026-02-18 13:43:13,3
Intel,o5vibv1,"> ""Nvidia is marketing fg/mfg as ""performance"" which upsets people""  Notably Intel's Tom Peterson seemed to echo that opinion when DF interviewed him a month or two ago.",hardware,2026-02-17 14:57:11,21
Intel,o5vilbp,Ya i cant believe my 3080 didnt get the basic fg. I already upgraded to the 3080 for better dlss and then was instantly told I cant hVe fg either.,hardware,2026-02-17 14:58:30,15
Intel,o5vyqiw,Likely?  Input latency goes up,hardware,2026-02-17 16:17:58,10
Intel,o5wzjr5,"Consciously ignoring the ""this is impossible without AI statement"" that was said immediately after",hardware,2026-02-17 19:14:01,14
Intel,o5wdoo4,I don't think you'll find a slide where Nvidia said the 5060 beats a 4090,hardware,2026-02-17 17:33:00,7
Intel,o5vquhe,"There was one slide where it was frame gen against native, where Intel advertised that with MFG, the maximum possible output was XXX better",hardware,2026-02-17 15:39:16,8
Intel,o5wmk8v,"Yeah, I know it isn't the same. My problem was more the title and not the technology",hardware,2026-02-17 18:14:13,14
Intel,o66x022,"no framegen is usable, i have tried it on 50 series gpus even at 120 fps and you turn on framegen you can tell shit is delayed, keep turning it on though makes it easier to get frags for me",hardware,2026-02-19 05:08:19,-1
Intel,o64xdnp,"1. I used it as example on why using the FPS multiplier is a stupid way to measure it      2. LSFG has a lower cost on system resources in my experience than FSR Frame gen      3. It works for nearly every game and nearly every GPU, means you can even use it on an RX 480     4. It works well. Yes it isn't perfect because it also uses frame gen on stuff like the HUD and so on, but it still gives a rather smooth experience      5. Nobody is talking about it being a replacement to in game FG",hardware,2026-02-18 22:13:29,3
Intel,o5wmfgy,I wouldn't recommend using 20x either. It was just me saying the title is stupid,hardware,2026-02-17 18:13:36,13
Intel,o5xtx37,"Incognito doesn't stop data collection, it literally says so when you open a private window in most browsers. It's just so you don't save cookies or that it doesn't show up in your browser history.",hardware,2026-02-17 21:37:22,6
Intel,o5vk47m,"The tech is going to create a lot of problems for reviewers soon. They used to somewhat avoid incorporating dlss in reviews, but its gotten to good stage & frankly ue5 games by default makes people upscale. 4x is already ""funny"" to use cant imagine 6x.  Honestly it makes low end cards bit tricky to criticize. ""bad gen-gen uplift, 8gb vram, but if u optimize + upscaling + fg/mfg, you get 200+fps"" The average watcher will be like what? 200+fps??",hardware,2026-02-17 15:06:08,8
Intel,o5vsw98,"But 30 series was never promised or got any ""better dlss""?",hardware,2026-02-17 15:49:18,1
Intel,o63bp7j,I think it's really neat how my 4090 is missing a function that's available on the $400 5060...,hardware,2026-02-18 17:51:06,1
Intel,o5wu4gs,One good thing about frame gen is that it forces the addition of reflex. Frame Gen + reflex can be better than native fps without reflex.  Of course native + reflex is the best option if you value latency above all else.,hardware,2026-02-17 18:48:44,10
Intel,o5wzqcc,"If you care about Latency, then you should buy an Nvidia card.  Dlss frame gen in a game means reflex WILL be there too. And that allows Nvidia cards to have lower latency despite having lower fps.  Here is what DF showed.  https://youtu.be/MnCHHSwa7R4?si=SDwuGtSbOHOIVO2h&t=4260  5070 - 37 fps - 53.2 ms  9070 - 43 fps - 63.2 ms",hardware,2026-02-17 19:14:52,19
Intel,o5yapi3,"""The RTX 5070: 4090 performance at $549""   Jensen's caveat statement was still deceptive because interpolated frames are not performance. It was the icing on the cake of the least exciting new generation in a long time. In the past we've gotten 70 series cards that competed with previous gen flagships. This gen the 80 series doesn't even come close to beating it. The 5070 is neck and neck with the 4070 Ti so they knew they had to spin some narrative.",hardware,2026-02-17 23:00:34,-1
Intel,o5wfg7r,"Oh my bad, it wasnt the 5060, it was the 5070.... such a big difference...  [https://www.neowin.net/news/nvidia-admits-rtx-5070-nowhere-near-4090-performance-without-dlss-fake-frames/](https://www.neowin.net/news/nvidia-admits-rtx-5070-nowhere-near-4090-performance-without-dlss-fake-frames/)",hardware,2026-02-17 17:41:22,0
Intel,o5wmw44,"Yeah, another sneaky thing I never see mentioned is that mfg also lowers base fps by like 10%+",hardware,2026-02-17 18:15:42,11
Intel,o7ezssw,If that's true then no AMD card is usable.  The input latency of framegen is paired with Nvidia Reflex comes out about equal to how it was before you turned on Reflex. Which is also where the AMD cards end up because their anti-lag 1 isn't very good and anti-lag 2 only supports three games.,hardware,2026-02-25 23:04:48,1
Intel,o655seg,"5. What? Everyone is misled. The entire ""need to cap fps"" is because of this stupid program! It's preventing proper information from reaching people. People are using this over dlss fg and you see posts everywhere.   I do own it. It's not bad for what it is, but its gonna suck on a 480. Need to move forward. Smooth motion blows this out of the water too.",hardware,2026-02-18 22:54:36,3
Intel,o67lvib,20x sounds like a neat idea for a challenge playthrough of your favourite game.,hardware,2026-02-19 08:42:11,5
Intel,o5ysfzq,"I guess it depends on how paranoid you want to be about it. Private mode in Firefox stops cookies from being saved and blocks ""tracking content"":  > Tracking content: These trackers are hidden in ads, videos and other in-page content. In Standard mode, tracking content is blocked only in Private Windows.  Source: https://support.mozilla.org/en-US/kb/enhanced-tracking-protection-firefox-desktop  It obviously can't stop them from getting your IP address, user agent, and other stuff like that but it's better than nothing.",hardware,2026-02-18 00:38:29,1
Intel,o5vnx8c,It made people better now that they know to care about frame pacing and latency too,hardware,2026-02-17 15:24:49,20
Intel,o6565n0,Well deserved. Burn them all down. Let people find out things on their own.,hardware,2026-02-18 22:56:31,1
Intel,o5w53x9,I have a 4k/120 and I donâ€™t need more than 110-120 fps. I refuse to use frame gen,hardware,2026-02-17 16:50:51,-10
Intel,o5vxslv,I mean sure. Doesnt change the fact its scummy behavior. Nvidia makes sure every single new Gen now isnt compatible with the previous one.  At a certain point sure. But when the 4080 released my 3080 should be able to do all the same things just slowly... its wild that even my old 1070ti could do everything AMD cards can lol and my 3080 can too but yet not Nvidia.,hardware,2026-02-17 16:13:14,3
Intel,o63ne1r,"If latency was so important, then it would be front and center of benchmarks instead of 0.1% lows",hardware,2026-02-18 18:41:58,2
Intel,o60anoz,"Then directly be angry at what you are really angry about. No one wanted the trash N3B node except Intel and Apple (who both flopped with it relatively speaking), N3E was not ready in time, so Nvidia made do with a refined architecture on same node and created MFG to try to push the subjective experience in a way the hardware could not.  But really,  despite widespread criticism based on latency, how many actually put active testing on latency and FPS with 5070 MFG vs 4090 except DF at launch? BTW, pretty interesting, DF says it's in between 4080S and 4090, but more like 4080S when all factors are taken into account.   Also, how many use latency even today, when making say, a 9070 vs 5070 comparison video? Or even benchmarking games. You said it's important right? That's why 5070=4090 is so bad right?",hardware,2026-02-18 06:08:05,2
Intel,o5wgocp,It was a joke based off your typo,hardware,2026-02-17 17:47:07,5
Intel,o5wnna6,"Yeah, that is also something I weirdly never see being talked about. In my experience, LSFG is also lighter than AMDs Frame gen, to a point where I would rather play something like Spider-Man 2 with lsfg, than with in game Frame gen",hardware,2026-02-17 18:19:08,-4
Intel,o6580qz,"Wasn't smooth motion only for RDNA3+? Plus for people that use Linux (including the steam deck, although I wouldn't use it on the LCD) wouldn't have access to it either way      I will be honest, I also used it over FSR Frame gen in Spiderman 2, because with FSR FG I felt a noticeable dip in FPS and smoothness, while it still looked perfectly fine with lsfg-vk and it ran smoothly. At that moment it was a better option for me, not in general but for me   It is also not preventing proper information from reaching people, that is what the news reporter and marketing departments do by themselves. It is their fault that people think of frame gen as something amazing that fixes everything. Not LSFG. LSFG is just a tool that helps people to maybe keep their device longer",hardware,2026-02-18 23:06:12,2
Intel,o68693b,If you like pain yesÂ    I think LSFG also caps on your screens refresh rate and then just lowers your FPS. Means a whole 12 FPS,hardware,2026-02-19 11:49:58,1
Intel,o60uu3i,Even Firefox doesn't stop data collection from Firefox itself.,hardware,2026-02-18 09:09:14,1
Intel,o5xgzxn,Reviewers don't factor in Nvidia's Reflex vs non reflex when comparing cards (at native or upscaled) though. It seems like latency only matters while complaining about Frame Generation.,hardware,2026-02-17 20:36:36,9
Intel,o5w8w8p,"That is the beauty, dont use it. Everyone happy",hardware,2026-02-17 17:09:46,11
Intel,o5x54ug,"There's nothing scummy about it. You just don't realize that ""my card can technically do it, but more slowly"" negates the whole point of tech that improves performance.  If you lose more FPS by turning on FG than you get from it, what's the point?  Also, DLSS 4.5 literally came to all RTX cards, so you're just flat wrong.",hardware,2026-02-17 19:40:17,4
Intel,o5vyiuu,"I'm not defending their scummy behavior, I'm pointing out that if you wanted something other than performance (and good performance/$, albeit with a crippled vram) from 30 gen, you made it up in your head. That is all.",hardware,2026-02-17 16:16:55,7
Intel,o6kuwk3,"It is getting there. For decades benchmarks used average fps. Then we started seeing 1% lows and after that again, 0.1% lows. Now frametime graphs are used. We will probably have to start benching image quality for upscaling as well.",hardware,2026-02-21 10:06:45,2
Intel,o65lk9g,"Smooth motion is nvidias ""lsfg"" that employs tensor cores.",hardware,2026-02-19 00:20:09,3
Intel,o73ra7y,It doesn't,hardware,2026-02-24 08:32:28,1
Intel,o615m76,the little df clip video covering reflex recently is crazy. i did not realize how much it helps despite always using it. ofc it's game by game but it can be drastic reduction in latency while adding fg is really not adding that much. and fg + reflex can still be lower than no reflex no fg lol.   idk why latency is not covered responsiveness is like one of the most important factors to games imo. antilag 2 is in like maybe 10 games this should be pushed more it's literally only nvidia competing when it comes to it. especially since you can inject reflex pretty easily through other means.,hardware,2026-02-18 10:47:28,4
Intel,o65sk6z,Well then there is also only a limited amount of people that have access to that. So LSFG can be used in the other use cases,hardware,2026-02-19 00:59:21,2
Intel,o4wh5yh,"Despite being from 2009, Unigine Heaven is such a timeless benchmark.",hardware,2026-02-12 00:55:01,4
Intel,o3ugjxp,"> At first I was told between the balanced mode on Microsoft Windows and balanced mode on Linux there can be up to a 15 Watt difference in the PL1 state. Then I was told the power limits were the same under Windows and Linux for this laptop, but that MSI programmed their balanced defaults lower than Intel recommends. Intel expects their OEMs to have higher PL1 minimum values for balanced mode but MSI set it lower at 15 Watts min and 30 Watts max for at least this laptop model.   Wait what, why did MSI set the power so low on the Prestige 14....   I really wanted the Prestige 16 Flip, hope that one has a higher power limit",hardware,2026-02-06 04:18:57,10
Intel,o3ucdeu,"This is a continuation of [the CPU-focused article from a couple of days ago](https://www.reddit.com/r/hardware/comments/1qv0bxv/intel_panther_lake_shows_strong_linux_cpu/), reviewing the integrated graphics now.  Bit disappointing compared to Windows (down ~31% in Cyberpunk), but it is Intel after all - there had to be _some_ flaw.  Hopefully drivers will improve over time.",hardware,2026-02-06 03:51:26,23
Intel,o3vp5tb,Looks like AMD is still king on Linux thanks to Valve doing a lot of the heavy lifting in mesa for them.,hardware,2026-02-06 10:41:48,12
Intel,o3uj0fo,"The numbers I found interesting are the vkpeak tests, just like the [B580 ](https://www.phoronix.com/review/intel-arc-b580-gpu-compute/3)before it (though, OpenCL), Xe arch has a strong showing for precision compute.",hardware,2026-02-06 04:35:58,7
Intel,o3ucpvf,"Seems like Xe drivers are still horrible, but Panther Lake brute forces its way to a win.",hardware,2026-02-06 03:53:39,20
Intel,o3uvucl,Has there been any articles commenting on battery life on Linux with Panther Lake? I know this article and the previous one was focused on perforamnce in the CPU and now iGPU but any word if battery life can be as ood as windows?,hardware,2026-02-06 06:12:20,3
Intel,o3ufgw7,"It is so odd to see the infatuation of the writer with Strix Halo wanting to compare it. It isn't comparable, it is a much larger chip, it is like comparing a Snapdragon Elite to a M4 Max. The writer keeps lamenting paragraph after paragraph about it.  They should compare comparable things. This fiat 500 isn't as fast as this Tesla type of thing is weird.",hardware,2026-02-06 04:11:38,17
Intel,o3ui8jz,"I understand there's a lot of fanfare around Arc and Panther Lake in this sub. Many are firm believers that Intel will finally break the duopoly of AMD and Nvidia and while that may be the case, everyone seems to gloss over the driver side of things, not to mention the massive CPU overhead.   Just yesterday, I was recommended the latest video from YouTuber ""zWORMz"" benchmarking the 'ancient' RDR2 on a B580.  And even on Windows, the guy was frustrated with various driver issues and how the GPU wouldn't even run in his mid-range machine with an i5-12600K. Some issues even popped up right when he was recording.  With that in mind, I can only imagine the experience on Linux!",hardware,2026-02-06 04:30:33,5
Intel,o3vi0et,Should be compared to Strix Halo based on price.   The power consumption does not look all that great:  > The Core Ultra X7 358H came out on a geo mean basis to 1.23x the performance of the Radeon 890M graphics with Strix Halo but at 1.25x the power consumption on average. The peak power consumption was also around 1.25x that of the Ryzen AI 9 HX 370. So rather even on a performance-per-Watt basis overall.  Intel has a lot of work to do. 2 nodes advantage and still just matching Strix Point in power efficiency.,hardware,2026-02-06 09:35:17,2
Intel,o3vnv5t,Semi serious answer probably they're trying to not making raptor lake repeat. On more related to the laptop msi probably trying to win some battery life benchmark or trying to not breaking their laptop chassis and components quickly because of heat.,hardware,2026-02-06 10:30:06,10
Intel,o3vr1i3,Itâ€™s also not the highest performance profile.,hardware,2026-02-06 10:58:19,2
Intel,o3uen4k,The fix that Will improve Nvidia performance in Linux should improve Intel performance as well,hardware,2026-02-06 04:06:09,15
Intel,o3xdz6s,"Redhat/Fedora worked a ton on the AMD Drivers as well, a lot of stuff in the RADV Driver says  >> based in part on anv driver which is Copyright Â© 2015 Intel Corporation  I think some of the people working at Valve use to work for redhat.",hardware,2026-02-06 16:36:57,8
Intel,o3uiby5,"at least the drivers were 100% stable  >Once sorting through the low power limits on the MSI Prestige 14 laptop, the Arc B390 graphics with the Core Ultra X7 358H were rather impressive on Linux. Besides the stellar integrated graphics performance, there were not any stability issues or any other functional problems to report with the Panther Lake open-source Linux graphics driver testing thus far. Thanks to Intel for providing the Panther Lake laptop review sample and more Panther Lake Linux benchmarks are forthcoming on Phoronix.",hardware,2026-02-06 04:31:12,30
Intel,o3ui2st,"I think they're trying to get ahead of all the comments complaining that Strix Halo wasn't included/accusations of bias in favor of Intel by not including AMD's top of the line (even though, as you say, it's in a very different weight class).",hardware,2026-02-06 04:29:26,11
Intel,o3uk2zw,"There's nothing wrong comparing it to Strix Halo. Regardless of the data measured from Strix Halo the conclusion will inevitably be made on price/perf, perf/area etc. It's simply another interesting data point that is shared and pull comparisons from, which I am all for. Phoronix is one of the few outlets that have such a wealth of data for every HW release, especially when it comes to linux and compute",hardware,2026-02-06 04:43:26,20
Intel,o3vop8u,"If Intel tries to sell the 12 xe version in $1500 plus laptops, it has to compete with strix halo not just with strix point.",hardware,2026-02-06 10:37:41,-5
Intel,o3uko6t,[https://www.phoronix.com/review/intel-b580-opengl-vulkan-eoy2025](https://www.phoronix.com/review/intel-b580-opengl-vulkan-eoy2025)  It's getting there,hardware,2026-02-06 04:47:33,9
Intel,o3v0tsd,"Yeah, the Intel Vulkan drivers on Linux especially is still pretty bad, among the benchmarks linked in the post is FurMark which has both an OpenGL and Vulkan version, and Intel takes a noticeable hit in performance when using Vulkan while AMD sees basically no difference between OpenGL and Vulkan. The Vulkan driver being the weak part is a big problem since Vulkan is what's used for running games from DirectX 8 up to 12 by default, and for DX12 Vulkan is the only choice.  In my case I have a Framework 13 with the Intel Core Ultra 5 125H (Meteor Lake) and the Arc (Alchemist) 7-core iGPU, and I can't get a stable 60fps on Forza Horizon 5 no matter what I do, even disabling every graphical effect possible and dropping the resolution to 1024x768 doesn't work (if anything it's worse since the minimums stay the same so the difference between average and minimum just gets larger and the stutters feel way worse), while the LCD Steam Deck can manage 60fps flat perfectly fine at 1280x800 minimum settings at least, and it sips less power then too (under 15W vs 25W for the Framework), even though the CPU is just Zen2 which shouldn't be faster than the Meteor Lake P-cores, and since it's the LCD version the RAM is just LPDDR5-5500 so memory bandwidth shouldn't be too different from the DDR5-5600 in the Framework 13.",hardware,2026-02-06 06:54:34,8
Intel,o3vlhnc,"This is a GPU test on linux, which is not Intel arc driver's strongest suit. They did great in CPU test on linux actually, a good improvement.",hardware,2026-02-06 10:08:11,11
Intel,o3voe66,"There is definitely something wrong with the drivers on the Linux side. In Windows Panther Lake was measured to beat both Arrow Lake-H and Strix Point in perf/watt comparison easily, and was even beating Radeon 8060S in the Ryzen AI Max+ 395 until 30W.",hardware,2026-02-06 10:34:52,9
Intel,o3wz2cx,"This is due to Linux drivers being horrible, not 12Xe being deficient.  In Windows, the gains are pretty obvious.  Also, this is a GPU test",hardware,2026-02-06 15:26:29,5
Intel,o40mp0b,It's 1 node advantage N4->N3.,hardware,2026-02-07 02:51:00,2
Intel,o3vrdxo,What makes you think so?,hardware,2026-02-06 11:01:18,4
Intel,o3x4ghu,I'm out of the loop. What fix are you referring to?,hardware,2026-02-06 15:52:25,3
Intel,o3vr8fn,Whatâ€™s the cheapest LAPTOP with strix halo?Â   Mind you strix point launched back in July 2024 in also $1500 up laptops for 890m models.,hardware,2026-02-06 10:59:59,12
Intel,o3vob4g,"> Yeah, the Intel Vulkan drivers on Linux especially is still pretty bad  I hope Intel layoffs of Linux team didn't impact their driver development long term. I'm not confident if AMD is the only viable choice in Linux ecosystem in the future because of potential intel linux performance regression compared to pre xe graphics.",hardware,2026-02-06 10:34:06,2
Intel,o3vgo2h,"I feel you. Assassin's Creed Syndicate runs at maximum \~30 FPS on Linux (normally lower) and almost double under Windows on my Intel Lunar Lake. Even on CachyOS and Nobara, and using some additional flags, the performance is not as good as on Windows.  My workaround was to create a Windows 2 Go installation on an external disk and use it to play games, which is a pity because I really wanted to get rid of Microsoft.  As for the Steam Deck, do you feel that you can play all your titles reasonably well? I was holding off on buying it because hardware-wise it seems to be weaker. But it seems to still manage to be more optimised?   Thanks",hardware,2026-02-06 09:22:12,1
Intel,o42kewf,"point taken, but the CPU is on 18A? So it's like 1.5 node advantage.",hardware,2026-02-07 12:40:31,1
Intel,o3xr6bq,"For DX12 games at least, Intel suffers from the same issues NVIDIA does due to a similar reason",hardware,2026-02-06 17:39:36,11
Intel,o3warsa,"I read that the issue on Nvidia is also present in Intel, and the fix will be done for all cards, so thats why i believe that it should help Intel GPUs there   But we have to wait and see",hardware,2026-02-06 13:19:29,9
Intel,o3xbmt3,"Vulkan API has a fix for the DX12 translation layer, its in the Nvidia Drivers now just it has to be added to DXVK, Wine, Proton etc.",hardware,2026-02-06 16:26:10,9
Intel,o3vjhl0,"Unfortunately I don't have the Deck anymore since I realized I'm not really in the target demographic, I was mostly interested because it looked like it could be a little PC that runs Linux natively, but the thickness of the controller part means it takes up more space than a laptop when put in a backpack, and I don't game on the go much anyway, hence the Framework lol.  As for ""can it play all my titles reasonably well"", I don't remember any specific titles that didn't run at all, so I guess the answer is yes? I do remember having tested Oblivion Remastered and Dragon Age Veilguard, both are Steam Deck Verified, and they do indeed run, but it's very much a 30fps experience reliant on FSR to achieve even that, so I'd guess it probably won't handle newer or heavier titles any better. If you have any specific titles you care about I'd suggest checking ProtonDB and filter the reports to just Steam Deck ones to get a picture of how the game might run, and sometimes games are marked unsupported by Valve but does actually run with caveats like input quirks so you'd be able to play if you're fine with those quirks.",hardware,2026-02-06 09:49:24,3
Intel,o49ogdh,"18A is basically N3B, so no",hardware,2026-02-08 15:51:54,1
Intel,o3vmjv5,"Thanks for sharing your experience. I'm not a big gamer myself, that's why I ended up buying the Lunar Lake laptop (battery life is more important than gaming), but yeah, I should have read a bit more on the Intel situation regarding gaming on Linux.   Anyway, thanks again. I'll take a read on the protondb page",hardware,2026-02-06 10:17:58,2
Intel,o49osgb,N3B doesn't have backside power delivery,hardware,2026-02-08 15:53:34,2
Intel,o43a5qo,Can't wait for the 10% improvement and 50% extra cost gen over gen.,hardware,2026-02-07 15:14:25,47
Intel,o44108k,">The fact that Intel will use nVidia technology in the iGPU sector in the long term  You're takin the nv intel deal outta context. Intel is still gonna use their own igpu ip for the average mobile socs and their dgpus. It's the premium dgx or halo type prosumer skus that are going with nv ip. There might be some gaming capable devices but just like strix halo they ain't primarily aimed at gamers. The price just ain't worth it compared to a standard mobile cpu + nvidia dgpu combo  Idk why this post is even a thing. All manufacturers are delaying dgpu products for gamers because they have shit margins compared to dc gpus. Nvidia's dc margins in the 70+%, why would they care about gamers? Gaming gpus have worse margins than diy cpus which have worse margins than dc gpus. It's the same for amd  Do ya know **how much money gamers are making for nvidia each quarter? $4.5-5b. How much do ai gpus make for nvidia? Over $50b.** How much for amd? Estimated at $4b+ for 2026. **Amd's making the equivalent of capturing 100% of the gaming gpu market from ai gpus and with even higher margins.** Why would they want to sell to gamers?  Fact that people just gotta face even if they hate it: The gamer money is nothing compared to the ai money. People are mad, but no amount of yelling at the sky and hoping for the ""bubble to pop"" will change that.",hardware,2026-02-07 17:26:53,19
Intel,o447pem,">The fact that Intel will use nVidia technology in the iGPU sector in the long term and that the development of HPC/AI accelerators no longer has much to do with consumer graphics cards from a technical standpoint also plays a role here.  If anything Nvidia just gave Intel another free year to catch up to the 50 series   By the time 2027 ends, Core Ultra 400 series iGPUs might actually be as powerful as a 5050 (but only use 1/3 of the power) lmao",hardware,2026-02-07 18:00:04,10
Intel,o4505hr,"Nvidia officially announced a vera rubin 128gb using gddr7 that's coming late 2026, the cutdown version of that should be the 6090, like the 5090 is to the rtx pro 6000 96gb. So I think there will be at least an announcement of the rtx 60 series in early 2027, even if its a paper launch and only for the 6090. I'd trust that more than rumors",hardware,2026-02-07 20:24:56,1
Intel,o4l4vs7,"Good breakdown - if the RAM crunch really drags into 2027, it makes sense we will see more quiet delays and soft cancellations rather than clean generational launches.",hardware,2026-02-10 09:18:43,1
Intel,o43qon0,"If Intel seizes the opportunity and makes the follow on to Panther Lake and Nova Lake with twice the number of Xe tiles, then this may meet the needs of 80% of the gamers, and Nvidia and AMD may find themselves with no commercial market for discrete GPU cards.  The high end Pather Lake has 12 Xe cores, and it brings 1080p gaming to all day laptops. I wonder what 24 Xe would do, and of course the same approach applies to doubling whatever Nova Lake comes out with.  Intel could repeat the early days of the PC market when embedded VGA graphics eliminated the need for cards like the Diamond Speedstar and all the other discrete cards like it.",hardware,2026-02-07 16:36:03,0
Intel,o45ontc,"I bought a 5090 a few months ago as I saw the RAM situation starting to go crazy in the background in the industry, plus I game at 4K  Honestly this will be very tough years for PC gaming across DYI and gaming laptops",hardware,2026-02-07 22:38:00,1
Intel,o44er96,Outlook is good for those who bought a 5090 T MSRP,hardware,2026-02-07 18:34:40,-2
Intel,o43rnqt,"The sad thing is that I'll likely get it anyways, not for the performance, but because of the spicy power connectors I want to keep my card under warranty...",hardware,2026-02-07 16:40:51,5
Intel,o43xyqx,"Donâ€™t worry, DLSS 5.5 / FSR 4.0 will be software locked to only work with the new ones",hardware,2026-02-07 17:11:50,-7
Intel,o45ny6i,"> Why would they want to sell to gamers?  Well.   > $4.5-5b.   4-5 billion dollars isn't nothing. Yes, it won't be the main focus anymore, but they would be fools to just ignore the billions",hardware,2026-02-07 22:34:01,6
Intel,o4l97w4,>Do ya know how much money gamers are making for nvidia each quarter? $4.5-5b.  wasnt gaming revenue 12B+? Or am i thinking yearly?,hardware,2026-02-10 10:01:03,1
Intel,o46frs3,"> Intel is still gonna use their own igpu ip for the average mobile socs and their dgpus.  Indeed. But for this, their Xe3p will be probably good enough for years. So, why invest in Xe4 or more Xes - from Intel's perspective?",hardware,2026-02-08 01:22:32,1
Intel,o43bpnk,"Thank you for your submission! Unfortunately, your submission has been removed for the following reason:  * It is a submission that is largely speculative and/or lacks sufficient information to be discussed.  Rumours or other claims/information not directly from official sources **must have evidence to support them.** Any rumor or claim that is just a statement from an unknown source containing no supporting evidence will be removed.",hardware,2026-02-07 15:22:20,1
Intel,o46fik4,"Consumer/Gaming-Rubin is _not_ a cutdown version of HPC/AI-Rubin. Different architectures, different chips, completely different projects. nVidia just give the same codename, nothing more (same as with Ampere und Blackwell before).",hardware,2026-02-08 01:20:54,6
Intel,o43vj7c,We've already seen how this plays out with Strix Halo. Large iGPUs drive up costs to the point they're not price competitive with discrete cards.,hardware,2026-02-07 16:59:50,15
Intel,o48ep11,Don't big iGPUs like that just get bottlenecked hard by memory bandwidth? Which is why they're not really a thing outside of platforms that sacrifice general memory throughput for iGPU memory throughput?,hardware,2026-02-08 10:44:09,2
Intel,o43t2bg,Intel will release a CPU with a RTX GPU chiplet soon.  It was part of the Intel-Nvidia deal,hardware,2026-02-07 16:47:45,2
Intel,o445r5s,"DLSS works on all RTX cards from 20 series to 50 series, currently. Framegen is limited to some cards, and Multi Framegen is 50 series only. DLSS 4.5 on 20 series cards does seem to incur a bigger performance penalty, but end users still have the option of whether to use or not.  FSR on the other hand is more complicated, and AMD isn't helping. AMD has shown they can bring FSR4 to older cards with the INT8 implementation with very little loss of fidelity, but refuse to so far.",hardware,2026-02-07 17:50:29,14
Intel,o45oto5,everything has an opportunity cost  they don't have unlimited capacity,hardware,2026-02-07 22:38:55,9
Intel,o46ladu,"There are 2 types, the one that uses HBM (which is datacenter only) and the gddr versions, which are both workstation and become consumer cards. Same in current gens the workstation and consumer ones use the same dies when they both have gddr7. The 5090 is cut down rtx pro 6000, and its successor vera rubin cpx is confirmed as gddr7 and coming in late 2026: https://nvidianews.nvidia.com/news/nvidia-unveils-rubin-cpx-a-new-class-of-gpu-designed-for-massive-context-inference",hardware,2026-02-08 01:57:26,6
Intel,o447asl,i think Intel going from 12 to 24 cores would be less of a size increase than AMD going from 16 core GPUs on Strix Point all the way to 40 on Strix Halo,hardware,2026-02-07 17:58:05,3
Intel,o449aml,"Rumoured to be Hammer Lake, 2027 or 2028. Probably 2028.",hardware,2026-02-07 18:07:53,1
Intel,o47gl4v,But there is also value in diversification. Nvidia diversified away from gaming even before LLMs blew up which is what allowed them to be perfectly positioned with CUDA.,hardware,2026-02-08 05:30:49,9
Intel,o4l9b5m,opportunity costs include risks.,hardware,2026-02-10 10:01:54,0
Intel,o46ur4l,"I wouldn't call these â€œtwo types,â€ because that implies a closeness that simply doesn't exist technologically. HPC/AI and consumer/gaming are completely separate fields of development with completely separate chips. I mean different chips at the chip level, not just versions of chips. Take Blackwell, for example:  - HPC/AI Blackwell is based on GB100, which is the dual-chip name for GB102 (~800mmÂ²) - HPC/AI Blackwell Ultra is based on GB110, which is the dual-chip name for GB112 (~800mmÂ²) - Consumer/Gaming Blackwell is based on GB202 (750mmÂ²), GB203 (378mmÂ²), GB205 (263mmÂ²), GB206 (181mmÂ²), GB207 (113mmÂ²)  Regarding Rubin CPX: This is also an HPC/AI chip that will not be used in the consumer/gaming segment, even though it looks technically similar. It is not a successor to RTX Pro 6000, but rather a separate development entirely for inference tasks.",hardware,2026-02-08 02:57:07,4
Intel,o44mvc2,"Maybe, hard to say without knowing more about Xe3P density. Either way it would still end up being a large chip that would in turn require to upgrade memory bandwidth to keep it fed.",hardware,2026-02-07 19:15:04,2
Intel,o46oom2,> Nvidia refused to backport dlss for a while too.  Where do people come up with these fantasies?,hardware,2026-02-08 02:18:45,5
Intel,o473kff,"blackwell has been a bit of an oddball. historically we got datacentre cards like the L40, T4, A40, P40, etc. that were fanless versions of the workstation cards. there's no B40 or B4 which is odd.",hardware,2026-02-08 03:54:55,1
Intel,o495bny,"This is purely frame generation. 20-series and above has always had day 1 access to the newest upscaling models, which is way more than can be said of AMD.",hardware,2026-02-08 14:08:05,2
Intel,o32auxy,"Nice, I like this approach, stops the end user who isn't an expert when reading specs from getting screwed over by slow ram killing their igpu performance. If you see the Arc B branding on the laptop you can be rest assured you're getting the right setup",hardware,2026-02-01 23:46:25,49
Intel,o31467g,"It is really impressive what Intel were able to achieve with Arc B390, using a traditional 128-bit memory bus.  Nova Lake (2027) is rumoured to keep the same memory setup same (perhaps bump up to 10700 MT/s), while [bringing a modest 25% improvement](https://www.reddit.com/r/hardware/comments/1qmk1e9/intel_nova_lake_xe3p_igpus_could_be_25_more/).  Razer Lake (2028) could be the next big leap forward, if it adopts LPDDR6.",hardware,2026-02-01 20:10:16,97
Intel,o352spx,"so 7467 MT/s or you lose the Arc badge, kinda savage lol",hardware,2026-02-02 11:51:51,8
Intel,o31rb60,I'm guessing lpCAMM2 would work as it can reach that 7467 MT/s no?,hardware,2026-02-01 22:03:16,13
Intel,o314ms4,I assume they also need 2 DIMMs to get the Arc branding?,hardware,2026-02-01 20:12:30,9
Intel,o33jrpo,Can someone with that special kind of mental situation explain the 140T vs b390 vs Iris Xe to me,hardware,2026-02-02 04:03:22,5
Intel,o316x95,They don't sell them with on-package memory?,hardware,2026-02-01 20:23:43,1
Intel,o35u0pc,"Man it's tragic that the CEO that saved the company, pat gelsinger created this product and got fired before he could see his projects come to fruition because the board was only concerned with short term profits and building chips is a long term endeavor. Now they are going to ruin the company again",hardware,2026-02-02 14:40:28,1
Intel,o31f948,Nobody seems to be talking about how Panther Lake is going to be more expensive than Gorgon Point,hardware,2026-02-01 21:04:57,-8
Intel,o329l0l,"So yeah, as I assumed, these will be closer to Strix Halo pricing than Strix Point.",hardware,2026-02-01 23:39:18,-5
Intel,o312xyz,"Hello Forsaken_Arm5698! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-02-01 20:04:19,0
Intel,o32rd2i,"This isnâ€™t entirely new, I think with the first generation of Arc iGPUs you had to have some minimum bandwidth to get to call it â€œArcâ€, otherwise it was the same generic â€œIntel graphicsâ€ listed here. _Might_ even date back before Arc with â€œIrisâ€ graphics branding?",hardware,2026-02-02 01:18:11,18
Intel,o3159fc,"> Razer Lake (2028) could be the next big leap forward, if it adopts LPDDR6.  Razor Lake won't. They'll just reuse the NVL construction. Titan Lake (2029?) would be the next big step assuming they don't go for a 3rd reuse, though there are rumors about it sticking with Xe3p. Xe4 should be a big leap though, assuming it's timely enough.",hardware,2026-02-01 20:15:34,41
Intel,o3201du,It should work fine based on their documentation.,hardware,2026-02-01 22:47:20,9
Intel,o319lrn,"Yeah, it's called Strix Halo and you do not see it anywhere because of cost.",hardware,2026-02-01 20:36:55,96
Intel,o315f3c,It raises costs a lot for a mainstream platform. Intel seems to have struck a good balance for the target market.,hardware,2026-02-01 20:16:19,48
Intel,o31hvo3,We used to have triple channel with Intel Nehalem.,hardware,2026-02-01 21:17:39,26
Intel,o316lgl,"256 bit memory bus you mean, which is technically 16 channels of LPDDR.",hardware,2026-02-01 20:22:06,8
Intel,o31b4xs,"Unfortunately only way such chips will get quad channels is if RAM will get into SoC. Like in Apple M series, ~~AMD's Strix Halo~~ or Intel Lunar Lake. Or wide bus soldered RAM with APUs like AMD's Strix Halo.   But that comes with no upgradeability/fixability and higher cost in general.  edit: correction as u/bazhvn pointed out",hardware,2026-02-01 20:44:28,5
Intel,o33z41s,wouldnt that make echo issues worse? the current board manufacturers are already doing everything possible to not invest a single cent into fixing this.,hardware,2026-02-02 05:51:39,1
Intel,o33reiw,8 ram pieces per laptop? thats a lotta ram,hardware,2026-02-02 04:54:05,1
Intel,o32veo9,The problem is they're doing better in the sector that isn't making any money these days. All the money is in AI currently and they're not getting any of it.,hardware,2026-02-02 01:41:43,8
Intel,o35uvib,"The guy who turned Intel around got fired lol. They were mad that Intel wasn't doing well but they didn't give enough time for the dudes projects to come to fruition. I.g. he was CEO for a few years but new chips take 5 years to build. Intel is going to have a short golden period while pat gelsigners projects come to fruition and then after like 3 years they are gonna be back to shit. The new CEO seems like the same as the ones before pat, just cares about next quarters profit and nothing else.",hardware,2026-02-02 14:44:54,4
Intel,o32syqr,Not the kind of being on fire you want though. Let's be honest.,hardware,2026-02-02 01:27:34,-7
Intel,o31o68v,"Given Intel only advertises LPDDR5x with the 12Xe tile regardless of the memory controller, I think it's safe to say they will not allow OEMs to use DIMMs with it.",hardware,2026-02-01 21:48:00,26
Intel,o32a083,"doesn't matter, can't get SODIMM LPPDR5X. and CAMM2 modules are dual channel",hardware,2026-02-01 23:41:40,11
Intel,o33r45n,"Iris Xe was Intel's more premium iGPUs for a while. Then Lunar Lake launched with a ~~140T~~ 140V iGPU that was their first actually competitive iGPU.  Now Panther Lake has a B390 iGPU option. Intel has finally, with the launch of Panther Lake, decided to give their premium iGPUs the same naming scheme as their dGPU line, because ~~""140T""~~ 140V made no sense, but ""B390"" does (for the most part)  Edit: Case and Point: Got 140V and 140T mixed up",hardware,2026-02-02 04:52:06,7
Intel,o33qlif,"You have a CPU that costs $150 for you to make that you sell to OEMs for $300. This gets you a 50% margin.  You have a CPU that costs $150 to make, then you add $50 of RAM on the package, so total is $200. OEMs aren't gonna like you taking a profit on RAM as a middleman when taking profit on RAM is one of their key revenue sources on laptop upsells. So you sell the RAM for cost, meaning you charge $350 for the SoC.  Yes, you've made the same nominal profit of $150, but your margin % dropped from 50% to ~43% *and* this simple math example doesn't take into account that there is a cost involved with having to source the memory yourself (i.e pay staff to manage contracts, make and track orders, receive shipments, package the RAM, etc.)  That's why it's very difficult for on-package memory to gain traction in the Windows PC world: Most CPU makers don't want to take the margin hit, and most OEMs would prefer to just handle memory quantity and product tiering themselves.",hardware,2026-02-02 04:48:31,10
Intel,o317syc,"Nope, that's why lunar lake is a one time thing, it's too expensive and has little flexibility",hardware,2026-02-01 20:28:03,21
Intel,o36g7i5,"I donâ€™t think that concern for short term profits was unfounded in this case, Intel was having crisis levels of cash flow problems and radical measures needed to be taken to shore up investors. Intel was on the verge of being a failed business and frankly they are not out of the woods yet",hardware,2026-02-02 16:27:39,6
Intel,o37mzf9,"> the CEO that saved the company  How did he save the company? His legacy is the fabs, which are still a dumpster fire.",hardware,2026-02-02 19:43:09,4
Intel,o31i1hq,"Because unless someone can provide the figures, then what's there to discuss? Whats the cost of a PTL-H SoC? How much more does it cost than Gorgon Point? And how does the cost of RAM/SSDs impact that overall total cost between the two when factoring in the total laptop BOM?",hardware,2026-02-01 21:18:26,26
Intel,o31g71c,"PTL being a better product prob helps justify a higher price tag.   Though how much higher is kinda hard to tell, with the whole component shortages due to the AI boom prob also playing a part.",hardware,2026-02-01 21:09:34,17
Intel,o32b1fs,Does not seem to be the case based on newly released laptop prices. Remember when full die strix point laptops started at $1600 back in July 2024?,hardware,2026-02-01 23:47:25,14
Intel,o337tk4,"If your laptop only has 1 stick of RAM, it'll show up as Intel UHD graphics and a few of the execution units are disabled. Slapping in another stick changes it to Iris Xe and unlocks all the EU.",hardware,2026-02-02 02:51:42,19
Intel,o31k751,> Razer Lake  Bloated batteries after 1 year will be a requirement for laptop manufacturers to use this./s,hardware,2026-02-01 21:28:47,16
Intel,o31fzuf,Xe4? When does nvidia come into the picture?,hardware,2026-02-01 21:08:36,6
Intel,o3loez0,It can be a leap in performance with LPDDR6 and more Xe3p cores. NVL-P stays on 12 Xe cores and same bandwidth.,hardware,2026-02-04 21:13:42,1
Intel,o32ir90,The power of a 4060 for the cost of a 4080!,hardware,2026-02-02 00:29:55,49
Intel,o33094p,"A better compromise for gaming handhelds would probably be 192 bit bus, running ~16 XE3 cores. Wider and slower can work very nicely for power efficiency but cost is an important consideration too. Ofc the real killer is economies of scale - nobody's going to do a bulk order of 5 million of these things to make it worth their while.",hardware,2026-02-02 02:08:56,8
Intel,o34in6h,"Intrinsically, 4ch is not *that* much more expensive. And even considering the CPU config STX-H has more than just product cost being reflected in its pricing.",hardware,2026-02-02 08:46:30,1
Intel,o32nfew,> Intel seems to have struck a good balance for the target market.  Technology and market-awareness... both are important,hardware,2026-02-02 00:55:36,1
Intel,o31mfsq,Shout out to my sweet i7-920 and 6GB of Dominator DDR3,hardware,2026-02-01 21:39:36,12
Intel,o34t7gy,quad channel on X79/X99,hardware,2026-02-02 10:28:25,4
Intel,o35piou,On consumer boards?,hardware,2026-02-02 14:16:33,3
Intel,o33zd42,5 different sticks running single channel was perfectly fine ever more further back :),hardware,2026-02-02 05:53:40,4
Intel,o31i4ua,"256=32*8, and also ddr5 channels being 32-bit is superficial",hardware,2026-02-01 21:18:53,11
Intel,o31mxrs,Eh Strix Halo doesnâ€™t employ DRAM on package.,hardware,2026-02-01 21:42:02,16
Intel,o33zm3i,"Yes, the sector does only 12 billion in revenue, completely nothing.",hardware,2026-02-02 05:55:41,2
Intel,o3783no,">The guy who turned Intel around got fired lol.  Intel isn't even turned around yet.   >They were mad that Intel wasn't doing well but they didn't give enough time for the dudes projects to come to fruition  The problem was that, if external customers would have ended up using their fabs, they would have known before hand, since the contracts have to be inked and it takes a year or two at least to even just port IP over.   >Intel is going to have a short golden period while pat gelsigners projects come to fruition and then after like 3 years they are gonna be back to shit.  Why do you expect Intel to have a short golden period soon?",hardware,2026-02-02 18:34:50,3
Intel,o31org8,what I meant was that there would be 2 memory modules. A laptop with only one gets half the memory bandwidth,hardware,2026-02-01 21:50:51,1
Intel,o33zwob,"Correction, Lunar Lake had the 140V iGPU, which was based on Xe2, or Battlemage.     Arrow Lake was the one that got 140T, which was basically a A380 (Meteor Lake got an A380 but without XMX).",hardware,2026-02-02 05:58:06,10
Intel,o34izoh,"There is one possible out, however. If someone can figure out how to design the package such that OEMs can own the memory attach.",hardware,2026-02-02 08:49:46,7
Intel,o31odce,"There is some humor in how their necessary front-loading of LNL packages mitigated needing to source more LPDDR5x after it spiked in cost. They sort of lucked into that one, with LNL able to simply clear out existing stock at its own natural pace during the transition to PTL.  \[Edited a weird sentence\]",hardware,2026-02-01 21:48:57,12
Intel,o3198jm,It's not expensive. The challenge is with the margins of Intel reselling memory to OEMs.,hardware,2026-02-01 20:35:07,11
Intel,o319pjz,Even QC is only doing on package memory for 1 SKU (now 2 but same),hardware,2026-02-01 20:37:26,5
Intel,o31hxkg,"> Though how much higher is kinda hard to tell, with the whole component shortages due to the AI boom prob also playing a part.  Ye, people comparing launch prices of today vs previously launched products. Are up for a rough awakening of what those previously launched products will cost in 6+ months from now when the DRAM apocalypse catches up to them and current stock is depleted.",hardware,2026-02-01 21:17:54,19
Intel,o32in8w,You can buy mini pc's with Strix Point for $600 now though. I doubt they're ever going to sell devices with this iGPU for a price that low with those requirements. Not with the way RAM prices are now.  It's going to be closer to Strix Halo pricing than Strix Point.,hardware,2026-02-02 00:29:19,-8
Intel,o32pvfn,"And overhauling the BIOS every 6 months, with a new BIOS X+1.",hardware,2026-02-02 01:09:34,13
Intel,o31xpgj,"If they ever use Nvidia, it'll be for Strix Halo tier chips. Intel isn't getting rid of their GPU department entirely.",hardware,2026-02-01 22:35:14,26
Intel,o338hbo,No idea. I *am* working on talk from before that announcement.,hardware,2026-02-02 02:55:30,5
Intel,o31k8uo,Hopefully never at this point.,hardware,2026-02-01 21:29:01,11
Intel,o35quus,I think those will be very AI-centered products.,hardware,2026-02-02 14:23:43,1
Intel,o3lq2nb,"When I say ""reuse NVL construction"", they will almost certainly reuse the SoC die. Meaning, no LPDDR6, and at best a bin or two higher LPDDR5 speeds. For that matter, they will almost certainly reuse the NVL graphics tiles as well.",hardware,2026-02-04 21:21:42,2
Intel,o33dguo,"idk man, it has a 16c32t zen5 CPU attached to it... so...",hardware,2026-02-02 03:24:50,18
Intel,o32ull5,More like mid range performance (5060 at BEST) for high end pricing.,hardware,2026-02-02 01:37:03,12
Intel,o319037,"But think of what range the Panther Lake platform covers. The B390 is the top end. You have essentially the same platform support down to the 4Xe entry GPU. So yeah, maybe dual channel is not quite ideal at the far end of the distribution, but you need to compromise somewhere.Â    If they went for 256b, they'd really need a significantly bigger GPU to justify it. Something like Strix Halo or the cancelled NVL-AX.",hardware,2026-02-01 20:33:57,20
Intel,o317v6i,"Yes, though it has tons of cache to service it (16 MB GPU L2, 8 MB SLC).  Whatever the means, the end result is what matters. In that regard, it's as good as a 4050. â€‹",hardware,2026-02-01 20:28:22,19
Intel,o373uqe,"oh boy, this brings back memories, my second ever PC build was this exact setup!",hardware,2026-02-02 18:15:42,2
Intel,o3agic1,My 7900x Intel at 5ghz all core still running today.,hardware,2026-02-03 04:52:26,2
Intel,o3886ls,"Yup, I used a Intel i7 920 with my Asus P6T motherboard and had 3x2GB memory.",hardware,2026-02-02 21:23:14,3
Intel,o3bnzxi,"High-End Desktop (HEDT), but yes. Would later move to quad channel.",hardware,2026-02-03 11:18:18,2
Intel,o34wun0,"Strix Halo has 16x16b LPDDR5X, not DDR5. And it's not superficial, they function exactly like separate channels always have.  It's just a convention to normalise to 64b channels when discussing components to make it clearer what the total bus width is, independent of differing channel widths.",hardware,2026-02-02 11:01:44,5
Intel,o31o7p0,Yeap. Thanks. I've made correction.,hardware,2026-02-01 21:48:12,3
Intel,o3ap2wy,"yea my bad, I was referring to Strix Halo which [literally uses 8 soldered ram pieces](https://cdn.discordapp.com/attachments/722681490097569804/1458271170729082982/Screenshot_20260107_082523_YouTube.jpg?ex=6982a0dc&is=69814f5c&hm=3797caca5cee2b406f2ab3e6567eb04103242084d2efac8f33436c4df44099d3)",hardware,2026-02-03 05:56:56,1
Intel,o341re1,He means by comparison duuh,hardware,2026-02-02 06:13:23,17
Intel,o31unz7,it'd be fabulously stupid to have half-populated soldered memory.  lpddr = soldered.  dimms are slow and power hungry.,hardware,2026-02-01 22:19:49,20
Intel,o34crnr,"> (Meteor Lake got an A380 but without XMX)  This was such a bizarre omission, I don't understand it.",hardware,2026-02-02 07:50:37,7
Intel,o39h81m,"Interesting. So chipmaker (such as Intel or Qualcomm), solders the SoC on package, then sends it off to the device OEMs, for them to solder their choice of RAM on the package.?",hardware,2026-02-03 01:19:19,2
Intel,o31gh6n,In what world is DRAM not expensive rn brother,hardware,2026-02-01 21:10:57,-4
Intel,o32lowc,"Why would the ram affect this but not strix point?Â   It doesnâ€™t say it canâ€™t support SODIMM to ship barebone mini pcs without ram. It just says it wonâ€™t be badged â€œB390â€.Â   Again why are you using 1.5 year old product pricing vs launch pricing in the first place? Like I said, do July 2024 pricing. If you canâ€™t, then i canâ€™t help you with current pricing.Â    https://www.lenovo.com/gb/en/configurator/cto/index.html?bundleId=83RWCTO1WWGB1   https://www.lenovo.com/gb/en/configurator/cto/index.html?bundleId=83Q6CTO1WWGB2   Hereâ€™s identical Legion 5 laptops with Panther vs Gorgon. Very similar pricing. Anything else?",hardware,2026-02-02 00:45:52,18
Intel,o31scit,Intel would have already been very familiar with how the B390 was going to perform when they signed that deal.,hardware,2026-02-01 22:08:22,17
Intel,o33fkah,"I would've been more interested in a Strix Halo machine if it had FSR4.  I'll be waiting for the next version of it, cause that would make for an amazing gaming tablet on the go, and a great workstation connected to a dock on the desk.",hardware,2026-02-02 03:37:29,22
Intel,o34bmf4,In a power envelope where it can't be used correctly.Â  It loses in a bunch of benchmarks to M4 Pro that only has 10P coresÂ    That's why it is mini pc only,hardware,2026-02-02 07:39:54,6
Intel,o31bgh1,Well the 4Xe3 nearly at level of 8Xe2 in LNL if you look at game benchmarks that alone is a achievement https://www.ultrabookreview.com/74624-intel-panther-lake-laptops/,hardware,2026-02-01 20:46:04,9
Intel,o31b0m7,Since when is nvlax cancelled?,hardware,2026-02-01 20:43:52,2
Intel,o39mluv,Itâ€™s an improper consumer convention to normalize 64b as a channel*,hardware,2026-02-03 01:50:09,0
Intel,o31qc1d,"On the topic, SOCAMM would be a rather nice solution repairability wise whilst not trading much real estate since 256bit requires only 2 modules. Not quite thin and light orientated but mobility is not out of question.",hardware,2026-02-01 21:58:29,12
Intel,o3au2pl,By that comparison noone should ever bake bread because all the money is in cakes.,hardware,2026-02-03 06:39:01,2
Intel,o32qtnw,"Fabulously stupid, but not without precedent. Iâ€™m pretty sure â€œNuclear Laptopsâ€ has covered at least one laptop that supported dual channel but the manufacturer only soldered a single channel without even an empty DIMM slot for the other channel. Granted, the premise of those videos are theyâ€™re cheap, ~$300 laptops that are all terrible in one way or another, probably not the _most_ likely to happen to top end Panther Lake.",hardware,2026-02-02 01:15:04,10
Intel,o34is6s,"> lpddr = soldered  Not with LPCAMM, but they haven't yet proposed a single channel module. Definitely not impossible though, and may even be likely.",hardware,2026-02-02 08:47:49,3
Intel,o3args2,pre-2024 Asus Zephyrus laptops actually do use 1 soldered memory + 1 SODIMM slot,hardware,2026-02-03 06:16:35,2
Intel,o39reli,"Yes, that would be the ideal. The current process has both the memory and SoC attached at once. If you could separate out those two steps, then you could have the customer source the memory, and problem solved.  And this isn't some pie-in-the-sky fantasy either. Would require some packaging innovation, sure, but I don't think anyone seriously questions the feasibility at a conceptual level. The problem is that the overlap between both ""can"" and ""want to"" has been pretty low.",hardware,2026-02-03 02:17:08,3
Intel,o31j798,It was removed before the DRAM crisis because managing so many SKUs is a nightmare for Intel.,hardware,2026-02-01 21:24:01,12
Intel,o33d7ur,Having it be on package isn't more expensive then on the motherboard.,hardware,2026-02-02 03:23:20,1
Intel,o33bj4i,"The Intel version of that laptop only has either a 356H or 386H, those SoCs only have 4 Xe cores so they wouldn't have the B390 or B370 branding regardless (that's only on the models ending in --8H)  Soldered RAM is mandatory for the --8H SKUs, the Intel product pages for them only mention LPDDR5X  [IntelÂ® Coreâ„¢ Ultra X9 Processor 388H](https://www.intel.com/content/www/us/en/products/sku/245526/intel-core-ultra-x9-processor-388h-18m-cache-up-to-5-10-ghz/specifications.html)  Other SKUs mention both LPDDR5X and DDR5  [IntelÂ® Coreâ„¢ Ultra 9 Processor 386H](https://www.intel.com/content/www/us/en/products/sku/245529/intel-core-ultra-9-processor-386h-18m-cache-up-to-4-90-ghz/specifications.html)",hardware,2026-02-02 03:13:18,-2
Intel,o32p8rq,">Why would the ram affect this but not strix point?   Because Strix Point doesn't have a 7467 MT/s RAM requirement like this one does to be called a B390 iGPU product.  That's the whole point of the article, for it to actually be a B390 it needs to have 7467 MT7s RAM or better. Which will make it very expensive.  >Again why are you using 1.5 year old product pricing vs launch pricing in the first place?   You skipped my point, again. This chip (in the configuration which makes it ""B390"") will never be as cheap as Strix Point. The B390 going to be a premium priced product closer to Strix Halo than to Strix Point. Yet Intel compares its performance to Strix Point and not Strix Halo.  >Hereâ€™s identical Legion 5 laptops with Panther vs Gorgon. Very similar pricing. Anything else?  None of your examples are B390 certified.",hardware,2026-02-02 01:05:53,-11
Intel,o33omqo,"Intel was also not really in a position to turn down a $5B investment. And the deal adds native NVLink support for Xeon, which benefits Intel more than any potential Nvidia deal for iGPUs may hurt them",hardware,2026-02-02 04:35:06,14
Intel,o33ph8o,"I had an HP G1A with 128GB RAM on release in checkout when I found out it was RDNA 3.5, I rubbed my eyes in disbelief but the spec list didn't change. It was the saddest tab closure of my days  That literally more than halved the value proposition of an already obscene price in an instant.",hardware,2026-02-02 04:40:52,7
Intel,o355lxb,"X86 loses in single threaded performance at every power range for consumers. As for the 10P cores. E cores are known to boost multi threaded performance by a lot. The M4 pro has 10 of them. Yes, it'll beat it in some benchmarks.",hardware,2026-02-02 12:13:20,9
Intel,o31chxu,What? Your link is showing the full LNL config being roughly 1/3rd better.,hardware,2026-02-01 20:51:13,3
Intel,o31cw26,"There've been rumors about its cancellation for a while, and with Intel marketing just saying they have no plans for such a product, think it's safe to say it's dead.",hardware,2026-02-01 20:53:10,8
Intel,o31tzax,"Yea, i hope that CAMM2/LPCAMM2 will succeed both on laptops and on PCs.   While CPU dont need typically wide bus with high bandwith, higher MT/s is helpful and CAMM helps with that and thats pain point on PCs.   While simultaneously can have wide bus for iGPU as You say.  Unfortunately with current RAM shortage getting any kind of ram is pain :/.",hardware,2026-02-01 22:16:26,12
Intel,o3aszw4,"so do lenovo thinkbooks.  it's the worst of all worlds- no lpddr, and you have to match the memory speeds and timings to the soldered memory.",hardware,2026-02-03 06:29:35,3
Intel,o5ck59o,"Possible - yes, economical? Not really - those packages need to go through additional testing - not the ones that is done on OEM, what would happen OEMs would need to send the memory and packages to some 3rd party to solder and test those packages and then assemble the board with the complete package. It's more complexed process no matter how you turn it, and it ends up with more expensive device. It can really work only where you're starting with expensive ""premium"" devices to begin with and you have vertical integration from silicon to the device itself - pretty much just Apple, maybe Huawei.",hardware,2026-02-14 15:03:09,1
Intel,o336pbe,"No, it's primarily the margin issue. Intel isn't shy about SKUs.",hardware,2026-02-02 02:45:24,8
Intel,o32ughi,">Â Because Strix Point doesn't have a 7467 MT/s RAM requirement like this one does to be called a B390 iGPU product.  It does not need to be called B390 to obliterate the 890m. 890m isnâ€™t as fast when using DDR5-5600 as with 7500 (promo material used during the original strix point presentation), either.Â   >Â You skipped my point, again. This chip (in the configuration which makes it ""B390) will never be as cheap as Strix Point. It's going to be a premium priced product closer to Strix Halo than to Strix Point.  >Â You skipped my point, again. This chip (in the configuration which makes it ""B390) will never be as cheap as Strix Point. It's going to be a premium priced product closer to Strix Halo than to Strix Point.  Strix halo is 256 bit memory and a significantly bigger die. Why would just the mere fact that Panther B390 requires soldered LPDDR ram mean it will be comparable to Halo? Does soldered memory automatically make strix point expensive like Halo?Â   >Â None of your examples are B390 certified.  It is sufficient to demonstrate that with equalised ramÂ itâ€™sÂ not more expensive than Gorgon.",hardware,2026-02-02 01:36:13,16
Intel,o3407ej,"Also an NVIDIA deal for iGPUs really doesn't hurt them, because it allows them to  develop a Strix Halo level APU competitor for laptops without having to take a risk on trying to compete with NVIDIA's mindshare or technology.     They are still perfectly free to pursue developing lower end iGPUs, which they most likely will be.",hardware,2026-02-02 06:00:34,14
Intel,o31csls,No? Have you checked the correct table it's not 1/3 better in game s,hardware,2026-02-01 20:52:41,6
Intel,o33le5u,With the DRAM shortages Intel could sell more chips with the supply it would eat   Doesn't make sense in current environment sadly,hardware,2026-02-02 04:13:46,1
Intel,o5dpuuq,"> Not really - those packages need to go through additional testing - not the ones that is done on OEM, what would happen OEMs would need to send the memory and packages to some 3rd party to solder and test those packages  That is an extra step, yes, but isn't much different than what's already done with stuff like memory DIMMs. And you do get the advantage of transferring some of the complexity away from the motherboard to the Intel-owned package.  Ultimately, don't think that would be a bottleneck if the procurement/attach model can be rectified.",hardware,2026-02-14 18:34:45,1
Intel,o333red,"But we're talking about the B390. And that's what Intel is using in its presentations. I'm making the point that according to this article, the B390 will most likely be priced closer to Strix Halo than Strix Point.  7467MT/s RAM (and above) is expensive. RAM is much more expensive than it used to be, cutting edge RAM speeds even more so.  >It is sufficient to demonstrate that with equalised ram itâ€™s not more expensive than Gorgon.  We're talking about the B390. The one Intel is using in its presentations. None of your examples are B390. The article is about the B-series. We shouldn't use the B390's performance numbers to compare to Strix Point, but only use the prices of the non B-series. They are irrelevant for this discussion.",hardware,2026-02-02 02:28:45,-6
Intel,o31dlw1,"Ah, you're right. Was scrolling on mobile and stopped at the synthetics. Well, good showing then, and also good to see more a focus on actual game performance.",hardware,2026-02-01 20:56:43,8
Intel,o33umwd,"What do you mean? If anything, the current environment would favor such a system.",hardware,2026-02-02 05:17:10,2
Intel,o5f0q8o,"Memory DIMM are tested on the OEM factories, this is quite different as you need to test the entire package with memory. Also such modules are more expensive (smaller size -> denser with smaller contact points) and harder to source. As I see it all it will do is move the complexity from the SoC manufacturer to the OEMs, and the OEMs each will need to invest quite a bit on expensive new testing equipment - end results you still pay more.",hardware,2026-02-14 22:47:08,1
Intel,o365bez,MSI Prestige 14 Flip with B390 is up for $1300. The cheapest upcoming Strix Halo laptop is ASUS TUF A14 $1800. Other laptops with Strix Halo are over $2000. Intel Panther Lake is closer to Strix Point in price,hardware,2026-02-02 15:36:52,6
Intel,o31dtxf,No problem seeing 4Xe3 performance so well feels like 12Xe3 is starving for more memory bandwidth,hardware,2026-02-01 20:57:50,2
Intel,o35gckm,OEM's can't buy enough DRAM   Intel can sell panther lake in 16GB laptopsÂ    While all the halo stuff is 32GB and mini pcs are 128GB,hardware,2026-02-02 13:25:13,1
Intel,o5f6nf3,"> Memory DIMM are tested on the OEM factories, this is quite different as you need to test the entire package with memory  The assumption is that the CPU part of the packaging would be tested separately. So only the memory attach would need to be validated by the ODM.   > Also such modules are more expensive (smaller size -> denser with smaller contact points) and harder to source  There's really no significant difference for on package memory. Not sure where you're getting that from.   > As I see it all it will do is move the complexity from the SoC manufacturer to the OEMs, and the OEMs each will need to invest quite a bit on expensive new testing equipment  Again, this is something they already have to do. Would not be such a burden for them. The main hurdle is someone funding the technology development.",hardware,2026-02-14 23:22:28,1
Intel,o3410v8,"Yeah, I feared PTL-U is going to be a significant regression from LNL, where the GPU is concerned. Well, I see that's not the case.â€‹",hardware,2026-02-02 06:07:17,3
Intel,o36eimp,"Problem is that dGPU systems would have to source VRAM in addition to DRAM, whereas a large APU only has to source slightly more DRAM.",hardware,2026-02-02 16:19:47,2
Intel,o5fbi90,"The SoC is tested before, but once you installed the memory it will be needed to be tested as a whole package before attach it on the board. About the packages - look at some M series chips that people removed it from, it quite different than your regular laptop LPDDR.   >Again, this is something they already have to do. Would not be such a burden for them. The main hurdle is someone funding the technology development.  Let say someone developed all the equipment so the OEM will be able to do it themselves - for it complexity it requires more employee training and qualification and the equipment costs lots more, so instead your regular equipment and testing you end up with much more expensive equipment that also needs to be configured for each SoC, and higher labor costs - you still end up with increased price, other option that OEMs will send it to specialized 3rd party won't be cheap either.",hardware,2026-02-14 23:52:38,1
Intel,o1yuwnf,"TLDR:    ""The performance advantage over the previous Arc Graphics 140T/140V iGPUs is around 70%. The advantage over the smaller Radeon 800 series iGPUs of AMD Zen 5 is also considerably high (between 50-80% depending on the benchmark).   Although the Strix Halo GPUs are even faster (but not more efficient), they operate at higher power limits. There are only a handful of corresponding devices on the market, which then are also quite expensive.""",hardware,2026-01-27 05:48:08,61
Intel,o22xnz4,"This will be nasty on handhelds, because a 4050 at 720p runs just as fast as a 3070 at 1440p.",hardware,2026-01-27 20:13:29,15
Intel,o20hx1c,What does this mean for handhelds.,hardware,2026-01-27 13:39:04,12
Intel,o1zktl8,"As much as its nice to see it competing with low powered 4050. The 4050 and even  full powered 5050 and 5060 laptops are significantly cheaper than the B390 laptops.  You can get 5070 to 5070 ti laptops for the price of of laptops using the b390  On Best Buy  Dells XPS 14 is 2249 with the X9 388H with 32 GB ram  https://www.bestbuy.com/product/dell-xps-14-copilot-pc-14-2-8k-oled-touchscreen-laptop-intel-core-ultra-x9-388h-2026-32gb-memory-1tb-storage-graphite/J3K4L6QWVR   The G14 is 2399 with a Ryzen AI 9 hx with 32 GB ram AND A RTX 5070 Ti  https://www.bestbuy.com/product/asus-rog-zephyrus-g14-14-3k-oled-120hz-gaming-laptop-copilot-pc-amd-ryzen-ai-9-hx-32gb-ram-nvidia-rtx-5070-ti-1tb-platinum-white/JJGGLHJXQ9/sku/6613954  And that's a premium G14. The Acer 16S with Ultra 9 288H, 32 GB ram and 5070 ti is currently 1699  https://www.bestbuy.com/product/acer-predator-helios-neo-16s-ai-gaming-laptop-16-oled-240hz-intel-core-ultra-9-nvidia-geforce-rtx-5070ti-32gb-1tb-obsidian-black/JJ8V8H38XT  Intel needs to get pricing to RTX 5050 levels or below to be actually competitive.",hardware,2026-01-27 09:32:42,37
Intel,o1z66zy,">and the two render slices each consist of six Xe cores.  I assumed it was 3 slices, of 4 cores each. Is there a benefit to doing it this way, or does it not matter?",hardware,2026-01-27 07:18:46,6
Intel,o2cfs7p,"Intel is back, it will be at 1 trillion market cap in next 18-24 months",hardware,2026-01-29 03:17:22,1
Intel,o74zkuy,Interesting but Intel lies all the time.Â  If they even hit stores I'll be surprised.,hardware,2026-02-24 14:09:40,1
Intel,o1yuic6,"Hello Antonis_32! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-27 05:45:08,0
Intel,o1zdksn,"Stop with these fake benchmarks.  You will see when the benchmarks are not cherry picked, it will be no where near a 4050 lol. They had to gimp the poor 4050 all the way down to 30w, which I donâ€™t even know how it even operates as such low wattages for the b390 to have a chance.  The full wattage 4050 (100w+) is about as fast as a 3060 desktop, this tiny iGPU in real world gaming tests will be at most best case scenario as fast as a full powered 3050, which is still  a massive 50-60% slower than a 4050 laptop.",hardware,2026-01-27 08:24:59,-16
Intel,o200pvk,"Digital foundry did a video where they ran a strix halo at the same tdp as panther lake and it was notably faster, albeit with a much larger die size.",hardware,2026-01-27 11:49:39,16
Intel,o25qn5j,It means the next MSI Claw will be sweet,hardware,2026-01-28 04:46:58,8
Intel,o248cwm,For now nothing but they're in development of a handheld version and when that comes out handhelds are gonna be a lot better. Right now the only strix halo handheld is the gpd 5 and it's expensive the ultra 9 CPUs are going to be more affordable so we might see more options,hardware,2026-01-27 23:52:34,6
Intel,o203epu,"[MSai Prestige 14 Flip AI](https://www.bhphotovideo.com/c/product/1939343-REG/msi_prestige_14_flip_ai_d3mtg_001us_prestige_14_flip_ai.html) $1299, 358H, 32GB/1TB.",hardware,2026-01-27 12:09:32,12
Intel,o1zqy52,"You must have no idea how having to deal with only the iGPU in a laptop simplifies things in terms of drivers on Linux, and especially compared to something like a 4050 which only has 6GB of VRAM.   Not to mention having no need to lug around a 300+ W power brick for a gaming laptop.",hardware,2026-01-27 10:28:33,15
Intel,o1zu2ha,"> Intel needs to get pricing to RTX 5050 levels or below to be actually competitive.  Not really. They offer a product that fits in small form factor and gives very good battery life while also providing with graphics that can play pretty much any game on steam if you know how to navigate the options menu. If you are a student and need a device that is light/portable, has good battery life but also plays games it is a perfect fit.",hardware,2026-01-27 10:55:41,16
Intel,o208cuc,the MSI Prestige 14 is like 1300 iirc,hardware,2026-01-27 12:43:26,5
Intel,o24s54f,Itâ€™s waaaaaay too early to compare prices. Give it 6 months time and manufacturing ramping up to see what the b390 laptops are actually priced at. Itâ€™s the same for any new shiny device,hardware,2026-01-28 01:34:28,5
Intel,o1zlqot,"Do they? It's not marketed as a gaming laptop in the first place, it was never meant to compete with a 5070 Ti laptop.  We will also have to see prices in a few weeks once more models drop and see how X7 models pricing looks.",hardware,2026-01-27 09:41:21,13
Intel,o1zm4m2,"Intel could maybe possibily drop pricesâ€¦but they donâ€™t have to. These new devices are targeted for people who dont necessarily care about perf/$. Students, office, devs, travelers, creators   â€œDoes it last all day, is it decent and not sound like a jet?â€  And theyâ€™ll pay extra for that. Its basically M series   But at least with apple the laptops are made to last...but Intel comptiability is advantagous & people are used to windows.",hardware,2026-01-27 09:44:58,17
Intel,o20nb5q,That's a good price for a Helios you linked... don't expect the 2026 refresh of it to be priced the same when it replaces that model in a few months.   XPS 14 *is* very expensive. But that's always been the case for the XPS. They've always been expensive given their performance level. But you do get some of the best build quality available on a Windows machine in exchange.,hardware,2026-01-27 14:07:08,3
Intel,o206djg,"This is just how the Windows laptop market is. MSRP on new laptops are high, while last gen are on sale, it's always like this I'm guessing you forget that AMD once marketed Mendocino as being in up to $700 laptops.. B390 laptops will start at around $1100, but I expect that to decrease after companies get their premium lineups out and then do more budget models.  Like you can find Lunar Lake for $500-$600 new these days if you wait for sales and aren't picky about other aspects.  Anyways, yes you can get a stronger GPU laptop for not much more, but those dGPU laptops will eat through battery like no other, so I wouldn't even consider them a direct competition.",hardware,2026-01-27 12:30:12,2
Intel,o20x7et,People aren't buying these for price alone.,hardware,2026-01-27 14:56:20,1
Intel,o1zws13,"I saw this yesterday as well... The edge is dead on arrival at these prices. At those prices, I'd rather get HP zbook with strix halo.",hardware,2026-01-27 11:18:24,-2
Intel,o1zci8w,"It means less of the fixed function units in the render slice, which IIRC is rasterisers and ROPs. Compared to 3x4 it'll be less performant, but also smaller and less power hungry. More generally the ratio of compute to fixed function depends on the complexity of the shading - coarsely, low settings at high resolution would prefer more fixed function, high settings at lower resolution prefer more compute",hardware,2026-01-27 08:15:06,14
Intel,o1zie3d,"It pretty much matches 60w version of 4050 which is good result for an iGPU, it gets beaten handily by 90w but considering the form factor of devices B390 can be used in, the most relevant comparison is to the 60 and 30w version.",hardware,2026-01-27 09:09:56,15
Intel,o204xz4,"It depends on the TDP chosen, as we see in other reviews as at around 35W it's even and under that Strix Halo chokes itself. No point putting Strix Halo in most laptops or handhelds at its cost and power curve. Desktop replacements/workstation, absolutely but if you want to play games on battery you might as well just use PTL.",hardware,2026-01-27 12:20:24,33
Intel,o23xs88,What's that supposed to prove? Obviously a chip with way more transistors is going to perform better.,hardware,2026-01-27 22:58:45,4
Intel,o74zqnd,Chip or whole system ðŸ¤”,hardware,2026-02-24 14:10:31,1
Intel,o209i6f,"That's significantly better pricing but by a 65w charger suggests, its a lower-powered X7 chip. Not the full power used in the XPS 14, which comes with 100w charger  Probably 25w tdp. Instead of 45 tdp.",hardware,2026-01-27 12:50:50,4
Intel,o1zxvm6,"To be completely fair: 4050 laptops don't use 300W power bricks. It's a 50W GPU typically, I think the very highest power limit I have seen for one of those is 65W.  It's fair criticism though, you can absolutely squeeze this type of GPU into most ultrabooks if desired though and a 50 tier chip used to be a common inclusion in ultrabook machines with reasonable power requirements.",hardware,2026-01-27 11:27:23,5
Intel,o1zx762,Why do people think the G14 is some chonky laptop? It's similar to a MacBook Pro is terms of weight and size.  And it has all day battery life as well.,hardware,2026-01-27 11:21:52,-2
Intel,o20nnmm,"Different classes of machines each putting their budgets into different things. XPS 14 is basically the most expensive Windows Thin and Light, but it's also *the* standard (on Windows). That budget is going into materials and build quality. You'll always be able to find more performance for your dollar if you're willing to get a thicker, heavier laptop with a plastic chassis",hardware,2026-01-27 14:08:56,5
Intel,o1zm0hr,Its need to drop to like $1000 to be competitive with rtx 5050,hardware,2026-01-27 09:43:55,1
Intel,o203e4o,">These new devices are targeted for people who dont necessarily care about perf/$. Students,  Ah students, famously wealthy and like to splash their cash ðŸ˜€ /s",hardware,2026-01-27 12:09:25,11
Intel,o1zn5ov,The G14 pretty much lasts all day without sounding like a jet,hardware,2026-01-27 09:54:24,5
Intel,o218o6h,"> B390 laptops will start at around $1100, but I expect that to decrease after companies get their premium lineups out and then do more budget models.  Not sure how many budget models with B390 will be there, and price might as well go up (for basically every laptop) due to RAM and SSD becoming significantly more expensive over the year.",hardware,2026-01-27 15:49:18,3
Intel,o20odih,Thereâ€™s a 1300 MSI model. The other models pricing is likely because theyâ€™re extra premium laptops that OEMs typically ship new chips in first to jack up prices,hardware,2026-01-27 14:12:36,3
Intel,o1zm816,"It does not lol LMAO.   It will get no where close to a 60w 4050, with its puny 12 low powered xe3 cores, the 8060s with its massive gpu die and high tdp is still 13% slower than a 4060 laptop in 20+ game average tested by Jarrodâ€™s tech.    Watch when the real gaming benchmarks come, itâ€™s good for an iGPU but no where close to a 4050.",hardware,2026-01-27 09:45:50,-11
Intel,o20nv3l,"XPS 14 is likely using the 100W charger for faster charging times, not to accommodate higher TDPs.",hardware,2026-01-27 14:09:59,13
Intel,o21e19g,"Ptl doesn't need more than 45w at most, perf gains after 30w are low. The difference is just core count, GPU is the same. I'd argue more cores for a regular person doesn't matter much.",hardware,2026-01-27 16:12:37,4
Intel,o20dh0d,"Probably, but that's what PTL is specified at. 25W TDP and 65-80W power cap. The B390 that everyone wants out of these should perform basically the same here. You don't need a 388H to have that GPU. One thing I'll admit I didn't notice before, and that may hurt it slightly, is that the RAM is running at 8533 instead of the full 9600mt/s.",hardware,2026-01-27 13:14:40,7
Intel,o236a2c,"65W seems right, they noted 60W total system draw when testing it in games at CES on one machine.   The X9 only seems to have +100MHz clock speed over the X7, everything else seems the same.",hardware,2026-01-27 20:52:35,3
Intel,o20nor6,No that seems to be just about enough power considering that the full power draw of a CES display unit used to test games by reviewers drew 60 watts.,hardware,2026-01-27 14:09:05,2
Intel,o26ji0i,Funny thing is a lot of laptops now come with power bricks much stronger than the laptop actually needs and this is because its used for fast charging.,hardware,2026-01-28 08:34:24,2
Intel,o2060jj,"What will you do when you run out of 6GB VRAM though? Not to mention that when gaming, a laptop with it will typically have to dissipate twice as much power as Panther Lake.  Either way, the 300 W power brick comes with the 5070 Ti Laptops that the OP is comparing against.",hardware,2026-01-27 12:27:47,0
Intel,o2062tj,"Well that thing is also expensive, only a bit less than the Dell XPS. We are talking about devices like the MSI Prestige that is like $1300.",hardware,2026-01-27 12:28:12,5
Intel,o20o6y5,"It definitely doesnâ€™t have all day battery life, and I doubt the PTL+dGPU model wonâ€™t have drawbacks the normal PTL models donâ€™t in battery life",hardware,2026-01-27 14:11:40,1
Intel,o1zn99q,Walk into an office or university and see how many people pay $1400+ for gtx 1050 perf.   Not everyone wants the performance if they can get other things.,hardware,2026-01-27 09:55:19,11
Intel,o26jaar,So you want it to be sold for 400 dollars?,hardware,2026-01-28 08:32:25,4
Intel,o200wfi,"You're comparing an iGPU to a dGPU. These are in completely different categories and they do not compete directly with each other. These devices are literally in different classes and are aimed at different customers.  Panther Lake can be paired with a dGPU too, but for anyone who does not want that, this iGPU is simply phenomenal.",hardware,2026-01-27 11:51:00,10
Intel,o217sc9,How is battery life on those ~$1k 5050 laptops and how much do they weight?,hardware,2026-01-27 15:45:21,7
Intel,o26jkrw,"Students are extremely likely to make poor financial decisions, yes. Theres a reason so many predatory loan agencies have offices on campuses.",hardware,2026-01-28 08:35:08,4
Intel,o20ec0f,"Depends on the college. I find my university to be basically just that, though itâ€™s typically less students being strapped and more parents being strapped with cash",hardware,2026-01-27 13:19:33,4
Intel,o1znsjn,Dude go to uni or office and see the laptops people using. G14 is not it. Too big and fat,hardware,2026-01-27 10:00:11,-3
Intel,o1ztvi2,What are you on about? there are gaming benchmarks in that review... difference between B390 and 4050 60w is 9%,hardware,2026-01-27 10:54:00,15
Intel,o20p27x,The real gaming benchmarks are saying exactly this: that itâ€™s only slightly behind a 60 watt 4050,hardware,2026-01-27 14:16:05,7
Intel,o26k0ub,The 8060s is much slower than this iGPU.,hardware,2026-01-28 08:39:17,3
Intel,o1zpnqs,"The G14 is 1.57 KG and a 1.59 \~ 1.83 cm  Are you like studying with kids?  For reference, the MacBook Pro 16 is 2.14 KG and 1.68 cm thick and Macbook Pro 14 is 1.55-1.62 KG and 1.55 cm thick",hardware,2026-01-27 10:17:03,10
Intel,o20puir,"Guy did an actual test.   https://youtu.be/jrygnUnBRNI  Skip to 12:40  Performs like a full wattage 3050, LIKE I SAID BEFORE. Or a heavily gimped 30w 4050, but anything remotely close to a full wattage 4050 it is majorly behind.",hardware,2026-01-27 14:20:03,-2
Intel,o26pjrz,"The 8060s is MUCH FASTER than the b390 lol, what are you on about, some people are so confident in being wrong.",hardware,2026-01-28 09:30:41,2
Intel,o1zqjn7,"Whatever you're right, Intel should drop pantherlake to 1k. G14 is a slim & sleek design. Honestly dont even know why its not more widly used in offices & univerties",hardware,2026-01-27 10:25:00,-3
Intel,o21aswd,"B390 laptops will start at around $1100, but I expect that to decrease after companies get their premium lineups out and then do more budget models.  Can you give me a link (review will be fine) to ~3lb/sub 1.5kg laptop with full, +100W 4050 that performs on par with desktop 3060?",hardware,2026-01-27 15:58:32,2
Intel,o4lsa0e,"8060s on 30 watts performs the same, at least according to YouTube benchmarks",hardware,2026-02-10 12:38:13,1
Intel,o20n16i,">Â Intel should drop pantherlake to 1k.  Intel isn't making the laptops, they're making the chips.  OEMs have decided to debut them in their premium, thin and light flag ships + a price increase from RAM shortage",hardware,2026-01-27 14:05:41,3
Intel,o23tz75,"4050 is 3 years old now, thereâ€™s not many laptops with that gpu anymore on sale.   But the 4050 at max wattage was only around 10% slower  than a 3060 12gb desktop gpu, which is also reflected in their benchmark scores.   Theres now Lenovoâ€™s yoga pro 9i with a full wattage 5050, which is even faster than the 4050 and performs slightly slower than a 4060 desktop, itâ€™s a thin and light premium laptop lol, is light years faster than a b390 iGPU.",hardware,2026-01-27 22:40:13,-1
Intel,o20oeoa,It makes sense to debut in premium models for oems. Cant complain if there's consumers willing to buy them.,hardware,2026-01-27 14:12:46,1
Intel,o249ukz,"> Theres now Lenovoâ€™s yoga pro 9i with a full wattage 5050 [...] itâ€™s a thin and light premium laptop lol  In what world is 4.5lbs light compared to 2.5-3lbs laptops with PTL? That's not even including power brick, which adds at least 1lb.  You could've chosen something like ASUS Tuf A14, but in most cases battery life will be significantly worse, despite bigger battery. Oh, and it will be loud under load.  It's also just under $2k now, so pretty big difference.  It might be a surprise for you, but what many people want from those igpus is the most performance at certain form factor while at the same time being efficient enough to last whole day (or like 7-8 hours) not having to go to every battery saving setting possible.",hardware,2026-01-28 00:00:12,2
Intel,o2ng91s,"it's very cool how optimized that game will be i think, especially for 2026 with all the components shortages",hardware,2026-01-30 18:40:46,61
Intel,o2o244k,Every gaming dev should target the B390 as the 1080p 60fps on medium/low tbh,hardware,2026-01-30 20:20:15,43
Intel,o2niprn,It seems odd that the Arc B390 was listed as the minimum required IGPU when the much weaker discrete Arc A380 is also on there. I'm sure most of the last generation IGPUs like the Radeon 880M or 890M are also powerful enough (considering they are faster than the Arc A380).  We've always known that the Forza Horizon games are very well optimized and are able to run on IGPUs just fine. Keep in mind that Forza Horizon 5 runs just fine on the Vega 3 IGPU in the 7 year old AMD Athlon 3000G.  The article is presenting the trivial fact that the B390 is listed as the minimum spec for the upcoming Forza Horizon 6 as somehow being a huge win for Intel. That's not very good journalism. It was going to run on IGPUs regardless of whether or not the B390 existed.,hardware,2026-01-30 18:51:29,23
Intel,o2o6640,Wonder how many wafer starts per month Fab 52 is up to now. Seems 18A parts are going to sell well and they have several more coming. Hope they can keep up.,hardware,2026-01-30 20:39:35,6
Intel,o2vo381,"Pricing is not one of those ""victories""",hardware,2026-01-31 23:39:16,3
Intel,o3905cc,"Unfortunately it's likely the B390 will be priced closer to Strix Halo than to Strix Point. Which is awkward since Intel only compares it to Strix Point if their presentations, not to Strix Halo.",hardware,2026-02-02 23:45:06,-2
Intel,o2reyst,Any game that has to run on a Series S will consequently have very low minimum system requirements.,hardware,2026-01-31 08:57:50,26
Intel,o2oxc4r,Low minimum system requirements were never a guarantee that a game would perform decently. Especially when they don't even list the resolution and framerate they're targeting.,hardware,2026-01-30 22:51:09,21
Intel,o2pelu4,"Gives me hope that the Fable reboot will be well optimized. Just hoping playground games nails the story, gameplay and charm that we love with the series.",hardware,2026-01-31 00:25:19,3
Intel,o2ooiyh,Do you all forget how raw Forza Horizon 5 seemed when it came out despite having relatively low system requirements? Low system requirements != good optimisation.,hardware,2026-01-30 22:06:45,1
Intel,o2qs0yz,"except you, unreal engine",hardware,2026-01-31 05:34:52,8
Intel,o2nle78,"Yeah, it feels a bit like marketing and there have been a few of those.  Right now pretty much any computer with the X9 or X7 is expensive. You can find 5050 laptops for the same price. This chipset isnâ€™t going to do that well unless it can be found at a decent price.   Also buyers should be careful because not every laptop presented has the power and thermals set to permit full power sustained boosting at the wattages tested in the reviews.   Those laptops should get cheaper and so on of course. Right now we donâ€™t know. A Dell XPS costs more than a new mid-range MacBook Pro though, that is steep.",hardware,2026-01-30 19:03:14,14
Intel,o2pedv9,"Sometimes even big developers put surprisingly little work into assessing system requirements. Assassin's Creed Unity lists the GTX 680 as the minimum spec GPU and the GTX 780 (same architecture, more cores) as the recommended GPU.",hardware,2026-01-31 00:24:07,9
Intel,o2nl9l7,"A380 is \~140% raster performance of 890M based on Steel Nomad scores, doesn't seem faster than 890M at least at the wattages those are ran at",hardware,2026-01-30 19:02:40,8
Intel,o2pinvg,"tbf the base requirement for Microsoft is probably the Series S, a case the B390 is probably competitive with.",hardware,2026-01-31 00:47:36,17
Intel,o3952om,"At the same time, it seemed to run at 30fps on the series X. So Iâ€™ll assume itâ€™ll be pretty CPU heavy.",hardware,2026-02-03 00:11:57,1
Intel,o2tujqp,UE5 is the ultimate benchmark for this GPU.,hardware,2026-01-31 18:13:30,5
Intel,o2phpsz,"> You can find 5050 laptops for the same price.   For now, I wouldn't hold on and wait for better prices. Panther Lake laptops are being priced in the post RAM hike world. Older laptops are still floating around and being priced of DDR prices from 6 months ago.  If that 5050 laptop launched today, it would have to deal with $100+ dollars in extra manufacturing cost.  Also the initial launch models for new Intel CPUs always come at a premium. Budget models will come and also prices will be slashed from launch prices. Something that has already happened with older generations.",hardware,2026-01-31 00:42:24,12
Intel,o2tumme,Shouldn't the base line be the new Xbox handheld?,hardware,2026-01-31 18:13:52,1
Intel,o2u6srb,"If you're talking about the ROG Ally, then no.",hardware,2026-01-31 19:11:08,5
Intel,o2uklwi,"No, I'm talking about the Xbox Ally X.",hardware,2026-01-31 20:18:06,2
Intel,o2umfgm,"Okay, so same thing. No, that is not Microsoft's baseline.",hardware,2026-01-31 20:26:59,10
Intel,o2uqvin,"Why not? Microsoft announced it like it's an official console, shouldn't their first party studios support it then?",hardware,2026-01-31 20:49:01,1
Intel,o2uv5er,"Probably two main reasons off the top of my head: the Xbox Ally released quite late in this console cycle and it's very weak. There already exists a large library of games that would not be playable and Microsoft suddenly (drastically) lowering the minimum target would be messy for devs. Even the Series S can barely do raytracing effects, for example. [Microsoft maintains a compatibility list](https://www.xbox.com/en-US/handhelds/handheld-compatibility) much like Valve with the Steam Deck.  Maybe things change with the next gen if Microsoft brings out a first party handheld, but for now it's just a bonus if runs on the Ally.",hardware,2026-01-31 21:10:17,5
Intel,o2x762v,"What purpose is going to be achieved by supporting superficial gimmicks like ray tracing? Four generations of NVidia, three generations of AMD in, it's still incredibly difficult to find a game where the ray tracing is visible without a side-by-side screenshot comparison AND where it doesn't make the game look worse. And forcing that setting to ""on"" with certain presets doesn't exactly fix that in any way. Sure, I can continue to waste power and processing cycles on it, but it doesn't change how I still have yet to find enough games to tick BOTH of those boxes to run out of fingers on the two hands. Even after padding out the list with Quake 2 and crap Minecraft variant.",hardware,2026-02-01 05:17:24,-1
Intel,o1vq8zu,">Both Arc Pro B70 and Pro B65 are based on the BMG-G31 GPU, which is now confirmed. The memory configuration is also the same. Both cards are said to feature 32GB of GDDR6 on a 256-bit bus. The main difference is GPU core configuration. [...]  >Based on conversations we have had, it appears Intel has put the gaming variant on hold, possibly to see how the memory market develops. This does not look promising for anyone expecting a near-term launch. The longer the delay, the less sense a release makes.",hardware,2026-01-26 19:55:37,26
Intel,o1ylj6k,"Another day, another exclusive BMG-G31 leakâ€¦",hardware,2026-01-27 04:42:16,10
Intel,o1vq3yo,"Hello PorchettaM! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-26 19:55:02,1
Intel,o1w7usf,Hoenstly B770 with Xe2 at this point makes no sense. Rather if they'd do a Xe3P dGPU release late 2027 especially if memory prices collapse.,hardware,2026-01-26 21:13:41,11
Intel,o1whebt,I wonder how different Xe3P is to Xe3. Xe3P could be further along than we expect.,hardware,2026-01-26 21:55:54,5
Intel,o20cbwr,Rumour suggests Xe3P has 20-25% architecture improvement over Xe3.,hardware,2026-01-27 13:08:04,5
Intel,o28rbgk,I mean in terms of release.,hardware,2026-01-28 16:47:13,1
Intel,nzd1gfc,"that amt of hair in a clean room, steve should wear a hooded rain coat lol.",hardware,2026-01-13 15:00:01,161
Intel,nzcvzjv,I thought that was Denis lol,hardware,2026-01-13 14:32:09,60
Intel,nzd3sjp,"Yeah this is a really surprisingly open factory tour, at least compared to when Sapphire took Linus and Alex along. It feels unprecedented to see this much access and insight, but that's the goodwill nurtured by Steve paying off in spades. The lament about AMD and Nvidia ditching the sub-250 side is real, so having Arc actually be a usable option down here is going to be significant later on.",hardware,2026-01-13 15:11:40,34
Intel,nzfe7p7,"Pretty good video with a lot of detail I haven't seen in previous factory tours. I probably still won't use an Intel GPU for gaming, but for a media server it's probably sufficient.  Side note: It's funny how whenever a GN video gets posted, the same cast of characters comes out and writes essays criticizing the video, apparently without watching it. I guess that's what it means to ""make it"".",hardware,2026-01-13 21:41:25,23
Intel,nzd7t05,"**GN:** NVIDIA and AMD abandoned this segment!  #Reality:  **B580:** $249 USD  **9060 XT 8GB:** $299 USD  **9060 non-XT:** $259 USD  **5060:** $299 USD  **5050:** $249 USD  I wouldn't say this is abandoned. I will say though the actual factory tour is cool, great content. But a dumb headline/title for the video.",hardware,2026-01-13 15:31:02,101
Intel,nzf5m3z,"Amazing video, GN is so good with content like this",hardware,2026-01-13 21:01:37,9
Intel,nzd8jmh,"rtx 5050 149 mmÂ² 128bit has similar performance to B580 272 mmÂ² 192bit. Selling for same msrp, winning by not ""trying"". Love how 5050 is actually at msrp now cheapest b580 is $349+  ""intel's gpu division seems like the only place in tech right now where the customers arent getting shafted these days""  People just want their Nvidia gpus cheaper  Nvidia crashouts making people hype an even worse product at current prices lol  [https://imgur.com/a/8iRI87Q](https://imgur.com/a/8iRI87Q)",hardware,2026-01-13 15:34:35,10
Intel,nzgtt65,"Love the Sparkle heatsink aesthetic, just wish there were some higher end card offerings from them instead of just Intel's lower midrange GPUs.",hardware,2026-01-14 02:14:38,3
Intel,nzd585j,He probably just single handedly killed 6 wafers worth of intel GPUs.,hardware,2026-01-13 15:18:37,5
Intel,nzd6pvl,AKA 'NVIDIA and AMD are going where the real money is.',hardware,2026-01-13 15:25:50,-2
Intel,nzcwvps,Finally a video that isnâ€™t â€œAI badâ€ or â€œcompany X badâ€,hardware,2026-01-13 14:36:46,-25
Intel,nzhjk4i,"Woah, a GN video that is actually interesting and informative for once instead of just ranting about the fact hardware companies exist to make money, not please gamers.",hardware,2026-01-14 04:51:57,-3
Intel,nzdfxes,That hat is so useful,hardware,2026-01-13 16:08:37,-6
Intel,nzdz3jo,Making them isn't important selling them is.  The reality is that this segment doesn't actually exist it has no buyers in it.  Seems intel is truly doomed trying to win segments that if they even exist aren't big enough to pay back their R&D even if the dominate them.,hardware,2026-01-13 17:48:25,-13
Intel,nzdogu2,"To be fair this is not a clean room, just SMT assembly. Basically, soldering components. A lose hair might be a bit of a problem (kind of like in any factory), but his hair is tied and doesn't seem to be an issue.  At an actual clean room (where the actual silicon is processed and etched), the standards can become very crazy. Some parts of it aren't even accessible to humans, just automated lines to avoid contamination.",hardware,2026-01-13 16:47:26,90
Intel,nzd8zmt,"Gaming Jesus could walk on wafers without corrupting a single tile, His body is that pure.",hardware,2026-01-13 15:36:40,-21
Intel,nzd5ucm,"same thought, just feels disrespectful",hardware,2026-01-13 15:21:37,-28
Intel,nzdzd0h,"He desperately needs to learn how to take care of his hair. I have no idea why nerds think a dry, frizzy mess of hair is some kind of enviable quality.   I feel like I'm walking into friday night magic every time he pops up.",hardware,2026-01-13 17:49:38,-18
Intel,nzhet37,"Intel hired him on the spot when they saw the ""Live, Laugh, Liao"" sign",hardware,2026-01-14 04:19:24,9
Intel,nzd0sl2,"who knew he can into GPU manufacturing? I thought he was a video editor and first real ""why are you employed"" guy on LTT.",hardware,2026-01-13 14:56:41,17
Intel,nzipwgx,I'm pretty sure Denis has neither long hair nor beard. Though to be fair I haven't seen him recently.,hardware,2026-01-14 11:05:09,1
Intel,nzhufav,"Itâ€™s crazy. I think Steve and GN are doing the lords work with their nonstop, accurate coverage of the absolute shit state of the computer component markets and the megacorps that perpetuate it. And their seemingly endless support and advocacy for *consumers*.   But then I come in this sub and see people are actuallyâ€¦ against this? I donâ€™t understand it. Probably the same mfs that bought MKBHDâ€™s wallpaper app lmfao",hardware,2026-01-14 06:14:39,9
Intel,nzd9rkp,"Wasn't that the B310? That was supposed to be the $100 bracket. To be fair that's an almost useless tier nowadays because iGPUs are capable of similar performance or even outperforming the cards in those brackets. Like the GT710 make no sense nowadays (their last attempt was what, the GeForce GTX 1630?), even though they sold like hotcakes for offices and for people who just wanted HDMI outs.",hardware,2026-01-13 15:40:18,61
Intel,nzdtxiv,"They're talking about the A310 and A380, of which Nvidia doesn't have anything made in this decade to compete with and AMD has the 6400 that came out 5 years ago.  The only cards with modern features in that price segment that consumers can directly buy are the A310 and A380.",hardware,2026-01-13 17:24:22,39
Intel,nzgaa45,He(Lucas) stated $100 was the market that was abandoned.  10:37  > Steve: So why still making A310?  > Lucas: Because what's the competitor have? Nvidia? like... GT710? GT1030? (laughs) No way. So literally Nvidia AMD already give up the segment of this like... $100 price card.,hardware,2026-01-14 00:24:50,12
Intel,nzfp313,This guy was talking about the A310 which is a $100 GPU. Basically said that the only other options at that price bracket are either a GT 710 or a GT 1030. And from AMD you can still get an old RX 550. The A310 may be slow but it beats those two gpu's by a mile.,hardware,2026-01-13 22:32:49,10
Intel,nze3uy1,> 9060 non-XT: $259 USD >  >   LOL. Good luck finding it. It's OEM exclusive,hardware,2026-01-13 18:09:42,10
Intel,nzdcsu3,The title is a literal quote from Lucas,hardware,2026-01-13 15:54:17,24
Intel,nzdb9mw,The title is in quotes. That is from Sparkle.,hardware,2026-01-13 15:47:16,9
Intel,nzejfd4,Now do SR-IOV and 16GB RAM for under $400.,hardware,2026-01-13 19:18:41,2
Intel,nzhv1ka,B580 has 12gb of vram though,hardware,2026-01-14 06:19:46,1
Intel,nzitx93,"Sad that all of those are 8gb cards. Even if comparable or faster than the B580 aside from that, the lack of vram makes them far inferior products imo.",hardware,2026-01-14 11:38:31,1
Intel,o07mcza,I just wish all of those options were more compelling tbh. I think what Steve was trying to say was that AMD and Nvidia have just half assed that market segment at the expense of the consumer. All these being 8gb cards for $250-$300 is kinda ridiculous in 2026. Iâ€™ll take a b580 over a 9060xt 8gb or 5060 any day of the week.,hardware,2026-01-18 01:21:19,1
Intel,nzenlzf,Go to r/PCMasterrace and they will downvote you into oblivion for even MENTIONING the possibility of gaming on a 5060 let alone 5050 XD,hardware,2026-01-13 19:37:44,1
Intel,nzdm48p,The title is just playing the YouTube algorithm game. It's stupid but they have to do it.,hardware,2026-01-13 16:36:44,-7
Intel,nze22z8,"> Love how 5050 is actually at msrp now cheapest b580 is $349+  Just an FYI but B&H has the [Acer Nitro B580 for $249.99](https://www.bhphotovideo.com/c/product/1874395-REG/acer_dp_z4bww_p01_nitro_oc_arc_b580.html) and the [Intel Limited Edition model for $259.99](https://www.bhphotovideo.com/c/product/1869297-REG/intel_31p06hb0ba_arc_b580_limited_edition.html). So you can get B580 for MSRP, but for how long who knows.",hardware,2026-01-13 18:01:44,22
Intel,nzdnidn,"die area is not the only cost indicator. B580 actually uses N5 fab, which is likely cheaper than N4, used by 5050. In reality, B580 only has about ~15% more transistors and if we assume N5 is cheaper per transistor and N5 has higher yields (by being more mature) i'd say their die cost might be very similar.  While yes, it has wider memory, the chips are clocked lower, so they can buy slower bins, reducing per chip cost.   B580 has more power draw, which in turn costs more for power delivery and cooling.  All in all, AIB manufacturer likely has lower margin per card as it stands, so i wouldn't be surprised if intel is taking a lower margin on the gpu/gddr combo to get more market share.  Nvidia on the other hand optimized their cost REALLY well, as they have been doing GPUs for almost 30 years.",hardware,2026-01-13 16:43:04,7
Intel,nzhfs1m,"Nvidia is getting such good performance out of such a small die because they're the best. Simple as. They've been doing GPU's for decades. It's not unexpected that at this stage Intel needs to use a larger die to match the performance - it would be incredibly surprising if that wasn't the case.  But a small *part* of that die size advantage comes from that narrow 128bit bus, and *part* of B580's appeal is its wider bus and subsequently more VRAM.",hardware,2026-01-14 04:25:57,1
Intel,nzppqcd,"I see b580 for â‚¬ 260. That's VAT included. rtx5050 is similarly priced.     And i found $300 lot on US amazon.  Considering other post, maybe search better than 1 place or something.",hardware,2026-01-15 12:00:19,1
Intel,nzd4pls,"Isn't the title pretty much ""AMD & NVidia bad""?",hardware,2026-01-13 15:16:07,55
Intel,nzd1pn1,Although even then the thumbnail is framed negatively.   But I much prefer these to Steves endless negativity ragebait.,hardware,2026-01-13 15:01:19,4
Intel,nzcyycp,> finally a company that isnt bad   ftfy,hardware,2026-01-13 14:47:24,-26
Intel,nzklu8k,"GN makes videos like these all the time, you just aren't watching them.",hardware,2026-01-14 17:20:56,2
Intel,nzgji2p,"Did you even watch the video? they are selling, and they are selling out, so much so that they want to ramp up production so they can push out more.",hardware,2026-01-14 01:15:59,8
Intel,nzfbt5x,I do not know if the numbering scheme from my workplace is common across the industry but the smt assembly would be in a class 5 or 6 clean room and the fabrication itself would be a 1 or 2 class clean room,hardware,2026-01-13 21:30:27,18
Intel,nzglawf,"I think it still matters to a certain point, thats why everyone in the factory is wearing a hat. The Factory boss decided to roll RNG dice and say *""Fck it, that small hat is fine, even tho wearing it is pointless now; I'll just pray nothing bad happen*"". lol  What hilarious is when you think about what going through uninformed factory-employee's head, after they saw some guy(Steve) walk-into the factory like that. Definitely a lot of ""WTF"" moment going through their mind lmao.",hardware,2026-01-14 01:26:15,2
Intel,nzdb445,"The factory boss told me not to bother tying my hair (""ä¸ç”¨ä¸ç”¨ä¸ç”¨, æ²¡äº‹å„¿æ²¡äº‹å„¿æ²¡äº‹å„¿. è¿™æ ·å¯ä»¥çš„"") when I started to put it under the hat... and after asking for a larger hat or hairnet.",hardware,2026-01-13 15:46:33,131
Intel,nzdfs8o,Nothing really surprises me with regards to gamers nexus at this point.  Edit: Downvote me all youâ€™d like. Theyâ€™ve been leaning incredibly hard into the rage bait type content of late.,hardware,2026-01-13 16:07:57,-58
Intel,nzi9ktg,"So, pray tell, who declared you the holder of absolute truth in terms of hair? What authority do you hold that allows you to determine what other people should do with the hair on their body?",hardware,2026-01-14 08:31:05,3
Intel,nziqu89,The other guy.,hardware,2026-01-14 11:13:11,1
Intel,nzklo11,This sub is genuinely weird. They complain about PCMR when the level of discourse here is much worse and a lot more mean spirited.,hardware,2026-01-14 17:20:09,3
Intel,nzk72ao,"They are truly rage baiting drama mongers sadly. The fact that the causes they support happen to be semi legit and consumer friendly in nature does not take away from the fact that they are essentially drama and negativity for profit at this point, and often out of touch with reality. You can be positive, polite, and have reporting standards while still calling out shitty behavior and ripping bad practices to pieces.   Companies are not your friend. That includes GN. They stoke hardware enthusiast anger for their own gain. Personally i think the harm they do to the space/peoples mentality is farm more than the amount of good most (key word most, fire risks and stuff deserve prompt drama) of their coverage does.   Even this title takes a topic and very educational opportunity to provide actual hardware coverage and immediatly tryâ€™s to stoke negative furry about some aspect of the hardware industry. Its just non stop with them. I dont think its healthy for anyone to watch then honestly, which is sad as i used to enjoy their review/news. I hope for their sake they are not actually so perpetually angry and they are doing what they do to knowingly manipulate their audience for increased views/engagement. I cant imagine living life so constantly angry at every possible point over computer hardware, as much as i am saddened by the current state hardware and love gaming.   As tough as it is with rising prices and such and sad to think of what we could have instead, modern hardware, gaming, and such is in a nearly historic good state (realistic cost to perf is probably lowest in history other than brief periods such as the 30 series launch), and is a very cost effective hobby compared with other ones for the time you get out of it. Many great games launch all the time (even if big name traditional games have largely went to trash). They are easier to run with hardware (like the ARC gpu) being quite cheap and capable. People, like GN, need to temper their overexagerated frustration with a dose of real world perspective and objectivity.",hardware,2026-01-14 16:13:54,-1
Intel,nzdm2vc,"Might be just the thing for older machines and a GNU Linux (or BSD like) migration. Or as a pass through GPU to Jellyfin, Emby, or Plex for media transcoding. Edit: or Small form computing tied together with a iGPU enhancing game performance...",hardware,2026-01-13 16:36:33,14
Intel,nze76iz,yeah there is a reason the a310 cards they are making are 4 hdmi out.  Gotta know your market,hardware,2026-01-13 18:24:10,18
Intel,nzeeiko,I assume those are being marketed to OEMs who make digital billboard systems or something. No idea why else you would want 4 HDMI ports on a card.,hardware,2026-01-13 18:56:32,6
Intel,nzddzc6,"> Wasn't that the B310? That was supposed to be the $100 bracket.  It's the A310, which is Alchemist and it's pretty much dogwater for anything beyond being a 'display out' card. The claim that NVIDIA has abandoned that segment is stupid... They've had offerings in this segment for years, plus anyone smart will just go buy a used GPU, your money goes way further. For example, the GTX 1650 performs basically 10-15% better, has better drivers, better encoding and generally is better supported. It's older, but I mean Alchemist wasn't exactly impressive either when it released and pretty much Intel has moved onto Battlemage and Celestial driver optimisations instead.   Plus let's be real here I went and searched and I found only weird places tend sell the brand new A310, the only local computer shop I found selling it in Australia for instance is a big one which is good surprisingly, but they had it for $189 AUD, a total rip tbh. A used 1650 is like $100 AUD and a used 1650 SUPER is like $120 AUD. No reason to buy an A310 tbh, pocket the cash and move on. Or if you're really intent on spending around that much buying a used RTX 2060 for like $20 AUD more, so a total of $200-210 AUD is better. Then on the AMD side you have the RX 6400 which had an MSRP of $159 USD and it's again a solid 10-15% faster, but much better off buying a used 6500 XT or 6600. Neither company has abandoned the segment, they had offerings for years and the used market basically obliterated any point to buying a brand new card like this.  >  To be fair that's an almost useless tier nowadays because iGPUs are capable of outperforming the cards in those brackets.  Yep this too. Honestly, I mean it's cool they're showing how they make cards on this factory tour, but to be like ""NVIDIA and AMD abandoned this segment"" is stupid when it comes to the A310. Almost anything these days is better than an A310.  >  Like the GT710 make no sense nowadays (their last attempt was what, the GeForce GTX 1630?), even though they sold like hotcakes for offices and for people who just wanted HDMI outs.  GT710 hasn't made sense for like 8 years at least, even when it was relevant people laughed at it, but it did the job for 'display out' and such which was all that mattered. GTX 1630 was okay but it was supposed to be $149 USD MSRP and it came out for like $200 USD in most stores due to GPU shortage at the time, not much NVIDIA could really do about that.",hardware,2026-01-13 15:59:39,-3
Intel,nzdyz47,They haven't made anything because the market has moved on. Intel might be making these but are they selling them?,hardware,2026-01-13 17:47:52,-11
Intel,nzdhs0m,"[Is it in reference to this moment in the video?](https://youtu.be/YwrUxG26ulk?t=648) If so, he doesn't say that as a literal quote, he says ""give up the segment"". Unless there's another quote somewhere else which I missed which may be possible or maybe it was edited out or cut from the video? I can't remember everything he said tbh but there was a lot of good information in this video and I think the title is better off without it. If it was called ""Intel Arc GPU Factory Tour with Sparkle"" I would have insta-clicked to watch anyways.",hardware,2026-01-13 16:17:04,14
Intel,nzdelag,You know what you're doing with the title... It's honestly unnecessary to use it on a factory tour video tbh.,hardware,2026-01-13 16:02:26,37
Intel,nzejqso,Stop defending your clickbait.,hardware,2026-01-13 19:20:08,15
Intel,nzf28eu,do you wanna address this then? Its kinda cringe ignoring the rest of the post  >**GN:**Â NVIDIA and AMD abandoned this segment!     >**B580:**Â $249 USD  >**9060 XT 8GB:**Â $299 USD  >**9060 non-XT:**Â $259 USD  >**5060:**Â $299 USD  >**5050:**Â $249 USD  Why include that in the title then too?,hardware,2026-01-13 20:45:49,2
Intel,nzg52uc,"Obligatory ""lol stupid pcmr amirite"" comment.",hardware,2026-01-13 23:56:47,1
Intel,nzdplnj,"The RTX 50 and 40 series are using the TSMC 4N node which is a custom version of the N5 node for NVIDIA. But anyway the N5, N5P, N4, N4P, N4X are all 5 nm class node, so have around the same price for the wafer. And I wouldn't be suprised that NVIDIA is paying less for these considering the volume compared to Intel orders.",hardware,2026-01-13 16:53:27,15
Intel,nzdpf4g,Well their gpus are much more expensive than amd & nvidia who arent even trying. When they try Intel wouldnt even have a chance  Nvidia increasing their entry gpu volume  [https://videocardz.com/newz/nvidia-reportedly-shifts-rtx-50-supply-toward-rtx-5060-and-5060-ti-8gb-in-2026](https://videocardz.com/newz/nvidia-reportedly-shifts-rtx-50-supply-toward-rtx-5060-and-5060-ti-8gb-in-2026),hardware,2026-01-13 16:52:06,1
Intel,nzi4dj7,Take a positive factory tour video. How can we spin it to make outrage?,hardware,2026-01-14 07:42:00,3
Intel,nzdqnhb,Yep,hardware,2026-01-13 17:07:48,6
Intel,o0a80gr,Baby steps.,hardware,2026-01-18 12:59:19,1
Intel,nzky9ti,Recently he's been making far more rants and less informative videos.,hardware,2026-01-14 18:16:33,-2
Intel,nzn1tla,those number grades are an ISO standard (14644) so they are indeed common.,hardware,2026-01-15 00:12:34,3
Intel,nzdueea,Thanks Steve,hardware,2026-01-13 17:26:35,32
Intel,nzdyjcv,But you still didn't to say hi to me at PAX West 2016 in front of the LEGO USS Missouri battleship...,hardware,2026-01-13 17:45:51,2
Intel,nzdtnpi,"makes sense, I coulda been more charitable in the way I said it",hardware,2026-01-13 17:23:03,-1
Intel,nze79m5,"relax dude, steve already replied, no need to whiteknight",hardware,2026-01-13 18:24:32,-13
Intel,nzdtsrk,"he replied in a comment to me to say that he was told to leave it alone, I guess assembly isn't as careful as the initial production is.",hardware,2026-01-13 17:23:45,7
Intel,nzdjfsy,"Given the impeccable, spotless, damn near saint-like moral & ethical code of Steve & GN, and their recent consumer advocacy and stepping on some very powerful toes, your comment sounds an ***awful*** lot like an astroturfing smear campaign meant to breed sentiments against Steve & GN.",hardware,2026-01-13 16:24:37,11
Intel,nze6q5q,Imagine defending the hair of a guy who looks like he judges anime conventions in his spare time.,hardware,2026-01-13 18:22:13,-12
Intel,nzdq8iu,"I get it, for those with old boxes. But intel has great transcoding according to self hosters, and the powr consumption is much better vs old i5 pairing with those dedicated cards.",hardware,2026-01-13 17:05:17,10
Intel,nzej077,day traders love having a zillion stock tickers running.  i'm sure there's more applications where a heap of monitors is useful.,hardware,2026-01-13 19:16:47,8
Intel,nzeua57,A lot of digital displays make use of DP MST to avoid the use of home run cabling.,hardware,2026-01-13 20:08:25,3
Intel,nzelrbi,The a310 and a380 are fantastical for a media server!,hardware,2026-01-13 19:29:14,13
Intel,nzdi8nv,Your wasting a lot of words defending a company about to rerelease a 4 year old GPU (3060) because they canâ€™t get memory for the current model.,hardware,2026-01-13 16:19:11,16
Intel,nzsvw7m,The A310 was released in 2022. Do your think Sparkle would still be producing them in 2025 if they didn't sell?,hardware,2026-01-15 21:18:53,1
Intel,nzdjgo9,"Sure it was paraphrased for the title, but that's just semantics.   AMD and Nvidia ""giving up"" vs. ""abandoning"" the segment mean the same thing either way, given Lucas' intention behind the statement.",hardware,2026-01-13 16:24:44,7
Intel,nzfgwnz,He is ignoring the post because the poster didn't watch the video and is spreading BS. The segment they are talking about is $100 cards.,hardware,2026-01-13 21:53:45,14
Intel,nzdqmql,"maybe the cost for the raw wafer, but that's not all TSMC will charge nvidia for. You also need to account for yields, which could be different depending on the type of node.",hardware,2026-01-13 17:07:41,0
Intel,nzlh47j,"Lately there's been a lot of bad shit happening in the industry and a lot less good tech to talk about. Shocking, right?",hardware,2026-01-14 19:40:42,5
Intel,nzp1ajx,Thanks you ðŸ˜Š,hardware,2026-01-15 08:17:06,1
Intel,nzdxfeg,"> Given the impeccable, spotless, damn near saint-like moral & ethical code of Steve & GN  You have to be joking, right?",hardware,2026-01-13 17:40:46,8
Intel,nzdlwqs,Yeah. Iâ€™m definitely an astroturfing bot account. You got me. My profile certainly *reeks* of botting / astroturfing ðŸ˜‚,hardware,2026-01-13 16:35:48,-14
Intel,nzi46nl,This has to be sarcasm.,hardware,2026-01-14 07:40:11,-2
Intel,nzdii5s,"You know I also talked about AMD right? Not just NVIDIA. Regardless, you think Intel isn't also going to have memory issues soon? They might just divert all memory they have to the SKUs that are selling.",hardware,2026-01-13 16:20:22,5
Intel,nzedwr3,>  a literal quote from Lucas  Does not line up with  > paraphrased,hardware,2026-01-13 18:53:49,12
Intel,nzdz5pq,Yields wouldn't meaningfully differ within the same family. Certainly not by enough to remotely cover for the die size difference.,hardware,2026-01-13 17:48:42,7
Intel,nzfzkw5,I'm starting to think that r/hardware is the circlejerk sub and I just haven't yet found the actual hardware sub that it's parodying,hardware,2026-01-13 23:27:14,2
Intel,nzegtti,Intel B570/580 already use GDDR6 which is what Nvidia is trying to achieve with the 3060 release. Intel presumably won't be effected.,hardware,2026-01-13 19:06:58,2
Intel,nzel47j,"...Yes, that's why I said it's a semantics issue.  The meaning is the same: The paraphrased quote isn't a statement made by GN like u/KARMAAACS implies.   What makes it worse is that Lucas said that in response to Steve's question about ~$100 A310 cards and why they're still producing them (adding that they see a healthy demand for them from their customers).   It's like he didn't watch the video and just reacted to the title.",hardware,2026-01-13 19:26:20,-11
Intel,nyracr1,"Solid product, nice foundation. Improve ST and intel will comfortably keep their mobile market",hardware,2026-01-10 08:56:00,115
Intel,nyr9ktg,"I think the ideal would be to get to a point where the flagship ""mainstream"" iGPUs (-H series, for Intel) compete with Nvidia's contemporary x50 GPUs, and then have big iGPU chips (Strix Halo, NVL-AX?) to compete with x60+ level.",hardware,2026-01-10 08:48:47,41
Intel,nyrhzxa,"Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.Â    Intel couldn't care less about that, they just need to be better than 890M and the game is done.",hardware,2026-01-10 10:07:58,93
Intel,nysrktd,Iâ€™d love to see its support outside of the approved games demo list. Intel has great hardware but their drivers and game support have always been the biggest question.  Whatâ€™s the point of hardware if you canâ€™t apply it to what you need.,hardware,2026-01-10 15:26:18,13
Intel,nyrvona,"If these chips end up cheap enough that they can replace the standard Intel CPU + 50/60 tier mobile Nvidia dGPU it will be very interesting.  I'm not sure they will be able to in the short term, Nvidia pricing on low end mobile dGPUs is very aggressive ($600 5050 laptops are the proof) but hopefully it isn't long before this type of powerful iGPU becomes a common thing.",hardware,2026-01-10 12:09:03,19
Intel,nyrjnuw,"This is against a 50W TBP RTX 4050 Laptop (which should be more at ease around 90-100W)  Not saying it's bad, but you can't compare Laptop performances without including TDP configuration and behavior.",hardware,2026-01-10 10:23:13,25
Intel,nyrfm1b,"""taking on strix halo"" -> result 50% of strix halo performance, ok.",hardware,2026-01-10 09:45:51,31
Intel,nyuib68,"TWELVE efficiency cores?.... that's nuts.   Anywho, these results look good. Assuming CPU and battery life are comparable or better than Strix Point/Gorgon Point, Intel might have a nice little advantage.",hardware,2026-01-10 20:27:23,6
Intel,nys66a2,Wtf is this article? Strix halo is another class product. Takes on strix halo being more than 50% slower?,hardware,2026-01-10 13:24:55,13
Intel,nyszn5m,"I'm sorry but nothing was more embarrassing than that guy from AMD the other day saying it doesn't matter because Strix Halo, a chip in so few devices that's an absolute behemoth, is still faster. Panther Lake is an absolute achievement for Intel. With the right drivers, they're going to have the perfect chip to forgo low end dGPUs.",hardware,2026-01-10 16:06:23,10
Intel,nyrf0ck,I will need at die fast ultrabook with 12hrs+ battery  Its Not a gaming product,hardware,2026-01-10 09:40:16,5
Intel,nyrhl2f,Website doesnâ€™t load with adblocker,hardware,2026-01-10 10:04:14,4
Intel,nyrng08,"I'm hoping for a thin and light 16 inch laptop with Panther lake and a B390, as it'll be perfect for photo editing, as Adobe seems to prefer Intel over AMD graphics and a discrete GPU is overkill.",hardware,2026-01-10 10:57:35,3
Intel,nyr7n57,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-10 08:30:43,1
Intel,nyvbh6q,I am more interested in next gen desktop APUs,hardware,2026-01-10 22:53:09,1
Intel,nz5usc0,"Compared to my 890M based laptop, the 890M numbers here are about 15-20% lower than what I'm seeing at the same settings.  This is likely due to power targets?  Even if the uplift is just 60% instead of 80%, that's still an impressive achievement for the B390M.  It's a shame that AMD appears to have dropped the iGPU ball in 2026.  Relying on the Strix Halo is not an option here.  It's pretty much impossible to find a good laptops that use it.  The upcoming HX470, and still without FSR4, isn't going to close the apparent large gap.  It seems that AMD forgot to stay hungry and they'll end up losing whatever ground that they'd gained in the last few years.  Mind you, Intel is known to play pressure games with laptop makers too in order to limit AMD adoption, which just makes it even crazier that AMD isn't doing what's required to keep the pressure on.",hardware,2026-01-12 13:57:45,1
Intel,nz6400s,"I think the B390 could be faster than even an RTX 5050 35W (As it could beat an RTX 4050 at 60W).       These thin and light laptops that Panther Lake is built for use way underpowered GPUs. Honestly, it makes sense why the Dell XPS 14 only has the B390 graphics. Before it used an RTX 4050, but it ran at just 30W of power.       Now that Integrated Graphics have beat the -50 Tier of GPUs I don't think we'll even see an RTX 6050 or RX 9050",hardware,2026-01-12 14:47:08,1
Intel,nzdzgjh,"They've basically maxed out the 128-bit normal socket iGPU now.  For them to beat it they need more memory bandwidth - they can put in a bit more cache, but realistically they'll need a quad channel bus (or maybe they can wait for LPDDR6 at 14.4+).  They can probably have a bit more physical room in the next generation of sockets, but without more bandwidth it isn't *that* useful.",hardware,2026-01-13 17:50:05,1
Intel,o3eyh45,"Way too expensive. For the price of a B390 notebook, you can get a Strix Halo notebook with 40 CUs, which is significantly faster. Sadly Intel has messed up again. and in addition with 16gb shared RAM it's pretty useless imho (8GB+8GB isn't enough these days).",hardware,2026-02-03 21:25:18,1
Intel,o3eyovg,"Yeah Strix Halo is way better in Terms of bang for the buck, sadly",hardware,2026-02-03 21:26:19,1
Intel,nyveqz5,"the 140v also got a 25% speed boost post launch, if something similar happens than this could be as good as a 5060 mobile... which is wild! I hope it dosen't cost as much as halo strix!",hardware,2026-01-10 23:10:01,1
Intel,nz7yfdg,Why aren't they comparing the AMD 8060S in the current Strix Halo flagship to the Intel B390? Probably because it doesn't go intel's way... interesting.,hardware,2026-01-12 19:54:01,0
Intel,nystund,And yet maybe 5% of customers will buy this version because its absolutely irrelevant for them whether their laptop would have an Iris iGPU from 2014 or a 2500watt RTX 5090.,hardware,2026-01-10 15:37:56,-7
Intel,nyrt6fi,"Thats great. If you are nvidia making dedicated gpu, then better make something that is not shit. 4050 is a joke",hardware,2026-01-10 11:48:14,-10
Intel,nyrumb5,"But how much does it cost? It mentions it having 16 cores so I'm guessing it's going to be overpriced if you don't need CPU performance, just like Strix Halo.",hardware,2026-01-10 12:00:16,32
Intel,nyrnltb,They need a 25% IPC increase to get back to the leading edge in CPU and honestly i don't see it with their current architecture. They need a new radical design   Edit: getting downvoted for what?. Currently Apple and QC have a very solid lead. Even ARM beats Intel and AMD in general CPU workloads and Intel/AMD have been very slow to update their uarch focusing on clock speed over efficiency and IPC,hardware,2026-01-10 10:59:01,1
Intel,nyyecmo,"AMD Ryzen AI Max+ 388 just dropped cheaper than the 395 with the same GPU, it will be cheaper than the panther lake.",hardware,2026-01-11 11:18:06,0
Intel,nyrbnk5,"Depends on Intel's & amd power targets. I dont think its rly feesible for them to target cpu + gpu power usage, 100W combined at least?",hardware,2026-01-10 09:08:17,16
Intel,nyxp50b,then we wouldnt have the 50 gpus anymore. The XX30 and XX40 GPUs died because of iGPUs competing with them.,hardware,2026-01-11 07:25:59,1
Intel,nyt45q3,> Too expensive for any meaningful customer to adopt and have real mainstream products.   So basically every decent APU ever made. Too expensive to the point it bumps into dGPU territory and not powerful enough to be a direct replacement.,hardware,2026-01-10 16:27:51,30
Intel,nyrnxmm,> Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.Â  >  >   Story of AMD APUs.,hardware,2026-01-10 11:01:59,41
Intel,nys70pu,"AMD aimed Strix Halo at AI users first and foremost, thinking those folks would pay the high premiums.   But of course anybody serious about AI would have an Nvidia GPU, and so many other AI users are still just using cloud-based services anyways.",hardware,2026-01-10 13:30:14,26
Intel,nytpb98,"AMD has always had lower supply compared to Intel and yet AMD client continues to grow. Strix Point at launch had little products (Asus being the only OEM per usual) and yet they still continue to grow, at a smaller scale relative to Intel. Strix Halo is still continuing to have designs made, it wouldn't be a 'failure' if we are still getting Strix Halo products at CES...  I wrote a [comment in a previous post](https://www.reddit.com/r/hardware/comments/1q7d67m/comment/nyhh23c/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) on the reality of the state of Intel and AMD in the mobile segment. Intel is really dependent on CCG, it is double in revenue to DCAI. They invest in what makes them money. Compared to AMD, client and DCAI is doing well, for client CPUs and GPUs are doing well, putting no pressure in mobile, in fact their strategy has remained the same in the past couple the years and even if marginally their % share is sufficient for them. Intel pushes a lot of supply for mobile, while AMD is smaller, it is all relative in the necessary investments they need to make in order to supply demand. Do I wish AMD stop stagnating in designs, yeah, RDNA3 needs to go, but these companies have motives in what they do.",hardware,2026-01-10 18:08:04,10
Intel,nysq66f,"We all saw it coming a mile away, when it came out in 2025 it was competing against discounted 4060 laptops as low as just $1000. Too little, too late, too expensive, dated on arrival with RDNA3.5, etc. But for some reason this sub and r / amd always have such a hard on at the concept of a ""big APU"" that in practice would never be economically sensible.",hardware,2026-01-10 15:18:57,15
Intel,nyrnx0m,Arc 140T is already on par with the 890m for most tasks excluding games.,hardware,2026-01-10 11:01:50,13
Intel,nyrimhl,Commercial failure indeed. Laptop with dGPUs at same price perform better. Laptops with solid CPU perf are much cheaper.,hardware,2026-01-10 10:13:42,7
Intel,nz23thg,AMD gave Strix Halo zero chance to compete by barely selling any of the lower end models. An 8 core with 32 CUs would be a great mini PC.,hardware,2026-01-11 22:50:43,1
Intel,nyt5d7h,"I dunno Battlemage was a big step forward on driver compatibility and every month Intel improves all their Arc compatibility. I'd be honestly surprised if Xe3 was worse than their current offerings. I'm sure it still has shortcomings as all Intel GPUs will because they're simply starting fresh, but even my A750 is pretty good right now at playing anything I throw at it.  The only aspect Intel kind of messed up that annoys me is their video encoder, once it gets pegged to 100%, it absolutely tanks your performance on the capture to the point where it skips frames and lags. It never used to do that and the driver also used to include capture software, now they just offload it to people having to download OBS and removed the capture aspect of the driver. Kind of dumb when both NVIDIA and AMD include it as a driver option.",hardware,2026-01-10 16:33:28,14
Intel,nz0bqe4,Driver support is better than ever and will continue to get better now that Intel has found its footing in the gaming GPU business (yes that includes igpus),hardware,2026-01-11 17:57:04,3
Intel,nysl2hn,"If TSMC does raise price on their node, Nvidia doesn't find another node for their lower-end bins and Intel can keep the price on their own node down low, we could see Nvidia simply slowly phasing out the -50 series like they used to do with the MX series.",hardware,2026-01-10 14:51:29,15
Intel,nyrlr4x,"The 4050 still has a sizeable memory bandwidth advantage, so it's still very surprising that the B390 comes so close.",hardware,2026-01-10 10:42:21,17
Intel,nyvfe2o,"oh damn, this should be a lot higher up! Most laptops have them clocked much higher so expecting 4050mobile performance is kinof a lie...",hardware,2026-01-10 23:13:25,3
Intel,nyribi5,"I get the feeling everybody is still unsure where these PTL chips slot in to and what to compare these against actually. Once we get more info on pricing, power consumption, CPU performance etc. we will get some actually useful comparisons.",hardware,2026-01-10 10:10:57,25
Intel,nys7fi0,I actually think they meant to say Strix Point in the headline there.,hardware,2026-01-10 13:32:44,12
Intel,nyrhzye,"[HP ZBook Ultra G1a 14](https://www.notebookcheck.net/HP-ZBook-Ultra-G1a-14-review-Powerful-MacBook-Pro-alternative-for-work-and-game.994758.0.html) would've been a better test  Load average: 83.3W   Cyberpunk 2077 ultra \* 110.9W = 80.7fps  Baldur's gate 3: 99.4fps   B390 wattage?   If pantherlake is designed for battery, is it better if it loses performance?",hardware,2026-01-10 10:07:59,8
Intel,nyxpaeu,So how many sub 1000 dollar laptops we have with Strix Halo?,hardware,2026-01-11 07:27:18,2
Intel,nzd2sww,It's definitely going to come down to pricing and availability,hardware,2026-01-13 15:06:48,1
Intel,nysgi6l,The new MSI Prestige 16 looks nice.  They all seem to lack Thunderbolt 5 though.,hardware,2026-01-10 14:25:49,7
Intel,nyt7vgu,"It seems the revived Dell XPS 16 will have the â€œB390â€ and no dGPU, as another option. LTTâ€™s video on it said Dell is quoting 27 hours of battery life in â€œgeneral tasksâ€ and 40 hours of video playback. Obviously remains to be seen how real those manufacturers claims are, but hereâ€™s hoping.",hardware,2026-01-10 16:45:16,4
Intel,o2dssd4,"WÃ¼rde das Thinkpad X9 15p bevorzugen, hat einen SD-Karten Slot, super Lautsprecher und einen richtig groÃŸen Akku.",hardware,2026-01-29 09:36:04,1
Intel,o5o4uqp,"Agree on performance, still always can improve perf/watt though.",hardware,2026-02-16 12:07:12,1
Intel,o5o4yhm,"Unlike the Strix Halo, Panther Lake appears to be an actual mass-volume product that Intel intends to produce and sell to more than just a few enthusiasts.",hardware,2026-02-16 12:08:00,1
Intel,nyrvj09,"$1100 for a little MSI 13"" laptop with one. there are also quite a few CPU SKUs that have the B390.",hardware,2026-01-10 12:07:46,41
Intel,nyt4dcl,"Those 16 cores are 4 performance cores, 8 efficiency cores and 4 â€œLow Powerâ€ efficiency cores. This is only doubling the core count of Lunar Lake, by adding the two plain efficiency core clusters. Or keeping the same core count as ArrowLake mobileâ€™s 285H (not HX!), trading 2 performance cores for 2 â€œLow Powerâ€ efficiency cores.  Iâ€™m not 100% on this but I donâ€™t think Stryx Halo used AMDâ€™s C cores, so it basically had an entire 9950x attached to the iGPU.  Prices should be more normal, as this is more part of Intelâ€™s normal lineup.",hardware,2026-01-10 16:28:50,16
Intel,nyych3q,There's an Ultra 5 chip with the B370 (10 Xe cores instead of the 12). Shouldn't be too costly,hardware,2026-01-11 11:00:58,2
Intel,nyvbkeg,It's 16 cores but it's only comparable to Strix Point 12 cores and not Strix Halo.      The highest end Intel chip here only matches the number of P cores in the M5,hardware,2026-01-10 22:53:37,2
Intel,nyrpy0o,"I really don't think that is so important for mobile devices though.  All Intel needs to do is be ""good enough"" and the OEMs will use them in flagship models.",hardware,2026-01-10 11:20:06,41
Intel,nyrzdl1,I doubt anybody is going unseat Apple from the ST throne in the near future.,hardware,2026-01-10 12:37:56,12
Intel,nyrolyj,Well unified core is supposed to be happening in the next couple of gens. Frequencies also seem to have taken a hit on 18A but I'd expect that to improve with time as usual,hardware,2026-01-10 11:08:05,11
Intel,nytxbwv,Not really. They just need to not completely bungle gaming and latency sensitive performance like with Arrow.,hardware,2026-01-10 18:45:01,2
Intel,nyylyv1,"This is about mobile devices, and since a high performing IGPU is included, the question is no longer how well the CPU performs in a system with a 5090 (what most cpu benchmarks focus on) but how well this IGPU/CPU combination performs compared to other IGPU/CPU combinations. I am positive the CPU is not the limiting factor in this IGPU performance tier, so ""leading edge CPU performance"" is not really relevant.",hardware,2026-01-11 12:23:19,1
Intel,nyrc6je,"Don't think it's completely absurd. Should get some efficiencies from less interconnect overhead and lower power memory, so not quite 1:1 with a dGPU. If we were to budget, say, 40W for the iGPU in gaming and 20W for the rest, should be perfectly in line with the higher end laptop SKUs.",hardware,2026-01-10 09:13:21,18
Intel,nyrnpsr,Intel Arrow Lake already uses 80W just on the CPU side in multicore,hardware,2026-01-10 11:00:02,0
Intel,nytxoe2,"It would be viable if AMD released their own small PCs with it to cut the MSRP of products, but they aren't interested.",hardware,2026-01-10 18:46:36,7
Intel,nyummcb,And yet AMD managed it for the PS5... it's clearly possible.  Of course we don't know the cost breakdown there as far as PS5 pricing goes.,hardware,2026-01-10 20:49:14,5
Intel,nyrpk3i,They just need to make the next iteration cost less. Most of strix halo's issues were the sky high price.,hardware,2026-01-10 11:16:37,7
Intel,nyspmud,"""Local LLM"" is such an incredibly niche thing I can't believe the tech nerd internet is so obsessed over it. Any real life business use case of AI is cloud based no question asked.",hardware,2026-01-10 15:16:05,17
Intel,nywuwh7,well said,hardware,2026-01-11 03:51:39,0
Intel,nytll12,Too much listening to MLiD who has a boner for APUs,hardware,2026-01-10 17:50:40,12
Intel,nytno6k,"Idk one can easily flip your statement. Panther lake coming in **2026** competing against continuing discounted 4050s prob less than 4060s. I don't dislike Panther Lake nor am I defending Strix Halo, but I wouldn't say your argument is a rather good one.",hardware,2026-01-10 18:00:26,-3
Intel,nyrtebm,https://m.youtube.com/watch?v=ymoiWv9BF7Q   It's already at least on par for reasonable power profiles unless you play stuck to the wall.,hardware,2026-01-10 11:50:08,6
Intel,nytp8ri,"Huge step forward, I just wish the didnâ€™t struggle with older and brand new games. Itâ€™s a great card if you are willing to do troubleshooting and know computers but I wonâ€™t recommend them to family yet.",hardware,2026-01-10 18:07:45,6
Intel,nz0sex5,I hope they start supporting dx11 stuff. Thatâ€™s a ton of games.,hardware,2026-01-11 19:10:35,1
Intel,nytykfy,Isn't TSMC planning to increase pricing on n2 by 20-30%,hardware,2026-01-10 18:50:42,6
Intel,nz0bimn,Nvidia will find another cheap node to use. Samsung will gladly oblige,hardware,2026-01-11 17:56:04,1
Intel,nyrsbne,"153 GB/s vs 192 GB/s is not that ""sizeable""  And the comparison against ""HP OmniStudio X 32-c0077ng"" is weird, even in the linked test they have GPU-Z screenshot displaying 1375Mhz memory speed instead of 2000 Mhz on most other RTX 4050 Laptop Review.  I don't understand this comparison against an All-in-One, and I'll wait for more in depth reviews to draw some conclusion.",hardware,2026-01-10 11:40:59,13
Intel,nyrm798,"PC World had power consumption tests under gaming loads. It pulled 60W through USBC with Cyberpunk, so probably 35-40W for the gpu. When they unplugged it, the benchmark numbers stayed the same. So it also pulls 60W on battery.  Unless the manufacturer actually configured the device to simultaneously pull energy from the cord and battery under full load.",hardware,2026-01-10 10:46:26,11
Intel,nyxt6pz,About as much as we have PTL laptops,hardware,2026-01-11 08:02:38,2
Intel,nysiaor,"TB5 isn't a big deal, although I don't like that they have a numpad keyboard, and usually MSI speakers are terrible.",hardware,2026-01-10 14:35:56,3
Intel,nyrxjeo,But can't you get a laptop with a 5060-5070 at that price?,hardware,2026-01-10 12:23:52,17
Intel,nyvevcg,"Damn, that's really good! it's pretty much macbook air pricing.",hardware,2026-01-10 23:10:39,1
Intel,nyvvmm4,"> Iâ€™m not 100% on this but I donâ€™t think Stryx Halo used AMDâ€™s C cores, so it basically had an entire 9950x attached to the iGPU.  I have a Strix Halo.  What you wrote is exactly what it is.  It's essentially a 9950x (so all P-cores) with a fat iGPU attached, and with a 4 channel memory controller instead of 2-channel.",hardware,2026-01-11 00:39:01,16
Intel,nyxcvyp,"The biggest difference between the P and E cores is fMax. The larger the core count becomes, the lower the all core clocks become, the smaller the gap between P and E core performance becomes.   The IPC difference between the two is like ~10%  At a certain point along the wattage curve, given a certain number of cores, there will be a point where E core performance can potentially meet or exceed what you would've gotten has you had too many P cores.    Its also more than just trading 2 P cores for 2 lpE cores. The lpE cores in ARL-H were *so* weak, they were functionally useless. In practice, it'll be more like trading 2 P cores for 4 lpE cores  edit: to be more specific, In ARL-H, below 5W per core, E cores outperform P cores. If you have 16 cores and are running all core workloads, then at 60W, each core is receiving less than 4W.",hardware,2026-01-11 05:45:32,5
Intel,nz7k3ly,No it's firmly ahead of strix its right in between. Strix point uses 8 ecores too and it gets demolished in multithread benchmarks as expected,hardware,2026-01-12 18:48:32,1
Intel,nyvb2cd,"It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.     AMD made them lower margins for laptop chips because they weren't very competitive. If they want fat margins, they need to be the best",hardware,2026-01-10 22:51:00,2
Intel,nyuqay0,Single Core is very important when Intel is doing these designs that lack P cores throughout. The cheapest X2 Elite has the same amount of P cores as the most expensive Panther Lake SKU,hardware,2026-01-10 21:07:30,0
Intel,nysbrxo,"Qualcomm is already super close with Oryon V3...  Perf/Watt for that single thread isn't close I guess, but absolute performance is breathing down Apple's neck for sure.  Also, don't compare Geekbench scores on windows vs Linux/Apple/Android... Windows just does something negatively about it and the difference is 5-7% vs non-windows.",hardware,2026-01-10 13:58:36,8
Intel,nyvsos5,>Well unified core is supposed to be happening in the next couple of gens.  I would be shocked if this has much to do with a large performance uplift. I imagine it would have to do more with rightsizing core area and power draw.,hardware,2026-01-11 00:24:01,2
Intel,nyuqf9s,Chasing above 5Ghz is stupid on laptops. It only matters for desktops,hardware,2026-01-10 21:08:07,2
Intel,nyu42wk,You are overly focusing on gaming. I mean general CPU performance,hardware,2026-01-10 19:16:57,1
Intel,nyrcv9u,Rtx 5050 is 61% faster than B390. I doubt if they change the wattage configuration and stick to 60W they'll match it. Unless the 5050 is capped to more reasonable wattages like 60-80W. Plus the 60W budget for Intel/amd will be used for other compotents and the apu budget reduces.,hardware,2026-01-10 09:19:52,8
Intel,nysfyyg,"I'm talking out of my knowledge base, but I think the switch from heat pipes to custom vapor chambers means we are less bottlenecked at power density / pulling heat from the chip and more constrained at what the radiator/fan system can push out of the system.",hardware,2026-01-10 14:22:47,2
Intel,nz3zzs3,"They announced a first party Strix Halo PC at CES, but it'll probably be really expensive.",hardware,2026-01-12 04:57:58,3
Intel,nyzybbo,"> And yet AMD managed it for the PS5... it's clearly possible.  Well for two reasons:  1. Sony bankrolls the R&D of the APU and it's underlying architectures which allows AMD to make it for basically cost and have a low BOM on it. They didn't pay as much as they normally would for the R&D, tapeout, testing etc.  2. It's a console APU, it literally has to be cost effective to make sense, otherwise it becomes like Strix Halo and SONY goes out of business. Also most consoles are sold on launch for a small loss with SONY and Microsoft recouping those lost funds off game sales, online subscriptions and store revenue. Then over time they tend to shrink console APUs on newer nodes which makes it more power efficient and less expensive to produce as a smaller chip on a newer node typically has better yields, it also allows SONY or Microsoft to put in lower quality components like less heatpipes in a new revision or Slim console, for similar thermal headroom and save on BOM cost.   I mean there's a reason why they do not offer the PS5 APU as an off the shelf product, only the cutdown bad yields go onto being some cryptocurrency mining board or some Linux APU and with the performance being cut its usually worse value than buying off the shelf dGPU parts like a 5060 or something.  I don't know why you're seriously arguing that APUs for Desktop and Laptop PCs are a viable product. For one, they've never been viable, not once. Even Strix Halo which is honestly the best APU I've ever seen has been ruined by its high cost. Don't get me wrong, I like the idea of an APU, an all in one chip that does it all. But unless you're like Intel and you're willing to do a tile based design and or basically have a true chiplet where you can link lots of smaller dGPU tiles together it doesn't really work. You're just better off buying dedicated CPU and GPU parts for better price to performance. If you don't believe me, I can buy an [RTX 5070 Laptop right now for $1900 AUD](https://www.centrecom.com.au/msi-katana-15-hx-14xwgk-156-qhd-i7-16gb-ram-512gb-rtx-5070-gaming-laptop-black) and that will easily outperform Strix Halo which has less performance and typically costs over $4000 AUD... [Even a lowly 4060 laptop fairs better.](https://youtu.be/RycbWuyQHLY)  The only thing APUs excel in is this, if you want something relatively cheap but capable. i.e it can run a game at 30 FPS with medium settings at a low resolution. i.e something like Panther Lake or Apple's M series chips. But if you want true performance, just go out and buy an RTX X060 series laptop it's far better price to perf each generation.",hardware,2026-01-11 16:53:47,6
Intel,nyvw6mt,"What is possible? PS5 uses GDDR6 instead of DRAM. And consoles are heavily subsidized by digital purchases. I bet AMD makes good money on PS5 (and Xbox X/S), Sony & Microsoft just subsidize the shit out of it with their 30% cut from selling games. Even the Steam Deck is barely profitable for Valve. High-end APU is just a waste of sillicon.",hardware,2026-01-11 00:41:51,12
Intel,nyrrj9f,Even their Ryzen 5 AI 340 laptop are too expensive and you can buy an older gen Ryzen 5 with Nvidia GPU laptop for same price or even lower with much better GPU performance.,hardware,2026-01-10 11:34:10,30
Intel,nyxoopw,Local AI (not just LLM) is universal on mobile and getting to be universal in corporate computers. You just dont see it. The background blurring in Teams meeting? 5x more battery efficient with AI. But its just going to be integrated into Teams and fire up if hardware supported without asking you.  >Any real life business use case of AI is cloud based no question asked.  All AI use cases at the place i work for is local due to confidentiality issues. We cannot and will never be able to use this on cloud. Unless the world completely flips its ideas about confidentiality i guess.,hardware,2026-01-11 07:22:00,4
Intel,nyu206m,"Panther Lake is a normal CPU, not some special ""big APU."" It doesn't make much sense to flip the argument the way you did.",hardware,2026-01-10 19:06:56,10
Intel,nysp4h4,"140T (Arrow Lake) isn't the same as 140v (Lunar Lake) though, the former is usually quite a bit weaker and inconsistent in games despite slaying all the synthetics.",hardware,2026-01-10 15:13:21,11
Intel,nysjpec,"It is probably because the AIO was one of their most, if not the most recent RTX 4050 tested (March 17th 2025) which probably enabled them to compare in newer title like F1 25 in the article, as it has already been around for like 3 years while RTX 5050 was released last year and received more attention in its place overall. From their database, the next most recent thing with RTX 4050 they reviewed was the Yoga Pro 7 in January 2025 with a 60W RTX 4050 (45 watts + 15 watts Dynamic Boost), which scored 50.8fps in Cyberpunk 2077 at the same setting and thus a bit lower than the AIO, so I would say the AIO is at an okay spot for a RTX 4050 to be compared to this Arc B390.",hardware,2026-01-10 14:43:54,6
Intel,nysoib9,"> 153 GB/s vs 192 GB/s is not that ""sizeable""  25%? What's sizeable?",hardware,2026-01-10 15:10:05,22
Intel,nyrwfu6,"It would be easier to compare mobile parts if laptop OEMs didn't lock down their BIOS and EC registers, blocking anyone from actually tinkering with the (godawful) default configs for TDP, boost behaviours and fan curves on most common laptops  You can buy the bestest Intel Core Ultra 9 285h but if some engineer at HP thinks that 45Â°C idle is too warm it will either throttle to the point that you wish you were using the Nintendo DS browser or crank the fans to Mach 3...",hardware,2026-01-10 12:15:09,6
Intel,nyxwid7,had no idea Strix Halo is this popular.,hardware,2026-01-11 08:33:14,2
Intel,nysx853,"the new Prestige 16 actually [doesnt use a numpad](https://www.notebookcheck.net/MSI-debuts-Prestige-16-AI-and-Prestige-16-Flip-AI-with-Panther-Lake-H-Core-Ultra-X9-388H-and-Arc-B390-graphics.1197009.0.html)!  and the flip version is especially intresting, they managed to tuck the stylus *under* the laptop with a slot that can also charge said stylus",hardware,2026-01-10 15:54:42,6
Intel,nys9rsz,"Laptops with 5060 at sub- $1000 weren't launch event laptops at CES. They came later as fairly cost optimized, ""compromised"" laptops that cheaped out on most of the total laptop in order to fit that CPU/GPU in its budget.   PTL-X is PTL-H with a ~60mm GPU tile. A 4050 is a binned ~160mm chip. Edit: that *also* requires its own VRAM and cooling  Intel is also on record saying 18A cost structure is flat vs Intel 7. I imagine costs between PTL-X and RPL-H + dGPU are much more competitive than you think, with the only caveat being discounts on old excess inventory and not having to redesign a new laptop (although I imagine the RAM pricing increases makes the total price different between the two shrink even more)",hardware,2026-01-10 13:46:47,35
Intel,nyrxu16,"5060 yes, but it's less power efficient",hardware,2026-01-10 12:26:08,22
Intel,nywhxd0,"5060 -5070 cannot be fitted into ultrabook or thin & light models. those item are power hungry and high temperature, need to fit it in bulky laptops which are bigger heatsink , more room space.",hardware,2026-01-11 02:38:34,3
Intel,nyt5pt1,"technically, but it will be a shitbox in basically every other aspect (and stuck with 16/512)",hardware,2026-01-10 16:35:08,13
Intel,o5o48gu,There's a lot of advantages (especially battery life) when you don't have to deal with the inefficiencies and overhead of a separate GPU.,hardware,2026-02-16 12:02:22,1
Intel,nyrycdm,i've been looking rn but have only seen those at $1400+,hardware,2026-01-10 12:30:01,-3
Intel,nyvtubh,"> It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.  Last year's leadership QC laptops had to be heavily discounted shortly after reaching market. Clearly there's more to it than IPC.",hardware,2026-01-11 00:30:06,4
Intel,nyxgar9,"The X2 Elite *may* be an amazing CPU. But customers don't buy mobile CPUs. They buy full, complete laptops, and that includes all of WoA's issues. Customers have so far, by and large, mainly rejected WoA. The biggest demographic of people who research and care about strong CPU performance are people who'd also want to play games, and QC has yet to demonstrate that that's viable.",hardware,2026-01-11 06:11:34,12
Intel,nyxo27r,But this product has 4 P cores?,hardware,2026-01-11 07:16:32,2
Intel,nysorh4,Do we have 285H/HX370 scores on Linux for comparison?,hardware,2026-01-10 15:11:26,5
Intel,nyu3bo0,I was going to wait for this - but driver support comments basically said wait for it to mature.,hardware,2026-01-10 19:13:18,2
Intel,nyw054m,"it is presumably lead by the e core team that's doing a lot better so we'll see, but at the very least saving area from debloating p cores would allow a bit more cache that the cores would love.",hardware,2026-01-11 01:02:05,3
Intel,nyurnsr,Chasing 5GHz is only stupid if it costs more power than it'd save. The lower end panther lake SKUs clock their cores a lot lower compared to LNL so it's likely just a node thing,hardware,2026-01-10 21:14:19,2
Intel,nz5bvpr,Apple and Qualcomm are both doing that right now though. It's cheaper than blowing up the area of the core to increase IPC.,hardware,2026-01-12 11:55:36,1
Intel,nyvv21f,But for non -gaming tasks arrow beats zen5,hardware,2026-01-11 00:36:07,8
Intel,nyxogf6,Gaming is the only segment where your previuos comment made sense though.  Everything else Intel is still leading edge.,hardware,2026-01-11 07:19:59,1
Intel,nyrdymf,"Yeah, I'm not talking about PTL. Clearly it's too far off. But clearly there's a lot of room left for Intel (and current AMD APUs) to catch up. Also worth noting that that 5050 is given 100W, which is particularly high for that chip. Gap obviously closes when the TDP is more reasonable.",hardware,2026-01-10 09:30:19,11
Intel,nyvf643,"it may not be this generation, but at the rate iGPU performance growing; pretty soon xx50 chips is no longer relevant. \*its not like Nvidia can make fat profit anyway.   Fyi, Nvidia has abandon their low profit margin xx30 line up, or Geforce MX series in laptop.",hardware,2026-01-10 23:12:14,3
Intel,nz57ion,Nah. the price of Strix Halo is the cost of the PS5 itself. AMD has fat margins for laptops and desktops,hardware,2026-01-12 11:20:38,2
Intel,nys1wlu,Laptops with dgpu always has poor battery life. Even tinkering with the best power optimizations. These ryzens have nearly double the real world battery life from my experience.,hardware,2026-01-10 12:56:23,7
Intel,nywmg0a,"Depends on what you consider to be â€œhigh end mobile gamingâ€, the laptop 4060 is currently the 2nd most popular gpu on steam, and thatâ€™s the level strix halo targeted",hardware,2026-01-11 03:03:40,1
Intel,nyxpn7n,"What youâ€™re describing is just inference. Runs on a phone Soc. Minimal memory requirement. Like faceID on the original iPhone X over eight years ago. Strix halo provides no additional benefit over strix point or lunar lake. If there are business use cases that use outlook or Microsoft 365 or Teams, they are using cloud based copilot. Thatâ€™s the mainstream business use case at present.",hardware,2026-01-11 07:30:29,1
Intel,nyu45c4,"The statement is directly comparable. 'Big APU' Strix Halo can literally be fit into a [handheld ](https://gpdstore.net/product/gpd-win-5/)and a [surface type tablet](https://www.ultrabookreview.com/71207-amd-strix-halo-asus-rog-flow-z13/). Regardless of the effective yields due to it's size, it is coming out another year later when compared to a 40 series gen, and a tier lower than the 4060. If you want to game, like many have argued with Strix Halo when it launched, just get a discounted RTX 40 dGPU laptop... Panther Lake has a great iGPU, don't get me wrong, but the argument isn't good.  A better one would be \~10-25W Panther Lake would be competitive than Strix Point/Halo and so on, not 'Strix Halo isn't economically sensible' because it's still on the market, with CES designs still being announced.  Some people in the sub think that if they aren't the ones the product is directed to (which is pretty much gamers), then they believe 'well it must've been a failure'.",hardware,2026-01-10 19:17:17,-3
Intel,nyt80oc,"Oh I'm blind lol, my bad.",hardware,2026-01-10 16:45:57,8
Intel,nyt0j4v,Yeah I'm bewildered by this take that it's not sizeable.,hardware,2026-01-10 16:10:35,10
Intel,nz5etqc,Easily offset with a slightly bigger cache.,hardware,2026-01-12 12:17:50,2
Intel,nyw80nh,"Don't worry, the engineer at HP also made sure you can never exceed 35W continuous power draw by giving it an undersized vrm and no vrm cooling",hardware,2026-01-11 01:45:48,2
Intel,nyt058y,"Oooo neat, close to perfect for me.",hardware,2026-01-10 16:08:46,6
Intel,nyu7ff7,"> Intel is also on record saying 18A cost structure is flat vs Intel 7  The 12Xe GPU die is on N3E, not 18A. Though I still agree with the conclusion that PTL should still end up relatively affordable, and cheaper than an equivalent dGPU.",hardware,2026-01-10 19:33:11,11
Intel,nyxbhd7,"Power efficiency is a curve. There will exist points along that curve where the B390 is more efficient than the 5060.   Efficiency is more complicated than just ""perf/watt at specifically both chips maximum power draw""  edit: May have misunderstood your comment. Thought you were saying B390 was less efficient than a 5060",hardware,2026-01-11 05:35:14,1
Intel,o058umn,Asus G14 would like a word.,hardware,2026-01-17 18:11:44,2
Intel,nyt7ejr,>(and stuck with 16/512)  Those typically have open ram and ssd slots. It's the premium thin models that have them soldered on.,hardware,2026-01-10 16:43:03,8
Intel,nysokej,https://www.bestbuy.com/product/asus-tuf-gaming-a16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-32gb-ram-nvidia-geforce-rtx-5070-1tb-ssd-jaegar-gray/JJGGLH8Y2Z  [Proof that the deal at least exists at the time of this comment](https://imgur.com/a/XYQm2fn),hardware,2026-01-10 15:10:23,10
Intel,nywgma9,"QC last year had a bad product.Â  It was competitive vs AMD and Intel but Qc was selling those for 50% less than Intel or AMD chips. OEMs at first decided to price these at Intel prices then it settled at Intel -100/200â‚¬   QC laptops still sold what QC and partners expected and OEMs are increasing new models for X2 (design wins went from 60 to 100+)   X2 has a 25% advantage vs Panther Lake and it will still be cheaper because QC is an underdog. If QC captures market share and reaches 10-15%, then Intel will start to sweat and then margins will be hit. I don't think QC gets anywhere near that till like 2028/2029. The laptop market is VERY slow to move. AMD had a better product for several generations and it only netted them +10%   While QC and Mediatek/Nvidia don't hit a bigger marketshare number. Intel and AMD won't need to lower prices",hardware,2026-01-11 02:31:29,2
Intel,nyt5fdi,"[285H GB6 Windows \~2900](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+windows)   [285H GB6 Linux \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+linux)  [HX370 GB6 Windows \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+windows)   [HX370 GB6 Linux \~3000](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+linux)  I'm just eyeballing results on geekbench browser 1st page but surprised by the HX370 results. Intel is all over the place but that may make sense since Intel is actually found on tons of laptops compared to AMD shiny hunting experience.  There was one source i had found testing X Elite on Windows and WSL2 on the same machine that showed Geekbench performing higher on WSL2 than native on windows but it may have been a yt video. Perhaps i'm mistaken.  AFAIK , the Windows Tax is real for Geekbench. this particular search on HX370 showing otherwise is a fluke imo. You can search other chips too, like 285K 3350win vs 3500linux  Unfortunately, i cannot bother looking for more controlled setups that had the same exact setup with both Linux vs Windows compared to definitely prove this, but i have seen those in the past here and there.  EDIT:   I found a source that compares GB6 Windows vs Linux relatively recent   [AM5 W11 vs Linux Performance Comparison in GB3,5,6 - Ryzen AM5 - HWBOT Community Forums](https://community.hwbot.org/topic/236884-am5-w11-vs-linux-performance-comparison-in-gb356/)  The Windows tax is still real.  [Qualcomm Snapdragon X2 Elite Extreme X2E-96-100 Processor - Benchmarks and Specs - NotebookCheck.net Tech](https://www.notebookcheck.net/Qualcomm-Snapdragon-X2-Elite-Extreme-X2E-96-100-Processor-Benchmarks-and-Specs.1127282.0.html)  Idk what actual source notebookcheck used here, but if x2 Elite Extreme reaches 4080 in GB6 on windows then +4% for linux would be 4240... Whether that's comparable to Apple M5 or not i'm not gonna say more on the subject...       I'd say Qualcomm is gonna be within spitting distance to Apple... Sure SD2X Extreme is highest end unicorn SKU vs base M5, that's valid argument, but still... within 10% of Apple i consider within spitting distance.",hardware,2026-01-10 16:33:45,4
Intel,nyt6hqk,I hope somebody tests Panther Lake GB6 on both linux and windows.,hardware,2026-01-10 16:38:46,0
Intel,nyvsukh,">Chasing 5GHz is only stupid if it costs more power than it'd save.Â   Chasing any GHz much above Vmin would cost more power than the performance it would bring, no?",hardware,2026-01-11 00:24:52,4
Intel,nyytnv8,"They are not. Like I said they are 25% behind Apple and Qualcomm in ST. Multicore the X2E can go up to 2x the performance of Arrow Lake and Panther Lake is just a refresh on 18A   Now there's more competitors.Â  They have 5 total. AMD, Nvidia, ARM, Qualcomm,Â  Apple",hardware,2026-01-11 13:19:34,1
Intel,nyrg65a,3nm will give 6050 another 20%. Whatever changes amd/intel do at power limit needs to be impressive. Otherwise I still see cpu + gpu combo yielding better perf.   Not perf/W or maybe perf/$,hardware,2026-01-10 09:51:05,13
Intel,nyrz0br,LPDDR6 coming hot with 50% bandwidth improvement...,hardware,2026-01-10 12:35:09,5
Intel,nyxwgwn,"Obviuosly. all AI *usage* is inference. Inference requires plenty of memory btw, it all depends on the model you want to run.  No, that is not the business case use.",hardware,2026-01-11 08:32:52,6
Intel,nyyeomk,"The Strix Halo was intended for local ai, the OpenAI OSS 120B fp4 model (or a 240B fp4 50% pruned like MiniMax 2.1) is run at 50 t/sec on a Strix Halo, or about 5 000 000 tokens day - 75$ if using Sonnet 4.5.  So in 20 days you get the money back (a 96GB RAM Strix Halo during 2024 has been sold for1480$ by a lot of OEMs and the 128 GB RAM - for 1600$-1700$), not to say you keep home your AI work",hardware,2026-01-11 11:21:07,3
Intel,nyyfatm,"if only AMD released a 384 bit bus Strix Halo with support to LPDDR5X 10700 MT/s, that would double the bandwidth - from actual 260 GB/s to 520 GB/s, putting it in the M4 Max category, which Apple is selling at 4000$",hardware,2026-01-11 11:26:37,1
Intel,nyua4zp,"They're completely different product classes. One is priced for mainstream and the other is decidedly not. One is purpose-built to go up against discrete GPUs, the other is not. That's why flipping that statement just doesn't work.",hardware,2026-01-10 19:46:22,8
Intel,nz565l5,"Donâ€™t even get me started! My current HP laptop straight up doesnâ€™t support any type of fan control on Linuxâ€¦  So even if I throttle my CPU manually based on temps, the vrm WILL burn a hole in my desk during prolonged use  I even found the basic EC registers for fan speed, but there is some other magic register that keeps resetting them. And trying to find the magic register might involve frying the board if you hit a voltage-related EC",hardware,2026-01-12 11:08:51,1
Intel,nyt1ch3,"ikr, im also heavily considering the Prestige 16 Flip atm (even tho I am an unhappy owner of a 8 year old MSI Thin...)",hardware,2026-01-10 16:14:27,6
Intel,nz39yac,Not using LPDDR is part of what makes it a shitbox.,hardware,2026-01-12 02:28:11,5
Intel,nz7jgiw,Screen is still dogshit,hardware,2026-01-12 18:45:44,0
Intel,nyv9rhl,"It's not just Geekbench, Linux usually has higher performance",hardware,2026-01-10 22:44:30,2
Intel,nyvzib4,"depends on the workload and the efficiency curve, but there is the race to sleep concept. Even assuming hanging around at low freq the voltage can sustain is always better power wise - which i don't think is true as you're dropping a lot of performance, you still have to power all the uncore around it  I saw someone run a couple tests on intel/amd for iirc a game server workload, and while intel peaked a lot higher from aggressive boosting, the amd cpu consumed more energy overall",hardware,2026-01-11 00:58:42,1
Intel,nz4duhh,"Neither apple nor qualcomm are real competition in a sense that Apple has its own segregated market that does not crosscompete and qualcomm practically does not exist in segments Intel is in.  ARM is hurting them in servers, but not really relevant for a laptop discussion.",hardware,2026-01-12 06:45:51,2
Intel,nyu8aqs,"> 3nm will give 6050 another 20%  But are Nvidia willing to use cutting edge nodes for their low end GPUs? If they don't move to N3 before Intel/AMD have an N2 GPU, a gap will remain. And of course LPDDR6 should be a big deal for bandwidth.   Obviously not treating this as a forgone conclusion, but doesn't seem like an unreasonable target for this part of the lineup.",hardware,2026-01-10 19:37:26,3
Intel,nystsgw,The only TRUE disable option on windows is to disable through bios for most laptops. Which becomes extremely tedious if you want to use the dGPU without constantly restarting.  I have never owned a laptop with a dGPU that didn't misbehave constantly and not fully idling.,hardware,2026-01-10 15:37:37,5
Intel,nyy6xsc,"Plenty of ""daily use cases"" have very minimal hardware requirement, the original iPhone X FaceID ran on a device with 3GB ram and it was sufficient for FaceID purpose. And I don't know nor care your particular business use case, since you made zero specific clarifications I only had to bring up one mainstream example which is Microsoft 365 and its cloud based subscription based Copilot feature.",hardware,2026-01-11 10:10:21,1
Intel,nyyfwoo,Probably that's what gonna happen with medusa halo. On N2. It will actually match Apple M6 Max pricing.,hardware,2026-01-11 11:32:03,3
Intel,nyue00o,"I am not talking about product classes though? The original statement is trying to say that an **SoC can compete with dedicated iGPU** regardless if Strix Halo is bigger. They are trying to say that it was obvious it was **going to be a flop, when competing against a 4060 that at the time was being sold at a discount**. **Panther Lake is literally coming out another year later one tier below a 4060 and a gen old**.  Yes, they are different product classes, but Panther Lake SKUs that have 10-12 Xe3 cores will most definitely be >$1000 with laptops. ""Mainstream"" pricing is subjective in this class, unlike GPUs where there are 5060s and 5080s segments. At CES, there are surprisingly dGPUs still being paired with Panther Lake, heck, Strix Halo was designed purely for it's iGPU, even the engineers stand by this (PCIe slots are being released in miniPCs because that's what the market wants).  Also, this ""big APU"" argument is based on chiplets/tiles. Strix Halo isn't monolithic, same as Panther Lake. They both have the same design strategy that makes it economically viable to tape out in the first place.  I am not trying to say that STX-H is better than PTL, PTL was like the only thing I was looking forward to at CES, but this whole thread surrounding around how STX-H is a failure doesn't make sense at all.",hardware,2026-01-10 20:05:42,-5
Intel,nyt2jkc,"I hadn't long bought a Zenbook S16, but if MSI can get a decent spec with the B390 under Â£2k then maybe.",hardware,2026-01-10 16:20:10,3
Intel,nz3mb97,Their target audience is more likely to complain about upgradability.,hardware,2026-01-12 03:35:16,2
Intel,nz9ccsy,New goalpost?,hardware,2026-01-12 23:57:14,3
Intel,nyxhb1b,"Race to sleep has value to a point. Does someone on battery want to, say, increase power consumption 4x to race to sleep 2x faster?",hardware,2026-01-11 06:19:34,3
Intel,nyxoaio,> there is the race to sleep concept.  i hope we can excise this concept as soon as possible. It leads to worst design choices.,hardware,2026-01-11 07:18:33,3
Intel,nzbv20u,"I don't think amd ryzen will match M6 Max pricing (amd is selling them at 400$), as those miniPC are manufactured by a lot of noname companies, making a true competition  There are 37 such ryzen ai max 395+ products [https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them](https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them)  And there are also nvidia with their dgx project, Qualqom with their Snapgragon X Elite 2, a lot of RISC-V platforms like tenstorrent with 512 GB/s (but only 32 GB VRAM at 1399$), so even apple will need to double the bandwidth in their upcoming M5 pro/max in order to stay competitive with actual prices",hardware,2026-01-13 10:18:10,1
Intel,nyyn7pe,"I assume at least intel and amd do some research there for how much the cpu should boost if the oems don't, and also have to consider user impact from lower performance but I guess that's more fighting against windows getting slower.   Presumably with current nodes 5GHz is always beyond the point of being worth it but no reason that has to carry into future gens",hardware,2026-01-11 12:33:11,1
Intel,nzbx72p,Medusa halo isnâ€™t strix halo if going by what you think it is going to be. It will be much bigger and on N2.,hardware,2026-01-13 10:37:51,1
Intel,nxxrlux,"One of the biggest things the current AMD driven handhelds lack is a decent upscaling option, so getting native XeSS support on a fast GPU would be a HUGE performance boost.",hardware,2026-01-06 02:39:52,110
Intel,nxy2sll,"I think the LPE cores and them going at chiplets a second time after Meteor lake is paying off. This chip is more efficient than lunar lake, a chip that could do 0.62W idle lol.",hardware,2026-01-06 03:42:57,37
Intel,nxy9wxm,"This is exciting. Hope some decently priced handhelds can drop, RAM prices notwithstanding.",hardware,2026-01-06 04:26:42,8
Intel,nxxr090,am confused. this is battlemage too right? because its a B series. but its supposed to be all new. and the old gen was battlemage too on the 200V series. so what is going on here?. is this just a bigger GPU or is this Xe3 so that would be Celestial.,hardware,2026-01-06 02:36:35,21
Intel,nxxhg0r,brah they straight up claiming it's equivalent to a 4050 on stream >!(a 60W RTX 4050)!<,hardware,2026-01-06 01:44:36,51
Intel,nxykdxb,"Even if that claim were overstated by 2x, would still be a colossal L for amd.",hardware,2026-01-06 05:39:32,19
Intel,nxyc1w5,Xps is a huge seller for Dell and they are straight up using Panther Lake and XE3. They are exclusively going intel. Intel is 100% securing up there dominance in Labtops. In the process also taking business away from Nvidia.,hardware,2026-01-06 04:40:47,20
Intel,nxxgruz,I hesitate to trust Intel's charts. But I am interested if Intel will actually get companies to adopt panther lake for their handheld pc. They did not have much luck with lunar lake.,hardware,2026-01-06 01:41:01,22
Intel,ny1ifg6,"Assuming intel also keeps those mobile CPUs a good price, this could be really good. Hopefully as well they add the B390 in their high power desktop CPUs, seeing a core ultra 5 with an iGPU like this would really mitigate the need for a dedicated GPU right away Mostly because iGPUs on other generations were bad, and only a select few Ryzen CPUs had the 890M. Budget systems could become much better for gaming on the low side for graphical intensive games",hardware,2026-01-06 17:30:54,3
Intel,nxz4ji6,>Intel reference platform; Memory: 32GB LPDDR5 9600;  I wonder how much difference that makes and if we'll even see laptops with such RAM in this economy...,hardware,2026-01-06 08:34:38,3
Intel,ny34ie7,"I'd love to see benchmarks comparing it to lower end discrete GPUs (like 5050, B570, etc). Could be a boon for ultra low cost builds depending on what price point it lands at.",hardware,2026-01-06 21:54:52,2
Intel,nxxyz1h,How many compute units does it have?,hardware,2026-01-06 03:20:58,3
Intel,ny1ev6y,XESS and native frame gen is going to make handhelds monsters with Panther Lake in them.,hardware,2026-01-06 17:14:42,3
Intel,ny0ibg4,I think people need to be ready for the fact that OEMs aren't going to use lpddr5x-9000,hardware,2026-01-06 14:42:06,2
Intel,nxxd70q,"We'll see. Every year they claim they're faster, and every year they have been proven not to be",hardware,2026-01-06 01:21:44,-28
Intel,nxxgaz9,NOTE: this might be because it has MFG (Multi-Frame-Generation).  We have to see reports to see if its true or not.,hardware,2026-01-06 01:38:30,-11
Intel,nxxoge8,"This ain't gonna matter. It's the sku with 50% more igpu cores compared to lunarlake, which already has an igpu that's larger than the hx 370, it's real expensive. Imagine a hx370 with 26cu instead of 16, that's the price range you're lookin at  Any system built with this is gonna need to run at extremely high mem speed to feed the really large igpu which in the current market with insane ram prices is gonna be priced out of most people's budget. Are ya prepared for a gaming handheld that costs north of $1500?  And since this is gonna compete against nvidia's entry level mobile gpus oems are gonna have to choose between nvidia and intel for the gaming brand on laptops. Amd learned this through the hard way that most oems would choose nvidia over a large igpu.",hardware,2026-01-06 02:22:32,-12
Intel,nxykt3q,Haven't they been making similar claims for all their failed GPU's?,hardware,2026-01-06 05:42:44,-10
Intel,nxyax11,"I mean Intel has never fudged the numbers before when they were behind, or do something crazy like literally bribe people.... Oh wait.... Uh.... Oh.....   Jokes aside, with what the current and future state of the market looks like, people might have to get used to iGPU graphics.",hardware,2026-01-06 04:33:13,-14
Intel,nxyljzq,"To be fair it kind of is, the die size is huge, larger than an RTX 5070 die",hardware,2026-01-06 05:48:23,55
Intel,nxxotcs,"Intel laptops were already better tbh, AMD had nothing to compete with Lunar Lake, and Arrow lake pretty much was better at high perf efficiency. Zen 6 better not be delayed or AMD will be buried under intel, qualcomm and apple all launching a real next gen shortly",hardware,2026-01-06 02:24:30,87
Intel,nxz1mq8,"Nah, because it's an ""AI chip"" and AMD will market it as so. AI equals fancy even though the AI capability can't match a regular desktop computer for far less.  Intel is probably gonna strategically match AMD in price.",hardware,2026-01-06 08:07:06,6
Intel,nxxvwrz,"With how wide the memory bus is, how much RAM it requires, nah the price is going up.",hardware,2026-01-06 03:03:31,24
Intel,nxzxy26,it has soldered ram ... so it's better then gold!,hardware,2026-01-06 12:45:44,3
Intel,ny04wij,I guess that depends on Intel pricing too. Considering it's using both the latest TSMC and Intel foundries in one chip package. Not to mention the LPDDR5 9600.,hardware,2026-01-06 13:28:38,3
Intel,nxxhzsm,You mean OEMs.,hardware,2026-01-06 01:47:35,-11
Intel,nxyn46u,No?  People will still value AMD more ue to brand so Intel will have to rely on volume for revenue  For reference only yesterday on this sub we had people talking about Intel lacking efficiency in comparison in mobile space,hardware,2026-01-06 06:00:32,-13
Intel,nxy8gkb,Crazy AMD haven't updated their iGPU to RDNA 4. I know they're probably waiting for UDNA but it would have been almost 3 years on the same architecture by the time we get the UDNA refresh next year (if they even bring it to their APUs right away). Sort of disappointing.,hardware,2026-01-06 04:17:21,70
Intel,nxxun0v,"tbf the most important issue is, few games implemented XeSS, just like AMD FSR.  And I think XeSS 3 being implemented in more games is a net positive for AMD GPU too.",hardware,2026-01-06 02:56:27,25
Intel,nxz85mh,"With everything happening around NVIDIAÂ´s price increases and AMDÂ´s lack of providing updates where it hurts, it **feels like** AI-Datacenters are more important right now for them (like the last 2 years).  But who can resent them as Intel had products that where not so much competitive that time.    Arrow lake (Desktop) at least closed on efficiency, but lacks a bit of gaming performance still, hopefully Nova Lake will be the step required to push more competition.   On GPU the same, AMD does not compete with NVIDIA in higher segments while NVIDIA is fairly comfortable with their setup and increases prices because they want to milk customers to increase their ridiculious margins (up to 70%) that they are used to from AI-Chips.   And now Intel also provides Multi-Frame generation, while a niche for me still, starting to compete with AMD and closes up to NVIDIA in terms of Software support, which they lacked the most and fixes a lot of problems.   Now let them release a B770 that is rumoured to be fairly mid/high range and we can hope for competition that actually learned from bad products recently and tries to make it better.",hardware,2026-01-06 09:09:24,9
Intel,nxz92l2,"Handhelds is a tiny tiny market, basing your product stack around them would be monumentally stupid.",hardware,2026-01-06 09:18:25,2
Intel,ny0x7rg,"It may not beat LNL in very low power envelopes (LNL was designed for ~10W, PTL for 15W+), but it's a much, much better baseline than what Intel's historically had in client. Even just extending vaguely LNL-tier efficiency across the stack is a very big deal. Looks like Intel finally has a respectable SoC architecture. Now just need to get the cores and such in shape.",hardware,2026-01-06 15:54:14,13
Intel,ny4y069,"I mean the Xbox Ally X handheld is considered a $999 ""console"" so it sets the floor for what the Steam Deck and other handhelds would be priced at.",hardware,2026-01-07 03:37:21,1
Intel,nxxwdna,"It's branded as a Battlemage for some reason, but the architecture is Xe3. It's much closer to Celestial than it is to Battlemage.",hardware,2026-01-06 03:06:08,50
Intel,nxxt4ce,"Battlemage is the brand name. The actual architecture of Lunar Lake is Xe2, same as desktop Battlemage, but they never explicitly called it Battlemage, only â€œArc Graphicsâ€.  What is meant to be desktop Celestial is Xe3P, but desktop Celestial is likely cancelled or significantly scaled back. Alchemist was a massive flop, and by the time the B580 came out to salvage Arcâ€™s reputation the axe had probably already swung.",hardware,2026-01-06 02:48:07,7
Intel,ny7ef2l,It's a mid-gen refresh of battlemage.,hardware,2026-01-07 14:33:10,2
Intel,nxxpiop,"They claimed ""10% faster"" than 4050   https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Famd-is-done-v0-8op4m6l6bmbg1.jpeg%3Fwidth%3D1851%26format%3Dpjpg%26auto%3Dwebp%26s%3Df229e1ff0e364a6db90715de23ba799261ffe9e3",hardware,2026-01-06 02:28:19,55
Intel,nxxqhlr,APUs are always way worse at gaming than synthetics when compared to a DGPU due to memory bandwidth limitation and power sharing with the CPU among other things like cache set up etc.  when they compare them to GPUs its always synthetics unless you get benchmarks of games,hardware,2026-01-06 02:33:43,22
Intel,nxxl8hj,"Where's the bandwidth coming from?   Reviewers were saying that the 890m was bandwidth starved, so how can this chip be neck and neck with a recent dgpu with multiple memory channels",hardware,2026-01-06 02:05:05,12
Intel,nxy0hn7,60W is the laptop power draw. It looks like 30W for the 4050  this is the laptop they used for the comparison https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor,hardware,2026-01-06 03:29:36,7
Intel,nxy6cgs,"At best, itâ€™s a 16% difference between a 100 watt and 60 watt RTX 4050 I believe, based on synthetic performance  Edit: Intel used a 30 watt 4050, this comment is incorrect",hardware,2026-01-06 04:04:08,0
Intel,ny0ce80,What do you mean that a refreshed Strix Point canâ€™t compete with an updated architecture?,hardware,2026-01-06 14:10:24,5
Intel,ny0j8e4,"I got downvoted everytime I brought this up, but this is precisely why Nvidia wanted a deal to have an Nvidia iGPU tile on an Intel APU: Large iGPUs in thin and lights are going to get good enough over the next few years to make them the new entry-level graphics option for people. This directly threatens Nvidia's consumer laptop volume in the entry segment. Intel is claiming close to 4050 performance at this lower TDP, and that's certainly good enough for many to not have the tradeoffs of having a dGPU in their laptop.  If the new market is moving towards putting GPUs on the CPU package instead of discretely on the board, Nvidia doesn't want to place all of their hopes on WoA becoming better, and are hedging by doing both their own SoC *and* an x86 APU with Intel.  The XPS line dropping Nvidia discrete all together is proof of this. In these sub 70W total laptop power markets, a discrete GPU is just eats into the power budget too much.",hardware,2026-01-06 14:46:50,9
Intel,nxzgk6d,"In the ultraportables market (like XPS), integrated graphics just make so much sense (energy envelope; cooling system required; battery life; etc); and that's already substantial and before considering the cost of a NVIDIA mobile dGPU itself.  I don't understand why AMD decided to price Strix Point and Strix Halo so ridiculously -- it's their market for the taking.",hardware,2026-01-06 10:28:32,8
Intel,nxzf0cx,I think theyâ€™re trying to take away business from Qualcomm/arm on windows before it takes off,hardware,2026-01-06 10:14:30,5
Intel,nxyetge,"Lunar Lake was a expensive product which didn't make sense in handhelds, Intel just didn't have anything else so they slapped that on the MSI Claw. Now the options should be much better considering they are selling a lower core Xe3 version for cheap too",hardware,2026-01-06 04:59:20,9
Intel,ny2yll4,"I'm just curious how they handle the need for such high speed RAM on desktop though? I guess this is an application where CAMM2 will be required, I don't think DDR5-9600+ is possible without it and this is presumably pretty key to the performance.",hardware,2026-01-06 21:27:43,5
Intel,nxzgr0g,"It certainly would make a huge difference because iGPUs are very memory bandwidth bound; and as the name suggests, LPDDR5 9600 has literally twice the bandwidth of the JEDEC standard 4800.  Unfortunately I doubt we'll see reasonably priced laptops with LPDDR5 9600 -- even as an add-on option. I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800; and many SKUs that were 2x16GB are now 1x16GB; yes **single channel**.... they charge you extra if you want 2x8GB.",hardware,2026-01-06 10:30:16,14
Intel,nxzfdoo,X9 and X7 have 12 Xe cores and the best Ultra 5 has 10 Xe cores,hardware,2026-01-06 10:17:51,3
Intel,nxxf8he,??? lunar lake has already shown to be faster than the 890m.  73 percent though seems like a bit much since panther lake was claimed to be around a 50 percent increase over lunar lake,hardware,2026-01-06 01:32:47,64
Intel,nxxhysy,"They did make a graph specifically to compare the performance of HX 370 and this Arc B390 while they were both using 2x upscaling, which is where this 73% number comes from. In another graph featuring supposedly ""native"" 1080p, they claimed Arc B390 was 82% faster than the HX 370 (why don't they just call it Radeon 890M though...)",hardware,2026-01-06 01:47:26,26
Intel,nxxhhap,"No, intel claims 73% with upscaling (both) and 82% native",hardware,2026-01-06 01:44:47,19
Intel,nxy83by,"If it was only a 73% gain *including* MFG, then that would be a serious performance regression. If they were using MFG in their graphs, it would easily be 200% - 300% ""faster"" at the same ""real"" performance",hardware,2026-01-06 04:15:06,5
Intel,nxxhnbn,"The graphs all listed games and I didn't see any synthetic benchmark scores were listed, so yeah.",hardware,2026-01-06 01:45:42,24
Intel,nxxsvz8,The relative proportion of the die isn't as important as the die size itself and the node ofc.   Lunar lake for example has an estimated die size smaller than the hx370 so even if they did make the die bigger I don't think that is going to massively raise the price. Not to mention intel owns the foundry unlike AMD who are outsourcing to TSMC. This isn't in the realm of a strix halo competitor with a 300 mm\^2 + die size.   Dell for example has already refreshed the XPS line with intel panther lake and cut out the option for a dedicated gpu.,hardware,2026-01-06 02:46:52,18
Intel,nxxhf0d,"There are 50% more GPU cores here than on Lunar or Arrow Lake. CPU is still 16 cores as well compared to Arrow Lake, just shifted from 6+8+2 to 4+8+4.",hardware,2026-01-06 01:44:27,26
Intel,ny08rcj,"Do you mean Strix Halo?  Halo is made up of THREE dies. Two are regular CCD and one is a ~300mm2 graphics die. Total die area is around 440mm2 IIRC.  It's expensive, but not THAT expensive.",hardware,2026-01-06 13:50:17,33
Intel,nxxs32m,Doesn't this depend on use case? AMD laptops are more capable for gaming and the iGPU can also use the lesser version of FSR. Intel is obv better for productivity.,hardware,2026-01-06 02:42:30,18
Intel,nxzgbtp,"I disagree, Arrow lake HX seems to be more expensive than AMD HX as least on Lenovo Legion Pro setup.   I would have buy Arrow lake for the same price but AMD is cheaper by $200.",hardware,2026-01-06 10:26:24,1
Intel,nxy0jsm,Eh? It's a standard 128 bit memory bus.â€‹,hardware,2026-01-06 03:29:57,38
Intel,nxz8yd7,"You can't price it higher than people are willing to pay, how high that is I have no idea, people bonkers buying CPU only laptops at these high prices if gaming is something they really want to do.",hardware,2026-01-06 09:17:16,1
Intel,nxyjv70,Oems magically dont price intel variants as if they were made out of gold?,hardware,2026-01-06 05:35:35,6
Intel,ny2zhn1,"Lmao check the data, Intel has 79% of the laptop market share currently",hardware,2026-01-06 21:31:48,5
Intel,nxz7zsg,"Reddit isnâ€™t indicative of anything really, most casual laptop buyers donâ€™t even know what AMD is.",hardware,2026-01-06 09:07:48,10
Intel,nxzevm6,"I wouldn't be surprised if this is because the team has chosen to focus efforts on UDNA because that's the architecture next-gen consoles would use. They only have so much talent and headcount on their graphics division after all, and consoles have much higher volume (even tho low margins) and thus take priority.",hardware,2026-01-06 10:13:17,33
Intel,ny7dxyt,AMD is planning on again using RDNA 3.5 on their next mobile chips as well.,hardware,2026-01-07 14:30:42,5
Intel,nxy8549,Not an ideal solution but Optiscaler exists,hardware,2026-01-06 04:15:24,14
Intel,nxzf3cy,"NVIDIA has increased margins but they haven't been that terrible. Part of the compounding issue at play is limited TSMC capacity; with both gaming and DC on the same TSMC node.  Ampere (crypto bubble ignoring) was priced well and many excellent cards in there since it was on Samsung, a cheap fab; while DC/workstation chips got TSMC.",hardware,2026-01-06 10:15:15,7
Intel,ny0vvvv,Who said anything about basing the entire product stack around handhelds?,hardware,2026-01-06 15:48:11,6
Intel,ny13z79,"That's not quite right. Power levels are determined by the frequency of a given CPU core. The LPE cores in Lunar and Panther lake both clock up to 3.7 GHz, so given the added IPC of the new Panther lake e-cores and better process node, it is more efficient. Base power levels tell you nothing really.",hardware,2026-01-06 16:25:13,1
Intel,nxz3zvx,"[It's actually closer to Battlemage than Celestial. Straight from Tom Petersen](https://youtu.be/P2AsCkKi-vs?t=1576)  >""Unfortunately that Xe3 name got decided years ago, it's actually spread around the Linux stack. Changing the name of that would have been very, very painful. So, that's why you're seeing this disjointedness abut Intel Arc ""B"" series. **Well, [Panther Lake] is B series because it's similar to Xe2** and we want to be transparent with our customers. Panther Lake has a new and improved GPU, that GPU is bigger and **it's very similar to B series.**""",hardware,2026-01-06 08:29:22,21
Intel,nxyzwrf,"Xe3 isn't Celestial, only Xe3P will be. See [https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake](https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake)",hardware,2026-01-06 07:51:09,13
Intel,ny0y47o,"Yeah, it's a proper generational jump. Intel marketing is just dumb, and the comments from Peterson claiming Xe3 is somehow a smaller jump than Xe3p are just laughable.",hardware,2026-01-06 15:58:20,4
Intel,nxyhb1s,The reason is marketing (the Battlemage brand is hot and filled with good will ATM so resetting to celestial so soon is not ideal regardless of panther Lake being xeÂ³) Peterson addressed this a bit ago.,hardware,2026-01-06 05:16:46,1
Intel,nxysdu1,Xe3p was alr confirmed coming im sure Celestial happens,hardware,2026-01-06 06:43:57,1
Intel,nxzg5fp,"We will see, while Intel's PR and marketing is extremely confusing, Intel did confirm Xe3P will come to desktop; and at least from driver updates (as a very happy B580 owner) driver support has been constant and lively.  I had some issues with an older Civ game, I reported an issue in [their app](https://www.intel.com/content/www/us/en/support/articles/000057021/graphics/other-graphics.html) with screenshots/etc, and while I never got any notification, the game works perfectly now a few months later. Dunno if they read those reports, but my card keeps getting better.  I actually think a MSRP B580 is another card that will age like fined wine -- YMMV depends on games you play, but in Australia they have been regularly sold slightly below international MSRP and represent phenomenal value in the price class.",hardware,2026-01-06 10:24:47,1
Intel,nxzdyw4,That's bloody good for an iGPU. It's been nice to see them finally get to respectable performance over the last few years. Intel in particular has really upped their iGPU game & it shows.,hardware,2026-01-06 10:05:03,21
Intel,nxxpg2g,not to mention it is a little skewed as they threw in a title which pushed the vram limit on the 4050 making the b390 over 800 percent faster in that title which obviously messes with the average.,hardware,2026-01-06 02:27:55,17
Intel,nxxsozj,It's 10% faster geomean across 45 games,hardware,2026-01-06 02:45:47,29
Intel,nxzfaqu,That can be resolved if either Intel or AMD decides to unlock quad-channel on consumer chips and mobos. It's artificial market segmentation; the die area needed to deliver more (LP)DDR5 channels is absolutely minuscule; for a huge boost in iGPU performance.,hardware,2026-01-06 10:17:06,2
Intel,nxy0rol,Cache. Lots of it.,hardware,2026-01-06 03:31:13,22
Intel,nxxpegk,"They are using 9600mt/s lpddr5x, could also have a lot of cache, (iirc 890m configs were nerfed in cache because they wanted to put a npu instead), and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.",hardware,2026-01-06 02:27:40,13
Intel,nxxumwa,Panther Lake still has a 128 bit memory bus so only models with 9600 mt/s will get slightly faster shared memory bandwidth than Lunar Lake.   I wonder how this will manifest in games as the only performance leaks have been from Geekbench and 3DMark which may not be as bandwidth intensive as real games and applications.,hardware,2026-01-06 02:56:26,4
Intel,nxy8e9t,"This seems to be correct, since checking NotebookCheck for the 30 watt 4050 shows that itâ€™s around 70% faster than the HX370 in games, which is roughly where Intel places their iGPU.  The performance difference between a 30 watt 4050 and full 140 watt 4050 is around 41 percent performance based on Time Spy",hardware,2026-01-06 04:16:57,14
Intel,nxy2xyi,"basically cheating tho, rtx cards in dell laptops are barely getting enough watts to even turbo",hardware,2026-01-06 03:43:51,4
Intel,nxzpm42,"That's the only way to do a fair comparison, really.   Because the 45 watts that Intel chip uses is shared for the entire chip.   So it's still 45w Intel + igpu vs 60w Intel+gpu",hardware,2026-01-06 11:45:18,1
Intel,ny0na4n,"Itâ€™s a super strong generational gain though, itâ€™s like the jump from Vega 8CU to rdna2 12CU. The kind of single gen gain you see once in 5 years at most",hardware,2026-01-06 15:07:11,10
Intel,ny2zoh2,The thing about that... what sort of tile are we expecting them to package up? As you say if we can get 4050ish performance from an Intel iGPU then they really can't be far off 5050M... and maybe even 5060M performance in future.  Do you think they'll offer something like a 5070 tile? that almost seems excessive (and difficult to actually package from a thermal point of view in a laptop) but it seems like the 5050/5060 sort of tier is going to be pretty well covered as a traditional iGPU soon.,hardware,2026-01-06 21:32:40,1
Intel,nxzk1m0,AMD actually introduced lower tier Strix Halos in this CES; and the first budget laptop thats gonna use it is [the Asus TUF A14](https://youtu.be/h27w0PXFBgk?si=Pa7UQhinywF-uFMj&t=306),hardware,2026-01-06 10:59:13,4
Intel,nxxklk7,They're a lot more accurate than whatever the fuck Nvidia has been doing where you have to decode their bar graphs for proper scaling lmao,hardware,2026-01-06 02:01:38,48
Intel,nxzfsf4,"I'm pretty sure Intel threw lots of ""marketing money"" for the MSI Claw too. There were heaps of MSI Claw promotional booths / draws at shopping malls / public places in Australia and it was heavily discounted.  I picked one up for about $550 AUD (after rebates; tax included), which is like $369 USD inclusive of tax.",hardware,2026-01-06 10:21:31,2
Intel,ny13km9,> Lunar Lake was a expensive product which didn't make sense in handhelds   What do you mean? All the tradeoffs LNL made were pretty good fits for a handheld.,hardware,2026-01-06 16:23:22,2
Intel,nybz77q,"I don't think that will really be an issue, laptops can be configured with soldered 8 channel RAM like AMDs Z2 extreme, or they can still manage easily with regular DDR5 6400Mhz sodimms, which run at 102.4GB/s  Plus the CPUs that have intels new B390 iGPU are 4P/8E CPUs, so I doubt there will be much issues from low ram speeds. Something like the Radeon 890M have done fine with such speeds",hardware,2026-01-08 03:26:50,1
Intel,ny13z0b,"> I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800   You're looking at normal DDR, not LPDDR. LPDDR5-9600 *is* a JEDEC spec, and already available in mobile.",hardware,2026-01-06 16:25:12,4
Intel,ny020mx,"The majority of the lineup still only has 4. Will be interesting to see what the pricing and performance is on those since these will likely be quite limited. What's also a bit crazy is there's three different nodes being used for the various GPUs, and the full 12 unit one is probably on N3.",hardware,2026-01-06 13:11:40,2
Intel,o2xamaf,cpu: compute processing unit,hardware,2026-02-01 05:43:22,1
Intel,nxxlxrw,"I doubt it's exactly 73% outside of cherrypicked games, but it should not be shocking that it's significantly faster than rehashed rdna3.",hardware,2026-01-06 02:08:52,-3
Intel,nxxjlfd,"Every Intel marketing benchmark for like a decade or so, but especially their GPUs seem to do far better in their benchmarks than they do in reality.",hardware,2026-01-06 01:56:13,-20
Intel,nxxhxjk,"Ice lake, Alder lake, Metor Lake",hardware,2026-01-06 01:47:15,-20
Intel,ny0aepy,"Different poster than OP.  Compute tile on Lunar Lake is 140mm2 on N3E with a small 46mm2 controller N6 tile. Strix Point (HX 370) as a whole is 233 mm2 on N4P. Lunar Lake is clearly cheaper, but given the newer node and packaging not massively so, likely by around 20-25%.  Panther Lake, with the B390, is going to be significantly more expensive than Lunar Lake. The B390 GPU has 50% more CUs, and that is very likely still on N3 or some variant (Intel only labeled this as external on their deck). CPU size 4xP+12xE as opposed to Lunar's 4+4, which should still be significantly more area with the core upgrades despite being on A18.  The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.  Intel Foundry in general isn't any cheaper than TSMC. With Intel being practically the only user and development expenses it's likely more expensive than TSMC despite TMSC's margins. For all purposes it's an accounting trick to hide CCG's and DCAI's 5-15% operating margins if you divide the foundry losses per group revenue.",hardware,2026-01-06 13:59:25,1
Intel,nxxpvi7,"Yes, although the actual low power 8 core successor to Lunar Lake is the 335/365 with half GPU cores and slower RAM.",hardware,2026-01-06 02:30:19,-8
Intel,ny0b9nu,"even if you exclude the CPU CCDs the graphics die alone is bigger than a RTX 5070Ti mobile GPU, which also retails for \~$2000-$3000, same as Strix Halo laptop",hardware,2026-01-06 14:04:11,11
Intel,ny393aj,"This is slightly splitting hairs but the 8c CCDs in Strix Halo is actually NOT the same chiplet as the ones in desktop zen 5 parts. It iirc is produced on a smaller node, slimmed down, and has different ( or no) TSVs.  It is similar to design and cache sizes to desktop however, but the changes to the CCDs were done to improve low power performance characteristics. They are likely a bit more expensive than Desktop CCDs.  I believe it is discussed in a chips&cheese deep dive.",hardware,2026-01-06 22:16:26,3
Intel,nxycvko,"AMD have largely been ""winning by doing nothing"" due to their better driver support stack for gaming on iGPUs, rather than actually throwing superior hardware at it.  It's almost ironic how AMD's mobile chipsets are now the ""Intel 14nm+++++"" of this generation.  Constant minor refreshes or even straight-up re-badges of old chips.  Now that Intel Arc has been around a while now and is getting quite capable.  I suspect Intel have a real opportunity to overtake AMD this generation in the iGPU space (ie. handheld and mini-PCs), especially since the new AMD APUs are just **another** refresh with a clock boost and Strix Halo is not scaled or priced to be actually affordable by normal people in that market.  XeSS can also act as a massive force-multiplier in power-constrained scenarios like handhelds.  AMD really shot themselves in the foot by either not building or not allowing FSR4 to function on RDNA3/3.5, which all current and now next gen AMD handhelds are stuck on.  Given how effective DLSS is on the Switch2, one could only imagine how kickass a Nvidia chip in a handheld PC could be with the far more ubiquitous DLSS support.",hardware,2026-01-06 04:46:15,77
Intel,nxyz1xa,Now they are not. The panther lake igpus are undisputed winners (excluding the 395+ from amd since it's just not gonna be mainstream). You can get a 358H or 368H and you'll have solid laptop for igpu gaming far cheaper than the 395+,hardware,2026-01-06 07:43:17,18
Intel,nxz8aui,"For business apps laptops have been good enough for 10 years now, iGPU and battery life is really the only differentiator.",hardware,2026-01-06 09:10:50,5
Intel,nxxvlpa,"Intel is plenty competent for gaming, and has XeSS which is way better than FSR3.",hardware,2026-01-06 03:01:47,36
Intel,nxxx4hv,Lol? No 6 or 8 core 3dvcache laptops and no 5080 or 5090 laptops. Strix Halo is a joke for gaming as well,hardware,2026-01-06 03:10:21,-9
Intel,ny1dwcs,The 9955HX + 5070Ti is $2240 and the 275HX + 5080 is $2540.   When both 5070Ti configurations are on sale they should be the same price.,hardware,2026-01-06 17:10:18,3
Intel,nxy23yz,My mistake I was thinking of Strix Halo,hardware,2026-01-06 03:39:00,28
Intel,ny0f479,"It's about compromise. I don't *want* a 4lb laptop. I don't want a laptop that runs hot when web browsing. Or a laptop that has loud fans, or gets poor battery.  I have a desktop for gaming and other demanding tasks. For a laptop, I, and most of the market, want it focused on portability. Light weight. Cool running. Long battery. These big iGPU PTL laptops are really interesting because they provide *good enough* gaming without sacrifice to the non-gaming livability of the device.",hardware,2026-01-06 14:25:02,1
Intel,ny02zjc,"It took me way too long to convince my sister the AMD laptop I bought her isnâ€™t going to blow up in her face and lose all her data, the Intel(and now Apple) CPU brands are very strong.",hardware,2026-01-06 13:17:30,2
Intel,ny0wejw,"> most casual laptop buyers donâ€™t even know what AMD is  We're past that point now. Even ""normies"" have heard of AMD from news.",hardware,2026-01-06 15:50:32,1
Intel,ny006yw,"its always ""fix it next generation"" with AMD.",hardware,2026-01-06 13:00:12,24
Intel,ny7wgeg,This is unfortunate news  (â•¥ï¹â•¥),hardware,2026-01-07 16:00:09,1
Intel,nxyndbc,"I mean yeah it's not ideal, but you could argue it's the same with XeSS or FSR 4 on RDNA 3. Since the OP said ""there's no decent upscaling on AMD handheld"", therefore I assume Opsticaler is out of the question too.",hardware,2026-01-06 06:02:32,9
Intel,nxzib6q,"Well you said it, it's TSMC capacity, meaning also a priority issue. They prioritize AI over consumers and then increase the price by reducing availability, meaning the same chip costs more, meaning more margin.  Seeing they increase the 5090 to roughly 5k (USD) is just the beginning and as I know all companies will use the increasing memory prices to say they must increase the product price, just not proportional to the memory costs.  next step: then they will use this to move more to streaming instead of owning",hardware,2026-01-06 10:44:14,-2
Intel,ny15kho,"> Power levels are determined by the frequency of a given CPU core   There are SoC and platform level targets that depend on a lot more than just clock speed for the same cores. Consider how LNL's PMICs scale vs FIVR/DLVR. Or what operating point benefits the most from the on package memory.Â   Especially at really low power, the cores are not your big concern. Consider the difference at 10W between 50% of your budget available for compute and 80%.Â   > so given the added IPC of the new Panther lake e-cores   We're talking a couple percent. DKT is a tick.Â    > and better process node   Very much unproven.Â    If you want to give credit somewhere, pretty much all of it should go to the SoC and GPU teams.",hardware,2026-01-06 16:32:30,3
Intel,ny0idmq,"Xe2, Xe3, etc. are the ""real"", more accurate names. Battlemage, Celestial are the marketing names.  Intel's decision to label the new Xe3 iGPUs as ""Battlemage"" is certainly an interesting (odd) choice - my best guess for this decision is that next year, Xe3P discrete will launch alongside Xe3P iGPU in NVL, and they're saving the new Celestial naming for that launch event.  Xe2 -> Xe3 is the bigger change.",hardware,2026-01-06 14:42:24,8
Intel,ny1hu62,"Peterson states explicitly it's to take advantage of good Battlemage branding, around 1:30 of this video. [Intel Talks Xe3 Improvements For Gaming - YouTube](https://www.youtube.com/watch?v=Bjdd_ywfEkI)",hardware,2026-01-06 17:28:12,3
Intel,ny0xhzi,"> Xe3p was alr confirmed coming  Not for client dGPUs, which are what get the Battlemage/Celestial brand.",hardware,2026-01-06 15:55:31,4
Intel,ny0xj91,> Intel did confirm Xe3P will come to desktop  They have not.,hardware,2026-01-06 15:55:41,1
Intel,nxy8fjz,"They also might be getting better value out of the ""2x scaling"" choice for benchmarking. Notice how they are behind Nvidia in all the none scaled titles except Dota2 that I saw.  Still very good results for a iGPU, but they are not entirely honest numbers either.",hardware,2026-01-06 04:17:11,15
Intel,nxxshuz,It's 1 game out of 45 in geomean which devalues outliers. ~~9.9% faster instead of 10% faster if you take it out.~~  Edit: Oh no it's actually 6 FPS on the 4050. Yeah that's way too big for geomean to smooth out.,hardware,2026-01-06 02:44:44,23
Intel,nxy7k6w,And the fact they showed 45 games shows how confident they are in this product.  I remember the Intel slides with 5 hand picked titles we used to get just a few years ago.,hardware,2026-01-06 04:11:44,28
Intel,ny17afz,You mean in desktop? Or do you want mainstream mobile to go quad channel?,hardware,2026-01-06 16:40:20,3
Intel,nxzgpgy,I wonder how 96MB cache would do had Intel put that much on it.,hardware,2026-01-06 10:29:53,3
Intel,ny08xk6,> and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.  They're benching 45 games dude.,hardware,2026-01-06 13:51:15,6
Intel,nxy8oam,41 percent difference in performance compared to a full 140 watt in Time Spy. Honestly a bit surprised it isnâ€™t more performance difference.,hardware,2026-01-06 04:18:43,1
Intel,ny55i5l,"No, it's disingenous. Because everyone would think 60w 4050 = 60w on gpu alone",hardware,2026-01-07 04:22:58,0
Intel,o0fmoja,"And it's not like the ""last gen"" GPU in lunar lake was bad either, so we are starting from already good and making the jump up.",hardware,2026-01-19 06:26:10,1
Intel,ny384o8,"Not really sure. I believe it's Hammer Lake that's debuting the Nvidia tile, and that's rumored for a 2029 launch, so still quite a ways off, and 2 generations ahead of Blackwell.  The only rumors I'm aware of that it's going to be a pretty big iGPU",hardware,2026-01-06 22:11:51,4
Intel,ny7dl7u,"Nvidia's graphics have shown to be more efficient for space than both Intel and AMD, so whatever they use it will likely be better than what Intel can currently put out.",hardware,2026-01-07 14:28:52,2
Intel,nxzkp7q,Fantastic -- but at least six months too late ;),hardware,2026-01-06 11:04:52,4
Intel,nxz5md2,You don't like graphs with zero scale claiming their latest 100W GPU is somehow a gazillion percent better than a 4090 or something?,hardware,2026-01-06 08:45:06,9
Intel,nxy8v6h,Wattage limited 4050 to 30 watts is the only slide thatâ€™s suspect.  Itâ€™s around a 41 percent performance loss based on Time Spy from the 140 watt 4050.,hardware,2026-01-06 04:19:56,7
Intel,ny7raf2,I think they meant that the chip is very pricey which sucks because the handheld is already low-margin otherwisr and can't be priced too high else it got undercut by its competitors.,hardware,2026-01-07 15:36:28,1
Intel,ny0l0pv,The standard 4Xe models use the extra die space they save to have more PCIe lanes. that large iGPU adds cost and doesn't make much sense to use that chip if you're gonna add an Nvidia dGPU,hardware,2026-01-06 14:55:55,3
Intel,nxxp4au,"yeah just looking at the game sample I can see a few that really don't perform well on RDNA architecture at least relative to nvidia(idk what really constitutes an ""intel favoured"" title)   Like stalker, csgo 2, civ vii, dying light the beast, and delta force ik run a lot better on nvidia relative to amd so im guessing the same holds true for intel vs amd.   A couple titles amd does well in were thrown in there too though like God of war and Cod but im guessing the real performance difference is more like 40-60 rather than the claimed 70-80.   Pretty large sample though which is nice so the numbers can't be that off.",hardware,2026-01-06 02:26:09,11
Intel,nxxtyhj,Why not?   It's 50% more cores + architectural improvements + clock  speeds,hardware,2026-01-06 02:52:42,11
Intel,nxzgxn6,"Please provide a **single** example in the past ~5 years of an Intel marketing benchmark that is materially inaccurate or untruthful.  NVIDIA is the one playing it loose with BS charts, AMD generally has a good track record (with some exceptions), and Intel on the GPU side has been pretty accurate. For example, these benchmarks have 45 games (!!) and use geomean to reduce outliers.  While I disagree with their choice of LPDDR5 9600MHz (hah, imagine a single consumer product shipping with that in this DRAM market), it is not untruthful.",hardware,2026-01-06 10:31:54,9
Intel,nxxkps6,All were pretty accurate.,hardware,2026-01-06 02:02:15,19
Intel,nxxkqio,But lunar lake igpu actually perform better than 890M.Like comparison of core ultra 7 and z2 extreme in handheld like msi claw.,hardware,2026-01-06 02:02:23,16
Intel,ny0lzbq,">The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.     The mainstream unit that's more directly comparable to LNL is the same core count (4+0+4) with a smaller iGPU tile. It'll be cheaper.  The 4+8+4 w/ 4Xe is the direct replacement to ARL-H, and that should also be cheaper than ARL-H.",hardware,2026-01-06 15:00:42,1
Intel,nxxvizf,"True, but then still, that's not a removal of CPU cores like they said it was.",hardware,2026-01-06 03:01:22,11
Intel,ny3ft4h,That is due to the Nvidia tax and AI bubble rather than the production cost of the chip. Even Apple ships cheaper silicon than that.,hardware,2026-01-06 22:48:46,3
Intel,nxyibby,"This is very topical and cyclical of Intel/AMD. Intel did really poorly for like a half a decade which was unusual but usually they go back and forth. One gets lazy and incompetent, the other curated a masterful product that becomes dominant for a while and then they get lazy and it flips around.  Intel is planning on socketing a ton of cache on their next breed of chips which will massively boost their gaming perfomance and they have pretty darn efficient chips now too.",hardware,2026-01-06 05:24:04,17
Intel,nxygca9,Thank you for the thorough explanation! Very excited for the future of miniPCs and handhelds since there's so many games I'd like to play on the go.,hardware,2026-01-06 05:09:52,2
Intel,nxz2wtv,"Yup, I am very happy to learn how wrong I was thanks to other people in this thread as well.",hardware,2026-01-06 08:19:04,6
Intel,nxzn2om,"For business apps 10 years ago yes, now even Office has bloated itself up so much it's genuinely taxing even on the Apple chips  And well, the better the chip, the more outrageous the user workload gets. I appreciate the modern laptop chip's ability to import a CSV the size of Excel's row count limit and make a pivot table out of that, but now that it *can* do that I'm *expecting* that to be possible as quickly and as efficiently as possible.",hardware,2026-01-06 11:25:00,3
Intel,nxxy7wk,I was under the impression that XeSS needed a dedicated GPU? If it can run on iGPU that's a whole different story.,hardware,2026-01-06 03:16:38,-14
Intel,nxz8pk3,"Their GPU's only look good when compared to 1 generation old bottom tier GPU's of their competitors. Its wild the praise they get.  Same thing will happen here, AMD will release a new iGPU architecture and Intel will be left comparing to out of date CPU's no one buys anymore.",hardware,2026-01-06 09:14:52,-5
Intel,nxxynsf,Sorry I should have specified that I'm talking about budget laptops with iGPUs.   I would sooner build a pc than even think about a 5080 laptop with 3dvcache options.,hardware,2026-01-06 03:19:11,16
Intel,ny01arz,"Yeah; meanwhile NVIDIA just released DLSS4.5 for **every single RTX GPU**... yes all the way back to Turing. It runs a lot better on more recent cards, but it's available on every single RTX GPU if you want to.",hardware,2026-01-06 13:07:13,19
Intel,ny6k0v5,Except with UDNA it might be the first time over a decade AMD isn't phoning it in.,hardware,2026-01-07 11:22:51,1
Intel,ny0x2bs,"XeSS and FSR 4 on RDNA 3 both use downgraded versions of those upscalers, that either look worse, perform worse, or both. In the case of FSR 4, it's a leaked one-off model that people got their hands on. All I really meant by ""decent"" was having an officially supported modern upscaler without all the downsides.  An Intel GPU running XeSS would presumably get the full version of XeSS without the performance hit and with good visuals.",hardware,2026-01-06 15:53:33,4
Intel,nxzipnz,I can currently buy a brand new 5090 in Australia for $2841 USD with express postage included; I'm not sure why it's 5k USD in your region; but there's no reason you should be paying 5k USD. Which country are you in?,hardware,2026-01-06 10:47:45,6
Intel,ny4uh4q,"The ultra X9 388H has a base TDP of 25W and minimal assured power draw of 15W. Meanwhile the ultra 7 155U has base TDP of 15W and minimal assured power draw of 12W. Both these numbers are lower for the meteor lake chip, yet the Panther lake chip is waaay more efficient (+2x). The base power level doesn't mean anything. It might be the point where the chip had the most perf/watt, but that doesn't mean that the performance at lower wattages is the same.",hardware,2026-01-07 03:17:13,-1
Intel,ny0xx01,"Battlemage, Celestial, etc are named they (usually) use only for the dGPUs, even if that does correlate with the B/C-series naming. I think at some point this is just reading the tea leaves. The name's misleading for the tech difference.",hardware,2026-01-06 15:57:25,5
Intel,nxyaxye,If you actually do the maths it'd go down to (1.1^(45)/9)^(1/44) = 1.049 = 4.9% faster,hardware,2026-01-06 04:33:24,12
Intel,ny3bk9e,"Oh wow that's a lot later than I expected, I was thinking this year or next.  Yeah no clue in that case.",hardware,2026-01-06 22:28:12,3
Intel,nxzfk6p,Infinity percent better at a feature the older GPU used for comparison does not support!,hardware,2026-01-06 10:19:28,1
Intel,ny0kqkf,"I don't really think that's ""suspect"". They said they're limiting the total laptop power on the 4050 to match the total laptop power of the PTL chip. If you want stronger performance out of a 4050, you're gonna need to have much higher power draw than the PTL laptop",hardware,2026-01-06 14:54:29,2
Intel,nxy8v14,"Yeah all depends on pricing, 6 core ultra 5 model is however technically downgrade from last generation and the same core config as the i3 1315U.",hardware,2026-01-06 04:19:54,-2
Intel,nxyk9c8,Honestly not that unusual. It takes an average of around 4-5 years to develop a processing unit from the ground up. If we assume each one does this when they get mushroom stamped by the other for being lazy it accounts for the 5 years gaps till they show back up with something to sell.,hardware,2026-01-06 05:38:33,23
Intel,ny0dmdk,"I think AMD is getting a bit lazy when it comes to consumer graphics. I think their attempts at laptop have been really half-assed given just how good their IP portfolio is.  But when it comes to their core businesses, they're definitely been keeping the heat on and have been quite aggressive. They're datacenter first and foremost, and that trickles down to amazing desktop CPUs too. They're heavily focused on building out their Mi series too...but they're just dropping the ball in laptop and consumer GPU",hardware,2026-01-06 14:17:03,6
Intel,ny39k1v,Pantherlake also has an oddity in that it has MUCH higher L2 cache than even desktop zen 5 parts. I'm curious to see its CPU performance in low resolution scenarios.,hardware,2026-01-06 22:18:39,1
Intel,nxxyz2v,"The good version of XeSS runs on any chip with XMX units (Intel's version of tensor cores). Lunar Lake, Arrow Lake mobile, and now Panther Lake have GPU tiles with XMX units, so they get the same XeSS as discrete Arc cards.",hardware,2026-01-06 03:20:59,30
Intel,nxyty97,"Dedicated hardware, not dedicated GPU. The new Intel CPUs have iGPUs with the necessary hardware.",hardware,2026-01-06 06:57:17,7
Intel,nxy622y,"It needs dedicated GPU hardware to run faster, but theyâ€™ve started incorporating it on Lunar Lake and Panther Lake",hardware,2026-01-06 04:02:21,6
Intel,ny0e8rw,Intel has been very aggressive in the iGPU space. AMD isn't going to have any real updates to their iGPUs until 2027 the earliest.,hardware,2026-01-06 14:20:20,9
Intel,ny2bpjc,"> Same thing will happen here, AMD will release a new iGPU architecture   ... Based on what history? AMD's iGPU has not significantly changed in years. It's still hugely memory bottlenecked and no matter how many times they add an extra 2 CU's, it will still be memory bottlenecked.  IIRC someone disabled 2 CU's on their 7000 series APU and their in-game FPS almost didn't change because the bottleneck was actually memory access.  Intel ARC is actually very good on this metric. Intel doesn't exactly need to sling anything better than ""slightly more Battlemage on a better transistor"" to completely swamp out AMD iGPU in this space.",hardware,2026-01-06 19:42:26,1
Intel,ny2y1c0,"Intel Panther lake base tdp is 25w, around the same as AMD Strix Point/ Gorgon Point. Why will they compare it to a 55w tdp Strix Halo?",hardware,2026-01-06 21:25:10,1
Intel,nxy5y1b,"Even on the budget laptops category the new Ryzen 7s suck compared to the Intel Lunar Lake options, they seem to be priced closer with Lunar Lake getting stuff like nice displays. In the really budget category I feel like they are tied on value and I don't know how sales affect that. This is partially cause AMD went cheap on the mid-range kraken point chips and also had to fit in the still dead weight 40 tops NPU for Microsoft. So it only has 8 GPU cores.",hardware,2026-01-06 04:01:39,10
Intel,ny6ka8x,"Yeah but basically unusable on pre 40 series. But at least NVIDIA gives users the choice.  AMD should just stop the BS pretending and just enable the full FP8 model across RDNA2-3 with FP16 emulation. But it prob runs so bad that they won't, far far worse than DLSS 4.5 on 20-30 series.",hardware,2026-01-07 11:24:58,2
Intel,ny6ob4j,"It would be good if that is true, but so far ive seen nothing that would inspire me confidence in AMD. And yes i remember the AMD patents you posted last year.",hardware,2026-01-07 11:55:47,2
Intel,ny7ebjk,"Yeah, it's DLSS4>FSR4>XESS (Intel)>=DLSS3>XESS (fallback)>FSR3      quality wise.",hardware,2026-01-07 14:32:39,1
Intel,nxzj5en,"First custom design OEM are fast, here 4400â‚¬ on Amazon https://amzn.eu/d/idxVW9M  And you know how this goes, one starts the other follow.  Here in the US for a normal founders edition for 4.2k USD + TAXâ€¦ one article from the first of January quoted ot that time being at 3.7, like 5 days ago.  https://www.newegg.com/nvidia-founder-edition-900-1g144-2530-000-geforce-rtx-5090-32gb-graphics-card-double-fans/p/1FT-0004-008V4?source=f",hardware,2026-01-06 10:51:32,-1
Intel,ny5eqws,"When I talk about ""design targets"", I'm not referring to an arbitrary TDP. There are very specific decisions each SoC made that have tradeoffs at different power envelopes.   Also, the context was LNL which is an entirely different beast from MTL.",hardware,2026-01-07 05:25:06,2
Intel,ny0ypvn,">The name's misleading for the tech difference.  Yeah, that's my point. People are reading too much into the ""B series"" naming scheme for B390.  As you said, ""(usually) use only for the dGPUs"". So if Xe3P is launching as a discrete Celestial Card, then it would make sense to have Xe3P tile be part of the ""Celestial"" launch, rather than Celestial Discrete being ""one year later than Celestial integrated""",hardware,2026-01-06 16:01:03,4
Intel,nxyf825,Oh dang you're right lmao.  The 4050 has SIX (6) FPS at 540p high. I thought OP was exaggerating with 800%.,hardware,2026-01-06 05:02:05,5
Intel,nxyahr2,Yeah those kinda suck. Should be Ultra 3s given they're basically WCL spec.,hardware,2026-01-06 04:30:27,2
Intel,ny2mxg2,"More like 10 years, 5 to realized that they are getting stomped in the face, and another 5 to actually make something of it.",hardware,2026-01-06 20:34:21,3
Intel,nxyak62,"Man, there are so many older and less demanding titles I'd love to play through on the go, but knowing that Lunar Lake laptops have better displays for the price is really good. Thanks for the info!",hardware,2026-01-06 04:30:54,4
Intel,ny6po5w,"If you're referring to the April dump, heck even the August dump (analysis of Kepler\_L2 patents) then that's not close to the complete picture. A lot of new patents have surfaced since that expand upon the design in many ways, but I'm waiting for the last RDNA5 to be made public before making a potential follow up post.  But regardless even if they fix HW situation completely they'll prob fail spectacularly with SW stack as they've done so far with FSR Redstone and FSR4 game adoption. Even hear a lot of people complaining about having to use Optiscaler, even in newer games.   Also NVIDIA will no doubt move the needle a lot nextgen yet again. They already did with DLSS 4.5 and DFG and something tells me that DLSS5 is gonna be even worse for AMD. They better prepare for what's to come.  Worst case it's a complete massacre. I can see the following scenario happening:  **HW:** NVIDIA invests all their silicon budget into fixing 5090 scaling bottlenecks (16 GPCs instead of 12, revamped scheduling etc...), fixes other problems with 50 series (redesign cachemem mostly) + goes Brr on ML and to some extent RT. Raster goes up 35-40%, everything else goes up multiple times.   Worst case ML HW gets bumped to 4-8X NVFP4 rate, although 2-4X sounds more likely.  **SW:** NVIDIA uses this new insane ML HW to make new DLSS models. DLSS5 goes all in on NVFP4 and is faster than DLSS4.5. DLSS5 SR and RR for 50 series + 60 series which is lightweight and fast on new GPUs (high FPS), and a new DLSS ULTRA SR for 60 series (released across stack but painfully slow for anything pre 60 series) striving for maximum Image quality. The smaller model will be better than DLSS4.5 and the big model another tier entirely (DLSS3 -> 4 leap easily on top of DLSS4.5).   They also make DRS compatible with DLSS SR and RR so users get greater flexibility here similar to DFG for framegen.   FG will also release in two versions one light and heavy. Will also work with Reflex 2. It's possible only the big model will be frame extrapolation + limited to 60 series. Should make FG result in lower ms instead of higher + overall image quality far superior and basically all issues solved up to at least 4X.   Oh and a flood of MLPs and a demo showcasing the absurd visuals the 6090 can push. Moves goalpost past ReSTIR PT and will look borderline offline render quality. Very close to Blender renderers. IDK how they'll do it but MLPs are borderline magic, so prob doable.  Thinking about it more you're prob right and even if RDNA5 HW is amazing even beats 6090 in PT, a DLSS5 feature suite this impressive + moving goalpost to MLP based neural rendering will make RDNA5 irrelevant. As always SW and marketing will kill any momentum from HW side. Really hope I'm wrong but don't think so.  Sorry for the rambling.",hardware,2026-01-07 12:05:46,2
Intel,nxzkmr7,"That's a marketplace listing, it's basically eBay, because Newegg is out of 5090FEs directly.  You can get it on the overpriced StockX for far cheaper: https://stockx.com/nvidia-geforce-rtx-5090-32gb-graphics-card-900-1g144-2530-000",hardware,2026-01-06 11:04:15,6
Intel,ny12fxe,"That would make some sense if they *did* plan a Celestial launch, but that's a big ""if"" and is just creating confusion for now. And it'll be even worse when NVL mixes Xe3 and Xe3p.Â    You also have Intel marketing actively making the situation worse like that Peterson interview people keep quoting to justify this nonsense. As if Xe3p isn't much more incremental than Xe3.Â    It's a particular shame when the product itself is actually good.",hardware,2026-01-06 16:18:11,1
Intel,nxyb20u,"Yeah, the first 6 core i/u5 series since 11th gen. :/",hardware,2026-01-06 04:34:08,-1
Intel,nxyoap3,"Yeah idk why but they typically got OLEDs exclusively, though could be a US market thing. I would also note I was mostly looking at decently built midrange to high-end laptops. I think AMD is more common in the plastic crap box design and may be a better value there, but those also typically seem to have a ton of older rebadged processors instead of the newer Kraken Point unless something changed.",hardware,2026-01-06 06:10:02,2
Intel,nycu2b3,"I enjoy reading your optimism. I hope it all comes true, but it sounds a bit too good to be true given the recent hardware developements. The 5090 scaling issue is that we stopped resolution scaling. If you go beyond 4k the 5090 scales a lot. VR resolutions report the 5090 being as much as twice the framerates of 4090.",hardware,2026-01-08 06:52:50,1
Intel,nydd2fq,I've rewritten prev reply to provide more info.  I'll also link the scheduling patent here in case anyone reading this thread is interested: [https://patents.google.com/patent/US12153957B2](https://patents.google.com/patent/US12153957B2)   It sounds like gains in workgraphs scenarios will be be even greater.,hardware,2026-01-08 09:42:10,2
Intel,nyd5bcu,"Yeah prob not realistic. I just tried to outline a nightmare scenario for AMD. As for the RDNA5 stuff we'll see how good it ends up being.  Agreed serious issues fs. RTX 5090 scheduling is brain dead. 16 SM GPCs, one central scheduler for 170 CUs. The smaller the internal res the harder it is to keep things going. Someone smarter than me could prob make a core scaling efficiency chart for different resolutions clearly showcasing how RT > raster and derive different formulas for 1080p, 1440p, 4K etc... . There's simply no reason why it has to be this bad moving forward.       But it's also interesting to entertain that RDNA5 could be a nightmare for NVIDIA. If NVIDIA doesnâ€™t fix scheduling AMD's nextgen could be a real nightmare scenario for them. The modular and decentralized scheduling will be a gamechanger and based on what patents have said scaling is almost perfect and can scale to [arbitrarily large configurations](https://patents.google.com/patent/US12153957B2), yes they used that wording. AT0 will function like 8 x AT4 instead of running into massive scaling issues. In fact based on what the patent has said it might be even better. Consider each scaling domain with a local cache independent of the L2, where the global command processor only acts as a distributor of work, not an orchestrator. Gains will be observed across the stack but expecting IPC gains to scale with number of CUs. Is this a big deal for RT and 4K native? Yeah but even more so for lower res gaming.    And assuming they reduce CPU overhead even further in new uarch AMD will easily take the max FPS crown although I suspect NVIDIA can finally address their driver overhead issue after booting Maxwell-Pascal. Weâ€™ll see who comes out on top in CPU overhead nextgen.  I thought most of that gain vs 4090 was due to extra BW? But yeah high end perf scaling falls apart at sub 4K internal res.",hardware,2026-01-08 08:30:40,1
Intel,nyjq3tx,"I dont think much can be done with overhead. AMDs overhead is already small, basically letting the API go directly to GPU as it is. While for Nvidia side, isnt most of the overhead related to how Nvidia handles DX12? in that case i dont see it going away for a long time.",hardware,2026-01-09 05:42:46,2
Intel,nyfqcxo,"Interesting, GN usually gets Tom to do discussions like these but instead decided to publish whatever that previous video was on 'Intel pulling an Nvidia'. I bet GN will probably have their own video with Tom, but I appreciate DF a little bit more with this discussion.  At around 21min, it's interesting to hear his talk on cross-vender SR, mentions how they'd like to work more on Nvidia's Streamline and a candid talk about DirectSR and how it isn't really the concrete solution for the work on cross vendor SR. At around 23min, Alex brought up something interesting about research they've published before on joint denoiser and SR. He kinda skirts around it, but continues on suggesting they have more plans on it. He also then continues on the state of PT, DXR 1.2, obviously it isn't a real focus with something on their iGPUs, but any future HW, will be their primary goal to tackle. Alex mentions Valve/Linux, and Tom says it isn't entirely their focus right now, at least for gaming.",hardware,2026-01-08 17:45:35,42
Intel,nyhyq7q,"Super interesting that he randomly announces that Intel will be dropping a pre built shader program for Panther Lake. And not build with the new Microsoft framework/infrastructure, but just on their own?? How can Intel randomly drop this, but nvidia and amd canâ€™t??",hardware,2026-01-08 23:46:46,13
Intel,nyhbk2i,"Seems like we're not the only ones that think FG isn't ideal rn. I really hope Intel succeeds in their efforts to pair Framegen with reprojection, but it'll prob be NVIDIA that gets there first. Might be the killer app for 60 series, but pure speculation of course.  The stuff about using AI to smoothe frames is interesting as well.  Things prob gonna change a lot in the coming years. We'll see if it's for the better.",hardware,2026-01-08 21:55:47,19
Intel,nyfhyad,Are PC games becoming more stuttery or we're just paying more attention to it?,hardware,2026-01-08 17:08:37,27
Intel,nyfezgd,This will probably piss off MLID since he hates Tom Peterson,hardware,2026-01-08 16:55:39,12
Intel,nyikqbo,This future of gaming is ridiculous.  Aggressive upscaling (360p) and one-in-four frames actually rendered and the rest FG?   For what?  Path tracing?  Nanite?,hardware,2026-01-09 01:41:36,-6
Intel,nyhaj0j,You still watch GN? Dude only farms drama after realizing how much clicks they generate.,hardware,2026-01-08 21:51:23,42
Intel,nyhmk4k,Thank you for the summary!,hardware,2026-01-08 22:45:36,3
Intel,nyjni69,"He's talked about it before I believe in a previous interview, might have been with PC World from memory or perhaps GN. Regardless, it wasn't exactly new iirc. [Anyways this is definitely old news.](https://overclock3d.net/news/software/intel-plans-to-make-shader-stutter-a-thing-of-the-past-with-arc/)",hardware,2026-01-09 05:24:04,10
Intel,nyja51l,"What do you mean? The Shader delivery program was launched on an AMD handheld, so I presume they will use the MS advanced shader delivery infra.",hardware,2026-01-09 03:59:46,7
Intel,nyimpss,"Intel already talked about this a couple months ago. It's not an announcement here, but it seems people are more interested in AMD and Nvidia news so those threads don't get as much traction.",hardware,2026-01-09 01:52:08,6
Intel,nyflgv9,I feel like more people paying more attention since the marketâ€™s grown a lot.   I remember old games I played in the early 2000s having micro and regular stuttering depending on the game. I chalked it up to â€œhuh guess itâ€™s loading in data as I playâ€ when I didnâ€™t know much.,hardware,2026-01-08 17:24:06,46
Intel,nyfmkrk,We are paying more attention to it but I suspect as we push higher frames with new engines and techniques the micro stutter is getting worse. It just starts becoming less perceptible to people than say the micro stutter from SLI and other stuff that caused issues in the past.,hardware,2026-01-08 17:28:57,21
Intel,nyfuxga,"I think digital foundry answered that question on their podcast. Compared to 15 years ago the frame rate is higher on average for most gamers, but it also stutters more. So it's huge fps with huge drops and hangs.",hardware,2026-01-08 18:05:15,21
Intel,nyfrkdx,"Worth remembering no one even gave a shit about stuttering enough to measure it until someone at I think it was anandtech back in the late 00s was so fed up with shitty perf on his crossfire system he started doing 1% lows on benchmarks.   Back in the 90s people would run like 6x SLI voodoos and not even care that the game hitched every 10 seconds down to sub 30s fps lol...  Edit: it's also key to remember that a lot of the games that stutter on PC these days stutter in the exact same places for the exact same reasons, and worse considering the lack of CPU power, on consoles. Console gamers just don't give a fuck lol....   DF did a good video showing this truth with the Silent Hill 2 remake.",hardware,2026-01-08 17:50:48,30
Intel,nyg5v26,"With a lot of PS3 games dipping to 20s, I guess we are just paying more attention to it now",hardware,2026-01-08 18:52:01,7
Intel,nyj9uk1,There was DF Clip of this exact question.  https://www.youtube.com/watch?v=pxsfT4c-F-Q,hardware,2026-01-09 03:58:01,4
Intel,nyfihv5,They're more surgery stuttery.,hardware,2026-01-08 17:11:01,2
Intel,nyfksy4,Everyone hates MLID,hardware,2026-01-08 17:21:10,42
Intel,nyfg44c,Funny piece of lore. Why so?,hardware,2026-01-08 17:00:30,15
Intel,nykk9lw,Who's MLID?,hardware,2026-01-09 10:03:38,3
Intel,nyjx60j,If the end visuals are better who cares. we already did a lot of such things in engine just didnt tell the players about it. One in four frames actually rendering shadows is a thing for example. Heck some games go as bad as once a second shadow updates.,hardware,2026-01-09 06:38:04,17
Intel,nykocr4,"Well, I think that most people (myself included) don't really care how it's done if the end result is looking good and feel good to play, off course something like 30fps based with MFG to 120 is bs, but I quite regularly use 60 -> 120 using frame gen because It feels okay input wise to me at 60, and the added visual smoothness is quite nice. So it all depends how it's implemented and talked about.",hardware,2026-01-09 10:40:00,10
Intel,nykneij,"yeah i had to tap out a couple months back. it's a shame, they did great work, but i refuse to support ragebait.",hardware,2026-01-09 10:31:33,17
Intel,nyhh357,"Doesn't help that there isn't any particular new hardware to review or anything new besides AI, the PC industry has hit stagnation in the consumer market.",hardware,2026-01-08 22:20:10,26
Intel,nykpeuy,"No it didnâ€™t sound like they were going to use the MS shader delivery program. Or no, they said they want to, but they have their own solution that theyâ€™ll launch before that.   And I also donâ€™t think the MS solution is ready. The ROG ALLY XBOX also doesnâ€™t have this feature already as far as I know",hardware,2026-01-09 10:49:11,2
Intel,nykoz3l,"> The Shader delivery program was launched on an AMD handheld  i assume you're referring to the steamdeck, which to my understanding was done by valve... so the point kinda stands, you just add valve to the preamble.",hardware,2026-01-09 10:45:26,2
Intel,nyg0pvl,"I dunno I remember many games, especially based on Quake engine, being buttery smooth if you could get to reasonable FPS",hardware,2026-01-08 18:30:02,19
Intel,nyjwwd6,also more cause for stutering. Back then we could compile shaders real time with no siginficant issues because they were small. Now we have to compile shaders real time that are huge to the point where we pre-compile half of them before we even start the game.,hardware,2026-01-09 06:35:52,6
Intel,nyfy60k,Console gamers *of certain genres* care.   The FGC rejected the idea of playing Street Fighter IV & MvC3  on PS3 because the Xbox 360 port had better input latency.   Even the PS4 port of USFIV wasn't liked.     Part of why I stopped going to tournaments is because SFV & Tekken 7 on PS4 always felt *off* compared to PC.,hardware,2026-01-08 18:19:10,12
Intel,nyg711t,"Not really. In many cases, console versions of games just don't stutter whereas PC games do because modern games are mainly designed for consoles and then ported to PC.  A couple of good recent examples are Wukong and Outer Words 2. Neither of those games on consoles have the horrid stutters that are prevalent on PC.",hardware,2026-01-08 18:56:57,11
Intel,nymcty0,MLID more like MID,hardware,2026-01-09 16:26:52,2
Intel,nyfkp1n,Heâ€™s been calling Tom a â€œsnake oil salesmanâ€ for performance claims on Alchemist,hardware,2026-01-08 17:20:43,14
Intel,nzdp1mq,"Moore's Law is dead, he's a YouTuber who makes predictions and purports to have insider information from Nvidia/Intel/AMD but in reality he is better described as FanFiction for PC enthusiasts.",hardware,2026-01-13 16:50:02,1
Intel,nym0alm,"I worry about things like latency.  Or what happens when you move suddenly, or fire, and the whole image falls apart.  If the hardware can't do it all yet, then wait a few years instead of using these... methods.  And just to be clear, I think very highly of  Mr. Tom 'TAP' Petersen; I'm just not liking where this is all heading.  Are you happy with Borderlands 4 or Outer Worlds 2?  If you are then we live in two different worlds.",hardware,2026-01-09 15:30:18,-2
Intel,nyjn1ql,"There's plenty of old stuff he needs to review. I've said it before but I will say it again and beat it like a drum: He STILL has not reviewed the 9060 XT 8GB for instance. Plenty of content he could have farmed off that, especially BEFORE the RAM shortage where having an 8GB card was some sort of sin in his eyes (except for some reason he ignored it but raile roaded the cheaper RTX 5050, lol did someone say bias?). Instead, he just tore the 9060 XT 8GB down and never touched it again after that teardown. I'm sorry but there's GENUINELY stuff he could be doing instead of farming clicks about 'NVIDIA bad' and 'Intel pathetic', but that won't get him views from drama farming NVIDIA which is what he craves these days. So sad to see a big channel like his who got big off doing solid technical content become a drama-hype ""news"" channel.",hardware,2026-01-09 05:20:56,24
Intel,nyk71tg,No wonder that strategy has proved successful. There are a lot of people that only care about complaining and bitching. Which I get up to a point but it starts to feel childish and pathetic quickly.,hardware,2026-01-09 08:02:47,20
Intel,nysczdh,"I love how reddit down votes you,  reddit posts all day complaining about the industry, GN does a video on it   Redditors ""whiney cry baby engagement farmers """,hardware,2026-01-10 14:05:38,-2
Intel,nyogfa3,"Valve does have one on Steam, but Microsoft announced a store agnostic, eventually hardware vendor agnostic one launched with the Asus ROG Xbox Ally X last year.  It was supposed to be working already but there's no sign of it just like everything else Microsoft releases about gaming half-baked like their gaming UI, their attempt to unify upscaling, and Directstorage.   So I imagine they're referring to what Microsoft called ""Advanced Shader Delivery"" that they've done little with but name and announce to sell more Asus Pretend-Xbox's.",hardware,2026-01-09 22:10:03,3
Intel,nyoi6ny,Iâ€™m talking about the Xbox Ally as the other commented said,hardware,2026-01-09 22:18:25,1
Intel,nygmjs6,We considered solid 60 or 75 smooth back then. Now at least I complain as soon as I can't stay above 100.,hardware,2026-01-08 20:05:12,15
Intel,nyhs8ie,"Shader compilation stutter is a PC problem thatâ€™s been especially bad in UE4 and 5 since the transition to DX12. Other types of stutter and bad frame rates used to be equally bad on consoles, or even worse in the 360/PS3 era.",hardware,2026-01-08 23:13:28,11
Intel,nygf3pi,Outer Worlds 2 is fairly consistent for a UE5 game. To me its just very heavy,hardware,2026-01-08 19:32:17,5
Intel,nygrcy6,"funny coming from MLID, notorious snake oil salesman",hardware,2026-01-08 20:26:46,47
Intel,nyflbj9,"Eeeegh, considering MLID overall story - can see that, can see that.  OTOH usage of such terminology - snake oil salesman - reminds me of that very infamous Intel presentation...",hardware,2026-01-08 17:23:27,14
Intel,nymda4b,You say that as if we will reduce our base frame rate. But the direction of travel is to use MFG to drive the 480Hz and above monitors that will become more and more common in the future.,hardware,2026-01-09 16:28:52,6
Intel,nyqwqj9,I dont like borderlands franchise and i havent played Outer Worlds 2 yet so i cannot comment on them from personal experience. However things like Reflex/Reflex2 has actually decreased latency for me.,hardware,2026-01-10 06:52:45,1
Intel,nylgvk7,"I havent really seen it as some sort of drama farm, I think a lot of GN's videos do show how bad the current computer hardware hobby is doing and Im glad someone is focusing on that",hardware,2026-01-09 13:55:07,1
Intel,nysckbg,Whata wrong with calling companies out? Like redditors cry all day but stop at GN when the channel brings up issues within an industry.  Sorry a person isn't running their channel the way the reddit collective wants.,hardware,2026-01-10 14:03:14,-2
Intel,nysv24y,"honestly, i had completely forgotten that was a thing.   i wish i had more faith in microsoft to pull this off, but i remain skeptical in basically anything they try until proven otherwise.",hardware,2026-01-10 15:44:02,1
Intel,nyguz4e,"People on gaming forums used to go around insisting the human eye could not distinguish greater than 60fps.  It was accepted as gospel in many places/by many people.  Granted, that was in the age of CRTs and almost no one had actually seen >60, but funny to think back on.",hardware,2026-01-08 20:43:03,12
Intel,nyjx1ak,snake oil salesmen dont like competition.,hardware,2026-01-09 06:37:00,9
Intel,nyjz26x,"MLID aka ""RTX 50 super will be released november 2025""",hardware,2026-01-09 06:53:37,5
Intel,nyknw8m,this just makes me like tom more,hardware,2026-01-09 10:35:57,7
Intel,nyjwmn8,"Replace that 60 with 30. Yes, people insisted we could not see more than 30 even while playing on 85hz CRTs.",hardware,2026-01-09 06:33:40,8
Intel,nyhcljr,"Meanwhile quake players kept dropping resolution to hit triple digit refresh rates :) But yeah, it's wild that 30 was the norm for so long on console",hardware,2026-01-08 22:00:09,5
Intel,nyko1gh,That's far from the worst prediction he's missed.  Remember SMT4 for future Zen architectures? 24 and 32 cores on desktop Zen4? L4 cache for Zen 4?,hardware,2026-01-09 10:37:14,13
Intel,nyrvn0z,Which Tom?,hardware,2026-01-10 12:08:40,1
Intel,nyjbd32,"console used to lock at 60, dont know why they choose to drop down to 30.",hardware,2026-01-09 04:06:56,4
Intel,nyjivip,"Because when the hardware cant handle all the sprites, it slows everything down including game logic which was tied to frame rate back then. It happens in plenty of hardware from snes to arcade cabinet.",hardware,2026-01-09 04:52:53,9
Intel,nyjwrq4,"also happened the other way round, older games would run faster than they should because they tied it to framerate rather than delta time. This bad practice was so common you can still find studios like bethesda do this.",hardware,2026-01-09 06:34:49,6
Intel,nz6q876,"for context, this G14 was the one that gave us [the first ever Geekbench scores for Panther Lake](https://www.reddit.com/r/hardware/comments/1ocaslx/panther_lake_geekbench_leak_its_good/) over on r/hardware",hardware,2026-01-12 16:33:10,35
Intel,nz6w6zw,"The price probably wasn't going to be low enough compared with other SKUs.  In the past, Intel skuss with Iris HD etc were so expensive and were only included in $2000 ultrabooks. This is probably going to be same.",hardware,2026-01-12 17:00:15,18
Intel,nz73dw5,When I first learned about I was confused why it existed. Not surprised Asus decided to cancel it.,hardware,2026-01-12 17:33:23,10
Intel,nz6r3wa,"It makes sense, a B390 only G14 kind of defeats the whole purpose of the Zephyrus, even as a base model.     Basically brings the GPU performance down to 3060 levels (actually a little worse than that), which is roughly 4 years ago at this point.",hardware,2026-01-12 16:37:10,37
Intel,nz6q33b,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-12 16:32:31,1
Intel,nzujefe,"Ugh this would have been such a cool business laptop option, I'm finding current-gen integrated graphics to not be enough to really handle Teams + external monitor + a presentation. Sounds like this would have hit the performance mark without having to pay through the nose.",hardware,2026-01-16 02:30:35,1
Intel,nz770nj,Itâ€™s also the one that didnâ€™t have a dedicated GPU in Geekbench,hardware,2026-01-12 17:49:53,14
Intel,nzcyo9s,"I don't think so, they're making it available on more than just the top 9 series, they also confirmed a custom chip for gaming handhelds which we'll prolly see at computex",hardware,2026-01-13 14:45:59,4
Intel,nz7invn,It probably made more sense as an low-end gaming Zephyrus when fitting it with 32GB LPDDR5x wasn't half the BOM,hardware,2026-01-12 18:42:14,19
Intel,nz7xzli,esp since on CES 2026 they launched an even more premium 14 inch laptop that actually does have the B390 (ExpertBook Ultra),hardware,2026-01-12 19:52:01,4
Intel,nz6vvy1,esp since Asus ended up launching the ExpertBook Ultra at CES  an even more premium 14 inch laptop compared to the G14 which does have a B390,hardware,2026-01-12 16:58:51,11
Intel,nz6ryqs,"But it actually can be used on battery unlike normal laptop gpus which make your laptop die in 5 seconds or performance is awful and it dies in 10 seconds, so what's the point of having a powerful GPU on a laptop if I got to plug in all the time it sucks.",hardware,2026-01-12 16:41:01,30
Intel,nz6tste,"A 3060 is still useable imo. If the efficiency is right, they can make usb-c powered gaming laptop and completely remove the dumb dc brick.",hardware,2026-01-12 16:49:18,16
Intel,nzcpz2s,"It would have made _some_ sense, but the Arc B390 mustâ€™ve been real good. But as you say, with the raise in RAM prices, you can forget about it.  I wonder whether Intel themselves would pair a mid-range Panther Lake with a top-notch iGPUâ€¦ It would make sense, since the high-end CPUs tend to have dGPUs.",hardware,2026-01-13 14:00:36,1
Intel,nz6tn0v,"For the Zephyrus you can also just disable the dGPU to game only on the iGPU, and the customers of the Zephyrus are minimum looking for a decently powerful dGPU",hardware,2026-01-12 16:48:35,19
Intel,nz6snj4,I hope their plugged out power scheme is permanent and doesn't vary based on application,hardware,2026-01-12 16:44:08,3
Intel,nzaau06,"> so what's the point of having a powerful GPU on a laptop if I got to plug in all the time  I have a laptop for travelling. I travel to places that have plugs, and I don't need to use a laptop while I am actually in transit.",hardware,2026-01-13 03:03:12,1
Intel,nz8xpz8,"It still pulls 60W (and apparently 80W, briefly) at full pelt...  That is to say an hour and a half battery life when gaming should be expected. It's not magic.",hardware,2026-01-12 22:40:05,0
Intel,nz71x4x,Usb c powered gaming laptops are more common now. The new ideapad 5 pro with panther lake and 5060 (combined 110w system power) uses usb c exclusively,hardware,2026-01-12 17:26:41,12
Intel,nz753k0,"> A 3060 is still useable   Nvidia seems to agree with you, which is why they are looking to put them back into production.  /I'm sorry, I couldn't resist.",hardware,2026-01-12 17:41:14,11
Intel,nz6wes7,"Useable, yes, but what customers of the Zephyrus are looking for, no.",hardware,2026-01-12 17:01:16,11
Intel,nz6vzj4,"Yeah, and it gets pretty good battery life on that and in power saving mode/60fps screen mode. Like 8 hours. Best I've had on a gaming laptop- not a high bar, but it's nice to be able to take it into the living room and use it light a normal laptop instead of it dying instantly.",hardware,2026-01-12 16:59:19,7
Intel,nz920o1,Still better than 45 minutes  Also it can obviously be put in a lower power mode to save battery while not hurting performance too drastically.,hardware,2026-01-12 23:01:47,2
Intel,nz6x4bl,"The G14 used to have gtx1650, so not sure what do you mean.",hardware,2026-01-12 17:04:35,-1
Intel,nz724pz,Which was the 5050 of that time which is significantly more powerful than panther lake's igpu.,hardware,2026-01-12 17:27:38,10
Intel,ntyjuf7,Why does this need an article? It's a tweet by an official account praising their own product.,hardware,2025-12-14 10:28:41,114
Intel,ntynem1,"The B580 has 200W TDP, in a perfect world and TDP scales linearly, the B770 would be 50% faster, that would put it around the 5060Ti/9060XT.  If the price also scales linearly, that would be around 375â‚¬, seeing that the 9060XT is going for 350â‚¬ now, it's gonna be tough competition.",hardware,2025-12-14 11:02:45,43
Intel,ntypvez,Im really looking forward to panther lake X. 4-4-4 core configuration and Xe3 iGPU with sr-iov is perfect for running a Linux-Windows mixed vm environment without having to get a gaming laptop with a dedicated GPU for virtualisation.,hardware,2025-12-14 11:26:24,7
Intel,ntyixk6,I hope the Linux driver support and performance is good in these,hardware,2025-12-14 10:19:48,21
Intel,nu8l0vn,"Intel ARC needs to maintain their momentum. They have an excellent pricing strategy and genuinely compelling features, it's time they released a card that competes in the midrange. And no, I don't count the A770. As a B580 owner, increased ARC adoption rates will be sure to benefit all cards in the range, so I really hope that intel is committed for the long-haul here. They are not in the position to be burning consumers anymore",hardware,2025-12-15 22:51:09,4
Intel,nu1xwz6,"Releasing a GPU more than 1 year after the B580 came out seems weird to me. Unless this is a new architecture, or is using Intel's own process, and fabs.",hardware,2025-12-14 22:08:28,7
Intel,ntyjaqa,"4070 performance for $350-400, I'm calling it now.",hardware,2025-12-14 10:23:22,11
Intel,ntyxo3a,Hopefully they've seen Nvidia and AMD fuck things up by having two VRAM configurations and know not to do that.,hardware,2025-12-14 12:35:17,7
Intel,ntyfbh6,"Hello Revolutionary_Pain56! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-14 09:44:11,1
Intel,nu28x8z,They wouldnâ€™t need a B770 or mystery GPU if they actually released more than just a B50 to the masses.,hardware,2025-12-14 23:06:54,1
Intel,ntzwyfc,"I don't know what the driver situation is like a year later, but B580 was anywhere between a 4060ti and a 3060 (or less if the driver really choked), so comparing B770 to a single Nvidia point of reference probably isn't the whole story.  Intel has been selling a big chip with a lot of hardware relative to what they charge, so when the drivers work Battlemage can punch way above its price class. I expect the same this time.",hardware,2025-12-14 16:10:14,1
Intel,nu3z9nt,"It's been deleted, so it might even be inaccurate.",hardware,2025-12-15 05:31:41,9
Intel,nu1v2ky,Ad revenue.,hardware,2025-12-14 21:53:55,6
Intel,ntzk2x5,Trying to apply logic or rules to the internet is a waste of time.,hardware,2025-12-14 15:02:28,16
Intel,ntyxt68,"Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds. That however will mean a bigger die and viability might be questionable (considering they're already massive for the performance).",hardware,2025-12-14 12:36:26,29
Intel,ntzjh8i,"Price doesn't scale linearly because die sizes make defects scale quadratically. so pricing is the same, 2 50mm\^2 dies are cheaper than 1 100mm\^2 die     However in GPUs there is a fixed cost for every GPU so there is a sweet spot",hardware,2025-12-14 14:59:01,13
Intel,ntz8bta,"As always, TDP is a semi-arbitrary figure and has little to do with what the GPU requires.  Most GPU's of today have heavily inflated TDP's simply to try and juice benchmarks on review day as much as possible.",hardware,2025-12-14 13:51:23,4
Intel,nu40836,"The BMG-G31 is supposed to have 32 Xe cores in 8 render slices on a 256-bit memory bus, compared to the 20 Xe cores and 5 render slices on a 192-bit memory bus for the BMG-G21. Unless Battlemage is seriously memory bandwidth-limited, it should be almost 50% more performant.  The only question is die size. If it's 50% larger than the 270 mm^2 BMG-G21, that would exceed 400 mm^2. The GB203 in the RTX 5080 is 378 mm^2 for context.",hardware,2025-12-15 05:39:14,2
Intel,ntywdzf,With tdp of 300 w it better be RTx 5070 or 9070 territory for much low price,hardware,2025-12-14 12:24:39,-1
Intel,nvdrawd,Intel never confirmed SR-IOV on Panther Lake - did they?,hardware,2025-12-22 15:23:42,1
Intel,nu14qjw,"You can choose between high performance and crashes (xe) or low performance and stable (i915), and with Intel firing linux devs left and right I wouldn't expect much improvement any time soon.",hardware,2025-12-14 19:45:02,11
Intel,ntz5xy4,That would be an amazing value proposition.,hardware,2025-12-14 13:35:49,3
Intel,ntypyrq,Rtx 5070 16gb for 380$,hardware,2025-12-14 11:27:18,-1
Intel,nu2970r,Iâ€™d be happy if they didnâ€™t gate the Arc Pro B60 behind bad distributors.,hardware,2025-12-14 23:08:26,3
Intel,ntz0y7e,"So banking on the hope, that *everyone* ***else*** *somehow falls behind by accident*, only for Intel to succeed?  If that's their business-plan (looking at their foundry-woes, it seems it is), that's an awfully idiotic business-model.  ---- Last thing I heard, was redditors moaning about en masse that monopolies are bad. *Which one is it?!*",hardware,2025-12-14 13:00:48,-21
Intel,nu8khh7,"The B50 is not a gaming GPU and actually underperforms in gaming tasks compared the the B580. They need to have an actual range of cards, not just a budget option, and even more budget option, and a server/workstation GPU. The B770 is essential to compete in the midrange",hardware,2025-12-15 22:48:12,1
Intel,nu8kn1x,"A year later the drivers are fantastic, seriously not even a single hiccup. Been playing Hogwarts legacy at 4k 60fps with Xess Quality upscaling, and no frame gen.",hardware,2025-12-15 22:49:02,1
Intel,ntyyf0i,"> Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds.   The problem with this idea, is that this would cost them far more money, as you need more die space, which they already use relatively inefficiently compared to nVidia.  They can't really afford not to use every bit of die space they have for all that its worth.",hardware,2025-12-14 12:41:19,20
Intel,nu40pti,"Battlemage doesn't have the ability to add more Xe cores per render slice, this is something Intel has changed for Xe3. The BMG-G31 will have 128 ROPs, the same as an RX 9070 XT, or more than an RTX 5080.",hardware,2025-12-15 05:43:07,2
Intel,nvdwqwo,"afaik it works on every iGPU since skylake, but the driver is not in the mainline kernel",hardware,2025-12-22 15:51:22,1
Intel,nu2hk6w,I'm using an Arc A770 right now in Linux.  With i915 performance was unusably (for me) low.  With xe it's been fine.,hardware,2025-12-14 23:54:28,6
Intel,nu2tqb3,"the driver is already open source right? i think it will get better over time on virtue of being open source, but relying on intel to fix it now probably isnt gonna pan out.",hardware,2025-12-15 01:01:50,0
Intel,ntz7z9x,"Well it kinda has to be, the 4070 came out nearly three years ago.",hardware,2025-12-14 13:49:08,24
Intel,nu096mb,5060 performance for twice the price isn't a good deal.,hardware,2025-12-14 17:11:40,-6
Intel,ntz10sx,"I could see that. Nvidia really bailed out Intel by making the 5070 not much faster than the 4070 without using MFG to cheat lol   Edit: for all the Nvidiots downvoting, [the truth hurts](https://www.techspot.com/review/2960-nvidia-geforce-rtx-5070/#RT-1440p-png)",hardware,2025-12-14 13:01:20,5
Intel,ntz7opy,This seems an absurd overreaction. All I'm saying is they don't do a 5060ti or 9060xt situation where there's a 8 gig model and a 16 gig model.,hardware,2025-12-14 13:47:16,18
Intel,nubvjrs,"Itâ€™s not but the B50 is the only Arc Pro that isnâ€™t gated behind a bad vendor like Hydratech.    If they canâ€™t properly launch the B60, why should I trust Intel or itâ€™s partners with the B770 or some mystery GPU?",hardware,2025-12-16 13:27:34,1
Intel,nu1491m,5060 is not nearly as performant as the 4070,hardware,2025-12-14 19:42:34,18
Intel,o52s2yg,"I agree that it looks like a software problem. Just to be safe I would still try running benchmarks for the cpu and gpu seperately and compare them to other peoples scores to confirm where the problem is. I don't have experience with trouble shooting an intel gpu but if you have a freesync monitor my best guess would be that its a freesync problem. Take a look around in your monitors settings menu as well, there might be something in there causing it. The only other fix i can think of is formatting the drive and reinstalling windows to completely rule out any software or settings.",buildapc,2026-02-13 00:06:06,3
Intel,o52wgbu,"You have checked everything on the PC side of things, but have you checked the settings on your monitor? Some monitors will have different modes and will sometimes limit FPS in some of them.",buildapc,2026-02-13 00:31:18,2
Intel,o52dkyk,what resolution are you using? what kind of cable are you using?     if HDMI and running 4k it might not be a 2.1 cable and that could be limiting you,buildapc,2026-02-12 22:45:07,2
Intel,o538csp,Check your PCIE lane speed in BIOS,buildapc,2026-02-13 01:42:40,1
Intel,o52d0bw,"yeah I know a couple of news outlets did a peice on this, pretty much ur CPU is limiting ur GPU via CPU overhead. not a short topic but essentially ur CPU is a ok match with the a770 but Intel GPUs really want a CPU that can push it",buildapc,2026-02-12 22:42:05,0
Intel,o54ip8g,"Yes, like 3D benchmark or correct OCCT? And I'll tell you more, I've disabled every freesync setting directly from the Intel arc control app for every application. And anyway, as a monitor, I have an excellent 180Hz Asus ROG.",buildapc,2026-02-13 07:10:15,1
Intel,o54jubv,"I have an ASUS ROG monitor, connected with its original cable, previously set without VRR obviously and set to 180Hz, and I directly deactivated any type of freesync from Intel arc control.",buildapc,2026-02-13 07:20:33,1
Intel,o52dt0w,"I play in 1080P with the displayport cable that came in the box with my ASUS ROG monitor, so I think it's good quality",buildapc,2026-02-12 22:46:19,2
Intel,o52dhno,"Strange, I instead encountered, at least on The Finals, a GPU bottleneck, where I saw the CPU at around 60/70% and the GPU constantly at 100%. And keep in mind that The Finals is a CPU-intensive game.",buildapc,2026-02-12 22:44:37,1
Intel,o52fqvr,"Furthermore, I can't upgrade to AM5 for obvious reasons, so the best I could get on AM4 would be a Ryzen 7 5800x, but I wouldn't be able to get an absurd performance increase.",buildapc,2026-02-12 22:56:42,1
Intel,o54pief,Yes but you need to check the settings on the monitor itself using the buttons on the monitor,buildapc,2026-02-13 08:12:47,1
Intel,o52fdtl,"yeah its a weird thing that they havent fully resolved, its like an artifical CPU limit in a sense. just the way the drivers interact with the CPU I guess",buildapc,2026-02-12 22:54:46,0
Intel,o52fu2s,"Exactly, I really don't understand.",buildapc,2026-02-12 22:57:11,1
Intel,o52i4nn,If your GPU is at 99%+ it is not a CPU bottleneck. It is not worth upgrading your cpu for an A770.,buildapc,2026-02-12 23:09:40,1
Intel,o52iwp7,"Yes, I know very well, the bottleneck is clearly the GPU and in any case on AM4 there wouldn't be an upgrade that would give me particularly better performance.",buildapc,2026-02-12 23:14:01,1
Intel,o7e9cp3,"Anything somewhat modern. I upgraded to 6750xt from 970, and that was about a 3x improvement. 3060ti is comparable, but has 8gb of VRAM so maybe some ue5 games could be grumpy about that, but for 1080p high it will hold up.",buildapc,2026-02-25 20:57:24,3
Intel,o7e5xdm,"With that, perhaps a used 2070 or 5700XT, or similar. Any more and I'd want a new platform to go with it.",buildapc,2026-02-25 20:41:29,1
Intel,o7ebnjc,"Talking strictly about Escape from tarkov, its very cpu intensive and just a gpu upgrade will not give you 60fps. I have a 9600x paired with 7900gre and i was dipping below 100fps in 1080p medium setting",buildapc,2026-02-25 21:08:05,1
Intel,o7fextm,I would look at the RTX 2060S or 2070S. The cpu bottleneck shouldn't be that bad with either of those. I had a 4790k paired with a 2070S for years and it ran great at 1440p.,buildapc,2026-02-26 00:27:54,1
Intel,o7eeowe,"Never had any experience with AMD cards actually, is there any big difference in general?   Thanks for the answer :)",buildapc,2026-02-25 21:22:05,1
Intel,o7edpzl,"I had a 1070 before and shit was smooth as butter, but had to sell it due to economic issues!",buildapc,2026-02-25 21:17:38,2
Intel,o7ees2g,"Thanks for the answer! What do you mean with ""a new platform""? :)",buildapc,2026-02-25 21:22:30,1
Intel,o7ef36w,"We don't have to dig so deep into the games listed, but ""games like these"" maybe is a better definition :) I get that only upgrading the GPU won't save it fully, but it's my first step atleast! :)   Thanks for the answer!",buildapc,2026-02-25 21:23:54,1
Intel,o7egnzm,"From GTX cards? None at all. From rtx the upscaling on pre 9000 cards is subpar, but not everyone uses it so depends on the person. I wouldn't count on rt on any older and cheaper GPU.",buildapc,2026-02-25 21:31:17,1
Intel,o7egqc7,New CPU/Motherboard and probably memory.,buildapc,2026-02-25 21:31:35,1
Intel,o7ei3wj,"Just hopping odd what he said, but yea Tarkov is.... a bit of a mess in comparison to throw other games youve listed. For example I bought a fairly cheap prebult from microcenter. Was the rx 7600 build. Arc raiders is flying at 120fps+ while im begging the targods for steady 60 fps over there haha. So when you do upgrade the gpu, just dont be alarmed if tarkov still runs like shit",buildapc,2026-02-25 21:37:57,1
Intel,o78uzfg,You can change out the hard drive for an ssd for certain. Otherwise this system is a bit old and impractical to upgrade any other part. A newer graphics card would be held back quite a lot by the old CPU.,buildapc,2026-02-25 01:17:46,9
Intel,o79khi7,"If you really want to stretch out your system, you could probably find an i7-6700 cheap. Also there also some inexpensive gpus on the used market that would provide a nice boost over the 950: gtx 980, 1060 6GB, 1070, 1650, 1660, and RX 480/580. There are other cards, but these stand a better chance to play nice with your PSU. Also upgrade to 16GB of memory. If you are running one 8GB, try to find a second that closely matches the one you already have.",buildapc,2026-02-25 03:43:36,3
Intel,o79nqge,Have you looked at the new Mini PC's ?   You get a lot for your money   [Mini PCs of 2026 ](https://www.youtube.com/results?search_query=mini%20gaming%20pc),buildapc,2026-02-25 04:03:43,1
Intel,o79o1yl,"Bump the ram up to 16GB, change out the storage to a SSD and bump the GPU up to like a 2060.  It should feel a lot faster.",buildapc,2026-02-25 04:05:45,1
Intel,o79tqdp,Buy a used GPU. People swap them out for the latest generation all the time even though it's still good. This upgrade I just did is the first time I ever bought a new GPU. My old RX5700xt served me well. I was just wanting to go 1440p,buildapc,2026-02-25 04:43:42,1
Intel,o79xuo7,"If you plan on gaming, build the PC around the GPU.  GPU's are the most expensive part of a gamers' system.  If you want to enjoy video games, you need a great video card.  Motherboard, CPU, and RAM are best as combo deals.  AMD wins the budget.  SSD's are nice, fast and quiet... but you get less storage space for the price.  A power supply can usually be found cheap, just make sure it's 650w or higher, and is enough for the GPU.  Stick with well known brands if possible.  A good monitor is also important.  Many to choose from, and you likely already have one.  The choice of PC case is the most personal aspect.  Hopefully you've got a modern one with good airflow.  Don't buy a cheap keyboard or mouse... they will affect your input performance.  Always get a backlit keyboard.",buildapc,2026-02-25 05:12:52,1
Intel,o7905sq,Agreed. Thereâ€™s not really a worthwhile upgrade path here. Moving from an HDD to an SSD is going to feel like a whole new system though. Itâ€™s one of the most impactful upgrades you can make to a computer.,buildapc,2026-02-25 01:47:33,2
Intel,o7939ze,"Ah, thatâ€™s too bad. I didnâ€™t have too much hope since the old gal is well over a decade old now, but I figured itâ€™d be worth it to check. Guess Iâ€™ll be saving up for a bit and sending her out to pasture ðŸ˜­",buildapc,2026-02-25 02:05:27,1
Intel,o795n6n,Well you could get a used graphics card like a 1060 for around $70-$80 but I just don't think that's a good use of your money.,buildapc,2026-02-25 02:18:53,1
Intel,o5m4qzt,"I was in a similar situation and I went b580 since the price difference was just too high, I have no problem with the card and I play everything I want in 1440p. It really depends, if you have to save money for a bit then it becomes complicated since there's no guarantee prices won't hike up again, but if you can afford it now then go 9060xt, it will last you longer even though it's more expensive. Xess 3 however made me realize this card will last me a good while with no issue.",buildapc,2026-02-16 02:15:50,4
Intel,o5lgtec,"I bouht a 9060 XT last month because of the Nvidia production shortage that's coming this year. I think the B580 is the play here at these price points still a good GPU regardless, it's not like you won't be able to game on it.",buildapc,2026-02-15 23:48:25,3
Intel,o5lhowj,"The 9060 XT 16gb is like 30% faster than the B580, AMD's drivers are better than Intel's.      9060 XT 16gb is a much better GPU and is definitely worth paying more for. 2 months ago the 9060 XT was MSRP. If it was MSRP right now, would you buy it? If yes, you should just pay more, if you can afford it. You already know what you want, buy it.    I'm not telling you to eat plain rice for a month to get the 9060 XT, but Intel GPUs are the ultra budget option. It's not really a like-for-like comparison imo.",buildapc,2026-02-15 23:53:35,3
Intel,o5loh1z,9060XT 16 Gb.,buildapc,2026-02-16 00:33:28,3
Intel,o5lgniq,"Having personally tested both of these GPUs I must say that 9060 XT is far superior and you should definitely go for that one if you can.   Btw it was 450 euro here in Serbia three months ago when my brother bought it, so maybe you saw an 8 GB variant which costs that much.  If you're planning to use the gpu for several years, definitely go for the 9060 XT.",buildapc,2026-02-15 23:47:26,4
Intel,o618sre,"CS2, Valorant, and R6 are CPU limited games, so they'd perform about the same, the big performance difference would be apparent in Cyberpunk @ 1440p.   You do still occasionally get deals for them, last month there was one for the 9060 XT 16 GB for 366â‚¬ but there's none right now. I'd say try to get a deal somehow.       Otherwise Arc B580's have a resale value of like 230â‚¬, especially if you keep the packaging so that's also an option.",buildapc,2026-02-18 11:14:25,2
Intel,o5lso7q,I got the 9060xt 16gb a couple of months ago for 340â‚¬.,buildapc,2026-02-16 00:58:50,4
Intel,o5ngc7b,Lucky you :) that's an amazing price!,buildapc,2026-02-16 08:24:52,1
Intel,o6j8xxn,"Strongly recommend saving up a bit more and going RX 9060 XT 16GB, or RX 6900 XT 16GB is also fine.",buildapc,2026-02-21 02:12:42,6
Intel,o6jgsd5,"I run an RTX 3080 10GB and play at 3440x1440.  It can handle most games well enough at high settings, so I would go for that. For 1080p, you wouldn't have to upgrade really ever in the future until the GPU itself starts kicking the bucket.",buildapc,2026-02-21 03:02:17,5
Intel,o6j97ly,"3080 is significantly faster than a 4060 and has 2GB more in VRAM if youâ€™re talking about the 10GB model. Itâ€™s honestly a great deal at that cost and would out perform for 1080p and even allow you to got to a 1440p probably.   The 6900XT is probably more on par with the 3080, but more VRAM on that model and more expensive. The 6800XT is probably more expensive as well with more VRAM (might be slower than the 3080). At 1080p, these 8GB models are probably fine for you.   I think the 3080 is ideal for the performance/cost. All of your options are fine for 1080p though in general.",buildapc,2026-02-21 02:14:22,2
Intel,o6j9cdz,Your 2060s canâ€™t play old games at 1080p? What games are you looking to play? I havenâ€™t hit a game yet I canâ€™t play at 1080/60 with a very similar rig (5500/2070),buildapc,2026-02-21 02:15:12,2
Intel,o6js7tr,"You want AMD for lower CPU overhead compared to Nvidia and especially Intel. 9060XT 16GB is the best option, below that the 8GB variant is still okay, but 8GB can limit you in a lot of new games. Otherwise you can go RX 6000/7000, however RDNA4 improves on many aspects like RT, upscaling, encoding etc.",buildapc,2026-02-21 04:20:00,2
Intel,o6k5r0u,I'd go for the 3080. I had the 3070ti and it was the perfect 1080p card for max/ultra settings on any game.,buildapc,2026-02-21 06:05:47,2
Intel,o6ktv0n,Get the RTX 3080 I own a RTX 3070 Ti,buildapc,2026-02-21 09:56:26,1
Intel,o6j9is0,"Id say 5060s, but really aim for 5070 imo. You could save little bit more and get a better gpu that'll be worthwhile.  If youre committed, save whatever money you have, take your time, and get a fresh desktop. That way, when you do get a NASA computer, you dont have to worry for years and enjoy with all your hearts content.",buildapc,2026-02-21 02:16:19,1
Intel,o6j8rkm,"3080 would be my pick, as it is the most powerful and still has access to the latest DLSS version  I use mine for 1440p still, although I'm not one to play the latest AAA games or use the highest settings. So the 10gb of VRAM isn't an issue for me.",buildapc,2026-02-21 02:11:37,0
Intel,o6javxg,"Yeah I'd really like to get the 9060xt 16gb but it goes way beyond my budget, every one that I've seen is over $450",buildapc,2026-02-21 02:24:54,1
Intel,o6mfx5f,16gb while offering lots of longevity is grossly overkill for 1080p,buildapc,2026-02-21 16:32:49,1
Intel,o6jbv1p,What about the b580? Is it good?,buildapc,2026-02-21 02:31:03,1
Intel,o6jbbok,"Tbh it's more like a 1650 or a 2060 non super, cause it's from a Chinese brand that limits its performance. Tho I don't have any complaints so far on old games, it doesn't hold up well on newer AAA",buildapc,2026-02-21 02:27:40,2
Intel,o6jbjj7,"Yeah, leaning towards that one or the b580.",buildapc,2026-02-21 02:29:02,1
Intel,o6jom85,The B580 is around what 300? They normally advertise it as an entry level 1440p but would be great for 1080p. Comes with decent VRAM as well at 12GB. I think itâ€™s a good option if youâ€™re around that 300 mark.  The 3080 is still the better option though imo cause theyâ€™re so close in price.,buildapc,2026-02-21 03:54:35,2
Intel,o6jxsk1,B580 is peak but it's a bit worse than even 8gb competitors when it comes to 1080p,buildapc,2026-02-21 05:01:15,1
Intel,o6jj6s5,Why would the performance be limited? Itâ€™s the same GPU chip regardless of brand.,buildapc,2026-02-21 03:17:57,1
Intel,o6jdip7,"Gotcha. A 30 series card will do the latest version of DLSS which should get you all the way there. If youâ€™re going new, anything should get you 1080/60. Iâ€™d go for a 5060 or a 9060 if youâ€™re budget conscious - Iâ€™m not sure you have the CPU overhead with a 5600 for a b580. Those tend to be more demanding with their drivers.",buildapc,2026-02-21 02:41:31,-1
Intel,o6jcq1n,I picked up a gently used 2080 S for a friend for $145 who was still rocking a 1660.   Still very good for a 1080p build. Iâ€™d but something cheap and used and wait this whole mess out.,buildapc,2026-02-21 02:36:30,1
Intel,o5yf963,I think upgrading the CPU Mobo here would be good. 2500k is incredibly dated way more than the GPU. GPU upgrade after the cpu would be ok. But cpu i think should go first.,buildapc,2026-02-17 23:25:24,24
Intel,o5yhbn9,"Upgrading the CPU + motherboard would be a  bigger upgrade than that GPU.  Does the computer have a HDD or ssd? Because if it's that old, there's a good chance it's still running off of a hard drive, and upgrading to a SSD would make a huge difference.",buildapc,2026-02-17 23:37:03,6
Intel,o5yy5gu,"Thanks for all input.  To answers a couple of questions it has 500gb ssd, 500w power supply and a Ruix case (donâ€™t really know much about the case other than it looks great).  Newegg_PC_Support posted a link to the exact CPU/Motherboard/Ram combo I was talking about in my post.  I think Iâ€™ll go with the CPU upgrade option for his birthday.  He can start saving up for a better GOU down the road.  A few of you mentioned we might gotten scammed or at least over paid a little - you are likely correct.  Iâ€™ll justify the extra $100 or $150 as the cost for me and my son to learn about/upgrade/build his PC together.   Once again, I really appreciate all the help and advice, thanks!",buildapc,2026-02-18 01:09:55,4
Intel,o5yi5l6,"The machine you got from Marketplace is about 12 years old, so some things are critical to confirm   \- Does it have a newer power supply above 550w?    \- Does it have a solid state drive? (above 500gb) This is a must  The Intel ARC Cards aren't great on older hardware so your next step is to find something modern (most likely still used). Some absolute madlad on youtube threw a RX6700 Video card into a 2500K System and.... it was fine?       [https://www.youtube.com/watch?v=t\_ob3Hlo9Dg](https://www.youtube.com/watch?v=t_ob3Hlo9Dg)",buildapc,2026-02-17 23:41:43,4
Intel,o5ykht0,"I agree with the other repliers that you should upgrade the CPU MB and Memory first, we have thos COMBO listed [https://www.newegg.com/Product/ComboDealDetails?ItemList=Combo.4853120](https://www.newegg.com/Product/ComboDealDetails?ItemList=Combo.4853120) that fits your sons budget. Feel free to contract CS for more information!",buildapc,2026-02-17 23:54:38,3
Intel,o5ygkfu,"I would 100% get the new cpu, mobo, ram set first. I'm honestly shocked someone other than an enthusiast would even know about the arc gpus. Can't say I would 100% recommend for someone who doesn't know how to troubleshoot just yet as it is a very new department for Intel. Cpu first though",buildapc,2026-02-17 23:32:44,2
Intel,o5yieas,"Which Ryzen 5 and which Arc? I'm assuming it's a Ryzen 5 5500 and Arc A750, but the Ryzen will offer a lot more of a performance increase than just a GPU upgrade. The current CPU is quite old and will hold back the performance of a modern GPU, so upgrading the CPU makes the most sense for now.",buildapc,2026-02-17 23:43:04,2
Intel,o606v4q,Def the CPU/Mobo/RAM. 1000%  Later look for a used or refurbished GPU. GTX 1660 supers are pretty decent and readily available for around $150.,buildapc,2026-02-18 05:37:56,2
Intel,o60mgsq,"Any intel card on a platform that doesnâ€™t have resizable Bar will perform horribly, so do the platform first. Also what intel arc card sells for 230$ with 8gb of vram? Because the b570 with 10gb should be around that price too and is a little more stable with the drivers.",buildapc,2026-02-18 07:51:16,2
Intel,o60qcdt,"Just to add are you anywhere near a micro center because thereâ€™s a really strong cpu bundle if you can stretch to $350 with ram and a motherboard. Itâ€™ll get you onto a current gen platform that could last well into the middle of the 2030s. Again I know this is over your intended budget but itâ€™ll give you the performance very similar to the best am4 cpus at a lower cost.   https://www.microcenter.com/product/5007290/amd-ryzen-5-7500x3d,-msi-b850m-vc-pro-wifi-am5,-gskill-flare-x5-series-16gb-ddr5-6000,-computer-build-bundle",buildapc,2026-02-18 08:27:13,2
Intel,o5yjpwb,Ryzen 5 + mobo + ram for $250 is your first upgrade. Then toss in a nvme and a used gpu down the road and this will be an awesome budget rig.,buildapc,2026-02-17 23:50:24,1
Intel,o5zykiz,"That or throw down on an 800 dollar system on eBay. At this point, itâ€™s a starting point. I have a 15 year old nephew running off a 6700k and a 1070 and 32gb on 4 2.5 ssds in a raid 0 array. Had a 1080ti it took a shit, put in a 1070 and upgraded the ram from 16gb to 32 Iâ€™m not the parent, just help. It just has to work.",buildapc,2026-02-18 04:37:52,1
Intel,o5yfn9a,"Unfortunately that motherboard uses DDR3 type memory which is not compatible with the Ryzen. So you would have to add new RAM into the price, which will increase the cost quite a bit.     Unfortunately that system is no longer very upgradeable. About the only thing I can think of is, does it have a hard drive instead of an SSD? A new SSD would be a big help if it has a hard drive.",buildapc,2026-02-17 23:27:35,-1
Intel,o5yj6e3,"Get an arc B570 10GB for 199USD or arc B580 12GB, the whole system needs overhaul but this way you can push it for longer, get him a 2700K on eBay for 5-8$usd as well.",buildapc,2026-02-17 23:47:25,-1
Intel,o5ygj91,"Agree. While both parts are really really old a cpu with more than 4 cores 4 threads will be helpful.   For a considerable upgrade an AM4 Ryzen 2nd or 3rd gen (5th if money allows) with an A320 or B350/B450/B550 board, DDR4 16GB ram, and even money on top for a card like an RX6600 is possible. PSU needs to be compatible but it's not too difficult, just hope it isn't dodgy.  I know OP didn't ask for this and I'm not sure when the initial PC was bought for 320$, but unless the power supply and case are very convincingly modern/expensive, I'm sorry to say you were scammed.  There is a chance of picking up an i7 2600k or 2700k for 10 bucks and overclocking it on top for a performance uplift as well. Getting a gtx1080 level card would still be okay with such a CPU. Just that within the budget for upgrades that was mentioned it doesn't make sense to keep most parts if deals can be had.",buildapc,2026-02-17 23:32:33,5
Intel,o603sjk,yup. dis is da wei. CPU upgrade first before GPU upgrade if budget is an issue. also sometimes a GPU upgrade also means a PSU upgrade.,buildapc,2026-02-18 05:14:36,0
Intel,o5zrhs1,After you upgrade the mobo and cpu the GPU will be an easy enough upgrade by comparison.   I suggest going with a good AMD(Radeon) gpu option because they offer good price to performance  and there are a lot of good budget options and if you go higher on budget youll get better performance from AMD vs NVIDIA typically at your price range. If you are spending less than 600 on a gpu i cant in good conscious recommend NVIDIA.,buildapc,2026-02-18 03:51:16,3
Intel,o5zglub,That would be a massive upgrade.   Curious- how much GPU usage does he get with the current CPU when playing games?,buildapc,2026-02-18 02:46:29,2
Intel,o60vy2f,"I think if my friendâ€™s 3060 eventually bites the dust, Iâ€™ll recommend him an Intel Arc. He runs a low budget (as low as that was possible given prices) AM5 system I put together for him but his 3060 only got its first use this year, so it might still have at least 5 years on it.",buildapc,2026-02-18 09:19:51,1
Intel,o5ygf3g,"His upgrade path is:     >\-A Ryzen 5 + motherboard + 16GB DDR4 bundle for around $250  Which includes the memory  Anyway, the SSD is a good suggestion",buildapc,2026-02-17 23:31:53,6
Intel,o5yfxqu,"They said ryzen 5 + motherboard. Yes, the new motherboard they would buy should work",buildapc,2026-02-17 23:29:12,1
Intel,o5ylmtx,"Arc cards are not worth it without rebar support, as intel mentions themself. Also with a cpu that old, the overhead issues will be horrific.",buildapc,2026-02-18 00:00:52,3
Intel,o5yhpty,"I would agree.  That mobo+CPU+ram combo on eBay would easily be under $50.  So the rest of the money went to case, fans, PSU, and GPU.  It's not the worst scam of all time or anything, but depending on the rest of the parts, it was at least likely overpriced.",buildapc,2026-02-17 23:39:15,3
Intel,o63vr4d,"I wouldn't go Arc, a lot of driver issues, game without proper support and no idea if intel will keep that part of their business afloat to give proper support in the next years... Given that, ARC is on really enthusiastic side of it, I would just throw that curve ball on him if he really likes to do this kind of stuff",buildapc,2026-02-18 19:19:21,1
Intel,o5yhxre,That may have been edited - I'm really sure the memory part was not there when I replied.,buildapc,2026-02-17 23:40:29,2
Intel,o5yrj6g,Upgrade the system later on B580 12Gb now makes sense why upgrade now to upgrade later meaning in near future?? Don't want to buy obsolescence,buildapc,2026-02-18 00:33:30,0
Intel,o5zrpgq,Because youre suggesting an upgrade that doesnt make sense to do first,buildapc,2026-02-18 03:52:37,2
Intel,o7ex5ak,There are a bunch of posts lately with this happening on x3d chips. I haven't found a solution for mine but some people have luck by reinstalling the Ethernet adapter. https://www.reddit.com/r/apexlegends/s/JDom2hHtxH,buildapc,2026-02-25 22:50:58,10
Intel,o7ez7sy,"when entering those areas just for the first time? or does it continue every time you go back to a different area? cuz if its only the first time, its shader cache loading, which is normal in certain games",buildapc,2026-02-25 23:01:43,2
Intel,o7ezkgh,"also. do you have x3d mode in the bios turned on? if so, turn it off. thats only for dual CCD cpus(X900x3d and X950x3d chips)",buildapc,2026-02-25 23:03:34,2
Intel,o7famzp,"Iâ€™ve seen people talking about these kinds of issues on the MSI forums.  You may want to consider going through a few BIOS versions for your board and see if any of them offer a better experience.  Also just out of curiosity, try using GPU-Z and seeing if your PCI-E mode is switching from 5x to 1x when this is going on.  This is another common problem Iâ€™ve read about on the MSI forums.  The BIOS updates could address this as well.",buildapc,2026-02-26 00:04:19,2
Intel,o7eqxk4,Did you try new GPU drivers and chipset?,buildapc,2026-02-25 22:19:47,1
Intel,o7ewfk3,"Do you happen to use a riser?  Either way I'd check and make sure your PCIe is set to 5.0 (when not using a riser), GPU-Z will show what it's running at.",buildapc,2026-02-25 22:47:19,1
Intel,o7eypa6,"This thread seems to have similar issues with that same cpu/motherboard, but I'm not sure they actually found a solution, but there might be some helpful stuff in the comments. I don't use the chip itself, so I can't really speak much to that, it seems like this isn't an unheard of issue though.       [https://www.reddit.com/r/AMDHelp/comments/1p2lmcl/new\_pc\_w\_9800x3d\_stuttering\_have\_tried/](https://www.reddit.com/r/AMDHelp/comments/1p2lmcl/new_pc_w_9800x3d_stuttering_have_tried/)",buildapc,2026-02-25 22:59:03,1
Intel,o7fyesf,"Some people had issues with x3d + rivatuner OSD, try disabling it and see if you're still having problems.",buildapc,2026-02-26 02:18:01,1
Intel,o7f0kl0,Thank you! Planning to uninstall my Ethernet adapter and see if that fixes my unresolved issue.,buildapc,2026-02-25 23:08:55,2
Intel,o7fhi8v,Replying to my own post to share this post that is very relevant.  https://www.reddit.com/r/pcmasterrace/s/6uzZ13qYVW,buildapc,2026-02-26 00:41:48,2
Intel,o7es8b9,"Yes, both are up to date.",buildapc,2026-02-25 22:26:15,1
Intel,o7esyvs,"Bios update? And do u happen to have another video card to try? If not, is integrated GPU fast enough for bioshock infinite to troubleshoot?",buildapc,2026-02-25 22:29:57,1
Intel,o63ddst,B580.,buildapc,2026-02-18 17:58:20,16
Intel,o63es1x,B580. I have it and love it.,buildapc,2026-02-18 18:04:24,5
Intel,o63fk05,Out of those the B580.,buildapc,2026-02-18 18:07:53,3
Intel,o63m3ck,"B580, but any of those choices would work for 1440p as well (not sure on 5050) but have 6600/7600xt and b580.",buildapc,2026-02-18 18:36:13,1
Intel,o63n8mi,the 5050 and 5060 are priced at basically the same pont where im at so id never consider a 5050 however id still go with a b580 over a 5060 because of the 12 GB Vram same goes for the RX 7600,buildapc,2026-02-18 18:41:18,1
Intel,o63pkal,Wait and get a 5060 if you can otherwise the 5050.,buildapc,2026-02-18 18:51:29,1
Intel,o65zct7,"Save for the 9060XT, even the 8GB version is over 30% faster than any of these, apart from when everything but the B580 runs out of vram due to its 4GB extra. However the B580 also has CPU overhead issues (Intel has it worst, then Nvidia, and AMD has the least overhead).",buildapc,2026-02-19 01:39:09,1
Intel,o66tu5g,B580 easily of the options you have shown.  If you can save a little more the 9060XT is a great 1080p-1440p card.,buildapc,2026-02-19 04:45:58,1
Intel,o63dlkh,"New cards make little sense at this price point, look for good deals on used market instead.",buildapc,2026-02-18 17:59:15,1
Intel,o63fveq,"Depends on the price. Are you in the USA?  B580 might not perform well with your 5500 cpu  I would go 7600, they were available for less than $200 but I don't know recent pricing.  If I was going to spend $200 on a video card I would go 3070 all day. Get one on ebay with their guarantee.",buildapc,2026-02-18 18:09:18,0
Intel,o63f46k,"For $250-300 which is average price for these cards I would look for a used card like a 3070ti/3080 or 7700xt, yes it's overkill for the r5 5500 but that means you won't need to upgrade as soon. Plus if you have a 650w good quality psu it should be plenty for these cards",buildapc,2026-02-18 18:05:56,0
Intel,o63elhs,"This. The 7600 is lacking modern software features, and the 5050 is terrible value.",buildapc,2026-02-18 18:03:34,7
Intel,o64odgl,You're not gonna hit 12 gigs at 1080p if you're not running Indiana Jones with max texture buffer (you wouldn't in the first place with these cards).,buildapc,2026-02-18 21:32:19,1
Intel,o63gug6,"i am not from USA, and in my country are taxes for products from outside, which means that a graphic card that in the us is $200, in my country that would be the double of the price",buildapc,2026-02-18 18:13:31,1
Intel,o63mc2x,"Why would you think that? I haven't tried my b580 with the 5500, currently it has a 6600 and I think it's a near perfect combo, but it should be plenty enough cpu for the b580.",buildapc,2026-02-18 18:37:18,1
Intel,o63xs5t,do you think that it would be worth buy a b580 thinking about buying a better cpu in the future?,buildapc,2026-02-18 19:28:50,1
Intel,o63h8nw,If you can find a used 3060ti or 3070 for the same price as as 7600. I would do that otherwise go with the 7600.,buildapc,2026-02-18 18:15:12,1
Intel,o63p01k,"[Arc B580 Overhead Issue, Ryzen 5 3600, 5600, R7 5700X3D & R5 7600: CPU-Limited Testing : r/hardware](https://www.reddit.com/r/hardware/comments/1htbgu8/arc_b580_overhead_issue_ryzen_5_3600_5600_r7/)  Your 5500 is equivalent to a 3600 I believe",buildapc,2026-02-18 18:49:03,1
Intel,o63rzh4,"That was a year ago, there have been many driver updates since then. I use mine with an 11400 and have never had any issues with it. Maybe the pcie 3 of the 5500 could hurt.",buildapc,2026-02-18 19:02:12,3
Intel,o5l5q4t,Arc b580 or 9060 XT 16gb if those GPUs are still in that budget.,buildapc,2026-02-15 22:44:05,3
Intel,o5l8ans,"A modern GPU. The ARC B580 at $300 is alright, although the driver has inconsistent performance in older games. Consider looking for a used RX 6800 or 7700XT around the same price.  https://pcpartpicker.com/product/WscBD3/asrock-steel-legend-oc-arc-b580-12-gb-video-card-b580-sl-12go  https://youtu.be/aV_xL88vcAQ  Overclocking your RAM is worthwhile. Even achieving DDR4 3000 would yield 5-10% higher minimum FPS in most games.  https://github.com/integralfx/MemTestHelper/blob/oc-guide/DDR4%20OC%20Guide.md  Your R5 3600 is fine, it will run games smoothly. R5 5600 is only 15-20% better for gaming.",buildapc,2026-02-15 22:58:14,2
Intel,o5ns4kp,Used RTX 3080 for ~$300,buildapc,2026-02-16 10:16:40,1
Intel,o5px30b,"I think focusing on a GPU upgrade is smart. If you want to go with new components, I think you need to decided if you spend $250 to get a solid upgrade, and pocket the $100 for other stuff; or spend $350 and get an ever better GPU upgrade, but then have spent it all. Used market could really help.here, or it could be bust, that's just the nature of things. A B350 is tempting, if you are sure it's driver splay nicely with your games (IIRC, Intel.GPUs had the most compatibility issues with older games, which matches your game list, idk, maybe these have been resolved). Talking new: at the $250 end you have the B350 and the RTX5050; you then can take a step up to the 9060xt and rtx5060, but that gets you closer to $350 and starts presenting vram questions.   CPU; idk, I think I would want to upgrade to an 8 core, and a 5700x is in your budget, but can't really fit in with a GPU also. Probaly best to focus on GPU first.",buildapc,2026-02-16 17:46:43,1
Intel,o5qb303,"After looking at a lot of options, I'm strongly considering an XFX 6650 xt for 300ish, and then saving up to upgrade the CPU to a 5700 series. Thoughts on this approach?",buildapc,2026-02-16 18:50:42,1
Intel,o5l6wmo,"B580 is an awesome choice, but you would definitely have to upgrade to a better cpu and a 5600 is $180 on amazon right now, same price as a Ryzen 5 9600x unfortunately.   Honestly I would go with a 5060 Ti 8gb for now. As much hate as the 8gb card gets it will be an awesome upgrade over the RX 480 and you wouldnâ€™t have to upgrade your cpu to get the most out of it",buildapc,2026-02-15 22:50:35,0
Intel,o5m8fu3,"Hit the marketplace for a 1660 super ($40-60) and save up for a full build. Or go balls deep 9070xt/5070ti and use the rest of the system as a placeholder for a legit gaming rig (9800x3d, 1000w PSU, 32-64gb ddr 5, b850, 5th gen NVME)",buildapc,2026-02-16 02:39:46,-1
Intel,o5l7fv5,"Looks like the 9060 8 gb is right at 350, but 16gb is more like 500.",buildapc,2026-02-15 22:53:32,1
Intel,o5lqh47,5600x and 9060xt is your best bet,buildapc,2026-02-16 00:45:26,2
Intel,o5llgsa,Since you already have an AM4 mobo the 5600xt makes more sense than getting the 14400 and a new LGA 1700 board. Look into the 5800xt as well.Â   The 5060 is 20%+ stronger than the b580. Iâ€™d rather get the faster card over the extra vram. Since longevity is important to you I would recommend the 9060xt 16GB. Itâ€™s the best card of the bunch by far.Â   I would get the GPU first then upgrade the CPU. I donâ€™t expect the cpu prices to change much.,buildapc,2026-02-16 00:15:44,1
Intel,o5lyngg,"I built a home server with 32gb ddr4, 5600XT, and a 5060. It games pretty well for what it is. Like others said, a 16gb card is a better idea for long term.   The 5600xt performs great.",buildapc,2026-02-16 01:36:48,1
Intel,o5ld26o,"First, avoid the 8GB cards. If you want this to last for years, plan ahead with the VRAM. I know you didn't list AAA-titles, but you could catch the GTA6 bug.  Ignoring ray tracing, dlss, cuda and the 8gb cards and **WILDLY** over-simplifying, you get this for relative card performance:  Card | Street Price | bogoFPS :----|:----|:--- 5090 | $3700 | 175 5080 | $1300 | 125 5070TI | $950 | 113 9070XT | $730 | 109 9070 | $630 | 100 5070 | $620 | 92 5060TI | $525 | 66 9060XT | $440 | 63 B580 | $300 | 48 B570 | $250 | 42  *bogoFPS is based off TechPowerUp relative performance at 1440p*  The 14400f is a better pick over the 5600x, as it's competitive with the [5800X in gaming](https://www.tomshardware.com/pc-components/cpus/intel-core-i5-14400-cpu-review/3), but honestly both are good. One sizable advantage for the 14400f is you can switch to ddr5 without buying a new cpu - just upgrade motherboard and memory.",buildapc,2026-02-15 23:25:48,1
Intel,o5lelw1,"Thanks for the input! The 8gb of VRAM on my RX590 probably is the reason it lasted so long, so your advice is well received. So the question now is, would 12gb be enough? Or should I go for the RX 9060 as the definitive pick?   As for the CPU, I've considered i5-12600KF as it was recommended over 14400f, but at â‚¬170 (that is without a fan - effectively pushing the price to â‚¬200) would it be worthy upgrade over 14400?",buildapc,2026-02-15 23:35:01,2
Intel,o5llr85,This is an incredible comment! This could answer at least 30% of the posts on this sub. Top notch ðŸ‘.,buildapc,2026-02-16 00:17:24,2
Intel,o5lon9c,">would 12gb be enough   *I* would go big here, especially as the price difference isn't massive and the performance is so much better. You're not just paying for VRAM, the card is all-around better  >RX 9060*XT 16GB* as the definitive pick?    If I had money to get a new GPU, I'd get the following depending on my budget:  1. 5070ti 2. 9070XT 16GB 3. 9070 16GB 4. 9060XT 16GB 5. B580  >worthy upgrade over 14400?  **I** would get the 12th Gen because I don't trust Intel 13/14. Many people do not feel as I do, but I just don't trust the long-term stability of either generation.",buildapc,2026-02-16 00:34:32,1
Intel,o6dqc2z,"Totally possible. For a full build list, head over to /r/buildapcforme",buildapc,2026-02-20 06:47:11,2
Intel,o6fy6oy,"for 1080p, sure.",buildapc,2026-02-20 16:06:19,1
Intel,o4lpfu2,"Any modern igpu, let alone a discrete card, can render your desktop in 4k@whatever refresh rate, they can only be capped by the supported DP/HDMI version. Ex. B570 (a supposedly $220 150W card) can do DP 2.1 and I think should be able to do 4k 10bit at 180hz.",buildapc,2026-02-10 12:18:23,19
Intel,o4lps3f,"Wouldn't the Intel B380 do that? It can max do 8K, DP2.0 with DSC and has HDMI 2.0b. Has a solid AV-1 decoder.",buildapc,2026-02-10 12:20:48,6
Intel,o4lpf65,"The Intel arc a380 supports display port 2.0 uhbr 10. That can do 4k 172hz at 8 bit, or 142hz at 10. I think that's plenty for your use cases",buildapc,2026-02-10 12:18:15,6
Intel,o4lqgl3,I believe Arc gpu do support 4k120hz through displayport.,buildapc,2026-02-10 12:25:35,3
Intel,o4lop19,Have a look into the RX 7600,buildapc,2026-02-10 12:13:02,2
Intel,o4lxsz4,"I have a very similar use case in my linux build and for it I've chosen sparkle A310. Your particular refresh rate needs would require A380. That said, can't beat these cards for AV1",buildapc,2026-02-10 13:13:42,2
Intel,o4mbml7,Intel has the best encoder for AV1. Intel B570 might be suitable for OP.,buildapc,2026-02-10 14:30:15,2
Intel,o4n3jze,Definitely try find an a380. Similar encode decode with the latest cards and much cheaper.,buildapc,2026-02-10 16:45:31,2
Intel,o4mh7li,"Thanks to all who took the time to reply. The A380 (or rather: Pro40) was high on my list.  Interestingly, the search results about its support for >60Hz changed dramatically from yesterday into the opposite.  However, over at the intel arc subreddit, there still is a comment about the A380 not supporting 4k refreshrates >60Hz.  I would like to avoid the 5xx Series, A or B, due to their high power consumption. Same is probably true for AMDs 7xxx Series.  Edit: At least for their Pro cards, on their support site, intel reccomends DP40 cables, which are capable of uncompressed 4K at 144HZ.   Next to the already mentioned UBHR10 this will make it most likely worth a try, if I can source a used A380 or Pro40",buildapc,2026-02-10 14:59:24,1
Intel,o4ltbal,amd's just pretending to be a gpu now.,buildapc,2026-02-10 12:45:07,-4
Intel,o4lxaqu,Define modern igpu? Intel 8th gen igpu? Ryzen 3000G?,buildapc,2026-02-10 13:10:38,2
Intel,o4lozly,Or the intel arc 380,buildapc,2026-02-10 12:15:10,2
Intel,o4lzppk,"I'd like to define it as ""released in this decade and not previous"", but I'm pretty sure zen 2+ igpus support DP 1.4.",buildapc,2026-02-10 13:25:01,7
Intel,o4nbhny,"Though Picasso based APUs don't have hardware av1 decode, for that you'd need an AM5 based model as those have an iGPU based on RDNA2",buildapc,2026-02-10 17:22:08,1
Intel,o66bp2z,"Nice job reacting quickly to unplug everything and letting it dry for a couple days. Most parts in a computer should be fine getting wet, as long as electricity isn't running through it while it is wet. The fact that it turned back on at all is a great sign, since if things were still wet and then you turned it on things could've definitely fried.  I agree with the other commentor that checking your cables is a good first step, since having high CPU usage and low GPU usage and worse performance could point to the issue being as simple as that.  I'd also check your hardware temperatures while playing a game. How hot is your GPU getting? Your CPU? Could a fan on your graphics card have gotten fried therefore leading to high thermals on your GPU and severe thermal throttling?",buildapc,2026-02-19 02:50:43,9
Intel,o66aebs,Is it possible you plugged your display cable into the wrong port? Itâ€™s possible something inside shook loose while you were unplugging everything also,buildapc,2026-02-19 02:43:16,6
Intel,o66vf1a,Did you re-enable XMP? Might have disabled itself.,buildapc,2026-02-19 04:57:01,2
Intel,o66fp9e,When you dismantled it:  Did you remove: 	â€¢	GPU? 	â€¢	CPU? 	â€¢	RAM? 	â€¢	PSU?  Or just let it sit assembled?  Water can pool under GPU or inside PCIe slot. Try cleaning everything with 98% isopropyl alcohol.  When that dries it will evaporate any remaining water.,buildapc,2026-02-19 03:14:45,1
Intel,o6767xt,"When did you last power off / reboot your PC? It's possible that the CPU issue has nothing to do with the spillage, other than you were forced to power the system down. If it hadn't been restarted for a while, there are a number of possibilities as to what has changed to adversely affect performance, including but not limited to:-  - updates to the operating system, drivers and other software which had not yet been applied - the system is running maintenance procedures triggered by the reboot - some volatile configuration settings have reverted to defaults  Good look in your quest!",buildapc,2026-02-19 06:20:22,1
Intel,o66ggyu,In lobby temps  Gpu: 57c CPU: 47c   In game temps  Gpu: 64.3 c (hotspot 83 c) with a utilization of 50% CPU: 50c with a utilization of 80%,buildapc,2026-02-19 03:19:25,3
Intel,o66cunh,Thank you for your reply. Iâ€™ve changed display cable ports gpu slots and ram slots nothing seems to work,buildapc,2026-02-19 02:57:31,4
Intel,o6cvrfd,Not sure what xmp is but I will look into it,buildapc,2026-02-20 02:59:22,1
Intel,o66gp0l,I removed the gpu and ram sticks from the motherboard. I left the cpu attached because I donâ€™t have any experience in removing it and did not want to damage it,buildapc,2026-02-19 03:20:46,1
Intel,o6cvpcd,I powered it off multiple times since the accident,buildapc,2026-02-20 02:59:00,1
Intel,o66gye5,If it helps   memory is 13.2/15.9 gb 84% utility  And disk 0 is at 0%   Iâ€™m not familiar with these pieces of hardware,buildapc,2026-02-19 03:22:20,3
Intel,o66ddi7,Could you let us know what your cpu and gpu are?,buildapc,2026-02-19 03:00:37,3
Intel,o6e0hrj,"No, I was asking how long the system had been up **before** the incident. Sorry, I realise now that I didn't specify that.",buildapc,2026-02-20 08:21:00,1
Intel,o66epm9,Itâ€™s older pre built I got around 2022   Intel(R). Core(TM) i5-10400f Cpu@2.90 ghz  Nvidia GeForce gtx 1660 super,buildapc,2026-02-19 03:08:47,4
Intel,o60t59y,I think a 5800x/5700 would be enough if you're playing on 2k imo. Much cheaper and u stay on the same platform.,buildapc,2026-02-18 08:53:29,16
Intel,o60w4g8,"You can update your bios and then drop any ryzen 5000 series cpu into your current motherboard, so that would likely be the best cost to performance. As you noticed DDR5 is obscenely expensive so you are looking at spending at minumum $400 more to go to am5 vs buying a 5800xt and reusing your current mobo + ram, and thats just for a 7600x build.  My vote is for a 5800xt as the most practical option. Unfortunately it's nearly impossible to get a 5800x3d for a sane price, however if you get really luckey and find one for a good price, any of the 5000x3d varients would be the ideal option imo",buildapc,2026-02-18 09:21:31,9
Intel,o60w2ve,"> If i upgrade to AM5 what should I look for in a motherboard?      Feature set (how many expansion slots you need, whether you need wifi or faster ethernet than 99.999% of people on the planet have a use for). If you aren't running a ryzen 9 or a threadripper you don't need a beefy, overpriced vrm on your mobo.      > the best cost+performance      14600k on a decent motherboard for it. Any AM4 option is either ludicrously expensive (x3d), or significantly worse (5700/800/xt etc). Any AM5 option is ludicrously expensive because of ram.",buildapc,2026-02-18 09:21:06,3
Intel,o60x5mq,"5700X, 5700X3D,5800X, 5800X3D, 5800XT. All of these options are enough for the 5070, and won't cause any bottlenecks. Best part, you don't need new ram and motherboard",buildapc,2026-02-18 09:31:19,3
Intel,o60y52l,Define â€œcausing a massive bottleneckâ€,buildapc,2026-02-18 09:40:35,2
Intel,o615fqq,"Honestly, go shop around at Newegg and MicroCenter. They usually have combo/bundle deals where you can get a new CPU, RAM and mobo for a reasonable price. May as well grab an AM5 B650 combo if you can get one. The 7800X3D can be had in bundles for well below MSRP these days. I'm assuming that you're in the US.  Also, the Intel Core Ultra 7 265K can be had with bundles for pretty cheap too. While X3D is the gaming champ right now, the 265K is still a damn good CPU at the current prices, and usually comes bundled with a Z890 motherboard...which presents an amazing value over MSRP/regular price per item.",buildapc,2026-02-18 10:45:55,1
Intel,o61b78s,"Right now, it makes 0 sense to jump to an AM5/DDR5 platform, unless you are going all out for the top of the line build  Just Update your motherboard BIOS and get the best 5000 series Ryzen CPU you can afford (currently the 5800xt seems like the most reasonable option)  After that as a QOL thing Right now you still have a chance to find some ddr4 ram on the used market for not absolutely ridiculousÂ price. You should aim for 32gb (2x16) 3600-3200mhz with the lowest cas latency you can find.  Do not just get an additional random stick(s) of ram, as almost always you will have compatibility problems. Instead, get a full kit from someone and then sell the one you have. That goes for all of the rest of your old components- sell them to get some additional money for your upgrade.Â   Right now you can get about 120$ for your 2060 and about 80$ for the 3700x according to the average of eBays recently sold listings,Â   And that is enough to get the 5800xt",buildapc,2026-02-18 11:33:51,1
Intel,o61rqut,"It's a tough one because of the extreme cost of RAM. Trying to balance 'enough performance' at a price point that makes sense is hard nowadays.  Way I see it you have 2 options:  Grab a 5700X/5800X/5800XT and call it a day. 20% more CPU performance for <$200, it's a good value upgrade, but not revelutionary.  Go AM5 with 7800X3D+32GB. Significantly more expensive but a night and day upgrade. Could make an argument that 7600X would be sufficient but that only brings the total cost from ~â‚¬890 to ~â‚¬760, so the X3D represents better value when you consider total platform cost. Particularly when you compare 5800X to 7600X, they're the same multicore and about 20% different single core but a â‚¬500+ difference is just not worth it for similar overall performance.",buildapc,2026-02-18 13:23:57,1
Intel,o62ah24,"Best value option right now is going AM5 with the 7800X3D. Huge FPS uplift thanks to 3D V-Cache and itâ€™ll stay relevant for years.  The 5800X3D makes no sense at that price, 7950X3D is overkill for gaming, and the 7600X will struggle more in CPU-heavy titles.",buildapc,2026-02-18 15:01:23,1
Intel,o62op3l,"quick question for anyone here, i also have a 3700x and was thinking of upgrading to a 5060ti for 1080p 360hz will this be a huge bottleneck for what im trying to achieve?",buildapc,2026-02-18 16:07:11,1
Intel,o611bsk,"Do not buy the 5700 non-x , get the 5700X. The 5700 is just a 5700G which is significantly worse than the 5700x.",buildapc,2026-02-18 10:09:29,13
Intel,o60t6ny,If you can find the x3d chips even better!,buildapc,2026-02-18 08:53:50,1
Intel,o60ykqu,"Playing Arc Raiders can put graphics settings to whatever I want and still get pretty much the same FPS, 50 FPS min 90 FPS max.",buildapc,2026-02-18 09:44:33,2
Intel,o616mlz,"If OP mentions â‚¬ then their nearest microcenter is one ocean away from them. Tell me about the Microcenters you can find in Europe, Iâ€™ll wait.   Newegg also only exists in the UK, which doesnâ€™t use the â‚¬ and is no longer in the EU.",buildapc,2026-02-18 10:56:01,3
Intel,o63au6m,"Thanks for the informative response, yea just estimating the prices of what I wanted to get it would cost around minimum â‚¬600(All used from Ebay).",buildapc,2026-02-18 17:47:20,1
Intel,o636dve,Thank you for a good reply that references the information I provided!,buildapc,2026-02-18 17:27:09,1
Intel,o639s7e,"As I just upgraded to a 5070, it definitely just depends on which games you are trying to get 360 fps in.  I have doubled my performance on some games(GPU intensive) and in others have improved by about 30-40 fps from the 2060.  I could see you getting to around 360hz in a game like CS2 or fortnite but I am yet test these titles myself with my new specs.  I was going to buy a 5060ti but just when I started looking, a new 5070 showed up for msrp!...",buildapc,2026-02-18 17:42:41,1
Intel,o611uqv,"Yes sorry, 5700x i meant",buildapc,2026-02-18 10:14:14,2
Intel,o611pql,"Weird, if I look up benchmarks, the 3700x should perform way better in that game. There isnâ€™t some eco mode enabled or pbo disabled in the bios?",buildapc,2026-02-18 10:12:59,2
Intel,o63u22z,"Yeah...didn't need your snarky BS, bud. Go sit on a spear.",buildapc,2026-02-18 19:11:36,0
Intel,o65kozr,"You mad, bro? Such big feels about this so called ""US defaultism."" I said ""assuming that you live in America"" with the knowledge that there was a possibility that OP didn't live there. It was put there to acknowledge that they may not be. As for you ""calling me out,"" you're doing online what you lack the courage to do in real life. That says more about you than it does about me, kid.",buildapc,2026-02-19 00:15:25,0
Intel,o66lcl6,"It must suck to have such a meaningless life as to get pressed about someone mentioning deals at an American retailer in an attempt to help someone save money on an upgrade. I bet you're fun at parties. Your evaluation of my reading comprehension means nothing. At the end of the day, you're still the one who is so insecure that they need to seek validation by belittling others on the internet. I feel sorry for you.",buildapc,2026-02-19 03:49:37,0
Intel,o61pe7b,"Yeah it's cut down to 16MB cache, kinda like a reverse X3D",buildapc,2026-02-18 13:10:20,2
Intel,o6179xh,"Yea I honestly really don't want to upgrade my CPU yet but if you check my last post I have no idea what is causing the low fps.  Feels like my performance has gotten worse on select games mainly arc raiders since i upgraded my gpu.  I'll double check my bios settings, should I just reset bios to default settings?(Was undervolting CPU with stability tested)",buildapc,2026-02-18 11:01:29,1
Intel,o63bz05,Ended up resetting all BIOS settings to default(not even most performant) and my FPS stability has improved.  Now getting consistent 100+ FPS on The Finals and about the same in Arc Raiders.  Thanks!,buildapc,2026-02-18 17:52:18,1
Intel,o63f45p,"Good, glad it improved. At 1080p, the 5070 will be held back somewhat by the 3700x, but it should not be horrible. Glad that the issue got resolved.",buildapc,2026-02-18 18:05:56,1
Intel,o5veoww,You're playing modern games at 1440p. IMO any 8GB GPU would be unwise.   9060 XT 16GB sounds good to me.,buildapc,2026-02-17 14:38:30,12
Intel,o5vffi2,If you can afford the RX9060XT 16gb then that is the better GPU.  If the RTX5060Ti 16gb is a similar price in your area then get that.,buildapc,2026-02-17 14:42:22,3
Intel,o5vflug,"9060xt 16gb is your best choice, or even better a 9070 non xt would work also, got a 5600x myself and a 9070 for 1440p, runs good",buildapc,2026-02-17 14:43:17,2
Intel,o5w2yac,Thanks everyone  I guess 9060 it is,buildapc,2026-02-17 16:39:37,1
Intel,o5w9311,9060 XT won't be overkill. I have a 5700x and 9070 XT and it's great for 1440p. Your 5600x will handle the 9060 XT just fine.,buildapc,2026-02-17 17:10:42,1
Intel,o5vfahz,Well said!,buildapc,2026-02-17 14:41:38,2
Intel,o5vipvy,"i had 2600 overclocked with rx580 and I upgraded to 6800xt and got a 5600x after. the 2600 is in another build with a 3050 8gb, surprisingly good for 1080p.",buildapc,2026-02-17 14:59:08,3
Intel,o4y694o,of course the 3060ti  https://preview.redd.it/2y9y63fqw0jg1.png?width=560&format=png&auto=webp&s=be14b85c26966999027870e53a576aa56bca6dc6,pcmasterrace,2026-02-12 08:21:38,2
Intel,o7fvpgm,"The a750, i beleive, is quite underrated.. in my country u could get a new one for 200 bucks which is a great deal! rather than a 3060 or a 6k series",pcmasterrace,2026-02-26 02:02:33,43
Intel,o7g0vtm,"Always wanted to try an intel card. If PC prices wern't out of control currently, I would build myself a nice Mid to play about with,",pcmasterrace,2026-02-26 02:31:59,19
Intel,o7fyo0x,Please intel donâ€™t screw this up lol.,pcmasterrace,2026-02-26 02:19:26,9
Intel,o7fz69w,Pretty good deal with multi-frame gen capability brought to this generation.,pcmasterrace,2026-02-26 02:22:20,5
Intel,o7g5xai,Very pretty card. I like the simple aesthetics.,pcmasterrace,2026-02-26 03:00:31,3
Intel,o7g8d0p,"i use their b60 pro card for my workstation and it's a fucking monster. handles 32b llms, massive fusion 360 and solidworks assembly files, and still shreds battlefield 6 lol",pcmasterrace,2026-02-26 03:14:44,3
Intel,o7g7du1,Been running one since launch and its been great. Don't game much these days but its great at AV1 encoding. Def wanted to support Intel in GPU manufacturing.,pcmasterrace,2026-02-26 03:09:04,2
Intel,o7g2dtw,"A750 can't compete with a 3070 at all. It's more comparable to a 3060, however 3060 works better in some games since the games are better optimized for Nvidia GPU's  Edit: I seem to get downvoted by some people, please check some benchmarks on YT, 3070 is around 30% beter",pcmasterrace,2026-02-26 02:40:28,4
Intel,o7ge6lh,"I love my A750 and Ryzen 5600GT combo. With quality upscaling I play just about anything on my 4k monitor. (HDII, BG3, HWL) Highish settings for everything. Solid card got mine for $179",pcmasterrace,2026-02-26 03:50:06,1
Intel,o7gfdo1,I hear those are crazy for video transcoding. $150 ainâ€™t bad,pcmasterrace,2026-02-26 03:57:33,1
Intel,o7gfxt4,"damn I could've saved $40 buying that instead of a 2080S, if only I wasn't building an eGPU...",pcmasterrace,2026-02-26 04:01:06,1
Intel,o7ggg4u,don't forget to enable resizeable bar!,pcmasterrace,2026-02-26 04:04:24,1
Intel,o7ggr9h,"My A770 did alright. Played everything I wanted at 1440p at or slightly above 60fps and a good amount of eye candy enabled, though some newer games did need XeSS (if supported) or FSR to get there. I got lucky and was able to score an open box 9070XT at MicroCenter about a month ago, before GPU prices really started going to shit again, otherwise I'd still be running it.",pcmasterrace,2026-02-26 04:06:24,1
Intel,o7ghpoq,"I've had one in my proxmox server passing through to various things since it was released, enjoy!!",pcmasterrace,2026-02-26 04:12:37,1
Intel,o7goph2,That GPU runs Arc Raiders just fine.,pcmasterrace,2026-02-26 05:00:37,1
Intel,o7g98v6,I never see someone happy with intel gpu,pcmasterrace,2026-02-26 03:19:56,0
Intel,o7fx3uh,"Thought the same, the sales of this was so poor that the new selling price was down alot from the original launch price. while most gpus price increase with time this followed the opposite trend , this and its budget ome the a580 both were great and underrated, and with time shown great price cuts which made them look even good. It had issues with its driver afaik , should have bought one would have been good as a spare for some builds.",pcmasterrace,2026-02-26 02:10:37,7
Intel,o7g480f,"For what it's worth I've picked up a Ryzen 5500 ($55), 16GB (2x8GB) Crucial Ballistix 3000mhz DDR4 ($40), MSI A520 mATX mobo ($50), This GPU ($140), Coolermaster 301 mATX case ($40), SP 500GB NVMe ($60), ID Cooling Frozn A400 ($12) and an MSI MAG A750GL 750W ATX 3.1 80+ Gold PSU ($55). Besides the case I picked all up in the past 60 days for a total of $452. Will be selling it for \~$525 once built.",pcmasterrace,2026-02-26 02:50:48,12
Intel,o7g3bjd,"Grab a B580. Can get one now in the US for ~300 plus tax. I went Intel to support more competition. I don't regret it at all. 1440 solid 60 fps, max settings.",pcmasterrace,2026-02-26 02:45:43,2
Intel,o7g9bm9,I've worked as a field engineer at some of their fabs in the US and overseas. I also want them to succeed.   I also know their layoff cadence seems to follow their old CPU tick-tock strategy and I'd never work for them directly because of it.,pcmasterrace,2026-02-26 03:20:22,2
Intel,o7g5glf,"In 23-24 that was absolutely the case. Today, there are edge cases where it does. That's why the spread is everywhere from the 6600 to the 3070.  In case you weren't aware, the Arc A750 has 62% higher pixel rate, 69% higher texture rate, and 14% more memory bandwidth than the 3070. Not to mention the RT cores are much newer. On paper it and in practice it is clear drivers have been the only thing holding it back. As those have improved, so has performance.",pcmasterrace,2026-02-26 02:57:53,6
Intel,o7g1lhw,very underrated indeed!,pcmasterrace,2026-02-26 02:36:00,1
Intel,o7gkar5,"Really nice. I've recently discovered the wonderfulness of fb marketplace for used PC hardware. 2x16gb ddr4 3600 (gskill) for $90. Ryzen 3600 for $35. Fractal ridge case for $80. Everything works and is like new. For larger purchases I'd still go new with warranty and I'd want receipts so that I can write off for tax purposes, but small stuff like this, if I can get it really cheap, is great to have.",pcmasterrace,2026-02-26 04:29:55,1
Intel,o57nbwt,"Strongly, strongly recommend just saving up a bit more and going RX 9060 XT 16GB, it's much stronger and will last you much longer than these other cards.",pcmasterrace,2026-02-13 18:58:20,4
Intel,o5a0x5k,"CPU overhead on Intel rules out the B580, the PCIE X8 + CPU overhead (to a lesser extent than Intel, but still much worse than AMD) rules out the 5050/5060/5060TI. So 9060XT, get the 16GB version if possible but 8GB is still somewhat okay.",pcmasterrace,2026-02-14 02:51:06,2
Intel,o57o0hb,Like the other comment but if it has to be between these 3 then the B580 is something that I would personally pick for myself,pcmasterrace,2026-02-13 19:01:35,2
Intel,o57vxej,I bought the 9060 XT 16GB late last year and I love it so far. Great performance for $470,pcmasterrace,2026-02-13 19:40:32,1
Intel,o57pb7x,B580 supremacy!,pcmasterrace,2026-02-13 19:07:54,0
Intel,o57o0qq,"to be honest  I was considering this as an option, but in my country it is already kinda expensive for a normal 9060xt (around 400$), the 16gb one is like 550$, that is why I am considering 8gb cards and the intel arc",pcmasterrace,2026-02-13 19:01:37,1
Intel,o5a0yk9,CPU overhead would kill it here.,pcmasterrace,2026-02-14 02:51:21,2
Intel,o57o96e,I heard that intel drivers are not really good that is why I am sceptical,pcmasterrace,2026-02-13 19:02:45,1
Intel,o5a14we,CPU overhead would kill it here.,pcmasterrace,2026-02-14 02:52:31,2
Intel,o5cgshh,I saw in some performance tests that the b580 is closer to the rtx5050 than the rtx 5060. The 12gb of vram are good but if the graphics card isn't that powerful what is the point ?,pcmasterrace,2026-02-14 14:44:27,2
Intel,o5awkrm,Go Intel then the performance gap isn't massive.,pcmasterrace,2026-02-14 06:54:04,0
Intel,o5avbad,Considering they're asking for GPU upgrade advice then it means their GPU is quite dated. It's better to have GPU overhead where they can upgrade the CPU later down the line.,pcmasterrace,2026-02-14 06:42:37,0
Intel,o57p2vy,It has gotten quite reliable nowadays. GN and I think hardware unboxed has revisited the driver sometime last year,pcmasterrace,2026-02-13 19:06:46,1
Intel,o58zixv,Battlemages drivers aren't as bad as it was with Arc. but however do consider that intels gpu driver days can potentially be limited down the line with intel and nvidias partnership in the future.,pcmasterrace,2026-02-13 23:00:59,1
Intel,o5a2f1h,I didn't consider this.,pcmasterrace,2026-02-14 03:00:52,1
Intel,o5axwpx,"9060XT is 30% or so faster in games that don't hit the vram limit, plus CPU overhead on the B580. It makes no sense unless you know you're playing games that will use over 8GB of vram.",pcmasterrace,2026-02-14 07:06:13,2
Intel,o5a137f,"Just a small thing, Arc Alchemist was the A series, Arc Battlemage is B series, the Arc name is like Radeon or GeForce, it stays no matter what.",pcmasterrace,2026-02-14 02:52:12,1
Intel,o5b8sxs,"I understand that but if the person is planning to keep them for a long term it's better to get one that has more headroom (not everyone is able to afford swapping hardware every few years let alone all in one go)  At the end of the day, it's the OP's calling since we don't know their circumstances. Where I live, this type of approach is more common since (modern) PCs are seen as a luxury still.",pcmasterrace,2026-02-14 08:50:17,0
Intel,o75fih9,which euro hellhole? cant make a pcpartpicker build without that little bit of info.,pcmasterrace,2026-02-24 15:28:48,1
Intel,o75q8e7,"you can probably get a 5600 with any B550 mobo and that Arc GPU for less than 500e.  EDIT: [https://be.pcpartpicker.com/list/DvKw8Q](https://be.pcpartpicker.com/list/DvKw8Q)  ideally this is what you want, definitely gonna be much stronger than what you currently have.",pcmasterrace,2026-02-24 16:17:44,1
Intel,o75gapq,The beautiful shitty country of Belgium,pcmasterrace,2026-02-24 15:32:28,1
Intel,o75l2lw,"[https://be.pcpartpicker.com/list/MrTq9C](https://be.pcpartpicker.com/list/MrTq9C)  low power cpu because your cooler is lame. honestly, maybe look for a used cpu. get more performance that way.",pcmasterrace,2026-02-24 15:54:22,1
Intel,o7fajbf,Pcpartpicker.com if you want to build one. NewEgg if you want a prebuilt,pcmasterrace,2026-02-26 00:03:45,1
Intel,o7880rd,The single channel ram would definitely have a effect on the fps  20-25% performance hit with single over dual channel ram,pcmasterrace,2026-02-24 23:12:23,1
Intel,o5hthpc,I had a similar problem with my 12600k when i was using a 5070 ti. The game is heavy on the cpu and with that combo there is already a cpu bottleneck. Increasing settings actually helped keep the fps more stable. But the only thing that really worked was frame generation. I bought a better cpu and now its a much better experience. My GPU can hit 99% usage even with DLSS on and my 1% lows dont dip as often.,pcmasterrace,2026-02-15 11:57:44,2
Intel,o5j71l0,Game just runs like shit 70-130fps on a 5800x3d/3080 for me lowest settings. Tons of input delay,pcmasterrace,2026-02-15 16:48:23,1
Intel,o5u9ec9,12th gen cpus severely underperform with ddr4 simple as that. Just need to look at 12400 gap here between ddr4 and ddr5 on arc raiders https://imgur.com/a/ouNqx6W,pcmasterrace,2026-02-17 09:48:59,1
Intel,o6ep600,"I have the same GPU but with an R7 9700x. Pretty sure I don't have a bottleneck. I play on high 1080p settings with FSR quality, and Im getting 90-130 in-game FPS. This is just outrageous. I saw someone on TikTok the other day using a 9060xt with a 12400f and was getting more fps than me.",pcmasterrace,2026-02-20 12:00:12,1
Intel,o5hxvgp,What cpu did you bought?,pcmasterrace,2026-02-15 12:33:57,1
Intel,o5jb55n,I dont have that much input delay. Only if I use frame generator.,pcmasterrace,2026-02-15 17:08:07,1
Intel,o5uddee,Do you have the numbers for 2k res?,pcmasterrace,2026-02-17 10:25:35,1
Intel,o5vn4zm,Might just wait for an update. I dont have that problem in bf6 versa. Both cpu and gpu are running around 80%. Frame generation gives to much latency.,pcmasterrace,2026-02-17 15:20:59,1
Intel,o5hzpub,9800x3d.  14700k should be more than enough though if you go that route.,pcmasterrace,2026-02-15 12:48:27,2
Intel,o5ujmyq,"2k or 200k res is pretty much irrelevant as long as you are not gpu bound. If someone else is getting similar fps with better system but same gpu you can achieve those too with the ram switch. So short answer, no, because there is no point anyone benchmarking at those for people who primarily care about achieving x (or as high) fps. If you are looking for an easy solution without changing anything, you can always consider using frame generation.",pcmasterrace,2026-02-17 11:20:58,1
Intel,o5i0pox,Okay. Gonna be when ram prices are going down.  im still on LGA 1700 motherboard.,pcmasterrace,2026-02-15 12:55:55,1
Intel,o5i1l9g,I'm still a bit uncomfortable recommending people get the 13th and 14th gen i7 and i9 CPUs knowing the bad history they've had. Maybe just save for AM5  OR  get a good i5 14th gen?,pcmasterrace,2026-02-15 13:02:17,1
Intel,o5ill40,"They only have a problem if you dont update your bios for the fix, or is there something new im not aware of?",pcmasterrace,2026-02-15 15:02:21,1
Intel,o5klbo9,"even with the BIOS fixes, some people are still having problems with their i9 cpus, at least from what I saw from some of the posts here.",pcmasterrace,2026-02-15 20:58:01,1
Intel,o732poa,So what's the total cost?,pcmasterrace,2026-02-24 05:01:46,261
Intel,o731ynb,Beware of the 12V Octopus high failure connector.,pcmasterrace,2026-02-24 04:56:20,121
Intel,o738wmg,Good PC but the chair looks uncomfortable to use.,pcmasterrace,2026-02-24 05:49:29,145
Intel,o73gy5f,You sir have clearly just thrown money at the situation with absolutely zero knowledge on what you're doing. Absolute madlad.,pcmasterrace,2026-02-24 06:57:13,114
Intel,o73fbx4,"From a 285k user, if this is supposed to be a gaming setup, the 265k is pretty much the same, since you really don't use the e-cores that much.  Also, the Proart is an odd choice for the motherboard, as there are better ones cheaper than that.",pcmasterrace,2026-02-24 06:43:06,30
Intel,o738gpm,Best gaming GPU and worst gaming CPU combo. Also wtf is that chair.,pcmasterrace,2026-02-24 05:45:58,121
Intel,o73emxz,i mean i hope you paid for all this months ago,pcmasterrace,2026-02-24 06:37:05,9
Intel,o73zm8f,"OP: ""I mostly play on my PS5""  Also OP: I just dropped 3k into the GPU  Well, you do you",pcmasterrace,2026-02-24 09:52:33,7
Intel,o72ygkg,That msi gaming trio OC is a beast! Cost like $5000 now on Newegg but I have the same card! Super quiet and looks great! JUST DON'T USE THE INCLUDED POWER OCTOPUS ADAPTER THAT COMES WITH THE CARD!! USE THE PSU 12V-2x6 instead.  https://preview.redd.it/mzqwqujoedlg1.jpeg?width=4000&format=pjpg&auto=webp&s=96906f5c95b7e54be0175c569f90fcd0c599e64e,pcmasterrace,2026-02-24 04:31:27,32
Intel,o73k89d,Iâ€™m more curious about the chair,pcmasterrace,2026-02-24 07:26:42,9
Intel,o73fzgr,"Genuinely confused about this build. So many things make little sense. If work was your primary goal, then 9950X3D is simply the best for that. 32GBs of ram makes 0 sense as well for work load, You will regret not getting 64gbs very soon. I know that I have been regretting not getting 64gbs. You can most definitely afford it as you got 5090.   If gaming is your goal, then 9800X3D/9850X3D is the best and getting an overpriced ProArt MOBO for Intel's CPU (which sucks by the way) once again makes no sense. If aesthetic is your thing, then why get noctua fans? Why not go fully Noctua as well?  You also got a fishtank case, but your build is genuinely going to look mad wild, because you are going to mix several different aesthetics. I really want to see the end build.   I mean congrats, but I'm so confused, considering you got 5090, which cost overall more than everything else in your build.   Also the hell is that chair lmfao, looks like a total gimmick and not as good as you probably think its going to be.",pcmasterrace,2026-02-24 06:48:48,37
Intel,o737cev,bro took out a mortage (obligatory comment)  good luck with your build!,pcmasterrace,2026-02-24 05:37:01,15
Intel,o745f38,"lol, wtf is that chair",pcmasterrace,2026-02-24 10:45:41,6
Intel,o73fdyy,"No budget left for a good Chair, Keyboard, AudioðŸ¥²",pcmasterrace,2026-02-24 06:43:36,15
Intel,o732k0e,Jealous. Havenâ€™t built since 2018 but Iâ€™m really getting the itch. Wish you the best getting it going!,pcmasterrace,2026-02-24 05:00:36,4
Intel,o737ztb,Minecraft only rig,pcmasterrace,2026-02-24 05:42:14,3
Intel,o745b6d,So whatâ€™s the story with that chair??,pcmasterrace,2026-02-24 10:44:42,4
Intel,o73n4f7,"Did they seriously call an external HDD a ""*Gaming Hub*""?",pcmasterrace,2026-02-24 07:53:23,7
Intel,o74m66d,Big money no taste.,pcmasterrace,2026-02-24 12:53:29,8
Intel,o7360yb,![gif](giphy|w2ldbBLfoB37AcqVem|downsized),pcmasterrace,2026-02-24 05:26:37,13
Intel,o73fjkw,"What will be your eye distance to the monitor? For 43â€, it must be a lot. Iâ€™ve recently bought a 31.5â€ 4k one, and I am still thinking about the 80 cm depth of my desk.",pcmasterrace,2026-02-24 06:44:56,3
Intel,o740sgt,WTF is that chair,pcmasterrace,2026-02-24 10:03:30,3
Intel,o742x2z,Interested to see a discussion on that chair. Canâ€™t be good,pcmasterrace,2026-02-24 10:22:50,3
Intel,o74yl3k,The wealthy use to impress with cars and homes. Now it's PC bling.,pcmasterrace,2026-02-24 14:04:20,3
Intel,o74z3w1,"OMG, it's very expensive!!! ðŸ˜³",pcmasterrace,2026-02-24 14:07:09,3
Intel,o750j5h,Rich fucker!! xD,pcmasterrace,2026-02-24 14:14:42,3
Intel,o752333,In this economy?,pcmasterrace,2026-02-24 14:22:46,3
Intel,o75e9e7,I feel like 5090 is the PC equvilent of the suburban person buying a huge diesel truck.,pcmasterrace,2026-02-24 15:22:55,3
Intel,o733l4w,how much did that cost?,pcmasterrace,2026-02-24 05:08:13,6
Intel,o73ngle,That keyboard is gross idc what anyone says,pcmasterrace,2026-02-24 07:56:30,5
Intel,o7372a9,"Intel cpu, wtf. Hot garbage.",pcmasterrace,2026-02-24 05:34:46,14
Intel,o73l4tc,Sweet!  Add a Wireview Pro II to that so the card doesn't die.,pcmasterrace,2026-02-24 07:35:05,2
Intel,o73lh9k,Pc para ofimÃ¡tica y poco mÃ¡s.,pcmasterrace,2026-02-24 07:38:21,2
Intel,o73xvbs,"Lol, bro just used a pc part randomized to build this.",pcmasterrace,2026-02-24 09:35:43,2
Intel,o74ccvr,What kind of job do you even do,pcmasterrace,2026-02-24 11:44:14,2
Intel,o74db7p,and about to melt.  jokes aside I hope you won't fall victim to the melting 12vhpwr cables,pcmasterrace,2026-02-24 11:51:29,2
Intel,o74fdx2,I have the money .. why i cant pull the triger ? Everytime i think about building one am powering up my ps5 pro and forget about it,pcmasterrace,2026-02-24 12:06:53,2
Intel,o74if6k,Hey big spender,pcmasterrace,2026-02-24 12:28:30,2
Intel,o74oe2u,Make sure to grab a compatible RTX series fire extinguisher.,pcmasterrace,2026-02-24 13:07:14,2
Intel,o74s3i0,If you're going that big dick then there should be a QD OLED display there for that 5090 imo.,pcmasterrace,2026-02-24 13:28:53,2
Intel,o754992,Good ol crucial. Never let's ya down,pcmasterrace,2026-02-24 14:34:02,2
Intel,o754igg,"Damn, dude bought a chair and all.",pcmasterrace,2026-02-24 14:35:22,2
Intel,o755w5q,Spent a lot for the housefire90 to just take it all.,pcmasterrace,2026-02-24 14:42:25,2
Intel,o758xgx,Cool PC to play stardew valley on in 4k,pcmasterrace,2026-02-24 14:57:28,2
Intel,o75954y,I love the choice of 8bitDo retro keyboard. I have the same.,pcmasterrace,2026-02-24 14:58:29,2
Intel,o75cuju,Everytime I see one of these pictures itâ€™s literally top of the line parts. How are yall affording these lmao,pcmasterrace,2026-02-24 15:16:19,2
Intel,o75jjwq,"Iâ€™m so confused, whatâ€™s that orange box? An external â€œgamingâ€ HDD? But why, thoughâ€¦",pcmasterrace,2026-02-24 15:47:30,2
Intel,o75jwlc,Great pc but man that chair looks HORRIBLE,pcmasterrace,2026-02-24 15:49:05,2
Intel,o75m6ys,A $7k build now that was $4k 18 months ago,pcmasterrace,2026-02-24 15:59:27,2
Intel,o76ye8k,"Best way to buy a computer chair: Go to the store after a long day at your desk, and sit in every chair one at a time. Find the one that hits the sweet spots that ach the most.",pcmasterrace,2026-02-24 19:35:43,2
Intel,o778cl0,HE'S GOT RAM!!!,pcmasterrace,2026-02-24 20:21:53,2
Intel,o73a1w2,Wtf is this keyboard bro?,pcmasterrace,2026-02-24 05:58:47,3
Intel,o75qbjm,This looks like what happens when you go to microcenter and they sell you on the most expensive parts imaginable because you can finance it... oh wait that might be exactly what happened here...,pcmasterrace,2026-02-24 16:18:07,2
Intel,o73txch,"I bought all the part before the start of the year except the CPU, monitor and keyboard. Iâ€™m a photographer but I dabble with video, music and CAD. I do game as well my first game will probably be Crimson Desert. Most gaming is done on the PS5. Will update when Iâ€™m done.",pcmasterrace,2026-02-24 08:57:38,2
Intel,o74bczk,"REEEEEEE WHY DIDNT YOU GET THE 9800X3D ITS LITERALLY THE ONLY CPU ANYONE SHOULD EVER BUY REEEEEEEEEEEEEEÃˆEEE!  There, I saved you half an hour of scrolling.",pcmasterrace,2026-02-24 11:36:21,3
Intel,o72xl4x,Hell yeah you are ðŸ¤˜ðŸ»,pcmasterrace,2026-02-24 04:25:23,2
Intel,o73kmeg,why the 2x8TB externals? 6TB of nvme not enough? You planning to download some linux isos? ;),pcmasterrace,2026-02-24 07:30:19,2
Intel,o73yqsm,meh,pcmasterrace,2026-02-24 09:44:12,2
Intel,o741lsd,2 yr salary right there,pcmasterrace,2026-02-24 10:10:57,2
Intel,o743av3,"Mans rich, in this economy?",pcmasterrace,2026-02-24 10:26:19,2
Intel,o74xt19,Intel and a ProArt motherboard... Doesn't seem like it's just for gaming.,pcmasterrace,2026-02-24 14:00:07,2
Intel,o757o19,Op fuck all these haters man. Looks awesome and enjoy it!!!,pcmasterrace,2026-02-24 14:51:17,2
Intel,o75dn4k,"Awful lot of folks seem to not be able to just say - wow congrats, enjoy.",pcmasterrace,2026-02-24 15:20:01,2
Intel,o73y690,Only 32 GB RAM?! This is ridiculous!,pcmasterrace,2026-02-24 09:38:41,1
Intel,o7399o5,20T of storage. That's a lot of pictures.,pcmasterrace,2026-02-24 05:52:25,1
Intel,o73a4y8,Have that exact same keyboard and love it,pcmasterrace,2026-02-24 05:59:28,1
Intel,o73qwwo,Hope you enjoy! Im almost finished with my first build now and the experience has been exciting and terrifying at the same time. Convinced im gonna break something.,pcmasterrace,2026-02-24 08:28:55,1
Intel,o748fic,Oof,pcmasterrace,2026-02-24 11:11:53,1
Intel,o748mm2,Looks like fun! I just got the same arctic cooler,pcmasterrace,2026-02-24 11:13:36,1
Intel,o74bb1e,I have that same keyboard and I love it!,pcmasterrace,2026-02-24 11:35:55,1
Intel,o74bi1p,Great time to buy,pcmasterrace,2026-02-24 11:37:29,1
Intel,o74d8us,Chair first,pcmasterrace,2026-02-24 11:51:00,1
Intel,o74ehrx,"Waiting for the inevitable ""my pin connector melted"" post lol.",pcmasterrace,2026-02-24 12:00:17,1
Intel,o74eqrz,5090 and only 32GB with 6tb SSD storage??? AND INTEL??? Get out of here!!! /s  Happy building mate! Go with the chair first so that your ass can have a good place to rest in between the build.,pcmasterrace,2026-02-24 12:02:08,1
Intel,o74ngkr,I got the same gpu! Youâ€™re going to love it!,pcmasterrace,2026-02-24 13:01:30,1
Intel,o74t0za,"I might be completely wrong on this, but there like 6k (give or take +-1k) on the table. But the small part we see from the room itself does not look as it would be a  mansion with expensive furniture.   It seems like the priorities where the money is spend is clearly set.   I might be in the wrong on the wooden table is some kind of rare wood and costs a fortune.",pcmasterrace,2026-02-24 13:34:10,1
Intel,o74tn4e,Dude just out here saying I'm a millionaire.,pcmasterrace,2026-02-24 13:37:33,1
Intel,o74tntv,Finally! A perfect 5090 build!!!,pcmasterrace,2026-02-24 13:37:40,1
Intel,o74u1pp,"Great choice on the AIO, and this GPU is the best looking this gen IMO. I have the same one, but 5080. Interesting choice on the CPU, been a while since i saw Intel on this sub.",pcmasterrace,2026-02-24 13:39:48,1
Intel,o74vw1r,"I'm gonna start saying this more often. While I know everyone is excited with a new build and it's a HELL of an investment, the more we keep glorifying how much money we are WILLING to spend on PC stuff, the insane price increases, independent of supply and Datacenters etc., is never going to come down to sane again.  Frankly I think we need some buying strikes for a few quarters.",pcmasterrace,2026-02-24 13:49:47,1
Intel,o753ra9,"We out here still buying crucial, after all theyve done to us... /s kinda",pcmasterrace,2026-02-24 14:31:27,1
Intel,o7541yn,That GPU is an absolute beast. Enjoy it dude! I'm in love with mine.,pcmasterrace,2026-02-24 14:32:59,1
Intel,o75endh,Have the same case. Pretty fun to build in.,pcmasterrace,2026-02-24 15:24:45,1
Intel,o75i4t5,The ones complaining about op spending money like this are the same ones that would rather type in reddit that there are no jobs then to go out and get one! Free will is a great thing and OP is using it to the max! Congrats on the build!,pcmasterrace,2026-02-24 15:40:59,1
Intel,o75ki9s,Damn retirement fund gone.,pcmasterrace,2026-02-24 15:51:50,1
Intel,o75pte1,10K in Canadian money easy dude must of sold a arm 3 kidney and left nut,pcmasterrace,2026-02-24 16:15:51,1
Intel,o75qrek,"prices dont matter, here is proof. Ram/storage/videocards could be 10x higher and we'll see still get pics like these.",pcmasterrace,2026-02-24 16:20:05,1
Intel,o75rahq,Exciting!,pcmasterrace,2026-02-24 16:22:30,1
Intel,o75v2hv,nice keyboard choice,pcmasterrace,2026-02-24 16:39:18,1
Intel,o75x4m5,Value must be a the deposit for a studio flat in NYC?,pcmasterrace,2026-02-24 16:48:27,1
Intel,o763tl4,https://preview.redd.it/haj7q0tj7hlg1.jpeg?width=600&format=pjpg&auto=webp&s=693309d87b6fc9451c3ffa23a0a0e8f8d1c12f54,pcmasterrace,2026-02-24 17:18:36,1
Intel,o768svl,"Damn, that is the exact same case I used for my build. I love it, hope you get the same enjoyment from it. I recommend using 140mm fans, they fill it in quite nicely!",pcmasterrace,2026-02-24 17:41:26,1
Intel,o76aym2,I think OP is trying to game in 8K,pcmasterrace,2026-02-24 17:51:05,1
Intel,o76dg04,"Congrats, nice keyboard,ðŸ˜‰",pcmasterrace,2026-02-24 18:02:05,1
Intel,o76dvq2,All that cash and you still went for intel cpu???,pcmasterrace,2026-02-24 18:04:03,1
Intel,o76j4kk,That's a good gaming chair,pcmasterrace,2026-02-24 18:27:09,1
Intel,o76jku5,Did you have to sell your house for that rig? Super jealous! You've got more money in RAM than I have in my 401k,pcmasterrace,2026-02-24 18:29:10,1
Intel,o76jml9,Just make sure the case is big enough for the 5090 and you set up your fans in/out good. It gets super hot. Heats up my room hot,pcmasterrace,2026-02-24 18:29:23,1
Intel,o76l1k4,So many haters in the comments,pcmasterrace,2026-02-24 18:35:42,1
Intel,o76lroy,Keeb alone is like $500,pcmasterrace,2026-02-24 18:38:55,1
Intel,o76mgb8,Man I wish I had more money than sense.,pcmasterrace,2026-02-24 18:41:54,1
Intel,o76rtrj,"Interesting choices I must say... Clearly it's ""no expenses spared"" and yet it's an Intel cpu/platform, 2 ssds, and a low end keyboard. May I ask why? ðŸ¤”",pcmasterrace,2026-02-24 19:05:37,1
Intel,o76vdf8,"Looks a lot like my build 3 years ago, youâ€™re gonna regret going with intel though. Intel runs too hot these days even with liquid cooling, how do I know? Because I have a liquid cooled intel chip and I regret buying intel. Theyâ€™re behind amd so they cheat to get performance by sacrificing thermals.",pcmasterrace,2026-02-24 19:21:49,1
Intel,o771c3m,I was hyped for you until I saw the intel boxâ€¦,pcmasterrace,2026-02-24 19:49:16,1
Intel,o77690y,Not the Backup UPS lol,pcmasterrace,2026-02-24 20:12:02,1
Intel,o777q84,Oh shit you got the duel channel ram. Look at this baller.,pcmasterrace,2026-02-24 20:18:57,1
Intel,o77890h,Make sure the 5090 power connectors are fully seated and enjoy your gaming!,pcmasterrace,2026-02-24 20:21:23,1
Intel,o77fhl1,https://preview.redd.it/50p4jne7ailg1.jpeg?width=734&format=pjpg&auto=webp&s=96807a1b3a276ec5ea67cca21552c6e162ef9883  Amazing build,pcmasterrace,2026-02-24 20:55:15,1
Intel,o77gtp0,The amount of ram is *Crucial* at this time  I'll let myself out,pcmasterrace,2026-02-24 21:01:22,1
Intel,o77lb97,My wallet...,pcmasterrace,2026-02-24 21:21:48,1
Intel,o784qru,Money isnâ€™t an issue.,pcmasterrace,2026-02-24 22:55:16,1
Intel,o787ci7,Lego for adults,pcmasterrace,2026-02-24 23:08:52,1
Intel,o78ikr6,Bro is about to make the best gaming chair ever,pcmasterrace,2026-02-25 00:10:27,1
Intel,o78jknx,Good build minus the Intel part. The thing is right now until has been struggling for a while and that platform may or may not be abandoned sooner. I would have gone AMD with maybe the next two or three years at least with AMD they have a very recent and proven track record. And some pretty cool things on the horizon if things pan out.,pcmasterrace,2026-02-25 00:15:54,1
Intel,o78mfnm,Talk about the worse possible time to build.... so far. Gratz,pcmasterrace,2026-02-25 00:31:10,1
Intel,o78nnhs,"Dang, Iâ€™m still stuck with my intel hd graphics.",pcmasterrace,2026-02-25 00:37:35,1
Intel,o78wh21,Are you an Arabian oil Prince or smt?,pcmasterrace,2026-02-25 01:26:21,1
Intel,o79790w,No new desk?,pcmasterrace,2026-02-25 02:27:52,1
Intel,o799acb,"...a cardboard fort?! Awesome, good luck!",pcmasterrace,2026-02-25 02:39:12,1
Intel,o79rs95,I hope you didnâ€™t forget the thermal paste,pcmasterrace,2026-02-25 04:30:23,1
Intel,o7b7qqf,"Not a bad way to spend $10,000",pcmasterrace,2026-02-25 11:52:41,1
Intel,o7crjaq,"May I ask why you got 2 of those gaming hub things? In addition to 2 2TB SSDs. Curious what your use case is, or if you just wanted it just cus",pcmasterrace,2026-02-25 16:50:40,1
Intel,o7dujyy,Is this for me?,pcmasterrace,2026-02-25 19:48:12,1
Intel,o74fnv3,"Top tier GPU, CPU, drives, motherboard, even a UPS!, and one of the most budget AIO's? Not bad performance, but there are better ones, just curious why that was the one thing you didnt go all out on?",pcmasterrace,2026-02-24 12:08:56,1
Intel,o739j4r,8bitdo keyboard lets go,pcmasterrace,2026-02-24 05:54:34,1
Intel,o73j2lg,"Well, hello there, Mr. Money Bags, sir.",pcmasterrace,2026-02-24 07:16:09,1
Intel,o74vew5,"HE HAS RAM, GET HIM!",pcmasterrace,2026-02-24 13:47:14,1
Intel,o7ajrtj,Pro art? Intel? Ok so we just throwing in bullshit I see,pcmasterrace,2026-02-25 08:17:25,1
Intel,o734p6p,![gif](giphy|9Hx2Jhutoccy75DzIm)  Well done. Post an after Pic too!,pcmasterrace,2026-02-24 05:16:26,0
Intel,o73bb51,If you wanna know what you could have saved money on we can start with hdd  5090 in the picture gonna guess you dont care tho,pcmasterrace,2026-02-24 06:08:59,-2
Intel,o73wh73,"Gaming trio, the worst GPU in the market",pcmasterrace,2026-02-24 09:22:16,0
Intel,o73ni9z,"He has 32gb 6400 mt/s DDR5 RAM!!!!! AND THAT TOO CRUCIAL!  CMON GUYS LES GO GET HIM, WE CAN SPLIT THE GAJILLION DOLLARS.",pcmasterrace,2026-02-24 07:56:56,0
Intel,o74svm4,"So much money for so many questionable choices, primarily an Intel chip?",pcmasterrace,2026-02-24 13:33:19,0
Intel,o73vu2e,Why Intel?,pcmasterrace,2026-02-24 09:16:05,-1
Intel,o73eay9,ia rig or what? why intel?,pcmasterrace,2026-02-24 06:34:11,-1
Intel,o737mxr,Are you from india ?,pcmasterrace,2026-02-24 05:39:22,-1
Intel,o73kmqn,![gif](giphy|ukGm72ZLZvYfS),pcmasterrace,2026-02-24 07:30:25,-1
Intel,o73p4fx,"I see you're in need of heating up your room some more with that Intel CPU, I know mine contributes greatly",pcmasterrace,2026-02-24 08:11:53,-1
Intel,o7302d2,Only 32gb of ram?,pcmasterrace,2026-02-24 04:42:47,-7
Intel,o73crtn,https://preview.redd.it/0mbmqn89ydlg1.jpeg?width=736&format=pjpg&auto=webp&s=34a8de3f94d22cbc8651fa6d74ebc24638dc4b1c,pcmasterrace,2026-02-24 06:21:10,0
Intel,o73cy72,How many kidney remaining ?,pcmasterrace,2026-02-24 06:22:39,0
Intel,o73hfa7,"https://preview.redd.it/4andrz675elg1.png?width=1200&format=png&auto=webp&s=ace7ba120a95650672c469779203285190e3b84c  Nah, my own system is good enough. It's me that is not good enough to finally tackle my backlog, even though my PC more than enough to beat all those games.",pcmasterrace,2026-02-24 07:01:20,0
Intel,o73psw4,HE HAS RAM!! GET HIM!!,pcmasterrace,2026-02-24 08:18:20,0
Intel,o73vkkd,"Judging by the components selection, streamer build confirmed.",pcmasterrace,2026-02-24 09:13:31,0
Intel,o7433kl,Nice setup,pcmasterrace,2026-02-24 10:24:30,0
Intel,o7445jr,![gif](giphy|w2ldbBLfoB37AcqVem|downsized),pcmasterrace,2026-02-24 10:34:12,0
Intel,o74gzvp,Good luck brother. Be sure to ground yourself so you dont have any ESD.,pcmasterrace,2026-02-24 12:18:34,0
Intel,o745kwi,Tell that your rich without telling your rich,pcmasterrace,2026-02-24 10:47:05,-1
Intel,o73cn11,Must be a millionaire,pcmasterrace,2026-02-24 06:20:02,-1
Intel,o738pnl,Is this how millionaires build their PC?,pcmasterrace,2026-02-24 05:47:57,-2
Intel,o73e3rg,This is a hard ass flex with no x3d.,pcmasterrace,2026-02-24 06:32:29,-3
Intel,o73aute,Kidney,pcmasterrace,2026-02-24 06:05:16,353
Intel,o739miy,~Radio silence~,pcmasterrace,2026-02-24 05:55:20,58
Intel,o73g5gv,I'll take a stab: everything in the picture?  At least $7000 USD.,pcmasterrace,2026-02-24 06:50:15,49
Intel,o74fpms,*Processing img umzgfz2doflg1...*,pcmasterrace,2026-02-24 12:09:17,16
Intel,o74v75s,In two months we will be like â€œwow there is like $30k in this pictureâ€,pcmasterrace,2026-02-24 13:46:04,4
Intel,o750ec9,![gif](giphy|wLXo0vTZSM7GU),pcmasterrace,2026-02-24 14:13:59,7
Intel,o76iliz,Whole body,pcmasterrace,2026-02-24 18:24:49,1
Intel,o77khlv,Yes,pcmasterrace,2026-02-24 21:18:03,1
Intel,o73zf16,Under bosses table for rest of life.,pcmasterrace,2026-02-24 09:50:40,1
Intel,o744dd3,"Their PSU is a modern 3.1 one, so they most likely got a 12vHPWR - 12vHPWR cable included with it.  It still could melt but I have seen way more melt-posts with the adapters.",pcmasterrace,2026-02-24 10:36:12,12
Intel,o73e3ig,"Blows budget on parts, cheaps out on one of the most important parts.",pcmasterrace,2026-02-24 06:32:25,94
Intel,o745bfq,I have a $15 office chair from Goodwill. A stack of bricks is more comfortable for me.,pcmasterrace,2026-02-24 10:44:46,6
Intel,o7523kr,Always get a high-quality office chair. I don't know why people buy these inferior products when there is one made exactly for our needs.,pcmasterrace,2026-02-24 14:22:50,5
Intel,o744gxz,Corners had to be cut,pcmasterrace,2026-02-24 10:37:06,3
Intel,o74us0u,Because it's in the box,pcmasterrace,2026-02-24 13:43:47,2
Intel,o75jxt7,Blows my mind they picked a chair without adjustable arm rests.,pcmasterrace,2026-02-24 15:49:14,2
Intel,o74hdbw,"What's wrong with the setup, are the parts incompatible?",pcmasterrace,2026-02-24 12:21:11,-8
Intel,o73i48c,"I agree that the difference isnt major between 265k and 285k. But, e cores are definitely used alot with core ultra. Turn off your e cores and see how much the performance tanks. The e cores in these chips are doing alot of heavy lifting right along side the p cores.   E cores on raptor lake didn't really do much for gaming though.",pcmasterrace,2026-02-24 07:07:31,7
Intel,o73pyb2,"if this is supposed to be a gaming setup, no point in buying intel cpu at all",pcmasterrace,2026-02-24 08:19:46,13
Intel,o76izme,I have Proart for my AMD rig; it was the only motherboard I could find with 2 PCIe 5 X16 lanes: one for the GPU and one for whatever else.,pcmasterrace,2026-02-24 18:26:32,1
Intel,o73g98f,Lol the chair is what threw me off,pcmasterrace,2026-02-24 06:51:09,18
Intel,o73hr7n,"https://preview.redd.it/h0po2r6y5elg1.jpeg?width=673&format=pjpg&auto=webp&s=ce888674192ddce5927c59f94f2e0c10f71531a2  I changed my cpu to AMD due to Reddit comments, and I don't even have that much time to game, lol",pcmasterrace,2026-02-24 07:04:17,7
Intel,o749qvb,Needs a Herman Miller to complete the setup.,pcmasterrace,2026-02-24 11:23:04,2
Intel,o746hfi,And $1000 on the mb?,pcmasterrace,2026-02-24 10:54:58,6
Intel,o74c801,Bought a great chair for helping with storing photos,pcmasterrace,2026-02-24 11:43:11,1
Intel,o735vnc,Wake the fuck up samuraiii,pcmasterrace,2026-02-24 05:25:28,22
Intel,o73b3hs,Is it really bad on using the 12vhpwr adapter (to 3x 8pin)?  Maybe only for 5090 or anywhere else as well?,pcmasterrace,2026-02-24 06:07:15,3
Intel,o74xc8a,what case is that?,pcmasterrace,2026-02-24 13:57:35,1
Intel,o738i5g,All that to live in an apartment.,pcmasterrace,2026-02-24 05:46:17,-12
Intel,o74f1nv,I was as well so I looked it up. It's a modular chair and I shit you not [THIS](https://b.kickoffpages.com/assets/114484/459fc06a-7f9f-4f9b-b1e8-936e744609f5/wrrkh2yutkpcfchgwy10/Increased%20%20Productivity.png) is one of the configurations.,pcmasterrace,2026-02-24 12:04:22,7
Intel,o73htfb,"I think it might be a mixed productivity build given the  number of SSD'S, but a bit of an odd choice to overpay for two external 8 TB HDD enclosures that would almost certainly be slower than internal drives.    Unless he was really set on this particular case which only has a mounting cage for 2x 2.5""/ 1 x 3.5"".",pcmasterrace,2026-02-24 07:04:50,9
Intel,o76r4q0,As much as OP spent on this I would guess living with 32gb for a few years is acceptable until pricing gets better.,pcmasterrace,2026-02-24 19:02:29,1
Intel,o76vi35,"I made that mistake when I first built my PC. I had a NZXT case with NZXT black case fans and a white iCue AIO with white fans. Two completely different fan styles, colors, and RGB patterns. I ended up spending a few hundred dollars to replace the NZXT fans with more Corsair fans so it all matched.",pcmasterrace,2026-02-24 19:22:24,1
Intel,o76zayp,"I didn't even see the chair, lmao. I hope they have really good posture already or they are going to turn into a shrimp.",pcmasterrace,2026-02-24 19:39:58,1
Intel,o75faoa,"32gb of ram is plenty for both gaming and work unless you do something very specific, no? I have had my pc for a year now and haven't found a need for more.",pcmasterrace,2026-02-24 15:27:47,1
Intel,o74af3l,That keyboard is pretty nice. I've typed on it,pcmasterrace,2026-02-24 11:28:38,1
Intel,o734oy5,Same my 9700k is still a beast but it feels like age got to it,pcmasterrace,2026-02-24 05:16:23,1
Intel,o75qm8f,"Everyone going on about the chair, 16tb of external HDD â€œgaming hubâ€ is what got me too",pcmasterrace,2026-02-24 16:19:27,1
Intel,o770o4k,RGB = Gaming. Duh,pcmasterrace,2026-02-24 19:46:12,1
Intel,o74tc8r,"Kickstarter, I have other chairs just caught the early bird and wanted to try something different on that one.",pcmasterrace,2026-02-24 13:35:52,1
Intel,o74hk3m,This has to be at least 6-7k,pcmasterrace,2026-02-24 12:22:30,1
Intel,o74dxy3,![gif](giphy|fxZ7cC3zYIVXi),pcmasterrace,2026-02-24 11:56:10,1
Intel,o74huoa,![gif](giphy|wLXo0vTZSM7GU),pcmasterrace,2026-02-24 12:24:33,1
Intel,o74sv7p,I got the 5090 for only 2500. I got it before the shortage.,pcmasterrace,2026-02-24 13:33:16,1
Intel,o76xyog,Hell it was 4k just 6 months ago,pcmasterrace,2026-02-24 19:33:43,2
Intel,o73g3w4,"the keyboard is actually goated, probably the best decision in his build, well, the 5090 is a good one too",pcmasterrace,2026-02-24 06:49:52,-2
Intel,o73fkgn,8bitdo retro mechanical keyboard.,pcmasterrace,2026-02-24 06:45:09,-1
Intel,o75077h,Donâ€™t listen to the haters! Itâ€™s a dope build.,pcmasterrace,2026-02-24 14:12:56,4
Intel,o7ajmja,â€œWhy get a 5090 when you can get a 5080!â€ Thatâ€™s what you are saying,pcmasterrace,2026-02-25 08:16:03,-1
Intel,o74v2wy,"Iâ€™m getting a NAS for storage consolidation, I have  too many separate hard drives and ssds",pcmasterrace,2026-02-24 13:45:26,1
Intel,o74vibf,Not even half of it.,pcmasterrace,2026-02-24 13:47:45,-1
Intel,o73hhme,I have the C64 version. Replaced the switches tho. Kahil blacks.,pcmasterrace,2026-02-24 07:01:54,2
Intel,o74r70v,"After researching the more expensive ones still didnâ€™t put out that much more cooling, it was between them Thermalright or Corsair.",pcmasterrace,2026-02-24 13:23:44,1
Intel,o74rnww,Lol no it isn't.,pcmasterrace,2026-02-24 13:26:25,0
Intel,o749fo8,"This is definitely an outlier here, these parts are pretty unique in that they are absolutely moronic.  So much wasted money and weird combinations.  Could have spent less and gotten a faster PC.",pcmasterrace,2026-02-24 11:20:26,5
Intel,o7ajvf1,"Quite the opposite, the cpu mobo combo is actually not optimal choice neither the most common",pcmasterrace,2026-02-25 08:18:21,1
Intel,o737b4q,That Intel CPU can't handle any more,pcmasterrace,2026-02-24 05:36:43,0
Intel,o746e68,They spent all the money on the computer.,pcmasterrace,2026-02-24 10:54:10,0
Intel,o73g5hh,millionaires? more like people who have no clue what they are doing,pcmasterrace,2026-02-24 06:50:15,9
Intel,o73ri9x,"X3D isnâ€™t the be all and end all you know. I have a 14900k and a 9800X3D. If I can ever be bothered Iâ€™ll do tuned benchmarks of both systems, most you you people on here would be shocked by the results.",pcmasterrace,2026-02-24 08:34:38,1
Intel,o73c5j8,Both,pcmasterrace,2026-02-24 06:15:59,173
Intel,o73nkph,And prosthesis,pcmasterrace,2026-02-24 07:57:33,8
Intel,o74tpow,Cheap kidney nowadays /s,pcmasterrace,2026-02-24 13:37:57,5
Intel,o74z88b,kidneys are cheap nowadays it cost balls...,pcmasterrace,2026-02-24 14:07:48,4
Intel,o76hk4a,"Can confirm. Some people budget, and some people just click ""add to cart""",pcmasterrace,2026-02-24 18:20:14,2
Intel,o73l03i,I guess at least $9000 USD,pcmasterrace,2026-02-24 07:33:51,31
Intel,o75znu1,"Mine 4-5 months ago --> vs now   5090 FE $2,000 --> $3,600 (lowest in stock 5090)   96gb Corsair DDR5 6000 CL30 $300  --> $1,500   Samsung 4TB 9100 Pro $330 -->865    That's $2,630 vs $5,965 ($3,335 more!!)",pcmasterrace,2026-02-24 16:59:41,10
Intel,o74wipv,"Even if their PSU has up to date features the risk is still there and a LOT higher than on the good old, reliable 8 pin. Also that connector is rated for 600W and yet the 9070XT with an around 300W (depending on model, overclock, load could be more) also melts it. I really can't understand people trying to defend it. It caused so much damage for people. Just because greedvidia saves a bit of money on each card",pcmasterrace,2026-02-24 13:53:10,5
Intel,o73jev2,"I looked it up, and it's a $400 chair. Looks gimmicky asf but I can kinda see the practicality, BUT NOT AS AN OFFICE CHAIR, so OP will have to report back to us for that one.",pcmasterrace,2026-02-24 07:19:16,33
Intel,o74ysok,"I wouldn't say $400 on a chair is ""cutting corners"" ....   Maybe compared to an HM Embody..   But that's getting solidly into the lower mid-range region at least.",pcmasterrace,2026-02-24 14:05:29,0
Intel,o75l7ug,https://preview.redd.it/sjxvixjmsglg1.jpeg?width=4000&format=pjpg&auto=webp&s=677aaa589b346f6bc686246a5ebb1120cedaa4b2  Just did a search. The arms can fold down for petðŸ™ƒ. I don't like the back being too short but it is kind of fun for dog owners.,pcmasterrace,2026-02-24 15:55:02,0
Intel,o74qpiw,"For example why do he need an external back up power supply? That thing cost 300 Euro and is a 100% waste. This is Like paying Vulcano insurance for a doll house, or a garage for an hot wheels car",pcmasterrace,2026-02-24 13:20:55,-19
Intel,o73ix1c,"It's still not much of a difference between either of them, as both do have the same p-cores and only a difference of 4 e-cores. It affects core ultra more due to the P-cores not having hyperthreading tho, that is true, but the performance bottleneck on these cpus is more on the memory subsystem and not the raw performance of the cores themselves.  So for gaming, 265k is a more sensible choice, especially with the pricetag.",pcmasterrace,2026-02-24 07:14:44,2
Intel,o73uck2,"If it's supposed to run at 4k tho, it's irrelevant.",pcmasterrace,2026-02-24 09:01:43,-10
Intel,o73ylnp,wdym you had a 285K before ?,pcmasterrace,2026-02-24 09:42:50,3
Intel,o73msbx,lmao,pcmasterrace,2026-02-24 07:50:20,-2
Intel,o73a7yy,We have a power connector to burn,pcmasterrace,2026-02-24 06:00:09,30
Intel,o73bpac,"So the issue is that the adapters have less tolerance to bending. That and they have more points of failure.  The 12V-2x6 that comes with ATX 3.1 certified PSUs are more flexible and have less points of failure: both on the GPU side and PSU side of the plug. Less points of failure = less chance of your cable melting.   Point is you want to eliminate as many possible points where a plug can melt due to a bend, or bad insertion.",pcmasterrace,2026-02-24 06:12:12,4
Intel,o74xpn8,NZXT H9 Flow RGB  2025 version.,pcmasterrace,2026-02-24 13:59:37,1
Intel,o738u2s,He'd probaly need atleast 10 of those pcs worth to even put a down payment on a half decent starter home in a okayish area. Maybe 5 if you wanna live in a 250 square foot house where you get stabbed as soon as you walk outside or live in bumfuck and have to drive 40 minutes just for the closest convenience store.,pcmasterrace,2026-02-24 05:48:54,4
Intel,o73axzb,"Dude, it was not our problem, the economy fucks us so bad lolol",pcmasterrace,2026-02-24 06:06:00,2
Intel,o738oi2,Awe....you mad? Lol...don't care to live in a house RN even though I could.   But by all means keep getting triggered. I love that for you!,pcmasterrace,2026-02-24 05:47:42,-2
Intel,o74i8rb,"â€œI shit you notâ€  https://preview.redd.it/itdlancfrflg1.jpeg?width=720&format=pjpg&auto=webp&s=f406bdc9691ea1d5c2ef110e8f9b8432a8240df9  Jokes asides I would totally do that, not for productivity but for my fatass to eat my DoorDash meal",pcmasterrace,2026-02-24 12:27:15,6
Intel,o73mauv,"I think it's more of a case ""I'm different than everyone else and my choices are better, because they are different"". If mixed productivity was his goal, then 9950X3D is still the best and there's also a ProArt AM5 board as well. 32GBs of ram still remain the oddest choice to me, because it does not fit the overall productivity aesthetic of this build, but I guess, paying 1000 dollars for 64gb might have been too much, which is fully understandable.   There'a also the fact he got a fish tank case, but let's face it, noctua fans might be amazing, but the redux are weirdly grey and aren't as good as regular noctua fans anyway. They are 140s, while his AIO is 360. It's just gonna look weird. He got 0 RGB, but went with grey fans for a blacked-out build?   I would not criticise any of it if he didn't get the 5090, which he probably paid 3500-4000 dollars for. The rest of his build isn't even half of his GPU.   It will work great overall IMO, just could have gotten better for the money spent.",pcmasterrace,2026-02-24 07:45:58,8
Intel,o74gm51,Buying a mechanical keyboard on a 5090 budget is still weird. Same for the chair. Its obvious the 5090 was not truly in the budget and other parts suffered to make room for it,pcmasterrace,2026-02-24 12:15:51,-4
Intel,o760xnz,I was thinking the same thing. I was wondering if anyone else questioned it.,pcmasterrace,2026-02-24 17:05:28,1
Intel,o7516ra,really? A 5090 costs 5090,pcmasterrace,2026-02-24 14:18:06,1
Intel,o74tlpt,Sell the display. Get QD OLED lol do it!,pcmasterrace,2026-02-24 13:37:20,1
Intel,o7epam6,![gif](giphy|qmfpjpAT2fJRK),pcmasterrace,2026-02-25 22:11:50,1
Intel,o755tky,"Gotcha, downvotes people think i'm hating on the arctic, i'm not, just usually builds like this people min/max on everything, expected a pano360 or kraken360. But yeah the 360 AIO's are all pretty similar.",pcmasterrace,2026-02-24 14:42:02,1
Intel,o74w8nx,Oh I can assure you it is,pcmasterrace,2026-02-24 13:51:40,0
Intel,o73rpes,Haha what? Intel has better ram compatibility by far.,pcmasterrace,2026-02-24 08:36:32,2
Intel,o73ir2k,Intel has much better memory support than any AMD chip. AMD cant even run CUdimm ram. So im not sure what you're on about here?,pcmasterrace,2026-02-24 07:13:16,1
Intel,o745wpu,All they really should expect is maybe a single digit percent increase in fps but much better 1% Iows and better frame pacing. The numbers don't really do the difference justice imo.,pcmasterrace,2026-02-24 10:49:57,2
Intel,o73cgkc,"1 for 5090, another 1 for the rams/ssds",pcmasterrace,2026-02-24 06:18:34,72
Intel,o73emuk,All 3,pcmasterrace,2026-02-24 06:37:04,21
Intel,o74f9rs,![gif](giphy|MvedbKot538WY),pcmasterrace,2026-02-24 12:06:03,43
Intel,o7575am,"Yes, but what I was responding to was the â€12V Octopusâ€ -part, which refers to the adapters lol.  3x 8pin is absolutely the way to go on GPUâ€™s like the 9070XT that actually give you the option to have it instead of the 12vHPWR.",pcmasterrace,2026-02-24 14:48:41,2
Intel,o75fo8x,I have one word for OP: MARKUS,pcmasterrace,2026-02-24 15:29:32,1
Intel,o76gkw3,"Iâ€™m a steel case, leap man myself. I do like Crandall office supply for stuff like that. My current leap was built in 2007. I had it reupholstered last year. I plan to get another 20 years out of it.",pcmasterrace,2026-02-24 18:15:56,1
Intel,o74uoap,"Wait, UPS are a ""waste"" ? If you have a super expensive system like this, how is it a waste?",pcmasterrace,2026-02-24 13:43:13,20
Intel,o74z7jj,"A back up UPS thatâ€™s got enough juice for your rig will ensure you have a clean sin wave and consistent clean power to your rig. People often wonder why parts in their computers go bad, typically itâ€™s from dirty electricity.   Iâ€™ve been running UPSâ€™s on all my build for the last 15 years and I have never once had a single component failure. Also, most of the UPSâ€™s include insurance.   $300 to protect your $7000 PC seems like a no-brainer to me.",pcmasterrace,2026-02-24 14:07:42,8
Intel,o751641,A UPS is a waste? Hahahahahahaha,pcmasterrace,2026-02-24 14:18:01,3
Intel,o75iqzo,"You clearly are misinformed on what a UPS does. I have had a PC killed from a lightning storm when I wasn't home. Just the pure pain of trying to recover data, and failing is now worth buying a UPS.",pcmasterrace,2026-02-24 15:43:49,2
Intel,o76kx2p,"Wait you critique the UPS but not the 16 tb of mechanical hard drive being branded as ""game hub""  UPS is needed especially depending on where you live.",pcmasterrace,2026-02-24 18:35:08,1
Intel,o76z2ri,"Idk out of everything to point out in this build, the external UPS shade was not what I was expecting. I would recommend one to anyone and everyone.",pcmasterrace,2026-02-24 19:38:55,1
Intel,o73j6pb,"Sorry if my comment didn't make it clear. I absolutely agree with you. The 265k is a much better buy in any use case. The price difference is too large to justify it.   I was just simply pointing out that the e cores do alot for the architecture.   Once you tune these chips, they can actually rip pretty hard",pcmasterrace,2026-02-24 07:17:12,2
Intel,o74f3hy,The ultras are just weird man. Even I regret my CPUâ€˜s purchase.Â   Shouldâ€™ve gone with AMD tbh,pcmasterrace,2026-02-24 12:04:44,2
Intel,o741kht,"This is a screenshot of an order from a shop, which you can modify before they complete your order (gather all the stuff). At first this order had a core 9 ultra 285K and a Z890 Aorus Master motherboard, but I requested a change and they accepted.",pcmasterrace,2026-02-24 10:10:37,1
Intel,o73be99,Fuck I laughed too much at that,pcmasterrace,2026-02-24 06:09:41,6
Intel,o73v1ac,Thanks for making my day! I really needed it!,pcmasterrace,2026-02-24 09:08:22,2
Intel,o73cdv2,"Hmm, I am kinda dumb when I first try to use this 450w 12vhpwr cable from my new MSI mag psu (with that yellow tip for visual guide)  But it never booted my gpu, little did I know, the wire flexed and pulled the sensor pins (the tiny 4 pins above the 2x6) so now I just use the adapter from my gpu in which just works, altho kinda scared what would happen, altho I only have 5070ti  And recently JayzTwoCents made a video that an adapter in good quality is better than the 12vhpwr cables, atleast, in his words, to eliminate the issue by 50% (because we can only see one port with it)  Anyways, I pray for the day would not come that itll burn my gpu, I am thinking if I should buy an aftermarket 12vhpwr cable, like those cablemods?",pcmasterrace,2026-02-24 06:17:56,1
Intel,o73xk22,Is this as much of an issue on 5080's or more problematic on the 5090 because of the much higher power draw?   I only ask as I've used the squid connector on my first build. Was under the impression that if something does go wrong and you were using the cable supplied the warranty would be easier to claim on.,pcmasterrace,2026-02-24 09:32:42,1
Intel,o74hnpd,Why would you specifically need an expensive keyboard if you have a high end card?,pcmasterrace,2026-02-24 12:23:12,2
Intel,o753t45,What keyboard would you have in mind if not a mechanical one?,pcmasterrace,2026-02-24 14:31:42,1
Intel,o75iqjz,"Hall effect doesn't automatically mean a better keyboard. The typing experience on an HE is significantly worse than a mechanical with tactile switches. Considering the build, it looks like a mix of productivity and gaming and I'd never sacrifice my typing experience for a 5% advantage in games with an HE keyboard.",pcmasterrace,2026-02-24 15:43:46,0
Intel,o757631,"I didnâ€™t spend a much as people think the GPU cost me 2500, CPU 500, Ram 220, NVMEs 225 + 189, motherboard 500, Case 100, keyboard 60, monitor 350, Hdd 150, and chair 100",pcmasterrace,2026-02-24 14:48:48,-2
Intel,o753e39,"That's what I thought...no proof. Just more ""trust me bro"" BS. GTFO with all that...",pcmasterrace,2026-02-24 14:29:31,1
Intel,o74wkhk,What data do you have to back up that claim?,pcmasterrace,2026-02-24 13:53:26,0
Intel,o73rrno,These AMD cultists are actually really insane. It almost makes me not even want to own AMD CPUâ€™s.,pcmasterrace,2026-02-24 08:37:07,1
Intel,o746d3x,"Yeah, the 14900k has better 1% lows than the 9800X3D when fully tuned.",pcmasterrace,2026-02-24 10:53:54,2
Intel,o764ivr,"Dated, horrible armrests, cushioning dies in 1 year, lumbar support is non existent for most users.    Used haworth fern or Herman Miller liberate is the way to go",pcmasterrace,2026-02-24 17:21:48,4
Intel,o74zdm0,They are not a waste! They stabilize your electricity and provide your computer with good clean consistent power. Itâ€™s a smart move to protect your investment.,pcmasterrace,2026-02-24 14:08:36,13
Intel,o750ex0,"It's not..  Some people just take things to extremes. ANYTHING that doesn't *directly* increase fps us seen as stupid and a waste of money..   I haven't run my main rig on straight wall power in 20+ years. Could I have gotten a Slightly better ""x""? Sure, but I have different priorities than other users.   I work from my pc, losing progress or potentially corrupting a blender/ unity file means loss of money, missing a deadline, or even losing a contract entirely.",pcmasterrace,2026-02-24 14:14:05,5
Intel,o74xzmz,Idk  Apparently some people get power outages  I have had exactly one outage in the 4 years that Iâ€™ve lived here. It did kill my PS3 sadly,pcmasterrace,2026-02-24 14:01:07,5
Intel,o750qee,"I don't know how expensive has something todo with UPS. Ups ist for resiliency, not insurance. The real deal question is rather, do you have anything that needs to run 24/7 and if that collapse, does the world around you also collapse? If not, waste of money. If yes, buy one. But I couldn't imagine any use case for a private person needing a ups.",pcmasterrace,2026-02-24 14:15:44,-3
Intel,o758fzz,"To be fair, the ""clean power"" side of an UPS is pretty misunderstood. The AVR boost and buck doesn't mean clean power.  Modern PSUs have a lot more, and better filtering than almost any UPS. Also a UPS big enough for say, your system that also carries a real, non-stepped sine wave is very expensive.",pcmasterrace,2026-02-24 14:55:05,1
Intel,o77x6tb,Isn't it better to just get a surge protector for your entire house though? It also protects everything in your home instead of just one device.,pcmasterrace,2026-02-24 22:17:17,1
Intel,o75vh3v,"I can say that I totally forgot that there are places less Safe structured as where I live. Where I live, buildings are secured and we have protective contact mandatory, so this feature gets pretty irrelevant and expensive here, mostly a well protected PSU is good enough here. The last PC that was fried by lightning/Spike I heard from is almost 20 years ago, and I worked for a PC service center between 2009-2022. Germany has an average outage time of 14min per Year. We rarely need them to level our power net. I agree to use them in places where your power could easily drop out or the electric protection is poorly.  But yeah, from another pov your right, the 100% waste was a bit over the top",pcmasterrace,2026-02-24 16:41:06,-2
Intel,o73kdb4,"Ah, understood.",pcmasterrace,2026-02-24 07:28:02,1
Intel,o74poka,"I bought mine more because of the whole package than just the cpu. 6 nvme slots and still able to use TB4 and no lane sharing on the gpu slot is something you can't do that well on AMD, and the x8 DMI (quite literally 8 pcie4 lanes) on the Z890 does leave a little bit more of headroom for everything on the chipset.  AMD does have the better cpus tho, that is undeniable and chipset wise, the B850 is better than the B860 if only for how much you can tune the cpu, but the Z890 is quite a bit more flexible than anything AMD has.",pcmasterrace,2026-02-24 13:14:59,4
Intel,o74je1a,Good lad.,pcmasterrace,2026-02-24 12:35:10,-4
Intel,o73brq1,I gotta admit it was pretty damn funny! ðŸ˜†,pcmasterrace,2026-02-24 06:12:45,2
Intel,o73d8w1,"Jayz2Cents is really not a good person to take tech advice from. He has been wrong many times. I'm not saying he is shit, but he is also not the best.   I also think you might be mistaken on a few things: you have a 5070TI, which draws DRASTICALLY less power than a 5090/4090...so the 12vhpwhr/12V-2x6 won't suffer melting for you. Rather the reason your GPU didn't power up was just like you said...not inserted all the way. Because your card draws less power....using the adapter is safer for you.   If you were using a 4090/5090 I'd recommend against using the adapter.",pcmasterrace,2026-02-24 06:25:10,5
Intel,o74plkx,Wouldn't worry about it on the 5080.,pcmasterrace,2026-02-24 13:14:30,1
Intel,o74hw48,"Why do you need a nice steering wheel in a 180k Porsche? Just use any shitty wheel.  Also HE is not expensive anymore if you dont go wooting, which i do suggest tho",pcmasterrace,2026-02-24 12:24:50,-5
Intel,o75eynn,Hall Effect,pcmasterrace,2026-02-24 15:26:14,1
Intel,o76cj8i,"I have tried MANY keyboards, many switches, optical, mechanical, and i just got the Wooting 60HE V2 and it is the best keyboard i have ever used. Typing is amazing, being able to lubricate switches to customize your feel and sound is amazing, you can swap out springs to change the force, you can change the dampening.  Most importantly: you choose the actuation point between 0.1 and 4mm. It absolutely is objectively better if you care.",pcmasterrace,2026-02-24 17:58:04,0
Intel,o75my2j,Bruh thatâ€™s $4894. You must have a lot of expendable money if $5k isnâ€™t shit to you lol,pcmasterrace,2026-02-24 16:02:53,3
Intel,o75dqc7,that ram is a good deal,pcmasterrace,2026-02-24 15:20:26,1
Intel,o78q0vt,I have one,pcmasterrace,2026-02-25 00:50:15,1
Intel,o75json,"You notice half the comments on this post are asking why he didn't get an AMD x3d cpu? They have zero idea what OP is using this build for. But it doesn't matter. If you dont buy an x3d cpu, then its utterly trash. The AMD cultist are in full force for sure.",pcmasterrace,2026-02-24 15:48:36,2
Intel,o7ak46b,Same way burning chips makes you not want intel cpus? Oh wait,pcmasterrace,2026-02-25 08:20:40,-1
Intel,o74ra9g,That would not surprise me.  I went from a 13700k + Dark Hero z790 setup to a 9800x3d MSI MPG TI z870e and my synthetic benchmark scores went down and so did some of my games... HOWEVER.... for VR games my performance is better for the 1% lows (very notably) so I am happy with the  change as I only play VR games 90% of the time.  This is using  the same ram (team xtreem 48gb cl34 7200 on intel and tuned to 6200 cl30 on ryzen).,pcmasterrace,2026-02-24 13:24:15,1
Intel,o768hzi,"I've had mine for at least 3 years and the cushioning is fine ðŸ¤·ðŸ¼â€â™€ï¸ ... I wouldn't make the recommendation if my experience wasn't good ðŸ˜…  That being said, nothing wrong with essentially playing devil's advocate.",pcmasterrace,2026-02-24 17:40:03,1
Intel,o754ony,"So in your personal experience, an UPS is smart choice.",pcmasterrace,2026-02-24 14:36:15,7
Intel,o755lfm,"Ah, you're just uninformed then.",pcmasterrace,2026-02-24 14:40:53,4
Intel,o76gfen,"Not sure why you're getting downvoted, because you're not wrong, as long as there's a high quality PSU in the build, and high quality surge protection is used, and power is properly grounded and the subject in question is not concerned with data loss.  Waste of money is subjective - it's peace of mind. I've always used surge protectors and test them regularly. They do just what they're named for - surge protection. A top-tier PSU can mitigate brownouts.  I think the exception to this would be primarily for those who live in an area prone to power issues. In that case, a UPS is a no-brainer.",pcmasterrace,2026-02-24 18:15:16,0
Intel,o75aqpa,"Well, all I can tell you is that I have 2 1500 watt UPSâ€™s one for my home build and one for my work office. My office has the dirtiest electricity Iâ€™ve ever seen. Power dips and flickers all day from large equipments nearby and it has kept my 4090/i9 build safe and sound all these years. My home computer would cost nearly $8000 to replace today so buying a $300 UPS is worth every penny if it protects my PC even 5% better than without it",pcmasterrace,2026-02-24 15:06:17,1
Intel,o78fhdl,"Maybe they will trip? but when? I have personally seen electrical storms that don't take out the power right away if at all. I have gotten some serious under voltages and over voltages before the power goes out. Also, have you ever been near to a transformer that grenades? It didn't trip any of my surge protectors, but man did the lights go dim, bright, dim, bright before it exploded.  A really good UPS will have surge protection built in, and also feature Automatic Voltage Regulation, and will cut incoming power issues before a surge protector alone would trip. I can set mine to a narrow band of acceptable voltage before it regulates it.",pcmasterrace,2026-02-24 23:53:14,1
Intel,o74xjwi,"Based knowledge, my brother.Â   Personally, I donâ€™t really care about having the ability to use more nvme or other things. Besides, my gpu is vertically mounted, so I canâ€™t even use my other pcie slots lmao.Â   I got two nvme ssds, and three 2TBÂ 870 EVOs in soft Raid 0. I luuuuv it so much. Also a 2.5 Gbit/s connected NAS.",pcmasterrace,2026-02-24 13:58:45,5
Intel,o73mnef,"Yeah I agree, I always watch many creators and go back and forth in communities and forums to ask about my concerns, it is just one side from his lol  \>I also think you might be mistaken on a few things: you have a 5070TI, which draws DRASTICALLY less power than a 5090/4090...so the 12vhpwhr/12V-2x6 won't suffer melting for you. Rather the reason your GPU didn't power up was just like you said...not inserted all the way. Because your card draws less power....using the adapter is safer for you.     that's what I meant, 5070ti can only draw 300w (some can go beyond, mine did 330w) and that is reassuring, I already know 300w difference from 5090 and my card is too drastic, just want to be sure lol  anyways, I really dislike how finesse I have to be for that 12vhpwr cable that it just flexed even with 1 degree angle and not even close to the plastic when I did that, too delicate just to plug the power, the 4 pin actually was pulled, and I assume it is already broken, but easily fixable if I can refit it with the plastic  anyways, thank you",pcmasterrace,2026-02-24 07:49:05,2
Intel,o77y8q3,Haven't all the melting cables been from the 12V-2x6 cables?  The adapter is completely fine to use.,pcmasterrace,2026-02-24 22:22:29,1
Intel,o75nllu,"Didn't know Hall effect tech was used in keyboards. Then again, i also can't afford a 5090 lmao",pcmasterrace,2026-02-24 16:05:49,1
Intel,o7703hx,"Not worth it if you aren't gaming, imo. And honestly if you are just typing, traditional switches are better as you have 10x the variety of switch feel. If he's gaming though, yeah it's a fuckin no brainer. I grabbed the Nuphy Air 75 HE and it's ridiculous for gaming lol.    (I like a low profile or I would have snatched the Wooting 80HE. )",pcmasterrace,2026-02-24 19:43:35,1
Intel,o76j2vq,"Adjustable actuation points don't matter when there's no tactility whatsoever. HE keyboards are effectively adjustable red switches which have never been good for typing. They're noisy, rattle-y, and don't offer a good typing experience if you spend 8+ hours a day writing emails or code. I've tried wooting and razer HE keyboards and they are horrible for everyday typing. They're amazing for games, but I'm not going to have 2 keyboards on my desk when I spend most of my time typing at work. My custom keychron with gateron azure dragon switches blows every HE away when it comes to getting actual work done and does a great job in high ELO CS2.",pcmasterrace,2026-02-24 18:26:56,0
Intel,o75nib7,I didnâ€™t say it nothing but it was a long time coming. And itâ€™s gonna make me money.,pcmasterrace,2026-02-24 16:05:23,1
Intel,o78qazg,I thought so. None.,pcmasterrace,2026-02-25 00:51:46,0
Intel,o7al4uf,They only degrade if you donâ€™t know what youâ€™re doing. Mines delidded running at 1.22V.,pcmasterrace,2026-02-25 08:30:15,1
Intel,o74zhle,"Iâ€™ve never played any VR games, but I have heard that they really benefit from the 3D cache.",pcmasterrace,2026-02-24 14:09:12,1
Intel,o76adgf,ive had the markus for almost a decade now,pcmasterrace,2026-02-24 17:48:31,2
Intel,o766a7d,Iâ€™m not putting a UPS on my PS3 lmao  And that one power outage in 4 years was rare enough that people still talked about it 3 weeks later  Iâ€™ll survive,pcmasterrace,2026-02-24 17:29:51,1
Intel,o78sz8l,"You're generally correct, however the subjectivity of what's a waste for peace of mind scales down when the price of what you're protecting goes up.",pcmasterrace,2026-02-25 01:06:27,1
Intel,o75c0ik,"Yeah, especially if you know you have bad power, it's a no brainer.",pcmasterrace,2026-02-24 15:12:24,2
Intel,o7awnwg,"I have never experienced what you described, i've experienced issues/outages maybe 3-4 times in the 30 years i'ved lived in my city. Only one time during a thunderstorm did the surge protector get used and it was very easy to replace.  Unless you live somewhere with weak infrastructure experiencing frequent outages I don't really see the point in an UPS. It's not going to protect you better than the industrial surge protectors would for your entire home.  I feel like it gives people a false sense of security, a direct hit will fry your PC no matter what. Only way to 100% protect your device is to unplug it during storms and for the one in a million extremely rare incidents, you should have insurance on your home.",pcmasterrace,2026-02-25 10:18:06,2
Intel,o756yr1,"5 nvmes here. A 1TB P5 for OS in one of the cpu slots, 2 SN850x and 2 SN550 on the chipset slots, all 1tb each. Btrfs'ed and the 850s are ""raided"" too (through Linux, doesn't even need to be turned on in the bios). The pcie5 slot was supposed to get one nvme (4tb) but then this whole AI crap show came up.",pcmasterrace,2026-02-24 14:47:47,3
Intel,o76cr6o,"Wooting popularized it, i just hot tje 60HE v2 which is the best keyboard i have ever used but they have been copied by plenty of brands and you get 80% of the product for signifcantly less money too",pcmasterrace,2026-02-24 17:59:02,2
Intel,o771zrh,"If you dont game at all on a 5090 you made the wrong purchase imo lol, hes sure as shit not typing all day",pcmasterrace,2026-02-24 19:52:17,1
Intel,o770j8y,"I'm with you that you can find more variety of switches in mechanical rather than magnetic, but I have a HE board and it's not rattly at all. It really doesn't sound too far off my mech boards either. I'd also agree that mech is better than mag for anything other than gaming, but if you game at all, it's worth it. I can still type around 120wpm on my HE. It's not like you forget where the keys are.",pcmasterrace,2026-02-24 19:45:34,1
Intel,o7divzc,"Btw there are tactile HE switches, like Gateron Jade Tactiles.",pcmasterrace,2026-02-25 18:54:14,1
Intel,o76kil4,"Fuck tactility and wtf? My 60 HE V2 is slightly thocky but barely audible especially when you dont use all 4mm of travel, its dead silent. Easily tops having shitty noisy typing the whole time.  Maybe you do not know how to properly lubricate switches and stabilizers?   For gaming there is no discussion of course, HE is just better. For typing i can accept its subjective but the reasons you mentioned are not present in my keyboard except tactility, in which case optical switches would still be better than mechanical.",pcmasterrace,2026-02-24 18:33:21,0
Intel,o76n7ts,"Avoid all the nitpicking, redditors are a bunch of elitist c-nts who cannot accept that someone else is spending more money than they do. Seriously, just ignore it.  Most are making jokes about the chair, and most of them are also elitists who think a $2000 circle jerk herman miller chair is the perfect embodyment of what a chair has to be, while most of them never even sat on one, and also while ignoring that most of these ultra high end office chairs look like cheap $20 yard sale office chairs. Buy whatever is comfortable for you. I got a german made new $500 office chair, and the only thing it doesn't have is concrete-stable arm rests, because apparently that's worth 1500 more. They just slightly wiggle if you move them, that's it, and one silly design choice with a very rattly bottom cover on the chair legs, which was fixed with 2 tiny strips of double sided tape. They cheaped out on 1-2 really silly things that are mostly fixable with 50 cents of glue, permanently, and splurged on everything else. Totally fine to me.  Enjoy your setup, make sure to build everything correctly, take your time, and have fun with it. Yes you could've saved some money here and there and wouldn't have lost any performance, but.. who cares. If you felt like splurging, do so. I spent $500 more on my setup just so I can have tiny LCD screens in my fans. I never used them again, but I.. have them. Just because it's funny.",pcmasterrace,2026-02-24 18:45:14,-1
Intel,o7ala4l,"And you can ignore he loud minority of Reddit technocrats, easy as that",pcmasterrace,2026-02-25 08:31:39,1
Intel,o76ez0b,>Iâ€™ll survive  Unlike your hardware.  Losing my system every four years seems like good incentive.,pcmasterrace,2026-02-24 18:08:52,1
Intel,o75ljfc,"We are lucky. Be grateful for the immense fucking luck (I know I am, bought all my stuff late September- riight before this catastrophe)",pcmasterrace,2026-02-24 15:56:29,2
Intel,o77a3je,"Well he made a lot of ""wrong purchases"" from looking at this picture, so I don't think that is probably too far from the truth.   Also 100% could be using it for his own language model too, but IDK OP so tough to say. There's lots of things that you can do with a 5090 outside gaming.",pcmasterrace,2026-02-24 20:30:03,1
Intel,o76t06b,Yep. That's definitely an opinion.,pcmasterrace,2026-02-24 19:10:59,0
Intel,o7alknt,99.9% of people here genuinely know nothing about hardware.,pcmasterrace,2026-02-25 08:34:27,2
Intel,o76ir4x,"One outage in 4 years doesnâ€™t mean an outage every four years.  Itâ€™s just the four years I have lived here. I have no reliable data about the other years this city has existed. Though a friend claims it was his first outage too and heâ€™s lived here about 13 years.  Also, itâ€™s the Netherlands. Electric infrastructure is very reliable here.  My only device that can at least pretend to be â€œcriticalâ€ is my home server. That runs ZFS.",pcmasterrace,2026-02-24 18:25:31,1
Intel,o7aogfd,"Ok then â€œthe amd fanboys make me not even want an amd cpuâ€ doesnâ€™t make sense then, you have a 14900k and you like, simple as that",pcmasterrace,2026-02-25 09:01:35,0
Intel,o78ryfe,"It's true that if you have a well trusted grid, it's easy to consider an UPS superfluous. Especially when modern PSUs are so advanced.  That being said, even the most developed infrastructure won't stop lightning striking a transformer, or a storm causing surges.  If your setup, like OP's is, runs up towards 10k, a few hundred dollars more seems reasonable for a safeguard.",pcmasterrace,2026-02-25 01:00:45,1
Intel,o7aoogf,I also own a 9800x3d and two 7500fâ€™s,pcmasterrace,2026-02-25 09:03:42,1
Intel,o264q60,"They're same thing, I don't even see point of pro tbh unless you need it for some reason.  The difference is Intel vProÂ® Essentials & Intel SIPP (Stable Image Platform Program) that basically it.",pcmasterrace,2026-01-28 06:28:30,2
Intel,o26kebw,the difference is money. if you ask intel they'll answer you with that:  https://preview.redd.it/mbnrjbrwy1gg1.png?width=1080&format=png&auto=webp&s=9d19e620addc72804f671b1c9706b6e77f93f7b0,pcmasterrace,2026-01-28 08:42:47,2
Intel,o25cjn8,I think one of the key differences is that one has the word Pro in it while the other doesnt,pcmasterrace,2026-01-28 03:22:52,3
Intel,o28cw15,"The real answer is that it supports a different driver suite, and probably will have better support for photoshop and other softwares.",pcmasterrace,2026-01-28 15:44:59,1
Intel,o3jm6lp,I thought it would be like more memory n stuff but thanks for clarifying,pcmasterrace,2026-02-04 15:29:58,1
Intel,o2dmp7c,I love the krabs meme,pcmasterrace,2026-01-29 08:38:20,1
Intel,o3jm3qs,ðŸ˜‚,pcmasterrace,2026-02-04 15:29:35,1
Intel,o3jmc9l,Oh k,pcmasterrace,2026-02-04 15:30:42,1
Intel,o71nvx6,What is your budget and what programs will you be running on the new GPU?,pcmasterrace,2026-02-23 23:55:00,1
Intel,o71ocp1,The biggest question is what apps you'll be using for video editing. Some apps don't support the Intel GPUs and you'd be better off with any Nvidia card.,pcmasterrace,2026-02-23 23:57:38,1
Intel,o71obxl,"About $300 at most, im wanting to dive into da Vinci But currently just using CapCut for simplicity, OBS for streaming",pcmasterrace,2026-02-23 23:57:31,1
Intel,o71pter,Iâ€™m not opposed to it itâ€™s just the current market place is terrible and trying to get a 12gb gpu thatâ€™s decent is almost out of the question.,pcmasterrace,2026-02-24 00:05:49,1
Intel,o71p8p6,"Well looks like they [fixed Arc support](https://community.intel.com/t5/Graphics/The-B580-works-terribly-in-Capcut/m-p/1726172) in CapCut, so it's hard to argue against a B580.",pcmasterrace,2026-02-24 00:02:35,1
Intel,o71qdes,Would an RTX 3080 work for your purposes? It'll run rings around an an Intel GPU.,pcmasterrace,2026-02-24 00:08:56,1
Intel,o71qrfe,"I know it would but I canâ€™t seem to find them around my budget, most are over priced or people are hoarding them. Iâ€™ve been keeping an eye out on my local microcenter for a refurbished but sadly none :/",pcmasterrace,2026-02-24 00:11:06,1
Intel,o71rkn0,Keep an eye on r/ hardwareswap. It looks like a half dozen 3080 have sold right around $300 in the last week.,pcmasterrace,2026-02-24 00:15:39,1
Intel,o71qyz3,Just means that the game is mainly limited by your CPU.,pcmasterrace,2026-02-24 00:12:17,1
Intel,o71u0p3,"Arc Raiders is very CPU intensive. You don't know exactly what CPU you have, but any of the *many* 12th gen i5s should be able to keep up with a little 3050.  How are temperatures on the laptop when you're playing Arc?",pcmasterrace,2026-02-24 00:29:05,1
Intel,o71rsed,Even at 50% usage? I have played cpu heavy games and any of them got less than 80% at all times. Shouldnt a i5 12450h be even a little overkill for a 3050 laptop?,pcmasterrace,2026-02-24 00:16:50,1
Intel,o71uf76,"I5 12450h, its a gaming laptop, usually the temps hover at 90câ°. I do a cleanup inside the laptop avery 2 to 3 months so no dirt problems. Just the laptop experience lol. Gpu temps hover around 85 too.",pcmasterrace,2026-02-24 00:31:17,1
Intel,o71snwr,"It depends entirely on the game/app, you can't make generalizations like that. And arc raiders is CPU heavy game. Just like bf6 is mostly limited by CPU on even modern X3D chips.   You're always ""bottlenecked"" by something, just varies how much.",pcmasterrace,2026-02-24 00:21:41,1
Intel,o71t0wx,"SÃ­, puede ser normal que en Arc Raiders tu RTX 3050 se quede en 50â€“60% de uso aunque en otros juegos vaya al 80%: normalmente significa que el lÃ­mite lo estÃ¡ poniendo otra cosa (CPU, cap de FPS/V-Sync, energÃ­a/temperatura, o el propio juego).",pcmasterrace,2026-02-24 00:23:39,1
Intel,o71uldg,Shouldn't be too bad. CPU clocks hitting north of 4.2 GHz in-game?,pcmasterrace,2026-02-24 00:32:13,1
Intel,o71v7d8,3.7ghz at 96câ° rn at 82% usage and max fan speed and cooling settings,pcmasterrace,2026-02-24 00:35:32,1
Intel,o71wa34,"That's a bit lower than I'd expect it to be, especially at 96C. What's the CPU power while it's at those settings and usage?",pcmasterrace,2026-02-24 00:41:26,1
Intel,o71wr13,I do t know where to look this up but all power settings are set to default. Some people even say undervolting is good on nitro 5s but my motherboard dont let me do it...,pcmasterrace,2026-02-24 00:44:04,1
Intel,o71xev3,"HWinfo will show you CPU power.  We're trying to get to how heavy this CPU is throttling. We know it is, and we know the clocks aren't what they could be. Power is the last piece of that puzzle.",pcmasterrace,2026-02-24 00:47:46,1
Intel,o7283t5,https://preview.redd.it/fmk8uuzulclg1.jpeg?width=8160&format=pjpg&auto=webp&s=476cb7d5d2e1e4179ddf91e7f022870ab8c8b86e  This helps?,pcmasterrace,2026-02-24 01:49:55,1
Intel,o73sovq,Keep scrolling down until you get to power.,pcmasterrace,2026-02-24 08:45:57,1
Intel,o4tka4g,The install manager is unable to... install things?  What,AMD,2026-02-11 16:15:56,213
Intel,o4tqxms,"For anyone struggling with path tracing in Cyberpunk on RDNA 4, you can fix it with this simple mod: https://www.nexusmods.com/cyberpunk2077/mods/25673  Naturally, the cause of the crashes is SHARC, a proprietary Nvidia tech for radiance caching, which when disabled, stops the crashing.",AMD,2026-02-11 16:46:49,75
Intel,o4u5yie,Did they fix the overlay freezing your mouse issue?,AMD,2026-02-11 17:57:14,24
Intel,o4tm77x,Add FSR 4 for 7000 series gpu,AMD,2026-02-11 16:24:51,164
Intel,o4ulbwu,Please fix that Adrenalin Software closes itself randomly.,AMD,2026-02-11 19:08:21,18
Intel,o4tnhd6,You can tell the gaming team has been slashed to the bone by these lackluster updates,AMD,2026-02-11 16:30:50,68
Intel,o4tjeot,"Darktide still not listed as known issue , amd is even ghosting Fatshark ( dev of darktide / vermintide ) for 1 year....Â [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)  Known issue for roughly 1,5 years and i got a ticket with them open since 5\~ Months.  **( performance sucks as soon as you have a 6000-9000 series amd gpu AND / OR a X3D CPU any yes from 5700X3D to 9000series CPU with X3D  even with a Nvidia GPU  Broken by amd driver / chipset updates and never fixed)**",AMD,2026-02-11 16:11:55,69
Intel,o4utfbs,Where's the FS4 support for us peasants?,AMD,2026-02-11 19:46:50,20
Intel,o4tumb3,"Finally, I was wondering if the messy clouds were from fsr or the game or the driver.",AMD,2026-02-11 17:04:09,9
Intel,o4u0fy3,did they fix DDC/CI after the last update they broke it can someone confirm,AMD,2026-02-11 17:31:32,10
Intel,o4trnzl,I need these guys to fix GoW 2018 already,AMD,2026-02-11 16:50:12,13
Intel,o4ymnqz,Running drivers from a year ago because all of the latest releases have been terrible.,AMD,2026-02-12 11:00:34,8
Intel,o4tu0a0,Ah glad they sorted the issue with the clouds on ARC raiders. It wasn't the biggest of deals but definitely looked odd.,AMD,2026-02-11 17:01:15,5
Intel,o4u1bra,"I've been attempting a driver-only install to avoid the Adrenaline software, but it keeps forcing the software on me. What's the issue, AMD?",AMD,2026-02-11 17:35:45,5
Intel,o4tkwl1,Is DDC/CI fixed with this release?,AMD,2026-02-11 16:18:51,19
Intel,o4txc8y,"""Corrupted clouds may beÂ observedÂ while playing Arc Raiders on Radeonâ„¢ RX 9000 series graphics products.""  Finally!",AMD,2026-02-11 17:16:54,10
Intel,o4txdyg,"Man, the clouds on ARC Raiders was so annoying. But when I swapped to Bazzite, it wasn't a problem. I haven't enabled FSR4 there, yet though.",AMD,2026-02-11 17:17:07,5
Intel,o4u18f1,Thank you for the ARC clouds fixed issue! Was a bummer for a long time and it was kinda ruining the immersion.  Thank you once more devs!,AMD,2026-02-11 17:35:19,4
Intel,o4zbfp6,Why is no one talking about or fixing the memory (VRAM) leak that occurs in RDR2 when SAM is enabled?,AMD,2026-02-12 13:53:52,6
Intel,o4tnnm2,That Cyberpunk issue has been plaguing me for so long and I am still waiting for it to be fixed.,AMD,2026-02-11 16:31:38,11
Intel,o4tp722,What about fixing stuttering issues. When i have my 2nd non freesync monitor connected,AMD,2026-02-11 16:38:47,8
Intel,o4tlzdr,"Does minimal installation work? As there is no option to install those over the previous one, there may be a problem with minimal installation because it doesn't work since October.",AMD,2026-02-11 16:23:51,5
Intel,o4tksfe,"Copying /u/The_Countess reply below so it's seen before my comment: >Disabling nvidia's proprietary SHARC fixed it.  >Here's a mod that does it for you.   [https://www.nexusmods.com/cyberpunk2077/mods/25673](https://www.nexusmods.com/cyberpunk2077/mods/25673) (courtesy of [u/Dat\_Boi\_John](https://www.reddit.com/user/Dat_Boi_John/))  --- Original comment:  >Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled  This is fucking hilarious. This isn't the issue. Playing the game with Path Tracing enabled WILL result in a crash on a 9000-series card, full stop. Path Tracing cannot be used in Cyberpunk 2077 on an RDNA4 card, yet this is the copout shit they'll fess up to.",AMD,2026-02-11 16:18:19,26
Intel,o4tv6iv,Is Hogwarts Legacy fixed?,AMD,2026-02-11 17:06:47,7
Intel,o4tvt98,Is this worki g on 7900xtx ? Previous driver giving timeouts.,AMD,2026-02-11 17:09:44,3
Intel,o4umxpp,Still using 25.8.1 for Path of Exile 2 and even then I still crash from time to time using both Vulkan and DX12. I'm using Vulkan now because it can recover most times from a graphical crash where as DX12 will crash to desktop. The only thing with Vulkan is fog renders in squares and so maps have a checkered pattern overlayed on top of them with stuff like delirium and other fog effects.   I've done full reinstalls of Windows from nothing and still see this effect.,AMD,2026-02-11 19:15:57,3
Intel,o4uxsof,"Can anybody confirm if XeFG is working again in Cyberpunk?  EDIT: Ok, I tested it myself... still broken.",AMD,2026-02-11 20:07:43,3
Intel,o586df1,"No 60Hz mode for HPreverb G2 with Oasis driver, again... AMD keeps adding issues with VR, like no VRS in DX11, it's getting harder to select amd over nvidia.",AMD,2026-02-13 20:32:37,3
Intel,o58g2a6,"No FSR 4 on RDNA 3, No Vulkan FSR support, optional indeed",AMD,2026-02-13 21:21:08,3
Intel,o4tmwmv,Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.Â      LOL,AMD,2026-02-11 16:28:09,7
Intel,o4wcgtf,"So... when are they going to fix the issue of the 9070/XT boosting itself way too high and causing a driver crash? ""Underclock the card by 300Mhz"" is not a proper solution, and it's definitely a software issue since doing a driver-only install gets rid of the problem.",AMD,2026-02-12 00:27:33,3
Intel,o4tycdi,"I'm amazed and apalled at the same time. For 2+ years of regular crashing on 7900XTX, still no fix for WoW.  I regularly experience the following:  ""display driver amduw23g-196284-dd24e58f stopped responding and has successfully recovered.""   And no fix work, neither reinstalling windows, fiddling in the registry, reinstalling drivers completely nor disabling overlays of any sort. And it's crazy to me that WoW is the only game experienced this in, that is Until recently, now Arknights Endfield also does it in certain areas. It's driving me crazy.",AMD,2026-02-11 17:21:38,5
Intel,o4tkcgi,\*yawn\* nothing interesting,AMD,2026-02-11 16:16:14,8
Intel,o4u1e43,"I know the game updates are to fix bugs, optimal performance etc, but Nioh 3 doesn't support FSR4 on this driver (and neither on previous).. weird. I know OptiScaler has issues with it.",AMD,2026-02-11 17:36:03,2
Intel,o4u7keq,polaris update when?,AMD,2026-02-11 18:04:40,2
Intel,o4uux5t,Does anyone know if GTA V Enhanced still crashes with RT on in certain areas of the map?,AMD,2026-02-11 19:53:57,2
Intel,o4v5tvd,did they fix ml frame gen?,AMD,2026-02-11 20:46:59,2
Intel,o4x67ba,Does it fix twinkle tray?,AMD,2026-02-12 03:27:07,2
Intel,o4x83o9,"Well this feels more stable than the other ""latest"" one, on my rx 7600",AMD,2026-02-12 03:39:34,2
Intel,o4xokoy,we need an option to not install the amd install manager  it is utterly useless and causes issues for me by having it installed,AMD,2026-02-12 05:40:19,2
Intel,o4y7vl3,"They managed to fuck up display brightness over displayport on the last version and I had to roll back, I guess they didn't even note that",AMD,2026-02-12 08:37:47,2
Intel,o4yp240,Did they stop optimisations for 5000 series GPU,AMD,2026-02-12 11:21:36,2
Intel,o4z0tsa,Since 26.1.1 I've been getting massive lag spikes when streaming a game on Discord. Anyone with similar issues? (9070 XT and 7800X3D),AMD,2026-02-12 12:50:20,2
Intel,o4zsdl9,anyone knows why this driver has 1.6 GB download size ?,AMD,2026-02-12 15:22:01,2
Intel,o50r8bc,"The last update broke my Asus monitor software, had to roll back to December on my 7900 xtx. Anyone else have the same issue or can test if it's working on this release",AMD,2026-02-12 18:05:25,2
Intel,o649z79,I am still getting crashes every now and then while simply being on desktop doing nothing... it's become so frustrating,AMD,2026-02-18 20:25:33,2
Intel,o4txqhl,I wonder if AMD will fix the BF6 related issues before EA fixes BF6,AMD,2026-02-11 17:18:46,3
Intel,o4tnn0s,I learned the hard way about using the record feature during BF6. First time trying to use the AMD option and the driver completely corrupted to the point that I had to DDU to even use my PC.,AMD,2026-02-11 16:31:34,2
Intel,o4ucknk,"I welcome the Nioh 3 support, but AMD can't ignore people upset with them forever.",AMD,2026-02-11 18:27:49,2
Intel,o4tjuhy,Looks like the install manager doesn't see this as an optional driver and same when you uninstall it and just do a check for updates so manually downloading it right now.,AMD,2026-02-11 16:13:56,2
Intel,o4u1v13,"The reason why these drivers are so ...underwhelming... is, the consumer computing market is currently dead. This affects everything from PC parts, prebuilts, consoles, smartphones. There is zero incentive to update drivers, make fixes or add new features (FSR4 Int8) when the enterprise AI-driven market has so much higher margins. No new consumer GPUs are in sight this calendar year, so customer/consumer retention is no reason at all. AMD deserves the same blame as Nvidia. Hold out with the hardware you have, play something from your backlog. This year, buy only something if you REALLY REALLY need to.",AMD,2026-02-11 17:38:16,2
Intel,o4ts6c0,# Known Issues wtf if you knew it why not fix it,AMD,2026-02-11 16:52:34,1
Intel,o4tz1oe,Does it bring FSR 4 to Nioh 3? ðŸ˜,AMD,2026-02-11 17:24:57,1
Intel,o4v1jer,GoW 2018 still broken I assume?,AMD,2026-02-11 20:25:54,1
Intel,o4v8bdc,"Whee.  Hopefully this fixes the countless frustrating driver crashes I've had while playing Nioh 3.  However, the install manager doesn't list this as available for download and the website is still on 26.1.1",AMD,2026-02-11 20:58:57,1
Intel,o4vu2dw,![gif](giphy|5xtDarDewDfNyPrYSbe),AMD,2026-02-11 22:45:27,1
Intel,o4yzq25,"They finally banished the evil shadow squares from the clouds over topside. Honestly, I wasn't even sure it was a graphical bug - thought it might be some visual effect caused by the Arc in space.",AMD,2026-02-12 12:42:57,1
Intel,o53woew,Does this update support all generation computers and laptops,AMD,2026-02-13 04:17:17,1
Intel,o5dtm5i,"Is FSR4 implementation fixed ? I mean quality mod is unusable in 99% of games, if not injected with optiscaler",AMD,2026-02-14 18:53:24,1
Intel,o5qgoll,anyone try it with black ops 7,AMD,2026-02-16 19:16:48,1
Intel,o61xang,"Is there any possible way to fix tessellation problem?   My 9070xt goes crazy when im applying tessellation shader in unity, meanwhile my 3070 doing completely fine  Also having image freezes in SteamVR every 1-2 minutes for 10-20 secs ( Driver roll-back fixed the issue )  Feels like every driver update is killing something that worked normally before",AMD,2026-02-18 13:54:07,1
Intel,o6394a9,anyone know how the drivers perform on the 7900xtx  sapphire nitro,AMD,2026-02-18 17:39:39,1
Intel,o66vigc,"Suddenly got 2 beeps through my headphones and a lot of artifacting on my main screen. Artifacts dont show up on recordings, and a restart fixed it, but I've never had this since launch on any of the other drivers. Not 100% sure it's this driver that caused it but I have no idea what else it could be. I wasn't even playing anything I was just browsing.",AMD,2026-02-19 04:57:39,1
Intel,o6j54lz,"this update is crashing my games and caused to many fps drops, itâ€™s actually insane, is there anyway i can revert this?",AMD,2026-02-21 01:48:53,1
Intel,o6o93qg,AlgÃºn valiente que lo pruebe?,AMD,2026-02-21 22:06:23,1
Intel,o7fd43i,"Can you look into the picture in picture issues already when running wallpaper engine on multiple high res displays especially dualscreen 4k setups like 4k ultrawide + 4k in potrait for example ? and stop ignoring the noise suppression issue as if it does not exist, depcrate noise suppression already if you not gonna fix it.",AMD,2026-02-26 00:17:58,1
Intel,o4tlbkz,amd drivers makes me wnna go nvidia,AMD,2026-02-11 16:20:47,1
Intel,o4tmu9p,Skip skip skip...  But when does these drivers get good?!,AMD,2026-02-11 16:27:51,1
Intel,o4vzadg,AMD Software Update ðŸ‘ðŸ‘ðŸ‘ Thank you AMD!!!,AMD,2026-02-11 23:12:48,1
Intel,o4tmfee,"Optional driver..no fsr4 support on rda3 , cringe",AMD,2026-02-11 16:25:55,0
Intel,o4u23g6,![gif](giphy|7k2LoEykY5i1hfeWQB),AMD,2026-02-11 17:39:22,1
Intel,o4uc6xz,The last two versions of adrenaline crash for me anyways I'm not updating I doubt it's any better if it's optional,AMD,2026-02-11 18:26:03,1
Intel,o4ud019,"""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.Â ""  if I had a beer for every time they didn't fix that, I'd be an alcoholic, lol",AMD,2026-02-11 18:29:47,1
Intel,o4uuoj9,"Hmmm, why is the driver 1.6GB? However, I really hope this update fixes the black screen flickering caused by GPU hardware acceleration.",AMD,2026-02-11 19:52:48,1
Intel,o4va5m6,"6900XT owner here, still on 25.9.1 cause it's the newest one without major problems..funny there are still basic bugs since Adrenaline initial launch not yet still fixed (***insert MGSV coma meme here*** I'm afraid it has been 8 years...)",AMD,2026-02-11 21:07:52,1
Intel,o4va62m,"AMD are acting shady. In this release, theyâ€™ve split the RDNA1/2 codebase from the rest, so now the driver package includes two variants: Display and Display2. The marketing team clearly dropped the ball, and thatâ€™s the real reason the package size has ballooned! Just unzip the driver package yourself and take a look",AMD,2026-02-11 21:07:55,1
Intel,o4vfb5n,"Hopefully they have fixed the UI being extremely slow and buggy until you install/add a game. Minor inconvenience and fixed once you do that, but, still needed to be addressed.   I also really hope to see â€˜Added HDR10+ Gamingâ€™ support at some point as itâ€™s just unacceptable that AMD are the only ones who donâ€™t support it. Even Intel with their small user base do.",AMD,2026-02-11 21:32:33,1
Intel,o4wdec3,Feels like every Adrenalin update at this point just makes the cards more unstable :/,AMD,2026-02-12 00:32:56,1
Intel,o4to3aj,"Jesus Christ this is awful. I've never had a product where the driver support is so bad I want to switch to their competitor. I mean these idiots can't even label the games we play correctly, Why do I keep hoping for a driver update that includes new features, fixes, and the smallest attempt at polish and shine.   Fucking pathetic.",AMD,2026-02-11 16:33:39,-2
Intel,o4u09ds,"Pathetic driver update Even as optional. 2 whole new games supported, wow!",AMD,2026-02-11 17:30:40,-2
Intel,o4vlnh3,What fuhed this time?,AMD,2026-02-11 22:02:38,0
Intel,o4yl4jz,"On the other side of the coin, Mesa 26.0 was released for Linux today, big improvements overall, including almost parity with RT enabled on some games against the Windows driver. ðŸŽ‰",AMD,2026-02-12 10:46:41,0
Intel,o5avtca,When will they fix Discord driver timeouts,AMD,2026-02-14 06:47:10,0
Intel,o4vvbfe,What? What's my ai stuff,AMD,2026-02-11 22:51:58,-1
Intel,o4yviuk,"Fantastic, I had my first driver timeout while gaming in years, roughly 1 hour after installing and using this driver version.  Needless to say, it reset all my settings as well. If I didnt already write it: fantastic (not). I fear the team has been outsourced to the most incompetent and cheapest option.",AMD,2026-02-12 12:13:15,-1
Intel,o4tpz3n,Lol they dont care,AMD,2026-02-11 16:42:24,76
Intel,o4u36m1,"its a checkmark feature , its technically there if it works they dont care.",AMD,2026-02-11 17:44:26,21
Intel,o4vy0bb,Yep.,AMD,2026-02-11 23:05:58,1
Intel,o4w2wp8,What is that even meant to do?,AMD,2026-02-11 23:32:30,1
Intel,o4xg6re,"Was that even working, it's hard to tell other than the toggle",AMD,2026-02-12 04:35:32,1
Intel,o562uw2,My Vega 64 has been broken since release and they never fixed it...and it no longer is getting updates,AMD,2026-02-13 14:25:25,1
Intel,o56fpgz,You can fix it if you want it.,AMD,2026-02-13 15:29:46,1
Intel,o4uf6f6,"AI has been progressing very fast these last years, aren't there much better open source solutions these days?",AMD,2026-02-11 18:39:53,-6
Intel,o4tkx87,[r/onejob](https://www.reddit.com/r/onejob/) AMD following in MS footsteps I see.,AMD,2026-02-11 16:18:56,91
Intel,o4w4irh,This is genuinely hilarious and an apt summary of the Radeon division in general.,AMD,2026-02-11 23:41:42,12
Intel,o4u2hsg,"Typical amd thing , makes a feature with 1 or 2 purposes.  fails at 1 or both.",AMD,2026-02-11 17:41:14,24
Intel,o4u7oty,Seriously they either get rid of that stupid app that almost everyone uninstalls after the driver installs or they make it an useful app that actually works as intended.,AMD,2026-02-11 18:05:14,13
Intel,o4tod2i,"becuase its optional update, you have to download it from their site. didnt released to all users yet.",AMD,2026-02-11 16:34:56,4
Intel,o4u58s4,"installing optional drivers was recently added. seems it there's still a bug.   It can install things, just not that particular thing.",AMD,2026-02-11 17:53:58,0
Intel,o4uoxjw,Isnt it because the PT becomes less demanding when the mod is installed?,AMD,2026-02-11 19:25:23,-6
Intel,o4xv8qg,"Oof, that one hurts",AMD,2026-02-12 06:38:09,5
Intel,o501sbl,"This has to be one of the most frustrating bugs ever. You can't even undo the issue by trying to close the overlay with a hotkey too. Radeon Settings just locks the mouse to itself and will not give it up. Worst part is that the overlay is invisible.  I had to log out with keyboard inputs because trying to force close anything in Task Manager is an exercise in patience without a mouse. I did manage to get mouse control back at one point by opening main Adrenalin Settings app via Start Menu, which must've closed the invisible overlay and unlocked mouse focus.",AMD,2026-02-12 16:06:36,2
Intel,o4zw79j,"So thats what it was, rolled back to 25.9.2 and everything works much better",AMD,2026-02-12 15:40:28,1
Intel,o4tpjpz,We can Dream they dont care,AMD,2026-02-11 16:40:24,51
Intel,o4v6hmw,and 6000 series,AMD,2026-02-11 20:50:09,32
Intel,o52t12r,INT8 works on 6xxx as well. Us too!,AMD,2026-02-13 00:11:33,8
Intel,o4wql3n,![gif](giphy|gflbaFyKl7DHvpDVZY),AMD,2026-02-12 01:52:39,2
Intel,o4znbrn,Freshly installed Windows 10 with this latest drivers and AS still closes by itself randomly.  Cmon AMD do something,AMD,2026-02-12 14:57:00,5
Intel,o4vskqs,"I haven't had this happen with the last update (26.1.1). At least, it hasn't happened yet that I can remember. It usually happens when I try to ALT+R in-game.",AMD,2026-02-11 22:37:43,1
Intel,o54pdt6,"I fear the solution is near as the moon... I don't even see it in the known issues of whatever they call their todo list. I've tried every solution/trick/workaround you can find online but none has worked. The icon is present in the SysTray at startup, as I try to open the Adrenalin Panel (from Start Menu, from SysTray icon or from context menu on Desktop, doesnt' matter), the splash screen is displayed for a second, then closes and the icon disappears. If I do nothing at the startup and simply leave the pc on, after a few minutes the SysTray icon pops away.  It's quite incredible how a company as AMD does not know why their software simply closes itself... no registry to investigate?!? I bet my best precious stuff it's a software incompatibility.",AMD,2026-02-13 08:11:35,1
Intel,o6mrdm0,"Good to know, that I'm not the only one who has this issue. Everytime I use the Adrenaline Software and open something, the Software is closing itself, doesn't matter what you do, its really randomly. Happened to me after a Windows update, Windows updated my driver and then I installed my current one. I guess I have to DDU my current one and see if this fixed it.",AMD,2026-02-21 17:30:18,1
Intel,o71vmyn,El fix se llama 25.12.1  A servir,AMD,2026-02-24 00:37:54,1
Intel,o4uzl9y,Yo need to run a DISM health check and repair.,AMD,2026-02-11 20:16:24,0
Intel,o4we18f,It's Homer with the toy pressing the key.  ![gif](giphy|xULW8N9O5WD32L5052),AMD,2026-02-12 00:36:35,16
Intel,o5wyu38,Moved over to the CDNA team from RDNA.     This AI bubble needs to pop already!,AMD,2026-02-17 19:10:38,1
Intel,o4tlpcs,"I read up on this thread and lol. Lmao, even. Radeon team please fix",AMD,2026-02-11 16:22:33,24
Intel,o4unhdm,"I just use the popular workaround that modify 2 .ini files, it will make texture look worse but no more hard stutter every 5-10sec that makes me wanna throw up, and game is now never dropping below 60fps on my 6700xt",AMD,2026-02-11 19:18:33,3
Intel,o4wkwyn,And this is supposed to be their radiance caching game,AMD,2026-02-12 01:17:49,3
Intel,o4uce21,The issue doesnâ€™t persist on linux if thats any consolation. Game runs like clogged sewage on windows since day one in my experience but its like warm butter when its running through proton for some reason.  EDIT: Lying through your teeth then blocking the people who call you out on it doesnâ€™t make you look rational just fyi,AMD,2026-02-11 18:26:58,3
Intel,o4ui1ya,Because its windows not amd,AMD,2026-02-11 18:53:06,-1
Intel,o4ydrcp,7900XT not peasant at all,AMD,2026-02-12 09:36:09,8
Intel,o6kjcd4,"Claw 8's XeSS 3 blows past AIMAX395's FSR 3.1 in quality, at 1/3 the priceâ€”pure joke on the pricier 395.",AMD,2026-02-21 08:12:02,1
Intel,o4u60nd,2 people in this thread said it did in the replies to this comment.      [https://www.reddit.com/r/Amd/comments/1r21f5b/comment/o4tkwl1/](https://www.reddit.com/r/Amd/comments/1r21f5b/comment/o4tkwl1/),AMD,2026-02-11 17:57:31,9
Intel,o4u8382,"Yes, it's fixed now.",AMD,2026-02-11 18:07:06,7
Intel,o4wl1n2,What's wrong with it?,AMD,2026-02-12 01:18:37,3
Intel,o4ufj2p,may I ask why? Adrenaline is kinda the only thing AMD does better then Nvidia.,AMD,2026-02-11 18:41:28,1
Intel,o4tugv9,It has been fixed.,AMD,2026-02-11 17:03:26,14
Intel,o4ts4cg,"updated, seems to work fine now",AMD,2026-02-11 16:52:18,9
Intel,o4u7edr,I was about to ask about this too,AMD,2026-02-11 18:03:53,2
Intel,o4tmga7,install it and test it and report back?,AMD,2026-02-11 16:26:02,-1
Intel,o4upb9j,"Wonder if it improved the performance overall in Arc Raiders, will check it out",AMD,2026-02-11 19:27:12,2
Intel,o4u38zg,"I might need to look up every now and then lol, had no idea this was a thing",AMD,2026-02-11 17:44:45,1
Intel,o4u2sv8,It's caused by nvidia's proprietary SHARC.   Seems like another one of their deliberate sabotages of AMD.  Here's a mod that fixes it by disabling SHARC [https://www.nexusmods.com/cyberpunk2077/mods/25673](https://www.nexusmods.com/cyberpunk2077/mods/25673)   as linked to by u/Dat_Boi_John,AMD,2026-02-11 17:42:40,45
Intel,o4u3rxx,I'm curious as to what GPU and resolution you're using where Path Tracing in Cyberpunk is viable. I tried it on a 9070XT and it's not viable at 4k regardless of FSR settings.,AMD,2026-02-11 17:47:12,5
Intel,o4uhkov,The previous Adrenaline update caused massive stuttering on my 240hz monitor. I had to disable Freesync and it acted fine. I ended up reverting to an older version. I'm also wondering if Freesync issues have been resolved as they aren't addressed in the fixes.,AMD,2026-02-11 18:50:53,3
Intel,o4u3b6u,Disabling nvidia's proprietary SHARC fixed it.  Here's a mod that does it for you.   [https://www.nexusmods.com/cyberpunk2077/mods/25673](https://www.nexusmods.com/cyberpunk2077/mods/25673) (courtesy of [u/Dat\_Boi\_John](https://www.reddit.com/user/Dat_Boi_John/)),AMD,2026-02-11 17:45:02,37
Intel,o4uidw0,Blame nvidia,AMD,2026-02-11 18:54:38,4
Intel,o5fwc5n,What was broken about it? I haven't had any issues playing it,AMD,2026-02-15 02:06:09,3
Intel,o57zzq4,"ok, I think some dudes from internet are smart.   You need delete HogwartsLegacy folder from appdata folder.   System drive > users>currentuser-> appdata-> local -> hogwarts legacy. Delete   BEWARE - there are save files. Copy them or download from cloud",AMD,2026-02-13 20:00:38,1
Intel,o4v9n25,"friend with 7900gre had these issues, told him to use nvidia's mpo disable registry fix and he said no problems so far",AMD,2026-02-11 21:05:23,4
Intel,o4twafy,"set frequency, voltage and power limit. and you will no longer have timeout. obviously you have to find the values â€‹â€‹for your card",AMD,2026-02-11 17:11:58,1
Intel,o4u4pfm,nvidia's proprietary SHARC broke it (and i wouldn't put it past them if it wasn't by accident).  There's a mod available [here](https://www.nexusmods.com/cyberpunk2077/mods/25673) to disable SHARC and then it works without issue.,AMD,2026-02-11 17:51:29,9
Intel,o4wsf6i,Did you do any other tuning like OC/UV? Do you have a card with dual BIOS?,AMD,2026-02-12 02:03:41,1
Intel,o4wts0e,It is just the stupid AMD deciding to let the card boost as long as there is a thermal headroom. My card also boosts up to 3.2ghz at stock. Lmaooo,AMD,2026-02-12 02:11:47,1
Intel,o4u619s,"just amd things , use Dx11 and your hopefully having less crashing issues.  its known for wow since like 5 years , and sporadic for like 12 with wow and amd.",AMD,2026-02-11 17:57:35,5
Intel,o4ye8s4,"Pretty convinced this is more of a wow/windows issue then an actual driver issue. I can be on the same driver for months with no problems then they drop a wow patch or a windows update and all of a sudden the issue comes back. It has been an issue on and off since I got my 7900XTX at launch and I have thousands of hours in wow since that time. I went deep down the rabbit hole trying to find a solution but there are none that stick. When it gets bad I turn off ray tracing and that seems to reduce how often they occur but not stop them completely.  There are also people more recently having the same/similar problem on Nvidia cards too, so its not specific to brand anymore it seems.",AMD,2026-02-12 09:41:05,3
Intel,o4uil7z,Not one crash on wow for me,AMD,2026-02-11 18:55:35,2
Intel,o4udsbz,"do you have any OCs on your Card, CPU or RAM?   It did work fine with my 7900XTX, doesn't mean it's not an AMD problem just checking :)   OC Stuff can sometimes work well in 99 games and crash on the 100th",AMD,2026-02-11 18:33:27,1
Intel,o4tptur,Exactly.,AMD,2026-02-11 16:41:43,3
Intel,o4uawhj,"Because the developers are lying about the game using FSR 3.1. In reality, it's FSR 3.0, which is built into the game engine, without the separate DLL files that AMD provided several years ago. Therefore, it's impossible to enable FSR4 in the game because the driver doesn't recognize the DLL files, which need to be overridden. It's completely absurd that the developers released a game in this state in 2026",AMD,2026-02-11 18:20:06,2
Intel,o4u8wxw,Polaris got a new driver 2 weeks ago with the 26.1.1 driver.,AMD,2026-02-11 18:10:54,2
Intel,o6qygnc,"If you uninstall ""AMD Install Manager"" from Control Panel, it should give you the regular driver updates within Adrenalin and not reinstall itself.",AMD,2026-02-22 09:56:42,2
Intel,o5wjbqe,"RDNA1 and 2 continue to have game support and bug fixes. Supposedly, they do get optimized specifically for those games listed.",AMD,2026-02-17 17:59:23,2
Intel,o5b0srh,"They didn't bother to create separate packages for rdna1/2 (variant a) and rdna3/4 (variant b), only made a combined one for both (variant C)",AMD,2026-02-14 07:33:26,2
Intel,o5ybx2y,It's because it includes all the GPU drivers for rDNA 1 through 4 in one package.,AMD,2026-02-17 23:07:08,1
Intel,o66b55i,"Same here, I've had bad stuttering in all my games soo rolled back to the 25.12.1 December update which it seems since the 26.1.1 update they've only made the app and drivers worse and worse",AMD,2026-02-19 02:47:33,2
Intel,o4ws325,The simplest fix is switching to Linux. Every update is exciting and only brings you more fixes & new features. I fell in love with computing again.,AMD,2026-02-12 02:01:40,1
Intel,o4u29ay,"Yes because fixing things is as easy to saying to a developer ""fix it"" and 5 minutes later he's done.",AMD,2026-02-11 17:40:08,6
Intel,o4u8ynw,They post the known issues so that people don't keep sending them reports about the same thing constantly. They will be working on a fix which can take time.,AMD,2026-02-11 18:11:08,3
Intel,o4u9di1,The Nioh 3 devs are to blame for not implementing FSR3.1 properly. There is no external dll (amd\_fidelityfx\_dx12.dll) in the game directory so Adrenalin cannot hook into it to upgrade to FSR4.,AMD,2026-02-11 18:13:01,3
Intel,o4umc3o,do u really need it? Nioh graphics arent that intensive.,AMD,2026-02-11 19:13:06,1
Intel,o4ts5h8,"Nvidia drivers were just as bad last year.. Wouldn't pick a GPU based on that, personally",AMD,2026-02-11 16:52:28,29
Intel,o4ue21q,"no upgrade path for an 7900XTX and no  FSR4 for 7900XTX made me switch to Nvidia, and although I lost 8 gigs of Vram, for MY purposes it was a very good idea!",AMD,2026-02-11 18:34:42,4
Intel,o4tvaon,"Drivers is a terrible metric to judge hardware by, especially nowadays when the drivers are updated even week to week sometimes. Nvidia drivers can go from crap to fantastic in a single update, and the same applies to AMD. It's not a matter of which is better, it's finding the one that doesn't have specific issues to you.",AMD,2026-02-11 17:07:20,4
Intel,o4u7sjd,no issues with amd drivers for years   nvidias still broken in game I play,AMD,2026-02-11 18:05:43,-1
Intel,o4tpvyg,Yeap. I was an og supporter,AMD,2026-02-11 16:41:59,-2
Intel,o4tmn1v,Your post is cringe as there is zero mention of FSR4 in the driver notes.,AMD,2026-02-11 16:26:55,-9
Intel,o4wvisi,"They shipped variant C, which contains the separate driver for RDNA1-2.",AMD,2026-02-12 02:22:11,2
Intel,o4vbr02,"Damn, it's still not fixed!",AMD,2026-02-11 21:15:30,2
Intel,o4wsdty,![gif](giphy|FoH28ucxZFJZu),AMD,2026-02-12 02:03:26,0
Intel,o52kiew,">Hopefully they have fixed the UI being extremely slow and buggy until you install/add a game.  Have you tested it? Is this bug fixed?  In my case, I use the Minimal driver installation type, so the ""install/add a game"" trick doesn't work or isn't a viable option.",AMD,2026-02-12 23:22:52,1
Intel,o4tq2c4,We are delusional exactly,AMD,2026-02-11 16:42:49,3
Intel,o4u20vf,"Going to 3 hotfixes on 2 months already? Dunno man, both (amd and nvidia) has horrible drivers",AMD,2026-02-11 17:39:02,1
Intel,o4ws2qj,Maybe two more than the next update... ðŸ˜œ,AMD,2026-02-12 02:01:37,2
Intel,o4wrzm9,"You can tell cos many with this issue, while AMD never acknowledges it in their release notes.",AMD,2026-02-12 02:01:05,14
Intel,o4x61xl,Consoles > PC gamers,AMD,2026-02-12 03:26:09,-6
Intel,o4uk2pp,"Basically the entirety of radeon, probably got one guy working there so technically they can say theyre still doing something",AMD,2026-02-11 19:02:26,20
Intel,o4tol50,IIRC there used to be an option to install â€œoptionalâ€ driver updates in Adrenalin.,AMD,2026-02-11 16:35:58,31
Intel,o4u2thq,"The driver literally got a ""Recommended + optional"" setting   Man if people would stop spreading false stuff like facts.",AMD,2026-02-11 17:42:45,7
Intel,o4v13ou,"You have the exact same performance with it enabled/disabled, so I don't think so. Visually I also don't see a difference either.",AMD,2026-02-11 20:23:46,12
Intel,o5dta15,"You can go to the task manager, do everything with TAB and shortcuts to finally kill adrenalin and restart it, until it breaks again lol",AMD,2026-02-14 18:51:42,1
Intel,o4x6a2e,They care about about PSSR bc they make more money there than a PC gamer,AMD,2026-02-12 03:27:36,6
Intel,o4uhxtd,Or its not feature complete or steam signed exclusivity or etc etc etc,AMD,2026-02-11 18:52:34,-13
Intel,o4x6in9,"""But but RDNA 2 customers won't upgrade to RDNA 4"" - Radeon division",AMD,2026-02-12 03:29:07,15
Intel,o4vbe0b,Also for HD series back from 2014,AMD,2026-02-11 21:13:47,-4
Intel,o55keyl,"Everytime it crashes i send a bug report.   Do the same, guys.",AMD,2026-02-13 12:39:52,1
Intel,o6n9zz0,"I disabled the shortcuts within the AMD software and downloaded and installed the latest version of Visual C++ from Microsoft's own website, and it hasn't closed on its own for about a week now. I think I've finally solved this problem.",AMD,2026-02-21 19:02:19,2
Intel,o4v0ghz,even with fresh installed windows?   Tried DISM and it says No component store corruption detected.,AMD,2026-02-11 20:20:37,4
Intel,o54090l,I've done this a million times (and my Windows installation is not so old) but didn't help.,AMD,2026-02-13 04:42:26,1
Intel,o4tn3ou,"Worst thing is i sent amd like 50 links where people discuss it on steam , reddit , and a few other sites usually within 3 comments someone or MULTIPLE people say its a known amd bug  Just check the same forum of darktide i linked the posts theres tons , check the darktide sub reddit just search like ""Bad performance"" or ""amd"" and you will see HUNDREDS.",AMD,2026-02-11 16:29:03,15
Intel,o4usc2f,"issue is , its not only the stutters but also generally lower perf , like it did ran better on my 5700X and a 3070 as test than on my 7800X3D and 6800XT system.",AMD,2026-02-11 19:41:35,0
Intel,o4wpw52,I bet it works... Maybe...  Performance is just optional.,AMD,2026-02-12 01:48:29,2
Intel,o4uob5j,>The issue doesnâ€™t persist on linux if thats any consolation.  You know why ? the drivers are open source and get fixed by users.  we on windows need to rely on amd to fix their mess.,AMD,2026-02-11 19:22:26,7
Intel,o4us5mq,its amd sadly.,AMD,2026-02-11 19:40:44,9
Intel,o4unbux,Dude dont excuse this with fake stuff.  Darktide literally ran better on my 5700X with a 3070 ( my wifes ) as test than on my 7800X3D 6800XT system ( both times RT off because it sucks anyway on my 6800XT )     Most people report the same .,AMD,2026-02-11 19:17:50,0
Intel,o4wy56g,Weird lighting issue. Looks like a checkerboard. Others have been having the same issue.,AMD,2026-02-12 02:37:44,2
Intel,o4ukhbm,"I don't see any value in this software for my setup. It constantly hogs 200-300MB of VRAM in the background while offering zero interesting features, at least for the RX 7700 XT. The voltage tuning is a joke â€” it resets every time the driver crashes, regardless of whether the crash was caused by the app itself or unstable CPU/RAM overclocking.  Features like AFMF or Sharpening? Optiscaler does a much better job, plus it's a must-have for FSR4 in certain titles. The Hyper-RX/Quality/Power Saving profiles are pure cringe, and I'd throw Radeon Boost, Chill, and Anti-Lag into the same garbage bin. On top of that, Iâ€™ve been soft-locked several times trying to open the overlay with Alt+R; the app intercepts the input but fails to launch, freezing my cursor and forcing a hard reboot.",AMD,2026-02-11 19:04:20,8
Intel,o4zqkqr,"Honestly, driver only works waay better for me, amd 7900xtx and rx 5 7600. With the software in any game(i mean mostly heavy games like rdr2 and returnal in 1440p) i get drops like 5-10fps, which doesn't seem a lot, but i feel it, playing at mostly high settings.",AMD,2026-02-12 15:13:11,5
Intel,o4wr64s,Are you serious?  ![gif](giphy|kopN26K2ThF9j7RL4K),AMD,2026-02-12 01:56:11,1
Intel,o4uyyb7,>Adrenaline is kinda the only thing AMD does better then Nvidia.  not anymore their new UI of nvidia is compareable and even better in many areas.,AMD,2026-02-11 20:13:20,1
Intel,o53y0kh,Thanks! Just installed and can confirm it works. Was annoying have to DDU after getting random crashes / freezes using the AMD installer to roll back to 25.12,AMD,2026-02-13 04:26:37,1
Intel,o4tpm5v,No?,AMD,2026-02-11 16:40:43,-5
Intel,o4u414l,"there were like so many points and lines in the night raids or electromagnetic storms, was very weird.",AMD,2026-02-11 17:48:22,1
Intel,o4uqbzl,"If you are on a 4k display then you can use optiscaler to render at a custom resolution i.e instead of fsr4 performance mode which renders at 1080p, you can change it to render slightly lower like 960p or 847p ect. It still looks better than playing at 1440p Quality/balance mode. Combine with using RIS2 and a slight overclock, the performance is not bad at all.  You could also just use fsr4 performance mode at 3200x1800, overall better experience than playing at native 1440p.  Path tracing even without any additional help from NRC, Ray regeneration and the help of the devs optimisation is still a viable option on the 9070xt.  There are some good resolutions in-between 1440p and 2160p to explore.",AMD,2026-02-11 19:32:02,3
Intel,o4wdbr3,"There's also a PT mod called ultra plus or something, I think it reduces the light bounces (image looks almost the same)  Mod could push the game above 60fps",AMD,2026-02-12 00:32:31,4
Intel,o4udruv,"1440p with FSR 4 balanced on a 9070xt is very playable, especially on a controller. You can even drop to FSR 4 performance mode and it's still okay, although it starts to get a bit more shimmery.",AMD,2026-02-11 18:33:23,3
Intel,o615ltl,Which version btw i might try it,AMD,2026-02-18 10:47:23,1
Intel,o4w6dtz,"Blame Cd Projekt Red, there's other games that use the tech (Doom Eternal, Indiana jones, Blackmyth) that work fine on AMD cards  (and it's supposed to be at least compatible with all cards that support DirectX raytracing), but it's only Cyberpunk that's crashing",AMD,2026-02-11 23:52:23,4
Intel,o4to4nx,9000 series man. You have a 6950xt.   And can confirm that my 9070xt crashes all the time with CP2077 PT on.,AMD,2026-02-11 16:33:50,7
Intel,o4togz2,What's your frame rate? 15? Lol jk.. my 7800 sapphire nitro overclocked heavy still struggles with path tracing on CP and fsr,AMD,2026-02-11 16:35:26,3
Intel,o5h2dfk,This game is broken. My thoughts about this - after reinstalling drivers you have two Hogwarts Legacy folders in app data with compiled shaders and the game just cannot solve it so it crushes.   Just delete those folders manually and the game will compile shaders again as it should from the beginning.,AMD,2026-02-15 07:39:15,1
Intel,o7fvggg,"That reg tweak does not work on latest builds as far i know, and there new tweaks you can do as well now, because its capable of kicking out stuff from MPO if it does not meet certain threshold now as well",AMD,2026-02-26 02:01:09,1
Intel,o4usu8w,"I don't think it was done on purpose. It's just unfortunate that a certain feature is clashing with how rdna4 handles path tracing. I tried cyberpunk path tracing on my 7800xt previously numerous times and didn't get any crashes, the only issue was a lack of performance.   This fix looks like it has to come from the developers side. And we know their priority is for Nvidia.",AMD,2026-02-11 19:44:01,4
Intel,o4uez0r,"tbf Nvidia sabotaging AMD is very likely, wouldn't be the first time.   But it's also very likely AMD just doesn't have or want to spend the ressources to get that resolved, lol.   As far as I read in this thread AMD still has issues with Warhammer 40k Darktide although the devs have contacted them a year ago.",AMD,2026-02-11 18:38:55,6
Intel,o4wwdbn,"I just got the card recently so I haven't done too much tuning yet. Unfortunately it doesn't have a dual BIOS (RX 9070 Challenger). But it's annoying that my drivers are consistently crashing at stock settings, and I see HWInfo reporting the clockspeeds going way above the stock values.",AMD,2026-02-12 02:27:13,1
Intel,o4z4kda,"It may very well be a wow problem, but when it occurs in games using completely different engines, I feel it's more of a driver issue.  I don't play WoW with raytracing, even then, I tried many different option types and the crashes still happen.   I've come to the conclusion some time ago that I can't fix it and I just have to live with random crashes until either Blizzard or AMD does something.",AMD,2026-02-12 13:14:13,1
Intel,o4uqmg0,"It crashes regardless of OC or not, my buddy with a 7800XT gets the same errors and it doesn't matter whether it's Retail or Classic WoW or whether it's directX11 or 12, low or highest setting. Unfortunately I don't ever think I will get a fix, despite reporting the error everytime.  The problem is that it's so random. I can play regularly for a month+ no problem and sometimes it crashes 3 times within an hour. Never saw it crash 4 times, yet anyway...  Only OC I have is my RAM @ 6000Mhz, but my friend has no and runs on default speed.   As said, only problem is WoW and Endfield(in some areas of the game, not all).",AMD,2026-02-11 19:33:25,2
Intel,o4uy5fe,its a known issue for 5-12 years with amd and wow and dx12.,AMD,2026-02-11 20:09:27,1
Intel,o4ufccx,"Ah ok, it can be forced via OptiScaler but apparently there are issues and it's prone to crash the game.",AMD,2026-02-11 18:40:37,1
Intel,o4ub281,"ik, cant wait for the next one!",AMD,2026-02-11 18:20:50,1
Intel,o6rfcbn,"Yeah, i do that every update but it has corrupted my drivers when uninstalling it sometimes",AMD,2026-02-22 12:30:22,1
Intel,o5yc4hw,I'm glad they finally did this because I have a desktopÂ  system with an IGPU that has rDNA 2 and Discrete GPU that is rDNA 3,AMD,2026-02-17 23:08:16,2
Intel,o4tlzlq,Calm yourself.  I never said it was an issue just pointing out that the app isn't seeing it yet.  And I play the finals so there is a reason for me to be interested in this driver.,AMD,2026-02-11 16:23:52,-1
Intel,o4ufd33,"No but some of these issues have persisted for over 6 month now.... <.< I guess they are completly understaffed, but doesn't fcking matter what matters is user experience.",AMD,2026-02-11 18:40:42,7
Intel,o4ujs3b,Is it even fsr3.1 then?,AMD,2026-02-11 19:01:03,2
Intel,o4tusa5,At least nVidia has cleaned them up.  AMD's drivers just keep getting worse.,AMD,2026-02-11 17:04:56,2
Intel,o4u3qrz,"Not true , my GF got a 3070 and installed EVERY SINGLE update.  No issues infact her performance slightly improved in certain games and got giant DLSS upgrades.  Meanwhile the 6800XT i have is my first amd GPU in like 10 years and i have DDU permanently on my PC because the driver randomly breaks in some way and i need to DDU to get it back to normal.  Like right now the amd interface wont autostart anymore ( i added it to the startup folder for now cause i dont want to DDU AGAIN )     if the memory craze wouldnt be here i would have bought THIS MONTH a 5070TI for me and my wife.",AMD,2026-02-11 17:47:03,2
Intel,o4u30d6,"I bought the amd 6xxx series for my two pcs but before that, I had Nvidia, and I never had issues with nvidia drivers.   Amd has always given me issues at some point when I bought em.  Had an hd 7870 that had some issue with final fantasy XIV, which was and is my main game.   The game kept crashing, so I thought the card was bad, so I rma for another card. New card same issue. So I just tell em send me whatever the equivalent Nvidia card you have. I got a 660ti, and it just worked.  Bought myself a 770 and 980ti, which both worked no issues.  The computer died right as I moved for a new job, so I just needed something cheap, so I bought an Rx580, and while it was mostly fine, it still had a few crashes.  The 3060 I got later had zero issues.  I have s 6800xt and a 6900xt and im on some older driver (like maybe 2 or 3 drivers ago) because when I update (full clean with DDU and all), i get crashes when I game.   Nvidia may have had driver issues last year, but I doubt their issues overall are as consistent as amd.  Last amd card I remember having no issues was my 5770 and my x850xt back when.",AMD,2026-02-11 17:43:39,1
Intel,o4wrudy,"All you had to do was switch to Linux, btw. You can have your cake and eat it too.   Why do you need FSR 4 on an XTX anyway?",AMD,2026-02-12 02:00:12,3
Intel,o4z63c3,"Im at the same boat as you, I hate the fact that I would lost 8gb of vram switching to 5080 :(",AMD,2026-02-12 13:23:25,2
Intel,o4tpe5f,"So? I want support on rdna3 i dont care about optional driver. I want something more. If you are happy its good for you, im not anymore.",AMD,2026-02-11 16:39:41,0
Intel,o4wrrho,It will be fixed with RDNA5. ðŸ˜…,AMD,2026-02-12 01:59:43,2
Intel,o52phks,"It has fixed it, yes. Although if you use minimal install it doesnâ€™t matter as you donâ€™t use adrenaline anyway.",AMD,2026-02-12 23:51:15,1
Intel,o4u4mqp,Nvidias drivers are close to pristine atleast i never needed to hard save DDU on my PC while i had nvidia cards. and neither does my wife with her 3070.,AMD,2026-02-11 17:51:09,1
Intel,o4wx7fn,What issue exactly?,AMD,2026-02-12 02:32:13,5
Intel,o4y8wxy,"my xbox series X died without apparently reasons after 5 years, my pc rocks even i unmounted gpu several times and changed thermal putty and paste",AMD,2026-02-12 08:48:01,0
Intel,o4x657u,I see a lot of people switching to nvidia despite the price hike for reasons like this,AMD,2026-02-12 03:26:44,5
Intel,o4wfotf,Someone tell them NVIDIA has a working feature...,AMD,2026-02-12 00:46:14,-1
Intel,o4v9k0r,"Huh? What's that got to do with noise suppression? There was already open source AI solutions well before AMD or Nvidia implemented them, such as RNNoise. It's not something that AMD or Nvidia specifically need to create but looking around, it seems it's still a bit a slow area in regards to AI advancements. There's DeepFilterNet but it hasn't really been updated in years though there's been a lot of claims of progress of DeepFilterNet3 with I'm assuming private branches.  As for easy for users to install there's SteelSeries Sonar but no idea of it's quality.",AMD,2026-02-11 21:04:58,1
Intel,o4torpx,this not a thing in years.,AMD,2026-02-11 16:36:49,-23
Intel,o4yhywr,"It's an optimization for PT, if anything the optimizations may have the potential to decrease the visual quality.",AMD,2026-02-12 10:16:51,1
Intel,o4v630f,"I know it for a fact that PT/RT with optiscaler being stable vs FSR is the fact that some stuff is not exactly there in the pipeline with optiscaler. Saw some discussion pointing this out, could be similar case here too.",AMD,2026-02-11 20:48:12,-6
Intel,o5idsdc,"I know, but I just don't have patience for that any more. Tab, scroll with keyboard, Alt-E for most AMD stuff. Eh, I'd rather just nuke the entire log in and start over. The only exception is if I haven't saved recently, but modern games are pretty good at auto-saving regularly. If I'm playing an older game that doesn't do that, I'll probably do it so that my last hour wasn't for naught.",AMD,2026-02-15 14:18:57,1
Intel,o4v7evj,>Or its not feature complete  This is the company that launch antilag+ intercept and redirect dlls in multiplayer games.   >or steam signed exclusivity  Valve isn't AMD with FSR exclusivity on sponsored titles or Epic Games. They don't do that.,AMD,2026-02-11 20:54:34,17
Intel,o4ydo63,they would infact upgrade to nvidia,AMD,2026-02-12 09:35:16,8
Intel,o4wv7a0,"RDNA2 supports INT8 instructions, older Radeons don't.",AMD,2026-02-12 02:20:17,6
Intel,o4x6c9u,we don't revive dinosaurs here,AMD,2026-02-12 03:28:00,3
Intel,o4yq6b8,AII ATI radeon 9800 pro gpus should also get added,AMD,2026-02-12 11:31:04,1
Intel,o6nxp3b,"Turned off the shortcuts within the Software for now. If it still closed itself, I will download the latest version of Visual C++ as well. Thx for the Info.",AMD,2026-02-21 21:06:00,2
Intel,o4utrbv,Yeah youâ€™re right. I wish they had a mesa driver on windows people could use tbh.,AMD,2026-02-11 19:48:27,3
Intel,o4usarp,So is that why the issue doesnt exist with the exact same drivers on Linux,AMD,2026-02-11 19:41:25,1
Intel,o5fva55,Peasants are consoler gamers. We're like Knights or something... As opposed to Nobility,AMD,2026-02-15 01:59:05,1
Intel,o69kq5j,"Hey OP â€” Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2026-02-19 16:34:36,1
Intel,o4zwc65,"Had that as well, rollback fixed it but very annoying issue to have",AMD,2026-02-12 15:41:06,1
Intel,o5e7594,"FYI no need to hard reboot. Just press ctrl-alt-delete, and using the arrows, go to task manager and find Radeon Software. Navigate to it using the arrows and press Alt+E to force close it.",AMD,2026-02-14 20:04:00,2
Intel,o5fvly1,"My undervolt never resets if it is just a game crash, which are rare for me anyway.  For example, rhe other day I DLL swapped FSR 4 INT8 into MH Wilds and it crashed on startup everytime but never reset my undervolt on my 7900 XT (turns out after Title Update 4 of Wilds you only DLL swap the upscaler file and not the other two)",AMD,2026-02-15 02:01:17,1
Intel,o5153mx,"Yes, massively.   AMD can show you more details and pretty much offers everything Nvidia does in ONE app not stupidly split between Nvidia app and the Control panel",AMD,2026-02-12 19:10:08,1
Intel,o514yqj,"It's still missing a lot of information I can get out of AMD like Vram Util, CPU TEMPS instead of just a percentage + AMD has everything in Adrenaline while Nvidia spilts all of it's features between the app and the control panel, yes Nvidia DID get better but I still think, if we just look at the software, AMD is ahead, but again that's the only thing they are better at.",AMD,2026-02-12 19:09:29,2
Intel,o4tqgeo,or not I don't have this issue so it doesn't matter to me.,AMD,2026-02-11 16:44:38,1
Intel,o4uvmbq,I was wondering if thatâ€™s what this meant. I noticed on dark sky maps instead of stars it would just look like a grid with dots all over it.,AMD,2026-02-11 19:57:14,1
Intel,o4utu5v,I need to check into optiscaler,AMD,2026-02-11 19:48:49,4
Intel,o4wlx6l,I'll have to look into that mod,AMD,2026-02-12 01:24:02,1
Intel,o4urn3n,"Yh, some of the flaws would actually be resolved with a good denoiser. If you turn off Ray reconstruction on my 5070, shimmering/flickering is very noticeable with dlss. Ray reconstruction actually stabilize the image and can add certain details, so the upscaler is not doing all of the work visually. AMD RR would definitely be a game changer. Unfortunately I can't see the devs going back to optimise path tracing and include RR specifically for AMD.",AMD,2026-02-11 19:38:17,2
Intel,o4uj4q5,1440 I can see for sure.,AMD,2026-02-11 18:58:03,1
Intel,o669vw4,Try 25.12.1 it's the update i'm on and it's running pretty good so far for me,AMD,2026-02-19 02:40:22,2
Intel,o4w6i82,https://www.nexusmods.com/cyberpunk2077/mods/25673  Its nvidia,AMD,2026-02-11 23:53:05,3
Intel,o4udh40,7000 did as well btw (didn't produce playable performance in the first place tho),AMD,2026-02-11 18:32:01,2
Intel,o5hmg05,What does that have to do with AMD drivers?,AMD,2026-02-15 10:53:32,2
Intel,o4yotze,"I'm sorry to say, but this isn't normal behavior. It absolutely should not happen, and it screams hardware defect to me. If this keeps happening on older drivers, then I'd RMA the card entirely - sign of bad silicone!",AMD,2026-02-12 11:19:40,2
Intel,o4ugfrt,"As usual, I have to use Optiscaler. If it weren't for the developers of this mod, AMD would be even more screwed than it is now. I use it in this game and I have no problems. The crash occurs when using frame generation via Optiscaler; the game simply won't launch until you remove the mod. Otherwise, everything works fine",AMD,2026-02-11 18:45:39,1
Intel,o60b7x5,"Ehm, haven't they always provided a combined one since driver branching started? The issue here is the lack of the separated packages. Sometimes though you had to hunt a bit for one or the other...",AMD,2026-02-18 06:12:44,1
Intel,o4uj6r8,Or maybe theyre issues amd cant fix like cyberpunk being a literal nvidia feature causing the problem,AMD,2026-02-11 18:58:19,12
Intel,o4vky95,they have 300 billion dollar cap they dont have excuse except they want shareholder value,AMD,2026-02-11 21:59:16,1
Intel,o4uquzw,It shows as fsr3.1 in the game menu but it might be fsr3.0 and the idiots labelled it wrong. Not sure if there is way to identify the version.,AMD,2026-02-11 19:34:34,1
Intel,o4uiz8j,https://www.tomshardware.com/pc-components/gpu-drivers/users-celebrate-50-percent-performance-gains-following-nvidia-hotfix-driver-patch-fixes-october-windows-11-cumulative-update-that-broke-performance-in-some-games  At least amd drivers work when windows updates,AMD,2026-02-11 18:57:21,7
Intel,o4u6bkd,its just amd fan people that didnt had a nvidia gpu the last 10 years so they literally dont know that you dont experience all the issues you do on amd and just think what they read here is true that they have the same issues.,AMD,2026-02-11 17:58:54,0
Intel,o4uis36,Nvidia cant make a driver that doesnt shit itself whenever windows updates so,AMD,2026-02-11 18:56:27,-3
Intel,o5161w9,"No thanks, I play games that don't work on Linux (yes Kernel lvl anti cheat)   Also, I'm a software dev for windows applications.   And also, what does switching to linux have to do with the whole topic?  And I need FSR 4 Because FSR 3 looks significantly worse then even XESS on it's fallback mode a lot of the times and even the 7900XTX needs upscaling for pretty much any RT game, and no I am not turning down settings on a 1200â‚¬ GPU (when I bought it)",AMD,2026-02-12 19:14:41,1
Intel,o5166u5,"Yeah that sucks :c but the bonus RT performance, the WAAAAAY better upscaler + the fact that most of these software features get actually used in games, still keeps me happy",AMD,2026-02-12 19:15:20,1
Intel,o4tpzvk,until AMD says something about it officially its pointless asking about it at every driver release.  So instead of wasting your own time focus on the driver drop what it fixes and if you have games that need support.  I'm an RDNA 3 users aswell and I don't see the point asking for this every driver release they will or will not do it just let it be.,AMD,2026-02-11 16:42:30,0
Intel,o4u580t,"Nah, the problem is with 4000 and 5000 gen, check forums, they have problems with 1 year without solution (like the recent problem with hdmi that amd fixes in the next drivers), i think it's maybe because of all the new features with the app, DLSS, FG, MFG, and so...",AMD,2026-02-11 17:53:52,3
Intel,o4xsgyd,"It doesnâ€™t turn on. The spinner spins for a second after clicking the button, and the option stays off.",AMD,2026-02-12 06:13:25,5
Intel,o4y9abu,What issue? It doesn't work whatsoever lol.  Hasn't for months.,AMD,2026-02-12 08:51:38,3
Intel,o4x8a23,"Yeah im glad i switched before the hike, best way is used now",AMD,2026-02-12 03:40:45,3
Intel,o52a31g,"They're not much better, they've all taken the AI road.   But Nvidia had such a head start with dlss they are still way ahead.   Tons of problems with Nvidia.   The problem is the future though they're not dedicating people to this anymore they're doing it to AI where the billions to be made until it dies off.",AMD,2026-02-12 22:26:51,3
Intel,o4xsu3s,The last nVidiaâ€™s card IÂ had was the GTXÂ 275. But the next one would be nVidia noÂ doubtâ€¦,AMD,2026-02-12 06:16:40,1
Intel,o4tpe9q,https://i.redd.it/07wggcrp8wig1.gif  Very much a thing.,AMD,2026-02-11 16:39:42,24
Intel,o4tuu9k,I just did it yesterday...,AMD,2026-02-11 17:05:11,5
Intel,o4u1k8f,For the longest time it was indeed not a thing.      But it was recently added.   But it seems to have a bug.,AMD,2026-02-11 17:36:51,3
Intel,o4u2msd,This was actually introduces like Half a year or year ago and STILL is a thing.  Press the 3 dots next to the driver in the manager.     Man if people would stop spreading false stuff like facts.,AMD,2026-02-11 17:41:53,1
Intel,o4yl5kd,"From the human eye, there's no visual difference. I also would be pretty confused if the visuals degraded but the performance did not at least increase.  Its like if I enable dlss ultra quality and degrade the visuals but keep the performance of DLAA, it wouldn't make much sense.",AMD,2026-02-12 10:46:56,1
Intel,o4v9ndr,"No. Optiscaler is not missing anything to make PT/RT stable. I think you are getting confused when dlss inputs are used and PT is enabled, Ray reconstruction is automatically enabled which doesn't work on AMD gpus, so the denoising looks grainy or not 100% rendered, which then boost performance. When xess/fsr inputs are used then Ray reconstruction is disabled, therefore only the in-game denoiser is being used and everything is rendered properly.  Optiscaler doesn't make PT anymore stable or less stable. Optiscaler produces overall better quality visuals than the in-game fsr4, because it uses the correct fsr 4 models for each quality settings. but that's it.   Disabling sharc doesn't boost performance on my 9070xt and doesn't change anything visually.",AMD,2026-02-11 21:05:25,10
Intel,o5igfto,This issue has been fixed for me by first using ddu to uninstall my driver and then updating to 26.2.1,AMD,2026-02-15 14:34:13,1
Intel,o4v831b,Steam machine? Hello?  And antilag still doesnt disprove my point?  Stay on topic,AMD,2026-02-11 20:57:49,-10
Intel,o4zzt4r,"Radeon VII does, but any generation of GCN should be completely out of the picture.  Navi 10 doesn't, yet Navi 14 does, then all of RDNA2+. AMD likely ran out of time before tapeout for Navi 10, as Sony needed the silicon finalized to set their semi-custom chip up. Doesn't make sense that a lower tier Navi GPU has DP4a otherwise.",AMD,2026-02-12 15:57:22,1
Intel,o4ux27l,"i wish ! i mean amd clearly doesnt want to put in the quality they should , and amd absolutely got a few hardcore  people in their user base which would happily fix it for them.",AMD,2026-02-11 20:04:11,1
Intel,o4usnp8,"Linux drivers are open source from amd and get fixed by users (including that issue  and tons more )  , we on windows need to rely on amd to fix stuff.  so yes a good eye of yours the community is fixing amds mess on linux ! same for FSR4 on Vulkan works on Linux ( which doesnt on windows due to amd negelect )  i know you really want to defend amd here  ( for some reason you think a company is your friend and needs defense ? idk ) with half researched fake facts but man ... pls atleast research.",AMD,2026-02-11 19:43:08,4
Intel,o4utc5o,u/evonos blocking ppl because your wrong really tells everyone what they need to know  Btw my homie uses amd drivers not community drivers and still has zero issue with darktide lol,AMD,2026-02-11 19:46:24,2
Intel,o4ybgvb,"the hell?   It's not the same driver lmao, not even ""the same game"", as it has DXVK and all the bs",AMD,2026-02-12 09:13:16,2
Intel,o5g9dp5,I feel like a peasant though ðŸ˜”,AMD,2026-02-15 03:36:27,1
Intel,o51daia,">AMD has everything in Adrenaline while Nvidia spilts all of it's features between the app and the control panel  Which features are missing in their new panel ? i just checked on my wifes PC and couldnt find any.  >but I still think, if we just look at the software, AMD is ahead  HAHA no.  specially not if you just say ""Software"" again since i own the 6800XT i basicly have DDU permanently installed , i never had this with my prior 6 or 7 nvidia gpus.",AMD,2026-02-12 19:49:31,1
Intel,o4y07pk,"Only volumetric clouds at night with ""Effect"" setting higher than ""Medium"" were affected. Rest of the sky was fine.",AMD,2026-02-12 07:23:31,1
Intel,o4v0lkj,"Optiscaler is just an overall great tool to have, just like afterburner, Lossless ect. I had it even when I was using my 7800xt,7600.   Play around with the custom  resolutions/ Upscale ratio override as well.",AMD,2026-02-11 20:21:18,3
Intel,o4xin5z,"Yeah, I don't think it'll happen either. The only way that happens is if the OptiScaler guys find a way to inject Ray Regeneration into the game.",AMD,2026-02-12 04:53:46,1
Intel,o4xiqm0,"To be fair, the 9070xt is a 1440p card, not a 4K one",AMD,2026-02-12 04:54:29,1
Intel,o5hpafj,- you lunch game  - game crushes video driver  *I think my motherboard is broken* /s  Also I have found that other people had the same issue with Radeon cards.   I forgot that Steam had this community board and maybe I will find more info there and it was more info and solution.   It looked like a GPU driver problem. Like I had with Helldivers and Forza Motorsport before (both were fixed by driver update),AMD,2026-02-15 11:20:11,1
Intel,o5e7lwl,He said that doing a driver-only install gets rid of the problem.,AMD,2026-02-14 20:06:32,1
Intel,o6qxtck,"Wanted to follow up after more troubleshooting - Installed Adrenalin 25.9.1 and haven't had any issues, even with a full install. Similar situation that I've seen a lot of others mention, where the issue pops up on versions newer than 25.9.1.",AMD,2026-02-22 09:50:32,1
Intel,o4vkv3d,"Looked into it now and apparently using Xess inputs it's fine (I can confirm, started using it and played for an hour with no crashes).. the reason I went with AMD on my upgrade is because OptiScaler exists, absolute god send.",AMD,2026-02-11 21:58:51,1
Intel,o60mh46,My mistake I just realized I was referring to my desktop My framework is all already in A3 on both the APU and discreet GPU. My desktop has an RDNA 2 IGPU which I do use in some workloads and a discreet RDNA 3 GPU The annoying thing though when I used DDU of my desktop it uninstalls both drivers and when they're not in a combined driver I end up with a APU that is running the Microsoft basic display adapter driver. I like to have both working troubleshooting purposes and also just in case if I ever need a fifth monitor or need to use hybrid display mode for light if my power is out and running on a generator.,AMD,2026-02-18 07:51:22,1
Intel,o516r9s,"why would they say ""we are working on it"" if they can't do anything about it?   Yes, this problem IS caused by Nvidia.   But AMD says ""AMD is actively working on a resolution with the developer to be released as soon as possible.Â ""   For several month so either they are lying about working on it, or they are not able to fix this, idk what about this is true, but they can't constantly post that and expect people not to blame them lol.",AMD,2026-02-12 19:18:03,1
Intel,o4usiqr,"I know theres a registry edit to show the exact dlss version but it seems like it doesnt work for fsr, but what about optiscaler, usually it shows the inputs version",AMD,2026-02-11 19:42:29,2
Intel,o4z5x1t,this sounds funny because AMD had to lock clean install for a year on their driver installation because it could brick w11 lmao,AMD,2026-02-12 13:22:22,2
Intel,o4ukhy4,Yea they just dont work when updating their own drivers from my experience.,AMD,2026-02-11 19:04:24,0
Intel,o4vnev8,"as an owner of every generation of nvidia gpu.... it's hilarious that people think that nvidia's gpus are never without issue, historically having the highest rate of issues AND catastrophically so that has require nvidia to pull them entirely.     Both have roughly the same experience on the whole, with a slight lean in favor of amd's gpus for stability and cross game support. Nvidia's however, doesn't.",AMD,2026-02-11 22:11:22,6
Intel,o4uiw5y,https://www.tomshardware.com/pc-components/gpu-drivers/users-celebrate-50-percent-performance-gains-following-nvidia-hotfix-driver-patch-fixes-october-windows-11-cumulative-update-that-broke-performance-in-some-games  Sooooo,AMD,2026-02-11 18:56:58,-4
Intel,o4uk6v6,Ive never experienced that on any Nvidia card ive erver had so. Ill dig out my 3060 and see if thats the case,AMD,2026-02-11 19:02:58,0
Intel,o519faw,"If you switched to Linux or dual booted with Windows as your 2nd option (the distribution being CachyOS, for example) you'd get access to their modified Steam Proton version that has an FSR 3.1 upgrade function (the one 9000 series GPUs utilize for FSR 4 support), except it'sÂ  for RDNA 3 as well.   Most invasive kernel-level anticheat games either don't support FSR 3.1 anyway or they don't need upscaling at all, since they're e-sports optimized games (also why are you allowing ring zero access to third party corpos?). The rest will work perfectly, which is about 95% of FSR 4 supported titles.   And yes, I agree that FSR 3 obviously looks much worse since it doesn't have any intelligent upscaling, just algorithmic voodoo stuff. But for native AA it's perfectly enough, from my own testing. For RT upscaling, though - tough luck. Only Linux or Optiscaler will help.",AMD,2026-02-12 19:30:51,1
Intel,o4tv3u5,"everyone will stop asking just because you asked, lol. it's obviously not pointless and it's much needed feedback, people aren't happy about it",AMD,2026-02-11 17:06:26,5
Intel,o4tu5wd,how are supposed to let them know we are not happy? lol. makes no since. if they wanna see the comments go away then they should release fsr 4 for rdna 3.,AMD,2026-02-11 17:01:59,3
Intel,o50qapw,"Yeah the app its trying to run is being blocked by Windows you see the message pop up if you try to manually run that file for noise suppression, its basically a corrupt file which makes Windows block it, World of Warcraft in rare case has it as well fix is deleting the file and then scan repair on the game, but that obvious not an option on the drivers as reinstalling them wont fix the issue.",AMD,2026-02-12 18:01:05,2
Intel,o4zyibq,"You sure you haven't messed with services and task scheduling? I did and I know that one of the things I disabled gives support to noise suppression. I'm not dismissing anything here, just trying to help.",AMD,2026-02-12 15:51:22,1
Intel,o4x9ioh,Would you recommend Nvidia to all your friends and family for the foreseeable future?,AMD,2026-02-12 03:49:01,1
Intel,o4tq9o4,Its there but as this driver notes says   **AMD Install Manager is unable to install Optional Drivers. Users are recommended to use the installation link available on this page.**,AMD,2026-02-11 16:43:45,-13
Intel,o4tq6qm,"from their site:  >preferred method for downloading the latest driver for all AMD products can be found using the Auto-detect and Install Driver Update Option Available here.  if you want download the optional click on the link  and go to The ""AMD Software: Adrenalin Edition 26.2.1 installation package can be downloaded from the following link""",AMD,2026-02-11 16:43:22,-18
Intel,o4tux9z,No you didnt,AMD,2026-02-11 17:05:35,-8
Intel,o4ywutn,"Well, in this particular case it seems the devs are running it on AMD when they shouldn't be causing it to crash and not actually end up running so it shouldn't touch visuals on AMD cards. On Nvidia I imagine it increases performance.",AMD,2026-02-12 12:22:55,1
Intel,o4ygtgp,Dont ever criticise Nvidia for doing anti-competitive things that they've done time and time again bud...  /s if it wasnt obvious.,AMD,2026-02-12 10:05:54,3
Intel,o4vc0mo,> Steam machine? Hello?  I'm going to point you to the fact that any of the software improvements the steam deck brought about have been open to everyone on other hardware and distros.   Why the fuck would Valve pay for exclusivity for an FSR offshoot? Especially when it'd be screwing some of their existing customers on Windows and Linux?  If you're going to make up hypotheticals at least keep it within the bounds of reality.    >And antilag still doesnt disprove my point?  AMD has no issues historically with shipping halfbaked software.,AMD,2026-02-11 21:16:48,10
Intel,o4v81tb,I mean there is an alternative in the form of linux at least folks could just give dual booting a try,AMD,2026-02-11 20:57:39,0
Intel,o4yf25w,"If you use the amd official drivers on linux its literally is the same drivers. DXVK doesnâ€™t make it a different driver nor does it magically become a different game when using it, saying it does just makes you look ignorant af.",AMD,2026-02-12 09:49:09,0
Intel,o5bec8r,AMD just shows you a more metrics like Vram util or CPU temp,AMD,2026-02-14 09:45:27,1
Intel,o4v17xg,"Yeah, I'm finishing up Cyberpunk 2077 and that has FSR4 in it, so I haven't rushed into optiscaler yet. I will definitely for games that do not.",AMD,2026-02-11 20:24:20,1
Intel,o4yl5u9,I'm playing 4k cyberpunk at steady 60fps with some RT on and mostly high settings,AMD,2026-02-12 10:47:00,1
Intel,o51b4t0,"Because if amd could do it, it would be done by now. Amd can only do so much when its the actual game that needs tweaking. Working on it is them telling cdpr what to fix and cdpr needing to fix it",AMD,2026-02-12 19:39:06,1
Intel,o4uzbm8,Optiscaler doesn't detect the fsr input so cannot get any info. It's very likely it's fsr3.0 built into the game code. It's sad that devs are still using deprecated versions when fsr3.1 is available.,AMD,2026-02-11 20:15:07,1
Intel,o4zwc84,So yur saying amd actually did something proactively instead of just letting their shif break unlike nvidia?,AMD,2026-02-12 15:41:06,2
Intel,o4ulb6a,Sounds like a skill issue seeing as adrenaline is 1 click updating,AMD,2026-02-11 19:08:15,1
Intel,o4vot24,thats a 10/10  parody / irony  comment ( hopefully it wasnt serious then its just fake ),AMD,2026-02-11 22:18:19,-2
Intel,o4un02b,"I think Iâ€™ll take that over timeouts any day, hotfix was sent out two days after btw I specifically remember applying it for my SO",AMD,2026-02-11 19:16:16,2
Intel,o4unzu0,"this was Microsoft who fucked up which they later fixed in a Windows update and nvidia made a shoe horn fix for their users.  there was even discussion in the amd subs if amd hardware was affected too by this windows bug and there was the usual conspiracy stuff in nvidia subs discussing if microsoft wants to damage nvidia.  So "" Awesome nvidia thanks for fixing windows bugs "" would be correct  also it was literally in **2 days**.  how long does amd take to fix the issues ? years ? if ever?",AMD,2026-02-11 19:20:56,2
Intel,o4utp0q,"Nvidia themselves acknowledge it, so nobody is saying you are lying, but if the creator of your GPU is acknowledging a windows problem, then I wouldn't dismiss it just because you personally haven't had any issues allegedly.  Majority of AMD users don't have driver issues, but the minority that do can't be dismissed. They most likely need additional help figuring out their system.",AMD,2026-02-11 19:48:08,2
Intel,o4ukdzj,https://www.tomshardware.com/pc-components/gpu-drivers/users-celebrate-50-percent-performance-gains-following-nvidia-hotfix-driver-patch-fixes-october-windows-11-cumulative-update-that-broke-performance-in-some-games,AMD,2026-02-11 19:03:53,1
Intel,o4ui8sj,What AMD gpu are you using?,AMD,2026-02-11 18:54:00,1
Intel,o4tuvrr,you guys constantly post this in many threads I think they got the idea already.  And nothing has changed.,AMD,2026-02-11 17:05:23,-2
Intel,o4zz3pu,"Didnâ€™t change the services or task scheduling, but have disabled some startup apps. Having said that, the `AMDNoiseSuppression.exe` is enabled, and I doubt it uses `OneDrive.exe` or Teams (which are the only ones Iâ€™ve disabled).  Thanks for the idea, though.",AMD,2026-02-12 15:54:07,2
Intel,o4xari8,"Depends on the price and use case, if u just wanna play rocket league and sims, an old used amd gpu might be the best value",AMD,2026-02-12 03:57:16,3
Intel,o4ttz97,Yes. Thats what people are talking about. They broke their own feature in this update,AMD,2026-02-11 17:01:07,25
Intel,o4tvhxm,Yes he did. Im a doctor.,AMD,2026-02-11 17:08:16,9
Intel,o4twpkc,I did. I installed the 25.6.x something optional because it was the only way I could get my ABI game to launch after I'd fiddled with the graphics settings and I didn't want to redownload the game.,AMD,2026-02-11 17:13:57,5
Intel,o4vgyjl,"Unless valve signed exclusivity to help sell the thing, you know the underpowered as is with 8gb of vram thing?  Amd also has improved the fuck out of how they deliver their software and in what state they do",AMD,2026-02-11 21:40:20,-6
Intel,o4v9nzp,or .. amd could fix their stuff. sadly dual booting wont fix the incompatibility issues with some of my hardware.,AMD,2026-02-11 21:05:30,1
Intel,o4yhqzl,"DXVK makes it run with the Vulkan driver, not the dx12 driver, which doesn't even exist on Linux.  And besides, who even uses the official amd driver?  99% of the users use Mesa, hell, AMD discontinued their own linux vulkan driver in favor of RADV",AMD,2026-02-12 10:14:48,2
Intel,o5bu1df,that absolutely most people use with the lack of good fps limiter ( FRTC is heavily outdated even a amd employee said )  and no Background fps limit that amd is missing vs Nvidia ( I use for both Afterburner )  and dont forget the metrics of the amd driver randomly breaking on Updates needing a full DDU too.,AMD,2026-02-14 12:12:30,1
Intel,o4yrpks,"Yeah, but for long term it's a 1440p card, especially with the 16GB VRAM.",AMD,2026-02-12 11:43:52,1
Intel,o4v1g3v,What is the file called?,AMD,2026-02-11 20:25:27,1
Intel,o54zw3y,"yeah, it took them a year, lets see, if this will took nvidia a year aswell.",AMD,2026-02-13 09:51:23,1
Intel,o4um1a8,And that one click updating often installs drivers that need to be Uninstalled because they are buggy.  On the other hand when I have done installs with nvidia cards it has not resulted in that behavior.,AMD,2026-02-11 19:11:41,6
Intel,o4unri6,It wasnt two days it was 2 and a half months of my friends bitching about it,AMD,2026-02-11 19:19:52,0
Intel,o4uln9t,"You can keep spamming that at people if you want. Im telling you, as a person, that has owned more amd cards than Nvidia 5 amd to 4 Nvidia. I have had no issues with the with the Nvidia cards.",AMD,2026-02-11 19:09:52,1
Intel,o4u478m,and yet its super important to not gatekeep feedback and keep going with it!  WE WANT FSR4 ON rdna 2 / 3 !!!!!!,AMD,2026-02-11 17:49:09,3
Intel,o4txq82,"even if they choose to ignore us, customers who are thinking of switching to amd will think twice after seeing this many unhappy customers. not to mention the brand loyalty they are gonna lose from current customers.",AMD,2026-02-11 17:18:43,3
Intel,o4vh9uh,"> Unless valve signed exclusivity to help sell the thing, you know the underpowered as is with 8gb of vram thing?  Again when has Valve ever done that? They're bankrolling proton and shit that benefits everyone. Why would they do that?  How would it even help FSR adoption for them to do so? No one is buying it for FSR.",AMD,2026-02-11 21:41:50,5
Intel,o4vgbo6,And if they donâ€™t then what? Just live with it? Everything you listed in your flair should run just fine on any modern gaming distro.,AMD,2026-02-11 21:37:20,1
Intel,o4zw6h3,"My homie who lost a fuckton of data to a ""community driver"" that was malware",AMD,2026-02-12 15:40:21,0
Intel,o4z169o,"Long term I'm not sure many cards of this gen are going to be 4k cards save for the 5090, and 5080. Then again, with RAM shortages now and no new gpus this year, developers are going to have to constrain to some degree.   I'm also hopefully that AMD continues improving fsr quality as well as availability, but I'm not holding my breath on it.",AMD,2026-02-12 12:52:37,1
Intel,o57581j,amd\_fidelityfx\_dx12.dll is the FSR3.1 dll file that Adrenalin needs to upgrade to FSR4.,AMD,2026-02-13 17:31:48,2
Intel,o4uma6t,Then just dont update?,AMD,2026-02-11 19:12:51,-1
Intel,o4upbof,The article you sent acknowledges it as an issue that started in October and was patched in November. With a hotfix at the beginning of November. After Microsoft rolling release ended at the end of October. Did you read what you sent my dude?,AMD,2026-02-11 19:27:15,2
Intel,o4unyr5,The link above proves thats a lie because that issue affected *everyone* with an up to date nvidia driver,AMD,2026-02-11 19:20:48,2
Intel,o4uih83,lol you don't even know what you are asking?  you want FSR 4 on RDNA 4?  Really?,AMD,2026-02-11 18:55:04,1
Intel,o4txyyw,no you guys will continue to do what you do all i'm saying is not the best use of your time but you do you.,AMD,2026-02-11 17:19:52,1
Intel,o4vib6i,"When has valve ever made a full blown ""console""? And fsr will basically be required for anything remotely recent on it  Not to mention it does help adoption when the largest storefront on pc makes fsr their go to",AMD,2026-02-11 21:46:43,-2
Intel,o4vzva0,"yet the flair is too limited to list it all , and i sadly have incompatible things.  >Just live with it?     nope thats why i highlight all the issues.",AMD,2026-02-11 23:15:54,0
Intel,o5i3uvb,"Yup, that's my point. Only the 5090 and 4090 are true 4K cards.",AMD,2026-02-15 13:18:01,1
Intel,o57cs5p,Right but whats it called in the game,AMD,2026-02-13 18:08:09,0
Intel,o4umsaf,I dont which is what I said in my initial post. Im on  2 or 3 version old drivers because they work.,AMD,2026-02-11 19:15:13,2
Intel,o4uppzp,Oh so there was *another* issue that needing hotfixing earlier than december? Lmao,AMD,2026-02-11 19:29:08,-1
Intel,o4uq9yv,"Are you not reading? I said i have a 6800xt and a 6900xt. I said that in my buying history ive never had issues with nvidia cards.   Did you not see me write that 'll dig out my 3060 to see if that's the case?   So let me put this clearly. Im saying in my time owning and having an nvidia card i have never had any issues with drivers. Thats from the 660ti to the 3060.   I will install my old 3060 and see if it breaks with every update, as you said, as I can roll back windows and then update again to test.  Even should thst be the case, it would still put the score and nvidia driver related issues experienced by me at 1 vs amd driver related issues experienced by me a numerous.",AMD,2026-02-11 19:31:45,0
Intel,o4unodi,Oh yeah sorry auto correct and literally making the comment on the shitter.     Fixed so you feel better !,AMD,2026-02-11 19:19:27,2
Intel,o4u4cw8,>all i'm saying is not the best use of your time but you do you.  Your arguing about people giving feedback in a comment chain with tons of replys from you quite ironic.,AMD,2026-02-11 17:49:53,2
Intel,o4vjcz5,"The Steam Deck is no less a ""console"" than the ""gabecube"" will be.",AMD,2026-02-11 21:51:43,1
Intel,o4wckvf,â€œHighlighting issuesâ€ without doing anything about is just living with but also added complaining about it. Itâ€™s thoughts and prayers but with added whining and I bet dollars to donuts your â€œincompatibilityâ€ is you just doing something wrong or easily fixable with a small tweak.,AMD,2026-02-12 00:28:11,1
Intel,o4uqcjl,i mean.. do you want to ride a windows issue nvidia fixed in 2 days now for eternity and want people do dig out issues for 5-12 years on amd to prove things to you ( like WoW dx12 on amd ) or what ?,AMD,2026-02-11 19:32:06,3
Intel,o4urp36,"Your moving goalpost, idk what issue youâ€™re even talking about in December. I hate nvidia shills and AMD shills and your the latter.",AMD,2026-02-11 19:38:32,1
Intel,o4uqt05,Well we are in the present my guy  Old experience before nvidia moved half the team to enrerprise drivers means jack shit when evidence in the present is contrary to your experience,AMD,2026-02-11 19:34:18,1
Intel,o4uhzhc,Really what feedback does asking for FSR 4 add to this specific driver update?  Is that feedback about this actual driver?,AMD,2026-02-11 18:52:47,2
Intel,o4uz4nc,a top 1% commenter telling us how to use our time properly might be the single greatest ragebait.,AMD,2026-02-11 20:14:10,-1
Intel,nyiyx49,I'll believe them (either Intel or AMD) when I see the benchmarks.  Until then this is all just pointless noise.,AMD,2026-01-09 02:57:02,214
Intel,nyj1h7b,well yeah you can't compare them because strix halo is on a signficantly larger die wheras panther lake is more comparable to something like the hx370.   If amd is able to get strix halo at a competitive price then sure it will compete but the issue is that with such a large die I don't think it is possible for them to compete in price with panther lake,AMD,2026-01-09 03:11:08,60
Intel,nyj2ydj,Iâ€™ll never understand why AMD is not committing to design RDNA4 based APUs and at this point I just take RDNA 3.5 as a joke because they canâ€™t even support FSR4 on it officially nor the RX 7000 cards.  Itâ€™s like they are losing on purpose,AMD,2026-01-09 03:19:12,62
Intel,nyja8ig,"AMD has this â€œitâ€™s good enough for a while and weâ€™ll release something great that people will forget this happenedâ€  Vega lasted in mobile for nearly 5 years and got RDNA2 designs. Now, itâ€™s RDNA3.5 being built for mobile platform and betting on that to be good enough until RDNA5/UDNA bridge die designs releases (unverified rumor)  AMD also has this weird obsession with competitor naming. Sure, itâ€™s meant to confuse buyers but itâ€™s hurting them than helping, maybe it does help in terms of inventory.  Theyâ€™re not intel-like of stagnation. Theyâ€™re competing but not for us in the consumer market and weâ€™re just getting scraps until enterprise trend die down (currently AI trend/bubble).  Well, itâ€™s understandable as Zen designs are really focused in Epyc and scale down to Ryzen SKUs.  And the 400 series is a bad refresh when Ryzen 6000 mobile is the definitive refresh they have done, Zen 3+ and move to RDNA2. AMD couldâ€™ve done similar commitment but itâ€™s not currently.  Also, AMD forgor Strix Halo laptops are still nowhere to be found aside from 1 or 2",AMD,2026-01-09 04:00:20,11
Intel,nykgnvm,"Except that the B390 will be far more common as it will be seen in far more laptops. Yes, the 8060S & 8050s can be found in some laptops, but for the laptops you'll find in places like Currys, Best Buy or Mediamarkt, the B390 will be the most powerful iGPU you'll likely find & it'll happily outdo a Radeon 890M",AMD,2026-01-09 09:30:29,7
Intel,nyj02vs,Amd really doesnâ€™t gaf about anything other than data centre these days,AMD,2026-01-09 03:03:22,11
Intel,nyjy5yc,Idgaf when Strix Halo products are nowhere to be seen (notebooks),AMD,2026-01-09 06:46:12,7
Intel,nyj9p6p,"AMD is at the point where Intel was before they went down the route and are recovering, history repeats before it's too late.  Not going to believe either until we get actual benchmarks and results.",AMD,2026-01-09 03:57:09,8
Intel,nynggco,"AMD is playing the same intel book a few years ago. Except now instead of 14nm+++++++, it is RDNA 3.5555555.",AMD,2026-01-09 19:23:52,3
Intel,nz0u8e4,Meanwhile AMD keeps putting out new chips with years old GPUs.,AMD,2026-01-11 19:18:48,3
Intel,nyjg6co,Core Ultra 2 is already a better mobile soc.  I don't know why AMD thinks 12-16 cores is more important than battery life when it comes to laptops.,AMD,2026-01-09 04:35:45,8
Intel,nykbcoh,The Intel igpu has a better upscaler by far. FSR3.1 is a third class competitor in comparison. People have been crying out for AMD to release FSR4 for RDNA3.5 but AMD has some seriously stupid execs in charge.,AMD,2026-01-09 08:41:31,4
Intel,nyjsbo1,It does sound complacent but ultimately the proof is in the pudding.,AMD,2026-01-09 05:59:29,2
Intel,nyku0xe,"If it is not even fair to compare (because Strix Halo is WAY more watts) then why is AMD comparing them? B390 will exist, Strix Halo virtually does not in laptops.",AMD,2026-01-09 11:27:40,2
Intel,nykofoz,Lot of markets and Intel did bribe the oems for decades and still do,AMD,2026-01-09 10:40:44,3
Intel,nyjsl7o,Benchmarks first,AMD,2026-01-09 06:01:32,1
Intel,nyn2qhh,Beware hubris.,AMD,2026-01-09 18:22:54,1
Intel,nynbvp5,"There are already benchmarks, look them up.",AMD,2026-01-09 19:03:00,1
Intel,nynoj5f,Don't they already have an integrated GPU that's on par with an RTX 4060? According to Framework?,AMD,2026-01-09 20:00:51,1
Intel,nyorjcc,"strix halo is nice and all, but too prohibitively expensive to be considered for many people  arc b390/b370 will be available in much cheaper products for which amd doesn't have a proper answer to atm. amd's next lineup can't be lazy if they want to stay competitive",AMD,2026-01-09 23:05:05,1
Intel,nyqdhsx,"The GPU doesn't matter if you don't have proper drivers and they are so far behind still Intel.   Great progress, but the drivers are still going to be the thing that makes people say no.   If intel keeps on chugging away and they work with all the DirectX games backwards and going forward.   I'm talking past DirectX games, you can't just worry about the new games there's games that are older that don't run well.   It took AMD many many years to get decent drivers, Intel I don't know if they're just focusing on hardware and not the drivers, but that so far is what's been holding it back.   Hopefully they can release a true dedicated GPU back in rival something that's out there at a much better price that will bring at least some competition back until the AI scam is over.",AMD,2026-01-10 04:30:09,1
Intel,nzbi19w,Panther Lake is using a superior process technology. So they are right. But it doesn't matter as customer will choose what's better. But until AMD has something out that uses 2nm then yes they will be behind probably,AMD,2026-01-13 08:12:42,1
Intel,nzgbs70,I hope Intel stays competitive and AMD also brings its best to the table.,AMD,2026-01-14 00:33:01,1
Intel,nyj94g8,They should be worried about DLSS 4.5 though. Fix stuttering on FSR 4 and improve image quality,AMD,2026-01-09 03:53:50,0
Intel,nyj4pf7,I just wish Intel would make a very cut down panther lake offering to be the successor to the N1xx/N3xx line of efficient chips that have found their way into mini PCs.,AMD,2026-01-09 03:28:57,1
Intel,nyoak0m,amd unfazed? I bet they are talking big shit again then will get absolutely demolished (as it happened with vega too),AMD,2026-01-09 21:42:52,1
Intel,nyjappo,"Yeah it's old architechture, that's the point, Intel moved to the lastest node and barely manages to eek out a win. A win is a win nonetheless, but AMD still have plenty to dials to turn up.",AMD,2026-01-09 04:03:08,-6
Intel,nyju3k9,"I don't understand the fuss about iGPUs? Like why do they assume the average Joe would care about an IGPU at all? That's maybe 5% of the market and even then...most of them would get a dGPU anyways.  And apart from the GPU, what's special about the CPU? Combining (Lunar Lake) efficiency with (Arrow Lake) power? Sorry but my Ryzen AI 7 350 does that already. The top of the line x9 388h is about ~10% faster in single core aka the only thing that matters and will probably be in 2500â‚¬+ laptops whereas my 7 350 is in 500â‚¬ laptops.  I tried a 285h laptop besides the AI 7 350 and not only did it run hotter and less efficient, it also felt less snappier.    And the AI 7 350 was designed as a Lunar Lake competitor anyways so it was never worse in efficiency and ahead of Arrow Lakes like the 255h in that regard.  So I don't see why anything should really change...?",AMD,2026-01-09 06:13:30,-12
Intel,nyj661m,"AMD doesn't want to bring RDNA 4 to APUs, so as not to give FSR4 to users other than those with dedicated GPUs.",AMD,2026-01-09 03:37:14,-5
Intel,nyjyeyc,a brand new product on a newer node is better than an older product on an older node?! who knew?,AMD,2026-01-09 06:48:14,-2
Intel,nyj7q2f,"And the price. I used to be an AMD fan, but as soon as competition with Intel was gone, AMD raised their prices and now act as if they believe they are a luxury brand. Hope Intel gets back into the game and if Intel can slash their prices it might end-up being the right choice.  At the same price, Intel is dead on arrival. At a serious discount they will take the place of AMD. No one is buying Strix Halo for handhelds, it is too expensive.",AMD,2026-01-09 03:45:51,89
Intel,nylsap7,"Both are right/wrong.  Intel made their comparisons to Strix point because theyâ€™re in the same power class. Panther lake is much faster than Strix Point at the same power level (according to Intel, AMD doesnâ€™t deny that) at 45W.  AMD says it doesnâ€™t matter because their Strix halo (up to 120W) is faster which is pretty obvious.   Itâ€™s not technically lying, AMD is just referencing an entirely different class of product.",AMD,2026-01-09 14:52:54,5
Intel,nyjgluo,Plenty of folks tested it at CES.  Intel was confident enough to let reporters run benchmarks and it's basically around 4050 level.  You should be able to run most games at 1080p at medium-high settings in an Ultrabook form factor.,AMD,2026-01-09 04:38:25,19
Intel,nyj2b71,"Well lunar lake is a monster and competes directly with the z2e both on performance and efficiency, so no reason to think panther lake will be worse.  Even if it falls short of Intels claims it will still be the leader until next year.",AMD,2026-01-09 03:15:43,24
Intel,nyjcak9,The noise is doing a great job advertising for them. A war between them with fighting words will get them tons of free advertising.,AMD,2026-01-09 04:12:29,1
Intel,nylt6fh,And a much higher power budget.  AMD says they win because their 120W chip is faster than Intels 45W chip.   No surprise to anyone.,AMD,2026-01-09 14:57:05,26
Intel,nyjcmeq,"I generally agree but i suspect that the 388h is using a much larger gpu than people suspect. I think its probably ~165mm2 in size, not 55mm2. i suspect the 55mm2 die varient is for the 4xe version, and the 12xe version is 3x that size.  I also dont understand why strix halo is so expensive. It would be interesting to see bom and packaging costs.",AMD,2026-01-09 04:14:25,7
Intel,nyje36x,"It sounds like RDNA4 just doesn't scale at all. All the rumors point to them going straight from RDNA3.5 to RDNA5 in APUs, just skipping RDNA4 all together.",AMD,2026-01-09 04:23:13,37
Intel,nyjuufi,RDNA 3.5 was the only reason I didnâ€™t invest in a STRIX HALO mini PC. The price is too much for outdated unsupported tech.,AMD,2026-01-09 06:19:22,20
Intel,nyji62l,"Might be the same reason they stuck with Vega for so long in APUs.  At the current available desktop memory (DDR4 at the time) an architecture change wouldn't have made a huge difference.    Once DDR5 came out for laptops, we finally saw RDNA 2+ APUs (Ryzen 6000 APUs).  I'd bet once DDR6 starts appearing on laptops we'll get a similar iGPU architecture leap.",AMD,2026-01-09 04:48:17,9
Intel,nynizjw,"AMD probably just didn't bother making a new APU design when they didn't have new CPU core to go with it. Medusa Halo is rumored for 2027 with Zen 6 and UDNA/RDNA5, so the Point version will likely release then too.",AMD,2026-01-09 19:35:27,1
Intel,nyiu18v,AMD has and always will be their own worst enemy,AMD,2026-01-09 02:31:09,77
Intel,nyiyjxo,"It's not like they aren't developing something this whole time, releases are planned many years in advanced. Intel will have some rope and then will get inevitably leap frogged",AMD,2026-01-09 02:55:05,7
Intel,nyj83ic,It's not like Intel isn't doing the same. Panther Lake and ARC are holdovers of things developed under Gelsinger.,AMD,2026-01-09 03:47:56,8
Intel,nyj28hs,"> AMD is going to f--- around and let Intel catch up, in CPUs and GPUs.  this is what we actually need: competition. AMD kicked intels butt, now intel is kicking back. it's a win for us either way.",AMD,2026-01-09 03:15:19,11
Intel,nyiwa7d,I hope Intel will catch up and encourage AMD to compete. Having cleat leader in CPUs or GPUs is bad for consumers.,AMD,2026-01-09 02:43:07,9
Intel,nyjspa3,"I mean, we know that AMD is innovating. They literally showed Zen 6 at CES. Its just not ready yet for mobile, and Intel caught up. Same thing happened with Alder Lake, where intel released that before Zen 4 was ready.",AMD,2026-01-09 06:02:25,7
Intel,nykmhz9,"So AMD having much faster iGPUs for decade or more did not do much.  But now Intel rolling out something at unknown price/power package will absolutely decimate AMD.  Regardless of what will happen, ""AMD's fault"" indeed. (amazing silicon designers and experts at everything posting for free on reddit have convinced me)",AMD,2026-01-09 10:23:35,5
Intel,nynjyg2,"Laptops haven't really been AMD's focus, and apart from Zen1, AMD's focus has been mostly on data center, with desktop being the natural offshoot.",AMD,2026-01-09 19:39:58,1
Intel,nyjozjz,Yeah they ain't immune to being complacent.  And bad press doesn't make Intel stay bad.,AMD,2026-01-09 05:34:41,0
Intel,nzb2682,"News at 10: ""Companies prioritise profits""",AMD,2026-01-13 05:54:06,1
Intel,nynkzx7,"so either AMD doesn't have the capacity to produce them, or OEMs aren't interested, neither option is a compelling reason for AMD to focus on mobile",AMD,2026-01-09 19:44:43,1
Intel,nyjj3pe,"Weâ€™d wish they were, but theyâ€™re not. Intel was struggling on all fronts due to their fabs. Amd is actually moving super fast in data centre so both epyc and instinct which is where they believe their money will be. They just donâ€™t care to do anything in the consumer market.",AMD,2026-01-09 04:54:22,8
Intel,nynm27a,"Halo is so much faster that the upscaler difference doesn't matter at all. Of course, it is probably also bigger.",AMD,2026-01-09 19:49:33,0
Intel,nyoktyw,"Yes, Strix Halo",AMD,2026-01-09 22:31:24,1
Intel,nyjieva,And having fsr4 supported in mobile at all,AMD,2026-01-09 04:49:53,4
Intel,nzat1bf,FSR 4+ may be great but game support (number of titles + GPUs supported) is embarrassingly low,AMD,2026-01-13 04:48:21,1
Intel,nyjm8fi,Dlss 4.5 not that great in my opinion. It fixes some ghosting but creates more shimmering because it has so much sharpening. I had to dial back to 4.0.,AMD,2026-01-09 05:15:20,0
Intel,nyj9k89,wildcat lake.  only issue it seems to be using 2 P cores and 4 LPE cores instead of E + LPE,AMD,2026-01-09 03:56:23,2
Intel,nyjicwx,70% faster being â€œbarely eke out a winâ€? Go ask why amd is stuck with 18 month old architecture despite intel managing to replace arrow lake after 12?,AMD,2026-01-09 04:49:30,10
Intel,nyjvty1,"Youâ€™ve got it backwards. The majority of laptops use IGPUs. IGPUs being as powerful as integrated graphics allows for cheaper thinner devices that are more power efficient. The entire intel CPU/IGPU performs on par with a 4050 at 60w at only 45w. When you factor in the 10-15w the CPU takes with the 4050 and youâ€™re looking at similar performance at like half the power.   Being power efficient opens up a lot of form factors to be able to game with such as thin and light laptops, tablets, or gaming handhelds",AMD,2026-01-09 06:27:13,11
Intel,nyl2kgg,"> I used to be an AMD fan, but as soon as competition with Intel was gone, AMD raised their prices and now act as if they believe they are a luxury brand.   That's why it's silly to be a ""fan"" or ""supporter"" of one company or the other.  They don't care about you, they care about making money and when they have a dominant position they will exploit it.  > At the same price, Intel is dead on arrival. At a serious discount they will take the place of AMD. No one is buying Strix Halo for handhelds, it is too expensive.  Outside of that one device (Ayaneo maybe?), you're right.  But now AMD is also releasing an 8-core version of Strix Halo with the full 40 GPU CUs, which should be cheaper.  I expect that we'll see that in more handhelds at the high end.  Realistically speaking, it's easy to make the case that on a 7""-9"" screen the 40 CUs is way overkill.  There's still room for a middle ground that Intel could easily fill.",AMD,2026-01-09 12:30:34,34
Intel,nyksz11,"Absolutely consumers win when competition is hot, AMD has a bit too much of a lead ATM so they are cashing in and getting lazy. That said I am glad they are having their day, only because a few years ago they were on the brink of bankruptcy and I really want to see them on a fairly level playing field with Intel... If we end up with 2 juggernauts training blows, having big resesrch budgets, etc we'll get lots of innovation and competitive pricing.",AMD,2026-01-09 11:19:09,7
Intel,nykkoyj,"You had to shovel $1k for a 8 core CPU for about a decade, before AMD came.  So ""it just hiked the price"" is BS.  AMD cannot keep prices low while TSMC, effective monopolist, keeps posting record profits quarter after quarter.",AMD,2026-01-09 10:07:29,40
Intel,nyjf6qu,AMD is basically just waiting for their chance to do the bad things. They are a corporation after all.,AMD,2026-01-09 04:29:52,38
Intel,nyo271t,"Help us Cyrix, you are our only hope...",AMD,2026-01-09 21:04:11,3
Intel,nykfx0k,AMD has always been like this. The OG Athlon FX line from ~22 years ago were $1000 CPUs.,AMD,2026-01-09 09:23:31,-5
Intel,nyjjlvs,Wasn't it equivalent more or less to the 4050m as it was power limited to 30 watts?,AMD,2026-01-09 04:57:40,13
Intel,nykm4lu,"It's the price of the final product that will matter.  And given that Intel has lion's share of the mobile market, I don't see why the would not ask outrageous $$$ for it.  It is ""impressive"" only in the ""for iGPU"" context.   Based on the benches shown, laptops were consuming around 60W.  While AMD""s 370 HX has been shown to be able to game at below 20W, so uh.  Let's bait for wenchmarks in any case.",AMD,2026-01-09 10:20:17,5
Intel,nyl1xad,"Sure, sure.  I'm going to wait for proper benchmarks done under lab conditions and documented by more than ""Intel let me run this game with the FPS counter on.""  I mean, the general impressions for Intel are quite positive and if they're true then I hope it spurs AMD to do more.  I'm just not going to blindly accept ""first impressions"" as a replacement for proper testing.",AMD,2026-01-09 12:26:10,1
Intel,nym6wt0,*With 64gigs of ram at 9600mhz,AMD,2026-01-09 16:00:23,0
Intel,nzialor,"Tbf, it's like dgpu winner is claimed by who has the strongest one, so in that way it's kinda fair.  But how's the availability? Is halo in laptops actually? What's the pricing?  And what's the bang per buck on point and this?",AMD,2026-01-14 08:41:12,1
Intel,nyoht8c,"https://x.com/jaykihn0/status/1812898063502938260/photo/1  ""PTL-H 12Xe pictured."" so according to that the 55mm2 die variant is xe12",AMD,2026-01-09 22:16:37,5
Intel,nynhnmj,"12 Xe cores is 60% of the 20 cores in the B580 and that's 272mmÂ², but of course that also has GDDR memory controllers, and such that aren't needed on a GPU chiplet, but it's likely that the die for the top SKU is quite a bit bigger than 55mmÂ², I'd say between 90 and 130 mmÂ².",AMD,2026-01-09 19:29:20,3
Intel,nykps9b,">I think its probably ~165mm2 in size, not 55mm2.   Which might open an ""dGPU sized iGPUs"" race.  NV could be the main victim here Surely AMD can oversize its iGPUs too.  I actually thought that AMD was forced to do so, by Filthy Green's GPP effectively banning AMD dGPUs. Typing this from G15 AMD Advantage Edition TUF.",AMD,2026-01-09 10:52:22,1
Intel,nyjhzne,What matters is the intel chip regardless of actual die size runs on quad channel LPDDR memory instead of the octa channel of strix halo and is fitting into mid and small size laptops 15-45w.,AMD,2026-01-09 04:47:09,-7
Intel,nykcsfk,Then how is the exynos 2600 using rdna4 fron samsung if it doesn't scale?,AMD,2026-01-09 08:54:32,12
Intel,nyktb13,RDNA5 doesn't exist. The actual name for the next generation architecture is UDNA1.,AMD,2026-01-09 11:21:54,-3
Intel,nzasd0f,"Same here. No new FSR tech and ROCm was just as poor. It works now but Vulkan is often better.. Very disappointing. For AI, NVidia is so far ahead.",AMD,2026-01-13 04:43:51,1
Intel,nym1e1u,If intel can extract more out of LPDDR5x with B390 then I don't see how AMD can't. Just too stingy to give more die area to cache?,AMD,2026-01-09 15:35:23,5
Intel,nyjxf3k,So basically RDNA4 is just another RDNA1,AMD,2026-01-09 06:40:05,1
Intel,nytzgu6,Kepler said in another subreddit that Medusa Premium and Halo is launching in 2028. You're only getting the crappy RDNA3.5 iGPUs for the third time.,AMD,2026-01-10 18:54:53,1
Intel,nyj1177,"Intel: but the enemy of my enemy, is my friend.  Intel ðŸ¤ AMD  we're cooked guys /s",AMD,2026-01-09 03:08:41,22
Intel,nykbk7y,"Yeah the people saying AMD is stagnating are just wrong. AMD is kicking all kinds of ass... They just don't care much for the consumer market currently.    The other issue is that there's no point releasing a new line of products when no one can afford anything because nand flash is so expensive.    Companies CAN afford this because they need to ride the ai wave, but consumers can't because the average PC cost almost doubled.",AMD,2026-01-09 08:43:25,5
Intel,nyjeicz,"Wow! Excellent. Hopefully we'll see the products coming to market soon, and hopefully the 2 P cores won't matter as much since we're seeing a major lithography improvement. Intel is really impressing me lately.",AMD,2026-01-09 04:25:42,2
Intel,nyltrkg,This. Plus even lunar lake outperformed Strix Point in many scenarios already. Intel is at least one generation ahead here,AMD,2026-01-09 14:59:54,2
Intel,nyk0tw7,"It's not the 4050 at 60W. The laptop they compared only allows for 30W to the 4050. Nvidia's specs for the 4050 is 35W minimum, so I don't know how Dell even got to 30W. Below a certain wattage, gpu performance decreases exponentially because a minimum level of power is required to even have the gpu turned on.   Panther Lake is built on Intel 18A, which is supposed to be much better than the 'ancient' TSMC 5nm the 4050 is built on. The 4050's cpu is also Arrow Lake, which is less efficient than Lunar Lake. Again, that skews the agenda.   You can already game on thin and light devices with discrete graphics. Laptops like Asus's G14 is only 3.3lb, but sports a 4060 which is like twice as fast as intel's new igpu. The dgpu turns itself off when on battery, and the integrated graphics takes over. Anything more intensive should be used with a charger plugged in.   In short, paying for a big igpu doesn't make much sense for anyone interested in performance. And gaming handhelds? Does anyone really care about those useless bricks for investment into integrated graphics? It's not like the cost of Panther Lake is going to be cheap when its laptops start at $1300. With that kind of money, you can get 2025 Asus Zephyrus G14 with a 5060 and blow its shit out the water. Or for those on a budget, 5050 laptops have been seen for $600.   Integrated graphics have come so far, pairing Intel's newest 18A Panther Lake with an RTX 4050 could still make a lot of sense.",AMD,2026-01-09 07:08:24,-1
Intel,nyjxwil,"Ehh not the mayority but *all* computers use iGPUs. The thing is, for the average Joe aka 95% of the market, there won't be a difference in the usage between an Intel Iris or RTX 5090 dGPU. And the efficiency would only come into place if they would game on battery (who does that anyways) or create/edit videos (again, virtually no one would do that without a dGPU). And even then, having your laptop drained in 2 hours 15 minutes instead of 2 hours is not ""gamechanging""   So it does not affect the efficiency at all during webbrowsing, watching videos, creating documents etc.  Thats also the reason why Intel has the non X 5,7,9 which will properly be by far the more demanded version as, again, the average Joe does not care the slightest about iGPU.  And apart from the iGPU, PTL is just a tiny step up from the Ultra 200 series...",AMD,2026-01-09 06:44:02,-1
Intel,nyqzsy8,"It's so weird that people treat their computer parts with a cultish following. Most of the people I know don't think about their cards at all and are just happy to play whatever games.   Super weird to be ""team red"" or ""team green"".   I can't imagine describing myself as a ""my computer chip manufacturer fan"". Cringe lmao.",AMD,2026-01-10 07:19:21,2
Intel,o0v4u9k,"How are you determining that AMD has a ""lead""?  In terms of Marketshare, Intel absolutely dominates x86, especially mobility (laptops), and if you walk into a Best Buy and ask 10 random customers ""Would you ever consider buying an AMD laptop?"", five of them would ask ""What's AMD?"", another three would say ""Isn't that a budget brand?"" (Their ""awareness"" of the zeitgeist of PC hardware is stuck in 2005), and maybe, and that's a big maybe, two out of ten would have a favorable opinion of AMD hardware so long as they've been paying attention for the last few years.  In terms of budget and expenditure, in 2025, Intel spent $17 billion in R&D versus AMD's $7.4 billion, outsizing it by a large amout. Total sales for 2025 are projected at approximately $53 billion for Intel and $33 billion for AMD.  I see this all the time and I've seen it for the past 8 years.... the only place that AMD has a ""lead"" is in the mind of PC hardware enthusiasts.... because it's not in the numbers as seen above (sales, expenditure, market share, etc), and it's not in the ""mindshare"" of your average consumer.  Hypothetically speaking, we could say that in a duopoly, as with Intel/AMD in x86, the BEST situation a consumer could hope for would be an even 50%/50% split in marketshare.... this would bring about the fiercest competition and would hopefully lower prices, increase innovation, etc. (and yet another reason why any fanboy who wants their favored company to dominate is LITERALLY cheering against their own interests as a consumer) Even with AMD's seriously impressive turnaround, their capture of marketshare, their ability to compete with two of the largest companies in the world while having considerably fewer resources, they still have a very, very, long way to go before approaching 50% of the x86 market across all segments.  In fact, to get any closer to that idealistic 50/50 split, AMD would have to continue winning and Intel losing for for many more years.  In other words, fears that AMD is ""becoming what Intel used to be"" and ""getting lazy"" are not an accurate reflection of the reality.",AMD,2026-01-21 14:59:24,1
Intel,nykyb9y,"What no AMD deserves much worse. How can a company fuck up so much and still survive. I would rather good competition rather than competing for the sake of competing. Marketing is bad, products are bad and they keep shooting themselves in the foot. Id rather Qualcomm or some other ARM company compete with X86. Its ARM or RISCV time to shine.",AMD,2026-01-09 12:00:44,-16
Intel,nylsh5z,Ever head a look at their profit margins?,AMD,2026-01-09 14:53:45,1
Intel,nykktsq,"Bullshit.  For starters, pricing is not a ""bad thing"".  Bad thing is, pick any piece from blue/filthy green's arsenals:  1) Strongarming OEMs 2) Strongarming Journalists 3) Proprietary standards",AMD,2026-01-09 10:08:42,15
Intel,nylsknw,Waiting? Theyâ€™ve been busy doing that for years now,AMD,2026-01-09 14:54:13,0
Intel,nyosqz5,"I WISH they were still around. I think their IP got sold to Via, who's not doing anything with it.  At this point, we might only get competition in the desktop x86 space if the government forces Intel and AMD to license x86 and x86-64 to some other chip designer (like Qualcomm or Mediatek) or Windows on Arm and Linux on Arm start getting wide application support, including office software and games.",AMD,2026-01-09 23:11:26,2
Intel,nykpg6r,"""Being good"" was never about price.",AMD,2026-01-09 10:49:30,2
Intel,nyjpruz,"They said it rivals a 4050 at 60W. The 4050 maxes out at 100W on paper but it's actually at 80W that it hits its peak performance. a 60W 4050 is about 85% of it's max performance.  So performance wise panther lake should be about on par with a full powered 3050Ti laptop.  That plus more advanced ray-tracing cores, it's running doom dark ages really well, AMD is still stuck at RDNA 3.5 and ray-traced games suck on the 890M.  I wish intel released a 24 Xe Core Variant with a 256-bit bus, double the cores and bandwidth. That would compete with the 5060/5070 laptop GPUs.",AMD,2026-01-09 05:40:22,14
Intel,nyl1feq,"Not synthetic benchmarks, we want to see benchmarks in games.",AMD,2026-01-09 12:22:47,7
Intel,nyld8l0,"You're either intentionally mis-stating this, or truthfully aren't aware, BUT, you can game sub 20w on any igpu. What actually matters is the performance scaling.  Also, just to clarify, while the 890m CAN game between 6-20w its performance is essentially identical to the 780m, z1e, etc. It only gets impressive at power draws 30+ (signed, a very happy 7840u handheld owner)  So, what we need to know is how well the new Panther Lake chips scale",AMD,2026-01-09 13:35:33,1
Intel,nyjhpru,Lunar lake came out after strix point. It was squarely a competitor to the 890m. Amd just officially released the cut down strix point as Z2E later.,AMD,2026-01-09 04:45:20,-4
Intel,nzinuqp,"No, not really. Making up a ""winner"" is stupid and nothing but fanboy behavior.  A faster dGPU is generally better because youâ€™re generally not power limited on a desktop. It doesnâ€™t really matter whether or not you have a 5060 or 5090 or whatever.  In mobile systems itâ€™s a huge difference.  There are entirely different power classes that donâ€™t compete with each other. A thin and light notebook with a 15W CPU cannot have a 100W CPU in it.  Panther lake aims towards unplugged performance which is the 45W power class and the same as Strix Point.  Strix Halo is a much higher power class that requires a laptop to be plugged in permanently for the full performance.  Itâ€™s for mobile desktops that are usually plugged in but can be mobile for some time with heavily degraded performance.  Itâ€™s an entirely different class of product and you wonâ€™t find (many) devices where Strix Halo and Strix Point/Panther Lake compete with each other.",AMD,2026-01-14 10:47:20,1
Intel,nyojqy6,"Yeah, i am gonna contend that either that is wrong, and is the 4xe version, or that intel basically lied on their benchmarks.  If none of those two things are true, Intel's new graphics architecture will absolutely dominate in the next round of discrete graphics GPUs.  with B580 intel needed \~80% more silicon to match nvidia performance. Now they need \~10-20% less die area. meaning their performance per transistor basically doubled gen/gen. which is unheard of. Even maxwell (largest architectural uplift in the history of GPUs in the last 10 years) did not achieve anything close to that. And it was a massive overhaul with huge changes.  So . . . there is something big i am missing . . . or intel is going to dominate in all things graphics going forward.",AMD,2026-01-09 22:26:03,1
Intel,nyjiq2t,"Let's not perpetuate this ""octa channel"" DDR 5 nonsense; it's a quad channel chip, and the Intel one is a dual channel",AMD,2026-01-09 04:51:54,15
Intel,nyk3wyi,"If you're going to be pedantic about it at least be correct please, they both use 16b LPDDR channels so the actual counts are 8 channels for Pantherlake and 16 for Strix Halo.  You're fighting a losing battle either way, the industry has long since settled on 64b as the standard channel width for marketing, independent of the actual number of address/command buses.",AMD,2026-01-09 07:35:06,3
Intel,nyki3az,Where did you find information of it being rdna4?,AMD,2026-01-09 09:43:42,6
Intel,nykhvc9,It's a custom implementation. IIRC it's not even RDNA4 but some Samsung derivative that probably has a ton of changes in silicon design to drive power down.  The short story is that AMD didn't bother to do low power optimizations in the architecture and silicon design. RDNA5 should change that.,AMD,2026-01-09 09:41:40,7
Intel,nynjaao,"Mark Cerny talked about RDNA5, AMD's leaked documents have talked about both UDNA and RDNA5",AMD,2026-01-09 19:36:51,9
Intel,nykyia5,This is nonsense RDNA5 does exist. I used to work there.,AMD,2026-01-09 12:02:08,-3
Intel,nyu9kyt,"Kepler's track record with AMD stuff isn't great, but everything is possible",AMD,2026-01-10 19:43:39,1
Intel,nyj1emc,">we're cooked guys /s  No sarcasm there lol  All tech companies are colluding right now, seeing as american business laws don't matter anymore",AMD,2026-01-09 03:10:44,13
Intel,nyj8sm2,"Yes, a frienemy.",AMD,2026-01-09 03:51:55,0
Intel,nyleo84,"Oh, you didn't say AMD was ""slacking off"" and letting intel ""catch up"". Figures.",AMD,2026-01-09 13:43:25,5
Intel,nylti7r,But thatâ€™s also more because of the increased demand from AI than anything else.,AMD,2026-01-09 14:58:39,1
Intel,nyjxwti,"It's 6C/6T and 2x Xe3, so don't expect a whole lot of performance. This is Intel's Mendocino.",AMD,2026-01-09 06:44:06,1
Intel,nylwgde,The slide specifically says 60w sustained for the 4050. I couldnâ€™t find your claimed 30w anywhere. If Iâ€™m wrong Iâ€™d be interested to see where you got the 30w number from because that would be shady by Intel,AMD,2026-01-09 15:12:36,2
Intel,nyp8y9d,"I posted elsewhere about the Framework Desktop with the Ryzen AI 385 and 32GB of RAM.  That's a pretty sensible config for a small gaming device, though it has 32 CUs instead of the full 40.  Still, that puts it ahead of anything in it's class other than the 395+.",AMD,2026-01-10 00:37:45,7
Intel,o5zp8ek,"the financial reports show that amd has the lead.   if you are looking at marketshare of people who use 15 year old laptops, sure i guess amd did not win that market 15 years ago.",AMD,2026-02-18 03:37:18,1
Intel,nym6sbh,"Yes, have you compared it to that of the competitors?  In general, pricing is not the issue to me.   Dirty play like blackmailing OEMs, proprietary standards and other misuse of the dominant market position is. (on top of being illegal)",AMD,2026-01-09 15:59:50,15
Intel,nytz6q1,"Let me guess, you're too young to recognize he's talking about before your time.",AMD,2026-01-10 18:53:34,8
Intel,nylxgon,"Yeah while AMD can't even announce their product AT A CONSUMER ELECTRONICS SHOW! and instead ONLY TALK about AI and government work......   AMD sucks just as bad as Nvidia just as bad as Intel, it's just a constant moving circle jerk as to whom is the least evil.",AMD,2026-01-09 15:17:16,9
Intel,nyr90di,How about this one: strongarming game developers into NOT including DLSS?,AMD,2026-01-10 08:43:32,0
Intel,nytewb4,"Nobody is interested in x86, otherwise Via would have been bought up.  We are in the age of the cloud and all software is custom made. Hence RISC+",AMD,2026-01-10 17:18:43,2
Intel,nyk5oi5,"Also Intel has XeSS which is very helpful for handhelds since they can't manage higher wattages, although not all games provide XeSS as an option",AMD,2026-01-09 07:50:42,6
Intel,nykdl7b,Is there really a 4050 or are you referring to the 4050m even when you don't add the m?,AMD,2026-01-09 09:01:51,2
Intel,nyobdr7,"they said 60W, but if you look at the laptop they used, it's 30W, probably 60W whole system",AMD,2026-01-09 21:46:41,1
Intel,nysouji,Nvidia paid intel 5B dollars ....Read whatever u can ... But it was to stop intel giving high power gpu to mainstream... .,AMD,2026-01-10 15:11:53,1
Intel,nylhxw8,"Perf + price + (to a lesser extent, but it still matters) power consumption together is what matter.   None of the 3 is decisive on its own.",AMD,2026-01-09 14:00:38,3
Intel,nylu1zs,"I think youâ€™re getting downvoted because it was sold by reviewers as a Z1E competitor as thatâ€™s what was available in regular devices.  Hx370 was only used in niche manufacturers, like GPD only when it launched.   Youâ€™re right though, it was supposed to be a competitor for hx370, but was held back by drivers and other things until mid to late 2025, which corresponded to Z2E (cut down hx370) release.  Fast forward to now, and it competes/beats both",AMD,2026-01-09 15:01:17,1
Intel,nyplsu3,"i mean you can legit put it over the intel provided slides and its pretty much a dead on match. the PCH is smaller in the presentation photos so they can make it look pretty, but the real chip is an exact match to that leak https://cdn.videocardz.com/1/2025/05/INTEL-PANTHER-LAKE-DEMO-1200x675.jpg  ~~this generation is seeing quite a significant leap forward in manufacturing technology (Gate All-Around/RibobnFET & Backside power delivery/PowerVia) these usually do result in big gains and that does make it a bit harder to compare to prior nodes. not to mention~~ **[GPU is TSMC N3E still quite a bit denser than N4 class tho]** its not the exact same uarch as B580, while still a derivative of battlemage, there does seem to be some (rather significant) improvements between xe2 and xe3 https://gamersnexus.net/gpus/intels-new-gpu-xe3-architecture-changes-handheld-gaming-cpus-xess3   but where a lot of the fps gain will be from is N-Frame and Pixel generation intel want to promote those numbers over native performance. AMD cant do with RDNA 3.5. id expect 8060s to be a much more powerful igpu but it is lacking what is essentially lossy compression for realtime graphics. that is a pretty big deal and i do think it will be what causes Strix halo to be a product that just ages poorly, costs too much for what it is really (308mm^2 io/igpu chiplet cant be cheap on 4nm) its really amds pipe cleaner for future packing tech.  people said they same when the zen chiplets were rumored to be the size they are. you save a lot of area not needing memory controller on that chip would be my guess. d2d bonding is very space efficient compared  for some perspective strix halo igpu block + media engine block is about 120mm^2 on N4P(143.7216MTr/mm^2) of the iod, the rest is i/o and the npu.  the 12 Xe3 chip is 55mm^2 on N3E(216MTr/mm^2) (so we could napkin approximate about 80mm^2 if it was on N4P)",AMD,2026-01-10 01:48:30,7
Intel,nyjjq7p,"Technicality is technicality, if the channel width is cut down by half but channel number is doubled then they still doubled the memory channel. People just need to know memory channels are not all equally wide just because thatâ€™s all they know being PCMR enthusiasts.",AMD,2026-01-09 04:58:28,-1
Intel,nykilu3,"https://www.thelec.kr/news/articleView.html?idxno=50232. Seems your out of the loop, you think amd can't scale rdna4 but samsung can?",AMD,2026-01-09 09:48:29,9
Intel,nykicmf,"Rdna4 is objectively faster than rdna3/rdna3.5 at the same clock, power' cu count and bandwidth  something 100% desirable for apus. Stop making excuses for amd and their bad decisions. Everything b your claiming samsung did for rdna4 to scale is something amd could have done aswell and has done as amd has made changes to rdna3(rdna3.5) for apu specifically the same can be done for rdna4.",AMD,2026-01-09 09:46:05,4
Intel,nyui8sr,Sure. weâ€™ll see,AMD,2026-01-10 20:27:04,1
Intel,nyj73k6,Have the business laws mattered since the dawn of post-dialup internet?  I don't think they have. Where's our fucking bell-style breakup? 41 years ago was the last *real* monopoly breakup... and they let it come right back.  EU does half measures and they don't come to the rest of the world. It's a travesty that we don't have nationwide GDPR or force allow sideloading on ios.,AMD,2026-01-09 03:42:31,5
Intel,nyjduwz,An enerend of sorts,AMD,2026-01-09 04:21:49,0
Intel,nylwn40,"Sure. But they're still not stagnating. Since December 2023, when Mi300X and Mi300A were released, they released Mi325x, Mi350x, Mi355x and soon, Mi400x.    The latest gen is running HBM3e and 3nm CDNA4. Those are some immensely advanced products.    On the Epyc side, they've got the 9965, a 192 core 384 thread monster that Intel can't even attempt to compete with.    Intel hasn't advanced in server stuff at the time either. Their top SKU was 18c in Haswell, and that hasn't moved until like Cooper lake? So from 2014 until 2020, they haven't moved an inch in server space either.    AMD has gone from 32 cores first gen in 2017 to 6 times that in 2024.    It's honestly not even comparable. AMD advanced more every generation than Intel did from Haswell to Kaby lake at the very least.",AMD,2026-01-09 15:13:28,1
Intel,nylaft6,"The modern Atom is fine by me, the N100 had more performance than a 6500t so this one should have more than enough compute for many different use cases whilst retaining low load efficiency. This product could potentially obliterate even the newest and best SBCs for home lab use cases, even regarding efficiency.",AMD,2026-01-09 13:19:41,1
Intel,nymqm53,"[Intel Performance Index](https://edc.intel.com/content/www/us/en/products/performance/benchmarks/intel-core-ultra-processors-series-3_1/) Search 4050. [Dell 14 Premium is this laptop, with a TGP of 30W](https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor)  [In PCWorld's test, they got 48 fps for Cyberpunk](https://www.youtube.com/watch?v=NdLYuQQPo5c). My 4060 gets 73 fps using 60W using high settings and 2880x1800 DLSS instead of XeSS. That's a game where Intel gpus performs well above average. A 4060 optimus laptop uses around 3.5W an hour at idle without the screen turned on. With the screen and igpu powering it, it's about 8W. Having discrete graphics in modern systems doesn't really impact battery life anymore.   So yeah, Intel was intentionally being misleading, hoping people wouldn't actually bother to check their figures. Panther Lake's massive igpu still doesn't make sense for anyone who cares about performance. Maybe a little bit for battery life, if it's more efficient to drive high resolution displays, despite its large size being wasteful. Most igpus go into office pcs. In terms of gamers, Steam's hardware survey suggest that desktops and gaming laptops with dgpu are the biggest share.",AMD,2026-01-09 17:28:27,2
Intel,nym76rq,"Yes, I did. AMD could hold prices low, they choose not to.",AMD,2026-01-09 16:01:38,-3
Intel,nzqu8wh,What AMD did a decade ago has no bearing on how they operate today. Corporations have to alter their behavior quarterly in order to maximize their legal obligation to constantly increase shareholder profits.,AMD,2026-01-15 15:45:18,0
Intel,nymy6xm,9850X3D lol gottem,AMD,2026-01-09 18:02:35,8
Intel,nyp6syu,you didn't just try to suggest that CES is for....consumers..... did you.... seriously?,AMD,2026-01-10 00:26:18,0
Intel,nyrhvdt,"Ahaha, lovely lie. And even if true, how would that change a lit of ""bad things"" lol.",AMD,2026-01-10 10:06:50,1
Intel,nytjk6m,"Seems like things are going that way. I guess all we can do is wait and see if the Arm takeover gets so complete that Intel and AMD have to join in, and then suddenly have to compete with Qualcomm and Mediatek.  If that ever happens, hopefully we'll see more competition.",AMD,2026-01-10 17:41:05,1
Intel,nykh31e,linux and optiscaler is the way,AMD,2026-01-09 09:34:24,5
Intel,nylckas,"4050m, although it really wouldn't matter either was as the 4060 and 4060m are functionally identical in regards to performance (+5-7% for desktop) so if there were a full size 4050 we'd expect it to be the same or even less of a difference",AMD,2026-01-09 13:31:44,1
Intel,nyljjoq,"That's all fair, I'm looking at it from a handheld perspective. Performance at power draws that are actually feasible in handhelds has been stagnant since handhelds have really gotten popular. That is, it has if you want more than an hour of battery life",AMD,2026-01-09 14:08:59,0
Intel,nylvmjs,"Actually, the supposed driver issue was only a MSI Claw specific issue and not a general lunar lake issue.  https://www.notebookcheck.net/Intel-Lunar-Lake-iGPU-analysis-Arc-Graphics-140V-is-faster-and-more-efficient-than-Radeon-890M.894167.0.html  Here's a review from September 2024 using a LNL Zenbook S14 with 28w TDP. It had no issues generally outperforming the HX370 in the Zenbook S16. As usual the PCMR-esque dominated crowd on here paid no attention to laptops (which is the real life volume) and only looked at some handheld (which is a niche irl) so they thought that supposed ""lunar lake issue"" was widespread.",AMD,2026-01-09 15:08:44,3
Intel,nyjl1zy,"Each DIMM of DDR5 has 64 bits total of bus width, same as DDR4, 3, 2, and 1. And I do understand what you're talking about (not to mention that Strix Halo can't even take SODIMMs), but nobody else talks like that. When you call it an ""octa-channel"" chip, what people read is that it has as much bandwidth as a Threadripper Pro, because that is how [AMD is marketing those chips themselves](https://www.amd.com/en/products/processors/workstations/ryzen-threadripper.html).",AMD,2026-01-09 05:07:19,9
Intel,nyko7q2,"Yeah, and AMD says Strix Halo has four channels in their customer facing spec as well, because they have 128b and 256b buses respectively. They use the 64b channel convention as it is a customer facing spec, that doesn't meant they actually have that many channels in hardware.   The Pantherlake datasheet isn't public yet, but you can see plainly in the [actual spec sheet](https://edc.intel.com/content/www/us/en/design/products-and-solutions/processors-and-chipsets/core-ultra-200h-and-200u-series-processors-datasheet-volume-1-of-2/memory-controller-mc/) for Arrowlake H that it supports 8 channels of LPDDR5X (additional [spec](https://edc.intel.com/content/www/us/en/design/products-and-solutions/processors-and-chipsets/core-ultra-200h-and-200u-series-processors-datasheet-volume-1-of-2/supported-memory-modules-and-devices/) for channel width). Pantherlake will be the same.   The equivalent AMD doc is not available for Strix Halo but you can see the 16x16b spec quoted by Chips and Cheese [here](https://chipsandcheese.com/p/evaluating-the-infinity-cache-in#:~:text=Strix%20Halo%20has%2016%20memory%20controllers%20and%20CS%20instances%2C%20each%20handling%20a%2016%2Dbit%20LPDDR5X%20channel).  You cannot gang these channels into a dual channel mode, that is not how modern memory works, and there is no allowance in the LPDDR5 spec for 64b channels. The 16b channels have separate command/address buses and burst for a sufficient length (32n) to fill a cache line with each access.  To be clear I think standardising on 64b ""channels"" for marketing specifications is a good thing, it allows quick mental calculation of memory bandwidth without having to get into the nitty gritty. But if you're going to be pedantic and use the actual channel count, it's best to be correct.",AMD,2026-01-09 10:38:46,6
Intel,nykiuny,"Im not original guy you responded to I just wanted to know, becouse I couldnt find it on google. thx",AMD,2026-01-09 09:50:43,10
Intel,nykjg5k,"I'm not making excuses just explaining the rationale, which I don't agree with BTW.       Yes I know AMD are some lazy mofos. RDNA 3.5 till 2029 for iGPU is cheapo strategy as usual.",AMD,2026-01-09 09:56:09,7
Intel,nyli3fq,https://en.wikipedia.org/wiki/Hyperbole,AMD,2026-01-09 14:01:27,4
Intel,nym2zdk,">It's honestly not even comparable. AMD advanced more every generation than Intel did from Haswell to Kaby lake at the very least.  >Intel hasn't advanced in server stuff at the time either. Their top SKU was 18c in Haswell, and that hasn't moved until like Cooper lake? So from 2014 until 2020, they haven't moved an inch in server space either.  Since we're talking about the server side now, Haswell-EP went from 18 cores maximum to 22 core Broadwell-EP to 28 cores on Skylake-SP. Cascade Lake-AP (rare bespoke sku) went up to 56 cores per socket. ""Haven't moved an inch"" is inaccurate.",AMD,2026-01-09 15:42:41,3
Intel,nym7k8z,"In CPUs theyâ€™re losing market share to arm, the Datacenter GPUs are mostly bought by companies who canâ€™t afford NVIDIA",AMD,2026-01-09 16:03:20,0
Intel,nymvxz5,"I think you were looking at the old core ultra series 1 testing not the current CES testing. For their claim they used the following settings:   Intel B390: Processor: Intel Core Ultra X9 388H (Panther Lake) PL1=45W; tested in Intel reference platform; Memory: 32GB LPDDR5 9600; Storage: Samsung PM9A1 512GB; Display Resolution: 2880x1800; OS: Windows 11 26200.6725; Graphics Driver: Intel Arc Graphics Pre-Production driver; NPU Driver: Pre-Production driver; BIOS: Pre-Production BIOS; Power Plan set to Balanced, Power Mode set to ""Best Performance"".  NVIDIA RTX 4050: Processor: Intel Core Ultra 7 255H (Arrow Lake); tested in Dell 14 Premium with Nvidia GeForce RTX 4050; Memory: 32GB LPDDR5 8400; Storage: Samsung 9100 Pro 1 TB; Display Resolution: 2k IPS; OS: Windows 11 26200.7171; Graphics Driver(s): dGPU: 32.0.15.8180 (GeForce 581.80) & iGPU: 32.0.101.8250; NPU Driver: 32.0.100.4404; BIOS: v1.4.0; Power Plan set to Balanced, Power Mode set to ""Best Performance""; Dell Optimized = Ultra Performance. Battery Size: 68Whr",AMD,2026-01-09 17:52:34,2
Intel,nyma4vw,"Why would AMD ""hold prices low""?  Gross margins are below 50% (48, as in 2022), while NV has it at 70%.  We know they are worse in PC/GPU market and better in datacenter.",AMD,2026-01-09 16:14:51,8
Intel,nypf46n,What did you think the acronym CES stands for?,AMD,2026-01-10 01:11:10,8
Intel,nyrmvf6,It would further validate what HisDivineOrder said which is that AMD is just another corporation.  Which they are.,AMD,2026-01-10 10:52:27,0
Intel,nyl953n,"Really wish Optiscaler had a better installer, something akin to Reshade. The whole manual process for each game makes it annoying to use.",AMD,2026-01-09 13:12:04,3
Intel,nyks16k,Intel still takes a heavy penalty on Linux in graphics vs. AMD. Hopefully that improves as well.,AMD,2026-01-09 11:11:24,7
Intel,nyjobcx,Well thereâ€™s more than one type of memory ðŸ¤·â€â™‚ï¸ PCMR crowd just defaults to DDR DIMMs but the world of mobile is mostly LPDDR from phones tablets to handhelds and most small laptops,AMD,2026-01-09 05:29:47,0
Intel,nynuctv,"Nope, I was looking right at the current testing. I do have to make a correction though: Panther Lake's cpu is built on Intel 18A, and the gpu is built on TSMC N3E  Let's summarize. In a head to head battle, Intel claims the 45W Panther Lake Core 388H with its ""massive graphics"" is 10% faster than a 30W 4050 paired with a 30W Arrow Lake 255H. Panther Lake's cpu is built on the most advanced silicon process node 18A, designed to compete against TSMC's N2 (2nm) which is set to release in products in the second half of 2026. Panther Lake's gpu is built on TSMC N3E, a significantly more efficient N3. The 4050 is built on a custom 2020 TSMC 5nm variant, and Arrow Lake is built on TSMC N3+6nm. Arrow Lake is designed for specifically for high power use vs the low Lunar Lake and Panther Lake.   The future of integrated graphics is truly bright. I can see it being exactly where it is now. Vital for battery life in office laptops and actual gaming laptops with discrete graphics. Big igpus? Mostly irrelevant and a waste of money.",AMD,2026-01-09 20:27:44,2
Intel,nyqxy0s,"Yes acronym has ""consumers"" in the name, but it's not directed or intended for consumers, it's intended for the big industry, the maker's manufacturers, the ones creating services, and the subtle parts of the distributors and such, it was and has NEVER been intended for the end users, the broad consumers.  Maybe bloody well look up what CES is and what it's for before asking a silly question.",AMD,2026-01-10 07:03:08,2
Intel,nys7tqn,"No, it would not. There is a difference between a shoplifter and a serial killer, even though both are criminals.  Filthy Green plays in a league of pieces of shit of its own.",AMD,2026-01-10 13:35:08,0
Intel,nynpovt,True but you put command once in your game and you forget about it,AMD,2026-01-09 20:06:13,0
Intel,nysa4r8,"This is so delusional  You're talking about AMD that made Int8 version of FSR4, which is THE hardest part, and then keeps it away from users to sell more RDNA4 cards.",AMD,2026-01-10 13:48:55,-1
Intel,nyov5va,"Missing the point. It's about accessibility and ease of use not how often you need to do it. If a tool to bring similar functionality as Nvidia isn't at a similar level of accessible and easy to use as the manufacturer apps, then it's relegated to enthusiasts only.   Reshade is one of the most popular modding tools for post-processing shaders because it's so easy to install, use, and manage for multiple games on the same system.",AMD,2026-01-09 23:24:12,1
Intel,nytis7z,"AMD had no reasons for such lock-in, it makes sense only for companies dominating the market, to push people to refresh.  I have not seen palatable proof that FSR4 could be ""easily backported"" but isn't.",AMD,2026-01-10 17:37:25,1
Intel,nx5elyy,Already seen card prices here in Canada jump 10-15% since last night. It's insane.   Now is not the time to build.,AMD,2026-01-01 22:05:04,4
Intel,nxlg7un,"Hi, I was wondering if there's any reason to worry if my 7800X3D sometimes spikes for 1-2 seconds to 100Â°C while gaming and then goes back to the usual temp. I have noticed the highest temp recorded by HWiNFO at one point was 104Â°, though I never noticed it on the OSD while in a game and never noticed a performance drop. Is there a problem with the cooling or something that could damage my CPU or is it just a sensor bug/issue?",AMD,2026-01-04 08:46:39,3
Intel,nx583ui,"If you're looking to do a PC build...just don't.  If you NEED to do one, do it right now. It's not getting any cheaper this year.",AMD,2026-01-01 21:31:55,5
Intel,nx5jal3,"Here's a dumb question that would be absolutely ridiculed if I dared to create a whole thread around it.  Is there any truth to my hypothesis that Play Station PC ports are likely to be relatively well-optimized for AMD GPUs, given that the Play Station 5 itself is indeed some variant of RDNA? I recently got a 9070xt and have been overall very impressed, but its achilles heel seems to be ray tracing. This isn't exactly surprising to me, as I researched my GPU options to death before buying one, and the general consensus is that Nvidia is stronger in the ray tracing department. But if I were to boot up, say, Ratchet and Clank Rift Apart, a game that supports ray tracing at 60 fps on the base Play Station 5, could I expect it to perform better than a similarly demanding game that wasn't particularly optimized for AMD hardware?  It's largely hypothetical question, as I already own the GPU, am satisfied with the GPU, and of course did my due diligence before buying the GPU so I would know exactly what to expect. But I just haven't really heard much discussion of what, if any, overlap we get optimization-wise for games that were optimized first and foremost for the AMD-based Play Station 5.",AMD,2026-01-01 22:29:53,2
Intel,nx6hz7y,"Thinking about doing a platform upgrade from a 5800X3D to a 9800X3D, how much of an improvement will I see with my RX 7900 XTX?   Obviously I know that DDR5 is priced high now but I think it's only going to get worse if I wait. I live near a Microcenter as well so I'll be doing one of their combos with the CPU, Mobo, and RAM.",AMD,2026-01-02 01:47:36,2
Intel,nx8k7tp,"Early 2025 I was thinking about upgrading to AM5 but there's no way that's happening, I only got a sapphire nitro+ 9060 XT 16gb on Black Friday.  Current setup is Ryzen 3600 on Gigabyte b450 Elite v1, 16gb ram 3200, 9060 XT. My question is, would an upgrade to 5800X make sense? It costs 165 euros where I am and it's the only upgrade I can make that I see. I play games like Helldivers 2, BF6 nowadays. Also I play on 1080p.   Thank you.",AMD,2026-01-02 11:26:25,2
Intel,nxim4kb,"Hi all  I'm about to give my water-cooled 6950xt to my brother as I picked up a 9070xt.  As I've got to.out the og heatsink back on I'd like to.replace the pads ofc. Does anyone know the sizes needed.  I'd also throw a kryonaut grizzly bear pad on the GPU, would this be a 1mm pad?  I'd like to get this right as he's on a 5700XT so it will be a good upgrade for him.  Many thanks.",AMD,2026-01-03 22:08:10,2
Intel,nzdyg7l,"Is PowerColor a good brand of GPUs?  Iâ€™m planning on upgrading my gpu from my almost 7 year old nvidia rtx 2060 to an PowerColor rx 7800xt Red Devil, and am bit worried if theyâ€™re a reputable brand.   Was holding off the upgrade due to not wanting to chase percentages, and now that I fully embraced Linux (Fedora 43 KDE) I wanted to get something that has better compatibility with the OS as I did encounter some issues due to NVidia drivers.  Edit: forgot to mention that I have a compatible system with 600w power supply",AMD,2026-01-13 17:45:26,2
Intel,o5y57xz,"My 3700x aint it anymore. I mean it works, but it is a significant bottleneck for my 3070 nowadays. The cores are always reaching 80-100%, now even in hoyo gacha games like Starrail for example (When at max everything, and editing the reg to unlock the framecap to 120fps.)  What would be the best upgrade? I am sticking with the am4 platform till my next full system upgrade. Once the performance increase warrents the price tag of a new prebuilt. I'm willing to do single component stuff, but full systems? Nah.  Realistically that point will probs be hit once the rtx 7000 series arrives.",AMD,2026-02-17 22:31:59,2
Intel,nxaeni9,"I could use some suggestions on upgrading a desktop box my son built for me in 2013. It was used for my graphic arts business (Adobe Suite) and has performed admirably for the last 12 years. It's running Windows 10 and most of the patches will not install. It can't be upgraded to Windows 11, and while I realize that every MS upgrade I ever did in the past caused major mayhem, I probably should go ahead and do it before it quits running altogether.  Below is a list of what he ordered and put in it.   What should I order that will swap out and last me another 5-10 years? I just used this for work and internet. No games.  â€¢ MB Gigabyte|GA-970A-UD3P AM3+R   â€¢ VGA Sapphire|100365BF4L R9 270 2GD5   â€¢ PSU Roswell|RX850-S-B 850W RT (has been replaced)   â€¢ CPU AMD|8-Core FX-8350 4.0G 8M R   â€¢ SSD 256G|Samsung MZ-7PD256BW R   â€¢ MEM 8Gx2|Corsair CMZ16GX3M2A1600C9  It also has a DVD RW Drive and I added a 12TB WD Hard drive   I'm sure most of you folks can look at that list and quickly see what I need to change. I'm thinking CPU, Motherboard and RAM? Thanks for your expertise.",AMD,2026-01-02 17:44:40,1
Intel,nxc2q2i,"I typically wouldn't do a pre-built but considering I can get my hands on this right now if I wanted and the prices of things going up, would this be worth grabbing?  $1,649.99 AMD Ryzen 7 9800X3D, AMD Radeon RX 9070XT 16GB, 32GB DDR5 RGB,2TB NVMe SSD  [https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5](https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5)  Thank for the input in advance!",AMD,2026-01-02 22:33:02,1
Intel,nxeipp1,"Quick sanity check: Am I right to say that there are no new production of X570 boards at the moment, and therefore I should just sit tight with my Asus X470 Stix-F board until the RAMmegeddon eases before moving up to AM5/AM6?",AMD,2026-01-03 07:51:43,1
Intel,nxfttws,"Bonjour, j'ai un vieux pc qui a malheureusement commencÃ© Ã  rendre l'Ã¢me fin 2025 et je dois donc me dÃ©pÃªcher d'en racheter un avant que les prix deviennent exorbitants. Je recherche un Pc fixe (si possible prÃ©montÃ© Ã©tant donnÃ© que je suis peu douÃ© lÃ  dessus) pouvant faire tourner les jeux d'aujourd'hui (E33, Dlc Baldur's Gate etc...) et si possible ceux de demain.   J'ai un budget correct (1200 euros max) et je risque pas de faire grand chose Ã  part jouer dessus.    Merci d'avance pour vos avis !",AMD,2026-01-03 14:01:31,1
Intel,nxqoxma,"I just installed my new RX 9070 XT today, replacing my RTX 3060 Ti, and after getting the new drivers set up and the old ones gotten rid of, i'm having an issue of intermittent audio crackling. Is there a know simple fix for this?",AMD,2026-01-05 01:59:57,1
Intel,nxu74dg,what are the best settings for my rx 9070 xt steel legend on adrenalin? should I prioritize lower temps or higher performance? and will the performance between settings be negligible playing in 3440x1440p? I'm currently running the default option under Performance>Tuning,AMD,2026-01-05 16:15:52,1
Intel,nxurns3,"When I'm playing a game, my screen suddenly goes black and I have no way to shut down my PC; I have to restart the power supply. Does anyone have any solutions, please?",AMD,2026-01-05 17:50:56,1
Intel,nxv130z,"Are there plans for chipset refresh for Zen 6 or 7 or there will be only firmware and BIOS  updates  for existing ones? I heard Zen 6 should have better memory controller , with higher 1:1 RAM speed support (perhaps 8000MT/s + ) etc. , but of course still same AM5 socket.",AMD,2026-01-05 18:33:10,1
Intel,nxv5q21,"Hey guys.  Whats the best way to get a smooth 60fps lock on a 120hz display?  I use MSI Afterburner and the adrenaline app, neither felt as smooth as native 60hz.  On nvidia i used the half vsync feature and that worked for me but AMD doesn't have an equivalent option.",AMD,2026-01-05 18:53:49,1
Intel,nxzp3f0,"weird issue as off 2 days ago: RX 7700 XT with 25.12.1 driver on W10 - when powering on the system, the secondary screen (HDMI) is not receiving any signal until the HDMI cable is unplugged and plugged back in. No recent updates installed.",AMD,2026-01-06 11:41:16,1
Intel,ny1f10g,"7800x3d SUSPICIOUSLY LOW TEMPERATURES   I just finished building my computer and tested it in two games, at 2k resolution and the highest settings: The Last of Us Part Two and Battlefield 6. My 7800x3d is showing temperatures below 50 degrees Celsius, even though I'd read on forums that it can get hot. I checked it on the cooling display, HWMonitor and in MSI Afterburner. Is it possible for air cooling to be this efficient, or do I need to configure something in the BIOS to get the processor to run at full performance? Bf6 runs with 180fps and TLOU have around 100fps.  I have rtx 5070 and 32gb ddr5. Cooler: Phantom spirit Evo vision with stock paste.",AMD,2026-01-06 17:15:25,1
Intel,ny4j9i2,"New to AMD and plan to keep the same cpu cooler, I have a NH-D15. I bought this cooler back in 2021-22. Would I need a new mounting bracket to accommodate this change?   I have upgraded to 7 9800X3D, Mobo is a Tuf gaming B650E-E if this information is needed. Any help appreciated!",AMD,2026-01-07 02:15:55,1
Intel,ny70dtx,"Hi there, hope everyone is doing fine and started new year on a good note :)      Recently became the proud owner of a 9070 xt 16GB Ram - Hellhound specifically (https://www.powercolor.com/product-detail214.htm)  I just want to double - check that my AMD adrenaline edition settings are correct - What do I need selected for maximum gfx quality?     Thank you !",AMD,2026-01-07 13:16:10,1
Intel,nyc3nmx,"looking for help understanding core parking on the 9950X3D, does it outright disable the other cores while gaming? or do other applications running use the non X3D cores?",AMD,2026-01-08 03:51:56,1
Intel,nyci0og,"I'm building my first ever AMD PC, and my second ever PC (My old one had a 2080 super, 10th gen i9 and sadly died a few months ago). I did not know that you were supposed to buy certain ram depending on what CPU/motherboard you used. I'm either going to be buying a 9850X3D or a 9800X3D, and the motherboard I have currently is the MSI MAG B850 TOMAHAWK MAX WIFI ATX AM5 Motherboard. My ram is the G.Skill Trident Z5 RGB 32 GB (2 x 16 GB) DDR5-7200 CL34 Memory. Should I return the ram and get the AMD EXPO equivalent? Will I lose performance if I keep it? Will I lose stability if I keep it? Will it even work properly?  Some extra info:   I can afford to return it and buy the equivalent for an extra $100 or so. My GPU is the Sapphire Pulse 9070 XT, I'll be gaming at 1440p, my I have an WD\_BLACK SN8100 2 TB SSD, and a Corsair RM850x (2024) 850 W Fully Modular ATX Power Supply.",AMD,2026-01-08 05:22:26,1
Intel,nygdcim,"Is a reasonable upgrade for my system possible?    Hello there,  i would like to know if there is any reasonable upgrade possible on my AM4 System...   I would like to play Call of Duty Warzone on a 1080p Monitor with 180 fps but ALSO use ~ 500 tabs at the same time.  Currently my System runs the 500 tabs but only gets ~ 120 fps in cod.   Since AM5 is very expensive currently due to RAM prices, i do not see any reasonable chances for a Upgrade and therefore am looking for advice :)    My current System:  AMD Ryzen 9 5900X - 12x3.7GHz  => OVERLOCKED at 4.7GHz with 1.304v (undervolted for that speed -> Temps below 80Â°C)   32GB DDR4 3600MHz Team Group T-Force Vulcan Z - DDR4 (2x16GB) => UPGRADED to 64GB (4x16GB)   AMD Radeon RX 7900 GRE 16GB => slight OC possible BUT Temps tend to go above 80Â°C, at higher OC even above 90Â°C... (possibly i could add more cooling to the Tower?)   * Systemtreff Gaming Mid Tower AirForce GT1   * Systemtreff ITS-Raven - Prozessor - LuftkÃ¼hler  * Gigabyte B550 Gaming X V2 - AM4  * 850W MSI MAG A850GL PCIE5 80+ Gold  => UPGRADE MSI MPG A850G  * 1TB M.2 SSD (NVMe) MSI Spatium M450 PCIe 4.0  * 1TB M.2 SSD (NVMe) MSI Spatium M450 PCIe 4.0   In the future i might want to play COD2026, which could receive a huge engine upgrade... and i also will run ~500 tabs at the same time.",AMD,2026-01-08 19:24:33,1
Intel,nyj771y,"Hi there, hope everyone is doing fine and started new year on a good note :)  Recently became the proud owner of a 9070 xt 16GB Ram - Hellhound specifically ([https://www.powercolor.com/product-detail214.htm](https://www.powercolor.com/product-detail214.htm))  I just want to double - check that my AMD adrenaline edition settings are correct - What do I need selected for maximum gfx quality?  Thank you !",AMD,2026-01-09 03:43:02,1
Intel,nyld64a,"What does the 18th byte do?  On my system it changes on a daily basis. Display port radeon software rx 580  Also Current Link Settings - 2.7 Gbps x 4. Seems I have a bandwith issue, should be more as i have a DP 1.2 standard gpu port cable etc  ""BestViewOption""=hex:00,00,00,00,00,00,00,00,03,00,00,00,01,00,00,00,08,89,ff,ff,00,00,00,00,00,00,00,00  ""BestViewOption""=hex:00,00,00,00,00,00,00,00,03,00,00,00,01,00,00,00,08,80,ff,ff,00,00,00,00,00,00,00,00",AMD,2026-01-09 13:35:10,1
Intel,nylqaba,"AM4 CPU Compatibility question.     I currently have an HP system with a Ryzen 2700.   I'm thinking about picking up an AM4 motherboard for a ""new"" build. To be precise, an ASRock B550M-ITX/AC.  I can get a Ryzen 2600 on the cheap and swap out the 2700 into the new mobo. That way I can hand the HP system to my wife as an upgrade.  But. According to the ASRock lists. These older CPU's are not compatible. Just the 3000 series and up.  My question is, what makes these older CPU's incompatible on the same socket? I see some Chinese boards that support the 1000 to 5000 series Ryzens.  Right now, I can get the ASRock new for a decent price. Given the DDR5 debacle, I still have enough DDR4 sticks laying around that makes sticking to AM4 an easy , affordable choice.",AMD,2026-01-09 14:43:10,1
Intel,nymut0x,"HELP - GPU not detected after Ubuntu boot repair and CSM toggle  SYSTEM SPECS CPU: AMD Ryzen 5 9600X GPU: AMD Radeon RX 9070 XT Mobo: AsRock B650M PG Riptide Main Storage: Crucial T500 M.2 NVMe (Windows 11) Secondary Storage: ADATA SATA SSD (Old Ubuntu install)  THE ISSUE My PC was working fine until I tried booting into an old Ubuntu installation on my secondary ADATA SATA SSD. Now my RX 9070 XT is not detected at all in BIOS or Windows Device Manager, and I only get display output from the motherboard.  WHAT HAPPENED To see the old SATA SSD in the boot menu I had to enable CSM in the BIOS. Booting into that drive resulted in a black screen. I then used a Live USB to run the boot-repair utility with the recommended repair settings. This seems to have installed the GRUB partition onto my Crucial T500 M.2 drive instead of the SATA drive. Now I can boot into both OSs, but the GPU is completely invisible to the system.  CURRENT STATUS Windows Device Manager only shows the Integrated Graphics. When I try to install AMD drivers, the installer fails because it cannot detect the GPU. I was briefly able to get graphics output from my GPU by unplugging the PC, flipping the PSU switch to off, and holding the power button to empty the capacitors. I then booted it up with the HDMI cable plugged into my GPU and I saw the asrock logo but it was stuck there for 2 minutes and I impatiently turned it off. I'm also considering trying this again and letting it run it's course  PLAN AND QUESTIONS I am planning to disconnect the SATA SSD and try to wipe the Ubuntu boot entries from the M.2 drive to see if the GPU reappears. Has anyone experienced a Linux bootloader repair or CSM toggle hiding a GPU from the BIOS? Specifically, could the Crucial T500 and ADATA drive conflict be causing PCIe initialization issues after the CSM change? Should I try clearing the CMOS first? Any help would be greatly appreciated. Thanks!",AMD,2026-01-09 17:47:28,1
Intel,nyp2whf,"I can buy a Ryzen 7 5700 (without X, the one that is 5700g without a built-in graphics chip) for 120 euros (\~$140) or 5800x for 188 euros (\~$218). Is it worth the extra? My GPU has 16GB, so PCIe shouldn't have too much of an impact.",AMD,2026-01-10 00:05:44,1
Intel,nz62itx,"So I have a question guess I want a 2nd opinion, with the new information of AMD potentially bringing back some old AM4 chips.   I recently bought a ryzen 5800x for my little brother to upgrade his 3600 thing is my brother is currently at the military until May so his upgrade isn't super urgent. Should I return it and potentially hold out on the chance that AMD brings back the 5800x3d?!",AMD,2026-01-12 14:39:27,1
Intel,nzq963u,# OpenGL to DX12 Wrapper on Windows? Do any of you guys know a program that wraps OpenGL API to DX11 or 12? Something like Dgvoodoo2.,AMD,2026-01-15 14:01:35,1
Intel,nzrzrx7,I have seen alot of posts about changing the Curve Optimizer to All cores -30 or -20 and it gives you same the performance and lower temps?  is -20 or -30 better?  and how lower does the temp go?,AMD,2026-01-15 18:51:03,1
Intel,nzssz2e,"Hello guys, hope u're doing well today.   I currently have a perfectly working system, Ryzen 7 9800x3d, X870E aorus pro ice, and 2x16gb trident z5 royal neo 8000mhz cl38.   got a pair of corsair vengeance rgb cudimms, 2x24gb 8000mt cl38, as an upgrade.   after I got them I found about the cudimms and the ""incompatibility"" with ryzen which makes them run as a normal udimms in ""bypass"" mode.   searched a lot on the internet about performance or benchmarks, couldn't find any.   since I'm thinking of returning them if they don't work, and I can't if I open them, I'm looking for someone that has tried cudimms on 9800x3d and could share his experience, that would be awesome and I'll be really thankful.",AMD,2026-01-15 21:05:23,1
Intel,o007h0r,"Question about dealing with overheating  Playing a game I'm getting 95ÂºC on my GPU and the performance is tanking. I know it's overheating. However, when I stop playing, the temperatures go down, I try playing again an hour later, now I'm getting 75ÂºC and the performance is just as bad as before. Has it not really cooled down somewhere inside?   It's a 5 year old 6900XT and I'm trying to see if I can reapply the thermal past, but until there I'm trying to see if there's anything else I can do to keep it from getting to the point of overheating, and I don't know what to do to really reset the test except waiting for the next day before I try again.",AMD,2026-01-16 22:23:58,1
Intel,o03mdoz,"Hi all,  Trying to use the curve optimiser for my 7800x3d so going Advanced CPU configuration>AMD Overclocking>Precision Boost Overdrive then setting it to advanced to use the curve optimiser. However every time I go back into the BIOS it's changed from advanced to enabled.  Does this mean it's not saving my settings or is it just a visual error - wondered if anyone else has had this?  The other reason I ask is ran three cpu cinebench tests with the curve optimiser set to -15, -20 and -25 and all returned results within 0.1% of each other and same temps too so doesn't seem to be working as I expected.  I'm pretty new to this so any help is appreciated thanks :-)",AMD,2026-01-17 13:18:40,1
Intel,o073hc7,"I have been looking at the user manuals for motherboards like H13DSG-OM which support eight MI300X Instinct GPUs.  *None* of them explain whether the midplane (like BPN-GPU-GP801) **must** be fully populated with eight GPUs, or if it will boot and operate with fewer GPUs installed (like four, two, or only one).  Does anyone know if any of the OAM motherboards (of any manufacturer, not just Supermicro) for MI300X support such partial installs?",AMD,2026-01-17 23:41:57,1
Intel,o075i10,"I have a 9800x3d , is it worth having a negative curve of -30 and +200 on core clock for games?   I tried cpu intensive games and I didnt notice really any fps difference at all",AMD,2026-01-17 23:52:44,1
Intel,o08w39x,"PLEASE HELP. I just finished a build for my friend and I'm getting frequent no display out on power on. This is only fixed by a restart but I'm PASSING all my POST lights. The only thing I can think of would be a bios update but I'm not sure, these are all brand new parts.",AMD,2026-01-18 05:58:32,1
Intel,o0dmvvr,"AMD Ryzen 9 9950x  ASUS Strix X670E  my current clock speed is over 5500 MHz (showing in NZXT cam, but Task Manager shows slightly lower), default is 4300MHz. Power hovers around 40-60W while browsing.  i have never touched anything regarding overclocking so dont know whats happening, didnt realize how long this has been happening.  I just tried disabling PBO in bios but that didnt change anything.  Any suggestions? Finding conflicting information online",AMD,2026-01-18 23:18:01,1
Intel,o0l1emh,"# Is the 6000 series cards stuck with 25.3.1 forever now for stable VR?    I keep trying the updated software for my 6800XT, and every time it still has the stuttering/ghosting issue that doesn't occur with the 25.3.1. So I keep rolling back (properly with ddu) but now games aren't just whining about the old drivers (ninja gaiden) but Battlefield 6 flat out won't allow me to play (without some registry hack) So with AMD not doing any work on this card's drivers anymore, does that also include this issue that is THIS old being completely ignored?",AMD,2026-01-20 01:12:52,1
Intel,o0n18nb,"Should I disable my iGPU?  I have a 9600X and 9060XT and wondering if there is any benefit to leaving the iGPU active. I noticed that it allocates 512 MiB of system ram as vram, but I wonder if I could miss out on something if I disable it.",AMD,2026-01-20 09:31:19,1
Intel,o0q9hqx,"Hi everyone, the refresh rate of my Lenovo Yoga Slim 7 13ACN5 drops suddenly when I connect it with the Gigabyte M027Q28G. I can't change the refresh rate back then in that case.  I use my laptop basically as a desktop so I only use the display of the Gigabyte monitor. The issue only happens when I connect my laptop with the Gigabyte M027Q28G, when I try to connect my laptop to other monitors the issue doesn't even appear and the refresh rate is stable throughout the entire usage of the monitor.  When my twin brother hooks up his laptop to the monitor he doesn't suffer from the refresh rate drop issue at all so the issue only appears when I hook up my laptop to our Gigabyte M027Q28G.  When I'm using the monitor I connect it with a HDMI cable to my laptop through the Baseus Joystar 7-in-1 USB-C Hub as my laptop has only USB-C ports so no HDMI port or a DisplayPort port at all.  I've tried the following things in order to try solve the mentioned issue:  * Reinstall the GPU drivers of my laptop. * Pull the HDMI cable out of the HDMI port on my USB-C hub and then plug it in again.  Regrettably these things I tried didn't solve the issue so now my question is how I can solve the refresh rate drop issue when my laptop is connected to the Gigabyte M027Q28G.  Thanks for your help in advance lastly! :)",AMD,2026-01-20 20:17:48,1
Intel,o18qds7,My amd icon in system trey has a ! mark next to it i don't have any notifications i restarted and it wont go away,AMD,2026-01-23 14:21:49,1
Intel,o1g8osf,"**Is it worth waiting for the 9950x3d-2?**  My current system is a ryzen 5950x w/ 64Gib RAM on an x370 mainboard (Asus Crosshair VI Hero).   The mainboard is 7 years old (bought back then with a ryzen 1800x) and I really want to replace these components in my main PC.  The sooner I am thinking, the better, since I am using it 99% for work (Software development, virtual machines, local LLMs, databases, etc.) and everything needs to work - always.  Now the question is: Wait out the upcoming 9950x3d2? Or just go for the existing one?  I would pair the processor with a Crosshair X870E Hero.  Is it worth the wait considering my main use for the rig? My feeling says ""no"", but I am curious what others think about this :-)",AMD,2026-01-24 16:39:29,1
Intel,o1l4dza,"Hey everyone!  Iâ€™m thinking about upgrading my GPU and found two used options: ðŸ”¹ PowerColor Red Devil RX 6800 ðŸ”¹ Sapphire Nitro+ SE Edition RX 6800  I currently have a Ryzen 7 5700X and Iâ€™m wondering if these cards are a good pairing with that CPU. I want to avoid any heavy bottleneck, and ideally get good performance for 1440p gaming.  Here are some pictures of the two cards: (attach your photos)  My questions: 	1.	Will the RX 6800 pair well with the Ryzen 7 5700X without significant bottleneck in most modern games? 	2.	Are there any major downsides to buying either of these used cards (e.g., reliability, VRM cooling, Coil whine, etc)? 	3.	Is one version (Red Devil vs Nitro+ SE) generally a better choice for long-term use?  Thank you!",AMD,2026-01-25 08:34:41,1
Intel,o1l4fkp,"Hey everyone!  Iâ€™m thinking about upgrading my GPU and found two used options: ðŸ”¹ PowerColor Red Devil RX 6800 ðŸ”¹ Sapphire Nitro+ SE Edition RX 6800  I currently have a Ryzen 7 5700X and Iâ€™m wondering if these cards are a good pairing with that CPU. I want to avoid any heavy bottleneck, and ideally get good performance for 1440p gaming.  Here are some pictures of the two cards: (attach your photos)  My questions: 	1.	Will the RX 6800 pair well with the Ryzen 7 5700X without significant bottleneck in most modern games? 	2.	Are there any major downsides to buying either of these used cards (e.g., reliability, VRM cooling, Coil whine, etc)? 	3.	Is one version (Red Devil vs Nitro+ SE) generally a better choice for long-term use?  Thank you!",AMD,2026-01-25 08:35:05,1
Intel,o1q6ffb,"hello. I recently built a computer with a 7900 XTX and a 9800 X 3-D. Iâ€™m getting really good frames, especially compared to the laptop I was using, but Iâ€™m really curious to see how far I can push this system. So far, Iâ€™ve been dissuaded by the bigger warning on the AMD overclock section on the app, and Iâ€™m not sure how to do it or what to do. my temps are fine both CPUNGPU are around 50Â°C under load. My power supply is like 200 W more than I need if that matters",AMD,2026-01-26 00:31:18,1
Intel,o2bf2cw,"Hey all! I recently started venturing into Linux gaming and everywhere it says that AMD GPUs are way better than Nvidia's on that OS. That said I currently have an RTX 4060 and I love the performance since I game only on 1080p for now. I want to get around the same amount of performance or better if its possible for the price. Also, I have a Ryzen 5 5600 CPU so it would be nice to have an all AMD system.   Is there a good AMD equivalent I can get? Thanks!",AMD,2026-01-28 23:58:34,1
Intel,o2dekrv,"I need help! My rig was having issues mounting drives, and crashing, and hang ups, so I figured my 13900k was finally dying (as expected from Intel). So I bought a 14900k and that wouldn't boot even with the latest bios. I had an Auros Z790 elite AX motherboard with 96gb Corsair vengeance 5200mhz DDR5 ram, and RTX 4090. So I said screw it, I'm gonna rebuild my PC and switch to AMD.   So I bought a MSI x870e Tomahawk Wifi Mobo, and Ryzen 9950x3d. I assembled everything, and it posts first try. Great. So I install Windows 11 and get to configuring things, removing bloat, etc, and I start having freezes. And my screens blinking off and a message telling me there was a failure and that it needs to put the graphics into safe mode.   On top of that, I was having a lot of random hiccups and lag. I checked with LatencyMon and was having all sorts of DPC latency with my Nvidia drivers. So I uninstall the drivers with DDU and install an older driver (566.36) and cool, things seem more stable. Except they're not. Now I'm getting high latency from other drivers like storage and network. So I'm thinking okay maybe it's the ram. So I run memtest86 overnight only to find my PC shut off at some point. I figured the ram must be faulty. So I took out one stick and tested it with the other and the test completed. I'm thinking okay, ram stability could explain a lot of things, so I've found the issue. So I stay with one stick and up the DRAM voltage to 1.35 to see how it goes.   I'm still getting intermittent latency. Some programs crashing, and on top of that, my PC won't shut down now. When I press shut down, my screens turn off and it seems like it's off, but my fans and light and everything else are still running. So I have to hold the power button to shut it down.   I am suspicious of my PSU because I think it could cause some of these issues, but I've had this PSU only since 2023 and it's a Corsair HX1000i.   The only things I kept from my old build were the PSU, GPU, and ram. And considering both systems were having issues, it makes sense that maybe one of them is acting up. But I don't really know if that explains all the problems I've been having.   I've updated the Bios, I've reinstalled windows, I've tweaked power plans and bios settings, reseated hardware, and I feel like I'm just on a wild goose chase.   I'm hoping maybe someone else here has had a similar experience and can help, or if anyone has any suggestions, I'd appreciate it.",AMD,2026-01-29 07:24:47,1
Intel,o2m6wri,I/O Clock stuck at 1200 please help,AMD,2026-01-30 15:19:05,1
Intel,o2mii1p,I/O Clock stuck at 1200 please help,AMD,2026-01-30 16:11:30,1
Intel,o2phbk0,Already bought a asrock b650 to pair with my 9600x and I was unaware of the burnt cpu issue and I cant really return it. Am I safe to use it with the newest bios?,AMD,2026-01-31 00:40:15,1
Intel,o39emib,"Hello,  I'm thinking of swapping from Nvidia gpu family to AMD to pair with my 7800X3D (love it) but I rely on Nvidia Broadcast a lot to remove voice echo from discord since I use a tabletop Mic and speakers (I hate wearing headsets). Broadcast completely eliminates this issue like a miracle so I'm wanting to hear if anyone else does a similar setup with success on AMD full rigs. Thanks in advance!",AMD,2026-02-03 01:04:28,1
Intel,o3cf6l0,"Iâ€™m currently on anÂ **i5-12600K**Â with anÂ **RX 7900 GRE**, and I have a chance to grab aÂ **Ryzen 9 9900X**Â through aÂ **Micro Center deal**Â (full AM5 + DDR5 switch).  Main use:  * 1440p high-refresh gaming * Gaming + YouTube / browsing on a second monitor * Some multitasking, nothing crazy  I know the 12600K is still solid, so Iâ€™m trying to figure out:  * Is the upgrade actually noticeable in real use? * Worth the platform switch, or better to wait / go a different route?  Would love to hear thoughts, especially from anyone whoâ€™s upgraded from Alder Lake or gone Zen 5.",AMD,2026-02-03 14:15:59,1
Intel,o3jaj5t,"I'm at my wits end. Started having issues with my 7800x3d build a few weeks ago randomly.  It will randomly shut down, or if I shut it down, then it will boot maybe 1/100th of the time. It just sits there with the DRAM and CPU lights lit up like its memory training, I've left it overnight and nothing. I can't even get into the bios, it won't post at all but everything spins up. My mobo is a MSI b650 tomahawk. Seems like the shutdown last night was the last one, hasn't come back up at all.  I have tried:  Resetting CMOS  Flashing new bios  Reseating everything and checking CPU pins (all clear)  Booting with a single stick of ram, booting with the other stick of ram only  booting with no GPU.  Nothing works.",AMD,2026-02-04 14:32:07,1
Intel,o3m4ym3,"I have a TUF Gaming X570 Plus (Wifi), BIOS 5021. It does not have the 2023 Microsoft certificates that will be needed for secure boot starting this June. Does the latest beta (5044) carry these certificates?",AMD,2026-02-04 22:34:08,1
Intel,o3zq2ta,"I am an aspiring PC junkie. I am still on the AM4 motherboard and want to max out my CPU so i can keep the PC for at least 7 to 8 years. I wanted a Ryzen 5950x but recently came across the 5900XT. I am the type to have pride and want the best of the best. Even with money not being an issue, the price difference, it's almost like AMD is forcing you to buy the 5900XT. Is there any other difference than just a 100mhz? Is it still worth the extra 100$?  I do content creation, editing, streaming, and gaming.",AMD,2026-02-06 23:35:01,1
Intel,o4gfcvo,"9850X3D vs 9800X3D:  I'm looking to move to one of these two CPU's soon and am wondering if anyone has experience with power limiting the new 9850X3D or has found any results. I'd be getting a Microcenter bundle so it would actually be easier/cheaper for me to pick the 9850X3D.  I limit my CPU and GPU power to minimize heat output as I'm in a hot climate.  I've had good results in the past with finding a point where there is minimal performance loss but far better power efficiency.  From [TechPowerUp's](https://www.techpowerup.com/review/amd-ryzen-7-9850x3d/22.html) review the single-threaded power draw is a lot higher (36W vs 25W) and overall test average was 97W vs 84W.  What I'd like to know is, with similar power limits in place between these two CPU's, does the 9850X3D at least match the 9800X3D?",AMD,2026-02-09 16:29:14,1
Intel,o4s9mqn,Can the 9950x3d integrated graphics handle a 2k 240 hz monitor without lowering the refresh rate?,AMD,2026-02-11 11:56:48,1
Intel,o4tpv8v,"I'm about to upgrade my PC and have a 9600x and an Asus X870 max gaming wifi mobo to pair with it, I just happened to stumble upon the information about x800 boards killing these kinds of CPU's so now I'm worried.  I intend to update BIOS first thing I do before anything else (new one dropped today) and then probably just launch into BIOS to see what voltages are looking like.  Do I dare turn on things like EXPO do you think or should I just run it without EXPO and any kind of boosting things enabled?",AMD,2026-02-11 16:41:54,1
Intel,o4unv7i,"ISSUE: swapping from the 7600x to the 9800x3d has given me better FPS and frametimes stability, but way higher input delay, input inconsistencies, mushy input feeling  CONTEXT:  I play competitive titles like rocket league and Fortnite where input latency and responsiveness matters more than anything, and I am really sensitive to any inconsistencies or latency, I play these games at 360 FPS and the lowest graphical settings.  I recently swapped from a 7600x to a 9800x3d, playing on the 7600x felt great but I would get FPS drops and struggled to hold 240 let alone 360 FPS in games, so it looked a bit choppy sometimes. I decided it would be a no brainer upgrade to go to the 9800X3D, after installing it, my FPS was way better and the gameplay looked way smoother, however the input 'snappiness' and 'responsiveness' had completely taken a hit, my inputs felt sluggish and mushy, like I was playing through a filter, alot of inconsistencies, like mouse not feeling 1 to 1 accurate all the time and occasionally being slower than faster. My car on rocket league felt ""floaty"" and ""heavy"". Over the time of lots of attempts to fix this issue I had improvements and then randomly it was worse again, very inconsistent day to day even game to game, but generally never felt good.  WHAT I TRIED TO FIX IT: Overall I have tried an immense amount of things, these are the main ones I can think of: - different bios versions - different windows versions, 25h2, 24h2, 23h2 - bios changes like c states and resize bar on Vs off - pbo with a negative all core undervolt and then aida64 for stability check - windows debloating - different powerplan - different in game settings - different mouse keyboard controller settings and hard peripheral resets - brand new chipset drivers, ddu Nvidia drivers, game cache clears - Chris Titus essential tweaks - Even went as far as to download 'tweaking programs' (weirdly these made the biggest positive impact) - a bunch of other recommended and random stuff  MY SPECS: Pc: MSI B650M-P NVMe - Futwiz 1TB NVMe Crucial 32 gb DDR5 6000MHz cl36 ASUS RTX 4060 CPU Ryzen 7 9800x3d Gigabyte Gold PSU 750w Artic liquid freezer 360 AIO Peripherals: Razer viper V3 pro Steel series apex pro mini Alienware aw2523hf Gamesir g7 pro   EXTRA INFO: -my temps even while running extreme CPU stress tests are well under 70 and sit at about 50-60 In game - I have full fibre fttp with a cat6a cable etherneted directly into my pc, my pc is the only device that uses this connection - my bufferbloat scores show unloaded 9ms and download and upload of 0ms with less than 1 ms jitter on all - i never had these issues before, and my friends have not got any issues currently, so it's not game server issue  This is driving me crazy, I've spent about 3 weeks straight testing and trying to change things to help, I've done every recommended thing available on the internet and still my games feel inconsistent and mushy, I will literally pay anyone who can fix this issue for me, buying any extra hardware needed isn't an issue either, please just help.",AMD,2026-02-11 19:20:20,1
Intel,o4uw0hj,What is the best am4 CPU to pair with the 16gb 9060xt New and Used under 200.00  I currently have a ryzen 9 3900x and want better game performance,AMD,2026-02-11 19:59:08,1
Intel,o53rq5e,"9700 X Underperforming   I recently got a new system and my CPU has been underperforming both in games and in benchmarks. Specs are as follows:  9700X  9070 XT Gigabyte OC  Asus B650E Max Motherboard  2 x 16GB Orion 5 DDR5 Ram 6000 MHZ CL38  850 W Gold PSU  1 TB MSI Spatium 450 NVME  Iâ€™m getting 4500 in Cinebench 2026 and like 18500 in Cinebench R23 for multicore scores. Iâ€™m also short up to 20-30 percent on frames in CPU intensive games like The Finals, Overwatch, etc.  Performance is still great, but seems like Iâ€™m missing out on a fair bit of what should be expected from a 9700 X.  Is there anything that could be causing this? Iâ€™m not running PBO, and my CPU temps stays at 60 C or below when benchmarking and gaming at 100 percent utilization. The one time I set PBO to enabled, my CPU was going at 90+ C immediately when benchmarking, so I just ended the test early and turned PBO back off. Iâ€™m using a cheap Gamidas cooler on the CPU. Could that be the cause? I have some extra PTM 7950 and I could get a better cooler, but would the thermal headroom make a difference if the CPU is already staying at 60 C in benchmarks and games?",AMD,2026-02-13 03:43:42,1
Intel,o5b9g0o,"crosspost from /r/AMDHelp due to lack of answers  Hello,  I am looking for a thermal throttling / all core boost frequency chart for the Rzyen 7 Pro 8700GE.  I found several benchmark sites showing such charts for more popular CPUs like the 7950X but did not manage to find it for the CPU in question.  My plan is to rent a affordable dedicated server to host a gaming server, in need of ideally utilizing the full 5.1 GHz on its cores to decide whether it would be a good choice.  Is there anyone using the CPU and can report from experience or point me to a source?  Thanks a lot!",AMD,2026-02-14 08:56:31,1
Intel,o5mclgn,"Hello all - I've tried a few things to fix this issue, but none have worked and I'm overwhelmed by the list of possible solutions. I'll preface this by saying I have an RX 5700 XT if that's any help.  Every time I launch Fortnite, at some point during the game (Could be seconds or minutes into the game, in the menu, between games - anything), both my monitors go completely black and I can hear the game for bit and then my computer totally crashes. This has only happened in Fortnite besides one instance when it crashed in the Epic Games LAUNCHER.  I have to restart my computer each time this happens and 50% of the time my GPU disappears from task manager, and I have to reinstall my drivers to make it show up again. (@ w @\* )  The last time this all happened, Fortnite had uninstalled ITSELF when I restarted my computer and I had to manually delete each file and redownload the launcher alongside the game itself.  Here is a list of things I have tried to resolve this issue:  \- Switch my game mode to performance  \- Switch to DX 11 from DX 12  \- Disabled integrated graphics  \- Made my GPU my primary card in both Windows and Bios  Things I know aren't wrong:  \- My GPU is plugged in correctly  So far, I believe all my specs are normal and everything is installed correctly. Help please?",AMD,2026-02-16 03:06:29,1
Intel,o5p7ces,"Quick question, hoping it is in the right place  What is the cheapest AM4 processor with video I can grab off eBay to test a b450 motherboard I got?   Iâ€™m talking the Celeron of the AMD world.",AMD,2026-02-16 15:47:09,1
Intel,o60597n,"How in the fuck do i download the driver updates, i go to grab it and it just....installs adrenaline addition and seemingly nothing else.  I look in and it says the driver software IS the adrenaline edition?? What????",AMD,2026-02-18 05:25:29,1
Intel,o61yzcy,"For the past two days, I've been having a recurring problem that appears occasionally when I watch a Discord stream. Basically, the stream becomes transparent, and I start seeing my desktop background through it, causing artifacts on the screen.  I don't know if it's a Discord issue, Windows problem, or my drivers, even though everything is up to date. I hope I'm not the only one experiencing this. My PC: R7 7800x3d 7800xt Nitro+ B650 Eagle AX",AMD,2026-02-18 14:03:02,1
Intel,o64dzvd,"I am considering getting my first actual PC to play games with, I've had laptops for the longest and while they work the overheating is an outing to deal with.   I've been looking through PCs on Amazon to buy pre built and was curious about this [one](https://www.amazon.com/ALCPOK-Desktop-3200MHz-Prebuilt-Computer/dp/B0FLX2J7XK?crid=30XU29Q7PS9TT&dib=eyJ2IjoiMSJ9.ceRfeNwV7dG-shxfkeZ2P6u1f2WrbHeuv9jJBylBm-swkDlDRZ21-SPON_29_vdNc-XnP-EpRESBzGiN1iJOkOMrUt1hdsJ3c0sH7NgvTBtYKz89Uzd-7pHxf_ZZM-waeYJyfygwMsKxBcdW7ADRb55dUpIicccIv6ojkuDRCvFIhu0JRb7DJ50ctieFczeCsRbxx54XoKeCIdwYsKbnjWdYOoCK6SuQIMO2h8ygJqU.EcSH4fRX6OYFE7CMZaQG-w3TtSIwg2y9BZnMaFaGUpQ&dib_tag=se&keywords=gaming+pc&qid=1771444313&refinements=p_36%3A-90000&rnid=2421879011&sprefix=gamin%2Caps%2C152&sr=8-12)   I do not know what motherboard it comes with or really what the specs mean as I'm a bit of a neophyte. Is this ok or are there better options preferably for under $1k?   My gf was also curious if her old gpu would be compatible with it, she has a AMD Radeon RX 580.",AMD,2026-02-18 20:44:46,1
Intel,o6dlwsv,"Just a lucky GUY here: I had a AMD Ryzen 7 2700X, for my new editing stuff for my work that I started in Dec 25, and I have 32 gigs of ram and a 3060. Nope, too much lagging with the CPU and just could not handle the jobs that I'm doing. I been talking to my neighbor and he does video editing for his job, and has a Ryzen 9 5900 X sitting in his desk He went to a am5, a while ago. I just gave him 200 cash and plus he gave me all his ram- 4 sticks of 128 gb 3200. I already have a 4070ti from my son, which he gave me that yesterday and coming from a 3060 gpu. Do you guys think this is a good buy? With all the prices I'm seeing out there; it's scary! I think I did pretty good? I do game a little everyday too.   Do I need to update the BIOS?",AMD,2026-02-20 06:08:18,1
Intel,o6nwl39,"Is there any practical difference between getting a BNIB 5900XT or shelling out on eBay for a used 5950X (both roughly the same price)?  Should I avoid 5900XTs because maybe theyâ€™re all 5950Xs that lost the silicon lottery? Or are they the safer option because maybe the used 5950s have a habit of being in bad condition? Or is this a non-issue either way?  Also is there any difference between a 5900XT and 5950X besides the 100 MHz clock difference? I know that thatâ€™s it on paper, but Iâ€™m not sure if in practice people have discovered anything else important.",AMD,2026-02-21 21:00:05,1
Intel,o6pl56p,"Best motherboard for 9800x3d and 9070xt?. I'm thinking of b850 tomahwak because of 8 layer pcb and LAN Guard, I only care about noise if I had to add anything. Thanks",AMD,2026-02-22 03:01:57,1
Intel,o6ttct5,"Please fix the MH Wilds stutters on 9070XT when Mesh quality is set above Medium, only turning ReBar off can fix this, and that works great for other games. Maybe a driver level toggle or game exclusion?",AMD,2026-02-22 19:45:31,1
Intel,o6z8n3o,PC has been recently hard crashing running games it used to have no issues with on high settings a year ago. I have an RX 580 and a Ryzen 5.  I cleaned the case and re thermal pasted the CPU. Task manager temps show CPU below 65Â°c but GPU is hitting 85+ by the time any game finishes booting.  Is it worth re-pasting the heat exchange and replacing fans on the 580 or just get a new one?,AMD,2026-02-23 16:50:24,1
Intel,o6zpmga,What happened here? https://old.reddit.com/r/Amd/comments/1rbs6ku/this_affordable_laptop_has_32_gb_ram_for_cheap/  /u/AMD_Mods,AMD,2026-02-23 18:09:39,1
Intel,nxdpvtk,"https://i.redd.it/n3zqy5xj72bg1.gif  Apparently my driver stopped updating nearly 2 years ago, and I was never concerned about it. Do I need to do some ridiculous workaround here?",AMD,2026-01-03 04:11:23,0
Intel,nxmzstu,"Hi. HWiNFO is a very reliable monitoring tool, so unless there is a known open issue regarding sensors for your CPU SKU, I'd trust these temp readings.  I don't use a Ryzen 7 7800X3D, but the maximum operating temperature (Tjmax) for the 7800X3D is 89Â°C. If you're seeing temperatures over 100Â°C, that's likely a cooling problem that could damage your chip over time. I'd probably check my cooling system and setup if I were you. That said, another 7800X3D user might think differently, so maybe there is nothing to worry about.  Based on my experience, CPU temps over 100 Â°C usually indicate poor thermal management or inadequate cooling.",AMD,2026-01-04 15:31:19,1
Intel,nxtun1i,"check if memory and/or fabric clocks spike at the same time (max values basically), if they do it is a sensor bug.",AMD,2026-01-05 15:16:47,1
Intel,nx5b3qi,You still can build a PC as long as you know where to get the parts you need at a price you can afford despite the crappy RAM and GPU prices.,AMD,2026-01-01 21:47:15,2
Intel,nx8r8gw,"China stolen Samsung DRAM tech, this year we may have influx of chinese cheap RAM from CXMT to save us",AMD,2026-01-02 12:24:29,1
Intel,nx92ypu,"> how much of an improvement will I see with my RX 7900 XTX?  Up to 50% but this is assuming heavily CPU bottlenecked games (stuff like Battlefield 6, Factorio, Stellaris etc). Less than 15% in a standard AAA grade single player title if you play at 1440p. 0% if you play at 4k.   There's no single metric here as it really depends on a game. If you love 4X games like Stellaris I would upgrade. If you prefer Silent Hill or Alan Wake 2 I wouldn't.",AMD,2026-01-02 13:44:35,2
Intel,nx92d32,"It actually might make sense considering you are playing CPU heavy games at a relatively low res. I would also check if 5700X is available since it's pretty much the same thing as 5800X, except often a bit cheaper.   I see techspot actually tested BF6:  https://www.techspot.com/review/3043-battlefield-6-cpu-benchmark/#2025-10-15-image-png  3600 got 62 fps 1% lows and 86 averages whereas 5800X reached 80 fps 1% lows and 108 averages. So theoretically up to 30% better. Still, in both cases it's playable, fps dropping to 62 probably won't kill you.",AMD,2026-01-02 13:40:59,2
Intel,nxq9rvb,It can vary model to model. I watch search for the sku you purchased and if you can't find it try contacting the manufacture to see if they can tell you. EVGA used to be good about providing this info but it really depends.,AMD,2026-01-05 00:39:44,1
Intel,nxav8dz,"CPU, motherboard and RAM yes. AM5 and DDR5 are the newest.  I don't like windows 11 and I will keep using windows 10 for as long as I can. Unless you absolutely need to upgrade I wouldn't bother.",AMD,2026-01-02 19:00:42,0
Intel,nxev4pq,"According to the review on that site it comes with 5200 MT/s RAM which is not ideal. 6000 matches the memory controller's speed so that's what I would recommend. It's not a big issue, only a small performance difference. Other than that it looks like a solid setup.",AMD,2026-01-03 09:40:03,1
Intel,nyidpo3,"The ram by itself is fine, though you'll probably need to manually set it to something like 6000 CL32. You can get the expo sticks, but the only thing that'd really change for you is the preprogrammed profile to allow you to get it with just a single setting.  The 7200 should run out of the box, but the CPU will switch into a different memory mode where it runs its memory controller at half clocks that lowers performance until about DDR5 8000, so that's why you'd go to the 6000 instead",AMD,2026-01-09 01:04:11,2
Intel,nyv1n08,good point,AMD,2026-01-10 22:03:25,1
Intel,o3l30vk,"its definitely going to improve a noticeable amount on all 3 cases, but if you're having some issues with the second monitor a GPU upgrade would probably help more",AMD,2026-02-04 19:31:46,1
Intel,o3l04ky,Could be a PSU issue. Check the connectors from tge PSU to the mobo for any damage or   that they're seated correctly.,AMD,2026-02-04 19:18:12,1
Intel,o6521wk,"Also look at Dell/Lenovo outlets - I would look at Dell small business workstations in the outlet, and similar models at Lenovo",AMD,2026-02-18 22:35:53,1
Intel,nxn91rb,"it's not constantly hitting 100 Â°C so idk what to think of it, hasn't happened today yet and I've been monitoring it so maybe it's nothing",AMD,2026-01-04 16:15:19,1
Intel,nxaknxe,"Thank you for your reply!  Price difference for me between the 5700x and the 5800x is 10 euros so cost isn't something to consider in my case. I'll look up thermals to see if it makes a difference. The benchmark you linked is so helpful for my purposes, kudos!",AMD,2026-01-02 18:12:20,1
Intel,nxrzsmj,"Thanks bud. It's the actual AMD card branded 6950xt, some people mis name it as a founders edition. I'll.try reach out to AMD today.",AMD,2026-01-05 06:52:53,1
Intel,o3l2dsb,Turns out my MOBO was bunk. Swapped it out and all Gucci.,AMD,2026-02-04 19:28:45,1
Intel,nxngkvz,"It's good that it doesn't happen constantly, but even if those readings occur occasionally or intermittently, it's generally not a good sign.  However, if it hasn't happened again today, and you're under similar or identical workloads to when you had those readings, you probably have nothing to worry about. It could just be a few inaccurate readings.  Continue to monitor your CPU temperatures and, if you notice occasional readings over TJmax again, it's worth checking your current thermal management (thermal paste, contacts, etc.) and cooling setup (fans, AIOs, and/or liquid cooling). Prevention is better than cure.",AMD,2026-01-04 16:50:14,1
Intel,nvc9b2o,"The Radeon RX 9060 XT offers the highest raw frame rates at 1080p, outperforming the competition by roughly 4-5% on average.  The RTX 5060 provides nearly identical performance but adds the advantage of DLSS 4 for superior upscaling and image quality.  While the Intel Arc B580 is the slowest card, its 12 GB of VRAM allows it to handle Ultra settings that cause the 8 GB cards to stutter.  Ultimately, the video recommends the 16 GB version of the RX 9060 XT as the best long-term choice for modern gaming.",AMD,2025-12-22 08:32:19,147
Intel,nvcj3xg,Had to sell the 6600 XT and went for the 9060 XT 16GB to play at 1440p. Iâ€™m loving it,AMD,2025-12-22 10:10:46,38
Intel,nvim4sd,I got my 8GB 5060Ti open box excellent BestBuy for $309. It was brand new.,AMD,2025-12-23 09:07:15,5
Intel,nvgivw5,"Bought a 9060XT 8GB for 247e (renewed on Amazon, Black Friday stuff) and sold the temporary 4060 non-TI 8GB for 220e on marketplace. Good deal...",AMD,2025-12-23 00:02:30,3
Intel,nvotif9,Personally out of the 3 I'd pick the 5060. Transformer model is but better than FSR4 at 1080p,AMD,2025-12-24 08:41:46,4
Intel,nvahjmg,Only compares 8GB cards from teams red and green since itâ€™s only considering <$300.,AMD,2025-12-22 00:43:27,9
Intel,nw2aruf,ðŸ˜®ðŸ«³ðŸ¿,AMD,2025-12-26 18:41:22,1
Intel,nwnskte,"I found an openbox 9060 XT 16GB at Microcenter for $305 and jumped on it. Very impressed so far, especially with undervolting.       I have the Powercolor Reaper model and it is legitimately impressive that they were able to make it that small.",AMD,2025-12-30 02:33:44,1
Intel,nvbruur,"I feel like the HUB guys are going too hard with their VRAM crusade. Why recommend a GPU that's slower now just because it might be faster in the future?   A slight downgrade in render resolution or texture quality is hardly even noticeable, and with looming shortages I feel like most studios are going to start optimizing for lower VRAM further reducing the long term disadvantage of 8GB GPUs.",AMD,2025-12-22 05:48:05,-14
Intel,nvcdgdm,"The real answer, buy a used 2080ti. Usable VRAM, DLSS4, it still is 250W so it can run on most PSUs.  It is the most balanced option if you can't afford a 9060XT 16GB.",AMD,2025-12-22 09:14:12,-15
Intel,nve7fwf,"all of them are power hungry junk, where are good cards?",AMD,2025-12-22 16:44:49,-7
Intel,nvm6i3z,real hero here,AMD,2025-12-23 21:49:44,4
Intel,nwe5oim,"Intel is on the right path, but they need to start using 384-bit memory interfaces on 12GB cards instead of the 192-bit memory interface they used on this card.",AMD,2025-12-28 17:23:33,1
Intel,o0nywwz,Does the 9060xt with fsr have higher performance than 5060 oc edition No Ti with dlss? And by how much?,AMD,2026-01-20 13:46:49,1
Intel,nvm9ob0,"The 5060 will crush, without less than a 40 percent difference, from dlls alone. Then add frame gen. WOW I can't believe you can get away with this.",AMD,2025-12-23 22:06:17,-11
Intel,nvftrl1,5060 then cuz way better in AI  5% performance cut to gain 2x-3x AI speed,AMD,2025-12-22 21:41:30,-30
Intel,nvfjawi,What processor are you using with 9060 xt?  Is it the same as you were using with 6600 xt?,AMD,2025-12-22 20:46:22,10
Intel,nvjicw9,"Can you tell me how well it runs games at 1440p? Have you played some of the demanding ones like Black Myth Wukong, Stalker 2, etc? Do you play at medium? high? I assume FSR is always on.   And also what's your target FPS? Would really appreciate the feedback, because I have the same card and I'm thinking on switching to 1440p but I don't know what monitor would be good refreshrate-wise",AMD,2025-12-23 13:36:03,1
Intel,o18s8r6,"Honestly? Pretty decent deal for a 8GB model, considering how the 16GB cost around 450$ these days, almost double the price.",AMD,2026-01-23 14:31:18,1
Intel,nvb3bfo,"Well yeah, the cheapest RX 9060 XT 16GB is [$380](https://pcpartpicker.com/products/video-card/#c=596&sort=price&page=1&P=11811160064,51539607552) and the cheapest RTX 5060 TI 16GB is [$430](https://pcpartpicker.com/products/video-card/#sort=price&P=11811160064,51539607552&c=593). When you're comparing $300 GPUs, you're not going to bring up a GPU that's nearly another hundred dollar.",AMD,2025-12-22 02:55:15,52
Intel,nvcb05n,"That would be a completely new phenomenon if you look at the past. Sure, some (probably indie) studio will optimize their games, but they would have done so already because they care about their customers.  Nothing will change with the current devs or tech, it's just a temporary issue that memory is that expensive. The prices will be lower in 2027, or we'll get used to it and buy more expensive stuff.",AMD,2025-12-22 08:49:20,10
Intel,nvda1uj,"6 ish year old product that is out of warranty from some rando, is not exactly comparable here and definitely not a ""real answer""",AMD,2025-12-22 13:46:25,21
Intel,nvd5m8s,"Dunno why you're being downvoted, the 2080 Ti is still very good value for the price and often has good OC headroom. Beats 5060 in most cases and you're right about 11GB being decent",AMD,2025-12-22 13:18:38,-1
Intel,nvckhpi,The real answer is to stop being cheap and spend money on your hobbies.,AMD,2025-12-22 10:24:05,-26
Intel,nvgc2fa,you tell us,AMD,2025-12-22 23:21:59,4
Intel,nvhwm4g,Why do power requirements matter?   Electricity costs pennies,AMD,2025-12-23 05:17:09,-2
Intel,nwno1my,So many think memory bandwidth matters more than it does. The 5060 ti has less bandwidth than the B580. Architecture matters a lot.  More bandwidth would do next to nothing for it.,AMD,2025-12-30 02:09:09,1
Intel,nvr8r0s,AMD cards have upscaling and framegen as well...,AMD,2025-12-24 18:35:16,4
Intel,nvgtg9q,"Can you elaborate what do you mean by ""AI speed""?",AMD,2025-12-23 01:04:59,29
Intel,nvlqflt,No. Dlss and frame gen is much less impactful in terms of performance boost at the low end and the latency is more noticeable. Itâ€™s also half the vram.,AMD,2025-12-23 20:25:01,9
Intel,nvm9h9y,WOW 25 so far for the TRUTH. HUB and fooling now.,AMD,2025-12-23 22:05:16,-5
Intel,nvfl8ph,"Yes, same processor, 5600x",AMD,2025-12-22 20:56:36,7
Intel,o18rwrw,"I only play Space Engineers, Cyberpunk 2077, Helldivers 2 and pre-2020 videogames, new games are meh for me.  All of them at high with ultra textures and a few middle settings at things i never notice mid-game at 1440p 27"" IPS monitor, 60fps (don't care for more, maybe one day i'll try 75fps).   Runs very cool and quiet, never hitting above 65Â°C on closed room 20Â°C ambient temperature, my case sounds the same as a 20"" metal fan at speed 1, which is the same white noise that helps me sleep at night, an upgrade compared to my old RX 6600 during the summer lol.  Not sure if this helped or way too late, just wanted to comment anyways.",AMD,2026-01-23 14:29:36,1
Intel,nvjk7kj,"yeah seriously, here b580 is noticably cheaper for example.",AMD,2025-12-23 13:47:03,4
Intel,nvcwed4,"Yeah, I just wanted to point it out because thereâ€™s people like me to whom prices in dollars means nothing (or who donâ€™t read the title) and then waste time watching an irrelevant video (though I skipped to the conclusion so not that much time).ðŸ™‚",AMD,2025-12-22 12:11:20,-8
Intel,nvdtbxt,"Point being? If a cap blows because it's old any repair shop can fix it, If it's a fan dying you can fix that yourself.  On the other side there's not much the warranty can do for running out of VRAM.",AMD,2025-12-22 15:34:11,-7
Intel,nvcpxpj,"In this economy? It doesn't make any sense to not keep perfectly usable hardware that does the job just fine out of a landfill.   A 2080ti or a 3070 or AMD equal is more than enough performance for most people. Easily, and is way better bang for your buck.",AMD,2025-12-22 11:15:29,9
Intel,nvgxp18,"In a time of global economic uncertainty, it's a horrendous time to overspend on hobbies.",AMD,2025-12-23 01:31:13,3
Intel,nvhzp95,"heat, noise, size, messy cables",AMD,2025-12-23 05:41:18,1
Intel,nvuuu5m,"Correct, but they do not have commercial dlss support. How many games do you not have the ability and ww do?   Thanks for the dowmvotes nvidia. Amd brainwashed.   Just to let you know: you have all been played. Look closer.",AMD,2025-12-25 11:21:47,-2
Intel,nw6d88g,"They're seemingly referring to the speed of running LLMs locally using that GPU, unless I'm also out of the loop. A good sub to look into that stuff would be /r/LocalLLM   I wouldn't recommend doing that with a 5060 but the 16gb version must be the best choice in that price range and would handle the very small models easily and the small ones with a little slowness.",AMD,2025-12-27 11:35:46,1
Intel,nvcxw7p,"News flash: We're always in ""this economy"". I know someone who works at a fucking McDonalds, has a kid, and spends more money on his hobbies than you do.",AMD,2025-12-22 12:23:17,-10
Intel,nvlzofj,nothing global about it,AMD,2025-12-23 21:14:23,-2
Intel,nvipuzv,"So you prefer low power for lower heat and smaller size.   I'm not space conscious, so those things don't matter.   What's with messy cables? The PC sits under the table, so it also doesn't matter how ugly it is.",AMD,2025-12-23 09:44:14,2
Intel,nvw2mmm,"Well actually people have been modding games to put FSR where neither AMD or NVIDIA added official support.   Pretty much every game had amd nvidia and even intel upscaling these days.   In fact, when i still had my 3080ti, i was able to use AMDâ€™s framegen in many games (cyberpunk, dying light, talos principle) because NVIDIA didnt provide any option for 3000 series.   I still bought nvidia because amd doesnt offer any cards at 5080 level, so no brainwashing here. Youâ€™re completely uneducated blinded by consumerism",AMD,2025-12-25 16:47:51,3
Intel,nvimsbd,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-23 09:13:38,1
Intel,nveay56,They sound irresponsible.,AMD,2025-12-22 17:02:16,8
Intel,nvfpbcp,"If you spend more on your hobbies than the things you actually need, you might be financially irresponsible",AMD,2025-12-22 21:18:17,7
Intel,nvj2hzu,"the unhinged extra power cables attached to the card itself, it makes everything harder to handle  and heat isn't just about the size of the case, it's noise and comfort of the room  and no, AC doesn't solve that as it's another source of noise that is even worse than the PC itself, only to be used when the weather is too bad to live otherwise",AMD,2025-12-23 11:42:04,0
Intel,nvwscdp,"Oh yeah? And who do you think had went through hundreds of accounts taking about that mod?   Came out DEC 22 2023 I remember the day I went from 22fps in Alan wake 2 to 50 (3070). I posted in this site non stop ban after ban just to try and get you this information. Go on the forums you will pick me out if you look back.   I, in all seriousness, would not be surprised if you know about that mod from ME.   Therefore, I am not blinded. I simply understand that an entire company propped up by manipulation on social media (and GPU) should not exist, and a real competitor would be in their place. There I have just demonstrated that not only am I not blinded, it might be you. Good on you for knowing about that mod (serious).   I will tell you another secret, maybe not meant for you. If you don't mind using dlss and mfg? 5060ti 16gb all day for 1440p or lower. Not only is it hundreds of fps, it has valuable vram on it that will see the card rise in price since it's discontinued.   Hows that for an uneducated prediction?",AMD,2025-12-25 19:18:39,-2
Intel,nvjm540,Gotcha.  What's your preferred card?,AMD,2025-12-23 13:58:18,1
Intel,nvkhap4,from this generation that'd be RTX Pro 4000 Blackwell generation SFF with replaced cooler  personally I own a passive A2000 SFF that replaced my modded 1650 (KalmX was released too late),AMD,2025-12-23 16:39:40,2
Intel,nvkshx0,"Hopefully I am wrong but there is no aftermarket cooler for the RTX Pro 4000 SFF, right ?  https://n3rdware.com/gpu-coolers",AMD,2025-12-23 17:35:10,1
Intel,nvkw08y,"unfortunately no, nothing ready to use that I know of  if you have access to measuring equipment machining a shim isn't even that expensive, haven't seen any publicly available projects for it yet",AMD,2025-12-23 17:52:24,1
Intel,nvkx5mu,"Hmm that sounds tricky.  Iâ€™m thinking about getting PCI express extensor and a GPU holder to be able to use it with my MS-A2, keeping the GPU externally til the n3rdware cooler is available.",AMD,2025-12-23 17:57:58,1
Intel,ntamglk,"From r/radeon   * Ray Caching: Only available in Warhammer40K today, more games next year. * Ray Reconstruction: Only available in Black Ops 7 today with more games next year. * AI Frame Gen: Available in Black Ops 7 today with 40 games by end of 2025.",AMD,2025-12-10 14:35:10,103
Intel,ntak2ov,It's almost 2026 and AMD keeps reinstalling the AMD Install Manager that I do not want and have to keep manually uninstalling. Stop this AMD.,AMD,2025-12-10 14:21:39,303
Intel,ntam2kl,What is fsr redstone? and which games use it?,AMD,2025-12-10 14:32:55,83
Intel,ntbp2n9,I just tested the release on four machines (76X&78XT/78X3D&79XTX/97X&9070XT/75F&76XT). Every system still suffers from crashing drivers when hardware-accelerated apps are used (Chrome/Discord/etc.).  Please fix. :),AMD,2025-12-10 17:49:10,25
Intel,ntak8pe,"I got a notification for the update in AMD Adrenalin Edition, but it does not appear in the actual install manager lol",AMD,2025-12-10 14:22:35,50
Intel,ntanibq,so can I open adrenalin on this one with a rdna 2 igpu and rdna3 gpu or is it still broken like the last version,AMD,2025-12-10 14:41:05,18
Intel,ntalx8c,<--- Int8 rdna2 enjoyer,AMD,2025-12-10 14:32:04,85
Intel,ntaottt,did they fix enhanced sync and noise suppression yet,AMD,2025-12-10 14:48:21,37
Intel,ntayll4,Did this driver fix purple visual glitches with the RX 7700 XT? It's a known bug that appeared after the driver 25.4.1,AMD,2025-12-10 15:39:11,13
Intel,ntav0ai,"The ignorance by amd of Rx 7000 users is astounding tbh, but this is 2025 AMD not prior AMD where they would try to appease a larger user base.  It's going to make me rethink my loyalty for future gfx purchases",AMD,2025-12-10 15:20:57,47
Intel,ntam4ms,So we cant test path tracing performance yet on Cyberpunk? Lol,AMD,2025-12-10 14:33:14,31
Intel,ntbddly,"This is a very underwhelming update for RDNA 4 users I get that this technology needs to mature, but they should already be at a point where the implementation is across more wide array of games. My fallen RDNA 2 and RDNA 3 brothers will be remembered. The only reason AMD gpus are still relevant rn is price, nvidia tax is crazy. GG",AMD,2025-12-10 16:51:29,10
Intel,ntbtbc6,"Thanks for nothing again, AMD.  Signed, 7900 XTX user.",AMD,2025-12-10 18:09:31,28
Intel,ntanq1w,So is there any point to installing this if I'm on RDNA2 and don't have any of the issues that they fixed?,AMD,2025-12-10 14:42:16,20
Intel,ntboygm,This is the worst driver amd made 9060xt for me. 2 games instantly crashes. Indiana jones and silent hill 2. With this driver if you enable ray tracing game hangs and give error.i already report bugs in 25.12.1 and same with 25.11.1 and amd does nothing. every ray tracing titles works ok with 25.10.1 driver and this is bad. amd does not listen users anymore. anyone has any crashes happen like me?thanks...,AMD,2025-12-10 17:48:37,8
Intel,ntcc5fr,AMD Software still crashes randomly,AMD,2025-12-10 19:40:52,9
Intel,ntan1w5,Nothing on Oblivion Remastered crashing? Intermittent application crash or driver timeout on 9000 series when playing Battlefield 6?,AMD,2025-12-10 14:38:30,15
Intel,nti2mdh,#AmdNeverAgain Give Fsr4 on rdna3,AMD,2025-12-11 17:48:52,8
Intel,ntamhuj,Pretty dissapointing ngl,AMD,2025-12-10 14:35:22,25
Intel,ntba2eq,New update new problems,AMD,2025-12-10 16:35:17,5
Intel,ntayron,"The adrenalin app just auto updated my 9070xt mid game, now my screen is black with no signal output to my monitor but my music is still playing lol. I waited for 10mins then I had to hard restart my computer for it to say the update failed",AMD,2025-12-10 15:40:02,11
Intel,ntb58wr,Should I get the RTX 5070 ti or 5080 at msrp? I am currently selling my XTX after radio silent news about FSR 4 int 8 on it.,AMD,2025-12-10 16:11:45,22
Intel,ntak0ko,Everything is RDNA 4 exclusive? awesome /s  RIP finewine.,AMD,2025-12-10 14:21:19,62
Intel,ntb9myj,Please add the broken noise suppression to â€œKnown Issuesâ€.,AMD,2025-12-10 16:33:11,4
Intel,ntbbh4c,"If  this driver update keeps crashing my gpu im not leaving 25.9.2 for a while, im also starting to think about selling my gpu and get nvidea, and really black ops 7 why not a real game like cyberpunk i dont want to waste 70 euro for fifa with guns",AMD,2025-12-10 16:42:08,5
Intel,ntbih3y,"Can confirm on my 9060XT that Silent Hill 2 is still crashing and Avatar Frontiers of Pandora currently has a bug when FSR4 is enabled where the entire screen starts flashing like a strobe light, shadowy areas seem to trigger it. This is with both games fully patched & up to date.",AMD,2025-12-10 17:16:44,5
Intel,ntbq2n7,"Let me see - all the new ""Features"" will be available for Cyberpunk 2077 in at least 1 year time and ONLY with RDNA4 ??",AMD,2025-12-10 17:53:59,5
Intel,ntcf8iq,AMD NoiseSuppresion still broken. Since September!,AMD,2025-12-10 19:56:49,4
Intel,ntedkus,"Are pink artifacts fixed on RX 7700 xt, anyone ? It was bugged in 25.11.1 driver last month.",AMD,2025-12-11 02:20:15,6
Intel,ntb4cu0,Whereâ€˜s support for 7000 series? Wtf is this dead meat,AMD,2025-12-10 16:07:26,13
Intel,ntaofpr,Iâ€™m on a 6000 card is there literally no reason for me to download this,AMD,2025-12-10 14:46:12,21
Intel,ntasl1x,"all i want is to be able to capture clips in my games but for whatever reason amd either doesnt understand im in the game, recognizes the game wrong (battlefield 6 shows as elder scrolls online which i dont even have).",AMD,2025-12-10 15:08:16,4
Intel,ntdy3yt,It's december and still no FSR4 for vulkan.,AMD,2025-12-11 00:46:14,4
Intel,ntf1v9l,25.11.1 was dog water driver timeout city for me I'm just gonna assume this new one will also be the same.,AMD,2025-12-11 04:59:50,4
Intel,ntaql28,Is this worth updating to from 25.11.1  Is it more stable?,AMD,2025-12-10 14:57:42,7
Intel,ntc0jb7,"I had to downgrade to 25.9.1 to have some level of stability, can somebody confirm that the new driver is safe to upgrade to without it messing stuff up?",AMD,2025-12-10 18:44:11,8
Intel,ntawnis,Still no fsr 4 support for rdna3 ðŸ™„,AMD,2025-12-10 15:29:24,8
Intel,ntb91tu,"Guys calm down. RDNA3 being moved to maintenance mode is part of their new strategy, no longer ""Fine Wine"", the new approach is Stale Ale. That way their products remain DOA after launch and people won't keep them very long.",AMD,2025-12-10 16:30:16,11
Intel,ntatk4y,idk why I find it so funny that a specific Roblox game got called out in the patch notes,AMD,2025-12-10 15:13:24,3
Intel,ntavzqu,Did they fixed the amd noise supression not turning on?,AMD,2025-12-10 15:26:00,3
Intel,ntbglg2,"Anyone know why Cronos: The New Dawn has been showing [""FSR 4""](https://i.ibb.co/nqW2VMng/Cronos-The-New-Dawn-2025-12-04-02-28.png) for me on a 7900 XT for a few weeks? At first it was 3.1.  I know it can be modded in but this is on a new Windows 11 install and I haven't done any modding.",AMD,2025-12-10 17:07:23,3
Intel,ntbm1c9,"Looks like new chipset drivers, too.",AMD,2025-12-10 17:34:16,3
Intel,ntc41oy,"I thought the application freeze fix might have stopped monster hunter wilds from crashing on me but nope still does it (DXGI_ERROR_DEVICE_REMOVED,)",AMD,2025-12-10 19:00:46,3
Intel,ntcb4ur,/u/amd_vik are you aware of assetq corsa evo vr not working on AMD cards since 25.9.1 ? It displays the left and right eyes out of alignment and therefore fails to show a cohesive single image.,AMD,2025-12-10 19:35:42,3
Intel,ntcgpo5,so no fsr4 support on Vulcan still? this is getting ridiculous,AMD,2025-12-10 20:04:11,3
Intel,ntdhmve,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.Â   Thank fucking god.,AMD,2025-12-10 23:10:51,3
Intel,ntdxxbg,Still experiencing 100% gpu usage almost constantly as soon as you boot up BF6 on newer drivers after 25.10.1 and higher temps in general  I'm locking my FPS to 144 but the older drivers is showing better overall temps and less gpu usage for me ðŸ¤”  [https://imgur.com/a/ctbMCx7](https://imgur.com/a/ctbMCx7),AMD,2025-12-11 00:45:09,3
Intel,ntf7tyk,BF6 crashing after a few minutes in game with that driver on a 6800xt,AMD,2025-12-11 05:46:22,3
Intel,nth0425,"ever since 25.9.2 still same bug is present even now and now it causes even more problems because ML based FSR and FG fails when it happens: Adrenalin app just shuts down randomly even when idle, no errors, no driver timeout, no dx12 trimeout, just adrenalin itself gets shut down in random times. why wont you guys do something about it finally? Seriously its been so long now... im on 9070XT Steel Legend Dark Edition from ASRock, 80% of your users or more report the same issue FIX IT for the love of GOD. I tried everything hoping its on my side but windows reinstall, DDU and AMD cleanup app and fresh driver install nothing helped its still here",AMD,2025-12-11 14:36:29,3
Intel,ntsip7g,"Both 25.12.1 and 25.11.1 drivers have the same bug on RX 9060 XT. When my screen goes blank and later i wake up screen, i have two mouse cursors on the screen, until i launch some app and then will second, fake cursor disappear.",AMD,2025-12-13 09:49:20,3
Intel,o14ytde,"u/AMD_Vik Hey there! Just want to let you know that I've seen a lot of people have problems lately (including myself) with Direct-to-Display/Directmode implementations for DisplayPort/HDMI wired VR Headsets. DirectMode cannot enable reliably even on SteamVR native HMDs, and the only way to get them running right now is by installing a older driver version, enabling directmode there, and then updating to a newer version with the directmode already set -> so directmode itself still works fine, but there seem to be problems toggling into this. I also saw this happening with other VR compositors like PimaxPlay though radeon users are generally not as common there.  Happens with every driver released after and including 25.4.1 on my machine, and driver older than 25.3.1 can toggle into Directmode just fine. It is still broken as of 25.12.1 and 26.1.1  at first I thought this was a somewhat individual issue but as I've looked into it I saw more and more people have that problem with RDNA3 and RDNA4 GPUs across the board for basically any VR headset that uses Directmode.  I remember you mentioning on the AMD forums once that you want to be hit up about VR related issues - but really I didn't know how else I would reach out to you.",AMD,2026-01-22 23:12:00,3
Intel,ntaw89a,I hope this fixes the many crashes I've had since the last update...,AMD,2025-12-10 15:27:13,4
Intel,ntc9i8c,"Still enjoying the piss out of the 7900XTX on 25.9.2. It chews through everything I throw at it at the settings I choose, don't care about new driver releases unless a new game I want to play doesn't play well on whatever driver I currently have installed.",AMD,2025-12-10 19:27:35,5
Intel,ntb1a2q,"Even tho I have a 9070xt this is still so underwhelmingâ€¦ We waited 6 months and got basically nothing yet. Sorry for all rdna2, 3 users.  Fun fact: Its been years now that the adrenaline software cant be opened, the only fix ist to press win+p and select only main monitor. Than start it, than swap monitor profile againâ€¦   Definetly buying nvidia next time, not supporting this big company anymore, which is behind in every aspect. Image you just want to play alan wake 2 (looks beautiful).",AMD,2025-12-10 15:52:19,10
Intel,ntaqa2o,"ass. no support for rdna2/3, no new features for rdna2/3, rdna4 have only one game that support all of that, redstone framegen almost identical to 3.1 framegen, frame pacing still there.",AMD,2025-12-10 14:56:05,14
Intel,ntazsry,hardware unboxed tested it and frame facing is broken when amd frame gen is on sadly,AMD,2025-12-10 15:45:05,5
Intel,ntaqj5o,So the HDMI crashing issues should be fixed in this version yes?,AMD,2025-12-10 14:57:25,2
Intel,ntayomq,Any news on fixing the gpu vram leak issue on bf6? Sorry Iâ€™m lazing not reading the patch notes,AMD,2025-12-10 15:39:36,2
Intel,ntb8vq9,25.12.1 does not even install on my Minipc (780M) + 6650XT eGPU Setup.   I thought I might fix 25.11.X not opening in an eGPU Setup.   Guess I will be running 25.9.2 for another few Months.  God why something always break? I thought it would be better going all AMD for the eGPU setup.,AMD,2025-12-10 16:29:25,2
Intel,ntbdrmk,"Yeah I'll still be with 25.9.1 until the texture flickering is fixed in BF6, also instant replay just didn't work in 25.11.1 for me.",AMD,2025-12-10 16:53:24,2
Intel,ntbt5fb,Will this help Warzone not look so blurry on 7900xt? Game is unplayable,AMD,2025-12-10 18:08:44,2
Intel,ntc1hhd,So there seem to be two links - going through support>picking GPU(9070XT in this case) downloads the 25.21.1 win 11-b.exe file meanwhile going from this release note article it downloads the win11-c.exe . Any difference?,AMD,2025-12-10 18:48:42,2
Intel,ntc4frb,"im using 6800xt the driver page has the win11-a version and article have win11-c version. which one should i choose i really dont know and this ""different builds"" confusing a lot of people",AMD,2025-12-10 19:02:40,2
Intel,ntcb9km,"Genuine question, why all the hype and rush to release this today when it has just two games to showcase the benefits?",AMD,2025-12-10 19:36:22,2
Intel,ntcl3bs,Jesus how long has that Cyberpunk Pathtracing crash been in the known issues. It feels like it's been more then half a year.,AMD,2025-12-10 20:26:14,2
Intel,ntcrk7m,Installed with no issues,AMD,2025-12-10 20:58:08,2
Intel,ntdoxgx,"I canâ€™t play Warzone because I canâ€™t update my bios, there doesnâ€™t seem to be a recent bios update available for my Acer Nitro 5, using Adrenaline. Anyone know if this will help?",AMD,2025-12-10 23:53:00,2
Intel,ntfskqf,Doesn't look like they fixed the bug with Enhanced Sync not working properly with Freesync.,AMD,2025-12-11 09:00:35,2
Intel,ntgkfzg,Any fix planned for 9070 users who cant enable Hardware Lumen on Oblivion Remastered? Game crashes as soon as we turn on the option.,AMD,2025-12-11 13:04:42,2
Intel,nthjjtu,Still no fix for Battlefield 6 for those with AMD 6800M GPU. I swear my next setup is going away from AMD if this is not resolved anytime soon.,AMD,2025-12-11 16:15:50,2
Intel,ntkinfb,u/AMD_Vik      In 2022 AMD made changes to OpenGL Driver. So since 2022 the extension gl\_ati\_fragment\_shader is missing in the driver. It cause problems in older games like Call of Duty 1 from year 2003. Stutter on some maps and broken water rendering because the games can't use the extension anymore.     Our Community is waiting since 3 years for a fix.,AMD,2025-12-12 01:38:24,2
Intel,ntn9tly,in black ops 7 only 25.9.2 driver work better even new 25.12.1 much worse fps drops,AMD,2025-12-12 14:01:19,2
Intel,ntp4ou0,Very unstable for me (7900XTX). Driver keeps crashing even when I'm just watching videos. Reverting to 25.11.1,AMD,2025-12-12 19:39:09,2
Intel,ntq9ysh,i just had to roll back to 25.9.2 because 25.12.1 kept crashing my system with poe2   even GGG straight up said don't use 25.10-25.12,AMD,2025-12-12 23:26:01,2
Intel,ntsszh2,"After observing you guys for a few days xD, 25.12.1 was installed along with new chipset driver on my system.  To my surprise, unlike previous 25.11.1, Adrenalin interface now runs properly with igpu enabled.  I need to test it out with real games, but for now, I've dodged instant roll back.  FYI, If you're using two or more GPUs, including igpu, on a single system with muti-monitor. Download the C package(1.65GB one including rdna1&2+3&4).",AMD,2025-12-13 11:33:00,2
Intel,nttlrcj,"NoUnfortunately, they don't work (( Random crashes remained + In some games, the inability to use frame generation through drivers was added (( Sad ( R5 3600 32gb ram Rx 7700 xt ) Rolled back to 25.9.1 everything works with it",AMD,2025-12-13 15:00:42,2
Intel,ntyj52n,"I had a very weird issue:     My PC would just crash when i did an Windows Defender Scan (only Full Scan, it worked fine with QuickScan or other programms like Malewarebytes) like the power was cut. I did a number of things even rollback the chipset driver but that didn't help. Then i rolled back to 25.9.1 + the newest chipset driver and everything worked fine again.   In case somebody had a similiar issue",AMD,2025-12-14 10:21:51,2
Intel,ntz35tj,"Anyone having problem with AMD overlay with this update? Somehow not showing at any game even if enabled, if I click to different monitor, it shows up. But when I click back to the game it disappear again.",AMD,2025-12-14 13:16:47,2
Intel,ntzh5jv,AMD Wattman settings don't apply for the first time they're set. They have to be changed and applied to a different setting and then to the desired one back and forth to get them to work. I use wattman to set my custom fan curve and it's been glitchy since 25.11.1.,AMD,2025-12-14 14:45:37,2
Intel,nu4w43f,Should I download the new driver version if I have 6800XT? There is nothing in the patch notes about this series... And if yes - why?,AMD,2025-12-15 10:40:37,2
Intel,nu8xc58,"Error code 182 for my AMD Radeonâ„¢ 780M integrated GPU on my Ryzen 7 8854HS CPU.  All other driver updates before 25.12.1 worked fine on my Lenovo Legion Slim 5 Gen 9, but this one says my GPU is incompatible, even though AMD's driver download page is providing [this download link](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe) to the installer:  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-8000-series/amd-ryzen-7-8845hs.html",AMD,2025-12-16 00:02:21,2
Intel,nuazy8e,7000 Series web browser glitch? and sound glitch? huh,AMD,2025-12-16 09:03:04,2
Intel,nwzds1t,"So I upgraded on a 6800xt and lost a lot of features like video recording, screenshotting, custom game profiles, and hotkeys. Is that intended?  Crazy to be missing hardware supported features",AMD,2025-12-31 21:37:22,2
Intel,ny008s3,"AMD sucks: FSR 4 is locked to RDNA 4, while NVIDIAâ€™s DLSS 4.5 runs even on RTX 20-series GPUs. My next GPU will be NVIDIA only, and I advise everyone against buying AMD. Itâ€™s a greedy company with no respect for customers â€” you buy a graphics card last year, and the next year itâ€™s already outdated",AMD,2026-01-06 13:00:32,2
Intel,ntb2bag,"Is it safe to update, 25.9.1 is stable for my 9070XT and causes zero crashes with the timeout bullshit from clock speeds going to 3300+ MHz",AMD,2025-12-10 15:57:21,3
Intel,ntapbf5,What does fsr Redstone means ?,AMD,2025-12-10 14:50:59,2
Intel,ntb3ek3,"Is this driver more stable than 25.11.1 it was causing driver time outs and i even got a blue screen. I rolled back to 25.9,2 and now im scared to update to this one lol",AMD,2025-12-10 16:02:45,3
Intel,ntc136m,These comments are all over the place is it better than 25.11.1 or not? ðŸ˜‚ðŸ˜‚,AMD,2025-12-10 18:46:48,3
Intel,ntc1b9d,"So in short, still no support for 7000/6000 series, yipee",AMD,2025-12-10 18:47:54,2
Intel,ntfncpt,"Idk what happened but after this update my game crashed and then my PC crashed and when I turned it back on AMD Adrenaline disappeared from my PC, completely gone. What did you do lol.",AMD,2025-12-11 08:07:39,4
Intel,ntavbyt,For Sale: 7900 XTX - $50 OBO  I know these are no longer desirable due to being left in the dust by AMD after only a few months of real support but hopefully it will be at least a good paper weight for someone.,AMD,2025-12-10 15:22:38,4
Intel,ntb0knq,So now driver frame gen is gone? Unless the game specifically supports it? And the overlay as well? Both are completely gone now after the update...,AMD,2025-12-10 15:48:53,2
Intel,ntb1r9y,What about the fixes for the 7900xtx crashing all the time?,AMD,2025-12-10 15:54:38,2
Intel,ntim4nj,"Â«#AmdNeverAgainâ€ Whereâ€™s the Christmas gift in the form of FSR 4 for RDNA 3? In the new 2026 year, it might be time to think about switching to Nvidia.",AMD,2025-12-11 19:23:42,2
Intel,ntbtv4u,"Toujours pas de FSR4 pour les sÃ©ries 7000 ? Câ€™est mort. Pour ma part, je nâ€™achÃ¨terai plus de cartes AMD. Si Nvidia continue Ã  proposer son DLSS pour les anciennes cartes, alors mon choix est fait.",AMD,2025-12-10 18:12:11,2
Intel,ntaitkk,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 14:14:29,1
Intel,ntaqe06,"Downloads ""whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe"" for 9070XT, ""whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe"" for 5700 XT  What does it mean?",AMD,2025-12-10 14:56:40,1
Intel,ntatgqs,It took a while for DLSS 4 to get implemented in a good way on 40 series cards too but a version made it there. Give it time. Now if they can just start prodding developers to incorporate that as well itâ€™ll be worth it. Not enough games yet but hereâ€™s hoping!,AMD,2025-12-10 15:12:54,1
Intel,ntau6wp,Any update the in fact that and adrenaline software is not working when second monitor is connected? Especially using iGPU for second monitor ?,AMD,2025-12-10 15:16:42,1
Intel,ntauct1,Omg I think they fixed the LG oled tv reboot bug,AMD,2025-12-10 15:17:33,1
Intel,ntb6asm,Should i install it directly or should I use AMD cleanup utility first?,AMD,2025-12-10 16:16:51,1
Intel,ntb80pv,some one have problem with instaling?,AMD,2025-12-10 16:25:13,1
Intel,ntb8lei,Does this fix the driver timeouts that were happening with Edge? I had to revert the November update because of that problem,AMD,2025-12-10 16:28:00,1
Intel,ntb9m3i,Any fix or still need iGPU disabled for 7000 and 9000 cards?,AMD,2025-12-10 16:33:04,1
Intel,ntb9mdl,The update is still not showing up in install manager,AMD,2025-12-10 16:33:06,1
Intel,ntbb7t2,Honestly this software was the bane of my card for the longest time. Not having it anymore stopped so many weird bugs and crashes.,AMD,2025-12-10 16:40:52,1
Intel,ntbbs61,Does AMD's Instant Replay record still bug out?,AMD,2025-12-10 16:43:39,1
Intel,ntbd79c,Anyone tried the new fsr redstone yet? I am hoping for a big improvement over the old fsr,AMD,2025-12-10 16:50:36,1
Intel,ntbif1z,do you guys remove the old drivers before you install new ones? or just install ontop,AMD,2025-12-10 17:16:28,1
Intel,ntbp8r8,The path tracing crash STILL on Cyberpunk is absolutely wild to me. Finally AMD has a card capable of playable raytracing but we can't use it on the 'Crysis' of modern times to even test it out.,AMD,2025-12-10 17:49:59,1
Intel,ntbtqi7,Adrenalin doesn't show this update for me yet lol,AMD,2025-12-10 18:11:33,1
Intel,ntbw2oz,"> Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Glad for this, it was annoying that we were stuck in 25.9.2",AMD,2025-12-10 18:22:46,1
Intel,ntc4icg,Are the issues with SecondLife fixed? Last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:03:00,1
Intel,ntc4u1h,9060 non XT 8GB can do the math 7900 XTX Nintendont,AMD,2025-12-10 19:04:36,1
Intel,ntcbbg1,"Iâ€™m at work, so I cannot check for myself: does this fix the constant crashing in Oblivion when hardware lumen is turned on?",AMD,2025-12-10 19:36:37,1
Intel,ntcdlil,Has anyone tested Marvel Rivals on 25.12.1 version of the driver? The only stable driver that works without crashing on that game is the 25.8.1 version.,AMD,2025-12-10 19:48:14,1
Intel,ntchv47,Adrenalin Panel not showing bug still presentâ€¦ :-((((,AMD,2025-12-10 20:10:02,1
Intel,ntcyy65,Am I the only one who doesn't have the new option in the drivers for frame generation with a 9070 XT?  https://i.redd.it/7r86hslx3g6g1.gif,AMD,2025-12-10 21:34:34,1
Intel,ntd63ea,"why are there 2 versions, b and c, 1.65Gb and 991Mb, release notes and through the support page, and is it stable or shall i just keep 25.8.1 as any other seems to crash call of duty, regular, other games seem fine,  ryzen 9 7950x3d/rx7900xtx",AMD,2025-12-10 22:09:16,1
Intel,ntdfkjs,Did it fix the god of war 2018 checkered shadows?,AMD,2025-12-10 22:59:20,1
Intel,ntdjg2j,"I spent all this time with 25.9.2 on my 9060xt because the following ones were disgusting to me, I will give this new update a chance and let's hope everything improves a little!!",AMD,2025-12-10 23:21:02,1
Intel,nteannh,Arc raiders crashes are fixed or not?,AMD,2025-12-11 02:02:44,1
Intel,ntek6ze,"Makes my 9070 XT to constant run on 100% load in bf6 no matter if i play or sit in the menu. Cause device hung, graphic glitches and high temps.   Same with all drivers above 25.9.   25.9.1 works flawless with no errors and the load varies depending on the scenery as it should.",AMD,2025-12-11 03:00:09,1
Intel,nteshht,Noticing in Hogwarts Legacy with the new FSR and FG enabled over a period of like 30 seconds my 9070XT will go from \~250W used and 200 FPS and then drop down to say 120W used and 90 FPS and then after a short period go back up again. With FG disable it stays consistent 140 FPS-ish,AMD,2025-12-11 03:54:05,1
Intel,ntfd9ta,"FYI for ""Driver Only"" guys, 25.12.1 still have an issue to install this option.  l've open ticket to support team for last 2 versions. but I can't follow their request to observe the issue.  Don't know how long to keep using extracted file method. lol  Will see how 25.12.1 ""driver only"" perform.",AMD,2025-12-11 06:33:01,1
Intel,ntfu45p,oh nice! they fixed the FSR4 Quality Presets artifact issue,AMD,2025-12-11 09:16:24,1
Intel,ntg07nz,"When AMD finished Orange, Yellow Green, PurpleStone, can we unlock FSR Infinity?",AMD,2025-12-11 10:18:18,1
Intel,ntgsz5x,"Is it worth updating to this latest driver? I am not planning to use frame gen, is the image quality better or are there any fps improvements in games?",AMD,2025-12-11 13:56:05,1
Intel,nti5u4n,"Updated to 25.12.1 now, before I was on 25.8.1, have a Rx 6800 XT and a Ryzen 7 7700X. Also updated my Chipset-Driver today. Haven't testet much yet, played now for like 1 hour Space Marines 2, watched some Youtube vids since I updated. So far looks ok. Only thing that worried me first was that I found in my Reliability History, 2 critical entries of LiveKernelEvents of code 1a8. But these were written down by Windows on the time, while I was updating my driver. We will see, if anything happens I will keep you updated.",AMD,2025-12-11 18:04:23,1
Intel,ntigzns,"Despite the device ID-based driver update blocking set in August, it has worked until now. The windows tried to install some driver on the 6700XT just now, and unfortunately, it also replaced the software itself somehow. threw an error message too.  Manual update would not go through unless i removed the driver update block.   What a sad situation.",AMD,2025-12-11 18:58:17,1
Intel,ntnwqsn,"Anyone else has problems with CS2/Fortnite? Started happening after i updated drivers to 25.11 My whole PC would randomly freeze for like a minute with the ""AMD software detected that a driver timeout has occurred"" error. Once the PC unfreezes i must kill the game from task manager.",AMD,2025-12-12 16:00:37,1
Intel,nto9zy8,Does it fix the arc raiders dxgi crash of the previous driver?,AMD,2025-12-12 17:05:14,1
Intel,ntpp1wj,"How do I downgrade from this driver?   Iâ€™ve tried four different older drivers and all of them give me error 182 â€“ GPU is not supported (RX 9070 XT) during install.   Iâ€™ve already used DDU and the AMD Cleanup Utility, but the only driver I can install successfully is 25.12.1.",AMD,2025-12-12 21:27:18,1
Intel,ntqd768,pc started to crash 7900xtx... reverted to 25.11.1,AMD,2025-12-12 23:45:59,1
Intel,ntw41n8,Hi me and other people I know. Also forums and Facebook pages . Have had an issue with the frame gen after 25.9.2 . When they released new features we have all had issues where its drops fps and is completely unplayable. Has this been fixed in 25.12.1 I have 7900 xtx 7800x3d. Friend has 9070xt 9800x3d Both have issues. And im on windows 10 he's on windows 11. I used ddu and tried all settings on frame gen and other settings to fix it. Not to mention the drivers where stutters and lower fps without frame gen. Thanks,AMD,2025-12-13 23:14:13,1
Intel,ntwqp19,"When I enable V-Sync in the game, I experience lag; it only runs smoothly with V-Sync enabled when I also activate the performance overlay. This problem has existed since driver version 25.11.1.",AMD,2025-12-14 01:37:28,1
Intel,ntytwdj,"I have a second card from amd. And both cards have driver problems. Now I have an rx 9070 xt oc. I don't do any undervolting. Everything is at factory settings including the bios. I had 4 driver failures in 7 hours. What good is FSR if the driver doesn't work? It would be good if you finally solved this problem. I can stand it for a while, but if it continues like this, I'm leaving AMD.",AMD,2025-12-14 12:03:18,1
Intel,nu0egj3,"Wish they would acknowledge the bug where turning on GPU scaling and integer scaling adds more input delay, so for example the mouse movement will feel sluggish.  Been having this issue for 3 months now since nya bought a a 9070 XT",AMD,2025-12-14 17:38:40,1
Intel,nu0h263,"On the RX 7600S graphics card, Adrenalin does not launch at all, and during installation it removed the driver PCIVEN_1022&DEV_15E2&SUBSYS_15131043&REV_60.",AMD,2025-12-14 17:51:39,1
Intel,nu2b8e5,"How are those with a Cezanne CPU supposed to install this?  Selecting the 5750G from the drivers download page offers 25.21.1, yet none of the 3 variants of the installer support it.  * whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe (Vega, supposedly?) - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe (combined? ""Systems with RDNA series graphics products"") - nope  Each of them return Error 182.  Even the minimal web installer, amd-software-adrenalin-edition-25.12.1-minimalsetup-251207_web.exe, only offers 25.8.1.  VEN_1002&DEV_1638 is nowhere to be found in the .inf for any of the 25.21.1 variants.",AMD,2025-12-14 23:19:40,1
Intel,nu3psfe,I'm still hesistant to upgrade on this driver until they resolve these driver timeouts hell I'm even on 25.9.1 still experiences time to time TDR's.,AMD,2025-12-15 04:22:56,1
Intel,nu4kg83,"Is it worth for my 9060XT to go from 25.11 to the latest, Im having some problems where ghost of tsushima crashes.",AMD,2025-12-15 08:42:46,1
Intel,nu51zrq,Driver is making valorant run like crap for me idk why .,AMD,2025-12-15 11:34:40,1
Intel,nv057ke,im using an rx6600 and up until today i was fine avg 200fps on r6 today the game says its at 22 usaeg when it avgs 1-4 and now it has major fps drops/tears,AMD,2025-12-20 08:25:06,1
Intel,nv2wvfi,This driver constantly crashes call of duty for me. Whatever windows update installs(which seems to be 25.10. something) is the most stable there is. 9070XT.,AMD,2025-12-20 19:41:18,1
Intel,nvtvg24,Anyone see if this fixes the issue of the graphics sliders not working at all and being stuck?,AMD,2025-12-25 05:17:11,1
Intel,nw7hqmm,Still crashes. They will never fix it. Just buy nvidia or intel.,AMD,2025-12-27 16:03:55,1
Intel,nw7j7uy,"u/AMD_Vik It says ""Intermittent application freeze when using the in-game Radeonâ„¢ Overlay."" in fixed issues but I've actually had my whole system lock up because of what seemed to be adrenalin having issues with the performance overlay....  I noticed one thing that pointed towards the overlay specifically: I was going through Adrenalin and when I was on the recording tab I switched to performance; it seemed like Adrenalin froze so I clicked Smart Tech. to see if it would respond.  Initially it didn't, before eventually swapping to the smart technology screen. I then went back to the record tab and tried again: same results.  That's about all I've got for specific steps. I closed Adrenalin and went back to doing whatever and I noticed my fans turned on and like two minutes later when I went to close my browser my cursor stopped before I got to the corner of my screen and I needed to hard power down my system.  Not sure if this is at all related to that issue. But i had it happen on the last driver as well, and came here trying to see if there was a known issue...",AMD,2025-12-27 16:11:24,1
Intel,nxehl3e,Any chance to support VR HP reverb G2 (WMR) 60hz mode with Oasis driver? I'm locked in win10.,AMD,2026-01-03 07:42:05,1
Intel,nxhczy3,"Indiana Jones crashing every 5 minutes, cant complete the game. Its just freezes and the PC barely responsive with these Timeout messages.  9070 with 5800x3d",AMD,2026-01-03 18:31:17,1
Intel,nymudet,"czeÅ›Ä‡, jest sen instalacji jak uÅ¼ywam 7900xtx i nie uÅ¼ywam Å¼adnych wspomagaczy ?",AMD,2026-01-09 17:45:31,1
Intel,nzbmbw2,"Hi devs!   I would like to bring again to your attenction this thread: [AMD Software: Adrenalin Edition 25.9.2 Release Notes : r/Amd](https://www.reddit.com/r/Amd/comments/1nk9qgo/comment/nfb2o2j/)  Is there any chance you can bring us 60Hz mode for ex WMR drivers, now working directly in steamVR with Oasis Drivers?",AMD,2026-01-13 08:53:49,1
Intel,o0k4npb,So uh... I just had an application crash on Blue Gate while playing Arc Raiders... I don't think it is fixed...    9070xt with a 7800x3D,AMD,2026-01-19 22:19:09,1
Intel,o3z0v4r,"u/AMD_Vik Hey Vik, I have a question/request that might seem a little strange and niche. Is there any way to have the Windows update AMD OEM drivers available for manual download? If this isn't your department, would it be possible for you to pass it on or redirect me to someone in that department?   I recently did a fresh Windows install, and during a Windows update, the AMD 32.0.21030.2001 driver was installed and worked incredibly well regarding audio and image quality (video & overall). Audio quality, specifically, is still the best I've heard from any AMD driver to date.  I thought maybe the fresh install itself might have been the reason for the better audio quality, so I installed an adrenaline driver that I'm very familiar with, and was disappointed because the audio was noticeably worse. I've now tried every driver besides 25.12.1 & 26.1.1, and the audio quality is substantially worse compared to 32.0.21030.2001.",AMD,2026-02-06 21:22:10,1
Intel,ntaqgkm,"Ray Caching in 40K?  Not sure how they got this to work on the tabletop in real life but sounds awesome  In all seriousness there are a large number of games in the Warhammer 40K universe, any chance they are saying which one?  Space Marine 2 Darktide Battlesector   Etc",AMD,2025-12-10 14:57:02,47
Intel,ntavwoz,"so pretty much nothing for today, shrug...",AMD,2025-12-10 15:25:34,11
Intel,ntapnnp,Is there a partial list of the 40 games with the new frame gen? Is it something different from the fg we already have?,AMD,2025-12-10 14:52:46,8
Intel,ntam71a,they wont,AMD,2025-12-10 14:33:37,55
Intel,ntf1fzq,"It's so annoying.  I would keep it if it didn't constantly pop up trying to get me to install ""AMD Chat"" and ""AMD Privacy View"".  I don't want your shovelware AMD, take a hint.",AMD,2025-12-11 04:56:41,22
Intel,ntbob9s,"There should be an option during install to exclude it, it can't be that hard to do. Same as you, u/MihawkBeatsRoger , I also uninstall it afterwards.       Notifying u/AMD_Vik",AMD,2025-12-10 17:45:29,18
Intel,ntasnol,"This.   Why I took it out are my own reasons and quite frankly, irrelevant. It's my PC and I don't want it. So please AMD, listen to me and keep it off.",AMD,2025-12-10 15:08:39,22
Intel,ntapviv,It's a rebranding of the entire FSR ecosystem. What's new today is machine learning enhanced frame generation for RDNA4 cards. You can enable it in the driver for any game with FSR 3.1.4 or newer.,AMD,2025-12-10 14:53:56,132
Intel,ntb2cv7,It adds denoising for Path tracing. In theory it should look way better now,AMD,2025-12-10 15:57:34,7
Intel,ntap7sr,All the games that don't use bluestone,AMD,2025-12-10 14:50:26,27
Intel,ntbfl85,"Only one , the new call of duty ATM. So if you enjoy shitty games , have at it",AMD,2025-12-10 17:02:21,4
Intel,ntcmrfi,9070xt i see brave or discord freezing and lagging when watching a YouTube video still. I dont understand how hardware acceleration bug hasn't been fixed yet. Wtf are they doing.,AMD,2025-12-10 20:34:27,15
Intel,ntcuvcq,Yup same here. Had to roll back to October to fix again,AMD,2025-12-10 21:14:37,7
Intel,ntem0sw,25.9.1 works on my 9070 XT. Everything after that is a mess for me,AMD,2025-12-11 03:11:43,9
Intel,ntfe4wd,Tagging u/AMD_Vik  so they are aware of the issues.       I encountered the same problems on my 6800xt. Figma on chrome is causing random BOSD. The system will just restart without notice. Every single web app seems unstable on my system and memory usage is all over the place. Rolling back to 25.9.1 doesn't fix everything but it eliminates 70% of the issues..,AMD,2025-12-11 06:40:43,5
Intel,ntf2h1v,Oh well. :/  Funny thing is I rebooted my PC again for a Windows update. The first thing that greeted me after opening a web browser was the driver giving up the ghost.  On 25.11.1.,AMD,2025-12-11 05:04:26,2
Intel,nvesl6s,Ive been wondering what this seemingly random crashing has been. Thanks for this comment!,AMD,2025-12-22 18:30:18,2
Intel,ntgomie,"9800x3d, 6950xt, no issue with either chrome or discord or firefox with hardware accelerated set",AMD,2025-12-11 13:30:30,1
Intel,ntkqfzv,"Me too.  Installed 25.12.1, whenever I use YouTube in Full Screen, the whole system freezes, while the sound is still audible, then I have to hard-restart my PC. Happened three times, decided to downgrade to 25.11.1 again.",AMD,2025-12-12 02:24:44,1
Intel,ntalgrc,"Same, and I'm still on the October drivers",AMD,2025-12-10 14:29:29,18
Intel,ntaosq5,You can download it from the website. The app release notification always lags behind the site. This is nothing new.,AMD,2025-12-10 14:48:11,8
Intel,ntaon6n,"This should be fixed, I'm not sure why it was omitted from the release notes",AMD,2025-12-10 14:47:21,27
Intel,ntbfx3u,<--- inte 8 rdna3 enjoyer,AMD,2025-12-10 17:04:01,35
Intel,ntbh75k,"How do I set this up, can't find any info",AMD,2025-12-10 17:10:24,1
Intel,ntaukfz,"I can't speak on enhanced sync, but noise suppression is still busted and not working =/",AMD,2025-12-10 15:18:39,17
Intel,ntarxtz,"I'm piggybacking, because I need that info too",AMD,2025-12-10 15:04:52,3
Intel,nwscpi6,"I can't seem to keep framerates under control in a lot of games, generally smaller simpler games, with the new 9070xt. Enhanced sync, vsync, chill, boost, whatever I do I'm still wondering why my pc is at 100% gpu, 600fps, and 300w power draw playing something like minecraft or geometry dash.  Even with a 144hz display. I'd be happy locked at 60 even.",AMD,2025-12-30 19:52:48,1
Intel,ntbjznn,"Been using it for a few hours with the 7900XTX, so far so good.   Hopefully it's 100% fixed.",AMD,2025-12-10 17:24:13,7
Intel,ntbqeln,I hope they fixed it. I will test it now,AMD,2025-12-10 17:55:35,3
Intel,nteci65,"Did the typical test that I usually do and it didn't show up for me and I'm on the RX 7700XT as well. So hopefully, it's fixed.",AMD,2025-12-11 02:13:47,2
Intel,ntb5cx9,"AMD stopped giving a shit about it's fans once the company was saved and they started raking in the money. The change in tone was clear as day. That said, I'll still buy their GPUs because I hate Nvidia far more and I don't see that changing.",AMD,2025-12-10 16:12:17,29
Intel,ntch9q3,"yeah my next one will be Nvidia, better features, better performance espacialy with RT/PT   And apperently longer support... and AMD cards in a simmilar performacne bracket don't even cost THAT much less sooo.... jeah I am mad aswell",AMD,2025-12-10 20:07:01,16
Intel,ntk9244,"I agree. AMD has shown poor support for 7000 series owners. If there was a FSR4 int8 leak, AMD should officially release FSR4 for 7000 series owners.  I bought my 7800xt only 2 years ago before RDNA4 cards came out.  Nvidia provides DLSS4 upscaling to their older generations like rtx3000 series",AMD,2025-12-12 00:39:32,3
Intel,nw3y7cj,"Your system is almost exactly like mine, did you also have crashing problems while having the Xbox Gamebar DVR feature turned on? I would have constant driver timeouts until I turned it off.",AMD,2025-12-27 00:12:39,1
Intel,ntaoqmx,"If you're referring to the app crashes with RTPT reflections enabled, we're working with CDPR on a fix",AMD,2025-12-10 14:47:52,59
Intel,ntcukk4,Signed /another 7900xtx user,AMD,2025-12-10 21:13:09,15
Intel,nu3780v,"I came over from NVDA last March, bought a 7900xtx, RMAd it a few weeks ago due to pink/purple pixelation that would randomly happen. Now it's non stop driver timeouts and random performance issues every time I boot my PC or games. I am never buying another AMD card. I'd rather get ripped off by NVDA and not have constant headaches.",AMD,2025-12-15 02:23:41,1
Intel,nugitp7,"Which driver are you currently on? I'm just curious; personally, I'm on 25.9.2, and surprisingly, I have 0 problems, unlike with previous versions. Should I try 25.12.1?",AMD,2025-12-17 04:25:49,1
Intel,ntapar1,"Nope. Generally if the driver does not massively increases performance in some game, or you don't have any issues or the issue you have isn't fixed, then it's not worth updating, unless there is some new feature you want.    I reverted back to 25.9.1 (from the top of my head) because with any newer driver BF6 crashes randomly, and neither DICE nor AMD seem to give a damn about it.    And before someone asks, I tried any other fix on the internet for Battlefield and nothing else worked.",AMD,2025-12-10 14:50:52,19
Intel,nte7fbw,Same here. Anything above 25.9.2 crashes ray tracing games like Silent Hill  2 and Oblivion Remastered.   Ihr never had a more crash prone GPU than the 9070XT.,AMD,2025-12-11 01:43:09,3
Intel,ntc8k09,"Try this, taken from another comment branch https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 19:22:53,1
Intel,ntissbw,Might potentially be fixed by a recent Windows update?  24H2 (and an earlier mini-patch that included this) apparently resolved a lot of crashing for folks.  See [here](https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/),AMD,2025-12-11 19:57:19,2
Intel,ntaohu1,"Yeah I was really hoping they'd have got buy in from a decent number of devs with updates to big RT showcase games like Indiana Jones, Alan Wake 2, Cyberpunk, etc. But Black Ops 7 and Warhammer 40K... and that's it (for the RT features)?",AMD,2025-12-10 14:46:32,10
Intel,ntb73f1,5070ti fs  basically a better 9070xt,AMD,2025-12-10 16:20:42,13
Intel,ntcro7s,"Iâ€™d wait on 5080ti with more VRAM but these are going to be obscenely expensive knowing nvidia + current RAM prices. Both 5070ti or 5080 are more of a sidegrade than upgrade, not worth the hassle IMO.",AMD,2025-12-10 20:58:42,2
Intel,nted84w,Get a 5070ti. I never thought I would say that. But this is what is is.. 9 months after release and the drivers are still D.S.,AMD,2025-12-11 02:18:08,2
Intel,ntb6h5d,What about a secondhand 5070ti?,AMD,2025-12-10 16:17:43,3
Intel,ntbx8qa,"I mean, I wouldn't get either. 5070 TI is a sidegrade from the XTX, and 5080 is only slightly better. DLSS and RT would be the only reason.",AMD,2025-12-10 18:28:25,2
Intel,ntbzcmw,Sidegrading for an upscaler sounds like a joke.,AMD,2025-12-10 18:38:32,2
Intel,ntam4ba,"I think Linux developers are doing some experiments As of now, FSR 4 (FidelityFX Super Resolution 4) does not officially support RDNA 2 or RDNA 3 GPUs, even on Linux. However, thanks to Develerâ€™s work on VKD3D-Proton 3.0, there is partial and unofficial support for RDNA 3 under specific conditions.  RDNA 3: Partial Support via Develerâ€™s VKD3D-Proton  - Develerâ€™s VKD3D-Proton 3.0 includes support for FP8 (8-bit floating point), which is required for FSR 4. - This means RDNA 3 GPUs (like RX 7600, 7900 XT/XTX) can run FSR 4 in some games via Proton, even though AMD doesnâ€™t officially enable it. - Global override toggles in AMDâ€™s 25.9.1 driver can bypass the FSR 4 whitelist, allowing it to run in FSR 3.1-compatible games.  I hope they succed it will be a slap in the face.",AMD,2025-12-10 14:33:11,28
Intel,ntazxem,Yeah AMD refusing to port features to any card released before the 9 series makes supporting them really hard.,AMD,2025-12-10 15:45:42,8
Intel,ntakt3q,Say thanks they haven't demoted 7000 series to only game drivers,AMD,2025-12-10 14:25:47,8
Intel,ntamwc6,"Your best case is your RX 7900 turning into Balsamico, whatever that means.",AMD,2025-12-10 14:37:39,1
Intel,nte0i5m,"Its because RDNA 4 added hardware that 3 and 2 don't have. Now before I get kicked to death by angry people, there is a version of FSR Redstone that uses and INT8 path that is compatible and will work on 2 and 3, however that has not been launched today and AMD have not confirmed it will be.   That isn't to say they won't do it, but right now it's not been announced. Perhaps there will be enough noise to get AMD to change their mind or it might be that they want to get it out on their latest cards first before complicating matters with older RDNA support.  Only time will tell",AMD,2025-12-11 01:00:33,1
Intel,ntf8us3,Same boat here. Tired of trying.,AMD,2025-12-11 05:54:50,1
Intel,nte6mdt,Thanks for testing. Have you perhaps tested Oblivion Remastered?,AMD,2025-12-11 01:38:14,1
Intel,ntf8lpl,Finally fixed! It's a christmas miracle!!!,AMD,2025-12-11 05:52:44,7
Intel,ntf2wry,I regret getting this 7800xt,AMD,2025-12-11 05:07:41,2
Intel,ntazn6o,Any card released prior to the 9 series.  Amd could give 2 shits as they chase the AI bubble (jokes on them if I was an exec I'd double down on the consumer market to insulate from the impending bubble burst),AMD,2025-12-10 15:44:19,19
Intel,ntaw82d,sadly,AMD,2025-12-10 15:27:12,6
Intel,ntedpok,"Yep, I go back between 23.9.1 and 25.9.2. I couldn't be happier.",AMD,2025-12-11 02:21:03,2
Intel,ntnjpo8,"If it's any consolation, I was on an NVidia card for 2+ years where I wasn't getting the DLSS updates. Then they actively removed features when they went to the NVIDIA app.  Looking at AMD's roadmap, RDNA4 looks like a stopgap anyway until RDNA5 (prob will be called UDNA?) comes out. So in another year and a half I'll be in the same situation with my 9060XT.",AMD,2025-12-12 14:55:34,2
Intel,ntbhjqe,"Use OBS, replay buffer",AMD,2025-12-10 17:12:09,3
Intel,ntg1daf,Was just thinking of giving a shot for Indiana Jones and the Great Circle - I guess not anymore since FSR4 doesn't work with it..,AMD,2025-12-11 10:29:39,2
Intel,nuji470,"That was a terrible driver for me also. New one has been night and day improvement, give it a shot.",AMD,2025-12-17 17:12:44,1
Intel,ntasabf,"Microsoft had bugs also causing hanging crashes. Everyone loves to blame GPU drivers immediately, but check this out:  https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 15:06:41,22
Intel,ntars3b,I also want to know this.,AMD,2025-12-10 15:04:00,3
Intel,ntasr85,"I'm wondering the same thing, 25.11.1 is still the most stable for me!",AMD,2025-12-10 15:09:10,3
Intel,ntbzovt,Wondering too. I bumped back down from 25.11.1 because it was unstable on my machine.,AMD,2025-12-10 18:40:09,2
Intel,ntf7xwz,Stay on 25.11.1 if you are on RDNA 1 or 2,AMD,2025-12-11 05:47:16,2
Intel,ntgynxw,squash hard-to-find sharp reach memorize fade husky divide subsequent plough   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:28:31,1
Intel,ntlwtqy,"~~It messed up my audio, now everything sounds 8-bit. If you're on RDNA4, avoid this update.~~  EDIT: it's not the drivers, after much tinkering I was about to deduce that it was my monitor. So it should be ok to update",AMD,2025-12-12 07:15:44,1
Intel,ntelprh,I can't use anything above 25.9.1 on my 9070 XT,AMD,2025-12-11 03:09:46,2
Intel,nth7dkb,forget it they gave u the middle finger move on fuck both amd and nvidia,AMD,2025-12-11 15:15:41,3
Intel,ntb6lhi,wonâ€™t happen,AMD,2025-12-10 16:18:18,4
Intel,ntbt1wv,"Fine wine is only a thing for very few and specifics types of wine, typical wine still goes bad over time.",AMD,2025-12-10 18:08:16,5
Intel,ntbeh2e,What is the source for this or is it trust me bro?,AMD,2025-12-10 16:56:49,3
Intel,ntecwqr,They should just remove this feature. It never worked from day 1..,AMD,2025-12-11 02:16:12,1
Intel,ntbm91h,what is the difference,AMD,2025-12-10 17:35:20,1
Intel,ntcnmrm,"Can you tell me if this is also applicable to 25.12.1? There are several (frustratingly unlisted) VR-specific fixes aligned, one of them closely relates to what you've just described",AMD,2025-12-10 20:38:48,2
Intel,ntekn1e,Same here. 25.9.1 makes my problems go away,AMD,2025-12-11 03:02:56,1
Intel,ntfr6xt,Same for my 9070 XT. Device hung error,AMD,2025-12-11 08:46:36,3
Intel,nu0gqvz,"Thanks for reporting, had that once with 25.11.1 + 9070XT (W10) before reverting to 25.9.1 (since then, it never reappeared).",AMD,2025-12-14 17:50:07,1
Intel,o155b38,Thank you for reaching out.   That's really weird - I don't suppose you have any links to posts about this for us to skim through?  I'll follow up with my colleagues about this tomorrow,AMD,2026-01-22 23:45:38,3
Intel,ntoma7o,Do you get a firmware update pop up? Is this one?  https://i.redd.it/w46j86mnct6g1.gif,AMD,2025-12-12 18:06:40,1
Intel,ntmvi3j,"I'm familiar with this impacting United Offensive, I don't believe we're reintroducing this old vendor specific extension, however. I do have a ticket for the performance issues though; I don't believe this is related to the missing extension.",AMD,2025-12-12 12:32:54,2
Intel,nu65nki,"Tested for 2 days(1day and 22hrs uptime)  No crash, No BSOD for me so far. Nothing strange.  MS Edge, Google Chrome video playback, youtube...etc all play nice while gaming on main monitor.  Diablo 4, MSFS 2024, Doom dark age, Forever winter(UE5), Witchfire(UE4)...etc All run fine.  Lossless scaling runs fine on spicy vids to all of the above games xD  HWinfo64 and MSI Afterburner, RTSS all run as they should.  (Win11 25H2 uptodate, X670E, igpu(98x3d)+7900xtx+6400 3gpus, 2 monitors, hybrid mode)  Edit) rx 6800 + r7 7700x on win11 25H2, X670E, Single monitor, igpu-disabled -> runs fine.  rx 6700xt + i7 8700k on win11 25H2, Z370, Single monitor, igpu-disabled -> seems good.",AMD,2025-12-15 15:39:46,1
Intel,nuizppc,"25.11.1 had pink artifacts glitch on chromium browsers with 7700 xt but i installed 25.12.1 yesterday and no issue so far, i did not see artifact pink glitches or sound issue so far ?",AMD,2025-12-17 15:42:58,3
Intel,ntemk82,My 9070 XT hate every driver above 25.9.1,AMD,2025-12-11 03:15:11,1
Intel,ntcw96j,I updated to this driver and immediately got a BSOD. Rolled back to October 25.10.2 again,AMD,2025-12-10 21:21:24,4
Intel,ntgzi2n,offer steep theory scale straight obtainable physical ad hoc selective test   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:33:06,1
Intel,ntc7ylk,thats what I wonder too! Is it more stable??,AMD,2025-12-10 19:19:58,3
Intel,ntb2i9s,haha r u fr,AMD,2025-12-10 15:58:18,7
Intel,nteeogf,la mÃªme. C'est scandaleux,AMD,2025-12-11 02:26:49,2
Intel,ntbcqw3,"Means that they've created separate driver packages tailored for the specific gens (A rdna1/2, B for RDNA3/4, C - combined fat package that contains both drivers for systems that might have both gens on the same machine (igpu + dgpu) )",AMD,2025-12-10 16:48:24,4
Intel,ntb2cwp,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 15:57:35,1
Intel,ntcubg5,"Cleanup utility first, always!",AMD,2025-12-10 21:11:55,3
Intel,ntbx46o,I also wanna know best way updating drivers. DDU kinda annoying but maybe must be done i don't know,AMD,2025-12-10 18:27:48,1
Intel,nte7te5,It's doing it there too.,AMD,2025-12-11 01:45:34,1
Intel,ntcm462,Never seen any crashes on it with latest driver prior to today 9070xt w11,AMD,2025-12-10 20:31:13,1
Intel,ntctang,"Just tested it tonight, and for me it's working fine, 9060xt here, windows 11 with the latest update, although i play with the ""medium"" preset which disables ""lumen"", can't say it might work for you but you can try it if it still crashes constantly",AMD,2025-12-10 21:06:50,1
Intel,ntdql6b,Might want to check [https://www.reddit.com/r/radeon/comments/1pjeonb/fyi\_fsr\_ml\_framegen\_requires\_windows\_11/](https://www.reddit.com/r/radeon/comments/1pjeonb/fyi_fsr_ml_framegen_requires_windows_11/) :|,AMD,2025-12-11 00:02:35,2
Intel,ntltbvr,nope. I still crash,AMD,2025-12-12 06:44:56,1
Intel,nujivd3,"I fixed my arc raider crashes (mostly in blue gate map load) by running DDU, installing 25.10.1 (down from 25.11.1), and deleting shader caches (dont know if the shader cache delete helped or not). I upgraded to the newest drivers the day after they released and haven't had a single crash since in arc raiders, including w overlay.",AMD,2025-12-17 17:16:27,1
Intel,nts13sv,"for me, DDU in safe mode, disconnect internet, install 25.9.1 fine for me(9060XT).  I've tried 25.10/ 25.11 and revert back to 25.9.1 with this way. Now observing 25.12",AMD,2025-12-13 06:52:17,2
Intel,nujhigl,Yes. I downgraded from 25.11.1 because of the crashing. Now been on 25.12.1 all week and havent had any issues come up. You also get proper fsr4 upscaling now.,AMD,2025-12-17 17:09:46,2
Intel,nv4w9xy,"I just want to say I think I found the culprit. It also happens on the winupdate one too, because it started crashing all the time.  Core clock boosts itself WAY past what it is declared on the card(I got a Sapphire 9070XT Nitro, supposed to be 3060MHz). Here's the moment before it crashes to a black screen:  [afterburner screenshot](https://i.ibb.co/3YpJFtzM/Screenshot-2025-12-21-030537.png)  The dip in clocks is the moment it crashes. As you can see, it is running well above boost clocks. Hence, freezing in a few minutes, proceeded by a black screen, and a crash. The ups and downs are from me alt tabbing in the graphs, by  the way.   This is with core clock -200mhz applied in Afterburner and no crashes, boosts to just above declared boost clocks. Here the dips in up and down on power are probably me toying around how much exactly -mhz is needed.  [afterburner -200mhz](https://i.ibb.co/YTQfGJtc/11111.png)  All of the crashing behavior so far is replicable in COD, CS2, Cronos New Dawn.  u/AMD_Vik",AMD,2025-12-21 02:43:00,3
Intel,nw7st51,thanks for reaching out - funny timing; I noted that on the internal ticket for this issue yesterday having seen other accounts of end users noting this issue persists even with 25.12.1. Perhaps the fix aligned to that point release somehow slipped.,AMD,2025-12-27 16:59:39,2
Intel,o3zmuas,"hey little confused about this one.  - Can you elaborate on your audio config? Is this directly driven through dGfx or system IO? - Is this on a mobile system like a laptop or handheld device? if so, can you tell me which model? - Are these subjective assessments of audio quality or are you factoring hard metrics like latency etc?  if you remove adrenalin, WU should step in and serve you their latest WHQL driver. If you have a mobile system, it may grab an OEM-specific package (providing hidden bits for their eDP etc).",AMD,2026-02-06 23:16:19,1
Intel,ntazsxe,Itâ€™s for Darktide apparently,AMD,2025-12-10 15:45:06,23
Intel,ntauw3f,any game with fsr 3.1 fg also has the new fg since drivers override it. itâ€™s also why they stopped versioning fsr. any game with fsr 3.1 should just automatically have any new version of fsr when the drivers update,AMD,2025-12-10 15:20:21,11
Intel,nthk5bz,I forgoed any amd software entirely  Use more clock tool  10x better with 0% of the bloat   ^^ helped me get my 4th in world furmark score (7900xtx user),AMD,2025-12-11 16:18:42,2
Intel,ntbig0n,If you want to be in control of whatâ€™s on your computer then Windows is not the OS for you,AMD,2025-12-10 17:16:36,19
Intel,ntaqzke,Thanks.,AMD,2025-12-10 14:59:49,16
Intel,ntb7o1t,Unfortunately Redstone FG is bugged with poor frame pacing,AMD,2025-12-10 16:23:28,20
Intel,ntaqis1,Nice to see the innovation continuing on,AMD,2025-12-10 14:57:21,18
Intel,ntbic15,But only on the 9060 and 9070 right?,AMD,2025-12-10 17:16:03,1
Intel,nte60vn,I remember this mentioned since the  GCN 1.0 days. Lol,AMD,2025-12-11 01:34:30,8
Intel,ntfp000,"On my end, the driver crashes. Most of the time it manages to recover (sometimes it will crash a few more times before stabilising). Sometimes it doesn't recover (leaving only 1 of my monitors working), so I had to reboot. Then after rebooting, strong chance it'll crash again the moment I open my browser.",AMD,2025-12-11 08:24:09,4
Intel,nzzy88u,Oh thank god it's not just me.,AMD,2026-01-16 21:39:12,1
Intel,nuur9u6,"My experience with switching to amd was so smooth and perfect until 25.9.1. Everything after that just caused stutter issues in games, programs randomly crashing, drivers crashing completely causing my pc to reboot, this is so sad i hope they fix this soon and bring back a stable version asap. Rolling back to 25.9.1 now aswell until that happens.",AMD,2025-12-19 12:46:06,3
Intel,ntwnl8a,"\+1 on this. Most games crashed drivers with any newer drivers except 25.9.1, but poe2 i cant play with vulkan or Directx 12 only with Dx11",AMD,2025-12-14 01:17:11,2
Intel,nv8ptlv,pÅ™esnÄ› zustÃ¡vÃ¡m na 25.9.1 vÅ¡echno jinÃ© crash,AMD,2025-12-21 18:59:03,1
Intel,nw3xh0a,"I had been having the absolute worst time with drivers when I first bought my 7600XT, but finally found stability with 25.8.1 (and turning the Xbox Gamebar DVR off...) but I'm so paranoid now to update my drivers again. The only reason I decided to check on updates now though is a sudden appearance of my screen flashing black at random times.",AMD,2025-12-27 00:08:20,1
Intel,nth6kuu,Are you able to tell us what the error code is on the BSOD? I don't suppose you have a kernel memory dmp pertaining to one of these failures over at      C:\Windows\MEMORY.DMP,AMD,2025-12-11 15:11:30,6
Intel,ntaoqu8,Yeah same,AMD,2025-12-10 14:47:53,2
Intel,ntapwco,"Remember when you could click ""Check for Update"" inside the AMD Software and if there was an update, it would download and install it for you?  Glad they fixed that awful experience, and we have the Installation Manager now.",AMD,2025-12-10 14:54:03,28
Intel,ntap5oq,Thanks will give it a try after I finish work,AMD,2025-12-10 14:50:07,9
Intel,ntchncg,"Wait, AMD Customer Support told me that 2 monitors connected to iGPU and dGPU has never been officially supported and that this configurations breaks performanceâ€¦ so they told me bullshit?",AMD,2025-12-10 20:08:56,1
Intel,nte3vcl,Any update on three Oblivion Remastered and Silent Hill  2 Remake crashes? A lot of us are still with the September drivers because of them.,AMD,2025-12-11 01:21:16,1
Intel,ntcb9cq,<--- Ditto,AMD,2025-12-10 19:36:20,6
Intel,ntbpv70,Optiscaler lets you inject it. Do not use in multiplayer games though.,AMD,2025-12-10 17:53:00,3
Intel,ntauof3,it cannot possibly be this difficult to fix when thereâ€™s already community workarounds,AMD,2025-12-10 15:19:15,12
Intel,ntb6tpy,both are still broken somehow,AMD,2025-12-10 16:19:24,1
Intel,nwsjipr,running at 600 fps with vsync on means that somethingâ€™s terribly wrong with something in your software thatâ€™s breaking vsync. thatâ€™s definitely not normal,AMD,2025-12-30 20:25:38,1
Intel,ntcnloa,I did some testing AND as far as I can tell I do think it's actually fixed finally,AMD,2025-12-10 20:38:39,3
Intel,ntbd1ml,I would continue buying their GPUs if they gave me something to buy.  The XTX has no upgrade path on RDNA4.,AMD,2025-12-10 16:49:51,20
Intel,nteixfg,"I had Nvidia for years, the main reason I switched was that the drivers went to shit last year. I'm just sick of them in general, too. The 7800 XT I bought has been one of the most trouble free cards I ever had, aside from Adrenalin randomly closing in certain versions.",AMD,2025-12-11 02:52:24,2
Intel,ntdc84n,"If I could get my hands on a 5070 Ti Iâ€™d happily switch. AMD likes to take advantage of the underdog, for-the-people image whenever itâ€™s convenient but theyâ€™ll just as quickly throw us under the bus and fuck us raw once theyâ€™ve got the bag.  Is Nvidia a gang of greedy fucks? Sure. But at least the bullshitâ€™s right out front where you can get a good strong whiff of it. You know what youâ€™re in for.",AMD,2025-12-10 22:41:15,3
Intel,ntm5vgi,"I purchased a 7700 XT and a 7600 8gb I'm March and while I'm satisfied with performance, it would definitely be awesome to have FSR 4 on both cards as FSR 3 and 2.2 (overwatch )leave alot to be desired",AMD,2025-12-12 08:42:08,2
Intel,ntapkhc,It's been so long bro :( Hopefully the fix comes with ray regeneration support?,AMD,2025-12-10 14:52:19,24
Intel,ntbfm3b,"Hey Vik, is there any info for FSR4 Vulkan support?  It's quite sad to see that there still isn't support for it as it has been 9 months by now since the release of the 90 series  Also is there any info about the EAC issue with Star Citizen and the latest drivers?",AMD,2025-12-10 17:02:29,15
Intel,ntc52w2,"Amd Noise Supression doesn't work, when I try to turn it on, nothing happens, but in 25.9.1 it works",AMD,2025-12-10 19:05:49,9
Intel,ntcc596,"Hey amd\_vik is amd Aware of the 1 year on going Darktide issues with amd  ( GPU , and specially X3D cpus ? ), and that even the Dev of Darktide ( Fatshark ) seemingly gets ghosted by amd ?  heres some more info specially first links includes a few Dev comments  [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)  [https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f](https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f)  [https://forums.fatsharkgames.com/c/darktide/performance-feedback/97](https://forums.fatsharkgames.com/c/darktide/performance-feedback/97)",AMD,2025-12-10 19:40:50,5
Intel,ntdbffr,"Vik, weren't you on holiday leave? xd",AMD,2025-12-10 22:37:02,4
Intel,ntc7hrz,Any fixes for the SecondLife issues we've had the last few months? last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:17:39,2
Intel,ntbwr0w,Will this update fix some of the artifacting Iâ€™m seeing in cyberpunk with fsr enabled?,AMD,2025-12-10 18:26:01,1
Intel,ntcji37,Also getting driver timeouts in Cyberpunk with RDNA3 with raster or RT. I did not have these problems with my RDNA2 card.,AMD,2025-12-10 20:18:15,1
Intel,ntcuy8r,"The AMD FSR ML-based Frame Generation option in the Radeon panel disappears in Windows 10.  So I have a question: Is ML-based Frame Generation no longer usable in Windows 10? This option is available in Windows 11, but not in Windows 10.",AMD,2025-12-10 21:15:00,1
Intel,ntdcmj2,Can I join if mine's just an XT?,AMD,2025-12-10 22:43:23,1
Intel,ntawh67,What GPU are you using?,AMD,2025-12-10 15:28:30,2
Intel,ntfkwf8,Try reinstalling Windows. That fixed it for me.,AMD,2025-12-11 07:43:52,1
Intel,nte7o94,"This doesn't work. We are talking about games that crash with or without it, the only difference being the older AMD driver working.",AMD,2025-12-11 01:44:41,4
Intel,ntc9ed0,I already install the latest update before update drivers its not update related. Vulkan driver is the problem in indina jones and silent hill 2 after windows update 25.11.1 not crashing ray tracing enabled but in 25.12.1 its broken again. So driver is the problem...,AMD,2025-12-10 19:27:04,2
Intel,ntamwm4,"They said earlier in 2025 they were working on FSR 4 support for RDNA 3, and then it leaked in September with the INT8 version...",AMD,2025-12-10 14:37:41,10
Intel,ntal44u,"They might as well have lol, they aint getting no new features",AMD,2025-12-10 14:27:29,16
Intel,ntbssdw,They also promised features to the few of us who bought 7900 XTX. Good luck defending them when it's your turn to be disappointed.,AMD,2025-12-10 18:06:58,2
Intel,ntbim9d,I expected them not to abandon their king card lmfao. Who does that,AMD,2025-12-10 17:17:26,2
Intel,ntimm5h,Maybe next time you should read the whole thread before replying.,AMD,2025-12-11 19:26:09,1
Intel,nthyzs0,"I have the 7800 xt hellhound i F love it, tbh i care less about this redstone thing but its frustrating why a 2 year old lineup is abandoned all of a sudden",AMD,2025-12-11 17:30:59,2
Intel,ntbkahh,"> I'd double down on the consumer market to insulate from the impending bubble burst  If that bubble bursts nobody is going to have much money to spare for consumer goods. That bubble bursting will tank the entire economy along with it.  *Long* term that might work out better, though.",AMD,2025-12-10 17:25:41,3
Intel,ntbdi9g,further reminder amd is not your friend sadly,AMD,2025-12-10 16:52:07,9
Intel,nth472g,Same for me but Doom Eternal. I play at 4k and it needs upscaling at that res.,AMD,2025-12-11 14:58:42,1
Intel,ntfl1sl,What if I'm on RDNA 4?,AMD,2025-12-11 07:45:19,1
Intel,ntheoob,Yeah there are no good choices,AMD,2025-12-11 15:52:11,1
Intel,ntbmlpj,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html),AMD,2025-12-10 17:37:04,1
Intel,ntcu71s,Adrenalin is for GPUs.   Chipset is for CPU & mobo.,AMD,2025-12-10 21:11:19,1
Intel,o15856l,"Most of the conversations happened across various discord servers, so I cannot directly link them, but I also found a few posts here on reddit that seem to point towards the same direction which roughly match with the timeframe.   [https://www.reddit.com/r/Vive/comments/1nl0540/direct\_mode\_not\_working/](https://www.reddit.com/r/Vive/comments/1nl0540/direct_mode_not_working/)  [https://www.reddit.com/r/AMDHelp/comments/1ljezuf/steamvr\_crashing\_on\_amd\_drivers\_after\_2451\_direct/](https://www.reddit.com/r/AMDHelp/comments/1ljezuf/steamvr_crashing_on_amd_drivers_after_2451_direct/)  It's a bit odd because if you set the flag beforehand with a older version and then update to newer versions, Directmode still works fine, but if you fully remove the drivers with AMD's driver removal utility or Display Driver uninstaller (which propably deletes some cached driver file for directmode, is my guess), Directmode disables fully and can't turn on anymore at all.      As far as headsets are concerned, I know at least Valve's implementation seems to suffer from this (I know about cases involving the Valve Index, Bigscreen Beyond and Vive Pro 1, all using Valve's implementation), but somebody in the pimax subreddit community discord seemed to have the same problem, which he also was only able to fix by rolling back the drivers.  I could propably reach out to a few more affected people over discord and encourage them to get active here, if you'd like.  But I do think that it is a fairly reproducable problem, at least on my end, it happened across two different systems.",AMD,2026-01-23 00:00:34,3
Intel,o16r403,"Hi - I am having this problem. I wanted to try the special ROCM driver so dutifully did a clean install as suggested.  Tried to launch SteamVR and it errored. Realised headset had reverted to being a monitor, running at low resolution (less than recommended) Increased the resolution to recommended (combined resolution of the two panels), and SteamVR started detecting the headset - though promoted to enable direct mode. Pressing the button to enable it restarts SteamVR, but then prompts again, and HMD is still a monitor.   Interestingly reverting to the RDNA 4 release driver, it instantly worked - Direct Mode was enabled without me having to toggle.  So it is like SteamVR is successfully toggling something, but the newer AMD drivers ignore whatever it is doing/don't act on it. As soon as older driver is installed, it does act on it, and the HMD disappears from Display Settings/stops being treated as a monitor.  This wouldn't affect the majority of SteamVR users who are streaming to a Quest, and it wouldn't impact people who upgrade to a newer driver without a cleanup.  It would affect people doing fresh installs with a recent driver, and people who do clean driver installs/use DDU.",AMD,2026-01-23 05:17:40,1
Intel,o178o4d,"I installed the latest driver from a couple of days ago, and my Beyond is a monitor again ðŸ˜­   I didn't explicitly choose to do a clean install, but maybe because I chose to install the AI Package, it decided to do a clean install for me automatically?",AMD,2026-01-23 07:37:51,1
Intel,o31fiek,"u/AMD_Vik Hey! just following up on it!   Did you guys manage to replicate it? Just curious if there is anything happening in that area.  I know development and testing takes time, so I completely understand if nothing happened yet, just curious if you guys had problems replicating the issue.",AMD,2026-02-01 21:06:13,1
Intel,ntpptsg,Yes thatâ€™s the one. I have no idea where to turn lol,AMD,2025-12-12 21:31:29,2
Intel,ntnglvb,Sad news. Nvidia still supporting old extensions.,AMD,2025-12-12 14:38:54,2
Intel,ntbuxj0,"Hey OP â€” Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-12-10 18:17:20,1
Intel,ntdymrv,oh wow i haven't had any issues yet but that doesnt mean much. 25.11.1 i didnt have issues for a week or so.,AMD,2025-12-11 00:49:22,1
Intel,ntcmz77,"Interesting, Iâ€™ll test it today. I was crashing non stop on 25.11.1 so hopefully this update fixes it",AMD,2025-12-10 20:35:33,1
Intel,ntlyzyv,Same,AMD,2025-12-12 07:35:41,1
Intel,nv973go,"I have the same model GPU inconsequentially boosting well above the advertised clocks (nearly 3.4GHz) in both windows 10, 11 and fedora 43 with no issues.  This has been discussed several times on this community; whilst the clock behavior may surface other issues or instabilities on the system, it's not in itself the cause of problems.",AMD,2025-12-21 20:28:00,3
Intel,nw7vn10,"I actually have one more potentially related thing for you!   During the game I tried to turn the overlay on using my hotkey. Noticed it didn't. Since I've seen this before (we can call this a ""soft lock"") I tried to open the full screen experience with the hotkey. Which brought up my mouse (was using a controller in game before pressing the keys) but I could not move it...  My workaround has been: ctrl+alt+esc to task manager, tab to the search bar, type ""radeon"" and force kill the host service.  The instance I reported before this was a ""hard lock"" that I've noticed while trying to use my browser over a borderless game running, before this time where it was when the gpu wasn't under any actual load as far as I knew.  Glad to hear it's a known issue and not my hardware though... Thanks for getting back to me!",AMD,2025-12-27 17:14:03,1
Intel,nxaf1nm,Which driver version DOESNT have this issue?   I've tried going back all the way to .10 and it's all having the issue...,AMD,2026-01-02 17:46:29,1
Intel,nylsudn,"Good morning, when will the new AMD Software driver be available?",AMD,2026-01-09 14:55:31,1
Intel,o417fmw,"Sorry for the delay, I had to leave for work.   Absolutely.   I'm on a desktop pc. I have powered speakers connected via usb to a pcie USB card installed in the pc, and I have a separate DAC (for my headphone setup) connected via USB to the motherboard.   All of the differences I'm hearing are from subjective assessments while testing & using the drivers.  All of my listening has been and is with Windows audio set to 16/44.1 for each device. I don't use any of the AMD audio outputs that populate the audio control panel, so I disable them. I don't use any software players; Roon, Jriver, etc. I just listen to music with Qobuz (Wasapi 16/44.1) and Spotify. Additional listening is done with Youtube and Twitch via Firefox. Just everyday simple stuff.  I hear you, the reason I haven't tried running WU again to maybe get that specific driver again is mainly because I don't want to update my system to a build that could potentially cause performance degradation or instability. That's why I would like to be able to download and install/uninstall that specific driver manually, similar to the Adrenaline drivers on the AMD website.  If you would like or need any more information, just let me know, and I'll be happy to share.",AMD,2026-02-07 05:14:00,1
Intel,ntc5yzi,Literally the one game I don't play lol,AMD,2025-12-10 19:10:11,9
Intel,ntb8cnz,"Yay, I own that one",AMD,2025-12-10 16:26:50,3
Intel,ntestwe,"It's not even out for Darktide yet either. Fatshark clarified that it's experimental and needs more work, so it's not in the live build",AMD,2025-12-11 03:56:23,3
Intel,ntazo47,So it's under the umbrella of the fsr4 override if I understood this correctly. For the fsr2 and 3.0 games I can use optiscaler right? Sorry I just bought a 9070xt coming from nvidia so I need to get used to these things.,AMD,2025-12-10 15:44:27,5
Intel,ntifqj7,I used to do that but a few games can use the FSR4 in driver upgrade.  The enhanced sync was nice too when it worked.,AMD,2025-12-11 18:52:16,2
Intel,ntbjqio,"Unfortunately I play games and run software that require Windows so I have it on a separate drive. When I do switch to it (and I update the driver to take advantage of new features), this shit typically happens along with a slew of forced updates.  You are right though, I do primarily run CachyOS.",AMD,2025-12-10 17:22:58,17
Intel,nte7ly0,found the linux user,AMD,2025-12-11 01:44:17,4
Intel,ntd5qr8,You're talking nonsense.  Engineer managing 2k endpoints and several hundred servers.,AMD,2025-12-10 22:07:29,4
Intel,ntcmjlp,Wasn't the dude's claim it has been always bugged with AMD,AMD,2025-12-10 20:33:21,5
Intel,ntctlcs,ðŸŒðŸ‘¨â€ðŸš€ðŸ”«ðŸ‘¨â€ðŸš€,AMD,2025-12-10 21:08:21,1
Intel,ntasjqd,It's barely an improvement.,AMD,2025-12-10 15:08:04,12
Intel,ntcmoxo,It's branding,AMD,2025-12-10 20:34:05,1
Intel,ntbkqjy,"Yes, RDNA4 refers to the RX9000 series.",AMD,2025-12-10 17:27:52,3
Intel,o012dhm,Disable mpo,AMD,2026-01-17 01:17:11,1
Intel,ntp6j29,I have one of these captures if you want it (error code 0x00000119). I've been having a TON of driver timeouts and BSOD for the past couple of driver versions and I've had to roll back to October to resolve them. Seems like any app that has hardware acceleration enabled causes it and exasperated when viewing the system via RDP.,AMD,2025-12-12 19:48:47,1
Intel,ntb0ccn,"Uninstalling the install manager brings back the ""check for updates"" functionality until you update again (and have to re-uninstall the install manager)",AMD,2025-12-10 15:47:46,12
Intel,ntap8zv,Let us know how it goes!,AMD,2025-12-10 14:50:37,10
Intel,ntci6s3,"I don't know how much of an impact this could have on perf since it's not something I've measured. I personally wouldn't do this, though. With a dGPU installed I keep iGFX off.",AMD,2025-12-10 20:11:40,5
Intel,nted5dt,"performance wise it should only be a couple frames of latency, when doing rendering on dgpu and going out through igpu it'll just copy over the frame buffers.   Main impact is on pcie bandwidth as it'll use up quite a lot there, and to a smaller degree RAM load, so you definitely don't want to run some other dynamic load on the igpu when gaming to overwhelm its pcie link. I think on 7000/9000 it's x8 so it may be fine? But I'm really not sure could be x4 too",AMD,2025-12-11 02:17:41,1
Intel,nth79az,"We're tracking a failure in silent hill 2 remake, I believe a fix is aligned to a future release. I'll need to check in with oblivion remastered",AMD,2025-12-11 15:15:04,3
Intel,ntdvql1,"Do you have to do that convoluted setup and download the drivers from Limewire, or has Optiscaler wrapped it in to their application?",AMD,2025-12-11 00:32:26,2
Intel,ntbvuyt,"So, no official release... ;(",AMD,2025-12-10 18:21:45,1
Intel,nte1rh2,Any tutorial for a noob on RDNA2?,AMD,2025-12-11 01:08:16,1
Intel,ntbfsb9,what workaround?,AMD,2025-12-10 17:03:21,5
Intel,ntbgebv,"Same issue with fsr4 on rdna1-3.   It shouldn't be this difficult, it's in a perfectly working state made possible by like one guy's few days worth of work.   And yet AMD just doesn't do it...",AMD,2025-12-10 17:06:24,2
Intel,ntbmhrj,FUG,AMD,2025-12-10 17:36:32,1
Intel,nwtucuk,"Oh, definitely not normal for sure... but I have this issue on multiple games and I did not have this issue on the 6080 it replaced. This seems to only be impacting my 9070.",AMD,2025-12-31 00:22:00,1
Intel,ntczm93,"Such a relief, but i am also annoyed because they are ignoring 7000 series... I can literally use FSR 4.0.2 on my 7700XT and it is WAY better than FSR 3.1....",AMD,2025-12-10 21:37:51,2
Intel,ntcztos,I hope it is fixed for me as well ðŸ˜­ðŸ™. Thanks for the info.,AMD,2025-12-10 21:38:50,2
Intel,ntchg2c,yep would have upgraded but with an XTX.... you can cut your vram in 2/3 and have less Raster performance for a good upscaler and better RT performance it's such a stupid fucking problem....,AMD,2025-12-10 20:07:57,4
Intel,ntaq6sy,"That's not something I'm privy to, but it could be worth reaching out to them to request looking into if they're not already.",AMD,2025-12-10 14:55:36,34
Intel,ntbho9v,"I'm not privy to any of the FSR stuff - that's a different team to mine. I can pass on the feedback.  The Star Citizen EAC issue should be addressed, please let me know how it is.",AMD,2025-12-10 17:12:47,24
Intel,nte0zy9,i still am!   so many fixed issues out of the release notes that I felt the need to stick around and help clear things up in the communities I frequent. I'll go back into hiding again soon,AMD,2025-12-11 01:03:33,5
Intel,ntciqi1,"I've seen something like this over at OCUK Forums but weren't given enough data to work with. We've attempted to reproduce a corruption issue but apparently we've not been successful.  Can you give me a step by step breakdown on how to hit this, as well as a clear depiction of the issue?",AMD,2025-12-10 20:14:24,4
Intel,ntqc750,"No, XT peasants needs to form their own group.",AMD,2025-12-12 23:39:51,2
Intel,ntcxhw3,6800XT.,AMD,2025-12-10 21:27:27,8
Intel,ntanrvb,Some of their marketing said they would like to get it working if possible.,AMD,2025-12-10 14:42:33,10
Intel,ntbkv5b,Yeah there are going to be serious consequences as major retirement funds have invested in all these AI stocks because they have made so much money.,AMD,2025-12-10 17:28:30,3
Intel,ntflk2b,"Give it a try, for my 6800xt it's crashing in almost all games...  ![gif](giphy|QMHoU66sBXqqLqYvGO)",AMD,2025-12-11 07:50:12,1
Intel,o17bv58,"Scrap that!   I ran SteamVR which caused Steam to crash but restarting it as prompted, my Beyond 2E disappeared as a monitor again - weird!",AMD,2026-01-23 08:06:21,2
Intel,o31hwxw,"Hey there,  Filed this to triage and debug some time ago. A few colleagues outside of that domain have attempted to repro on their personal systems with applicable headsets like the Index and Vive but haven't had any luck so far - the toggle's working for them just fine.  Should be picked up by the T&D team fairly soon, will see what they find.",AMD,2026-02-01 21:17:49,2
Intel,ntsqrgy,"Sorry, out of curiosity, if you close it, it won't let you play? What do you get? Could you send me a photo so I can understand?",AMD,2025-12-13 11:11:24,1
Intel,ntnluq2,"I agree. Please can you raise a ticket requesting support for this over at our GPUOpen and ask other end users and developers to upvote it and leave a comment registering their interest? (please share a link to it here if you do) https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues  As far as I'm aware, the impacted titles are: IL-2 Sturmovik: 1946, Neverwinter Nights Diamond Edition and Call of Duty. If there are any others, I would really appreciate you letting us know.  E: I believe it's posted here: https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues/80",AMD,2025-12-12 15:06:46,2
Intel,nthltl0,"Just an update - I ended up running DDU and re-installing the latest update and now things are pretty stable, no driver timeouts from hardware accelerated apps either. Could be something to do with the architecture change between driver packages - but doing a complete removal between updates seems required now.",AMD,2025-12-11 16:26:46,1
Intel,ntcop7s,I never seen 1 crash on 25.11.1 although I did use the preview update for windows 11 last week which fixed some amd gpu related crashing and that solved my arc raiders random crashing,AMD,2025-12-10 20:44:09,1
Intel,nv9cafd,"My apologies then - it seems latest driver on Windows seems to be the source of issues then, seems more people have issues posting on /r/AMDHelp , also with 9070XT's. Seems all device hung errors and timeouts recently posted are with 25.12.1. I had no issues on cachyOS (Hyprland) running CS2 too, latest amdgpu.",AMD,2025-12-21 20:55:29,1
Intel,nxbs3rv,I believe this was introduced with the 25.20 driver branch. it shouldn't be present in 25.9.1/2,AMD,2026-01-02 21:39:50,2
Intel,nym5qu9,"I think our SVP noted in a recent interview it'll be later in Jan, the date they provided was the 21st, though I'd treat this as a tentative timeline just in case anything crops up",AMD,2026-01-09 15:55:10,5
Intel,o44mqlk,"Two close colleagues have very kindly gotten back to me on their saturday to chime in with the following insights:  - One of them suggests that USB audio *should* be unaffected by the adrenalin package. - The other expands on this to mention ""If the audio is choppy, I'd suggest using latency mon to see if there's any long running DPCs or interrupts"", linking to the following  https://resplendence.com/latencymon",AMD,2026-02-07 19:14:23,1
Intel,ntb0k7p,"yes, 3.1 is where AMD adopted the same modular approach as nvidia so any game at fsr 3.1 or above just runs at whatever latest fsr version your driver supports, which is currently 4 although now the versions aren't numbered anymore",AMD,2025-12-10 15:48:50,6
Intel,ntcauqa,Hell yeah ðŸ‘ðŸ»   Impressive you can run that on a 5x86,AMD,2025-12-10 19:34:18,3
Intel,ntbryby,"Since you're already an advanced user, perhaps you could block it from installing by selectively blocking AMD in your hosts or pi-hole? It's not a dumb solution, but it's better than having to deal with push-installs.",AMD,2025-12-10 18:02:57,4
Intel,nthi3lk,I might be an ass but Iâ€™m not wrong,AMD,2025-12-11 16:08:50,2
Intel,nthi8dc,Sorry  If youâ€™re a **consumer** and want to be in control of whatâ€™s on your computer then Windows is not the OS for you,AMD,2025-12-11 16:09:28,2
Intel,ntfql24,"Yes, If you mean the bad frame pacing when fps is lower.  I still opt to spent 1-200 hrs of my gaming session with FSR 3 frame gen, 7900xtx.  It's not that bad when the output is close enough to monitor max hz, similar to what hardware unboxed did in thier test.  The generated frame still comes out too early but it has to wait for the monitor's nest refresh which is consistent.",AMD,2025-12-11 08:40:22,1
Intel,ntc2hr1,ty,AMD,2025-12-10 18:53:26,1
Intel,ntpa4lm,can you run analyze -v in windbg or fire it over to me via your preferred file sharing method?  I personally like to use https://send.vis.ee,AMD,2025-12-12 20:07:35,2
Intel,ntb1b5l,"u/amd_vik it sounds like this person doesnt want the manager to install again, but I am pretty sure you can do custom option to uncheck it. If you do express of course it will put it back sschuler.",AMD,2025-12-10 15:52:28,9
Intel,ntcew16,Can confirm this issue is fixed for me on 9800x3d + 9070xt (I had this issue on 25.11.1 and reverted to 25.10.2 until today) ðŸ‘,AMD,2025-12-10 19:55:01,8
Intel,ntb65up,"Seems to be working fine, though when I was installing the driver my igpu showed up separately from the dgpu in the installer with a download link. But when re-running it they both show under 25.12.1  Should I be installing some separate older driver for it to keep things like hw accel working or was that just some hiccup?",AMD,2025-12-10 16:16:11,6
Intel,ntaufrk,Oh great will also test after work itâ€™s been headache since last driver update,AMD,2025-12-10 15:17:58,6
Intel,nthzjga,Thank you for taking the time to respond. This has been very frustrating.,AMD,2025-12-11 17:33:43,2
Intel,ntlgdax,"I'm sorry to comment directly to you here. Do you have any report about monster hunter wilds performance drops in recent GPU drivers ?    I'm using 9070xt.    I have to use version 25.3.1 to play wilds with no stutters, anything newer gives a lot of stutters in many places.",AMD,2025-12-12 05:02:06,1
Intel,ntjjshb,"Yeah you still have to download it on your own, the creator of Optiscaler already said they aren't going to bundle it probably due to the whole legality around it.",AMD,2025-12-11 22:14:22,1
Intel,ntbpcf0,"i saw a post that detailed how to essentially replace noise suppresion with the working version in newer drivers, you can probably find it here somewhere",AMD,2025-12-10 17:50:29,3
Intel,nwtxl54,yeah somethingâ€™s definitely wrong. iâ€™m assuming youâ€™ve already tried ddu?,AMD,2025-12-31 00:39:33,1
Intel,ntbytau,Thank you for this! been waiting for a fix with Star citizen.,AMD,2025-12-10 18:35:58,7
Intel,ntcdxlc,Yeah SC seems to be working for now.,AMD,2025-12-10 19:49:59,5
Intel,ntcib7n,"Bonjour, pour le moment sur Star citizen le problÃ¨me avec EAC fonctionne pour la 7900XT. Merci d avoir rÃ©glÃ© le problÃ¨me. Bonne fÃªtes de fin d'annÃ©e.",AMD,2025-12-10 20:12:16,2
Intel,ntcro8s,That's good to hear. What about Noise Suppression not working since 25.9.2?,AMD,2025-12-10 20:58:42,1
Intel,ntcnbes,Hmm let me try. So pretty much having installed the latest driver (25.12.1) I just open SecondLife. I look closely at my avatar/character and my skin looks like this  [https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4](https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4) (excuse my outfit but just easier to show)  this is how it's supposed to look and also does on 25.9.1 [https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4](https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4)  I've heard that this doesn't occur on linux but only windows (But I don't have linux so can't say for sure)  I think you need PBR / Materials or some reflection on your skin to see the issue.   If you fly up to around 2000+ meters above ground it becomes easier to see  These are my settings [https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png](https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png)  I have an rx 7900XTX,AMD,2025-12-10 20:37:14,3
Intel,ntcnscv,"Hello! I am actually one of the developers on the client team for Second Life, and I have been trying to figure out how to get in touch - we have found at least one nasty bug on some of the Strix Halo chips with the current drivers.  Can you send me a message here so we can exchange emails?",AMD,2025-12-10 20:39:36,3
Intel,nu85qao,ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­,AMD,2025-12-15 21:31:35,1
Intel,ntdy4eq,"I had a similar issue with my 6800xt and the other thing that helped was to sit it to fullscreen or borderless and swap back and forth. Now I'm only playing in fullscreen (which is annoying), but it doesnt crash anymore.",AMD,2025-12-11 00:46:19,1
Intel,ntf5uuk,I have the same card and exactly the same problem. Can't install newer drivers or BF6 just constantly crashes.  I'm on 25.10.2 tho,AMD,2025-12-11 05:30:42,1
Intel,ntapogt,"And it is, and they did, we have the leaked int8 version from September... Just needs official driver implementation now.",AMD,2025-12-10 14:52:54,4
Intel,ntu98tq,"Before the Black Ops 7 (which I donâ€™t own) integration to Warzone, I could click off it & carry on. But since the integration it just closes the game.",AMD,2025-12-13 17:06:00,2
Intel,ntudqmy,"Yes, i have created this github issue.",AMD,2025-12-13 17:29:47,3
Intel,nva2mbl,"If those failures are avoided by clock limiting the board, the problem area could be a different domain entirely (CPU, memory, power, etc.).  The linux remark is interesting, it kind of calls back to similar failures with NV31 in certain apps like Helldivers 2; we had a little internal discussuon about how the amdgpu kernel driver managed to mostly avoid such issues, though I dont recall the outcome.  If you get the opportunity, I'd recommend a suite of system integrity routines as a sanity check; please take a look at [one of my older posts](https://old.reddit.com/r/Amd/comments/1l9ox9r/amd_software_adrenalin_edition_2562_optional/nn3yuay/) for some background.",AMD,2025-12-21 23:17:59,2
Intel,o12cvk0,issue persists in the newest 26.1.1 update....,AMD,2026-01-22 15:53:23,1
Intel,nym5z84,OK thanks.,AMD,2026-01-09 15:56:13,1
Intel,o46e0sj,"First and foremost, thank you for taking the time to listen/read and look into this, I appreciate it greatly. I'm not sure why, but it seems your reply about the issue possibly being related to ISR isn't showing up on here for some reason. Could just be the site being dumb.  So, the audio differences I'm hearing aren't related to functionality, or the ability to playback audio without interrupts (dropouts, clicking, popping, choppiness). That has been flawless, for me, which is great. I'm referring to audio fidelity differences between drivers.  For instance, I uninstalled the driver I've recently been testing (25.12.1) via DDU and listened to some music with just the Microsoft display adapter being used as the driver. The audio sounded pretty normal again, and what I would consider correct/good. I then ran windows update, and it thankfully installed the 32.0.21030.2001 driver. Listened to some music, and as I heard before, it's still the best sounding driver I've used. The only other driver I've used that comes close is 25.1.1. That one has some slight issues with audio fidelity and image quality, but it was close enough to correct at the time.   The only thing I can think of that sounds similar and would describe what I'm hearing with these newer drivers is when you have a bit-depth and sampling rate mismatch between your music and output device. I have my audio devices set to 16/44.1 in windows, but with these newer drivers (25.9.1-25.12.1), the audio sounds as if the bit depth and sampling rate are set to higher values, even though they aren't. It's extremely strange, and sort of difficult to explain, but that's what I'm hearing.   The frustrating and unfortunate reality that might be coming into play here, is that differences could also vary between systems. So, a bad sounding driver on my system (25.9.1), might possibly sound fine or even good on a different system with different components.  It could also be caused by or related to interactions between code or the registry entries of different programs on the system interacting with drivers, but that's above my level of understanding. I'm just a dude who is noticing stuff (audio & video) and would like others who are smarter and more skilled to look into why there are differences in the first place and what could potentially be causing them.    By the way, you guys have been doing a great job with the video/image quality of these recent drivers. They are substantially better and closer to reference than everything previously released. 25.9.1 and 25.11.1 are the best drivers I've tested/used, in regards to video & image quality.",AMD,2026-02-08 01:11:28,1
Intel,ntb3jrw,"They aren't numbered in the sense of like 4.0.2 or like there won't be an ""fsr 5""? Thank you very much btw, very helpful info!",AMD,2025-12-10 16:03:28,3
Intel,ntdazyo,Like a charm. :D,AMD,2025-12-10 22:34:45,1
Intel,ntbte7r,"I probably could, but AMD (and any other company, really) should be following the users preference anyways. It is a band aid fix and doesn't solve the problem.  Not a bad idea though.",AMD,2025-12-10 18:09:54,4
Intel,ntwpskf,I've been using computers since dos 3.  You're a spanner.  I'm sure MacOS is soooooo much more open.,AMD,2025-12-14 01:31:33,3
Intel,ntpczrx,Here you go: [https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw](https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw)  I did run in windbg but I have no idea how to save the output unless you just want a copy + paste of it here haha,AMD,2025-12-12 20:22:52,1
Intel,ntbtmtr,"Thank you for the idea, I just tried a custom install during an update, was given 2 choices (update/dont update driver and install/dont install privacy view). After installing drivers, step 2/2 was installing the install manager.    After updating through adrenaline using the custom option, I attempted reinstalling again using the auto-detect, custom install. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.   Installing via the WHQL package, custom install follows the same steps as above. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.",AMD,2025-12-10 18:11:03,1
Intel,ntci8tp,Appreciate the feedback,AMD,2025-12-10 20:11:57,6
Intel,ntb8e8t,Thank you for confirming.  That interesting though. I think the most seamless way to support products from both branches is to use the AMD auto detect tool. Can you tell me how the iGPU is represented in Windows' Device Manager?,AMD,2025-12-10 16:27:02,6
Intel,ntbkinx,"[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html)  https://i.redd.it/3vxsa8yave6g1.gif  If you suspect the installation is incorrect, download the package that includes the IGPU driver using the link provided above. The basic version does not include the IGPU driver, but provides a separate download option during installation.  Anyway, it seems like a lot of bugs have been fixed in this version.",AMD,2025-12-10 17:26:48,1
Intel,ntbxdgl,"if you can find it, you will be the goat",AMD,2025-12-10 18:29:03,4
Intel,nwv35gt,"This isn't every game, this is only some games. Not all games have a native vsync option either. That being said, from what I can find, this is a known issue.  https://steamcommunity.com/discussions/forum/1/601900047372731730/  https://www.wumeicn.com/screen-tearing-fix-for-rx-9070xt-and-freesync/",AMD,2025-12-31 04:49:26,1
Intel,ntciae3,Thank you for letting us know ðŸ‘,AMD,2025-12-10 20:12:10,8
Intel,ntcoi7s,appreciate the info. I'll ask our technicians to check in with the settings you've provided,AMD,2025-12-10 20:43:12,4
Intel,ntfvp58,I can confirm there is no issue in linux. A windows version running under proton in linux has no issues as well.   In the video there is flickering on head and body. I see only flickering on the head (when running it on the windows pc)  But my body has no layers attached - the body in the video usually comes with layers. But all heads have multiple transparent layers. The problem occurs even when that layers are not in use and are fully transparent.   Probably related.,AMD,2025-12-11 09:32:57,1
Intel,nte0wcf,"Hey there, thank you for reaching out!  I don't suppose it would be possible for one of our devrel folks to contact you via a linden lab email address like business@lindenlab.com?",AMD,2025-12-11 01:02:56,3
Intel,o49igbf,"Hope this gets fixed eventually, it's still an issue for both my 9070xt and 7800xt machine",AMD,2026-02-08 15:21:22,1
Intel,ntuvq16,"So if you click dismiss, the game closes, did I understand correctly? It doesn't let you enter the COD HQ ? I'm telling you this because I too should update the bios, in fact it happens to me too, but I click dismiss and it lets me play anyway.",AMD,2025-12-13 19:03:40,1
Intel,nva7np2,"for CS2, it was the newest driver that caused crashes exclusively, but on that driver I also got stronger boosts off the bat, hence it crashed faster. Now on 25.10.1(from windows update), COD still crashes with a black screen then tab to desktop with a driver timeout detected. Looking at afterburner(just using it to monitor clocks, no OC/UV applied or anything) the moment the GPU touches 3300+ I get thrown to the desktop. Can't even finish the training course even with ""speedrun strats"" before it crashes. It boosts [momentarily to 3300+](https://i.ibb.co/bgLFC0dp/coreclockcrash.png) and I get a screen freeze, crash, and sent to desktop with a driver timeout.   [These](https://send.vis.ee/download/103635cf66bdb907/#t2lRq409eeNwv6AaafhKJA) are both my crash report submissions. I'd go tomorrow over the stress tests, but I have managed to complete Time Spy/Steel Nomad without issues. And like I said, my system has has 0 issues before on a 2080ti.",AMD,2025-12-21 23:47:10,1
Intel,nvccr7w,"FYI, I passed [everything.](https://imgur.com/a/WyB9FeE)  This leaves the driver only. I made sure windows update didn't download its own driver this time, installed 25.12.1, still getting driver timeouts and crashes in games. I don't know what to tell you. Memtest86 also passed without any issues.",AMD,2025-12-22 09:07:04,1
Intel,o12dr67,that's... unexpected. Can you tell me what hardware this is with?,AMD,2026-01-22 15:57:19,1
Intel,o4f5q6m,"Hey there, I removed the other comment as I decided it wasn't relevant, I think you received the notification for it anyway, but that's fine.  I'm sort of worried to hear about all of this variability in AV quality between the driver packages, I haven't been aware of any significant changes to the multimedia stack, though the release notes we actually publish on the are unfortunately far from comprehensive, and I'm sure that team are busy.  If you have the time, I'd like to see if LatencyMon (the util recommended by my colleague) can highlight any differences between a good and bad driver on your system? I'm wondering if WU builds lack a specific SW component leading to issues (maybe something like the ANS service).",AMD,2026-02-09 12:11:49,1
Intel,ntb5lb9,"there won't be an ""fsr 5"" because any game with fsr implemented from here on out should, in theory, be compatible with every future version of fsr made, so numbering them isn't as meaningful. they're probably just going to stick with unofficial codenames like redstone for diffrentiation. Nvidia still uses versioning for DLSS despite it using the same system because it's good for marketing and diffrentiation so I'm not sure that dumping the version numbers is a wise decision but it also makes sense",AMD,2025-12-10 16:13:25,5
Intel,ntbuj2m,"I agree with you wholeheartedly, but super users do what they do best - sudo that shit. x)",AMD,2025-12-10 18:15:24,2
Intel,ntgknre,"I've never had AMD Chat or Privacy View force install, I hate they show up in the available software to install when updating, but I just dont click to install them lol, just update the gpu/chipset drivers",AMD,2025-12-11 13:06:05,1
Intel,ntpzp27,I guess a snippet of the faulting component from the output would work.  This is a minidump. Do you have a kernel memory dump>?,AMD,2025-12-12 22:25:23,2
Intel,ntqn26t,"sorry i missed this, seems it had expired. maybe someone downloaded it before i did?",AMD,2025-12-13 00:46:30,2
Intel,ntbdwdm,"Right now in devmgr with re-running the driver installer from the site things look like this https://u.numerlor.me/2faMBA . I also remembered adrenalin has full driver details and everything looks fine there https://u.numerlor.me/w1Snxw https://u.numerlor.me/EOclpA so I think it was just the installer being a bit confused.  Compared to the installer on the first screenshot, when doing the actual update (from inside adrenalin) the Radeon Graphics was a separate item, and had a ""Download driver"" or something along those with the link I mentioned",AMD,2025-12-10 16:54:02,4
Intel,ntbapq8,What about the combined exe? It's still available? That will install both gen but was bugged with control panel disappearing on previous driver.  The combined exe is around 1.6GB.,AMD,2025-12-10 16:38:26,3
Intel,ntc59vr,This might be it? Worth a shot I suppose.  Edit: This worked for me on the latest driver  [https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2025-12-10 19:06:45,4
Intel,nwvgoe4,"i donâ€™t have this issue in any of the same games, but i have no idea what could be causing it in your setup and not mine though",AMD,2025-12-31 06:30:39,1
Intel,o12hpew,"can you explain how one would reproduce this corruption who has never used second life and has no $ to spend in game? I am trying to reproduce the corruption you are describing but it seems that it has to do with in-game purchases or ""face layers"". can you explain how to apply these layers to the player?",AMD,2026-01-22 16:15:12,1
Intel,ntgiwq2,geenz@ but yes,AMD,2025-12-11 12:54:38,2
Intel,nvf2a9r,any news? SL are not updating their customers with anything constructive and it is affecting most of us.,AMD,2025-12-22 19:18:19,1
Intel,ntvblbb,"Hmm, when I can, Iâ€™ll have another look! Thanks!",AMD,2025-12-13 20:31:34,2
Intel,nvd9inn,"I see. Is this specific to CS2 or does it occur with other apps on your end?  We're presently tracking and working on TDRs in that game specifically, though I'm kind of worried in a way that clock limiting works around this failure.",AMD,2025-12-22 13:43:12,2
Intel,o131pj2,7900 xt!  I did a DDU and installed the newest driver too so I feel like it isn't carried over unless it was something from my settings ...,AMD,2026-01-22 17:45:13,1
Intel,o4hted2,"Ah gotcha. To be fair, I would say the differences between these more recent drivers are much smaller now compared to the drivers released in 2023, 2024, and 2025. Especially trying each driver from 25.9.1 to 25.12.1, specifically the audio, seems to get better with each release. It seems the team, or someone, also heard what I did with the 25.9.1 driver and tried to fix/improve it with each subsequent driver release.   I haven't tried 26.1.1 yet, but I will to test and compare. I'll also install LatencyMon to see if there is a correlation between good/bad audio and latency.  It really seems like the team that handles the display/video side of the drivers has mostly corrected the issues that were present with the old drivers (video output format/range, luminance levels, gamma curve, sharpness, white point, grayscale & color accuracy). There are still differences, but I wouldn't get too worried about it.  The oddball out of the bunch is 25.8.1. The audio is actually pretty good, but different. Sounds as if there is a surround sound effect turned on, or the audio output format is surround sound instead of stereo. It works really well for gaming, but music sounds a little strange. Due to the overall frequency response balance, it's honestly not bad. If it wasn't for the video/image quality side of things, I would probably use that driver.   On the video side, I think there is something wrong with the gamma curve and color. The gamma curve tracks higher (darker) in the lower luminance range. The white point and grayscale are more blue than neutral gray, and the overall color saturation is reduced, as if the color gamut clamp in the adrenaline software is being used. I checked to see if this was the case with my colorimeter, but the color gamut is normal for my monitor, so the actual color volume isn't being reduced. Very strange, but interesting.",AMD,2026-02-09 20:29:15,1
Intel,ntr6yhj,"I do not, only the minidump but I've uploaded it again here [https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN\_IxLREw](https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN_IxLREw)",AMD,2025-12-13 02:56:08,1
Intel,ntbd7hc,"Yes it should be fixed under that scenario, and the combined package is linked on the release notes:  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html  https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe  Kind of guessing here but I believe the '-c' towards the end of the file name denotes a combined package spanning RDNA support.",AMD,2025-12-10 16:50:38,7
Intel,nthusz8,This worked for me btw - did it a few days ago before these drivers dropped. When I update I'll be using the same method.,AMD,2025-12-11 17:10:36,3
Intel,nwwqpp5,"Are you running 4k in freesync on a 9000 series card?  I'm going by the radeon performance metric overlay saying minecraft/etc is using 300w power.  UE5 games are fine, games with an internal frame cap don't have an issue (well, they have their own frame pacing issues but that's not this).  I can always tell when framerate is going nuts because I can hear the squealing in my speakers when the gpu is at 100%. It's especially bad in menu's. If I turn off features/settings that improve quality or try a lower in game resolution, it gets much worse.",AMD,2025-12-31 13:15:38,1
Intel,o1w6e2g,just replying that I did find a regression point for the corruption in second life related to a change in the OGLP api :)   hopefully the change will be coming down the pipeline soon!   Id like to give a shout out to the user Eliza for being AFK where i first was able to reproduce the issue and i used their avatar to bug  check! o7,AMD,2026-01-26 21:07:09,2
Intel,nth5y4y,thanks a bunch. I'll pass this on to my ISV contact and see where we get with that.,AMD,2025-12-11 15:08:11,3
Intel,nvfir9a,You can find it here [https://github.com/secondlife/viewer/issues/5048](https://github.com/secondlife/viewer/issues/5048),AMD,2025-12-22 20:43:28,1
Intel,nub8ufp,news ?,AMD,2025-12-16 10:31:04,1
Intel,nve66vp,"COD is the greatest offender - I can't even get through the training course for Zombies without a black screen>driver timeout message, even if I try to speedrun it in a way (because I've attempted it so many times) it is inevitable it's going to crash, that one crashes with this [error](https://i.ibb.co/KjxynXH5/image.png).  Again, NO OC is applied. Other than the ram running at 2666, which as stated with both mem tests successful and went through both by Karhu's test and Memtest, have no issues. Including no issues with my previous GPU,2080ti, again. CS, I can't even start a match with friends because it'll inevitably crash randomly, sometimes it is within 5-10 mins, sometimes it is near instant in a couple of minutes. Tried everything from 25.12.2 to 25.9.1. PSU is a RM1000e, using the 12pin cable natively from the PSU. It is all the way in, this PSU I specifically even got for this GPU as I didn't want to use an adapter to power the card from all the experiences I've read with the 12pin + adapters.  Here is also a [video](https://www.youtube.com/watch?v=cSkaI6WSfJY) of it happening.",AMD,2025-12-22 16:38:34,1
Intel,o133wr0,"Okay, this is going to be tricky. I was under the impression this was completely eliminated, as we can no longer hit this internally.  Assuming that only your mouse input is blocked, I'll need your help capturing a usermode dmp of the RadeonSoftware.exe process via task manager.  This will involve setting some keys in windows registry. Are you comfortable with this?",AMD,2026-01-22 17:54:58,1
Intel,o4i8o89,I'd be curious as to how the minimal or 'driver only' install paths of several of your known good and bad drivers turn out? This should help eliminate additional components like ANS from affecting your experience (if they're even relevant in this context to begin with).,AMD,2026-02-09 21:44:39,1
Intel,ntvi492,"huh, that's odd. Do you have any larger files over at       C:\Windows\LiveKernelReports\WATCHDOG\",AMD,2025-12-13 21:08:32,2
Intel,ntbt0lr,"Installed the c one. And seems to be working fine. 780M and 6800 here. Still when selecting a specific GPU for a specific app, both energy saver and performance show 6800. This bug has been forever. And it's probably just a registry key when the driver install. Win11.",AMD,2025-12-10 18:08:05,4
Intel,o6e0rba,Any update on this? I posted here a few days ago   [https://github.com/secondlife/viewer/issues/5048#event-22751651175](https://github.com/secondlife/viewer/issues/5048#event-22751651175),AMD,2026-02-20 08:23:29,2
Intel,nwkq3wh,No updates there,AMD,2025-12-29 17:14:59,1
Intel,o6haxad,It's actively being looked into ðŸ¤˜ I found a regression point just a matter of it going though the process of integrating the fix ðŸ˜ðŸ‘Œ,AMD,2026-02-20 19:50:13,2
Intel,nonhqm9,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.Â   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",AMD,2025-11-13 16:12:15,83
Intel,nonf76t,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",AMD,2025-11-13 15:59:52,127
Intel,nonf5bq,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",AMD,2025-11-13 15:59:37,75
Intel,nooaz8h,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,AMD,2025-11-13 18:34:35,18
Intel,nonkrkq,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,AMD,2025-11-13 16:27:06,13
Intel,nonnn81,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too ðŸ˜¿.",AMD,2025-11-13 16:41:13,10
Intel,nonfuov,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,AMD,2025-11-13 16:03:01,16
Intel,noob2qb,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? Iâ€™ve spent 1 entire afternoon try every solutions given by Google but today the problem is still thereâ€¦,AMD,2025-11-13 18:35:02,7
Intel,nopg6ma,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",AMD,2025-11-13 21:59:54,7
Intel,nonp8tv,So does this mean Arc Raiders will stop randomly crashing in Windows?,AMD,2025-11-13 16:49:00,11
Intel,nonw7rh,Just installed these zero issues so far!,AMD,2025-11-13 17:23:17,6
Intel,nondz23,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",AMD,2025-11-13 15:53:59,23
Intel,nonlldq,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,AMD,2025-11-13 16:31:10,4
Intel,noofqtg,There was a long delay with the blank screen. Made me a bit nervous,AMD,2025-11-13 18:57:20,3
Intel,noo2zob,At this point i'm sure that cyberpunk will never be fixed.,AMD,2025-11-13 17:56:32,10
Intel,noolxx3,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,AMD,2025-11-13 19:27:59,7
Intel,nonj6l9,No fix for being unable to enable Noise Suppression...,AMD,2025-11-13 16:19:20,8
Intel,nooktgl,When does Linux get this,AMD,2025-11-13 19:22:26,3
Intel,nop2o04,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",AMD,2025-11-13 20:52:28,3
Intel,noqem0g,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",AMD,2025-11-14 01:15:45,3
Intel,noqnucr,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",AMD,2025-11-14 02:10:59,3
Intel,nou4y1d,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,AMD,2025-11-14 17:01:23,3
Intel,nouw9o1,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",AMD,2025-11-14 19:17:52,3
Intel,novj51b,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",AMD,2025-11-14 21:17:52,3
Intel,npexfdr,Windows update keeps trying to update my driver.,AMD,2025-11-18 00:54:41,3
Intel,noo4qjo,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,AMD,2025-11-13 18:05:01,4
Intel,noniqz3,No FSR4 on RDNA3 no care,AMD,2025-11-13 16:17:12,17
Intel,noo25hd,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",AMD,2025-11-13 17:52:27,6
Intel,nonthc8,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,AMD,2025-11-13 17:09:50,2
Intel,nooud97,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,AMD,2025-11-13 20:10:18,2
Intel,nortjvj,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,AMD,2025-11-14 07:15:55,2
Intel,nos3s8s,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",AMD,2025-11-14 08:57:09,2
Intel,not49x9,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",AMD,2025-11-14 13:52:51,2
Intel,noux5p5,This driver was way better than the version before it(for me at least).,AMD,2025-11-14 19:22:23,2
Intel,novpivg,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",AMD,2025-11-14 21:50:55,2
Intel,np4ombz,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",AMD,2025-11-16 11:19:30,2
Intel,npp1qov,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,AMD,2025-11-19 16:55:57,2
Intel,nqawzsb,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",AMD,2025-11-23 03:42:39,2
Intel,npaw51d,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",AMD,2025-11-17 11:47:34,4
Intel,nondc4t,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",AMD,2025-11-13 15:50:55,6
Intel,nonmrak,"Brooooo, they didnâ€˜t fix the flickering in BF6 when recordingâ€¦",AMD,2025-11-13 16:36:52,2
Intel,nonvub9,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.Â  * Fucking LOL.,AMD,2025-11-13 17:21:27,2
Intel,nonmi72,25.10.2 completely broke vsync... not even a mention about this in the notes?,AMD,2025-11-13 16:35:38,1
Intel,noncnxo,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-13 15:47:39,1
Intel,nonguv3,There is new AFMF features too.,AMD,2025-11-13 16:07:56,1
Intel,nonmglo,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,AMD,2025-11-13 16:35:26,1
Intel,nonn4xw,bf6 fps drop fixed?,AMD,2025-11-13 16:38:44,1
Intel,nonvhb6,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,AMD,2025-11-13 17:19:40,1
Intel,noo456j,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",AMD,2025-11-13 18:02:08,1
Intel,noo651n,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",AMD,2025-11-13 18:11:49,1
Intel,nooad23,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",AMD,2025-11-13 18:31:40,1
Intel,nooeeia,How is the driver ? 7700 XT here.,AMD,2025-11-13 18:50:53,1
Intel,noojnun,Finally a potential fix for CPU metrics? Look forward to seeing if itâ€™s true!,AMD,2025-11-13 19:16:39,1
Intel,noovdps,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,AMD,2025-11-13 20:15:26,1
Intel,noozjd6,do yall use ddu for every driver or do yall just update it with the app?,AMD,2025-11-13 20:36:35,1
Intel,noozq5o,"New AMD update ðŸ‘ðŸ‘ðŸ‘ðŸ‘, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",AMD,2025-11-13 20:37:32,1
Intel,nop06vu,I just canâ€™t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video Iâ€™m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,AMD,2025-11-13 20:39:55,1
Intel,nop4b7m,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",AMD,2025-11-13 21:00:46,1
Intel,nopfrqo,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,AMD,2025-11-13 21:57:50,1
Intel,nopilp6,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,AMD,2025-11-13 22:12:30,1
Intel,noplmto,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,AMD,2025-11-13 22:28:42,1
Intel,nopnmjz,Think this broke Vulkan in POE2,AMD,2025-11-13 22:39:24,1
Intel,noqjzdo,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",AMD,2025-11-14 01:48:12,1
Intel,nor6g8r,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,AMD,2025-11-14 04:06:42,1
Intel,nor7il2,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,AMD,2025-11-14 04:14:01,1
Intel,nor9p0f,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,AMD,2025-11-14 04:29:34,1
Intel,nord0sz,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,AMD,2025-11-14 04:54:17,1
Intel,norxf8j,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",AMD,2025-11-14 07:53:31,1
Intel,nos6z6k,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",AMD,2025-11-14 09:29:57,1
Intel,nos7i23,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:35:25,1
Intel,nos7vbg,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",AMD,2025-11-14 09:39:11,1
Intel,nosa7uh,subtract strong cats brave outgoing husky coordinated important rustic juggle   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-11-14 10:02:49,1
Intel,nosrlfs,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,AMD,2025-11-14 12:33:14,1
Intel,nosysjm,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",AMD,2025-11-14 13:20:27,1
Intel,not149u,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",AMD,2025-11-14 13:34:35,1
Intel,notb7lg,I'm glad the CPU metrics are showing again,AMD,2025-11-14 14:31:49,1
Intel,notcd57,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",AMD,2025-11-14 14:38:07,1
Intel,notlcun,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",AMD,2025-11-14 15:24:40,1
Intel,notm5ep,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),AMD,2025-11-14 15:28:36,1
Intel,notufou,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,AMD,2025-11-14 16:09:14,1
Intel,noue3ki,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",AMD,2025-11-14 17:47:54,1
Intel,nouooi9,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",AMD,2025-11-14 18:39:53,1
Intel,noutw0a,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 ðŸ˜…   What a fucking joke",AMD,2025-11-14 19:05:43,1
Intel,nov8foi,Shits been crashing my system since the update :( sapphire 7900xt,AMD,2025-11-14 20:21:15,1
Intel,novg42t,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",AMD,2025-11-14 21:02:02,1
Intel,nowdvrw,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,AMD,2025-11-15 00:14:02,1
Intel,noydj17,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",AMD,2025-11-15 09:37:48,1
Intel,noypu29,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",AMD,2025-11-15 11:42:58,1
Intel,noyv323,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",AMD,2025-11-15 12:27:57,1
Intel,nozb3zp,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",AMD,2025-11-15 14:16:00,1
Intel,noze8xv,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,AMD,2025-11-15 14:34:56,1
Intel,nozoxq5,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",AMD,2025-11-15 15:35:11,1
Intel,nozv077,Ð£ Ð¼ÐµÐ½Ñ ÐœÑƒÐ»ÑŒÑ‚Ð¸Ð¼ÐµÐ´Ð¸Ð° ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð»ÐµÑ€ Ð²Ñ‹Ð´Ð°ÐµÑ‚ Ð¾ÑˆÐ¸Ð±ÐºÑƒ. Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð° Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ðµ Ð´Ñ€Ð°Ð¹Ð²ÐµÑ€Ñ‹. (ÐšÐ¾Ð´ 28),AMD,2025-11-15 16:06:54,1
Intel,np0n0ro,Noise Suppression still broken. 3rd release without that functionality in a row.,AMD,2025-11-15 18:33:00,1
Intel,np0qihb,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalaciÃ³n del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",AMD,2025-11-15 18:50:19,1
Intel,np0sz88,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",AMD,2025-11-15 19:02:53,1
Intel,np2gy28,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",AMD,2025-11-16 00:44:12,1
Intel,np2igku,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,AMD,2025-11-16 00:53:10,1
Intel,np2iy25,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",AMD,2025-11-16 00:56:07,1
Intel,np2n7ns,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",AMD,2025-11-16 01:22:23,1
Intel,np2rc23,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,AMD,2025-11-16 01:46:41,1
Intel,np3zqgd,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me ðŸ™",AMD,2025-11-16 07:09:06,1
Intel,np4btup,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",AMD,2025-11-16 09:09:18,1
Intel,np4c4bj,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",AMD,2025-11-16 09:12:17,1
Intel,np59xsp,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,AMD,2025-11-16 14:05:46,1
Intel,np5tc80,Still not working AMD NOISE S,AMD,2025-11-16 15:57:27,1
Intel,np5w51d,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,AMD,2025-11-16 16:12:01,1
Intel,np6sb4d,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",AMD,2025-11-16 18:55:53,1
Intel,np75mw5,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,AMD,2025-11-16 20:02:08,1
Intel,np7fiy7,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,AMD,2025-11-16 20:52:53,1
Intel,np9tmrb,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",AMD,2025-11-17 05:29:10,1
Intel,npa497n,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",AMD,2025-11-17 07:05:30,1
Intel,npbc7th,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",AMD,2025-11-17 13:41:36,1
Intel,npbdww5,"Unfortunately, version 25.11.1 does not start with Windows.",AMD,2025-11-17 13:51:35,1
Intel,npcr8ua,Is AMD going to come up with another driver soon?,AMD,2025-11-17 18:04:51,1
Intel,npd465l,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",AMD,2025-11-17 19:07:02,1
Intel,npeqls5,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,AMD,2025-11-18 00:15:16,1
Intel,npgfe4k,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly ðŸ˜ž.",AMD,2025-11-18 07:12:00,1
Intel,npgq8pe,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",AMD,2025-11-18 09:03:29,1
Intel,npgujea,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",AMD,2025-11-18 09:49:30,1
Intel,nph1gio,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,AMD,2025-11-18 10:58:38,1
Intel,nphl085,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,AMD,2025-11-18 13:22:24,1
Intel,npikkr4,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",AMD,2025-11-18 16:26:09,1
Intel,npnxcnt,getting bsod randomly since 25.9.1 sad..,AMD,2025-11-19 13:20:00,1
Intel,npowfg1,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",AMD,2025-11-19 16:29:43,1
Intel,npwkypv,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",AMD,2025-11-20 20:27:52,1
Intel,nq842b2,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",AMD,2025-11-22 17:50:50,1
Intel,nq9u9z3,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",AMD,2025-11-22 23:33:52,1
Intel,nqwbryc,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,AMD,2025-11-26 15:56:06,1
Intel,ns8k1w2,"Sorry but for me the drive give me crash pop up message every time i boot up my pc. Also just right now i got a freeze, black screen to all my monitors.",AMD,2025-12-04 12:42:18,1
Intel,ns9soky,The worst driver this year so far,AMD,2025-12-04 16:45:18,1
Intel,nscxupo,"Still havent fixed the noise cancellation lmao, guess its another month+ of old version :) Thanks amd, truly doing wonders.",AMD,2025-12-05 02:44:53,1
Intel,nsgsekn,CS2 crashing with driver timeout after tabbing out or watching streams on 2nd screen 7900xtx,AMD,2025-12-05 18:29:05,1
Intel,nsqr7j8,"When is 25.12.1 coming out? I have read only bad things about 25.11.1 here, so I wanted to skip this one.",AMD,2025-12-07 10:40:19,1
Intel,nonf78x,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",AMD,2025-11-13 15:59:52,1
Intel,nond6d4,So no redstone yet,AMD,2025-11-13 15:50:09,1
Intel,nonqjy0,FSR AI frame gen??? Didnâ€™t they say thatâ€™d it would also have a driver toggle?,AMD,2025-11-13 16:55:25,1
Intel,nonv0vm,Did AMD ever add support for Cronos?,AMD,2025-11-13 17:17:25,1
Intel,nonxx39,Well Star Citizen will load now!  Now some longer term testing....,AMD,2025-11-13 17:31:41,1
Intel,nonw8zf,Anybody tried this with Anno 117 yet? Iâ€™m hoping it helps performance,AMD,2025-11-13 17:23:27,0
Intel,nooyqhv,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,AMD,2025-11-13 20:32:28,0
Intel,nooyuwp,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,AMD,2025-11-13 20:33:06,0
Intel,noqrxh3,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,AMD,2025-11-14 02:35:08,0
Intel,nozwu6t,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,AMD,2025-11-15 16:16:36,0
Intel,np07ekg,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,AMD,2025-11-15 17:12:12,0
Intel,noni2qa,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,AMD,2025-11-13 16:13:55,-4
Intel,noqc54j,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,AMD,2025-11-14 01:00:45,-1
Intel,np4dff7,"Here we go again, jetzt stÃ¼rzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen auÃŸer XMP war aktiviert, dann stÃ¼rzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das Ã¼bernehmen mÃ¼sst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team GrÃ¼n nicht.",AMD,2025-11-16 09:25:48,-1
Intel,nonpv4u,Yeah same here LG c5 42inch ðŸ˜°,AMD,2025-11-13 16:52:03,21
Intel,noockre,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesnâ€™t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",AMD,2025-11-13 18:42:09,17
Intel,noprnhq,"I have this but on display port, HDMI works fine",AMD,2025-11-13 23:01:25,6
Intel,nonyety,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",AMD,2025-11-13 17:34:06,10
Intel,nopqt8d,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",AMD,2025-11-13 22:56:47,2
Intel,nonu691,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,AMD,2025-11-13 17:13:13,2
Intel,nop2vm5,I have the same issue with display port but itâ€™s okay with hdmi :/,AMD,2025-11-13 20:53:31,1
Intel,nq0dwdl,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",AMD,2025-11-21 12:22:58,1
Intel,nonpu8n,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",AMD,2025-11-13 16:51:56,81
Intel,nonjytd,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,AMD,2025-11-13 16:23:11,26
Intel,noo9nj4,"V25.10.2  hereâ€¦ I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",AMD,2025-11-13 18:28:20,4
Intel,not85q8,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",AMD,2025-11-14 14:14:59,5
Intel,nonkdfa,combined again it looks like ðŸ¤·â€â™‚ï¸,AMD,2025-11-13 16:25:10,2
Intel,nongchq,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,AMD,2025-11-13 16:05:27,103
Intel,nono7wt,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",AMD,2025-11-13 16:44:00,3
Intel,nonhdck,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",AMD,2025-11-13 16:10:27,6
Intel,np0qz7g,You try install last chipset driver ?,AMD,2025-11-15 18:52:41,1
Intel,nongxu7,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",AMD,2025-11-13 16:08:21,1
Intel,nonrxcq,So it's the driver that's why that happens ðŸ˜¡ and it's not fixed?,AMD,2025-11-13 17:02:09,0
Intel,noogyei,Thank you for your service,AMD,2025-11-13 19:03:14,7
Intel,nopxjjg,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",AMD,2025-11-13 23:35:26,8
Intel,nov7gjn,Any update mate?,AMD,2025-11-14 20:16:03,1
Intel,nosfu5h,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",AMD,2025-11-14 10:57:06,0
Intel,nonw38z,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",AMD,2025-11-13 17:22:40,7
Intel,npdh2mf,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",AMD,2025-11-17 20:11:40,3
Intel,np42etk,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",AMD,2025-11-16 07:35:21,2
Intel,noroh5d,"I'm the opposite, IÂ just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",AMD,2025-11-14 06:29:26,1
Intel,nonifp9,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDUâ€™d it again to go back to 25.9.2 since games were stuttering.",AMD,2025-11-13 16:15:40,13
Intel,noozgtx,Same.,AMD,2025-11-13 20:36:13,3
Intel,nop6flo,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",AMD,2025-11-13 21:11:26,3
Intel,noxqsoq,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",AMD,2025-11-15 05:51:40,1
Intel,nosbqvm,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,AMD,2025-11-14 10:17:54,1
Intel,nozhfiv,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,AMD,2025-11-15 14:53:42,1
Intel,nopl6z7,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",AMD,2025-11-13 22:26:21,1
Intel,noppntf,If it still crashes set RTX Global Illumination to Static.,AMD,2025-11-13 22:50:26,4
Intel,nor7jw2,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",AMD,2025-11-14 04:14:16,2
Intel,nonlw78,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",AMD,2025-11-13 16:32:40,11
Intel,notyc45,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as theyâ€™ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since theyâ€™re much deeper in engine code/inputs.",AMD,2025-11-14 16:28:32,2
Intel,noocnzc,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it ðŸ¤“",AMD,2025-11-13 18:42:35,1
Intel,noo416z,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",AMD,2025-11-13 18:01:35,1
Intel,not2qjr,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",AMD,2025-11-14 13:44:02,2
Intel,nopbmoh,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,AMD,2025-11-13 21:37:14,2
Intel,nosw536,Ugh,AMD,2025-11-14 13:03:59,2
Intel,nooumki,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",AMD,2025-11-13 20:11:38,3
Intel,noqgvkg,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",AMD,2025-11-14 01:29:31,1
Intel,nos3g9h,"Linux doesnâ€™t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, thatâ€™s when you get driver updates, and theyâ€™re completely different from windows branch.",AMD,2025-11-14 08:53:47,1
Intel,noqn7n2,just uninstall it I prefer manual check myself.,AMD,2025-11-14 02:07:15,2
Intel,nor7u07,So AMDs default driver overclocks and doesnâ€™t reflect that in the values?,AMD,2025-11-14 04:16:14,1
Intel,nqsncxf,Same issues here i underclocked it but this new update just made it worse,AMD,2025-11-26 00:03:57,1
Intel,np5tu2z,ok it is still crashing ... complete reboot :(,AMD,2025-11-16 16:00:03,1
Intel,nq4e73q,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",AMD,2025-11-22 01:28:20,1
Intel,nope0rx,Okay.,AMD,2025-11-13 21:49:03,1
Intel,nonl1up,Iâ€™m hoping Valveâ€™s new steam machine will push them on that since itâ€™s RDNA3 based.,AMD,2025-11-13 16:28:30,17
Intel,noukbhw,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,AMD,2025-11-14 18:18:29,3
Intel,nooggfu,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",AMD,2025-11-13 19:00:46,4
Intel,noozx8g,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,AMD,2025-11-13 20:38:33,1
Intel,noptibm,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",AMD,2025-11-13 23:12:01,1
Intel,noo53y9,welcome to amd,AMD,2025-11-13 18:06:50,0
Intel,nonvvaf,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,AMD,2025-11-13 17:21:35,1
Intel,nooca2m,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",AMD,2025-11-13 18:40:45,1
Intel,np1vdc1,Same. Never even had Ryzen master installed.,AMD,2025-11-15 22:35:21,2
Intel,npiam42,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",AMD,2025-11-18 15:37:33,1
Intel,nowsbia,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,AMD,2025-11-15 01:45:44,1
Intel,nonegtb,What is redstone?,AMD,2025-11-13 15:56:21,7
Intel,nonnq47,What's weird is Black Ops 7 has ray regeneration.,AMD,2025-11-13 16:41:37,6
Intel,none418,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",AMD,2025-11-13 15:54:38,2
Intel,nontlx6,vsync issue fixed with win 11 KB5068861 update.,AMD,2025-11-13 17:10:27,12
Intel,nonxa48,had no issues with vsync on 25.10.2,AMD,2025-11-13 17:28:33,4
Intel,nons4sz,works fine for me,AMD,2025-11-13 17:03:10,6
Intel,noorn1m,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,AMD,2025-11-13 19:56:34,1
Intel,nopcb8w,"That it did, lol. My only complaint.",AMD,2025-11-13 21:40:35,1
Intel,noqh6ym,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",AMD,2025-11-14 01:31:26,0
Intel,nonl36f,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,AMD,2025-11-13 16:28:41,3
Intel,noovbth,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,AMD,2025-11-13 20:15:10,2
Intel,nonpyiq,"Fps drop over time? That's a game issue, it's got a memory leak",AMD,2025-11-13 16:52:31,5
Intel,nopz2ou,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",AMD,2025-11-13 23:44:30,1
Intel,nov7ewn,Iâ€™d settle for bf6 going one entire game without drivers crashing the game and freezing pc,AMD,2025-11-14 20:15:48,1
Intel,noorxgl,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",AMD,2025-11-13 19:58:00,1
Intel,nov7k59,Crashes?,AMD,2025-11-14 20:16:34,1
Intel,nowyxe0,I have this problem in all games.,AMD,2025-11-15 02:28:26,1
Intel,nprco16,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",AMD,2025-11-20 00:04:27,1
Intel,not2cbd,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",AMD,2025-11-14 13:41:44,1
Intel,np3fwq0,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",AMD,2025-11-16 04:25:12,1
Intel,norugaj,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",AMD,2025-11-14 07:24:32,3
Intel,nopmfkv,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,AMD,2025-11-13 22:33:01,2
Intel,noscuea,Epic version runs just fine.,AMD,2025-11-14 10:28:45,3
Intel,not9drm,Cyberpunk GOG last version patch runs fine on this driver.,AMD,2025-11-14 14:21:42,1
Intel,nosnl0p,"Hey there, can you give an example of how this looks now versus how it's supposed to?",AMD,2025-11-14 12:03:38,2
Intel,noso7o5,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,AMD,2025-11-14 12:08:26,1
Intel,nou0ebb,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,AMD,2025-11-14 16:38:48,1
Intel,nox9yy0,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,AMD,2025-11-15 03:41:47,2
Intel,nou7nae,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,AMD,2025-11-14 17:15:04,2
Intel,noypui8,"The game is booting, this message was for the 25.10 they just didn't removed it",AMD,2025-11-15 11:43:04,2
Intel,noza0c5,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,AMD,2025-11-15 14:09:14,1
Intel,np31dy5,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,AMD,2025-11-16 02:48:35,1
Intel,npgrqyr,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",AMD,2025-11-18 09:19:37,1
Intel,nqit1yt,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",AMD,2025-11-24 12:56:50,2
Intel,nsv6cts,"Good call, it caused nothing but problems for me and pretty severe. Were talking driver timeouts with black screens and even a couple bluescreens.",AMD,2025-12-08 01:50:11,1
Intel,nonny1j,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",AMD,2025-11-13 16:42:40,5
Intel,nopggve,My 9070 xt crushes while I try to use fsr 4 on new drivers,AMD,2025-11-13 22:01:20,1
Intel,noo04cb,Why don't you try it and let us know if you can. Would be helpful for lots of us,AMD,2025-11-13 17:42:31,1
Intel,nont8g8,It's in Redstone. Still not out yet,AMD,2025-11-13 17:08:37,5
Intel,nopd6c2,Didn't work for me...,AMD,2025-11-13 21:44:51,1
Intel,not23h8,Wait until you see how much your browser's cache is churning...,AMD,2025-11-14 13:40:17,2
Intel,notlyfp,Why cant you use Adrenalin? I'm using it on 25.9.1,AMD,2025-11-14 15:27:40,1
Intel,nq0kohy,I just received a windows extension update for my LG monitor. If you can boot up go check.,AMD,2025-11-21 13:08:34,1
Intel,nopw101,The last time I had this problem it was a RAM issue.,AMD,2025-11-13 23:26:32,3
Intel,npd560g,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,AMD,2025-11-17 19:11:57,1
Intel,norotfv,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",AMD,2025-11-14 06:32:31,1
Intel,nood411,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,AMD,2025-11-13 18:44:41,11
Intel,noo4uio,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",AMD,2025-11-13 18:05:33,4
Intel,nooyzy7,Do u reintall already up to date chipset drivers?,AMD,2025-11-13 20:33:49,1
Intel,norplxi,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,AMD,2025-11-14 06:39:36,1
Intel,nonuzmx,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",AMD,2025-11-13 17:17:15,3
Intel,nonxvx2,doing so (separation) will create a freak out shitstorm part 2.,AMD,2025-11-13 17:31:31,12
Intel,nonzgmu,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,AMD,2025-11-13 17:39:17,16
Intel,nonz6zk,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),AMD,2025-11-13 17:37:57,11
Intel,nons9ct,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,AMD,2025-11-13 17:03:49,18
Intel,nooofaj,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",AMD,2025-11-13 19:40:29,2
Intel,nony71m,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,AMD,2025-11-13 17:33:02,17
Intel,nonscqs,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,AMD,2025-11-13 17:04:17,1
Intel,nonuj7z,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,AMD,2025-11-13 17:15:00,22
Intel,nonzcwc,Thank you for communicating,AMD,2025-11-13 17:38:47,10
Intel,nononki,Unfortunately happens to me too. So for me itâ€™s a big issue as I canâ€™t update to this driver until it is fixed ðŸ˜°,AMD,2025-11-13 16:46:06,5
Intel,nooyj1v,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.Â  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.Â  Issue goes away using a non 4k 240hz display.Â Â    I believe this system crash is deeply related to DSC on Windows.Â  I only got these two PC bsods when I bought a 4k 240hz display.Â  Returned a monitor (bad oled) and the issue went away.Â  Got a new oled a few weeks ago and now I have these bsods again.Â Â    Never had a bsod before I got these 4k 240hz displays.Â  Fresh Windows 11 installs too between both PCs and between my first and second oled.Â  Systems are both solid and stable.Â Â    Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20Â  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.Â  Hopefully someone else had experience with them on 4k 240hz.",AMD,2025-11-13 20:31:24,5
Intel,nons1mi,Thank you AMD my bad for getting upset,AMD,2025-11-13 17:02:44,4
Intel,nongngq,Thank you.,AMD,2025-11-13 16:06:56,2
Intel,noobm6s,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why itâ€™s failing. Would be cool to see the technical details if thatâ€™s possible. (Iâ€™m actually more interested now on why itâ€™s not working vs just getting it fixed).,AMD,2025-11-13 18:37:36,2
Intel,nop1khf,Thank you!,AMD,2025-11-13 20:46:53,2
Intel,nop0wol,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,AMD,2025-11-13 20:43:34,1
Intel,nonlavb,Redstone when?,AMD,2025-11-13 16:29:44,0
Intel,nonhqde,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",AMD,2025-11-13 16:12:13,16
Intel,nonlm2a,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,AMD,2025-11-13 16:31:16,10
Intel,nonz8d7,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",AMD,2025-11-13 17:38:09,6
Intel,nonjrum,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",AMD,2025-11-13 16:22:14,4
Intel,nonovmq,Non pc monitor tvs are sometimes cheaper especially for larger sizes. Iâ€™m on lg c5 oled 42inch and it only has hdmiâ€¦,AMD,2025-11-13 16:47:11,4
Intel,noo0hf6,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",AMD,2025-11-13 17:44:18,3
Intel,nonq4py,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",AMD,2025-11-13 16:53:21,2
Intel,noolj45,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",AMD,2025-11-13 19:25:57,1
Intel,noqqvuj,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",AMD,2025-11-14 02:28:55,1
Intel,nosnlnp,> Are y'all playing on televisions?  Do you guys not have phones?,AMD,2025-11-14 12:03:46,0
Intel,nonhcwn,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,AMD,2025-11-13 16:10:23,2
Intel,noqf5pn,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,AMD,2025-11-14 01:19:04,6
Intel,noqno3l,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",AMD,2025-11-14 02:09:58,1
Intel,nov6ye9,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",AMD,2025-11-14 20:13:22,1
Intel,noyds7c,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",AMD,2025-11-15 09:40:31,1
Intel,nsoev4p,Why does it seem like driver quality/support has gotten substantially worse this past decade? Are we running out of skilled software engineers or is hardware just getting too out of hand?,AMD,2025-12-06 23:58:22,1
Intel,noshsep,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,AMD,2025-11-14 11:15:06,3
Intel,nonx5ls,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",AMD,2025-11-13 17:27:56,1
Intel,np52n5a,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",AMD,2025-11-16 13:17:38,3
Intel,np1d4kt,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",AMD,2025-11-15 20:53:05,2
Intel,nonis5q,OK thought I was the only one. 25.10 is bad bad,AMD,2025-11-13 16:17:22,4
Intel,nood354,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",AMD,2025-11-13 18:44:34,1
Intel,nos072w,Thanks for testing it,AMD,2025-11-14 08:20:41,1
Intel,np22kzb,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",AMD,2025-11-15 23:18:12,1
Intel,nosh56b,I thought FSR 4 was only on RDNA 4? ðŸ¤”,AMD,2025-11-14 11:09:11,1
Intel,nozuikm,My thoughts exactly. Thanks.,AMD,2025-11-15 16:04:18,1
Intel,nopzlun,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,AMD,2025-11-13 23:47:39,2
Intel,noq7kwh,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,AMD,2025-11-14 00:33:58,2
Intel,not1lyv,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,AMD,2025-11-14 13:37:28,1
Intel,noojjne,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",AMD,2025-11-13 19:16:05,1
Intel,nooj67g,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",AMD,2025-11-13 19:14:14,2
Intel,nopbvvl,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,AMD,2025-11-13 21:38:30,6
Intel,nouxgnr,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,AMD,2025-11-14 19:23:57,1
Intel,norm1yc,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",AMD,2025-11-14 06:08:00,3
Intel,npawrxf,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",AMD,2025-11-17 11:52:53,1
Intel,noo4anu,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,AMD,2025-11-13 18:02:52,8
Intel,nosbylr,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",AMD,2025-11-14 10:20:00,0
Intel,nphuo0h,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game ðŸ‘€?,AMD,2025-11-18 14:14:35,1
Intel,noojmmw,Fun fact - i am dual booting and on Linux this bug is not existent...:)),AMD,2025-11-13 19:16:29,2
Intel,noockb2,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",AMD,2025-11-13 18:42:05,2
Intel,nqw6fd5,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,AMD,2025-11-26 15:29:31,1
Intel,noznq2r,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",AMD,2025-11-15 15:28:41,1
Intel,nongh5i,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,AMD,2025-11-13 16:06:05,7
Intel,noozl1i,It's a thing you can search for on Google,AMD,2025-11-13 20:36:49,1
Intel,nonsdkw,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,AMD,2025-11-13 17:04:24,3
Intel,nonm0k8,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",AMD,2025-11-13 16:33:16,2
Intel,nonxcza,ahh i'm on Win 10 so probably why I didn't see it.,AMD,2025-11-13 17:28:57,2
Intel,noorjtx,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,AMD,2025-11-13 19:56:07,2
Intel,nonn11b,"Yes, but was it in the previous WHQL driver ? I'm not sure.",AMD,2025-11-13 16:38:12,1
Intel,nons5g1,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),AMD,2025-11-13 17:03:16,2
Intel,nos7pyk,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:37:40,1
Intel,nqrxf23,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",AMD,2025-11-25 21:39:49,1
Intel,nov8osy,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",AMD,2025-11-14 20:22:35,1
Intel,np3gi86,Either launch with curseforge or rollback,AMD,2025-11-16 04:29:38,1
Intel,nopmt4r,"Damn, didnâ€™t work for me last driver either. I can get FSR4 to work in other games just not BF6",AMD,2025-11-13 22:35:02,1
Intel,nr5w3fb,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,AMD,2025-11-28 03:55:10,1
Intel,noxy5g3,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",AMD,2025-11-15 07:00:01,1
Intel,noz0zh9,You 100 procent sure on this?,AMD,2025-11-15 13:12:06,1
Intel,np59d5s,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,AMD,2025-11-16 14:02:08,1
Intel,np32vom,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",AMD,2025-11-16 02:57:54,2
Intel,npgs1he,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic cardðŸ¤¡,AMD,2025-11-18 09:22:45,1
Intel,nswxbvi,"I had randomly black screens with 24.2.1, this was annoying as hell. Had to DDU the Driver and went back to 23.11.1, after this everything was fine.",AMD,2025-12-08 10:15:23,1
Intel,nonplo5,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,AMD,2025-11-13 16:50:46,1
Intel,noolo6b,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",AMD,2025-11-13 19:26:39,2
Intel,nov5qbd,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,AMD,2025-11-14 20:06:52,1
Intel,nq588dp,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,AMD,2025-11-22 04:54:10,1
Intel,noo85c3,They do not.,AMD,2025-11-13 18:21:19,5
Intel,np5srze,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",AMD,2025-11-16 15:54:29,1
Intel,noo8tps,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,AMD,2025-11-13 18:24:30,11
Intel,noo2nnu,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,AMD,2025-11-13 17:54:54,9
Intel,noolzkz,"AND is taking away one additional driver feature per day, you say?",AMD,2025-11-13 19:28:13,1
Intel,nooy45h,"Yes, Iâ€™m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select â€œGPUâ€ you get a file that has a different dimension from the one you download if you choose â€œCPUâ€. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with â€œminimal_install), but Adrenalin App does not open.",AMD,2025-11-13 20:29:17,5
Intel,noo1i55,Thank you for explaining it before the rage baiters go nuts.,AMD,2025-11-13 17:49:17,0
Intel,noo3cx3,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,AMD,2025-11-13 17:58:18,45
Intel,nooncln,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",AMD,2025-11-13 19:35:04,3
Intel,noo0xcy,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",AMD,2025-11-13 17:46:27,14
Intel,nopu61n,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",AMD,2025-11-13 23:15:46,6
Intel,nopc45s,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",AMD,2025-11-13 21:39:38,2
Intel,noo3fsu,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,AMD,2025-11-13 17:58:41,26
Intel,npp1edb,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,AMD,2025-11-19 16:54:16,2
Intel,nopvrx5,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,AMD,2025-11-13 23:25:04,3
Intel,noo53xx,Already launched in COD 7,AMD,2025-11-13 18:06:50,3
Intel,nonp7d7,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so youâ€™re saying i shoulf switch to hdmi?",AMD,2025-11-13 16:48:48,3
Intel,nonwqs3,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,AMD,2025-11-13 17:25:53,1
Intel,norbib0,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,AMD,2025-11-14 04:42:59,1
Intel,noni0s3,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",AMD,2025-11-13 16:13:39,1
Intel,nor1k1x,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,AMD,2025-11-14 03:34:28,1
Intel,nonzc4h,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",AMD,2025-11-13 17:38:40,2
Intel,noshb1m,With the compiled leaked DLL you can use it on RDNA3 as well.,AMD,2025-11-14 11:10:42,1
Intel,nosbtoj,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",AMD,2025-11-14 10:18:39,2
Intel,notnotg,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,AMD,2025-11-14 15:36:20,1
Intel,noqg8tt,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),AMD,2025-11-14 01:25:43,5
Intel,noonewp,Thank you! Exciting keen to see what itâ€™s like,AMD,2025-11-13 19:35:23,1
Intel,noosgem,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",AMD,2025-11-13 20:00:38,2
Intel,nopjngc,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,AMD,2025-11-13 22:18:03,2
Intel,nphu5po,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",AMD,2025-11-18 14:11:53,1
Intel,npb27so,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,AMD,2025-11-17 12:35:58,1
Intel,nopnm90,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,AMD,2025-11-13 22:39:21,1
Intel,nqw82e8,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",AMD,2025-11-26 15:37:49,2
Intel,nozpqvb,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",AMD,2025-11-15 15:39:28,1
Intel,nonmi38,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",AMD,2025-11-13 16:35:37,4
Intel,nony81v,lmao chill out dude go touch some grass,AMD,2025-11-13 17:33:11,5
Intel,nonozwd,Could be grounds for lawsuitâ€¦ Thatâ€™s funny!,AMD,2025-11-13 16:47:47,4
Intel,norvwn6,Because of MPO.,AMD,2025-11-14 07:38:45,5
Intel,noq77oq,yeah same with 25.11.1 25.9.2 works for me,AMD,2025-11-14 00:31:52,1
Intel,nonv3ns,"25.10.2 was the previous WHQL, so also yes :P",AMD,2025-11-13 17:17:48,2
Intel,noo30h0,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",AMD,2025-11-13 17:56:38,1
Intel,noxixmd,Which driver version and does it still crashing?,AMD,2025-11-15 04:47:44,1
Intel,nopn1gw,OK I will install it now and test it and get back to you. Give me 10 mins.,AMD,2025-11-13 22:36:16,2
Intel,noppoge,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,AMD,2025-11-13 22:50:32,2
Intel,nr7hjjv,I'll work with the engineer from that ticket check if that issue has somehow regressed.,AMD,2025-11-28 12:30:29,3
Intel,nrptkfo,We've not been able to reproduce this internally so far. Can you remind me which GPU (was this a 7900XTX?) + connectivity method you're using?,AMD,2025-12-01 14:48:30,2
Intel,noztt6w,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,AMD,2025-11-15 16:00:37,2
Intel,np0adh6,"Yup just need to say ""No""",AMD,2025-11-15 17:28:00,2
Intel,npgto9y,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",AMD,2025-11-18 09:40:23,1
Intel,nonps7q,I don't see how it would work on 23.9.1 lol,AMD,2025-11-13 16:51:40,-1
Intel,nov73co,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",AMD,2025-11-14 20:14:06,1
Intel,noprr9f,I did it this morning before the new driver and confirm chipset drivers were untouched,AMD,2025-11-13 23:02:01,3
Intel,noooxx5,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and donâ€™t use the latest drivers. At least AMD owned up to it so I canâ€™t be too upset but hopefully they really do fix this soon as new users may not understand whatâ€™s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly itâ€™s stable for them and they donâ€™t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs donâ€™t always have a DP connector at all.,AMD,2025-11-13 19:43:06,5
Intel,noo4q8p,"ah, that explains it. Thanks. :)",AMD,2025-11-13 18:04:59,1
Intel,nooab1c,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named â€œminimal installâ€). Obviously Iâ€™m referring to AMD driver download page.",AMD,2025-11-13 18:31:24,1
Intel,nop73kl,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",AMD,2025-11-13 21:14:50,1
Intel,novl7li,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",AMD,2025-11-14 21:28:30,1
Intel,noo4i0q,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",AMD,2025-11-13 18:03:52,21
Intel,noo7r27,What about Noise Suppression not working since 25.9.2?,AMD,2025-11-13 18:19:27,7
Intel,np8f5i6,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,AMD,2025-11-17 00:06:17,2
Intel,nopub91,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",AMD,2025-11-13 23:16:35,1
Intel,noo3ufw,Hell yeah ðŸ™‚ amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,AMD,2025-11-13 18:00:40,7
Intel,norjsvf,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",AMD,2025-11-14 05:48:36,2
Intel,nonrmjz,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",AMD,2025-11-13 17:00:38,9
Intel,noo81ru,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,AMD,2025-11-13 18:20:51,4
Intel,nosmcf6,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",AMD,2025-11-14 11:53:57,3
Intel,noniq65,That was my very first actual driver issue I experienced with AMD.,AMD,2025-11-13 16:17:06,3
Intel,noshl11,Oh that's nice! I'll look into it when I get the chance.,AMD,2025-11-14 11:13:15,1
Intel,nosh6j0,Cool. Thank you,AMD,2025-11-14 11:09:32,1
Intel,notszvs,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience ðŸ˜–,AMD,2025-11-14 16:02:13,1
Intel,not1h8l,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,AMD,2025-11-14 13:36:42,2
Intel,npb8iqb,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",AMD,2025-11-17 13:18:49,1
Intel,npbldor,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",AMD,2025-11-17 14:34:50,1
Intel,noprwpb,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,AMD,2025-11-13 23:02:52,2
Intel,nonmz2f,"Fair enough, and yeah sooner the better for all of us",AMD,2025-11-13 16:37:55,0
Intel,noofit0,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,AMD,2025-11-13 18:56:16,-1
Intel,nozg3tu,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",AMD,2025-11-15 14:45:59,1
Intel,nopq3va,Fingers crossed,AMD,2025-11-13 22:52:53,1
Intel,nsxlbh0,"Thanks for attempting to retest.  It's a 7900XTX with an Index connected via DisplayPort. I am on the latest 25.11.1 driver.  I run a monitor at 4k 120Hz 10bpc with HDR Off, which uses DSC, as my main and only display. I tried disabling DSC in the monitor settings which runs at 4k 120Hz 8bpc with HDR Off but I don't think I noticed a change in latency. I thought that DSC on and off on two different devices might contribute to the problem but I'm not sure.   I have also tried running the Index under a RX480 on another PC and I fairly certain the latency looks different under 90Hz and looks similar under 120Hz. Can't play much to test though as an RX480 runs the Index at a very blurry setting. Getting around to doing this test is what took me so long to reply.",AMD,2025-12-08 13:31:32,2
Intel,ntkkg7z,Were you able to find the issue?,AMD,2025-12-12 01:49:15,1
Intel,np0lx2u,"Allright ty, will Install new, any differences in performance?",AMD,2025-11-15 18:27:26,1
Intel,nphlmf1,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",AMD,2025-11-18 13:25:51,1
Intel,nonq3uo,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,AMD,2025-11-13 16:53:14,2
Intel,npb8jiw,Thank you for this. This was very helpful. Got adrenaline working fine now.,AMD,2025-11-17 13:18:58,2
Intel,noot79m,"I wish my LG C4 42"" had a display port. Its my primary monitor.",AMD,2025-11-13 20:04:24,3
Intel,nop8j9i,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man itâ€™s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for meâ€¦ and have zero time to reinstall Windows.",AMD,2025-11-13 21:21:55,1
Intel,nqeioib,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",AMD,2025-11-23 19:07:08,1
Intel,npaqybw,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",AMD,2025-11-17 11:00:13,5
Intel,nopyh74,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",AMD,2025-11-13 23:40:57,5
Intel,notchza,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,AMD,2025-11-14 14:38:51,2
Intel,nosoenw,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",AMD,2025-11-14 12:09:54,1
Intel,notd4le,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",AMD,2025-11-14 14:42:17,1
Intel,notu48n,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,AMD,2025-11-14 16:07:41,1
Intel,npblkwc,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,AMD,2025-11-17 14:35:55,1
Intel,nphlnml,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,AMD,2025-11-18 13:26:02,1
Intel,noru29k,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,AMD,2025-11-14 07:20:44,1
Intel,np08w4v,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",AMD,2025-11-15 17:20:05,1
Intel,ntmuect,We've still not been able to reproduce this unfortunately. I'll need to check in when I'm back at work next year,AMD,2025-12-12 12:24:51,3
Intel,np0tp7f,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,AMD,2025-11-15 19:06:34,2
Intel,nphr5q3,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",AMD,2025-11-18 13:55:53,1
Intel,nopc4t4,No you can't.,AMD,2025-11-13 21:39:44,1
Intel,nonrg54,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",AMD,2025-11-13 16:59:46,0
Intel,npbfbpp,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",AMD,2025-11-17 13:59:52,1
Intel,noq4fcn,"They are TV's, not pc monitors. Buy the right tool for the job",AMD,2025-11-14 00:15:47,-2
Intel,noxv18g,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",AMD,2025-11-15 06:30:19,1
Intel,np729v3,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",AMD,2025-11-16 19:45:29,1
Intel,npiownv,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",AMD,2025-11-18 16:47:14,1
Intel,nrkoujc,since last BF6 Update i had zero crashes also on 25.11.1,AMD,2025-11-30 18:13:02,2
Intel,noruco5,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",AMD,2025-11-14 07:23:31,2
Intel,np08z2a,What about 25.11.1?,AMD,2025-11-15 17:20:29,1
Intel,o3db89o,Happy New Year ;)  Were you able to check on this issue yet?,AMD,2026-02-03 16:52:08,1
Intel,npkeuqy,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,AMD,2025-11-18 21:52:44,1
Intel,nopdsez,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,AMD,2025-11-13 21:47:54,3
Intel,noo8n6z,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",AMD,2025-11-13 18:23:39,2
Intel,nonsm12,"Look online for fsr 4 on 6000 and 5000 series, you will understand,Â    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",AMD,2025-11-13 17:05:33,2
Intel,np73g8a,Did you reboot after setting that key? Is the display with chrome still only partially updating?,AMD,2025-11-16 19:51:24,2
Intel,norvx55,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",AMD,2025-11-14 07:38:54,1
Intel,o3jnizi,we've done so several times but have not reproduced the issue so far. I'll follow up with the T&D team about this ticket when I'm back at work next week,AMD,2026-02-04 15:36:18,1
Intel,npkhv83,thank you,AMD,2025-11-18 22:08:07,1
Intel,nopey1i,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",AMD,2025-11-13 21:53:39,1
Intel,noozt1l,"Not a typo, I was asking about something else and he missed my point...",AMD,2025-11-13 20:37:57,2
Intel,norw6su,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",AMD,2025-11-14 07:41:30,2
Intel,o6djmqo,I used my Index on 25.12.1 just now and tried 90Hz mode again and the lag is still very noticeable. Tried it again at 120Hz and 90Hz on a RX480 and it doesn't have the issue.  I haven't tried 26.1.1 as it broke my DDC display remote software so I rolled back pretty quickly.  I don't have Smart Access Memory enabled. Could that be the problem?,AMD,2026-02-20 05:48:54,1
Intel,nopm704,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",AMD,2025-11-13 22:31:45,2
Intel,o6ens2l,"I doubt ReBAR (or the lack of it) could introduce an issue like that, specific to a display mode on your HMD. I'll follow up again to see if we can have another crack at reproducing this. I'm not sure why we're not observing this so far but you still are.  I'm wondering if capturing a specific type of display log whilst your headset is exhibiting the issue in the  90 Hz mode (and then another from the 120 Hz mode as a baseline) may help us understand what's happening - will follow up.",AMD,2026-02-20 11:50:03,1
Intel,nopq646,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",AMD,2025-11-13 22:53:13,0
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,161
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,85
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,47
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,16
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,5
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,4
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,4
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,4
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,5
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,Youâ€™re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asfðŸ˜­,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,ng0v4qd,absolute cinema,AMD,2025-09-24 21:52:34,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,46
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,14
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,16
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,4
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here:Â https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,13
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D:Â https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md ðŸ––,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (Iâ€™m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,8
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,6
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,4
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,2
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,6
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,19
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,7
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,4
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,5
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,4
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  â€‹â€‹â€‹  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"ðŸ‘   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,nlb3nwr,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-10-25 13:40:37,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,22
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,15
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,5
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,35
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-7
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,8
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,5
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didnâ€™t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,74
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,10
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,24
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,16
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-17
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-15
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,5
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,6
Intel,m84dadg,"Iâ€™m pretty sure HUB doesnâ€™t like Nvidia *or* AMD. Theyâ€™re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,10
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,3
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,55
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,20
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-14
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,47
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-3
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,23
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,1
Intel,lgze3vw,"It depends on what your goals are for a laptop.Â  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!Â Â  I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.Â  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-8
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-10
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,12
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,16
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,6
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,5
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,4
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-2
Intel,lfkw8g2,throw it in the next steamdeck and Iâ€™ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,5
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if youâ€™re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,11
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,10
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, itâ€™s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,9
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,5
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,8
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.Â  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.Â  The system should have an actual TRUE quadcore memory setup.Â  Many of these systems have currently (and will absolutely continue to have)Â dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.Â  This matches my system that runs a 780m with 7500mhz lpddr5x.Â  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.Â  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"â€œAbsolute monsterâ€? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.Â   Â AMD has a long way to go before claiming â€œMonsterâ€ class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet arenâ€™t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Maxâ€¦ let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. Thatâ€™s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,3
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming â€œMonsterâ€ class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMDâ€™s biggest markets. You are kidding if you think they donâ€™t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they donâ€™t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Appleâ€™s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games arenâ€™t the only thing APUs are used for so PS5 isnâ€™t wholey in the conversation. PS5 also costs monthly to play online and their games arenâ€™t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Appleâ€™s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP â€” PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,30
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,19
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,7
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,4
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,8
Intel,lf385p0,"This was a graphics card, not a â€˜GPUâ€™ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,6
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-1
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,10
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,4
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, thereâ€™s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,22
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,10
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,4
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,11
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,-1
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,1
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I donâ€™t mind free markets, Iâ€™m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,10
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,222
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,24
Intel,kxiush3,"""The ability to â€œturn it off and on againâ€ should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,114
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,17
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,30
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,122
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,70
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,7
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,36
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,36
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,62
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,14
Intel,kxiah6c,"Iâ€™ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, itâ€™s usually a day late and a dollar short. Blender on Linux still canâ€™t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that thereâ€™s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but weâ€™ll have to wait and see.",AMD,2024-04-01 05:59:50,24
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*â„¢ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,22
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,4
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,3
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,25
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,5
Intel,kxp7mvs,Iâ€™ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,5
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,5
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,5
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,11
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100Âºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,15
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still canâ€™t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,5
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,7
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,17
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,3
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,2
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,3
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,3
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,6
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,0
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-5
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-22
Intel,kxk9iir,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-3
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-4
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-4
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,48
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  ðŸ˜¤ðŸ˜­,AMD,2024-04-04 17:12:00,3
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,18
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,4
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,33
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,13
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,12
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-3
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,20
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,7
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,4
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,7
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,3
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:Â Â  Â    ""Speaking of VRAM, The drivers use VRAM less efficiently. Look atÂ any side-by-side comparison between games on YouTubeÂ between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.Â    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?Â    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.Â    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?Â    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,5
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,30
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,9
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,8
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-4
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,12
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,2
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-6
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,24
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocksÂ EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,8
Intel,kxjbu8k,"I dunno man. Iâ€™ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I havenâ€™t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,4
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems ðŸ˜‚ but I guess some do idk ðŸ¤·. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,3
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me Â£540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent Â£200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,4
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,12
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,6
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,4
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,4
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-15
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,9
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,13
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,2
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-1
Intel,kyy38w2,"Hey OP â€” Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,31
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,4
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,6
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,7
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,45
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,30
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,3
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,14
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,4
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,8
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,9
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,7
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,10
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,1
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,1
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,2
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,13
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,2
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,9
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,21
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,20
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,5
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,6
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,4
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,5
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-13
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,0
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,31
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,30
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,1
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,5
Intel,kyhsjnw,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-3
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,7
Intel,kxj49ms,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,6
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,4
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,15
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,12
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,0
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,13
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,12
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,6
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting ðŸ˜‚ðŸ¤£,AMD,2024-04-01 14:23:58,3
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,9
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,8
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,5
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,7
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-3
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,3
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,7
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,-2
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,2
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,2
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,1
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-5
Intel,kxih401,Oh then just ignore my comment ðŸ˜…,AMD,2024-04-01 07:20:10,-1
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,9
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-14
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-11
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,7
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,1
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,5
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,12
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,3
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,26
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pcâ€™s are better and donâ€™t really complain about upscaling and 30fps.. youâ€™re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldnâ€™t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,6
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned offÂ   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,6
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,37
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,10
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,22
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,18
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,17
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,6
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,13
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,5
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,16
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete.Â And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,5
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers werenâ€™t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,3
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,6
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,-1
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,17
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?Â    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,1
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,5
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-8
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-7
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-3
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-2
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-6
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,4
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,6
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,16
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,6
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,7
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,8
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,7
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,-2
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,8
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,3
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-3
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,-2
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,4
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,5
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,o5l9vfi,"This will be a very popular CPU, but I wouldn't expect any availability until well into Q2.",Intel,2026-02-15 23:07:09,25
Intel,o5loer7,The GPU with 20% more Xe cores scores about 20% better on time spy. Makes sense I guess.,Intel,2026-02-16 00:33:04,8
Intel,o5mpbc5,"Performance is good, but laptop prices need to stay reasonable even with ram going nuts. It looks like the non b390/b370 igpus are worse than LL or AL, so I can't see any reason to get a lower tier CPU except maybe for battery life.",Intel,2026-02-16 04:34:54,3
Intel,o5lajsl,I thought the 388h was supposed to be in the ballpark of the 8060s? Itâ€™s showing quite significantly behind,Intel,2026-02-15 23:11:04,-10
Intel,o5m3psu,"I'd love to see these compared with identical memory speeds. I suspect the gap will be *very* small, hence the strict segmentation.",Intel,2026-02-16 02:09:15,6
Intel,o5lmi8y,"Intel's claimed performance was around the 60W 4050 mobile GPU. At typical handheld TDPs Panther Lake does beat Strix Halo, but when given unlimited power the latter can pull well ahead.  This makes sense. The 8060S is 5/3 the width of the B390. A full 2x the B370. Its optimal power range pretty much starts where Panther lake tops out.",Intel,2026-02-16 00:21:45,14
Intel,o5lcttq,They are not in the same price or power bracket.   But no it's not competitive with 8060s,Intel,2026-02-15 23:24:26,20
Intel,o5p4e1h,"Yup. This is the point everyone missing. At low wattage, the b390 is comparable/beats out the 8060s. At <=30 W.  However, the scaling per watt of the br390 past some point is horrendous, while the 8060s was designed to sap multiple times the power of the b390 and severely shows that if you let it.",Intel,2026-02-16 15:33:09,1
Intel,o5lw5p6,"It's not competitive with 8060S beyond the 30W mark. For lower than that, the Arc B390 can rival the 8060S fine (at least until we see what it can do inside the 392 and 388).",Intel,2026-02-16 01:20:45,9
Intel,o5lwlku,8060S is not meant to run lower than 30Ws.   Which is my point.Â    It's like comparing 5090 with B580. Yeah B580 is not really competitive. But doesn't really matter much given power and price.,Intel,2026-02-16 01:23:36,7
Intel,o5lytn3,There have been scenarios where it was forced to run below 30W though (handhelds). Also I don't think there is any range of wattage where the 5090 doesn't beat the B580.,Intel,2026-02-16 01:37:55,8
Intel,o5okh8v,If you power limit them to 0W they're damn near tied.,Intel,2026-02-16 13:49:28,5
Intel,o6m7sq5,My a770 is like gold now,Intel,2026-02-21 15:52:12,1
Intel,o674r75,"frame gen is snake oil, generating more frames doesn't help if the base frame rate isn't at least 60 fps, and it won't reduce latency like real frames.",Intel,2026-02-19 06:08:20,-2
Intel,o68cgnz,It actually increases real latency because base frame rate actually gets reduced.,Intel,2026-02-19 12:35:28,10
Intel,o6ovmt4,"Framegen is not snake oil, it has its pros and cons like most stuff",Intel,2026-02-22 00:18:53,4
Intel,o6i5dx1,Framegen is awesome,Intel,2026-02-20 22:21:53,2
Intel,o6deerd,exactly.,Intel,2026-02-20 05:07:18,1
Intel,o6pwck8,"Not good for competitive games, but a great add for RPG / simulation games",Intel,2026-02-22 04:19:15,2
Intel,o5o5iht,Nova lake will be strong.,Intel,2026-02-16 12:12:12,7
Intel,o5ty2nw,"Even the 4 Xe3 PTL was pretty strong, outperforming 8 Xe+ Arrow Lake and trading blows with 130v Lunar Lake. I wish they made 8 and 6 Xe3 SKUs fabbed in Intel 3 for the low end market. Maybe later, for Nova Lake.",Intel,2026-02-17 08:00:18,2
Intel,o7ebpl2,How much life spawn without burning CPU will it last this time...?,Intel,2026-02-25 21:08:21,1
Intel,o6dkfiz,"Buffing that smaller tile to 6 total would've gone over with everyone pretty well I think. Given the architecture gains, likely close to the 130V and 140V while still being quite small.",Intel,2026-02-20 05:55:39,3
Intel,o5z7fim,What is the TDP of the flip? Does it boost sustained to 60 or does it cut out before like the XPS?  What is the TDP of the stealth? Screen size? How robust is the thermal solution? What about noise? Wattage of the GPU?,Intel,2026-02-18 01:57:31,13
Intel,o5zj5fm,> The Prestige 16 Flip AI+  That's a mouthful.,Intel,2026-02-18 03:00:57,12
Intel,o61rcnz,"I was lucky enough to get the Prestige 14 Flip went it went up for preorder. Love it, great feel, battery, performance, size/weight, etc etc. I'm sure the 16"" models will be at the same level.",Intel,2026-02-18 13:21:42,2
Intel,o60yb32,"Why do you not offer good, non oled screens? I don't want to deal with degradation and burn in risk.",Intel,2026-02-18 09:42:07,3
Intel,o6035f0,Do you guys have something for the poor paupers.,Intel,2026-02-18 05:09:58,2
Intel,o60b5ok,Please please can we have the prestige 13Â± Ai with a oled 2k touchscreen!  The one that is under 1kg. Please!,Intel,2026-02-18 06:12:13,2
Intel,o5z7elj,Wish the marketing for these didnâ€™t say it was as powerful as a mobile 4050. Somewhat misleading. The one itâ€™s as good as is the version with a low power limit.,Intel,2026-02-18 01:57:24,2
Intel,o5zgk37,when will the prestige 14 / 13 be released?,Intel,2026-02-18 02:46:12,1
Intel,o5zk366,Bluetooth 6?,Intel,2026-02-18 03:06:29,1
Intel,o5zxwzy,Msi makes business laptops? Whoâ€™s buying these?,Intel,2026-02-18 04:33:22,1
Intel,o61kqo0,Hi! Any word on when the MSI Titan will be available with the new Intel CPU? Or an idea of when? That is what Iâ€™m holding out for,Intel,2026-02-18 12:41:45,1
Intel,o61r6ms,Make a good 2-in-1 with the Ultra 5 (around 1000 usd) and i'll get it next time :),Intel,2026-02-18 13:20:46,1
Intel,o6h1txe,Itâ€™s disappointing to see this new Stealth laptop ship with such an underwhelming keyboard.,Intel,2026-02-20 19:06:49,1
Intel,o715ovn,I'll never understand pen laptops without soms kind of storage area for the pen.,Intel,2026-02-23 22:16:34,1
Intel,o5zfo5k,"Core ultra X7 and X9 increased by 20-30% for price, right?",Intel,2026-02-18 02:41:17,0
Intel,o6w0vvq,"PL1 45W / PL2 60W for Prestige but no idea if it can keep 60W under lengthy stress test  PL1 & PL2 on Stealth are both 200W but again, not sure if it can truly deliver up to 200W under Prime95",Intel,2026-02-23 03:04:41,1
Intel,o6z8a8o,"Depending on specification, the Stealth 16 AI+ B3W can support up to an NVIDIAÂ® GeForce RTXâ„¢ Laptop GPU with a rated TDP of 125W in boost performance. The cooling solution features MSI's new CoolerBoost Intraflow technology which increases cooling efficiency while reducing both temperatures and noise. This new design enables the system to deliver stable, high-level performance when running demanding applications.   For a full breakdown, please visit the following website for more information: [https://www.msi.com/Laptop/Stealth-16-AI-Plus-B3WX/Specification](https://www.msi.com/Laptop/Stealth-16-AI-Plus-B3WX/Specification)",Intel,2026-02-23 16:48:45,1
Intel,o64bqn9,"I never want OLED either, especially on my work machine.  As a personal/consumption device, I guess it's ok but I'd rather have a high quality IPS.  Looking at all the announced Panther Lake models so far, most have OLED.",Intel,2026-02-18 20:34:00,5
Intel,o5zwqix,4050 comes with high power limit only in clunky gaming laptops with beefy cooling. Panther lake goes into thin and light laptops where it competes with other low power 4050 GPUs. So it's valid when you consider the class of laptop it competes with. Ultrabooks cannot really handle a 100W+ GPU well so it perfectly makes sense. It's not really misleading when they state the power limits tested. Efficiency is important for the thin and light laptops and it's important to compare it at lower power limits to market the performance advantage at lower power over previous ultrabooks that came with <60W 4050 GPUs.,Intel,2026-02-18 04:25:24,13
Intel,o5zrrbe,Yup,Intel,2026-02-18 03:52:56,1
Intel,o61riv4,"I preordered the day they went up on bhphoto and received mine on release. Not sure why they went discontinued, maybe there was strong demand?",Intel,2026-02-18 13:22:41,2
Intel,o5zxda9,Prestige 14 flip was on bhphoto and I was going to preorder but got an email that said it got discontinued,Intel,2026-02-18 04:29:40,1
Intel,o63btvx,"hi there! yes, our new PTL models all carry Bluetooth 6 capabilities",Intel,2026-02-18 17:51:41,1
Intel,o71bbkz,hi there :)  The Prestige 16 Flip AI+ comes with a dedicated groove to hold the stylus! It can be found at the bottom of the laptop.,Intel,2026-02-23 22:45:24,1
Intel,o6202kt,"Obviously, it's not AMD you're looking at.",Intel,2026-02-18 14:08:50,0
Intel,o6206sz,"I don't know a single soul that would choose poorer performance for a higher price just because the laptop is 0,2mm thinner lol",Intel,2026-02-18 14:09:27,1
Intel,o63cwgl,is it true that it gets very hot to the touch?  https://www.youtube.com/watch?v=H98eAifduFU,Intel,2026-02-18 17:56:17,1
Intel,o62lrfm,"Considering that laptop has only been getting thinner over time (look at Dell XPS's evolution and the prevalence of stuff like ROG Zephyrus), I would say most people like that trade off actually.",Intel,2026-02-18 15:53:55,4
Intel,o63jda4,"This is nuanced as most things, video does cover a lot but certainly leaves a lot out.       Doing basic stuff like browsing, word processing, etc it is cool and silent. When I was playing overwatch or D4 I didn't notice it any worse than one of my gaming laptops(I have a 3070ti and a 5060 laptop. The 3070ti has the hot intel 12k series in it). I also use Intel's ai playground and never thought of a heat issue during this.   Using HWMonitor the temp does spike to 95-97C before fans start to kick in or the voltage down regulates some. I have two devices with the last gen chips, the 258v and 256v. Basically identical beside the RAM. I actually use the 256v system as a ""work daily"" cus of low energy usage. Anyway, these new chips certainly suck more power if they need it and are set to performance mode but that also comes with a lot more performance.   Performance mode does kick the fans up, which would help with dissipating heat. This mode seems to be around 40w max. The balanced mode is about a 30w max and the eco-silent looks to be sub 20w.   A note, I am up north and the ambient room temp is 68F. She does mention being in Arizona in the video, but what is the room temp, do they keep it warm at like 75F for an office?",Intel,2026-02-18 18:24:20,3
Intel,o419xp2,"only +23% in Linux feels like bad drivers, when the B390's getting 2x performance on Windows",Intel,2026-02-07 05:33:34,24
Intel,o41k2xt,Good. AMD has been rehashing same crap lately. This is why competition matters,Intel,2026-02-07 07:01:00,21
Intel,o426cd8,AMD 14nm moment,Intel,2026-02-07 10:36:23,8
Intel,o42ncwr,"Nice, I'm tired of AMD rebranding old APUs and charging for them as if they were brand new.",Intel,2026-02-07 13:01:43,6
Intel,o420dq7,What about 8060s? Though?  Who fucking cares about 890M? Itâ€™s made for business laptopsâ€¦,Intel,2026-02-07 09:37:29,0
Intel,o3zmils,I want Intel to win but sadly they only win against the HX370 which is a generation behind the 395+. So intel is still behind. They better price it well below the HX370 to regain market share. AMD lost the plot in terms of pricing and HX370 handhelds are well over 1300. If they can partner with MSI and produce a handheld for under 1000 they have a winner. Anything above that is DOA.,Intel,2026-02-06 23:14:29,-24
Intel,o41ay0s,considering sr-iov and passthrough on linux are completely busted in the xe drivers for the B50 and that has been out for months it wouldn't surprise me if these are rough as well.,Intel,2026-02-07 05:41:39,5
Intel,o421h3i,RDNA 3.5++++,Intel,2026-02-07 09:48:29,17
Intel,o45uzpk,"Yes since they have this new loser as the head of client they don't build anything new. They just recycle old shit with new marketing name. I am sure he will kill the business in 2 years. Saving grace is AMD is untouchable right now for desktop, and Intel sadly is full of useless VPs that spend their days in meetings.",Intel,2026-02-07 23:14:29,1
Intel,o42u6kw,"8060s still comfortably beats out the B390, but it's designed for a different power class and draws a lot more power.",Intel,2026-02-07 13:44:59,12
Intel,o45a3to,"The 890M is in the Z2 Extreme. It's not just for laptops, it's their prime handheld GPU chip.  The 8060S only starts beating the B390 around 33 W, making it unfit for a handheld device where battery life is very important",Intel,2026-02-07 21:19:03,3
Intel,o3zrsvz,the 395+ is a different segment of product imo. They start at $2200+ while the intel chips start around \~$1200 so far. The battery life also isn't even close.,Intel,2026-02-06 23:45:02,42
Intel,o3zrwwh,Panther lake and 395+ canâ€™t be compared the 395+ is a much bigger die and with double the bus theyâ€™re completely different tiers,Intel,2026-02-06 23:45:41,25
Intel,o40cvka,"Strix Point and Strix Halo are the same generation, one is just much bigger and more expensive and for an entirely different performance tier. You're coming into a 5050 vs B580 comparison and saying that a 5090 is faster than the B580. Duh, of course it is.",Intel,2026-02-07 01:50:11,12
Intel,o5d0wok,"I saw reviews that b390 beats 890m only when 890m is power limited, and when 890m APU is allowed to draw its full PL2 power of 54w it beats b390 by ~29%",Intel,2026-02-14 16:29:55,1
Intel,o40lyhs,The price of panther lake is gunna be expensive too,Intel,2026-02-07 02:46:26,-4
Intel,o424dp4,"Exactly. I was really disappointed with their igpu tactics. Rehash, rehash, rehash.   I do think we need DDR6 for next major igpu scaling, as memory bandwidth is a problem too.",Intel,2026-02-07 10:17:09,6
Intel,o4trmyr,The 16C+40CU strix halo sucks down power.  I want to see the 8C+40CU or 8C+32CU halo's compared,Intel,2026-02-11 16:50:04,3
Intel,o40emvu,Which laptop is releasing at $1200 with the B390,Intel,2026-02-07 02:00:58,2
Intel,o45uscn,"Yes agreed, but if the machines offering them are in the same ballpark then it is a failure on intel. A machine sporting the B390 should be cheaper than an equivalent machine with the HX370. We need handhelds with B390 below the 1000 mark for example.",Intel,2026-02-07 23:13:16,1
Intel,o4ejoec,Is LPDDR5X/DDR5 bottlednecked already?,Intel,2026-02-09 08:47:30,1
Intel,o40fa3x,"[https://www.bestbuy.com/product/hp-omnibook-x-copilot-pc-16-2k-oled-touchscreen-laptop-intel-core-ultra-x7-358h-2026-32gb-memory-1tb-ssd-meteor-silver/JJGW34X2K5/sku/6665780](https://www.bestbuy.com/product/hp-omnibook-x-copilot-pc-16-2k-oled-touchscreen-laptop-intel-core-ultra-x7-358h-2026-32gb-memory-1tb-ssd-meteor-silver/JJGW34X2K5/sku/6665780)     there's this $1400 one. there's also only like 5 available series 3 laptops. if this 16"" is starting at this price, I'd expect the 14"" laptops to be a little cheaper.",Intel,2026-02-07 02:04:52,13
Intel,o4emvxo,For igpu purposes? I am sure it is. Compare to bandwidth dGPUs require. the difference is stark.,Intel,2026-02-09 09:19:22,1
Intel,o41rwbo,This seems like a significant outlier from what I can see and it's unavailable so unclear if it's an error.Â    I also can't see this on the hp website.   Guess we'll have to wait and see. From what I've researched the x /h processors are significantly more than the base,Intel,2026-02-07 08:14:15,2
Intel,o416ch9,it's unavailable .....possibly it's a subsidized product.....,Intel,2026-02-07 05:05:43,-4
Intel,o428rdn,"yep, this  basically all the new B390 laptops lists around a same ballpark price as the 395+ ones  just over 2k USD",Intel,2026-02-07 10:59:32,2
Intel,o41au1b,"it ran out of stock a couple days ago. There's a few open box ones you can still pick up though. most of the panther lake laptops just aren't released yet. They might go up in price due to ram, but everything else will as well unfortunately.",Intel,2026-02-07 05:40:45,5
Intel,o45ul90,"That is my point. Not worth the price if it barely beats the HX370. And just the fact we need to have this conversation shows how Intel Marketing miserably failed. If the B390 devices are cheaper, please make sure you plaster it everywhere and communicate it well. Even the people interested in it like me do not know or believe it.   Again if anyone at intel reads this, you better price this lower than the HX370 or you have no chance. I do not like AMD but sadly they have the best overpriced products right now.",Intel,2026-02-07 23:12:08,1
Intel,o45vxqk,this sounds about right.  Intel just said that the margins are PTL are below corporate average. which means yields on 18a are abysmal.  Expect prices to be high and supply to be low until yields improve (which the CFO head said wont be till 2027. But they should gradually improve through 2026.  AMD and intel are in a race. Intel need to improve 18a yields. AMD needs to get zen6 out. Whoever gets there first will be the winner. Until then it kinda looks like the laptop market is just a continuation of what it was before. AMD gets a slight pickup from the 400 series. Intel gets a slight bump from PTL.,Intel,2026-02-07 23:20:02,1
Intel,o431u16,well why don't they restock it ??,Intel,2026-02-07 14:29:14,1
Intel,o35qnw3,"Honestly, I wish Intel would give these 'Intel Graphics' model numbers. An Intel Graphics on a N series SKU is going to be far weaker than a B370/B390 masquerading as 'Intel Graphics'. That or just give these ARC iGPUs that don't meet the RAM speed a number like ARC B370e/B390e instead.",Intel,2026-02-02 14:22:42,29
Intel,o37ku63,Is panther lake getting an N series release?,Intel,2026-02-02 19:33:06,2
Intel,o397w1s,Wildcat Lake for the N Series,Intel,2026-02-03 00:27:12,1
Intel,o3ch73d,"That looks like it's filling in the ""ultra 3"" tier, not the N/celeron/pentium tiers.",Intel,2026-02-03 14:26:50,1
Intel,o3nclgj,"Wildcat Lake will succeed N200 Series, but will be named under Core 3 Series brand, likewise Panther Lake for the Core Ultra 3 Series.",Intel,2026-02-05 02:36:55,1
Intel,o1zvssn,iGPUS are replacing the low end discrete graphics. That's incredibly impressive,Intel,2026-01-27 11:10:17,39
Intel,o1zd4re,Will panther lake pc CPU's have this kinda powerful igpu?,Intel,2026-01-27 08:20:52,13
Intel,o2245xf,"Have any of the ""regular office worker""-level CPUs been seen yet?  I am hoping we can see 8 to 10 hours of battery life under real usage.  We get 12 to 14 with MacBook Air 13's, so we would like to at least see a full work day with the Intel CPUs.  We were really hoping we would have seen that with Lunar Lake, but we usually see closer to 4 to 7 hours under real usage.",Intel,2026-01-27 18:05:29,2
Intel,o1zd2wu,"Stop with these fake benchmarks.  You will see when the benchmarks are not cherry picked, it will be no where near a 4050 lol. They had to gimp the poor 4050 all the way down to 30w, which I donâ€™t even know how it even operates as such low wattages for the b390 to have a chance.  The full wattage 4050 (100w+) is about as fast as a 3060 desktop, this tiny iGPU in real world gaming tests will be at most best case scenario as fast as a full powered 3050, which is still  a massive 50-60% slower than a 4050 laptop.",Intel,2026-01-27 08:20:23,-20
Intel,o2004r5,"Because the low end discrete options are non existent or outdated, or bad value. The door is open for anyone to fill in that low end spot.",Intel,2026-01-27 11:45:12,21
Intel,o20fvx6,"It has to be cheaper first.  Current pricing of laptops with these chips is looking really high.  Granted they are mostly 'nicer' laptops, but still, really steep.  Check out the XPS pricing for example...",Intel,2026-01-27 13:28:05,6
Intel,o1zymzc,Impressive if the cost is good. Not so much if they're priced too high like Strix Halo.,Intel,2026-01-27 11:33:27,7
Intel,o206dww,yeh its amazing but nvidia and amd did it to themselves...  rtx 3050/4050/x should not be priced as they are currently.,Intel,2026-01-27 12:30:17,6
Intel,o2c33xf,Papaâ€™s here,Intel,2026-01-29 02:07:51,1
Intel,o1zdgl7,You mean desktop? PTL is mobile only.,Intel,2026-01-27 08:23:53,30
Intel,o20iisi,"For desktop, we're stuck with Raptor+Arrow Lake until Nova Lake, which uses entirely new P and E cores even from Panther Lake.    So no.",Intel,2026-01-27 13:42:14,4
Intel,o23m337,Laptops are pc so yes,Intel,2026-01-27 22:02:53,1
Intel,o1zdh08,No lol,Intel,2026-01-27 08:23:59,-8
Intel,o25o55z,"Lunar lake already gets over 12h battery     I wish people would stop comparing it to Macs, people in businesses and people who use specific programs or even games will never use Macs",Intel,2026-01-28 04:31:07,2
Intel,o1zlgrm,It trades blows with 65 watt 5050.   And it does that while using less than that full package. (including cpu)Â    No one is saying its better than 130 watt gpus.,Intel,2026-01-27 09:38:47,20
Intel,o20j2fp,[https://www.youtube.com/watch?v=jrygnUnBRNI&t=1s](https://www.youtube.com/watch?v=jrygnUnBRNI&t=1s)  Shows it's like 3050Ti performance.,Intel,2026-01-27 13:45:07,3
Intel,o22e0if,"Low-end cards don't make sense for anyone involved. the fixed costs are far too high, you have no margin, and it just looks stupid compared to a slightly more expensive card on which you can put twice the GPU.",Intel,2026-01-27 18:47:06,2
Intel,o21y41l,"True, but it's probably the ram situation. Unless intel is asking for a lot to recoup the 18A investment... Theoretically the PTL SOCs themselves shouldn't be the pricing issue",Intel,2026-01-27 17:39:38,2
Intel,o203kv0,"This. If 8060S Halo had been available.. at all, but also.. in sub 1600 laptops, or 8050S in sub 1200, it would have been wonderful. Same is true for PTL with B390 + 9600 MT/s RAM. I hope they will be competitively priced (also, hope the RAM pricing doesn't hit them too bad).",Intel,2026-01-27 12:10:46,13
Intel,o206nt4,There is laptop models priced at $1300 with the iGPU you want.  I don't know what the chip pricing is - we never really do with mobile - but it seems low enough that regular priced laptops can include a high end Panther Lake chip.,Intel,2026-01-27 12:32:08,4
Intel,o2076gf,"To be fair...they are priced very cheaply. There was a $600 5050 laptop on sale last year even.  Nvidia volume pricing on 50 tier mobile GPUs seems to be very attractive, it's up to manufacturers how much extra they want to tack on but you can build a very cheap, very capable laptop using this GPU.",Intel,2026-01-27 12:35:40,6
Intel,o1zho72,Until some chinese manufacturer makes a hacky desktop motherboard that is,Intel,2026-01-27 09:03:10,21
Intel,o2oxene,Is the rumored drop by end of 2026 accurate to your knowledge?,Intel,2026-01-30 22:51:31,1
Intel,o226459,Our fleet are the Dell Pro Plus 16-inch.  We can probably eek out 10 hours if we really baby it.,Intel,2026-01-27 18:13:53,1
Intel,o22efrd,"what's ""real usage""? i can do mildly CPU-intensive work on my 14"" premium pro whatever laptop and it won't last more than 10h.",Intel,2026-01-27 18:48:53,1
Intel,o25or77,>people in businesses will never use Macs  Wrong.,Intel,2026-01-28 04:34:57,2
Intel,o1zmcv6,"It does not lolâ€¦.  Once you see the real gaming benchmarks it will be wayyy slower with its low powered small gpu cores. Idk what all you people are smoking, heck the 120w 8060s with its massive die is still 13% slower than a 4060 laptop in a 20+ game average tested by Jarrodâ€™s tech.  Itâ€™s at best case scenario in real uncherry picked games 3050 performance, considering the 140v is about as fast as a 1650, even though that was leaked to be as fast as a 3050 in cherry picked benchmarks.",Intel,2026-01-27 09:47:05,-17
Intel,o20pa10,"Exactly, I hate these cherry-picked overhyped tests and claims.   Itâ€™s a small iGPU, you canâ€™t possibly expect 4050 and definitely not 5050 discrete level performance lol, and I get downvoted so hard even though Iâ€™m rightâ€¦ reddit moment.",Intel,2026-01-27 14:17:11,0
Intel,o220te4,"If that is true, then we should see some spiking up in costs across the space in which case you'll get a collapse in shipments since relatively few people can afford 2k laptops and I'd guess people are more sensitive to a laptop priced at 1200 vs 1500 compared to something priced 2000 vs 2300 since the most price elastic part of the market is going to just give up before they even get to 2000.",Intel,2026-01-27 17:51:18,1
Intel,o20rncp,They put too much CPU in them to get them cheap. Though my 8060s z13 with 64gb ram was $1650 (open box Best Buy a while back) that was a lucky sale unlikely to happen again. Iâ€™d rather have a super thing and light XPs 13 with panther lake though and probably will unload the z13 when I can get the XPs.,Intel,2026-01-27 14:29:00,4
Intel,o25nygi,"the die is like twice the size it will never be cheap, its a scam chip for people buying stuff for LLMs not for gaming/content creation",Intel,2026-01-28 04:29:55,1
Intel,o20esgd,"If you mean the MSI model I'll definitely be skipping that one. Not a fan of MSI.   $1300 certainly isn't terrible. Better than Strix Halo offerings I've seen, but I hope that's not going to be the low end. I'll keep an eye out. Given the price of all electronics these days I won't get my hopes up. I'd love to be pleasantly surprised though.",Intel,2026-01-27 13:22:05,0
Intel,o1zso7u,Nova lake will use that as well,Intel,2026-01-27 10:43:44,5
Intel,o2p97ux,"Not a rumor, that's Intel's word.  Which is certainly shaky these days.  Though they do seem to be delivering things on time better nowadays, so I'm cautiously optimistic.",Intel,2026-01-30 23:55:45,2
Intel,o24tg9c,"The 16 might have bigger batery than the 14.  I have a 14 really happy, big upgrade from old Laptitude with 11th gen. Hated the Keyboard at first",Intel,2026-01-28 01:41:28,2
Intel,o25p6pg,yeah totally the majority of big businesses with 1000+ employees has a lot of Macs and totally not windows laptops,Intel,2026-01-28 04:37:39,2
Intel,o1zqarn,It was shown to be running RDR2 at 75 FPS on High settings at 1080p and Forza Horizon 5 at 90 FPS on Ultra at 1440p. All while consuming 50 - 70 watts package power.,Intel,2026-01-27 10:22:46,11
Intel,o1zrsjo,"Can you confirm what suggests cherry picking in this review? Looking at it they seem like a fairly standard range of tests, 3D mark and a bunch of popular games.",Intel,2026-01-27 10:36:02,7
Intel,o23avv5,It's the RAM that is going to make them expensive I think... and unfortunately a lot of these with the good chip will have soldered RAM since it's basically a requirement unless they want to use CAMM2.,Intel,2026-01-27 21:13:08,1
Intel,o41aset,there's also the new HP Omnibook X 16 for $1449,Intel,2026-02-07 05:40:23,1
Intel,o208h65,please intel ðŸ™,Intel,2026-01-27 12:44:13,1
Intel,o22tcmj,"dang. that's real usage, alright.",Intel,2026-01-27 19:54:09,1
Intel,o25pe7y,Majority?  No.  But that isn't what you said.  Stop being a dumb troll.,Intel,2026-01-28 04:38:59,6
Intel,o1zt1i8,"Thatâ€™s with upscaling turned all the way to the max and frame gen on lol.   You people are so gullible, itâ€™s best case performance is 3050 perf, which is still decent. Intel deliberately says as fast as a 4050 because they gimped the 4050 to 30w ffs, a full wattage 4050 is in its own league compared to this iGPU, thatâ€™s around 8060s perf.  Inside thin laptops where a majority will have this chip it will be even slower because itâ€™ll be wattage capped so real world perf will be BEST Case 3050 perf or worse.",Intel,2026-01-27 10:46:51,-4
Intel,o1zuofi,"They gimped the 4050 to 30w, intelâ€™s official â€œtestsâ€ and even then, in their own cherry picked benchmarks it showed the gimped 30w 4050 was still ahead in a few games lol.    This guy I replied to stating itâ€™s on par with a 65w 5050, is stupid and gullible. At 65w the 5050 laptop loses a few percentage of perf, compared to if it was max 100w+. It is no where close to a 5050, at any wattage, whether the lowest 45w or not. Some people are so gullible.  Like I said the timespy score is inline with a 3050, the 5050 laptop has around 4060 desktop perf for crying out loud. This tiny little iGPU with its low combined package tdp is no where close to a 4050, and especially not even in the same universe as a 5050. The 5050 laptop is literally faster than the desktop 5050 because the laptop has gddr7 and desktop version only has gddr6.",Intel,2026-01-27 11:00:54,-2
Intel,o207hvy,Nope. No upscaling -   [Forza Horizon 5 1440p High 100+ FPS](https://youtu.be/AX_rvgsYHJE?si=YxbVFJj2NfiUpD2D&t=633)  [RDR2 1080p High/Ultra mix 70+ FPS](https://youtu.be/AX_rvgsYHJE?si=F4oig0C6V8SoFY1F&t=689)  With Upscaling -   [Spiderman 2 1080p High XeSS Quality](https://youtu.be/AX_rvgsYHJE?si=QatQtMt2flikdvMB&t=738) 75+ FPS  [CP2077 1080p High Xess Quality ](https://youtu.be/AX_rvgsYHJE?si=6s7MvEGrUMjWa3-r&t=753)80+ FPS,Intel,2026-01-27 12:37:47,8
Intel,o1zxp2x,"The article doesn't do this though, and even discusses it?   >Slower versions of the Nvidia GeForce RTX 4050 Laptop GPU up to 60 W are also in the range of the B390 â€” you need a fast version with a 90 W TGP, for example, as in the Lenovo IdeaPad Pro 5, to have a clear advantage.   The article is in the context of laptop components so obviously they aren't running at desktop TDPs, but your commentary seems... misplaced? Especially given that these aren't official Intel benchmarks.",Intel,2026-01-27 11:25:55,10
Intel,o23w1on,"In the very review this thread is on, they have the 4050 60W as 40% faster. The review itself is very sketch, having the 4050 30W as the same fps as the 4050 60W in Baldur's Gate 3. It's not trading blows with the 5050 65W. It's getting creamed by the 4050 60W.   That means if you have a laptop with a B390, it would/could actually make sense to pair it with the lowest end dgpu from 2023 if one valued performance. The RTX 4050 6GB which isn't even being made anymore.",Intel,2026-01-27 22:50:14,1
Intel,o202y5v,"You will see in real world gaming tests, itâ€™ll lag behind a 60w 4050 laptop by quite a lot, mark my word.",Intel,2026-01-27 12:06:11,-2
Intel,o20bodn,"Ahh, yes, real world gaming tests. As opposed to these tests of games by an independent third-party that (now I've had a bit more time to look) align with all the other reviews coming out and which were also conducted by third-parties.  People that get super emotional about certain brands are weird. Don't be those people.",Intel,2026-01-27 13:04:10,8
Intel,o238d3x,"dudes dense, id stop bothering. i have a lunar lake device, the GPU is nuts for its power envelope.",Intel,2026-01-27 21:01:52,2
Intel,nzwgm1n,Dual gpu? Huh? Havenâ€™t seen one of those since gtx 690. It used to be useless because memory wasnâ€™t shared. I wonder how it will work this time,Intel,2026-01-16 11:14:50,10
Intel,nzxbywf,"lol. Six months late and $300 (60%) over the announced price.  If the dual GPU version was really available for $999 right now (as announced), then Intel would make significant inroads into the local AI market.  As it is, buying this for $800 over a used 3090 is a really hard sell. Compared to a B60, the 3090 is readily available for $1100, and provides the same VRAM, double the compute and memory bandwidth, better perf/watt, and CUDA support.  With the dual GPU cards even at $2k each this does have one single niche - being able to get 144 GB of VRAM in a server at under 1500W for under $10k -  which is legitimately useful for LLM inference.  It's really sad that Intel didn't put in the investment a year ago to have a lot of capacity to produce these now. For the prices to be so high they seriously must be making like 10 chips a week.",Intel,2026-01-16 14:30:14,6
Intel,nzwvdau,NAND SHORTAGE WHO?,Intel,2026-01-16 13:00:20,3
Intel,nzva35v,"> Unlike Sparkle, Maxsun has two cards in its arsenal: the regular 24GB VRAM model with a dual-fan design, and the dual-GPU 48GB model with a blower-style fan  seems nice",Intel,2026-01-16 05:12:03,5
Intel,o04avmj,One card $799? Two cards $1598...,Intel,2026-01-17 15:32:16,1
Intel,nzz5o69,It's no different. Not even an SLI/Crossfire bridge.,Intel,2026-01-16 19:25:21,4
Intel,o135x8c,"To use this card you need your motherboard to support bifurcation. Without it only sees 1 GPU and 24GB VRAM  That wasn't the case with the likes of GTX690, R9 295 etc which the system saw them via SLI/Crossfire and could work on any mobo without supporting bifurcation.",Intel,2026-01-22 18:03:51,1
Intel,nzz8xk5,Those were also kinda useless as they only really handled sync as far as I understood,Intel,2026-01-16 19:40:31,2
Intel,nzzefbl,"What do you mean? They were a proper data bus, iirc.",Intel,2026-01-16 20:05:59,3
Intel,nzzoc4y,"i'm no expert but i dove in to Wiki      it was a [1GB/s to 3.25GB/s](https://en.wikipedia.org/wiki/Scalable_Link_Interface) interface. For refference PCI-E 2.0 was 1GB/s and 3.0 was 2GB/s BUT per lane so 16 and 32GB/s.  For refference HDMI 1.4 has 10.2Gbit/s  So as i understood it most of the data bandwidth were used for tossing the image output back and forth as only a single card were the one outputting the image (assuming you weren't running tripple monitor widescreen setup on SLI GPU's....) and our image data could more or less saturate the link  someone correct my math if i'm totally bonkers, but 2560x1440p = 3.686 million pixels. each using 24 bit (8-bit color on 3 chanels) which is 88Mbit/s. if you output 60 of those that is 5.3Gbit/s. 1GB/s = 8Gbit/s. Okay okay you wouldn't always throw a full image so lets call it 2.5Gbit. You were still using quite a lot of the bandwidth on the image alone, so from my understanding most of the interface communication was more related to ""who did what"".  and in AFR you would be handing over a full frame.",Intel,2026-01-16 20:52:43,1
Intel,o0g6w5o,"It was used purely to transfer rendered images, correct.",Intel,2026-01-19 09:27:32,1
Intel,o0g7wxp,"phew! my understanding was correct! And this is why the new nvlink is different. It's not just image data transfere. But also a LOT faster. NVlink 4.0 is 900BG/s. that is 300x faster than the FASTEST sli bridge recommended for 4K monitors. and that link is 15x faster than PCI-E 5.0 at a full 16 lanes. 15 times! the old sli was only the speed of a single or two pci-e lane. not 225 lanes equivalent speed  edit. One is numbered as unidirectional and the other bidirectional, so half the numbers above. But that is still VERY fast, even if it's a bit slower than the memory bus speed.",Intel,2026-01-19 09:37:24,1
Intel,o0g86uf,"It wasn't used differently on 20- and 30-series, and it's discontinued for the 40- and 50-series.",Intel,2026-01-19 09:40:01,1
Intel,o0g9x44,what do you mean by that?,Intel,2026-01-19 09:56:26,1
Intel,o0g9zvv,I'm saying NVLink did the same job as an SLI bridge for SLI setups. There's a reason it was discontinued,Intel,2026-01-19 09:57:10,1
Intel,nz79v91,"The fact that it can even compare to AMDs halo product, which the avg consumer canâ€™t afford is a win for Intel. Intel has plenty on leg room to expand the GPU too.",Intel,2026-01-12 18:02:37,9
Intel,nz5c8tz,This suffers from bandwidth bottleneck. Strix halo is Quad channel while panther lake is dual. An igpu would benifit greatly with a quad channel,Intel,2026-01-12 11:58:24,15
Intel,nzgoxuf,"This thing is absolutely nuts  AMD BTFO unironically, I'm floored. I never, ever would have considered an Intel chip before 2025, now this is the most obvious laptop part ever. AMD is surely sorely regretting recycling the same 780M and 890M chips for another entire gen, betting that Intel would continue stagnating.  This thing is gonna be a monster in handhelds.  I really, really wanted a Strix Halo laptop, but the lack of SKUs, price and the inflexibility with RAM kind of make it unappealing to say the least, not to mention the power draw compared to Panther Lake is unwelcome. These laptops are gonna be probably the best x86 in mobile has eaten in a very long time.  On top of that, it's almost making the 5050 look like a stupid part in a laptop. Why bother when you have a vastly more power efficient iGPU that will handle every desktop workload on top of being viable for gaming?",Intel,2026-01-14 01:47:11,2
Intel,nz4rv7t,"â€œTakes on strict haloâ€ at about half the performance (:  Title aside, this looks pretty great.",Intel,2026-01-12 08:54:08,3
Intel,nzi9w6l,"Amd hasnâ€™t even reached 30% market share mobile yet (oscillating between 20% and 26% since 2020) and are about to be almost wiped from existence again save some low end designs using Ryzen â€œAI 7â€ 445 (6 core, 2+4, 4CU iGP).",Intel,2026-01-14 08:34:10,1
Intel,nznrlo6,"The performance looks fantastic for high end $1k handhelds.    But the ""80% faster than AMD's 890M"" claim is absolute bullshit.  They tested against the HX370 with LPDDR5 5600.  That said, against an 890M that *hasn't* been crippled, it should still be 40-50% faster which is great.",Intel,2026-01-15 02:38:40,1
Intel,nz7d64j,"Will intel make it affordable for consumers though, or price it like LNL (2000+ USD laptops and up)",Intel,2026-01-12 18:17:35,8
Intel,nz7r7or,"""compare"", it is half the performance. Still good for what it is, assuming it is priced right",Intel,2026-01-12 19:20:44,3
Intel,nz5loww,"Well it's not just memory bandwidth. It's got about as much bandwidth as it needs to feed the Xe3 cores.   Panther Lake's GPU tile size is only 54mm^2 while Strix Halo's GPU is 308mm^2. For Panther Lake to compete with Strix Halo it would need 2-4Â times as many Xe3 cores probably. That'd be expensive. There's a reason Strix Halo is so expensive and kind of low volume, bigger CPU more RAM and more expensive motherboard aside.",Intel,2026-01-12 13:04:48,13
Intel,nzbn738,DDR6 can't come too soon for igpus too. But in reality memory bandwidth will stay an issue for a long time. Of course cramping enough compute power in such a format is an issue too,Intel,2026-01-13 09:02:19,1
Intel,nzwms44,Framework desktop motherboard?  https://frame.work/products/framework-desktop-mainboard-amd-ryzen-ai-max-300-series?v=FRAFMK0004,Intel,2026-01-16 12:02:44,1
Intel,nz5dj5h,"Half the performance, half the power, (more than) half the price.",Intel,2026-01-12 12:08:11,16
Intel,nzd3th0,I preordered X7 358H laptop for 1300,Intel,2026-01-13 15:11:48,3
Intel,nzgpiya,Bro nothing's going to be affordable in computer hardware at this rate,Intel,2026-01-14 01:50:29,2
Intel,nzi9j83,There are plenty of LNL laptops around 1000 what you on about,Intel,2026-01-14 08:30:40,1
Intel,nzoqfoa,Lunar lake is in sub 800$ laptops now and 12xe cpus are available for sale 1300$ despite the ram and cpu shortage.   The comparison is good even before likely price hikes for strix halo,Intel,2026-01-15 06:39:34,1
Intel,nz9efnf,"Weâ€™re talking mobile chipsets here, strix halo is what happens when you throw efficiency out the window, with Power (TDP) range, typically from 55W up to 120W. The ultra H 300 has default TDP of 25W, with Maximum Turbo Power (MTP) going up to 65W-80W. Intel has a better design, if they threw 40 XeSS3 cores on it, it would prolly run circles around Strix.",Intel,2026-01-13 00:08:25,4
Intel,nzhjs1h,"Strix Halo is double the die size, this should be compared to Strix Point.  But price will tell everything.",Intel,2026-01-14 04:53:30,1
Intel,nz5xoxm,"> It's got about as much bandwidth as it needs to feed the Xe3 cores.  GPU's will take all the bandwidth you can feed them. It won't help EVERY benchmark, but it will help many.  I'd rather see 256-bit bus on something like this. maybe 192 since you can do that with LPDDR5X etc.",Intel,2026-01-12 14:13:42,4
Intel,nz7ptl5,">Panther Lake's GPU tile size is only 54mm^(2)  is this confirmed for the bigger tile?  edit: also, Halo has all the IO, en-/decoders, etc. in the ""GPU"" tile, so the comparison isn't quite valid",Intel,2026-01-12 19:14:20,2
Intel,nz5j09r,And about four orders of magnitude more availability.,Intel,2026-01-12 12:47:16,14
Intel,nz8m3ma,"> (more than) half the price  Have we seen pricing? Not doubting it, I just haven't seen anything personally but probably missed it.  Strix Halo does seem to be a pretty mythical chip due to its price.",Intel,2026-01-12 21:44:42,1
Intel,nzanyas,Keep in mind that each Xe3 core is about as wide as an AMD WGP. We're looking at 1536 vs 2560 shaders. The B390 is 60% as wide as the 8060S. 20 Xe3 cores would match the 8060S in width. 40x Xe3 is as wide as the 7900GRE.,Intel,2026-01-13 04:15:53,4
Intel,nzgqkco,"Bingo  Strix is also limited by being RDNA 3.5 and no FSR4, so it's rather dependent on raw throughput, and it can't possibly fit in a comfortable handheld that would last for more than an hour and a half under load.  I really, really appreciate what AMD has done historically in the APU space, but it is genuinely time for vendors to start considering Intel. The strides here are absolutely immense. They went from an iGPU being a thing that can do basic graphics and 2D gaming to something that competes against lower end NVIDIA parts at less power draw and can actually legitimately game. It's bonkers. In mobile it's a no brainer.  Of course, it's going to be interesting seeing AMD's next UDNA architecture and what they can pull off, but competition never hurt nobody, and it was sad seeing AMD stagnate in the APU space of all things, their bread and butter that gave them pretty much the entire console market plus the Steamdeck. The entire Windows and Linux handheld market has been nothing but AMD for years. This is even better than Lunar Lake.  We're getting to the point where Intel could legitimately compete in the home console space and make a really great product, but realistically they can't undermine AMD's relationship with vendors at this stage. I hope they keep it up, it would really be cool to see an AMD vs Intel APU console war generation.",Intel,2026-01-14 01:56:20,2
Intel,nzixqmc,It's about 50% bigger die with 1 CCD (which seems comparable CPU performance),Intel,2026-01-14 12:07:34,1
Intel,o21p2wl,https://youtu.be/X9eYQTkzxqU?si=Uzdz-E3QJzzVM42N  Not the only one comparing. Intel actually outperforms at lower wattage.,Intel,2026-01-27 17:00:16,1
Intel,nz610q1,">GPU's will take all the bandwidth you can feed them.Â   Didn't deny that. But 12 Xe cores is presumably considered the sweet spot, that's all I'm saying. And Strix Halo only has twice as much bandwidth to feed a GPU die 6 times the size of Panther. I'm sure it has more cache, but still. I think Intel would consider triple or quad channel memory not worth the costs. It would require new i/o, new pins, new motherboard, more RAM, and all, for what's essentially the lowest volume product.  Besides, Intel already has Nova Lake AX in the backlog, or whatever it's going to be called. Practically intel's strix halo. It'll have Xe3P cores, more powerful than Xe3, thus deemed more worthy of the halo treatment.",Intel,2026-01-12 14:31:23,2
Intel,nzbnge8,"This is a big of exaggeration, as you can see with Nvidia moving to gddr7. While bandwidth has increased substantially, performance is clearly limited by lack of compute power",Intel,2026-01-13 09:04:52,1
Intel,nza15gk,"No official confirmation yet, but JayKihn leaked the tile size for the 12Xe SKU last year. Another user somewhere else said the 4Xe GPU is 33mm2.Â Â  https://x.com/jaykihn0/status/1812898063502938260   And even without the PHYs and NPU, from what I see, Halo's GPU tile is still like almost 3 times as big. So yeah, it's on another class, that's my whole point.",Intel,2026-01-13 02:11:11,2
Intel,nz68s35,Yep,Intel,2026-01-12 15:11:22,2
Intel,nzao8ks,"I have a pre-order in for an MSI 14"" at B&H for $1300. 358H, 32GB LPDDR5X-9600, 2TB, 1200p OLED. I've seen some lower-end PTL laptops rumored around the $900-$1k starting range, but those are likely the 4Xe chips. Wildcat lake with its tiny 2Xe GPU is probably going directly into the budget sector.",Intel,2026-01-13 04:17:34,2
Intel,nzdgaj0,Good catch.,Intel,2026-01-13 16:10:17,1
Intel,nzhkchb,AMD is dormant on the APU space since it had basically the monopoly for x86 because Intel was just bad.  They are taking one of the old Intel's book by releasing rebrands and reashes,Intel,2026-01-14 04:57:28,2
Intel,nz657hj,"> But 12 Xe cores is presumably considered the sweet spot  By what? much larger Xe3 GPU's exist.  We have nothing to compare against in Intel-iGPU-land that has 256bit memory.  Strix Halo die size isn't the metric you want either. It's only 2x the fps (and who knows, panther lake could be 2x its own fps with doubled memory bus, but we'll never know, because Intel won't release a strix halo competitor)",Intel,2026-01-12 14:53:19,0
Intel,nze3jni,"Halo's die is still quite a bit bigger, but from the Intel side, you need to include IO, GPU and about half of the compute die which has the MCs, encoders / decoders, etc. to match the ""GPU"" die of Halo, so it is more like 200mmÂ² to 300mmÂ² when compared",Intel,2026-01-13 18:08:18,1
Intel,nzkoj06,AMD is paying more attention to NVIDIA for sure. ESP on the data center side.,Intel,2026-01-14 17:33:11,1
Intel,nz696vi,">much larger Xe3 GPU's exist.  The biggest one for the moment is on Panther Lake X CPUs. I wouldn't know if there's something bigger tbh.  >Strix Halo die size isn't the metric you want either.\\  Sure you can't compare two different architectures. But all I need to know is it's faaar bigger.  >panther lake could be 2x its own fps with doubled memory bus  That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.",Intel,2026-01-12 15:13:24,1
Intel,nzgcub2,"Well Panther uses mixed processes, and hybrid tiles are bound to be a bit less space efficient than putting everything on a single die. And to be fair, Halo GPU uses N4P process while Panther GPU uses N3E So, still not directly comparable.   Gotta say though, Arc's PPA has improved a lot since Alchemist and Battlemage.",Intel,2026-01-14 00:38:45,2
Intel,nz753k2,"> That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.  Yeah, it doesn't matter how much memory bandwidth you have if the GPU doesn't have the raster performance to keep up with the flow of data. Case and point, AMD's R9 Fury X. Released with 4096bit bus HBM. Had a total memory bandwidth of 512GB/s. Yet the GTX 980 Ti released with a 384bit bus and 336GB/s memory bandwidth and it out performed the Fury X in pretty much everything.   That said, I have no idea how close the iGPU is to being bandwidth bottlenecked at 1080p. But I very much doubt doubling it would also double the frame rate.",Intel,2026-01-12 17:41:14,5
Intel,nziy0qm,"Yeah, I think Intel has done a great job with the improvements, I just don't want to overhype things.",Intel,2026-01-14 12:09:36,1
Intel,nyet7we,"I wonder if Valve would be considering Panther Lake for a Steam Deck 2. It sounds like a generational leap from RDNA2 (which is what they were looking for), and can be cheap enough for valve to slightly subsidize the cost.   I guess the main thing is the power envelope. I think Valve is only interested in making handhelds that deal with a 15W TDP or lower",Intel,2026-01-08 15:18:52,31
Intel,nyepcic,"Series 3 seems like the biggest Intel W in a long while.   Plays AAA games at 45-60fps with upscaling, and all other games at native at 60+. Not to mention this is 50% faster than AMD's equivalent HX370 while being massively more power efficient.   There's also already laptops listed on sites like Best Buy for reasonable prices (sub $1300). Compare this to the AI 395 from AMD that can't be found for less than $2500 while being significantly less power efficient. Granted, that APU isn't really comparable.",Intel,2026-01-08 15:00:39,33
Intel,nyf9pbk,"the handhelds with these chips are going to slap. Also people gotta remember that using upscaling on smaller screens especially 7-9 inch screens is a lot more tolerable.  and these benchmarks are of triple A titles, monster hunter wilds is a dogshit unoptimized game, and people will be playing a mixture of game from older titles, indies, emulation etc.",Intel,2026-01-08 16:32:58,10
Intel,nyhyhe2,"So remind me, the ARC B390 is not a discrete GPU?  although the ARC B580, and B570 are discrete GPUs?  and the ARC A380 is a discrete GPU?",Intel,2026-01-08 23:45:30,1
Intel,nyejwbi,"That should say ""playable at 540p""",Intel,2026-01-08 14:33:53,-7
Intel,nyf0j6v,"Linux driver support is the only problem at the moment. Lunar lake is already competitive with Z2 extreme in gaming in windows, but not even close in Linux. Hopefully this changes by the time handhelds with panther lake come around",Intel,2026-01-08 15:52:10,22
Intel,nz26s22,There's an Xbox project within the coming years apparently.,Intel,2026-01-11 23:05:33,1
Intel,nygbkt8,I think Valve is looking for an ARM chip. Intel could dip their toes in this market before Qualcomm catches up to Apple.,Intel,2026-01-08 19:16:45,-6
Intel,nyf1d4m,"Yeah, amd went to extremely greedy!",Intel,2026-01-08 15:55:52,7
Intel,nyg345w,AMD should have made a RDNA4 igpu... RDNA5 igpu is gonna be a huge boost.,Intel,2026-01-08 18:40:16,6
Intel,nyg3cne,AMD should have made an igpu from rdna4...  but nope... the engineers are too busy making AI gpus.,Intel,2026-01-08 18:41:17,4
Intel,nyfhqqg,"In a year or two,when they can get panther lake for cheap, this could be resolved. They could even get more efficiency with a refined 18a for the CPU and whatever node is available for the GPU. With the current RAM prices I wouldnâ€™t expect any major console style updates until 2027 anyways",Intel,2026-01-08 17:07:42,6
Intel,nygdjk9,Do you think there's any particular reason why they would want to go with ARM? Quite sure Intel proved here that efficiency is essentially equal between both ARM and x86 here,Intel,2026-01-08 19:25:24,7
Intel,nyhii5z,It's hilarious to think Steam would work with Qualcomm.,Intel,2026-01-08 22:26:42,4
Intel,nyhzkyv,"Very unlikely. Proton and DXVK work alright with x86 but adding ARM conversion on top is, uh, a **very** poor experience. In fact that first generation of Snapdragon laptops had among the highest return rate of laptops I have ever seen. Amazon [literally warns potential customers](https://www.tomshardware.com/laptops/snapdragon-x-powered-surface-laptop-7-gets-frequently-returned-item-warning-on-amazon) about it.  Valve definitely wants a wider adoption rate of SteamOS and ditching x86 is not going to help that. They are **not** Nintendo that can ask devs to target their architecture. It's going well so far because for most games you can just make a standard Windows version, slap Proton and it works within 10% of native performance under optimal circumstances. Anything that can increase incompatibility rate (and it VERY well can, ARM **does not** support AVX2 natively for instance meaning [a lot of games that might have issues](https://www.reddit.com/r/macgaming/comments/1dekmtz/avx2_game_list/) even starting or underperform).",Intel,2026-01-08 23:51:14,3
Intel,nyjz9hz,i'm so tired of people throwing ARM around for everything. superficial much?,Intel,2026-01-09 06:55:17,3
Intel,nyfnu8h,"Intel and AMD especially have this habit honestly, they hit the lead and stagnate *bad* and the catch up for whatever company stagnated takes a hot minute, though hopefully this level of pushing boundaries in the handheld PC space keeps both of them moving at a steadier pace for a while rather than one team moving way the fuck forward",Intel,2026-01-08 17:34:32,4
Intel,nyincie,why didn't they do one? Why use tech that is so old?,Intel,2026-01-09 01:55:33,3
Intel,nyi9wg9,Yep AMD ces presentation dry as a bone until that rack announcement when ceo turned giddy.,Intel,2026-01-09 00:44:04,2
Intel,nygxlyp,"Even more efficiency that doesn't exist yet. Snapdragon elite can do some heavy workloads for several hours at a time, but it's not powerful enough for heavy gaming. Valve says they know what they want, it's just not ready yet. I think as great as Panther Lake benchmarks are, the biggest complaint of the Steam Deck is still the 2ish hours of battery life in more demanding titles.  With the existence of other handhelds already, I don't think Valve is trying to aim for the beefier spec department here, if it means the same battery life.",Intel,2026-01-08 20:54:40,-2
Intel,nyidygm,"I'm not sure if this is sarcastic or if you missed the memo, but their new VR headset coming later this year is powered by a Snapdragon 8 gen 3.  [https://store.steampowered.com/sale/steamframe](https://store.steampowered.com/sale/steamframe)  A Qualcomm based steam deck isn't out of the question.",Intel,2026-01-09 01:05:28,3
Intel,nynct2x,"Valve's stand-alone VR headset uses Snapdragon, and x86 emulation, so will be interesting to see how well it performs",Intel,2026-01-09 19:07:13,2
Intel,nylpddj,"It depends. For a PC, yes. For a handheld, I don't think so. For the future of enterprise notebooks, probably, especially since Apple has been doing it for a while.",Intel,2026-01-09 14:38:38,1
Intel,nykkj1q,Save cost. AMD's assessment is that no one in mobile cares about gaming and if they do they should just get Strix Halo or build a PC.   cheapskate AMD as always.,Intel,2026-01-09 10:06:00,3
Intel,nyndytz,"probably didn't want to bother redesigning the APU without also having a CPU upgrade, it's expensive after all, and takes resources from other projects",Intel,2026-01-09 19:12:31,2
Intel,nyoo6vt,"""Even more efficiency that doesn't exist yet.""??????????????",Intel,2026-01-09 22:48:02,1
Intel,nyk1lrj,"But Qualcomm drivers on Linux are even worse than Intel lol. The custom AMD APU on the Steam Deck had the advantage of great Linux driver support for gaming(not just being able to support the GPU hardware and benchmarks, but run games at good performance which Intel still can't match). No other company has both high performance GPU and good Linux graphics drivers",Intel,2026-01-09 07:14:57,3
Intel,nyo6aao,isn't the APU a modular unit where they could just put the new one into the same die space?,Intel,2026-01-09 21:23:15,2
Intel,nyp4s4e,"Not my words. I'm taking Valve's. They said they know what they want, and if it was Panther Lake, we'd know already.",Intel,2026-01-10 00:15:40,1
Intel,nyo9nvh,"The Strix Halo is that basically, but the normal Strix Point APUs (e.g. HX 370) are not. The Strix Halo follow-up, Medusa Halo, is slated for 2027, and to use Zen 6 and RDNA5. While the Strix Halo could benefit from FSR4 if it got an RDNA4 update, it's still way stronger than the B390, even at similar power.",Intel,2026-01-09 21:38:47,2
Intel,nytzm0l,Medusa Halo = 2028,Intel,2026-01-10 18:55:32,1
Intel,nx9rf7h,How is the AI running on the B50?,Intel,2026-01-02 15:55:47,3
Intel,nxaevc4,Where did you purchase the B50?,Intel,2026-01-02 17:45:39,1
Intel,nxc8zfu,nice case,Intel,2026-01-02 23:05:56,1
Intel,nxlmpl5,"love the size of it. love the psu, are there any psus in this formfactor that are more powerful?",Intel,2026-01-04 09:46:01,1
Intel,nxsj613,100Â°C,Intel,2026-01-05 09:52:28,1
Intel,nx9s5yu,"cute fan lol  like other user, how is the B50 performance?",Intel,2026-01-02 15:59:17,1
Intel,nyktz5u,"surprisingly fast, it has its own suite on windows, but i want to use it in linux, trying to figure out how as im not that good ta linux.",Intel,2026-01-09 11:27:16,1
Intel,nxeazfq,"weird comparison. the mac mini is the real beast and its in part thanks to not having to cater to OEMs, but the 48gb mini pro is $1800 vs this $350 drop in card so that's a strange comparison.   m5 in the ipad has only about 150gb/s of bandwidth. good for light inference but I really doubt its practical for actual scale production.",Intel,2026-01-03 06:46:02,6
Intel,nxahmts,"As an ardent and lifelong Apple hater, I must admit that they will probably come out much stronger and on the very top of the current chaotic situation if they manage to keep the current price/perf ratio of their offerings. Even with the Apple tax, they are unmatched right now.",Intel,2026-01-02 17:58:24,4
Intel,nxlml7c,"apple stuff is hard to get used to for many pc nerds and mechanical engineers and engineers in this field. When pro software like catia/nx nativly will work nativly on arm then maybe the big car/air/motorcycle/""every day crap all around us"", then product developers will adopt arm/apple. but right now x86 is the king for these guys/this sector that design all stuff u see around u.",Intel,2026-01-04 09:44:53,2
Intel,nxb0egt,"its not hard, you can literally just buy them on newegg",Intel,2026-01-02 19:25:10,4
Intel,nxbf6k9,"[https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007](https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007)  They're finally back in stock as of this reply, though likely not for long.",Intel,2026-01-02 20:37:05,3
Intel,nxvcb13,"It's the Flex ATX form factor. I think the most powerful one that is also reputable is the Enhance ENP-7660L, which is 600w.",Intel,2026-01-05 19:23:31,1
Intel,nzf8l4a,Around 65c-75c under load,Intel,2026-01-13 21:15:43,1
Intel,nxs9peb,"a lot of commercial software also just does not give a shit about improving, and I don't mean that as a defense for apple.  after effects is just ass for a 2025 product. basic filters are still using legacy code and memory management is horrible. like you're not going to clean 64gb of memory until I hit 96gb utilization, then you're going to slow to a crawl and maybe crash because of threadlock? why even bring back MT rendering? 3rd party scripts people wrote in their basements outperform this stupid thing. spoofing multiple instances and then stitching the results works better than just running the software, its baffling.  anyway yeah, there's a lot of good to x86 and not having to reinvent the wheel, but god damn if so many companies are using it as an excuse to resell garbage.",Intel,2026-01-05 08:22:17,1
Intel,nxgupsq,"It's not exactly weird. They went PPC before Intel because powerpc was more effective for workloads most people used macs for. The switch to Intel was just because Intel had node leadership and performance leadership. Instead stick to A-chips for low power mobile where intel gave up on servicing. Intel loses node leadership to TSMC for a long time, Apple moves on to everything in-house is a pretty logical progression.  Apple having bespoke solutions isn't new either. they've been doing it since their G workstation days. Their current situation is pretty much on brand for apple, but the difference is the huge mistakes intel made (particularly firing so many top engineers) that led to staff fleeing to other companies, including leadership at Apple processor design.  basically apple did their own thing as usual and did a great job don't get me wrong, not taking anything away from apple. the biggest difference however was intel's CEO and board destroying the company.",Intel,2026-01-03 17:07:56,1
Intel,nyqxyvg,"> Memory - The Core Ultra X9 will feature soldered Dual Channel LPDDR5x 9600 MT/s memory up to 96GB   96GB of RAM? So it's $20,000?",Intel,2026-01-10 07:03:20,15
Intel,nyodemu,"Great, want one!",Intel,2026-01-09 21:55:54,2
Intel,nywvp1h,any plan for Wildcat Lake variant for very cost effictive mini PC solution?,Intel,2026-01-11 03:56:05,2
Intel,nyxxa7v,I hope it costs under 1600.,Intel,2026-01-11 08:40:19,2
Intel,nzlnhoq,Hadn't noticed this on the first read: there are separate SKUs for HDMI vs DisplayPort. Will all of the models (X9/X7/U7/U5) be available with both options?,Intel,2026-01-14 20:09:33,2
Intel,nyovlyi,It will be very interesting to compare it with the Nuc 15 pro. I am currently reviewing this model with a U5 225H processor. And I know what this processor is capable of in combination with ARC 130T.,Intel,2026-01-09 23:26:34,1
Intel,nyr50we,any word on availability?,Intel,2026-01-10 08:06:45,1
Intel,nz7zsaj,5x4 nopeâ€¦ stick with the 4x4 thatâ€™s been around for a decade at this point.  Any increase in size just dilutes the meaning of NUC,Intel,2026-01-12 20:00:12,1
Intel,nzknjzr,Man I wish they'd move on to 10Gb NIC already. I would be all over these for lab and canary. Using a thunderbolt dongle is miserable.,Intel,2026-01-14 17:28:45,1
Intel,o11azs2,"Am I seeing correctly ...your specs say Bluetooth 6, but your event photo shows Bluetooth 5.4 ðŸ˜¬",Intel,2026-01-22 12:34:06,1
Intel,o2iliz1,"This looks like a nice NUC for Home Assistant, Frigate, Openvino to run local LLM and Detection because of the Nvidia 4050'ish performing iGPU and AI focused optimizations.  Something I can leave on like an appliance for the cameras and IoT smart home integration.  But if it's priced too high and I can build a desktop with a dedicated GPU for the same money that will be easily upgradeable using standard ATX parts... it might not be worth it.",Intel,2026-01-30 00:51:16,1
Intel,o3haoyd,Unobtanium. No memory.,Intel,2026-02-04 05:17:08,1
Intel,o66kuq7,who is gonna by this piece of junk. Newegg has one with Core Ultra 7 356H listed and priced at $999 BAREBONE :( Apple M4 Mac Mini start at $599 ready to run,Intel,2026-02-19 03:46:29,1
Intel,o6bfqk8,Are the HDMI ports TMDS or FRL?,Intel,2026-02-19 21:56:33,1
Intel,nyryfbb,"Ok atleast this doesn't have co pilot button,plus one to that",Intel,2026-01-10 12:30:39,1
Intel,nzad5zk,"Nothing to share about that. I see some news about that platform, but nothing has been shared with me internally to say one way or the other. For these ASUS NUC models, they are all listed from the Intel Core Ultra 5 and higher.",Intel,2026-01-13 03:15:32,3
Intel,nzlkg2h,"I suspect this would wind up being ""NUC 16 Essential"" to replace the Twin Lake-equipped NUC 14 Essential.",Intel,2026-01-14 19:55:40,2
Intel,nyudelu,"It's mentioned in there, but late Q1 - early Q2 is our current target.",Intel,2026-01-10 20:02:42,5
Intel,nzjltxw,"5x4 yes, high TDP no",Intel,2026-01-14 14:32:17,1
Intel,o13l2qw,"Ah, I see the issue. I took these snaps from one of our product videos at the show. The display was featuring all of our products, and the one that lists BT5.4 was for the ExpertCenter PN55 (you can see the copilot button on it to compare). The other information, including the spec one-pager below lists BT6.",Intel,2026-01-22 19:10:41,1
Intel,o66rr0s,"I mean, if the Mac Mini M4 does what you need it to do, and it's priced well for you then maybe that's a good option.  But you can't really say they're the same. The Mac Mini M4 at $599 only has 16GB of RAM and a 256GB SSD, single Gigabit Ethernet, WiFi 6E, BT5.3, 5 USB ports total with 3 video outputs (over DP1.4). USB-C only, no USB Type-A. 1 Year Warranty. You can scale up to $999 on the Mac Mini M4, but even there it comes stock with 24GB of memory and 512GB SSD. However, it does have a 3.5mm headphone port, which the ASUS NUC 16 Pro, alas, does not.  We will also have Mini-PC versions that are ready to run out of the box. Also, the NUC 16 Pro listed on Newegg you referenced is listed by a third-party seller, and not officially by us.",Intel,2026-02-19 04:31:33,1
Intel,o6boe6a,"On the product page, we list them as HDMI 2.1, which would imply FRL.",Intel,2026-02-19 22:41:32,1
Intel,nyv67be,"If I were to buy one, are these like just hardware or do they include an OS with all the drivers installed out of the box?",Intel,2026-01-10 22:26:12,1
Intel,nyx7821,"Thanks, is that for global availability or just US? Also any word on what the lowest spec SKU will start at?",Intel,2026-01-11 05:05:43,1
Intel,o1l1llm,"Apologies, this is not a Nuc question, but any idea when the Zenbook Duo's with Panther Lake might ship? And any thoughts to a version with 64GB RAM? I use After Effects a lot, and AE is RAM hungry.  I was going to get a Mac, but I just saw these Zenbook Duo's today and I could be tempted... they look quite cool.",Intel,2026-01-25 08:10:12,2
Intel,o15ze6h,Talk about a poorly timed photo ðŸ§,Intel,2026-01-23 02:30:04,1
Intel,nyvi99v,"They will be available in both types of models. I don't have a full breakdown on which hardware will be included with the complete Mini PC (e.g. with memory, storage and OS), or the Barebone kit (no memory, storage, or OS), but you'll be able to purchase it in either configuration.",Intel,2026-01-10 23:28:53,3
Intel,nzacphp,"I would expect US availability to be around global availability, but that different barebone kits sometimes are configured a bit later.   To your second question, pricing information isn't available at this time, but if you're just asking the specs, I would follow what I posted above.   However, if you go to our global product page, you'll usually find a download link for our spec datasheet (sometimes this doesn't show on mobile + plus the document isn't available yet). This will be a better way to see this. I'll ask our team when the datasheet will be available.",Intel,2026-01-13 03:13:08,2
Intel,o16s6n3,"We do what we can. Thanks for noticing, however.",Intel,2026-01-23 05:25:17,1
Intel,nv0zs3r,"If only Intel had stayed in the memory business!   They'd be enjoying Micron valuations and wild profits and performance from copackaged CPU+GPU+LPDDR of their own design and manufacture...     But no, they'd rather invest billions in buying donuts as a service, or whatever their crazy investements went into.",Intel,2025-12-20 13:16:05,46
Intel,nv0mnlo,"damn an iGPU using 32GB of vRAM, I wonder if they're testing a Panther Lake laptop with 48GB RAM or even more (since X7 & X9 Panther Lake only accepts soldered memory)",Intel,2025-12-20 11:24:26,12
Intel,nv2mj8t,"If Intel is really about to release a B770, honestly the **only thing that could make it competitive is the price**. (FOR ME, competitive in 2026 means <400â‚¬) From a performance standpoint, it would need to undercut existing GPUs quite aggressively to make sense, especially given how crowded the mid-range already is.  That said, Iâ€™m pretty skeptical about how realistic that is. **With the recent RAM shortages and rising memory costs**, pricing a new card competitively while still keeping margins doesnâ€™t sound easy at all. Memory is a huge part of the BOM, and weâ€™ve already seen how shortages can push prices up across the board.  So unless Intel is willing to take a serious hit on margins (which seems unlikely), Iâ€™m not convinced the B770 will land at a price point that truly shakes up the market. Happy to be proven wrong, but for now the pricing question is the big unknown for me.",Intel,2025-12-20 18:47:27,6
Intel,nv30mtq,So there's a 20GB variant. A 28GB variant and a 32 GB variant?,Intel,2025-12-20 20:01:42,2
Intel,nv40zo0,Optane was practically **built** for the type of AI workloads that they're shoveling money at.  If Intel didn't give up literally only a matter of months before GPT released and the bubble began in earnest lol,Intel,2025-12-20 23:28:07,18
Intel,nv29blj,"If Intel stayed in memory business, it would be long dead in the 80s and killed by Japanese memory companies. CPU remains the top niche area with less competition and deeper moat. See how China has quickly come up with their GPU designs? Well it will take at least another decade for them to make 2nm CPUs",Intel,2025-12-20 17:39:17,14
Intel,nv2qfnf,"are people high or something? intel was losing money on optane and their SSD business became irrelevant the minute regular memory manufacturers slammed the market. don't get me wrong, they were some of the most durable on the market, but they were no where near printing money on the memory business.  optane may have survived if their nodes were on schedule, keeping CXL support on schedule, but not because it was profitable.",Intel,2025-12-20 19:07:24,9
Intel,nw63y3k,CXL killed octane itâ€™s that simple. No one wanted to be locked to just Intel. CXL was and is just better,Intel,2025-12-27 10:05:29,1
Intel,nvsy1nn,"I feel like the price has to be more than competitive. If they can undercut competition cards of the same performance by 100 or so (or maybe offer rebates or freebies) they could potentially steal the market in that category. With Nvidia and amd cards being tried and true for many many years, I feel like their marketing needs to grab the attention of consumers in a somewhat drastic way.",Intel,2025-12-25 00:56:50,5
Intel,nvbtvc9,If the b770 is 5060ti levels even â‚¬500 is competitive,Intel,2025-12-22 06:05:31,1
Intel,nw63tp3,Yeah sure it wouldâ€™ve been perfect but CXL killed octane and offers pretty much everything it did while not being loved to just Intel lol,Intel,2025-12-27 10:04:18,1
Intel,nv4153e,"it's not like any of this AI garbage right now is profitable for anyone except nvidia and the hardware companies anyway, it's not stopping everyone from shoveling money into it",Intel,2025-12-20 23:29:02,6
Intel,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
Intel,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
Intel,ny85o2z,Is Tiber cloud gone forever?  https://console.cloud.intel.com/ just gives a DNS error now.,Intel,2026-01-07 16:41:57,1
Intel,o04fx6u,"I have installed new Intel Wi-Fi 6 AX210.NGWG.NV in my ASUS laptop, bcz the old one died and couldnt connect to bluetooth since, WIFI works perfectly fine tho, so i dont know if problem is with drivers or not. Also i would just instal them from Intel, but i live in russia and i dont know any trustworthy sites, so if anybody knows, i would be really gratefull",Intel,2026-01-17 15:56:24,1
Intel,o07e6d2,"How do I know the legitimacy of an Intel wifi card, model AX210? I've been searching for it in Amazon and most are manufactured in Vietnam and China, with varying prices.",Intel,2026-01-18 00:38:33,1
Intel,o0uehap,"I keep seeing mentions of TPM in our system requirements and I'm honestly a bit lost on what it actually does for our security, so who is the best person in the org to chat with to get the full rundown?",Intel,2026-01-21 12:32:18,1
Intel,o29lgdq,"i've got an i7-14700kf on an asus rog strix b760-a with a corsair h100i elite capellix xt (240mm aio) and when i run after effects my cpu temps shoot up to 90 degrees, it probably wouldve gotten higher but i closed it cause it felt too high for what i was doing, i'm looking to undervolt my cpu but i dont know anything about it. i just want something safe and simple (im not looking for an extreme undervolt, just one that would lower my temps & possibly keep the same performance)",Intel,2026-01-28 18:56:54,1
Intel,o3j0qr3,"Killer Ethernet keeps asking me to update, i already uninstall, reinstall, safe mode and reformat my laptop and keeps asking me to update.   [Killer Ethernet](https://imgur.com/a/G3YuvJN)",Intel,2026-02-04 13:40:14,1
Intel,o3v6d3v,"Hi all, if i were to buy a series 2 intel CPU laptop with accompanying NPU and iGPU (likely arc 130 or 140). I wondered if anything mentioned on CES 2026 about the Panther lake software improvement is being guaranteed to get to the older CPUs via software/driver updates. I want to buy a cheaper older laptop now while still getting any supposed/promised gains from Intel than buy a panther lake laptop amid the nonsense ram pricings",Intel,2026-02-06 07:44:05,1
Intel,o597m5l,"Hello Intel, I saw a new microcode bios update for MPG Z790 CARBON MAX WIFI II! This is a new URGENT Bios Fix? Please, let us know!",Intel,2026-02-13 23:48:49,1
Intel,o5i257f,"Hi Everyone,  I have a 13600k (running stock thermals) on an ASRock 690 Xtreme motherboard with Windows 11. I have been having some odd issues in the past 6-8 months where under certain load my computer begins to stutter. It doesn't do it all of the time and it is difficult to pinpoint it. Also coming out of standby some odd things I have been seeing is that my BD-R drive sometimes will not transfer to my HDD and my USB ports will not read external drives.  Is there a possibility that the processor is having issues? I have not seen any crashes. Any way to pinpoint this to what could be causing it?  Thanks",Intel,2026-02-15 13:06:14,1
Intel,o67rbyi,Hi guys. I just put a RMA request in for my CPU and was emailed 10 minutes later saying that my SWR case was created. I got sent a set of instructions for packaging and shipping but no attachment was sent for an AWB label or RCI. Does this come later? Or would I need to contact Intel directly to recieve it?,Intel,2026-02-19 09:36:29,1
Intel,o6cm9pw,"Hello, I purchased an Intel 265K and am looking for a motherboard for it.  I would like to know if B860 motherboards allow us to undervolt or set power limit on the CPU.  I also want to know what we can do with memory?  I'm not really into overclocking the CPU, but the first two are important to me in order to control power and temperatures since I'm putting this in a small form factor mini ITX build.  I can live without detailed memory tweaks, just as long as we can set XMP profile.  If I can save money by avoiding Z890 that would be great.",Intel,2026-02-20 02:01:10,1
Intel,o6ywtey,I can't delete my [intel.com](http://intel.com) account. The request personal information deletion thing said I should be logged out and receive a receipt for this action (I assume this means an email) but this hasn't happened and I can still log in.,Intel,2026-02-23 15:55:34,1
Intel,nxwkozf,"u/Chelostyles Thank you for your inquiry regarding the CPU and RAM upgrade for your Dell Vostro 3670. As much as I'd like to provide my technical insights on this upgrade path, I'm not in a position to provide specific suggestions since this involves hardware modifications to an OEM system.  For the best compatibility outcome and to ensure optimal system performance, I strongly recommend reaching out to your system manufacturer directly. They can provide definitive guidance on supported CPU upgrades (i5-8400 to i7-9700) and maximum RAM configurations for your specific model. We don't want to inadvertently bypass any warranty terms and conditions on your system by providing modification recommendations that might affect your coverage.  Your system manufacturer's technical support team will have access to the exact specifications, BIOS compatibility matrices, and supported hardware configurations for your Vostro 3670 model. They can confirm whether the motherboard supports the i7-9700, the maximum RAM capacity (64GB vs 128GB), and any potential limitations or requirements for these upgrades.  This approach ensures you get accurate, manufacturer-validated information while maintaining your system's warranty protection.",Intel,2026-01-05 22:52:24,1
Intel,nxwjdkt,"u/I_like_carsyay  XPS 15 9530 hardware does support Resizable BAR, which is why Intel's system detection shows it as available for both your Arc A370M and Iris Xe graphics. However, the system manufacturer has designed their BIOS interface to prioritize stability and user-friendliness, often managing advanced PCIe features like ReBAR automatically in the background rather than exposing manual configuration options. This approach ensures optimal system performance while reducing complexity for users. I recommend checking for the latest BIOS updates from your OEM's support site and contacting their technical support team, as they would have the most current information about how ReBAR is implemented on your specific model and whether any additional configuration steps are needed to fully utilize this feature.     I've posted an article below in case you haven't yet come across it:  **Helpful Resources:**  *  [What Is Resizable BAR and How Do I Enable It?](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html)",Intel,2026-01-05 22:45:46,1
Intel,ny3upu3,"u/QunatumLeader Hi, thanks for your interest!Â  You can find and apply for all of our jobs online atÂ [http://](http://jobs.intel.com/)[j](http://jobs.intel.com/)[obs.intel.com](http://jobs.intel.com/). We donâ€™t currently accept submissions via social.Â  Good luck!",Intel,2026-01-07 00:05:20,2
Intel,o0l3yzt,"Late to this, but I'm a 13900K owner. I have not had any issues with stability since applying the BIOS update and haven't noticed any performance loss, so I think this is fine. I did not thoroughly benchmark before and after though, partially because of how high peak temperatures were before the update. I am using a Noctua NH-D15 and a contact frame to reduce CPU temperatures.  Up until a few days ago I would have said that thread scheduling isn't an issue, but then I played the game Maneater and it's basically unplayable unless you use launch options to force the game to only P-cores. There's the Intel ""Application Optimizer (APO)"" utility but it seems abandoned and you can't add your own games if Intel hasn't added a profile. I was a big proponent of E-cores but honestly it seems like a half-baked technology that Intel never put the effort in to support properly. That said I guess I could just entirely disable them if I cared so much, but that's a non-trivial amount of performance to just give up.",Intel,2026-01-20 01:26:55,1
Intel,nya3rq0,Hi u/ConspiracyPhD **Post**Â a question onÂ [IntelÂ® Tiber Developer Cloud Community](https://community.intel.com/t5/Intel-Developer-Cloud/bd-p/developer-cloud)Â forum for further investigation.,Intel,2026-01-07 21:48:00,1
Intel,o0e1nqe,"u/Far-Common2207 In this case, we suggest buying the wireless module from authorized Distributors to mitigate the legit concerns. Other than that, the OEM module warranty is not covered by Intel. For more details, you need to work with the Distributor or place of purchase for support to further verify if the wireless card is legitimate.  Check this article: [Where to find the Serial Number for IntelÂ® Wireless Cards](https://www.intel.com/content/www/us/en/support/articles/000092302/wireless.html)",Intel,2026-01-19 00:35:58,1
Intel,o0ztjvw,"[**Plenty-Solution-3692**](https://www.reddit.com/user/Plenty-Solution-3692/)**, TPM (Trusted Platform Module)** is builtâ€‘in security hardware that helps protect important data on your PC using encryption**. Intel PTT** is Intelâ€™s TPM that lives in the system firmware instead of being a separate chip, but it works the same way. Most PCs from the last few years already have TPM 2.0, sometimes it just needs to be turned on in the system settings. . If youâ€™re not sure how to do that, your motherboard or PC manufacturer should be able to help.  You can check this article for more information: [What Is Trusted Platform Model (TPM) and Its Relation to IntelÂ® Platform Trust Technology (IntelÂ® Pâ€¦](https://www.intel.com/content/www/us/en/support/articles/000094205/processors/intel-core-processors.html)",Intel,2026-01-22 05:04:22,1
Intel,o2kc1kd,"Individual\_War\_129, we do not provide typical temperature operating ranges for each processor or each core, as it can vary based on the system design and workload. Processors have internal protections to prevent against excessive temperatures. Operating ranges below the protection points are highly dependent on system configuration and workload.  In case you haven't come across it yet, you may check the articles below:  [Information about Temperature for IntelÂ® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [What Is Undervolt Protection and How Does It Affect Overclocking in IntelÂ® Extreme Tuning Utility (â€¦](https://www.intel.com/content/www/us/en/support/articles/000094219/processors.html)  [Thermal Design Power (TDP) in IntelÂ® Processors](https://www.intel.com/content/www/us/en/support/articles/000055611/processors.html)",Intel,2026-01-30 07:31:28,1
Intel,o3o6m00,"nfsanton, please be advised that the product you are reporting is an OEM (Original Equipment Manufacturer) device. As such, our support may be limited, since we do not have full visibility into the specific technologies, settings, or customizations implemented by the system manufacturer on your device.  For laptop systems, we strongly recommend installing and using the drivers provided by the system manufacturer, as these drivers are customized and validated to ensure full compatibility with your hardware.  That said, you may also choose to use the Intel generic driver if needed, which is available here: [**IntelÂ® Killerâ„¢ Performance Suite**](https://www.intel.com/content/www/us/en/download/19779/intel-killer-performance-suite.html). Please note that functionality and behavior may vary when using generic drivers on OEM systems.  You may also find this public article helpful: [IntelÂ® Driver & Support Assistant (IntelÂ® DSA) Keeps Showing Available Driver Update Notificatiâ€¦](https://www.intel.com/content/www/us/en/support/articles/000090127/software/software-applications.html)",Intel,2026-02-05 05:53:34,1
Intel,o4kauft,"Phaldaz, Iâ€™ll look into this further and will update you as soon as the information becomes available.",Intel,2026-02-10 04:54:29,1
Intel,o5mzlpw,"u/Infinite-Passion6886, we highly recommend keeping your system updated with the latest BIOS version. Each new release typically includes important fixes, stability improvements, and enhancements that help ensure your hardware continues to perform reliably and efficiently.",Intel,2026-02-16 05:54:40,1
Intel,o5t7cka,"u/jsmith1300, upon reviewing the symptoms youâ€™re experiencing, it appears that they may be caused by instability with the processor. To continue with the troubleshooting, please update the BIOS to the latest version, [21.01](https://www.asrock.com/mb/Intel/Z690%20Extreme/index.asp#BIOS). If the issue persists after the update, please let me know.  May I also ask if the processor has been overclocked?",Intel,2026-02-17 04:21:36,1
Intel,o6cklv0,"u/Careful_Classic_9959, I sent you a private message.",Intel,2026-02-20 01:51:01,1
Intel,o6wll7s,"u/MinimumMarsupial6782, Motherboards are made by thirdâ€‘party manufacturers, so the features can vary depending on the brand and model. Because of this, we recommend reaching out directly to the board manufacturer to confirm what specific options like undervolting, power limit controls, or memory settings their boards support.  For the memory side, the CPU can handle DDR5 up to 6400 MT/s, so you should be able to run XMP profiles within that range without any issues.  You may also find this public article helpful with checking supported motherboards: [How to Find Compatible Motherboards for Your IntelÂ® Boxed Desktop Processor](https://www.intel.com/content/www/us/en/support/articles/000005909/processors/intel-core-processors.html)",Intel,2026-02-23 05:29:52,1
Intel,o72vo35,"Hello [PowerZox](https://www.reddit.com/user/PowerZox/) May I ask when did you request to delete your information. And also, have you tried to check your inbox inside [Intel.com](http://Intel.com) profile for any updates? Kindly send me a DM for the exact link for me to check also since Intel have different login page for different programs or products.",Intel,2026-02-24 04:12:16,1
Intel,nyarzrn,Forum doesn't exist or access denied.  I guess Tiber is just gone now.,Intel,2026-01-07 23:42:22,1
Intel,o0fgizr,Do you know any authorized distributors here in the Philippines?,Intel,2026-01-19 05:37:43,1
Intel,o0zyc8d,"I see, all good thanks for your support!",Intel,2026-01-22 05:39:13,1
Intel,o4kdh9i,"thats much appreciated, thanks",Intel,2026-02-10 05:13:43,1
Intel,o5uplgc,Thanks for replying. I am on version 21.01 for a month or more and still running into these issues. I haven't overclocked the processor and using Intel's thermal recommendations base 125W max 181W.  I actually checked this in the BIOS and it must have been changed after an update. I changed the the long setting back to 125W. At first I thought it helped but the issue is still there.,Intel,2026-02-17 12:08:18,1
Intel,o77c02w,I made the request yesterday. This is the link where I made the account deletion request: [https://www.intel.com/content/www/us/en/secure/my-intel/profile.html#edit-personal-profile](https://www.intel.com/content/www/us/en/secure/my-intel/profile.html#edit-personal-profile)  Using the Request Personal Information Deletion button. Does it take multiple days to delete an account? The popup message said I'd be logged off and sent a confirmation so I assume it would be instantaneous like with most other services.,Intel,2026-02-24 20:39:07,1
Intel,nz1jsfl,u/ConspiracyPhD I just checked the forum and it looks like itâ€™s up and running. Could you try accessing it again using your Intel account?  [IntelÂ® Tiber Developer Cloud - Intel Community](https://community.intel.com/t5/Intel-Tiber-Developer-Cloud/bd-p/developer-cloud)  [](javascript:void(0);),Intel,2026-01-11 21:15:16,1
Intel,o0jlcxj,"u/Far-Common2207 According to the directory, these are the distributors in the Philippines. [Distributor Partners](https://www.intel.com/content/www/us/en/partner/showcase/partner-directory/distributor.html#sort=relevancy&f:@sfdisticountry_en=[Philippine,Philippines,Phillippines])",Intel,2026-01-19 20:45:26,1
Intel,o4r7vk6,"u/Phaldaz, upon checking, the graphics update for the IntelÂ® Graphics for IntelÂ® Coreâ„¢ processors (Series 2) will contain the following:  \- Driver improvements for Arc iGPU (graphics performance, game compatibility)   \- NPU software stack updates (AI acceleration libraries, framework support)   \- General software optimizations that aren't hardware-dependent  However, you would need to consider that the following will not be included on the driver update:  \- Hardware-level architectural improvements (new instruction sets, hardware security features)   \- Significant performance gains that rely on new silicon design   \- Power efficiency improvements tied to manufacturing process changes  If you're primarily looking for good performance now and aren't concerned about having the absolute latest features, a Series 2 laptop could be a smart buy. You'll get most software improvements, and the money saved could offset any performance differences.  However, if you're planning to keep the laptop for 4+ years or need cutting-edge AI performance, waiting might be worth it despite the RAM pricing issues.",Intel,2026-02-11 06:16:06,1
Intel,o600ym1,"u/jsmith1300, thank you for confirming that you're already on the latest BIOS version and that the issue still persists. Iâ€™ve sent you a private message to gather more information.",Intel,2026-02-18 04:54:27,1
Intel,o79moze,"Please wait for 24-48 hours since the request needs to enter our system and it may need some checking first before getting a notification. If you are still having the same issue, please update this post. Thank you",Intel,2026-02-25 03:57:12,1
Intel,nz301xe,"Nope.  https://imgur.com/a/tYRhYoV  Access denied and a nice ""This content is no longer available.""  Guess it's a completely dead project and should be removed from Intel's website.  http://console.cloud.intel.com/ is not accessible.",Intel,2026-01-12 01:35:48,1
Intel,o4wrexm,"this was a great response, thanks very much!",Intel,2026-02-12 01:57:39,1
Intel,nz3b0gd,"u/ConspiracyPhD Please check your inbox, Iâ€™ve sent you a personal message. Iâ€™ve already coordinated your concern with the respective team, and as per their instructions, youâ€™ll need to email them directly.  [](javascript:void(0);)",Intel,2026-01-12 02:33:45,1
AMD,o6p6aqk,"I really hope it's a Lenovo-AMD friction issue, and not AMD just dropping support for what's like a 2yo chip or something",hardware,2026-02-22 01:25:04,192
AMD,o6p3jub,"This ainâ€™t even old, this is completely unacceptable",hardware,2026-02-22 01:07:26,237
AMD,o6p5hav,">Finding the ""blame"" is proving to be difficult, as it could be that AMD is not bothering with new updates, or OEMs are not eager to test their specific configurations.  Was my biggest gripe with ASUS, they continued to sell Xonar sound cards for years even while not offering them active driver support.",hardware,2026-02-22 01:19:48,90
AMD,o6p2ght,Advanced Marketing Disaster destroying their reputation and community goodwill just to save pennies (yet again),hardware,2026-02-22 01:00:34,211
AMD,o6pagc9,"this is zen4  with rdna3 graphics released may 2023????  they don't have anything better yet in apu architectures.  apu graphics for amd are rdna 3 ("".5"") and zen5 is not better than zen 4 outside of the much higher clocks with x3d, which don't apply to those apus.  so amd is cutting drivers for what is effectively a current gen apu????  wtf is going on with this company.  are they having trouble paying the driver team as they march through mountains of ai money?",hardware,2026-02-22 01:52:02,55
AMD,o6ozjla,Can't wait to see how AMD the multibillion dollar underdog Indie company isn't to blame for this.   How its industry standard for a high value item to be unusable with in years of its release.   That its not a big deal people are over reacting.. and the drivers age like fine wine.... etc.,hardware,2026-02-22 00:42:36,110
AMD,o6p4658,This showcases how unviable Windows handhelds are. SteamOS will continue to get have supported drivers as long as Valve is maintaining it.,hardware,2026-02-22 01:11:22,108
AMD,o6ppgnl,As if the Panther Lake handhelds needed another selling point.,hardware,2026-02-22 03:31:30,28
AMD,o6p6pv8,Nvidia 2000 series are still receiving drivers and feature support. lol,hardware,2026-02-22 01:27:47,80
AMD,o6qmxha,Is there an issue with the z1 extreme sku specifically? My 7840hs had a driver update on the 8th of January this year,hardware,2026-02-22 08:05:55,9
AMD,o6p78ak,Seems to be the trend since they canâ€™t be bothered to add fsr4 to rdna3 when the 20 series can use dlss4,hardware,2026-02-22 01:31:07,43
AMD,o6u3s6m,"Absolutely bullshit if true. This is not an old chip, and is still being sold in several products. I really hope this is some sort of miscommunication, because this a REALLY bad look if they hope to keep being competitive in the handheld PC space. Hell, they're basically still selling this chip in a shittier variant as the Z2A, a ""new"" product.   Shame because the Z1E is an excellent chip with surprisingly good gaming performance for an APU. They're not a small company with limited resources, and it is not unreasonable to demand proper support. Oh well, at the very least the drivers for my ROG Ally are fairly mature, so not a big loss,",hardware,2026-02-22 20:38:02,6
AMD,o6p43xc,"AMD dropping support for something follows right behind the certainty of death and taxes. As a productivity use-case monkey it ended any question of Team Red being an option, ever.  ROCM stack users, all couple dozen of them, probably like to play a little game where they order a card and take a bet on if it gets delivered before support is axed.  I jest, but really last I had a peek average support lifetime was ~2-3 years. It's a testament to their marketing departments prowess that somehow they've cultivated an image on Reddit of being being the consumer focused brand. The scrappy little underdog fighting for the common man.  Only to get the ""Well, what have you done for me lately?"" treatment in two years time when they drop support for your hardware.  Causes a tickle in the medulla when you see NVIDIA still supporting cards that have long past the point of being considered e-waste.",hardware,2026-02-22 01:10:59,36
AMD,o6pm0q9,"Wow, and you thought desktop Radeon support was atrocious already compared to NVIDIA keeping Maxwell, Pascal, and Volta going until now (security fixes aside, which they're still doing a while more). This is properly atrocious right here.",hardware,2026-02-22 03:07:52,12
AMD,o6p8sfk,"this is why people should resist when Z1 was released. I had my share of experience on Raven Ridge 2500U and the protest lead to monthly reference driver for Ryzen Mobile that people take for granted today. Funny enough during that time we even cross install desktop APU driver for 2400G just to see if it plays well with our 2500U laptop. Stupidest thing that I heard is that was the first time they had to mainline APU graphics driver while Carrizo, which also is a mobile APU was slightly more than a year older from Raven Ridge",hardware,2026-02-22 01:41:16,19
AMD,o6p77p3,"Hardware vendors (e.g. Lenovo, Asus, MSI...etc.) being lackadaisical about keeping driver pages up to date for things like motherboards, laptops...etc. is nothing new. Shame that these Z series processors are reliant on such drivers though.",hardware,2026-02-22 01:31:01,19
AMD,o6r9z9p,"Man buying a intel handheld pc was definitely the best thing i did,  Because unlike amd specific chipset for mobile, intel handheld pc can install the main pc drivers with no friction, so the support will probably last a chillion years XP",hardware,2026-02-22 11:45:37,9
AMD,o74apee,wtf? fuck AMD. Trusted them to not just obsolete an extremely popular and pretty much brand new cpu,hardware,2026-02-24 11:31:02,3
AMD,o6qb8j6,"It's okay though, AMD is just a scrappy little startup!",hardware,2026-02-22 06:17:25,10
AMD,o6p53jp,What's the source for that claim?,hardware,2026-02-22 01:17:22,12
AMD,o6qdrfx,Why I never consider AMD for my productivity hardware stack,hardware,2026-02-22 06:40:06,6
AMD,o71mrwe,They gonna fk around and open the door for intel again with this dumb shit.,hardware,2026-02-23 23:48:48,2
AMD,o6sv7pp,"and dropping drivers for tech that's not that old, name a more iconic duo",hardware,2026-02-22 17:05:39,3
AMD,o6p78ac,Can we stop dooming until we get an official statement from AMD,hardware,2026-02-22 01:31:07,11
AMD,o6tmdm2,"This is why Iâ€™ll never buy a handheld outside of steam or Nintendo , everything else is treated as yearly e waste onto the next",hardware,2026-02-22 19:11:03,2
AMD,o6wx3vk,Maybe because handheld sell so poorly (and they should). Running PC games on vastly under powered hardware  should be a crime.,hardware,2026-02-23 07:08:41,1
AMD,o6xx2i6,This was based off 1 report of someone allegedly asking someone who doesn't even know or work for AMD at Lenovo customer support. I would take this with a huge pinch of salt since 1) it is based on the 780m chips which are still being supported. 2) it was a south korean representative from Lenovo who stated this and there is no proof. 3) AMD have not officially said anything on the subject. 4) the chip isn't even that old so i highly doubt they will stop support this early on. If there is any truth about it it is more likely a disagreement between Lenovo and AMD if anything and Lenovo aren't the best at keeping their products updated anyway so this could be some underhanded excuse as to why there will be little to no support for the LeGo 1.,hardware,2026-02-23 12:37:54,1
AMD,o6u2k87,Is this story confirmed? doent feel believable to me. why did AMD make a seporate driver for z1e thats different than 7840u/780m.  Im under the impression that the Legion Go 1(z1e) used standard AMD drivers. no?,hardware,2026-02-22 20:31:49,1
AMD,o71n8hh,"ugh, gaming on linux is so frusterating cause Nvidea has terrible support and AMD has no long term support... intel arc is also buggy as hell",hardware,2026-02-23 23:51:23,1
AMD,o6q9ve4,Seems pretty clear that this is Lenovo not updating the drivers.,hardware,2026-02-22 06:05:29,-5
AMD,o6wwmoz,"Clickbaity as most other sources would specify the SKU (Legion Go Gen 1) affected in title, although in general OEMs are slow to push driver updates (Ally / LeGo S)  LeGo Sâ€™ drivers work on LeGo for the time being.  In the long run one should move to Linux as AMD has been blocking Adrenaline drivers (obtained from AMD website) on these handhelds",hardware,2026-02-23 07:04:20,0
AMD,o6tt3h8,"Another dumb made up issue by the tech press trying to bash AMD. You can tell by them saying ""seemingly"" which means ""we will lie here but you can't sue us""  Of course this sub will lose its shit and rant about how AMD is abandoning everything until AMD has to make some announcements that in fact it is not going to stop driver updates forever   How stupid do you have to be to believe that AMD will never again update its drivers for an important less than 2 year old APU? What happened the last time this sub claimed AMD was never going to update RDNA3 drivers? Of course in a week this will all be retracted like the ""RDNA3 will never get updates again!"" bullshit but everyone whining here won't say a word about it because they just like bashing AMD and couldn't care less about whether the things they say are lies or not.",hardware,2026-02-22 19:44:13,-5
AMD,o6st4n7,I rather buy from AMD than from Intel.,hardware,2026-02-22 16:56:14,-3
AMD,o6vfx43,"On Windows it's marginal, right?",hardware,2026-02-23 00:57:27,-1
AMD,o6r36qc,ok but what's the issue?  edit. instead of downvoting maybe consider telling me what pressing issue requires an update right now?,hardware,2026-02-22 10:41:49,-8
AMD,o6pyqxj,Asus is also not getting drivers.  That means its an AMD issue.,hardware,2026-02-22 04:36:40,139
AMD,o6r15f3,Makes you appreciate the Valve & Linux support for the Van Gogh APU.Â    And ironically the same drivers support the Z1 lineup as well.,hardware,2026-02-22 10:22:20,9
AMD,o6pa84y,Literally Zen 4 and RDNA 3 no? 1 generation old CPU and GPU tech getting abandoned already is truly one of the decisions AMD has ever made.,hardware,2026-02-22 01:50:33,145
AMD,o6qmkmq,"Its still compatible with all software and games it was advertised to be compatible with, why does it need updating?  Whats with the obsession with constant updates? Its only in IT that we have come to accept unfinished products, maybe this one is just done?",hardware,2026-02-22 08:02:36,-30
AMD,o6pmlcg,"I had an old Sony laptop that was stuck with early GPU driver versions, all of which had serious bugs in them. It  could not play many games that were released after the driver version. Force installing the OEM driver caused other problems such as display brightness being locked to max with no way to turn it down.",hardware,2026-02-22 03:11:47,23
AMD,o6rlr6o,I've had an Essence STX for over 15 years now and it's still working with support from the unified drivers. Crazy how even when it was basically new the open source drivers were more stable than ASUS.,hardware,2026-02-22 13:16:50,3
AMD,o6x7bws,Said drivers where often broken anyway. I will never let that go.,hardware,2026-02-23 08:46:50,2
AMD,o6q3hft,AMD snatching defeat from the jaws of victory,hardware,2026-02-22 05:12:44,59
AMD,o6p4mz8,Always   Missing   Drivers,hardware,2026-02-22 01:14:24,92
AMD,o6phiwd,"You donâ€™t even need to justify how the zen 5 version is barely better, because the Z2 non extreme is just a renamed Z1 extreme.  The hardware IS sold as current gen product.",hardware,2026-02-22 02:38:02,56
AMD,o6rcbt9,Anything better? Ryzen AI+ 9 395 with Radeon 8060s is arguably the best APU. They do have better things but cutting support for something that hasn't even completed 3 years is wild,hardware,2026-02-22 12:05:41,-2
AMD,o6qs9oq,"This is what happens when people buy into ""underdog"" story... They do not invest only their money, but their identity, and when someone says something objectively bad about AMD, they lose their minds because they think they were attacked...  It's absolutely crazy especially when you read tons of negative stuff about NVidia like when they introduced FrameGen (even though it's a great feature) the whole reddit was full of ""fake frames"", or let me buy it with fake money type of memes, but AMD not releasing FSR 4 on their previous gens, or straight up cutting a driver support for GPUs only a few years old meanwhile NVidia supports their GPUs for like 7-8 years? And even after NVidia released DLSS 4.5 to all RTX cards (up to 8 years old cards)? Nothing...",hardware,2026-02-22 08:56:54,27
AMD,o6p0r22,> How its industry standard for a high value item to be **unusable** with in years of its release.  Holy overreaction. I didnâ€™t realize all Z1 Extremes just spontaneously stopped working!,hardware,2026-02-22 00:49:59,-75
AMD,o6p9plb,"Linux in general will still have updated drivers for the Z1E, Valve contribute to the driver but it's all upstreamed to the Linux kernel and Mesa.",hardware,2026-02-22 01:47:13,72
AMD,o6p8ytq,Here's hoping Intel will at least...,hardware,2026-02-22 01:42:26,25
AMD,o6qw870,this is just an incredibly open goal for Intel Panther Lake handhelds  2x the performance *and* better software support track record?,hardware,2026-02-22 09:35:14,4
AMD,o6pke9i,"Funny thing is amd actually promised new features for rdna3, then after redstone they go back and say actually rdna3 wont get anything[continued support for every radeon gamer](https://www.amd.com/en/blogs/2025/continued-support-for-every-radeon-gamer.html)",hardware,2026-02-22 02:56:57,54
AMD,o6q7fyo,Intel uhd 620 also recieved driver updates lol,hardware,2026-02-22 05:44:54,17
AMD,o6pqsoe,my 750ti was getting updates until last year too,hardware,2026-02-22 03:40:41,24
AMD,o6pnfsc,"Difference is, Nvidia sees themselves as a software company that happens to make good hardware accelerators. AMD barely even understands what software is. They talk the talk in terms of ""our software is open source and cross platform"" but in reality hardware support is a fragmented mess that scares off adoption like the plague.  AMD, if you're listening, _software is the thing that just needs to run everywhere on everything_.",hardware,2026-02-22 03:17:34,26
AMD,o6qpova,>Nvidia ~~2000~~ 1600 series are still receiving drivers and feature support. lol  Fixed,hardware,2026-02-22 08:32:15,8
AMD,o6sx16q,"The Z1 extreme can't use regular web driver releases without some inf file edit so that the installer can identify the chip.  AMD hasn't just suddenly stopped supporting Phoenix Point APUs, this is some crappy edge case with accountability lying somewhere between AMD's customer engineering team and whoever decided to create these gaming-specific APU SKUs to begin with.",hardware,2026-02-22 17:13:48,14
AMD,o6q3wvi,RDNA2 left in the gutter too. Wish I had FSR 4 on my 6950 XT,hardware,2026-02-22 05:16:09,14
AMD,o6q70uo,All the while they continue to make and sell products with RDNA2 such as the Ryzen 100 series (rebadged 6000 series) and Z2 Go.,hardware,2026-02-22 05:41:28,13
AMD,o6piurv,"It is really mind boggling to imagine that GPGPU became a thing TWENTY years ago and AMD still hasn't managed to create a stable, supported, unified API for it. Let's see how long the newest ROCm flaviour will last!",hardware,2026-02-22 02:46:44,22
AMD,o6q7ncy,"I've got a strix halo box, the thing AMD is actively advertising as a rocm llm machine.  Getting rocm to work on it involves actual black magic and will break if you so much as look at it wrong.  It's almost comical how bad it is.",hardware,2026-02-22 05:46:35,12
AMD,o6p4kjz,"fun fact, cuda stuff compiled 15y ago will run without issues on blackwell",hardware,2026-02-22 01:13:58,21
AMD,o6rz2q4,And some people in this sub will still ask why AMD doesn't get more adoption on mobile,hardware,2026-02-22 14:35:13,4
AMD,o6qyecv,The only thing that AMD GPU's division is good at is how they convinced gamers to think that they're fighting for them.,hardware,2026-02-22 09:56:07,10
AMD,o6p5pi2,Lenovo support. Prompted by a question why the latest drivers are now many months old. The latest update released almost exactly just 2 years after the chip was first sold.,hardware,2026-02-22 01:21:15,40
AMD,o6p76h0,Lenovo Korea...It's the very first sentence.,hardware,2026-02-22 01:30:48,17
AMD,o6pi4eg,It doesn't help that Lenovo - Korea announced the Z1E Legion Go will no longer receive driver or BIOS support. There also hasn't been an updated graphics driver for the Z1E since August of 2025.,hardware,2026-02-22 02:41:57,23
AMD,o6pauko,the dooming will continue until moral improves or the ai bobble bursts!,hardware,2026-02-22 01:54:35,2
AMD,o6wwxrj,"Yes and it used to be possible, not anymore (AMD blocked it)",hardware,2026-02-23 07:07:08,1
AMD,o6riq09,GPU driver updates often have game performance optimizations (especially for new title launches) and bug fixes.  I had an old Sony laptop where it was locked to early GPU driver versions and thus could not play certain games that were released (presumably because said games required later driver versions).,hardware,2026-02-22 12:55:49,9
AMD,o6t0ovu,"As a little example, here are the acknowledged Known Issues for the current Ryzen desktop onboard graphics -  * Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible. * Intermittent application crash or driver timeout may be observed while playing Battlefieldâ„¢ 6 on AMD Ryzen AI 9 HX 370. AMD is actively working on a resolution with the developer to be released as soon as possible. * Intermittent application crash or driver timeout may be observed while playing Roblox Player (Car Zone Racing & Drifting) when task switching between media on Radeonâ„¢ RX 7000 series products.  * Texture flickering or corruption may appear while playing Battlefieldâ„¢ 6 with AMD Record and Stream on some AMD Graphics Products.  So, without updates, these issues just aren't getting fixed. There will likely be a tonne more that AMD haven't publicly acknowledged, or yet been made aware of.  On top of that, the latest drivers give  **New Game Support**  * Starsand Island * Avatar: Frontiers of Pandoraâ„¢ - From the Ashes Edition  **Fixed issues**  * Some shadows may not render correctly while playing Call of DutyÂ®: Black Ops 7 on some AMD Graphics Products on Radeonâ„¢ RX 5000 series and Radeonâ„¢ RX 6000 series graphics products. * Texture corruption may be observed when playing Enshrouded while at the first â€œElixir Wellâ€ location on Radeonâ„¢ RX 7000 series and Radeonâ„¢ RX 9000 series graphics products. * Application may fail to launch while playing Diablo 4 with usernames containing non-english characters on Radeonâ„¢ RX 7000 series and Radeonâ„¢ RX 9000 series graphics products. * Some lights may not render correctly while playing Microsoft Flight Simulator 2024 during nighttime scenes on Radeonâ„¢ RX 9000 series graphics products. * Application crashes when using Unreal Engine 5.6 Editor with Lumen HWRT enabled on Radeonâ„¢ RX 5000, Radeonâ„¢ RX 6000, Radeonâ„¢ RX 7000 and Radeonâ„¢ RX 9000 series products. *  Intermittent visual corruption may be observed while navigating Chromium and Electron apps on Radeonâ„¢ RX 5000 series and Radeonâ„¢ RX 6000 series graphics products. * Slow window resizing may be observed (related to vkAcquireNextImage extension) while developing games that use the Vulkan API on Radeonâ„¢ RX 7000 series and Radeonâ„¢ RX 9000 graphics products. * Intermittent audio dropouts during minimized video playback on certain Xiaomi and Hisense TV models on AMD Radeonâ„¢ RX 9000 series graphics products. * Player sprites may not render correctly while playing Cassette Beasts on Radeonâ„¢ RX 7000 series and Radeonâ„¢ RX 9000 series graphics products. * Artifact may appear when playing Baldurâ€™s Gate 3 with TAA enabled on Radeonâ„¢ RX 5000 series and Radeonâ„¢ RX 6000 series graphics products.  This is really a repeat of the first point, but these are examples of the issues these systems will accumulate with software new and old going forward. Audio dropouts, weird glitching in games, sub-optimal performance, simply not working at all, etc. It's not even that a 2025 game or app or whatever will work forever with these old 2025 drivers - OS updates can break things, as can app and API updates.",hardware,2026-02-22 17:31:00,4
AMD,o6q4qnm,can't wait to see r/amd 's way to spin this,hardware,2026-02-22 05:22:49,80
AMD,o6wvwqj,AMD started blacklisting the custom HWIDs for Z1E (technically 7840U) mid last year. It was possible to just sideload Adrenaline before if one so wished. The new drivers should still work on them but idk how to bypass the checks.,hardware,2026-02-23 06:57:53,3
AMD,o6q6lre,And they continue to sell the exact same hardware as brand new product as the Z2 non extreme.,hardware,2026-02-22 05:38:00,75
AMD,o6q1v83,pedantically the GPU is 0.5 generations old ðŸ˜­,hardware,2026-02-22 05:00:11,28
AMD,o6qycg1,"Yeah, and RDNA3 seems to be in same spot unofficially for a while now â€” there will be no FSR4 or full redstone support for it. Even tho we know that full FSR4 version works fine on that architecture (I'm literally playing wuchang souls-like with it at 4k and max settings with ~90fps).",hardware,2026-02-22 09:55:37,13
AMD,o6qwb6h,"AMD will never miss an opportunity to miss an opportunity, nor an opportunity to get themselves the shittiest PR possible.",hardware,2026-02-22 09:36:03,10
AMD,o6qp08q,It's pretty standard these days for new games to need some sort of driver level optimization or tweak to work well. No driver updates means that there's gonna be games coming out that will never work well on pretty much any of the Windows AMD handhelds from that generation. Presumably no Z1E updates would also apply to the 7840u models too...,hardware,2026-02-22 08:25:34,20
AMD,o6qmxau,lol,hardware,2026-02-22 08:05:52,8
AMD,o73xh1z,do you buy pc hardware with the assumption that literally no game compatibility is expected for any game that comes out after the products release?,hardware,2026-02-24 09:31:54,1
AMD,o6t4pxr,"Aye, exactly! It blew my mind the Xonar community was developing their own drivers because ASUS couldn't be bothered to support their own products. Major props to everyone that contributed their time to those open source drivers!  ASUS was still releasing new models of Xonar hardware for more than a decade after the originals, even as they let the drivers stagnate. Something around 30 Xonar products they couldn't be bothered to driver support. Win 10 launched and it took nearly six more months before a beta driver belatedly showed up, then more than a year before a non-beta driver was released and that was the last update for my card. Even so there were some showstopper bugs in the only drivers ASUS released so those open unified drivers proved to be a lifesaver.  Given my other experiences with ASUS being reluctant to dedicate software support to its products I'd be far more inclined to suspect the fault is with ASUS on the delayed/lack of Z1 driver updates. I could be entirely wrong and it's an AMD problem, but ASUS hasn't done itself any favors and this behavior is already one of the reasons why I don't buy ASUS products anymore.",hardware,2026-02-22 17:50:05,3
AMD,o6q9n8c,Can't let Xbox steal the title somehow.,hardware,2026-02-22 06:03:32,15
AMD,o6qnygr,This js a confusing situation for me because there are drivers for the other pheonix apus. My 7840hs had its last driver update in January and its the same chip but clocked differently,hardware,2026-02-22 08:15:36,6
AMD,o6t5yw8,"when i said better, i was mostly thinking about architecture wise. so cpu and graphics architecture.  strix halo (eg ryzen ai+ 9 395) is architecturally basically the same as well. it has zen5 cpu chiplets, which are performance wise about the same as zen4 cores/chiplets. (again only x3d saw real meaningful uplifts due to massive clock boosts for x3d)  and graphics wise strix halo is still just rdna 3 ("".5"")  it is the same graphics architecture as the ryzen z1 extreme. the difference is, that strix halo is just a LOT wider and bigger right.  like a 9060xt vs a 9070 xt. same architecture, but one is vastly bigger and has a bigger memory bandwidth to feed it as well.  also strix halo got delayed a bunch, which may make it seem like it is sth newer, when it isn't.  no longer releasing drivers for the ryzen z1 extreme, but still releasing drivers for strix halo (eg ryzen ai+ 9 395) is like releasing drivers for the 9070 xt still, but refusing to release more drivers for the 9060 xt 16 GB, just because you feel like it.  it is just utterly absurd.  it is the same freaking graphics architecture exactly. so like how long will they feel like supporting strix halo? give it another year and then bye bye drivers, despite them still selling it and it being their fastest chip or some insanity?  it is just completely and utterly absurd to cut support for not just VERY YOUNG hardware, but hardware, that runs the same architectures as your current best and latest released stuff in mobile/handheld as well.  just crazy.",hardware,2026-02-22 17:55:40,6
AMD,o6p73z1,High on Life 2 asks for new drivers and refuses to run on the Z1E as we speak according to user comments.,hardware,2026-02-22 01:30:21,66
AMD,o6p1ddz,Right you think it was a joke or over reaction.   Until you notice that games require newer drivers to function. That the drivers that are released for the Z1 are to old not allowing the game to start thus rendering the unit useless for its intended function.,hardware,2026-02-22 00:53:51,56
AMD,o6ql7rz,Stopped software support means new games will have lower and lower performance vs competitors as time goes on,hardware,2026-02-22 07:49:45,7
AMD,o6q36hh,"Thinking about it, it is pathetic that the product makers themselves can't even provide driver updates nearly as good as the Linux community...",hardware,2026-02-22 05:10:21,33
AMD,o6pbzr7,Do you think so? I have a Z1e in my legion go 2 running steam OS. Like I only recently bought it and hearing that it won't get driver updates already would suck haha.,hardware,2026-02-22 02:01:52,6
AMD,o6parjn,"ah yes intel...  the company, that is switching to nvidia graphics chiplets in a few years...  yip those are gonna keep up with great graphics support for their current apus...  \_\_  all quite shit all around.  i guess the best move for handhelds is to just wait for the steamdeck 2, because valve would give enough of a shit and gnu + linux drivers are different matter, that this shit wouldn't fly of course.",hardware,2026-02-22 01:54:03,-36
AMD,o6qpzra,"A few weeks ago I read a comment that AMD on their showcase will try to sell you the features that they will add in the future, but the moment a new shiny toy is released they abaddon their previous GPUs almost instantly...  And the fact that they did this with RDNA3 & now this just tells me that I should never even look at AMD outside of X3D series.  The fact that 2000 series just got DLSS 4.5 that looks *exactly* the same like on 5000 series just with a bigger performance drop is absolutely crazy. Especially when FSR4.0 INT8 looks worse on previous GPUs compare to native FSR4 released for 9070 XT.",hardware,2026-02-22 08:35:11,21
AMD,o6r8wjr,"It obvious for anybody involved in software development that it was a damage control to avoid bad press, but it worked there were lots of passionate fans defending this move pointing to AMD PR as a proof.",hardware,2026-02-22 11:35:55,3
AMD,o6r9ero,"Terascale 1 owners: ""First time?""  It's been like 13 years yet it still stings.",hardware,2026-02-22 11:40:34,5
AMD,o6qqpoe,"This is what they say when people mention NVidia Tax. It's worth all the money.  I know that people keep mentioning GT 1080 Ti (ofc. a GOAT card), but I feel like people should start mentioning RTX 2080 Ti instead, especially after DLSS 4.5 release. Like if I would have RTX 2080 Ti, I think I would still be playing on it, cuz it would be still able to keep up with most of the games with DLSS. Like this is probably the exact thing I was hoping for back when DLSS was introduced.",hardware,2026-02-22 08:42:10,24
AMD,o6qa0dt,AMD open source software is their excuse to offload software maintenance side to community to maintain.,hardware,2026-02-22 06:06:40,29
AMD,o6qkc2b,"I've been saying this for years. Nvidia's hardware is competitive and technologically superior, but that's not their moat. Their moat is their software. AMD and Intel could magically catch up to their hardware overnight and not much would change.",hardware,2026-02-22 07:41:23,18
AMD,o6rc1h1,"Oh, Nvidia definitely sees itself as a hardware company, but they understand that you have to have decent software to get people to buy your hardware and developers to code the way you want.",hardware,2026-02-22 12:03:17,5
AMD,o6pzoq3,Yeah amds open software is just an excuse to do the bare minimum,hardware,2026-02-22 04:43:46,9
AMD,o6rgwaq,The 16 series was release after the 20 series as low cost alternative without the RT and Tensor cores.,hardware,2026-02-22 12:42:25,13
AMD,o6sx7sb,Thank you for explaining,hardware,2026-02-22 17:14:39,5
AMD,o6p5jll,But hasnt Blackwell dropped CUDA 32 bit support?,hardware,2026-02-22 01:20:12,19
AMD,o6r5wg1,"""Poor Volta""",hardware,2026-02-22 11:07:18,3
AMD,o6pyayn,"Except that's not what Lenovo Korea said. The sources listed in the article link the original Videocardz report, which phrases it as:  >""Lenovo also advises owners to install all system updates through Windows Update and Lenovo Vantage first. For graphics drivers, Lenovo recommends downloading AMDâ€™s latest â€œuniversalâ€ graphics driver directly from AMDâ€™s website, but adds that if the driver is not compatible with the Z1 platform, users should stick to the version delivered through Windows and Lenovo Vantage.""   >""The shared support message does not explicitly say AMD has ended driver support for the Ryzen Z1 Extreme. However, it confirms that Lenovo Korea support says Legion Go drivers are not planned to receive further updates, and cross installing Legion Go S drivers is not supported.""",hardware,2026-02-22 04:33:21,21
AMD,o6p85vy,Is there a link to some statement by Lenovo? Maybe I'm missing it.,hardware,2026-02-22 01:37:13,4
AMD,o6wx7rq,"In fairness, LeGo has not been receiving any drivers updates since late 2024 ish? We had basically gone from sideloading -> using Ally drivers -> using LeGo S drivers -> (moving to Linux TBD)",hardware,2026-02-23 07:09:38,5
AMD,o6qczd9,That pretty much convinces me to only stick with Steam Deck from here on out. None of these OEM's are seriously committed to hand held gaming.,hardware,2026-02-22 06:32:57,5
AMD,o6rkdkb,"yes, I know what updates are, what's why I'm asking",hardware,2026-02-22 13:07:28,-4
AMD,o6qpe9r,They will just say that it was a bug or a misclick like what even AMD themselves did last time when they wanted to stop supporting their GPUs with newest drivers.,hardware,2026-02-22 08:29:20,28
AMD,o6qy0d8,"People on that sub already defending it by saying ""it's to be expected from any manufacturer, they want profit"" and ""it's new reality, live in it"". Those guys either bots or just brainwashed, I tell ya.",hardware,2026-02-22 09:52:23,22
AMD,o6ss296,"Ayymd version: ""amd is just a small indie company compared to how much shintel and novideo spent on driver r&d""",hardware,2026-02-22 16:51:21,1
AMD,o6qhtac,"This is like Chromebooks losing software support purely by running out the clock, not because the hardware is old.",hardware,2026-02-22 07:17:26,15
AMD,o6qmfxi,"Its still compatible with all software and games, why does it need updating?",hardware,2026-02-22 08:01:23,-14
AMD,o745rx7,"I absolutely hate this, as well as that I do own a Rog ally.  But Jesus Christ, a game refusing to run because of a certain driver version is wack.Â   Warning, ok, yes. Refusing to run - unacceptable.",hardware,2026-02-24 10:48:48,3
AMD,o6tml0n,What comments? I have a legion go and haven't heard anything about that. Nothing even comes up when I Google it.,hardware,2026-02-22 19:12:02,4
AMD,o6p21ek,> Until you notice that games require newer drivers to function  Bruh I have a 560Ti in my HTPC and anything reasonably expected to play on it runs,hardware,2026-02-22 00:57:56,-54
AMD,o6va3zh,or they just won't work. A lot of games force you to update drivers now days for what ever reason.  My guess though is they got tired of Why doesn't my game run right support questions. The person running a 2 or 3 year old driver.   While don't think you need to update monthly (cause feels like Nvidia releases monthly drivers). It doesn't hurt to update your driver every 3-4 months or so. To keep things working or if a game demands it of course.,hardware,2026-02-23 00:24:10,3
AMD,o6rd2ul,We just had this thread yesterday   **Linux 7.0 Lands More AMDGPU Fixes For Old Radeon Hardware**  https://old.reddit.com/r/linux/comments/1ratc28/linux_70_lands_more_amdgpu_fixes_for_old_radeon/  The article mentions the  Radeon HD 7790 which is from 2013.  I know most of the people here are hard core gamers but not everyone is and some people are still using hardware that old and it is fine for their uses.,hardware,2026-02-22 12:11:51,14
AMD,o6q92vg,"If you are on steam os or linux, the drivers are a completely different codebase than windows. They are part of the Linux kernel and a package called mesa. So they're basically the most well supported open source projects in existence and will continue to be updated for decades to come.",hardware,2026-02-22 05:58:42,23
AMD,o6phh25,"Even if Valve doesn't update the drivers, there will always be someone out there will take it upon themselves to create them. Even though they make drivers for Windows only, NimeZ provides modded drivers for GCN all the way to Vega making those old cards work (to an extent) with modern games.   I'm sure the same will happen with Linux, especially with AMD being the bread and butter for the handheld market.",hardware,2026-02-22 02:37:41,16
AMD,o6zxqiy,but there are no legiongo2 with z1e.  You most likely have the S steamOS version,hardware,2026-02-23 18:46:25,1
AMD,o6pf3ix,Intel isn't replacing their own iGPU IP with Nvidia. Doing so would be incredibly stupid and leave them at the complete mercy of Nvidia. Intel/Nvidia SoCs will compete in the Strix Halo category.,hardware,2026-02-22 02:22:01,34
AMD,o6qzd0i,"Funniest part â€” full FSR4 does work on RDNA3. On linux, if you have fresh mesa and supported proton, you just need to addÂ PROTON_FSR4_RDNA3_UPGRADE=1 after that FP8 will be emulated using existing hardware for FP16 (as I understand it). It does mean higher overhead when running FSR, but at the same time I'm currently paying WuChang game at 4k and max settings, while having ~90 fps. And yes, FSR4 there way better than TSR.",hardware,2026-02-22 10:05:18,20
AMD,o6sfp21,"Once there is an X3D alternative, I would be very inclined to look there to avoid being subjected to AMDâ€™s support plans.",hardware,2026-02-22 15:56:49,1
AMD,o6qrub1,"Honestly, the 2000 series was a poor purchase on launch being barely faster than the 1000, but with DLSS, the 8GB and larger models have held up pretty darn well over the years.  2070 and faster still provide playable framerates at 1440p at reasonable quality settings on most modern titles with upscaling. Not bad for a GPU series that's coming up on 8 years old.",hardware,2026-02-22 08:52:55,13
AMD,o6qmtdk,"Intel is miles ahead of amd in terms of software and support, they were actually one of the best before nv/CUDA has blown up. This is going to change since Intel fired good chunk of drivers team :(",hardware,2026-02-22 08:04:51,10
AMD,o6qlmgl,"Nah, AMD's open source Linux drivers outperform Windows drivers",hardware,2026-02-22 07:53:36,-5
AMD,o6q32l6,At the same time Nvidia seems to be working on a translation layer for 32 bit CUDA based on the fact they did something like that to bring back 32 bit PhysX support with driver branch 590.,hardware,2026-02-22 05:09:29,20
AMD,o6r1xwt,u had 64bit in cuda 1.0 tho,hardware,2026-02-22 10:29:53,11
AMD,o6qjbne,"I feel the blame falls on both AMD and the OEMs. If we do get the official report that the Z1E is no longer receiving updates, from AMD themselves then the OEMs should share the blame in not fighting back and forcing them to continue. ASUS is the same story, no GPU driver since August. At least Valve provides support, and cares about their tech at least. Makes me love mine even more.",hardware,2026-02-22 07:31:45,7
AMD,o6r5dg2,"It's possible you're referring to now deleted threads, but I only see two posts on the topic in r/amd and that sentiment isn't present in any of the comments.  [https://www.reddit.com/r/Amd/comments/1rb5bhh/amd\_seemingly\_stops\_driver\_updates\_for\_ryzen\_z1/](https://www.reddit.com/r/Amd/comments/1rb5bhh/amd_seemingly_stops_driver_updates_for_ryzen_z1/)  [https://www.reddit.com/r/Amd/comments/1ratcm9/lenovo\_reportedly\_ends\_driver\_updates\_for\_legion/](https://www.reddit.com/r/Amd/comments/1ratcm9/lenovo_reportedly_ends_driver_updates_for_legion/)",hardware,2026-02-22 11:02:24,33
AMD,o6qz1vp,"This is what happens when you make AMD your identity... It's so obvious that reddit just can't accept the fact that NVidia even though I would say a monopoly in gaming space (by Steam's stats) actually delivers support, features for all the cards, and that everything they say about NVidia is exactly what AMD is doing.   Like this was pretty much the biggest shock to me. On reddit people keep saying how horrible NVidia is, which I do agree with e.g. VRAM, but almost everything they say is exactly what AMD is doing. Like AMD's only sellpoint is: ""Guys! See? I gave you a little bit of more VRAM, but please ignore the fact that your GPU won't receive anything new the moment we release a new one!""",hardware,2026-02-22 10:02:21,21
AMD,o6qx3k1,"Because software keeps changing, and there are new Vulkan and DirectX extensions every once in a while.  If you don't support them, you can't run the game, it's that simple   For example, that SoC won't be able to use [any of this](https://www.khronos.org/blog/vulkan-introduces-roadmap-2026-and-new-descriptor-heap-extension)",hardware,2026-02-22 09:43:38,23
AMD,o6vgptk,"/u/PastaPandaSimon are you talking about something extremely recent, or [is this video uploaded a week ago](https://www.youtube.com/watch?v=XcvErNWOqRs) fake?",hardware,2026-02-23 01:02:13,1
AMD,o6p3j7m,The Z1E is less than 3 years old. Everything should be reasonably expected to run. The 560ti had 7 years of driver updates.,hardware,2026-02-22 01:07:19,55
AMD,o6qdqrq,Ohh sick! Chalk another reason I'm about to dual boot Bazzite and windows until I get fully dialed into Linux.,hardware,2026-02-22 06:39:55,1
AMD,o6qsuer,"Lately on Linux [GCN 1 and 2 got a few fixes](https://timur.hu/blog/2025/love-song-for-linux-gamers-with-old-gpus-eoy2025). By Valve subcontractor, because he wanted to start somewhere with Linux GPU drivers. So it's very probable that RDNA will still be fine for the next 10-15 years.",hardware,2026-02-22 09:02:20,7
AMD,o70rqgj,Yep derp.,hardware,2026-02-23 21:08:52,1
AMD,o6qzmff,"My bad. I thought I mentioned somewhere in my comment that FSR4 does indeed work on RDNA3, but the fact that AMD still decided to not release it is really just shocking to me, and this was long before NVidia released DLSS 4.5 to all RTX cards...  It's just shocking how no one says anything bad about them because in redditor's eyes they're the underdog.   Still from what I saw from that leaked SDK it seems that FSR4 looks worse on RDNA3 compare to e.g. DLSS 4.5 on 2000 series vs. 5000 series. Also sadly... There is also a performance drop in FSR4 on RDNA3, so we can't even use the argument that at least it does not drop the performance... :/",hardware,2026-02-22 10:07:47,6
AMD,o6qsjxy,"Yeah, I remember 2000 series being a really poor value especially when Crypto boom was happening, but if people bought it I feel like the overall value kind of made it more acceptable due to these features?",hardware,2026-02-22 08:59:36,5
AMD,o6s8c3p,remember tech jesus and the rest crying over the launch prices and telling people to buy RX5700? lmao,hardware,2026-02-22 15:22:49,4
AMD,o6r64ct,"Oh, nah I'm just blind, my bad. I meant [this thread](https://www.reddit.com/r/radeon/comments/1rb5cal/comment/o6olkda) in r/radeon   Nothing better to start your day from falsely accusing wrong sub",hardware,2026-02-22 11:09:24,26
AMD,o6voqb5,"He's exaggerating, on purpose. The user in question did not defend AMD. They said, ""this is what they are doing, and likely why they are doing it."" Explaining isn't defending.  General-Ad had a bad take and got blasted, quite fairly, for said take.",hardware,2026-02-23 01:50:45,4
AMD,o6qzqpd,"Both are a wash for me. Had regression for VR for many Nvidia drivers, sometimes I roll the dice hoping VR doesn't stutter anymore. Radeon card is old Unity engine game is problematic instead.",hardware,2026-02-22 10:08:55,-7
AMD,o6r19j6,"Unless you slap Linux on it, in which case the community pick up the slack as usual.",hardware,2026-02-22 10:23:28,-7
AMD,o6wjhio,"Im talking about comments on the actual article linked that we are commenting on. I don't have a Legion handheld, so I can't validate them.",hardware,2026-02-23 05:13:24,3
AMD,o6sscms,"That's impressive, and reassuring. RDNAs popularity, and the wider adoption of Linux will surely help on that front. At least, as far as expediting the drivers are concerned.",hardware,2026-02-22 16:52:41,1
AMD,o6r2tmw,">Â Still from what I saw from that leaked SDK   You maybe talking about INT8 version â€” it uses lower precision datatype, basically a cut down version of full FSR4 FP8. As a result it runs way better than full precision version, but at the cost of not being a lot better than FSR3.Â    What I mentioned in my comment is a full version of FSR4 (exactly same as on RDNA4).",hardware,2026-02-22 10:38:17,10
AMD,o6s3u34,Radeon and ayymd are the pcmr of amd hardware. Mainline sub (as well as intel's) is very decent.,hardware,2026-02-22 14:59:59,19
AMD,o6r3680,"It almost like we need a third player on the market for those two to become competitive again!Â    Uh, what do you mean ""we already have third one""?",hardware,2026-02-22 10:41:40,11
AMD,o6rqrz9,"Ironic, because you're downvoted by people that have made Nvidia their identity.",hardware,2026-02-22 13:48:04,-2
AMD,o6raxn2,"Damn I did not know that. Ithought only FSR4 INT8 version was the one working on RDNA3. So AMD can literally do that without even INT8, yet they still didn't even acknowledge this? And people seriously think they work for gamers, yet NVidia was the one who introduced gaming features in CES 2026...  At this point I'm getting the impression that people are talking about NVidia, but for whatever reason they think it's the AMD that does this, even though they do the very opposite...",hardware,2026-02-22 11:53:55,4
AMD,o6vmdqi,Would be nice if they hurried up on the B770 tho. I was choosing between  the 9070xt and 5070ti.,hardware,2026-02-23 01:36:31,2
AMD,o6rg78l,">Â So AMD can literally do that without even INT8   Yep. On linux wmma trick (emulation part specific for RDNA3) was added within first month or two, after FSR4 was released.Â  (all praise opensource devs!)Â    It is true that RDNA2 completely lack needed hardware to run full FSR4 version (it can, but via software emulation, which is sucks), but almost everything AMD said about FSR4 on RDNA3 is a complete lie. My best uneducated guess is that AMD was working on INT8 version at some point, to provide as update for old hardware or maybe GPUs from other vendors, but greed won and they scraped idea completely, locking FSR4 to RDNA4 as key feature to sell more new hardware.",hardware,2026-02-22 12:37:07,5
AMD,o70vrys,"Had they even gave a shit about dGPUs, they'd be on C990 by now with D990 on the way next year.",hardware,2026-02-23 21:28:44,1
AMD,o6rhmee,"I fully agree with this, and that is what I thought as well as that is the only logical conclusion of why FSR4 INT8 wasn't released, or acknowledged by AMD to this day.  The issue here is that *I* as a customer will look on the track record of the both corporations, and compare how long they support their GPUs either with Drivers, or Features, which AMD is absolutely lacking, and in the last 6 months alone convinced me to never look at AMD seriously at all.  Last year I played a few Triple-A games on my RX 6600 (it was a temporary replacement after my GTX 1080 died), and FF7 Rebirth & The Last of Us Part 2 were both having driver issues *for months*, and this exact GPU was part of the ""misclick"" when they announced that they're dropping the driver support. A GPU that was supposed to be a newer GTX 1080, but *better* because of active driver support. A GPU that was only 4 years old when they announced a discontinuation, while GTX 1080 stopped receiving updates in October 2025. 9 Years Old Graphic Card.  Absolutely fucking crazy work AMD...  Whoever that was who made the comment that AMD is just selling a future features during their presentations for thier GPUs for right, and people who look for an upgrade should really look at both NVidia and AMD, and buy whatever is *currently* available for both of these cards, and not *what* will be available in the future. Because I fully believe that RDNA 3 owners definitely feel really fucking weird about this especially after FSR4...",hardware,2026-02-22 12:47:48,2
AMD,o6ghx61,Makes sense they would push it to CES 2027 in order to get Zen 6 rolling earlier into Epyc and Helios installs in the second half of 2026,hardware,2026-02-20 17:36:43,48
AMD,o6gr7eq,If Intel manages to get Nova Lake out the door this year they might finally get the DIY crown back... at least for a couple of months.  Does anybody know more about why Zen 6 would be delayed other than 'RAM expensive'?,hardware,2026-02-20 18:18:47,85
AMD,o6i58dv,Better include the ram in the package motherfuckers!,hardware,2026-02-20 22:21:05,8
AMD,o6gi8os,"At CES (the C standing for datacenter of course), AMD announced Zen 6 based Venice and Lisa holds up the chip. And while Zen 6 desktop was already expected only late 2026 (mobile potentially earlier). Pushing desktop into 2027 would truly be a masterclass of not caring about consumers.   It truly speaks for the datacenter money which causes everyone to forget about consumers.   And if they don't put out at least some decent 3nm mobile chips this year, they have truly lost consumers. That they refreshed their Zen 5 lineup doesnt give a lot of hope though...",hardware,2026-02-20 17:38:12,87
AMD,o6gg988,">Yesterday, we learned about potential Ryzen desktop CPU configurations. That leak was followed by additional reports, including a story from Benchlife, which often adds a small extra detail to its coverage. This time, the site focused on the launch timing for the same platform.  >Benchlife claims AMDâ€™s Zen 6-based desktop Ryzen family, often referred to as â€œOlympic Ridgeâ€ in leaks, is not expected to launch in 2026. The outlet, which has a relatively strong track record, describes 2027 as the earliest window for â€œOlympic Ridgeâ€ on desktop.  >*Olympic Ridge, based on the Zen 6 processor architecture, is expected to continue using the AM5 socket. Taken literally, that would mean current AMD 800-series chipset motherboards could remain compatible. Whether that will actually be the case will likely depend on later confirmation. Olympic Ridge is not expected to arrive before 2027 at the earliest, and AMDâ€™s AM5 socket has been in use since the Zen 4 generation launched in 2023.*  >This does not automatically mean Zen 6 as an architecture is â€œlockedâ€ to a 2026 launch for every segment. AMD can ship Zen 6 first in one product line and roll it into others later, especially if platform validation, packaging, or product positioning changes the order.",hardware,2026-02-20 17:28:55,13
AMD,o6goeys,"Honestly, I'm surprised they don't just launch it late 2026 - the volume they would have to allocate is negligible all things considered because consumers won't be able to buy RAM feasibly, and the only people who are potential buyers are those already on a modern DDR5 platform. Losing the performance crown to Intel in gaming (potentially) and productivity (badly) is not good PR even if it's basically unobtainium for the most realistic upgraders anyway.",hardware,2026-02-20 18:06:23,16
AMD,o6gm91u,Good move. No one is gonna afford RAM this year anyway.,hardware,2026-02-20 17:56:33,13
AMD,o6jeqgq,I can't believe that the Ryzen 9000 series is already 18 months old. That's a long time for CPU staganation considering how little of an imporvement it was compared to the 7000 series.,hardware,2026-02-21 02:49:11,5
AMD,o6j1fvm,Too bad no one can afford to buy any RAM for it,hardware,2026-02-21 01:25:32,2
AMD,o6gptr2,Is Zen 6 DDR5 still or DDR6?,hardware,2026-02-20 18:12:42,5
AMD,o6ignsm,Wonder if Olympic Ridge dies are being shifted to TSMC N2X from N2P? If that's possible.,hardware,2026-02-20 23:23:08,1
AMD,o6kax30,I donâ€™t think anybody wants new platform nowâ€¦,hardware,2026-02-21 06:51:43,1
AMD,o6gllya,"This is not news. That Zen 6 would be 2027 (and probably mid-2027) has been the rumor that's been going around for many many months now. It's always been plausible, what with TSMC nodes info and timing not exactly being a secret. Adding the same rumor/leak to the pile isn't telling us what we don't already probably know.",hardware,2026-02-20 17:53:38,-1
AMD,o6hb0fb,"I just hope the whole package works, the BIOS works, the DDR6 CAMM2 works, no funny business with early adopter penalties",hardware,2026-02-20 19:50:38,1
AMD,o6gf1f8,"Hello InsaneSnow45! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-02-20 17:23:16,0
AMD,o6k2ldb,AMD is never going to catch up to Nvidia.,hardware,2026-02-21 05:39:10,-2
AMD,o6heqlp,As long as this throw away filler release doesn't affect AM6 then whatever lol,hardware,2026-02-20 20:08:50,-3
AMD,o6hnagb,> Does anybody know more about why Zen 6 would be delayed other than 'RAM expensive'?  To use all the CPU chiplets in Epyc systems for datacenters.,hardware,2026-02-20 20:50:58,88
AMD,o6hks3p,"Im optimistic about Nova Ridge, but I'd need a new mobo, so Im probably just going to get Zen6 as it's a drop-in replacement.",hardware,2026-02-20 20:38:38,12
AMD,o6i75me,"Maybe, but feels like a real Pyrrhic victory given the fact Intel will be sending all that money out to TSMC with this big N2 dies while neglecting their own fabs.",hardware,2026-02-20 22:31:14,8
AMD,o6hgh78,"x3d models may be, but I dont think the regluar cache units would be. That should be on core die and part of the CPU litho process, not the memory nodes.",hardware,2026-02-20 20:17:17,15
AMD,o6hnlaz,"> Does anybody know more about why Zen 6 would be delayed other than 'RAM expensive'?  Assuming this rumor ends up being true, the likeliest reason would be precisely that Nova Lake launches in 2027 so AMD has no reason to launch anything in 2026.",hardware,2026-02-20 20:52:28,7
AMD,o6i641q,"Genuine question, who said Zen 6 is delayed? The original source - Benchlife - just said that Olympic Ridge isn't expected to arrive in 2027. That's not contradicting any public statements of timeframes.",hardware,2026-02-20 22:25:44,4
AMD,o6ok98o,> If Intel manages to get Nova Lake out the door this year they might finally get the DIY crown back  Similar rumor today about Nova Lake also launching at CES.,hardware,2026-02-21 23:09:17,2
AMD,o6n6tiy,Conveniently the day after you post this news starts slippling out about NVL being delayed too.,hardware,2026-02-21 18:46:45,1
AMD,o6j8w1w,"> Does anybody know more about why Zen 6 would be delayed other than 'RAM expensive'?   Because it uses TSMC 2N, which is still not in full production afaik.",hardware,2026-02-21 02:12:23,1
AMD,o6lqmra,"Except no one is going to be building diy pcs or buying gaming laptops this year. Of all the years to hold the consumer cpu crown, 2026 is not it",hardware,2026-02-21 14:18:55,1
AMD,o6qz5wp,"nvidia is shipping gpus with 192gb of memory essentially on the die itself, amd should just stick 32gb onto their cpu packages and sell it for $10,000",hardware,2026-02-22 10:03:25,2
AMD,o6gl8bz,Desktop sales will be shit this year due to RAM supply anyway. It makes little sense to release a new product in a down year.,hardware,2026-02-20 17:51:55,124
AMD,o6i6k4h,>the C standing for datacenter of course  i love it,hardware,2026-02-20 22:28:06,6
AMD,o6haz9e,"> (the C standing for datacenter of course),   nice one I chortled",hardware,2026-02-20 19:50:28,6
AMD,o6i2w6i,> It truly speaks for the datacenter money which causes everyone to forget about consumers.  Datacenter CPU sales have gone up in 2025 and continue into this year. Intel reported their fabs producing their datacenter CPUs are fully booked.,hardware,2026-02-20 22:08:56,2
AMD,o6hwio7,"> It truly speaks for the datacenter money which causes everyone to forget about consumers.  I don't get why anyone thinks this is some grand betrayal. Like yeah, welcome to capitalism where companies work to maximize profits for its shareholders.",hardware,2026-02-20 21:36:41,7
AMD,o6if7by,"> At CES  Ces hasn't stood for consumer since some years ago, they rebranded it into just ""ces"" and cut out the consumer part from their brand",hardware,2026-02-20 23:14:50,2
AMD,o6hka2g,> 2027 as the earliest window   So more likely 2028.  The classic title being off-by-one,hardware,2026-02-20 20:36:09,1
AMD,o6jcomq,"The leak quality is pretty poor. Also, they are already binning Zen 6 desktop chips. They would be paying  to store the inventory by not releasing sooner, and getting hurt by Intel that actually seems to have a dangerous (to AMD) chip coming with Nova Lake. Their market share is still a tug of war and they can't risk giving Intel such easy sales while already literally having the response ready. Let alone for so long.",hardware,2026-02-21 02:36:15,1
AMD,o6gq196,"Delaying to build up supply and shake out bugs worked somewhat well for RDNA 4, I suspect we'll see that strategy repeated in future.",hardware,2026-02-20 18:13:38,12
AMD,o6hprvv,"What about people upgrading just the CPU?  I mean I just upgraded to AM5 a few months back, actually the 64GB RAM kit I bought tripled in price just 1-2 weeks after I bought it. So I won't replace it any time soon, but I might upgrade the CPU to get more performance & cores.",hardware,2026-02-20 21:03:19,11
AMD,o6gqj6u,It's AM5 and DDR5.,hardware,2026-02-20 18:15:50,25
AMD,o6hcxkf,"> 'm still on AM4 and don't see any compelling reason to upgrade as I'm most likely throttled by GPU or quite honestly, non-optimized software  This is the most insane take, software runs slow so you don't get a faster CPU.",hardware,2026-02-20 19:59:56,13
AMD,o6gujlm,">Â It's always been plausible, what with TSMC nodes info and timing not exactly being a secret.   Node timing would point to a much earlier launch.",hardware,2026-02-20 18:33:40,14
AMD,o6j9gw3,This is not going to use DDR6. It's already annouced for AM5.,hardware,2026-02-21 02:15:59,4
AMD,o6k5jtv,Lol you're not seeing DDR6 on consumer boards until 2030. And Zen 7 is also gonna be on AM5. So you got time.,hardware,2026-02-21 06:04:01,4
AMD,o6krqsi,"Zen 6 is supposed to come with a new fan-out I/O layout that replaces Infinity Fabric. That fan-out layout has already been demonstrated with Strix Halo: https://www.youtube.com/watch?v=maH6KZ0YkXU&t=337s  On the server side, Infinity Fabric uses roughly 40% of the CPU power, which the new layout aims to cut that to a small fraction.  On the desktop side, the new layout is expected to greatly reduce idle power usage and increase bandwidth (allowing faster RAM), both which has been a major weakness of Zen 4 and Zen 5.",hardware,2026-02-21 09:35:43,1
AMD,o6nf6uz,> Nova Ridge  what? now I'm confused,hardware,2026-02-21 19:28:27,1
AMD,o6hsikq,x3d dies aren't on DRAM nodes either.  They're SRAM blocks on a logic litho node.,hardware,2026-02-20 21:16:54,26
AMD,o6ht01h,"SRAM is built on the same logic nodes, not a memory node, only using older nodes for cost since scaling has slowed.",hardware,2026-02-20 21:19:18,13
AMD,o6i62dg,Kind of makes sense too. Not like Zen 5 is a bottleneck of any sort. Even Zen 4 is still a beast.,hardware,2026-02-20 22:25:29,4
AMD,o6lqtfe,"AMD might have been just vague enough to not outright say 2026, but they at least strongly implied it. [These kinds of articles](https://www.pcgamer.com/hardware/processors/amd-confirms-next-gen-zen-6-cpus-to-launch-in-2026-and-medusa-apus-to-launch-in-2027/) were everywhere a couple of months ago.",hardware,2026-02-21 14:20:01,-2
AMD,o6vw5k7,"Intel almost always launches chips at CES, the non-K desktop chips.  That's where that misunderstanding is coming from",hardware,2026-02-23 02:35:38,1
AMD,o6nnlqq,"Yeah, it was always a big if.",hardware,2026-02-21 20:12:18,1
AMD,o6lf6co,Zen 6 Epycs are coming this year (sooner rather than later) and are also N2P,hardware,2026-02-21 13:05:20,1
AMD,o6nfg65,"Would it not be more important in 2026, since if you hold the crown you get all the high margin/flagship sales, and the more expensive CPUs are where people can weather the higher mem and storage costs as well, where lower end builders prob couldn't?",hardware,2026-02-21 19:29:46,2
AMD,o6gnh2i,that's assuming RAM prices will not get worse in 2027 :),hardware,2026-02-20 18:02:07,30
AMD,o6gyc89,"I sure hope Zen6 runs well with DDR5 5600. Or rather 5200, or maybe we can at least afford 4800 by then?",hardware,2026-02-20 18:50:51,8
AMD,o6i4h3w,"Well, these days when new nodes result in no cost reduction, yeah. Back in the old days, you'd want to shift production ASAP.",hardware,2026-02-20 22:17:07,3
AMD,o6lcppo,"If Zen 6 still holds the crown, youâ€™ll still get a decent amount of people upgrading from earlier AM5 chips who already have DDR5 who want the latest and greatest.",hardware,2026-02-21 12:47:15,2
AMD,o6gqs35,Yeah but AMD is going to still launch the 9950X3D2 thatâ€™s going to be a massive hit /s,hardware,2026-02-20 18:16:55,3
AMD,o6gplq5,Not to mention ferreting out whatever the hell this dying Zen 5 problem is in case it proves to be more major over time.,hardware,2026-02-20 18:11:42,1
AMD,o6neq25,"For Intel 7 and Intel 3, where they don't really plan on building out more capacity, sure.   Even then, they *can* build out more Intel 3 capacity if Intel really wanted too in a relatively short amount of time because of their bunches of EUV capable empty fab shells, however with the economics of Intel 3 still not being very competitive, and 18A DC products coming soonish, it's prob not worth it.   For 18A though, Intel has an impressive amount of flexibility on volume.",hardware,2026-02-21 19:26:05,2
AMD,o6igkss,No way Zen 6 Desktop doesn't go beyond middle of 2027.,hardware,2026-02-20 23:22:39,9
AMD,o6kr02p,"Unless their plan is to capitalize on the increasing datacenter CPU sales by focusing their inventory there. The margins on the datacenter CPUs are far greater than desktop/laptop ones.  https://newsletter.semianalysis.com/p/cpus-are-back-the-datacenter-cpu  > Intelâ€™s recent rallies and changing demand signals in the latter part of 2025 have shown that CPUs are now relevant again. In their latest Q4 earnings, Intel saw an unexpected uptick in datacenter CPU demand in late 2025 and are increasing 2026 capex guidance on foundry tools and prioritizing wafers to server from PC to alleviate supply constraints in serving this new demand. This marks an inflection point in the role of CPUs in the datacenter, with AI model training and inference using CPUs more intensively.",hardware,2026-02-21 09:28:15,3
AMD,o6p5ryf,">Also, they are already binning Zen 6 desktop chips.  Source?",hardware,2026-02-22 01:21:41,1
AMD,o6gr28b,"The thing is, you won't need supply practically; the potential upgraders will be so few until well into 2027 and approaching 2028. This is for winning the very important PR battle against Intel at very little cost to themselves.",hardware,2026-02-20 18:18:09,13
AMD,o6hrro3,The big update should be a memory controller that can compete with Intel and support faster DRAM speeds. Having more cores is great too but many of those same workloads which can truly saturate all cores over 2 CCDs needs faster memory interface right now.,hardware,2026-02-20 21:13:13,5
AMD,o6gqvyo,"I guess I Will have to wait more, thanks.",hardware,2026-02-20 18:17:23,3
AMD,o6iqwx7,"Non-optimized implies that devs could fix it. It's a stand-off to see who flinches first, us or the devs.",hardware,2026-02-21 00:22:23,0
AMD,o6gwm0r,"Not for N2P, N2X, as I understand it. Mind you, as far as I know, we don't actually officially know what node AMD will use for Zen 6. It could be there's a surprise there. Unless there's been official confirmation that I missed, we'll have to wait and see.",hardware,2026-02-20 18:43:02,1
AMD,o6kci5u,Damn they're squeezing Zen7 into AM5?    OEM's should come together and fix 12HPWR before moving onto CAMM2 or some hopes for pushing HBM onto desktops early skipping DDR,hardware,2026-02-21 07:06:17,1
AMD,o6ng6h4,">On the server side, Infinity Fabric uses roughly 40% of the CPU power, which the new layout aims to cut that to a small fraction.  Do you mean the IFOP links themselves?   The IO/uncore as a whole may take up that much power. But I don't think the infinity fabric links themselves are the reason for it being that high, and thus I don't think the new layout will cut it that dramatically.",hardware,2026-02-21 19:33:30,1
AMD,o6nvk6n,"They didn't, and that's obvious if you look at what they actually said. All they said in the slides there is that either Zen 6 or Zen 6C will be ready in 2026. They didn't say anything about Olympic Ridge, which is the desktop variant of Zen 6.  Just because the articles misunderstood what AMD meant doesn't mean that's what they meant.",hardware,2026-02-21 20:54:38,1
AMD,o6wd004,They'll launch NVL whenever it's ready. The question is when that will be. It would hardly be surprising if it got delayed to '27.,hardware,2026-02-23 04:25:37,2
AMD,o6nf3zx,"AMD has also only ever officially claimed N2, and that is apparently already in HVM (though for what product, who knows....).   But yea Mediatek also claimed they would have N2P stuff out by the end of this year, so it deff seems *possible* to launch N2 stuff in 2026.",hardware,2026-02-21 19:28:02,1
AMD,o6oaneb,"2026 Doesn't matter since no one is really upgrading this year Especially with Ram prices.  Most people are just waiting for Nova Lake to see if it's worth upgrading to, to upgrade from their 5-7 year old Intel CPU's.  And by the time 2026 ends, nova lake will be launching and could very well destroy Zen 6. Or it could be a dud. No one really knows. Considering how well received Panther lake has been I'd probably say it will be Decent to good.  Honestly though I wouldn't be surprised if Intel pushes Nova Lake to 2027 solely due to ram cost.",hardware,2026-02-21 22:14:50,2
AMD,o6gpsx7,"Eh, between the bubble teetering and the chinese vendors gearing up.. I ain't *that optimistic* about improvement, but I ain't that pessimistic about it getting worse.",hardware,2026-02-20 18:12:35,37
AMD,o6iiiq5,"It might be worse in 2027, but it should begin to get better according to various investors. Should peak somewhere around end of this year.",hardware,2026-02-20 23:33:59,3
AMD,o6gxlyh,"that's the thing tho, to business man, as long as you can accurately predict things, you can adjust output and profit margins to compensate.  they can allocate less to consumer z6 and hike the prices to be in line with the heightened memory and still come out okay in the books.  as long as the price and demand match step, they come out to it fine.",hardware,2026-02-20 18:47:34,7
AMD,o6h48kg,"I think the 3d SKUs will be as tolerant of low grade RAM as ever, but they'll benefit from the good shit more.",hardware,2026-02-20 19:18:09,6
AMD,o6lxp57,"Sure, but those are the customers who AMD should care LEAST about.   Most people buy a CPU/platform and don't touch it until a few years later, when they just replace the entire thing.    DIY upgraders are the vast minority.",hardware,2026-02-21 14:59:34,1
AMD,o6hjdj0,"It'll also depend on Zen 6's performance. If it ends up being in line with the optimistic rumors out there, then I'd bet that'll convince quite a few Zen 4 and 5 owners to upgrade.",hardware,2026-02-20 20:31:36,6
AMD,o6h3s27,Good supply and low bugs can be used to that end fairly effectively.,hardware,2026-02-20 19:15:59,2
AMD,o6hjljv,"The current rumors have Zen 8 as being the first to use DDR6 on AMD, so you may be waiting quite a while.",hardware,2026-02-20 20:32:44,14
AMD,o6j84u1,"You really don't understand how the software industry works, do you? Devs do not decide what to work on, they have managers, scrum masters and POs for that, and any but the most egregious performance regressions will be never be fixed, because it's not seen as a way to make money.",hardware,2026-02-21 02:07:43,1
AMD,o6gy9ib,"> Mind you, as far as I know, we don't actually officially know what node AMD will use for Zen 6.   AMD has been pretty public as one of if not *the* first N2 customers. So that would imply something earlier.",hardware,2026-02-20 18:50:31,11
AMD,o6kdhh8,"> Then I hope this is the matured implementation for Zen5,   Bruh it's Zen 6, what are you smoking?",hardware,2026-02-21 07:15:26,2
AMD,o6krg7f,"The only other viable option I see is AM6 supporting both DDR5 and DDR6.  Releasing AM6 for a single CPU generation before DDR6 would be a one-in-a-decade, self-inflicted ""it hurt itself in confusion"" move.",hardware,2026-02-21 09:32:45,1
AMD,o6o4io8,"Exactly, they didnâ€™t specify which product. Thatâ€™s where the â€˜vagueâ€™ part in my post comes from.  To me, being deliberately obtuse to circumvent accountability is just as bad as setting a target for a product and missing it.   But to your point - yes, technically they didnâ€™t delay. With the emphasis on technically.",hardware,2026-02-21 21:41:57,1
AMD,o6hrlby,"Oh cool, I'd be pleased if Zen 7 will be on AM5 still.  I have a 7800x3d and only want to upgrade to the final single-ccx x3d part they put out.",hardware,2026-02-20 21:12:21,9
AMD,o6iaku9,I'm sitting on a 5800x3d hopefully it'll be alive by the time Zen 8 releases.,hardware,2026-02-20 22:49:25,3
AMD,o6i37nm,"Yeah it really depends on the timing of DDR6 launch (assuming OpenAI doesn't buy the entire first year's production run). No point in launching AM6 if there are no DDR6 sticks to buy, unless it supports both DDR5 and DDR6.",hardware,2026-02-20 22:10:36,1
AMD,o6j8o03,"They say 'vote with your wallet'. If enough people aren't using software due to bad performance, the managers will have to prioritize it to keep making money.",hardware,2026-02-21 02:11:01,0
AMD,o6hshky,"It should be very impressive, especially if the optimistic rumors about Zen 6 end up being true! At the very least, you can expect 50% more cores and more cache and more IPC. I think youâ€™ll be pretty happy with that upgrade, when the time comes!",hardware,2026-02-20 21:16:46,2
AMD,o6iatli,"Hopefully. It'll be quite outdated by 2030-2031, but still, it's a great cpu!",hardware,2026-02-20 22:50:42,1
AMD,o6m5z6e,"Same. With a 4080 super it still runs all games at max settings native 4k 60fps, no up scaling bs.",hardware,2026-02-21 15:43:05,1
AMD,o6jqbwr,20 years of industry experience says otherwise.,hardware,2026-02-21 04:06:33,2
AMD,o6kedzh,"Ok? It's still not an ""implementation for Zen 5"" it's a new architecture.",hardware,2026-02-21 07:24:00,1
AMD,o6ng9cd,"if Zen 6 is coming at between 4Q26 - 1H27, we're not seeing Zen 8 anytime before 2032-2033",hardware,2026-02-21 19:33:54,1
AMD,o6jqmk5,"In the past 20 years, there hasn't been a situation with prices so bad (for the latest gear) and vibe coding existing.",hardware,2026-02-21 04:08:40,3
AMD,o6nkmjr,"That's very possible, sure, but only if the release cadence slips from it's usual roughly 2 years, to 3. I hope that's not the new normal now. The current rumor is a CES 2027 announce for Zen 6 (and Intel Nova Lake), which would put it at 2.5 years. Still, that could be a one-off, and then there'll be DDR6 added to the mix at (presumably) the end of the decade. So yeah, I think it's going to be interesting to see how that plays out in terms of actual consumer products and their timing.",hardware,2026-02-21 19:56:39,1
AMD,o6jr3m4,"Perf/$ is still near the best that ever was, and I really don't know why you expect vibe coded shit to be performant instead of even worse.",hardware,2026-02-21 04:12:01,3
AMD,o6p964x,"Nvidia already falling into a 2.5 years cadence, I don't doubt that the current semiconductor parts shortage is gonna make things difficult for everyone else as well",hardware,2026-02-22 01:43:46,1
AMD,o6jrbwt,"I meant vibe coding is bad. Plus for gaming, there is a huge gulf between optimized games and slop now.",hardware,2026-02-21 04:13:41,2
AMD,o6pab0k,"Graphics cards are different, though, and require a lot more silicon.. not only bc of the GPU dies, but also the VRAM needed. So, they aren't impacted anywhere close to as much, and that's why you haven't seen CPU prices rise much, if at all. Mind you, upcoming nodes like N2 will cost more compared to the nodes that Zen 5 uses today, but that's bc the fabs themselves are a lot more expensive to build.",hardware,2026-02-22 01:51:03,1
AMD,o74hz8t,Meta also gets 10% of AMD shares.  This is on top of OpenAI getting 10%  At this point everyone will own 10% of AMD.,hardware,2026-02-24 12:25:25,264
AMD,o76dlls,wow steam machine is cooked,hardware,2026-02-24 18:02:47,28
AMD,o74sq5a,All that money and still couldnâ€™t support the Z1 extreme driver updates for 3 yearsðŸ¤¡,hardware,2026-02-24 13:32:27,93
AMD,o74h62y,CPU's price increase in 3...2...1...,hardware,2026-02-24 12:19:47,56
AMD,o74q7p8,">The warrant will vest over the course of the deal and will do so after AMD stock price hits rising performance targets up to $600. In addition to the stock price targets, there are ""technical and commercial considerations"" for each tranche of the warrant that Meta needs to fulfill.  So they're gettin the option to get shares only if amd shares go up, double, triple and so on. That same reddit user tryin to be dishonest again.",hardware,2026-02-24 13:18:04,34
AMD,o74hc5y,Thought it was 100 billion?,hardware,2026-02-24 12:20:57,4
AMD,o7fxrey,This isnâ€™t going to improve GPU prices I donâ€™t think,hardware,2026-02-26 02:14:20,1
AMD,o74jj4g,"Shouldn't AMD, gamer's favorite corporation and charity, gift gamers $60 billion of free CPU and GPU over 5 years instead ?",hardware,2026-02-24 12:36:08,-24
AMD,o79igrn,"This kind of deal is absolutely bonkers. Its not free money or a straight contract for accelerators, lol. AMD is giving away a very large portion of the company (and diluting the fuck out of the shares) just to get these companies to hopefully buy a seat at the table. And they've done it twice now. Its a good thing that the only AMD stock I hold are in semiconductor ETF's. If I held direct shares, id sure as hell sell it before the pied piper came to collect. The long term consequences of this are astounding if they dont pull this off perfectly.  Look, I want Nvidia's monopoly to crumble as well, but this is not the way.",hardware,2026-02-25 03:31:29,-1
AMD,o78n3p6,AMD will never catch up to Nvidia.,hardware,2026-02-25 00:34:41,-2
AMD,o74ty4z,Waiting for AMD to say they can't supply Gorgon Point in 3 2 1...,hardware,2026-02-24 13:39:15,-7
AMD,o74p27i,mom says its my turn to own 10% of amd,hardware,2026-02-24 13:11:19,98
AMD,o74j5ls,Circular AI economy continues.,hardware,2026-02-24 12:33:35,126
AMD,o74rcqi,When do I get my 10%?,hardware,2026-02-24 13:24:38,15
AMD,o74mqv3,AMD Radeon exists purely as a hedge against Nvidia.,hardware,2026-02-24 12:57:05,11
AMD,o75pm73,>Meta also gets 10% of AMD shares.  If I was an AMD share holder I would be furious.,hardware,2026-02-24 16:14:56,5
AMD,o7gnahi,The life of a public traded company,hardware,2026-02-26 04:50:43,1
AMD,o74pvnd,That's roughly $32B in stock at current rates.   Are they selling these chips at a loss?,hardware,2026-02-24 13:16:08,0
AMD,o75rwxm,"I saw a documentary about this called ""The Producers"".",hardware,2026-02-24 16:25:17,1
AMD,o77gdhd,"Has there been a console with worse market timing? I can't think of any off top of my head.  Maybe Sega's Saturn where they prematurely launched with $399, pissing off their retailers (as they weren't ready), lacked enough launch titles and [gave Sony an opening with their infamous ""$299"" mic drop PS1 announcement.](https://www.youtube.com/watch?v=ExaAYIKsDBI)",hardware,2026-02-24 20:59:19,13
AMD,o74w114,AMD has given up on everything below RDNA 4. Their focus pathetically is all on AI now and only RDNA 4 GPUâ€™s and future APUâ€™s are able to handle AI well.,hardware,2026-02-24 13:50:31,36
AMD,o75nzbf,Me when I only read headlines,hardware,2026-02-24 16:07:32,2
AMD,o74zbxs,I will ride my 5800X3D to eternity.,hardware,2026-02-24 14:08:21,29
AMD,o77fzb4,Don't forget about consumer GPU inventory disappearing.,hardware,2026-02-24 20:57:30,2
AMD,o74rccz,"Same deal as OpenAI, except that Meta is profitable.",hardware,2026-02-24 13:24:35,20
AMD,o74v1bp,"Likely worth over $100B. First GW is probably like $10-$15B, and each generation is going to increase in price.",hardware,2026-02-24 13:45:12,5
AMD,o74qiul,"Nvidia is gamer's fav, over 90% of gamers buy nvidia. Where are the rewards for the loyal customers?",hardware,2026-02-24 13:19:52,21
AMD,o74r2r6,it's the least they can do,hardware,2026-02-24 13:23:02,4
AMD,o74l7hz,Gamers hate AMD these days.,hardware,2026-02-24 12:47:19,-8
AMD,o74wfly,Advanced ~~Money~~ Market Destroyer?,hardware,2026-02-24 13:52:42,23
AMD,o74kqv4,"Nvidia is sort of circular but isn't.  They invest and get stake equity in the company. This deal doesn't involve GPU purchases.  According to news, Nvidia will invest 30 billion into OpenAI and at 850 billion valuation that will give Nvidia 3.5% ownership of OpenAI. They did similar to Coreweave, Nokia and Intel  Most others also do this.  AMD is the only company who is selling both gpus and giving shares for free in the same deal.",hardware,2026-02-24 12:44:15,15
AMD,o7b8i1g,unlimited money hack,hardware,2026-02-25 11:58:20,1
AMD,o76liwn,"Itâ€™s not circular. This narrative is so dumb. Meta makes ten of billions of dollars without any genAI at all. It could be circular if Meta got a lot of their revenue from AMD. They obviously do not. This is just vertical integration.  Do you guys even know what most of metaâ€™s gpu compute is? Iâ€™m curious what you all think the split between genAI and their more traditional recommender systems are, in terms of compute.",hardware,2026-02-24 18:37:50,-2
AMD,o76d4ff,"This deal is so large it almost certainly is bigger than any purchase deal Meta has with Nvidia tho.  Its 60-100B in size, imagine building the Burj Khalifa like 60 times in 5 years, its crazy numbers.  I would actually lean more towards this being a strategic play rather than just a hedge.",hardware,2026-02-24 18:00:41,11
AMD,o74nxx6,Amd has effectively put themselves in a hole.  No one will buy AMD gpus at scale unless they get a similar deal where they get company shares for basically free.  And what's to stop OpenAI and Meta from wanting more after the current one ends?,hardware,2026-02-24 13:04:29,38
AMD,o78mut9,"AMD's GPU division doesn't do R&D as an investment vehicle. They do however build stuff to order, and that tech frequently trickles down to Radeon.",hardware,2026-02-25 00:33:23,5
AMD,o74o5zh,Radeon has nothing to do with the article or this current thread.,hardware,2026-02-24 13:05:50,21
AMD,o74nl0k,I would say only to supply video consoles.,hardware,2026-02-24 13:02:15,4
AMD,o74s6s0,Lmao! AMD haters absolutely seething so much they think Meta is buying Radeon GPUs  Another L for nvidia. Much more to come!,hardware,2026-02-24 13:29:23,-5
AMD,o7654bb,yeah investors everywhere are losing their mind over the stock going up 10%,hardware,2026-02-24 17:24:32,13
AMD,o74t34t,"Believe it or not, it costs AMD nothing to make and sell an extra share. Shareholders will gladly trade 10% for 3-4 years worth of revenue. The fact that you don't understand this is pretty sad. I hope you aren't managing anyone else's money.",hardware,2026-02-24 13:34:28,-16
AMD,o79g9mh,which we can all [thank Nintendo for](https://playstation.fandom.com/wiki/Nintendo_PlayStationÂ ) making sure happened,hardware,2026-02-25 03:18:28,2
AMD,o76oy1u,"Just wait 2 more years and AMD will also give up on everything bellow UDNA. RDNA 4 was just a stop-gap architecture after all, with no high end or server presence.",hardware,2026-02-24 18:52:48,21
AMD,o75034g,"As a business you would do the same though. You see $60b on the table, probably 50% profit off that. Throw $15b or more at R&D as your biggest customers (AI) are already committing to huge purchase orders with something specific in mind. Consumer market share is dwindling as capacity is being gobbled up by corporate greed. Only focus on new factors as old tech is no longer profitable (which sucks).  Im in no way in favor of the AI bs but from a business standpoint, this is fantastic for AMD and their shareholders.",hardware,2026-02-24 14:12:21,5
AMD,o78ozk4,AMD has been giving up in anything below the next big thing since like 2015.,hardware,2026-02-25 00:44:42,-1
AMD,o76w43d,"I just got a 9850X3D to update to, not because I needed it but it was a bundle deal to get ram at a semi reasonable amount.",hardware,2026-02-24 19:25:11,5
AMD,o75283o,Same.,hardware,2026-02-24 14:23:29,2
AMD,o75mn8a,I'd love to upgrade my 9950X3D to something with twice the core count though,hardware,2026-02-24 16:01:30,-5
AMD,o74n16u,"No they don't.  AMD screwing over everyone on RDNA3 and earlier by not porting over INT8 FSR4....*crickets*  AMD ending driver support early for some architectures that aren't even half a decade old...*crickets*  Literally millions of cards are affected by this, probably like 99% of their market share.  Meanwhile NV removing 32bit Physx support from a handful of titles only a 100 people or so play in 2026...Real shit! (which they added back even though very few actually missed them after the outrage cycle).",hardware,2026-02-24 12:58:52,2
AMD,o74q75e,Look up vendor financing too.,hardware,2026-02-24 13:17:59,11
AMD,o76mkgt,"> Selling gpuâ€™s and giving away stock for free.  So not really giving away stock for free then, huh.",hardware,2026-02-24 18:42:24,8
AMD,o79iwb4,This exactly. AMD is the only one offering up a large chunk of their company in order to secure contracts. That reeks of desperation.,hardware,2026-02-25 03:34:04,6
AMD,o74rreb,Thatâ€™s a valid point. I think the steel man argument would be that you have to do anything you possibly can to break Nvidia/ CUDA moat and get the next generation of developers using your ecosystem.,hardware,2026-02-24 13:26:58,8
AMD,o75w46o,"OpenAI and Meta will almost definitely push for more, but they are tied to AMD now. Increasing AMDâ€™s value innately makes OpenAI and Meta more money from their 10%, so thereâ€™s incentive to develop the AI market for Radeon GPUs and continue buying from them.",hardware,2026-02-24 16:43:59,4
AMD,o766y2k,"Radeon GPUs should certainly be impacted by this development.  A massive majority of GPU revenue now comes from data centers.  Gaming considerations were already down on the list of priorities. But now, gaming priorities should be relegated even further.     The first, second, third, fourth.. priorities of future GPUs from all brands will be AI data centers.    Gaming GPUs will increasingly inherit whatever features the data center demands, with minimal alterations.",hardware,2026-02-24 17:32:54,3
AMD,o76cda9,"I saw the stock. It's perplexing though. On one hand, sure, it's a 34%~ increase in yearly revenue for the company, going on fiscal 2025 numbers.   But then also (potentially) handing over 10% of the company to someone as absolutely shit-blastingly evil *and* incompetent as Meta? That's a hard no for me, both financially and morally.",hardware,2026-02-24 17:57:21,-12
AMD,o74uav9,Believe it or not shares are actually ownership,hardware,2026-02-24 13:41:12,18
AMD,o756psc,Bait used to be believable,hardware,2026-02-24 14:46:32,5
AMD,o74v4ch,Amd aren't making extra shares...  They have a warrant for Meta to buy 160m @ $0.01 shares of AMD's 1.63 billion shares.  This means $1.6 million (basically free) for ~10% of AMD,hardware,2026-02-24 13:45:39,2
AMD,o750lcr,Iâ€™m not bashing their business practices. Itâ€™s just rather pathetic that they arenâ€™t even bothering to support recent GPUâ€™s and APUâ€™s for 3 years.,hardware,2026-02-24 14:15:01,21
AMD,o75lbpq,"As a business, you don't get so blinded by one thing that you immediately ruin your reputation with a sector of the market you traditionally need and consistently rely on as your safe space. Gaming to AMD is actually traditionally a huge share of their revenue they wouldn't survive without on normal days. AI comes with a promise of a bigger reward, but an oversized risk. Making good business moves is about much more than bean counting. You don't just blindly go all in while screwing your other core business. That's a bad business move.",hardware,2026-02-24 15:55:30,12
AMD,o75b4zt,She may not look like much but she's got it where it counts,hardware,2026-02-24 15:08:12,2
AMD,o74rc5w,"> No they don't.  They do, over 90% buy nvidia. Is it really a surprise that single digit marketshare products get fewer complaints because so few people buy themï¼Ÿ",hardware,2026-02-24 13:24:33,12
AMD,o74rotm,You might be confusing gamers with what you see on Reddit which is a tiny echo bubble. The market (aka gamers) prefer Nvidia in very very large majority.,hardware,2026-02-24 13:26:34,10
AMD,o74q1la,Meanwhile NV....<infinite list of NV bullshit>  Radeon drama could never be as heavy as NV's bullshit.,hardware,2026-02-24 13:17:06,9
AMD,o74rxez,NVIDIA are the ones juicing this entire unsustainable AI cycle and who jacked up the prices of their GPUs to unaffordable levels in the first place.,hardware,2026-02-24 13:27:55,0
AMD,o79miex,"You dont need to give away 20% of the company to challenge the moat when things are already moving  towards open source. All it takes is time, enough translation layers or someone else coming up with a better ecosystem. The fact that theyre diluting the fuck out of the stock to try and force adoption tells me they gave up on the latter.",hardware,2026-02-25 03:56:04,-2
AMD,o75wz0n,"I really doubt Mark Zuckerberg and Sam Altman gives a shit about Radeon Gaming GPUs.  If anything there is a higher chance they would want to kill radeon to make more instinct GPUs, including PCIe based ones  The mi50 had pcie gpus.",hardware,2026-02-24 16:47:46,7
AMD,o76isfk,It's because you're financially illiterate like most of reddit,hardware,2026-02-24 18:25:40,13
AMD,o76nfh7,"If you dilute your company by 10% but make 34% more, is that a good or bad deal for the current shareholders?",hardware,2026-02-24 18:46:09,2
AMD,o74v9jy,"And? What does that actually cost the company? AMD doesn't pay a dividend  AMD needs revenue to fund R&D. Nvidia and other competitors would otherwise get that revenue and spend it on their own R&D. AMD just secured 4 years worth of R&D (at much higher than usual margin so actually a lot more) taken directly out of what would be Nvidia's pocket  This is an absolute coup and given how much more efficient AMD is at using their funds could allow AMD to replace Nvidia and others for other markets.  It also gives the whole market confidence in AMD's AI products, which is even more important.  You obviously don't understand any of this haha but please continue embarrassing yourself",hardware,2026-02-24 13:46:25,-12
AMD,o75mdbz,"it's quite clear by now that gaming from now on will always be only a small part of their business, so small that it's fine to make those customers unhappy.",hardware,2026-02-24 16:00:15,2
AMD,o77ft13,"But that runs counter to the ""pump up the numbers for the next fiscal quarter"" mentality.  > You don't just blindly go all in while screwing your other core business. That's a bad business move.  *Laughs in Broadcom's VMware and driving away their customers*",hardware,2026-02-24 20:56:42,3
AMD,o7807cs,"Yeah sure, bad business move. Nvidia is surely missing gamers despite their 72 billion usd annual profit.  The real bad move is that AMD didnt invest massively into their software stack 5 years ago.",hardware,2026-02-24 22:32:14,0
AMD,o74uxk8,AMD is doing the same thing theyâ€™re just not as good at it.,hardware,2026-02-24 13:44:38,4
AMD,o76n26m,Itâ€™s absolutely wild the amount of just idiocy I see every day on this site. Just zero thought put into hundreds of comments. It wasnâ€™t always like this. What happened?,hardware,2026-02-24 18:44:33,13
AMD,o76k3m2,So... You approve of what Meta does?,hardware,2026-02-24 18:31:30,-4
AMD,o74wa6h,Giving you parts of you company for free isn't a coup. It's a trap.  Amd can no longer sell GPUs at scale without giving parts of itself away for free,hardware,2026-02-24 13:51:53,13
AMD,o75rx6q,"that's what they think, at least.   It is not quite clear that they're right.",hardware,2026-02-24 16:25:18,5
AMD,o75w3bq,"Client and gaming were 40% of AMD's revenue in the last quarter where their DC demand was already well into its unprecedented boom (that will not last even if there is no bubble burst, as the world can't sustain this rate of DC build-up forever). That number already reflects a period when they were prioritizing DC sales as much as they could at peak demand. And even under those conditions, they still needed to sell most dies to client where they got the 40% of their revenue from that wouldn't exist without gaming.  So DC is where margins are, with cyclical and highly unpredictable demand patterns, and usually limited volumes. Client is where the volumes of the products they make actually reliably sell. Most Zen and RDNA dies will end in consumer products simply because there isnt enough lasting demand from business to buy them at their higher margins.  Importantly, gaming makes them tens of billions of dollars a year, is a revenue stream and demand channel that's consistent and growing over decades.   We like to make ourselves feel so irrelevant, but there would be no AMD without client even WITH the unprecedented AI/DC boom we're witnessing.  If even as much as a correction happened affecting the DC demand, client/gaming sentiments are what drives whether AMD stays in business.",hardware,2026-02-24 16:43:52,-2
AMD,o781b7p,They are doing a far better job catering to and supporting GeForce buyers on an ongoing basis than AMD does for Radeon buyers. And AMD didn't even hit gold on the AI boom.,hardware,2026-02-24 22:37:45,4
AMD,o77goxi,"good question. I think it might have been like this since a long time though, I might just have paid less attention to it",hardware,2026-02-24 21:00:46,5
AMD,o76l2ce,What does that have to do with anything? Do you approve of what nvidia does with Palantir? Lmao  If you want your tech company to be some moral paragon that ship sailed literally a century ago,hardware,2026-02-24 18:35:48,8
AMD,o76jkl1,"It's not ""for free"", it's for 5-10 years of revenue for AMD (actually more if you average AMD's rev for the last 20 years. That is an immense amount of money and will allow AMD to compete directly with Nvidia much easier  It's pretty pathetic that most of this sub can't comprehend these basic concepts, like the word ""free"" or what $100B in revenue can do for a company. The market understands this as we can see by the stock being up 10%  If you give me 10% of your company and I give you $100B, did I just get your company for free? Lmao",hardware,2026-02-24 18:29:08,1
AMD,o74x3xy,They can if the product is competitive. If it's more cost effective to buy AMD then that's what companies will have to do. These deals are a tool to get to that point in GPU accelerators. Companies buy Epyc without incentive,hardware,2026-02-24 13:56:20,2
AMD,o76nkhe,Itâ€™s not for free. There is a purchase agreement.,hardware,2026-02-24 18:46:45,1
AMD,o75scbf,"even without anything AI related, ""datacenter"" was becoming bigger and bigger for amd",hardware,2026-02-24 16:27:11,5
AMD,o76ltkf,"No and no.   But again, I'm not talking about selling products, I'm talking about giving company shares to genuinely evil companies. Sell as many chips as you can, it's a business. But opening up to give as much as 10% control to fucking Meta? That's horrifying.",hardware,2026-02-24 18:39:09,0
AMD,o74xyat,AMD said the Mi455x would be competitive and yet the big two deals they secured for it required give a total of 20% away.,hardware,2026-02-24 14:00:54,10
AMD,o76mzle,"Its leverage, welcome to the business world.",hardware,2026-02-24 18:44:13,7
AMD,o76n33q,How is it in any way worse than any other tech company? Why is AMD supposed to be some moral standout and essentially hobble their own business? If AMD doesn't sell them GPUs then nvidia will (and are) or somebody else will  This is like saying colt is responsible for the cartel deaths in Mexico because they sell guns to whomever they can. That's just business and people have to work and eat  AMD refusing Meta would just mean an even more evil company like nvidia or intel (both of which have or are building research centers in a country committing genocide... and nvidia literally is partnered with the company creating the world surveillance panopticon)  It's hilarious how every time AMD makes a deal with anyone they get bashed but nvidia blatantly aids fascists and genocidaires and people just clap  That's not to say that nvidia is responsible but they are way way worse than AMD and yet nobody says shit,hardware,2026-02-24 18:44:40,3
AMD,o751kpk,The MI455x looks to be very competitive but there is a larger barrier in software. It's getting a foot through the door and then on the next order Nvidia can't flash the software stack.   Nvidia is much more of a software company than people realise.  Nobody cared or even knew about ray tracing until RTX and then the only way you get it was with Nvidia They pretty much created the AI market the exact same way,hardware,2026-02-24 14:20:07,4
AMD,o76ndh5,"Where do I get off this ride? lol.   Yes, I understand what leverage is. I also understand that Meta is a dumpster fire in the making.",hardware,2026-02-24 18:45:54,3
AMD,o76nlgy,"> How is it in any way worse than any other tech company?   If you're currently asking that question, then there's no amount of data I can throw at you to change your mind. I'll bow out.  >It's hilarious how every time AMD makes a deal with anyone they get bashed but nvidia blatantly aids fascists and genocidaires and people just clap  I already fucking loathe Nvidia and Jensen. I wasn't currently hating AMD's leadership. I'm getting there now.  Edit: Typed AMD instead of Nvidia at the end. Fixed it.",hardware,2026-02-24 18:46:52,2
AMD,o5o76t8,I would be pretty confused if their CPU market share grew because of Radeon,hardware,2026-02-16 12:24:46,438
AMD,o5olchg,"The 5800X3D came out in April 2022. From that moment on, the gamer and enthusiast answer to ""what CPU should I buy?"" has been a Ryzen. But even outside of the enthusiast space, Intel's competition for the last couple generations has been underwhelming. The 13th and 14th gen used way too much power, before the stability problems began, and Core Ultra is ""ok"" but didn't raise the bar.  This has allowed AMD to charge higher prices than they've been able to for a long time, increasing revenue which is what this article is really about.  Intel's Panther Lake seems to be... good? At the very least its integrated graphics are good. The desktop version is rumored to have a high cache option to finally compete with the X3D.  So that's good, both major x86 CPU makers may have nice competitive products in 2026 just in time for the rest of the computer to be unaffordable.",hardware,2026-02-16 13:54:14,88
AMD,o5oaukr,"Market share in revenue, not in units.   AMD significantly grew revenue per unit, if it was any other company, the headline would probably be something like ""theyâ€™ve raised prices so much that their revenue per unit increased so much""",hardware,2026-02-16 12:50:50,84
AMD,o5o83mz,Who else would it be?? Athlon? FX? Opteron?,hardware,2026-02-16 12:31:30,102
AMD,o5och3a,I'm sure Intel's continuous fuck-ups had no major affect on sales.,hardware,2026-02-16 13:01:41,28
AMD,o5obbkf,Gaining share in a market that's about to shrink dramatically is an achievement of sorts.,hardware,2026-02-16 12:54:01,11
AMD,o5oqsq0,I went back to AMD last year after at least a decade using Intel. Iâ€™ve been very happy with my Ryzen 7.,hardware,2026-02-16 14:23:29,9
AMD,o5typsk,"Outside of performance, the Longevity of their platforms is part of the reason people choose AMD. in just 2 sockets, which will turn 10 years total next year, AMD delivered 6 gens so far (1000,2000,3000,5000,7000,9000) and might be 7 with 10000 series ending AM5  Since 2017, Intel delivered more Gens (10 so far), but on far more sockets (From LGA 1151 to now LGA 1954) - basically a new socket every two years.  2022 to 2026 Intel has 3 sockets with 5 gens. AMD has 1 socket with 2 gens and 3rd incoming. So you could have brought a motherboard in 2022 (B650) and still enjoy the latest gen from AMD). If you brought 2022 Intel socket, to enjoy latest, you need either LGA 1851 (Ultra 2) or wait for Ultra 4 (LGA 1954).  I was rocking Intel for almost 7 years (2013-2020 with Haswell), then ditched them for AM4 with 2600, 5600, 5700X3D, and now on AM5 with 7800X3D.",hardware,2026-02-17 08:06:23,3
AMD,o5oq8ts,Nova lake needs to be pretty amazingly good otherwise people will just boards & am5 platform.,hardware,2026-02-16 14:20:35,4
AMD,o5oalmd,And thanks to awful Intel cpu's  (I kinda expected more than 15% tho),hardware,2026-02-16 12:49:09,10
AMD,o5od447,"Great. How's AMD laptop market share doing? 3% growth apparently.   With Panther Lake coming out and the seriously impressive integrated graphics, I hope AMD is happy with their RDNA3.5 until 2029.",hardware,2026-02-16 13:05:53,9
AMD,o5puxii,Ok now grow the gpu division as well.,hardware,2026-02-16 17:36:29,2
AMD,o5qf9q7,Intel stepping on a rake repeatedly for the past 3 years has been helpful to AMD. I have a 13600K and love it but damn it's been lonely being an Intel CPU owner :D,hardware,2026-02-16 19:10:06,2
AMD,o5vedo5,"This is a weird title, it could also be written a dozen other ways.  Intel's fall from grace has a lot to do with it.  I've been building PCs since the 386 days.  I've owned Intel, AMD, Cyrix, IBM chips, basically all consumer chips.  I moved away from AMD after the early Pentium days and haven't gone back.  I've also deployed both AMD and Intel in close to a thousand servers, 10s of thousands if we count VMs.  I'm now about to upgrade to an AMD processor for the first time in about 25 years, for my personal PC.  To me, Intel's failing is the main reason for AMDs surge.  Nothing happens in a vacuum though, so you could argue that AMD being better is the story, rather than Intel being worse, but both are true.",hardware,2026-02-17 14:36:53,1
AMD,o5ofzv5,"Makes a lot of sense, almost every custom pc i encounter nowadays as well as prebuilt comes with Ryzen nowadays, you'd think too that they own majority of marketshare, but that isn't the case as there are plenty of laptops old PCs out there that still rocks older gen of Intel CPUs from 12th gen or under.  That said though this means bad for newer gen of Intel CPUs, the newer ones with rebranding that i can't even remember to type here, that is how forgettable they are, were so bad... That when i looked for upgrading from my 12600K, i came from AMD Ryzen before that, i simply decided to go back to AMD AM4 this time around where i used my old B450 motherboard and slotted a 5700X3D there and rode on that for at least a year, then eventually upgraded to AM5 when DDR5 Ram and motherboard prices were dirt cheap and now i am on a 7800X3D.  For me to be enticed once again to comeback to Intel, they need to offer an equivalent to 3D-V Cache for gaming performance while featuring significantly multi core workload performance than what Ryzen currently offers, that is what convinced me when i jumped from my previous Ryzen 5 3600 to an i5 12600K which was noticeably faster while giving me significantly more multicore performance compared to if i would have just upgraded to then $300 5600X which i felt is too expensive for what it offered back on launch.",hardware,2026-02-16 13:23:52,1
AMD,o5onab9,unfortunately because of the memory situation that is going to slow down quite a bit leaving intel in the technical lead for marketshare.,hardware,2026-02-16 14:04:47,1
AMD,o5rdiy0,Intel deserves all the failure they brought upon themselves. Complacency and lack of innovating for the sake of profits ... Now look at you Intel. Scrounging in the dirt to release anything somewhat competitive,hardware,2026-02-16 21:57:14,1
AMD,o5qob83,Bring back the 5800x3D and these numbers will increase.,hardware,2026-02-16 19:53:42,1
AMD,o5qp8o2,Idk.Â  I feel like I get more CPU for my money when I go AMD.,hardware,2026-02-16 19:58:13,-1
AMD,o5tftcf,CPU company's CPU market share grew thanks to CPU.,hardware,2026-02-17 05:22:55,0
AMD,o5tnqdq,as opposed to the other consumer line epyc,hardware,2026-02-17 06:26:57,0
AMD,o5p8uog,"And with Intel Panther Lake for laptops and Nvidia going deeper into working with Intel on making their on PC / Server CPU's im sure there are no black clouds on the horizon, oooooo wait  This coupled with AMD not doubling down on buying gamer market share with high RAM gaming cards by giving their AIB's discounts on 16gb 9060 / 9070 SKU's are how we will look back at AMD snatching defeat out of the jaws of victory yet again.",hardware,2026-02-16 15:54:15,-1
AMD,o5rmttv,"If only their GPU division could do the same.   It's kinda funny looking back, they spent all this effort to overtake Intel, while NVIDIA massively surpassed both.",hardware,2026-02-16 22:44:27,-1
AMD,o5q6suh,In the segment that represents less than 3% of the total market.,hardware,2026-02-16 18:31:05,-5
AMD,o5o7j7r,I'm flabbergasted Bulldozer contributed nothing to the equation.,hardware,2026-02-16 12:27:19,167
AMD,o5oyz90,lol first thing that caught my eye.   Might as well say â€œAMD CPU market share rises thanks to their CPUs.â€,hardware,2026-02-16 15:06:17,49
AMD,o5qm9mq,Threadripper desktop users unite!,hardware,2026-02-16 19:43:46,13
AMD,o5owtmj,There are Epyc desktop models. But yeah,hardware,2026-02-16 14:55:12,24
AMD,o5pcwt9,I was convinced people were buying phenoms again.,hardware,2026-02-16 16:13:10,3
AMD,o5s81rl,"Among AMD CPUs, there isn't just Ryzen. There's also Threadripper and Epyc. Including [Epyc models on AM5](https://www.youtube.com/watch?v=n1tXJ8HZcj4).",hardware,2026-02-17 00:43:52,1
AMD,o5s9ck7,"They are differentiating between Ryzen and EPYC, though both grew.",hardware,2026-02-17 00:51:17,1
AMD,o5riak4,remember when they were trying to tie radeon to ryzen because resizable bar somehow?,hardware,2026-02-16 22:21:08,0
AMD,o5rdiho,Intel deserves all the failure they brought upon themselves. Complacency and lack of innovating for the sake of profits ... Now look at you Intel. Scrounging in the dirt to release anything somewhat competitive,hardware,2026-02-16 21:57:11,34
AMD,o5shhih,"To be fair, for the most part the x3d haven't been outrageously priced and after a while you could buy them at a discount. I bought a 5700x3d for 170 a few years ago, a 7950x3d a bit after the 9800x3d launched for around 600 usd including a 19% VAT. And a 7800x3d for 260 around 6 months ago.Â    They're by no means cheap, but they are good value at those price points. Something I've missed from the GPU market for a long time. So it's not like AMD is using their pole position to nickel and dime everyone. At least with respect to CPUs anyway.",hardware,2026-02-17 01:39:43,7
AMD,o5obzjr,"A few years ago, customers were flocking to buy $150 Ryzen 3600s to run on $90 B450 montherboards  Now they're chasing $450 9800X3Ds to run on $250-350 X870 motherboards.  AMD have been very successful in repositioning their consumer CPU space from ""Good enough budget CPU"" to ""Absolute best (gaming) CPU)"".  They've managed to make themselves the premium brand, which is what you need to do if you're as volume constrained as they are.  Sadly, they've managed to do this right as the desktop PC market is about to significantly contract",hardware,2026-02-16 12:58:28,99
AMD,o5ousbu,"Hmm then steam survey wouldn't keep crashing towards amd right, right?",hardware,2026-02-16 14:44:35,7
AMD,o5q13d7,"> if it was any other company, the headline would probably be something like ""theyâ€™ve raised prices so much that their revenue per unit increased so much""  idk about increased prices, necessarily.  9950X3D $699 MSRP  7950X3D $699  --  9950X $649  7950X $699  5950X $799  3950X $749  --  9900X3D $599  7900X3D $599  --  9900X $499  7900X $549  5900X $549  3900X $499  --  9800X3D $479  7800X3D $449  5800X3D $449  --  5800X $449  3800X $339  1800X $499  --  9700X $359  7700X $399  5700X $299  3700X $275  2700X $329  1700X $399   --  7600X $299  5600X $299  3600X $249  2600X $229  1600X $249",hardware,2026-02-16 18:05:10,13
AMD,o5olraw,It was me. I did it.,hardware,2026-02-16 13:56:30,28
AMD,o5omnzh,Raptor Lake and Arrow Lake!,hardware,2026-02-16 14:01:26,15
AMD,o5pnz4h,"Maybe the [AMD EPYC 4005 Series](https://www.amd.com/en/products/processors/server/epyc/4005-series.html), AM5 socket EPYC chips that don't feature any Ryzen branding. Though they probably still fall under the ""server"" umbrella, despite using a desktop socket.",hardware,2026-02-16 17:03:57,7
AMD,o5p1twm,Never forget Phenom II,hardware,2026-02-16 15:20:40,5
AMD,o5o9k8k,Athlon and Threadripper.,hardware,2026-02-16 12:42:00,7
AMD,o5ou13e,"Yeah if Arrow Lake wasnâ€™t a complete dud on desktop it might have stemmed *some* bleeding because of how unimpressive Zen 5 was, but it sucked and was expensive so it didnâ€™t.",hardware,2026-02-16 14:40:38,12
AMD,o5pn8mu,"and you can still find people recommending Intel to others... and when you point out how terrible that advice is, they come back with ""but they fixed it!""",hardware,2026-02-16 17:00:34,2
AMD,o5ov1he,"The CPU market is booming thanks to AI, maybe not so much the consumer market. I think during the AMD q4 earnings call they said AMD will see their consumer revenue increase due to market share gains in 2026 but the overall consumer market will shrink. Datacenter CPU will increase substantially in 2026 due to both increased demand, driven from AI, and also the last couple years of AI GPU demand offset some Datacenter CPU spending that is now coming back. AMD probably okay with more wafers going to datacenters than consumers for the next few years tbh.",hardware,2026-02-16 14:45:55,4
AMD,o5tre62,Why would it shrink?,hardware,2026-02-17 06:58:38,0
AMD,o5rwzn8,"Last AMD I had was back in '02, athlon xp2200 and went with the first core2duo from intel in '04 methinks, Intel since. Got 7800x3d two years ago and damn its awesome",hardware,2026-02-16 23:40:28,3
AMD,o5oaw33,In one year?,hardware,2026-02-16 12:51:08,16
AMD,o5ogj94,No thanks to the good X3D. Zen 5 vs Zen 4 is less than an overclock,hardware,2026-02-16 13:27:05,10
AMD,o5osmk9,"Intel sales are largely a reflection of how many machines are prebuilts, and market inertia.",hardware,2026-02-16 14:33:11,4
AMD,o5tejf9,"OP's title isn't the clearest, the article says that AMD went from 28% market share to 42% market share, which is huge. They didn't go from say 10% to 11.5% market share.",hardware,2026-02-17 05:13:14,2
AMD,o5otdwz,Laptop? It's been stuck at between 20%-27% (both unit and revenue) since Renoir. Cezanne was actually their best relative generation.,hardware,2026-02-16 14:37:12,8
AMD,o5ozroe,"Arrow lake HX being too expensive kinda pushed me to get Ryzen Dragon range CPU.  AMD dragon age CPU is about $200-300 cheaper while offering comparable performance, no P-core, E-core thing and consume less power. (when full load)",hardware,2026-02-16 15:10:17,7
AMD,o5upw44,The competition for gpus is Nvidia tho.,hardware,2026-02-17 12:10:27,2
AMD,o5oqjt2,"The selection of X3D chips is quite limited though. Outside of high end gaming, Intel offers good enough value after they discounted 200 series CPUs.",hardware,2026-02-16 14:22:11,14
AMD,o5od6d9,â€œBulldoze the competition!â€  *Bulldozes its entire financial security and wellbeing instead*,hardware,2026-02-16 13:06:18,51
AMD,o5och11,No respect for the elderly these days,hardware,2026-02-16 13:01:41,19
AMD,o5ozfpx,Iâ€™m sure K5 sales will surge any day now!,hardware,2026-02-16 15:08:37,13
AMD,o5plwk4,"Not sure why this got downvoted, the EPYC 4005 series is AM5 based and they are classed as desktop chips.",hardware,2026-02-16 16:54:28,16
AMD,o5rbt67,AMD peaked with the Athlon 64 X2. Ryzen is nothing compared to the onslaught of that CPU for a whopping 14 months.,hardware,2026-02-16 21:48:46,5
AMD,o5sf8uk,"They were trying to hit 10nm instead of being on 14 forever, but that was a technical issue they were having problems with, same with EUV. They could have done better but they were trying.",hardware,2026-02-17 01:26:09,-4
AMD,o5u8wdy,"No, an 8 core for 600 is not good value in any way, even with vat",hardware,2026-02-17 09:44:21,3
AMD,o5ocfnq,And if you look at the amount of silicon that goes into a desktop CPU vs a GPU you see why they aren't super concerned about radeon,hardware,2026-02-16 13:01:26,31
AMD,o5ou73a,"Right now customers are flocking to buy $150 7500Fs or $200 7700s for their $90-120 B650 motherboards. These CPUs are no slouches either. They offer great gaming performance for any GPU at 1440p.  Prices for entry level/mid tier AMD CPUs are just as low as, if not lower than they were in the Zen 2 era, they have just ALSO become the go to for high end gaming.  What I mean to say is that AMD hasn't repositioned their consumer CPU market as much as they have managed to spread the hype to also cover high end segments.   For context I live in a very high cost country (Sweden), where PC parts are expensive as fuck. Just as the RAM prices started to creep up I managed to get my hands on a 7600X, B650 Eagle AX (by all accounts a great ATX B650 board with a good feature set) and 32GB of DDR5-6000. This came out to 4200 SEK, or about â‚¬400 EUR/$440, and this was when RAM prices were already up by 20-30%. That number includes 25% VAT.   AMD has absolutely not shifted away from the mid-tier market, they have just started dominating the high end as well as the mid tier.",hardware,2026-02-16 14:41:31,18
AMD,o5om4ee,I only bought the 3600 because i missed the 1600 af. I think intel was still on 8th gen at the time so 3600 looked readonable,hardware,2026-02-16 13:58:27,4
AMD,o5ocmgv,"And now itâ€™s pretty much the other way around.  With i5s or Core ultra 5 you get a decent performance (better than comparable AMD CPUs), significantly more multi core performance and a better platform (I/O) at the same or even lower price.  But AMDs marketing was able to convince gamers that spending 400-600 usd on a CPU for gaming is fine instead of 150.   Every time I remind people of the statement from AMD when they launched the 2700X that a 300 dollar CPU shouldnâ€™t have just six cores anymore just to release >300 dollar six core CPUs only two gens later they go absolutely crazy and start defending AMD for some reason",hardware,2026-02-16 13:02:40,9
AMD,o5qtn6v,"The steam hardware survey was always hogwash, and will always be hogwash. They don't know the first thing about how to do proper statistics.",hardware,2026-02-16 20:19:46,-3
AMD,o5ox02b,Crushed it homie,hardware,2026-02-16 14:56:07,13
AMD,o5r73m5,Raptor lake is garbage.  Arrow lake is decent,hardware,2026-02-16 21:25:49,3
AMD,o5p2if6,"Athlon isn't a desktop chip and Threadripper has Ryzen branding.  So...no, title still doesn't make sense.",hardware,2026-02-16 15:24:00,9
AMD,o5r7jux,Itâ€™s actually pretty awesome outside of gaming,hardware,2026-02-16 21:28:01,5
AMD,o5ouuw3,Anything after 12th gen was a dud,hardware,2026-02-16 14:44:58,7
AMD,o5r81hy,Gamers should buy amd. Arrow lake is fine for other use cases. I have two arrow lake chips and several ryzen systems here. They are good at different things. Advice should be based on use case not blind loyalty to a brand,hardware,2026-02-16 21:30:24,5
AMD,o5ti94p,"Hmm, people should buy products based on their individual capabilities.  Buying AMD because X3D exists when an Intel CPU that is much faster and cheaper all round may exist is not as enlightened as you are putting it.",hardware,2026-02-17 05:42:03,0
AMD,o5qdfk0,"> AMD probably okay with more wafers going to datacenters than consumers for the next few years tbh.  Honestly, the current CPUs available has more than enough power for most consumer activities.",hardware,2026-02-16 19:01:31,6
AMD,o5tvini,Um I don't know if you've noticed but there has been a recent change in the prices of a few important components,hardware,2026-02-17 07:36:17,2
AMD,o5un7gw,"Unless you're mainly doing an CPU upgrade, most people either don't ant to or can't afford to buy or build a new computer since Memory, Storage (both flash based and spinning disk), and GPU prices have shot to the moon.  This is going to result in the consumer purchasing fewer new computers/laptops.",hardware,2026-02-17 11:50:19,1
AMD,o5r8glu,I had to go back to intel after my ryzen 2500u pro based thinkpad died. New amd laptops have terrible wifi cards.  I need something that works.,hardware,2026-02-16 21:32:26,1
AMD,o5or2ym,A lot of people would like to believe they'll upgrade from 9060x to 10800x3d or something but they'll probably either go to 10600x or 10700x.    Its valid but the x3d users are a loud minority,hardware,2026-02-16 14:24:59,8
AMD,o5oxcu2,"And even high end gaming, you are better off with the cheaper I7/whatever Ryzen has and then saving $200 to invest in a better GPU.  I have a microcenter 20 mins away from my house, so their deals on combos was always insane.   Top end CPU's have never made any real sense lets be honest. X3D isn't any different. Unless you are playing certain games the CPU is irrelevant after a point.",hardware,2026-02-16 14:57:58,5
AMD,o5r6vy5,They are sold for small business grade servers. Threadripper would be the workstation chips,hardware,2026-02-16 21:24:47,7
AMD,o5v7qgx,They're chasing server market share 1st and foremost with Zen.  Huge profit margins there and they don't have to risk ordering as many wafers from TSMC as they would with going after desktop first.  Great business decision but kinda sucks for us.  Desktop are a distant 2nd class at best.  If they can figure out a efficient Zen that competes with latest Intel well on power then laptops will become 2nd class and desktop 3rd class.  Its all about money.,hardware,2026-02-17 14:00:41,1
AMD,o5shdvm,Doesn't matter. They kept the consumer chips on 4 core 8 thread for 6 years in a row with 0.1ghz increases each year. They already had capability for better but kept it to enterprise market. Fuck em,hardware,2026-02-17 01:39:06,16
AMD,o5uro2c,"I said 7950x3d, though. That's a 16 core. Did you miss that?",hardware,2026-02-17 12:23:15,8
AMD,o5okkmj,If it weren't for datacenters - you could easily fool me AMD never cared about Radeon.,hardware,2026-02-16 13:49:59,26
AMD,o5p1ug7,Sub-100 EUR market segment hasn't seen any new AMD cpu entries for years.   5700g and 5600g prices haven't decreased for 3 years or so.,hardware,2026-02-16 15:20:44,6
AMD,o5s2xvx,"The 3600 and 3600X was up against the 9600K which was more expensive and lacked HT/SMT, being a 6/6, for no reason other than Intel wanting to upsell people.  Coming from a decade of 4/4, 6/12 was a revelation for multitasking.",hardware,2026-02-17 00:14:33,3
AMD,o5osxm2,"2700x MSRP in 2018 was $329  9700x MSRP in 2024 was $359  https://www.techpowerup.com/cpu-specs/ryzen-7-2700x.c2011  https://www.techpowerup.com/cpu-specs/ryzen-7-9700x.c3651  Not too shabby imo. ""Between 2018 and 2024, the purchasing power of $1 decreased by approximately 20-25% due to high inflation"" google ai says.",hardware,2026-02-16 14:34:48,14
AMD,o5og9rl,"The situation isn't exactly the same. I'll be the first to admit Intels better in the Ryzen 5/core 5 range. But AMD has provided CPUs that are at that price point as well (7500f/7600 for the last few years) and Intel is still expensive in 7/9 range and at that price point AMD is just better. They've beaten Intel consistently in P/W since a few gens ago and the margins aren't small. I believe they've got the 3 best gaming CPUs on the planet right now (98/78/99 X3D) and they're sipping power compared to Intel for that. Plus, with the 7800X3D dropping to mid 300s for a fairly long time now you've got pretty much the best overall gaming processor on the market. As for the 9950X3D. Nobody is buying that for simple gaming outside of people with more money than sense. It's a workstation CPU that can game. It's meant to be a premium product like the 9900K was.  And the thing with platforms is that you're trading better ports and such now for being locked out of future gens. With AMD you still get decent specs but it's gonna get supported a lot longer.  This isn't anywhere near what Ryzens initial situation was.",hardware,2026-02-16 13:25:30,14
AMD,o5ov95k,"245kf is a decent CPU and so is the 14600k. They are about equal to a little bit better than a 7600X or 9600X in gaming. The problem is that Intel chipsets comparable to B650 or B850 are extremely, outrageously expensive and provide no upgrade path. Hence it makes more sense to go AM5 if you like DIY. I don't care if you don't actually overclock or upgrade the CPU. Intel non-OC chipsets are still more expensive than AMD OC chipsets and buyers clearly like having the option of OC/dropping in a new CPU even if they never actually do it.  This may change with Nova Lake/LGA 1951, since Zx70 boards are coming back and the socket will allegedly support four generations of CPUs. But as of right now it's really hard to justify any Intel CPU if you build your own PC, even if the price/performance of the CPU itself is slightly better.",hardware,2026-02-16 14:47:03,8
AMD,o5pyiy3,"If Intel actually kept their platforms around for longer than 1 or even 2 years, they might have been able to maintain some stickiness with DIY customers.",hardware,2026-02-16 17:53:25,3
AMD,o5p88ym,"yeah, for the same price your room can be 2-3 degrees warmer because intel never figured out how to improve performance without cranking the shit out of the power usage, and on top of that if you were unlucky enough to buy a 13th or 14th gen cpu you can expect it to just shit itself randomly. intel is a failed engineering company that deserves to be utterly shit on for the chokehold they had on the desktop market in the 2000s and 2010s and the stagnation consumers suffered from during that time, let alone all their failures to keep up with amd post ryzen.",hardware,2026-02-16 15:51:26,-4
AMD,o5qtv3v,It is. But its the best we got. And post name change intel chips are deep down the gutter levels of bad sales,hardware,2026-02-16 20:20:51,2
AMD,o5py9wb,My Athlon 64 X2 5200+ begs to differ,hardware,2026-02-16 17:52:17,7
AMD,o5t2qcq,So is Zen. It just has a mediocre uplift in gaming as well.,hardware,2026-02-17 03:50:27,5
AMD,o5qiwaf,"If it weren't for the degradation issues, 13th gen should have been considered decent IMO.",hardware,2026-02-16 19:27:25,5
AMD,o67h9x3,"How would that shrink desktop CPU market share? the memory and storage shortage impacts all markets, not just desktops.",hardware,2026-02-19 07:57:34,1
AMD,o67ic63,"Firstly, this is wrong. Most people can absolutely afford to buy or build a new computer even with the increased prices. Secondly, GPU prices have not shot to the moon. Thirdly, the memory/storage shortage is affecting all markets, so market share would not be impacted.",hardware,2026-02-19 08:07:30,1
AMD,o5v9aj2,Usually replacing the wifi card is no harder than replacing RAM or a hard drive.  Bigger issue is dealing with the BIOS whitelists many laptops vendors have.  Find that out first before trying to upgrade anything Intel or AMD.,hardware,2026-02-17 14:09:18,1
AMD,o5ti07x,"I mean, their trusted reviewer HUB, talked about how it's better to go 7600X and upgrade later, than buy 12600K that was much cheaper than 7600X at that time.",hardware,2026-02-17 05:40:03,3
AMD,o5trjqq,they will probably go to 1460x and wont make any use of multigen sockets either. Most people dont upgrade frequently and never upgrade in same socket.,hardware,2026-02-17 06:59:59,1
AMD,o5rfs19,"Technically server vs workstation is just a label, and both chips can do either",hardware,2026-02-16 22:08:26,5
AMD,o622294,Server class chips are hugely profitable.  Intel makes faaaaaar more money on mobile than desktop. If only AMD could capitalize on a mobile chip with a yearly release cadence that made sense (100mhz increase is nothing). Intel at least was willing to throw more cores at the problem while skylake cores were aging.,hardware,2026-02-18 14:19:15,1
AMD,o5tjmea,Ryzen 5 and 7 has been stuck at 6 and 8 core respectively for 8 years straight with any higher core count sold at an upsell and some of you continue to live in 2017.,hardware,2026-02-17 05:52:52,0
AMD,o5ot1ge,"Well they did officially remove the ""Radeon"" branding from Instinct a while back......",hardware,2026-02-16 14:35:22,7
AMD,o5puhx6,> Sub-100 EUR market  sub-100 EUR market for CPUs has pretty much not existed for years due to inflation and cost of other components. The only things you can find in that price are some rejects that are a waste of money.,hardware,2026-02-16 17:34:27,12
AMD,o5tyd8i,"There is the 8500G or 8400G, but yes you're correct that the sub-$100 segment is pretty dead. But that was never a good price point for a CPU tbf.",hardware,2026-02-17 08:03:03,1
AMD,o5tiu9n,"Of course, 2700X came with a fan, that used to be pushed as a reason why AMD CPUs were better than even discounted Intel looking at forum debates of back then",hardware,2026-02-17 05:46:41,1
AMD,o5r9cvh,Not too shabby for people who aren't old enough to remember when Moores Law was in full effect and performance per dollar was increasing exponentially.,hardware,2026-02-16 21:36:49,-2
AMD,o5p1n6q,Not to mention the increase in power consumption going to Intel.  Arrow Lake however did take this to the proper direction of increasing performance per watt. So I'll give credit where credit is due. But AMD still has the performance per watt clinched fairly well. However it is changing.,hardware,2026-02-16 15:19:45,7
AMD,o5p8f02,Current Intel CPUs outperform Ryzen CPUs in power efficiency,hardware,2026-02-16 15:52:14,0
AMD,o5pzmd4,that belongs in a museum!,hardware,2026-02-16 17:58:25,5
AMD,o5tid0k,Doesn't justify the 80:20 split non X3D zen is getting,hardware,2026-02-17 05:42:54,1
AMD,o69pily,"Because individual consumers are seeing the largest price increases, and the largest encroachments on their ability to pay those price increases.",hardware,2026-02-19 16:57:29,1
AMD,o5v9zl1,"Well some newer wifi adapters only work on intel or amd.  They have support in the CPU or chipset for it.  They aren't m.2 cards universally anymore.    Mediatek doesn't support my OS so that's a deal breaker.  On older laptops, I've replaced m.2 wifi cards multiple times.  It's not guaranteed to be possible with newer models outside of whitelists.    I pretty much always want an Intel wifi adapter.  ax200/201, ax210 or possibly 9000 series or 8265.",hardware,2026-02-17 14:13:06,1
AMD,o5six9a,"They can but epyc versions tend to cost more and donâ€™t provide a benefit versus consumer ryzen.   (Iâ€™m running a ryzen 5700x, intel 245k, opertron, and a few Xeon servers in my basement)",hardware,2026-02-17 01:48:25,1
AMD,o625biy,AMD can only get so many wafers from TSMC.  And a good but low power mobile chip would probably have to be a very different product from either desktop or server which would complicate things a lot for them.  As is any of the lower binned dies are easily put into desktop instead of server parts.  That wouldn't be true with a dedicated mobile only die.     Probably why they keep sticking with monolithic APU dies for laptop which is a middling solution at best for power management.,hardware,2026-02-18 14:35:53,1
AMD,o5tklya,Are you all there?  Ryzen 9 is still a consumer sku. You're argument literally doesn't make sense,hardware,2026-02-17 06:00:57,6
AMD,o5t1ksw,"Moore's law has been dead for a while. It was in full effect on the early stages of CPUs becoming mainstream and up to the mid 2010s. Unfortunately, that's the time Intel was a monopoly.",hardware,2026-02-17 03:42:59,5
AMD,o5qt68l,"If by ""current"" you mean desktop chips, then that may be true in some massively multithreaded applications, but hardly in gaming.  But it's [hardly true in general for multithreaded either](https://cdn.sweclockers.com/artikel/diagram/34935?key=7a741bd411a393894791dd639ac37f7fu) with the 7950X3D completing a Blender render using less than 80% of the energy that a 285K uses.",hardware,2026-02-16 20:17:27,7
AMD,o5p9g7v,"honestly pretty impressive, i still would never trust hardware from that piece of shit company but nice to see you can at least get decent power efficiency now. i hadnâ€™t kept up with intel chips after they changed their naming system and i switched over to an x3D chip",hardware,2026-02-16 15:57:04,-3
AMD,o5r7dfx,So do you Dr. Jones. (Sorry couldnâ€™t resist),hardware,2026-02-16 21:27:08,1
AMD,o5v8t57,People really like mobo upgrade paths.  Especially right now if it lets them keep using their same RAM too.,hardware,2026-02-17 14:06:39,5
AMD,o5vank5,"Sure but its not hard to figure out what will work with what.    Yes the whitelists are the problem, like I said before, not the process of changing out the wifi card.    But that isn't Intel or AMD's fault.  That is the laptop OEM's fault.  Doesn't matter what processor you buy.  Only careful checking on what the OEM allows before you buy can save you there.",hardware,2026-02-17 14:16:43,1
AMD,o5sohzu,They can use proper buffered ecc memory,hardware,2026-02-17 02:21:56,3
AMD,o5tlhe6,Which was a new price tier released in 2019 and also has had the exact same core count 6 years later?Â   Your original goalpost was â€œ6 yearsâ€. There you have it.,hardware,2026-02-17 06:08:04,-1
AMD,o5t1yqk,I mean no AMD had the best processor for several years in there.,hardware,2026-02-17 03:45:28,1
AMD,o5p9uls,You know that X3D chips fail a lot?,hardware,2026-02-16 15:58:56,-7
AMD,o5vbck0,"AMD systems typically ship with Mediatek garbage now.  Intel has historically pushed their wifi for certain branding.   Again, it's a chipset thing now with wifi 7.  It's not a problem before wifi 6 at all aside from white listing.  As far as I know, lenovo and hp mostly did that.    On desktops, some motherboard vendors are now hiding the m.2 wifi cards under heatsinks to discourage replacing them.  I've seen pictures of a few that were soldered just like the RAM :(",hardware,2026-02-17 14:20:28,1
AMD,o5sqdxg,They support unbuffered ecc but you can do that with other amd cpus too,hardware,2026-02-17 02:33:24,1
AMD,o5tolbq,Nope. Bought my 5900x 6 years ago and it was like $100 more than 5800x. Really no big deal.   You got shares in Intel or something?,hardware,2026-02-17 06:34:14,5
AMD,o5t35kz,Depends on when you consider mainstream ig.,hardware,2026-02-17 03:53:09,2
AMD,o5pa606,have not seen any widespread issues ala the 13 and 14th gen shitshow,hardware,2026-02-16 16:00:24,6
AMD,o5vc957,"Its not a chipset thing its a whitelisting thing from the laptop OEM.  The chipset is not what is banning you from installing other wifi cards.  Laptop OEMs were whitelisting hardware on their laptops for years before even wifi5 came out.  Hell it was thing even the 90's!  It has nothing to do with wifi7 or AMD or Intel.  Its all about cutting support costs and forcing customers to buy new laptops by limiting upgrade possibilities.  For desktop it doesn't matter that the ""integrated"" wifi is just a soldered down m.2 wifi.  The PCIe slots are right there.",hardware,2026-02-17 14:25:18,1
AMD,o5stz80,"good luck getting proper ECC working on AMD consumer, with operating system level error reporting",hardware,2026-02-17 02:54:50,2
AMD,o5tt2pk,"â€¦so the tier itself still got no generational core count increase yet, right?Â   Thatâ€™s the original goalpost.",hardware,2026-02-17 07:13:48,-2
AMD,o5tr0sy,you defined timeframe as up to mid 2010s and in that timeframe AMD did have good CPUs.,hardware,2026-02-17 06:55:24,1
AMD,o5tr5ca,"both 7800x3d and 9800x3d had widespread burning issues reported, you just werent looking. Even AMD techfluencers talked about it.",hardware,2026-02-17 06:56:29,3
AMD,o5pam77,Because no one talks about it.,hardware,2026-02-16 16:02:31,-6
AMD,o5suvmo,I have a system with it now. Depends on motherboard support,hardware,2026-02-17 03:00:17,2
AMD,o5pd33n,"if it was so widespread it would be talked about, do you think thereâ€™s some sort of conspiracy wrt the hardware community at large to keep x3d failure rates hidden and intelâ€™s amplified?",hardware,2026-02-16 16:14:00,9
AMD,o78q3de,Meh. Need to see 16 core vs 16 core. 16 vs 8 kind of skews,hardware,2026-02-25 00:50:37,13
AMD,o79w9nx,Has anyone dug up what sort of fabrication process and die size those Google Axion chips are using?  Would be interesting to compare that against Xeon and Epyc chips.,hardware,2026-02-25 05:01:33,7
AMD,o79sqid,"Even with half the cores, the EPYC Turin system won more often than not, often by significant margins. And this is from a processor architecture that's been on the open market for nearly two years now.  There's cases where Axion wins quite convincingly (which it should -- the VM instance being tested has twice the cores), but it really goes to show how powerful AMD's processors actually are. As far as CPUs go, EPYC convincingly rules the data center. If only AMD's GPUs were as successful.",hardware,2026-02-25 04:36:49,30
AMD,o7gby3l,https://www.trendforce.com/news/2025/10/21/news-googles-axion-cpu-reportedly-built-on-tsmcs-3nm-set-to-drive-foundrys-data-center-revenue-growth/  TSMC 3nm apparently.,hardware,2026-02-26 03:36:23,2
AMD,o7cvgu2,"agreed, but I'd be nice to see how wide the gap is on a core-to-core comparison",hardware,2026-02-25 17:08:45,2
AMD,o7bkvmw,"This uses ARM N3 which is an E core, it's more than expected. Size wise it's very small and efficient so it wins on costs",hardware,2026-02-25 13:19:59,-3
AMD,o7cvaiu,It wins on cost because google can eat more of the costs.,hardware,2026-02-25 17:07:56,10
AMD,o6nw29q,"TLDW: HUB does a retrospective look at the AM4 upgrade paths, does some benchmark tests of Zen 1 and Kaby Lake all the way to Zen 5 and Arrow Lake (showing 5800X3D trading blows with i9-12900K, 285K and 7700X on 14 game averages with a 5090 running 1080p medium), and argues platform longevity is a factor (assuming AMD continues to maintain multiple CPU generation compatibility and continues to improve CPU performance to justify upgrades).",hardware,2026-02-21 20:57:17,61
AMD,o6o95rx,"Steve raises a good point, after building my own desktops for 23 years (and ironically all of those being on Intel platforms) I am having a super hard time envisioning Intel managing to stick to a single socket for just four years, certainly forgot about 6-8. Intel doesn't have the clout or the performance lead to get away with two year, even two generation sockets anymore. And I say that even despite Intel's new modular chip approach, which would allow Intel to maintain socket compatibility more easily than it could with monolithic chips. Intel has yet to commit to even a mid-length socket plan.   On the flipside, given the bulk of the volume of Intel's chips are sold through OEMs how much does the DIY segment ultimately matter? OEMs have to buy all the various parts anyway so what does it matter if the platform changes yearly. And it has even less bearing on servers. Seems like another decade of this could see Intel as the small player in the DIY space yet still remain the vast majority of OEM offerings.  My previous desktop was Haswell, the last DDR3 generation. Quite literally between the 4th gen and 9th gen there just wasn't a point to upgrade. I started getting antsy by 10th gen, but the 11th gen disaster that was Rocket Lake nixed buying into another dead socket platform. So after seeing how stable and long-term AM4 support had been, combined with the incredible 5800X3D capstone for the socket I was sold on investing into AM5. Sure DDR5 was pricey at launch but I'd already skipped the DDR4 generation entirely. And since X3D chips mitigate the effects of slower memory one can continue to use the same memory across the life of the system without having to replace it which more than makes up for the early upfront cost. Picking up a drop-in CPU upgrade in 4-6 years meant that for the early adopters one AM5 system can effectively last a full decade without compromise, for only the cost of a second CPU.  The third angle is that all the AM5 early adopters are now sitting in the catbird seat, even in spite of the AI shitfest dragging everything down into a cesspool of misery they can pick up a Zen 6 X3D chip for a drop in upgrade and remain entirely unaffected. Clearly the expectation we've all held about how things will remain normal and that hardware will always be getting cheaper (sans GPUs) probably needs to be reassessed. Sure prices will probably return to normal (or even crash given all the fab capacity being built today that will be online by 2030), but by building a system you won't have to replace means builders won't be exposed to begin with when the next crazy thing comes along. We've already had two crypto bubbles, covid supply disruptions, two separate trade tariff wars, and now we have AI induced mass inflation. All in the span of **one** decade. I think way more people are going to eventually recognize just how good having platform longevity is, particularly when it comes without compromises.",hardware,2026-02-21 22:06:41,38
AMD,o6rgyxw,"Great video, these revisions/retrospectives are some of my favourite HUB content. In spite of being very relevant, so few outlets do them - and nobody seems to do them as well.  AMD made huge gains within AM4 for sure, with the fantastic X3D CPUs to finish with a dominant gaming platform. What this video doesn't even highlight, is that when I bought a brand new system in mid 2021 (before the release of Alder Lake), AM4 was still straight up the best value for an all new gaming platform, and I bought it without any reasonable expectation of upgrading.  I feel Steve kind of glosses over AM5's expected upgrade path towards the end, where he does rightfully mentions that an in platform upgrade path mostly makes sense if you start on the lower end. Even now (or at least before RAM prices blew up) getting a 7500/7600X the upgrade path was a huge consideration as the 11800X3D (or whatever the name will be) should be a huge in platform gain (though nothing like a 1700X to 5800X3D). If you get in on AM5 now with a 9800X3D, I don't think the remaining platform longevity will ever provide a significant in socket upgrade.  Lastly, Steve mentions his surprise at the high end 7800X3D and 9800X3D being 2 of the top 3 best selling CPUs on amazon. While I do think the hype and FOMO are a part of it, with the increasingly high GPU prices we've gotten used to, and especially alongside obscene RAM and NVME prices we've had as of late, the added cost to the total system for going to an X3D CPU just isn't that big a part of overall system cost anymore. If you spend $800 on a GPU and $350 on 32GB of RAM and $250 for 2 tb of storage, how much is that $200 extra for the best gaming CPU money can buy? Purely speculative, but that's my take.  Either way I'm looking forward to what zen 6 will bring. The rumoured new IO die should be at least interesting to see if they can squeeze more performance out of this socket, though still not expecting anywhere near the gen-on-gen improvements they managed to make on AM4.",hardware,2026-02-22 12:42:59,17
AMD,o6qmzqw,"am4 was good because cpus were discounted very quickly in 2nd market. Motherboards used to be ""cheap"" as well.  I had 2600x with 1070 then upgraded to 6700xt then 5070ti still using 5700x because it does the job.",hardware,2026-02-22 08:06:31,3
AMD,o6om4g2,"Even for DIY builders I would argue that it is a clear minority that upgrade their CPU's more regularily than every 4-6 years. I of course don't have any data to back that up, but anecdotal evidence plus the fact that CPU bottlenecks take longer that a generational gap to truly become relevant I think kind of backs that up.  Assuming you upgrade every 5 years, is swapping out a motherboard really that big of a deal, both in terms of cost and time spent? I honestly don't see a world in which it is.  Taking price into consideration, the logic kind of undercuts itself. If youâ€™re upgrading every 1-2 years, youâ€™re likely not that budget-constrained to begin with, but an upgradable platform indeed makes sense in that case. If you are budget-conscious, the rational move is to hold onto your as-is system for longer, and at that point, a full platform upgrade often makes sense anyway because youâ€™re getting meaningful improvements beyond just the CPU.  So platform upgradability sounds great on paper, but in practice it only meaningfully benefits a narrow group of mid-cycle upgraders. For most buyers, total platform cost over time isnâ€™t materially different, and the flexibility advantage is smaller than itâ€™s often made out to be.",hardware,2026-02-21 23:20:31,9
AMD,o6nsqai,"Most people buy a new cpu every 5 or 6 years.Â  Yeah, it's annoying to occasionally need to buy a new mobo, but it's not the end of the world.",hardware,2026-02-21 20:39:27,22
AMD,o6t5hvw,Motherboard oems actually like to carry the same parts and materials for long lasting am4. This helps their supply chain management cost a lot.   They donâ€™t need to support the same board for 10 years.   They can simply change the model number of the exact same board every year and give you 1 year warranty and 2 or 3 year supports.,hardware,2026-02-22 17:53:40,4
AMD,o6updi8,AM4 was still  a good platform to buy into even when AM5 released.,hardware,2026-02-22 22:27:58,3
AMD,o6r64f0,"Every time I got a new am4 cpu I got a new mobo and ram, but that was just because I had sold my older am4 stuff. If not I would still stick with a b350 itx mobo and would use an 5800x3d. But I remember how Amd screwed over many am4 owners that had to upgrade to newest am4 board just to use zen3 cpus before AMD change its mind when am4 was eol. And remember when zen/+ was still a thing many techtuber recommended very expensive ram at the time, b-die.   Then when intel launched its skylake 8/9/10 the ram question was not as important anymore for the techtubers and yet fast ram was and still to this day is important for intel. Am on my 4th am5 system as well :P But I would not have been so if I did not damage one of the pins in the cpu socket on my lga1700 system that I loved the most of all my modern systems.   My lga1700 cpus before last summer did top out at say 140-160w of power draw in gaming, ie the loading of the map is included. My am4 cold pull 110w and so could my am5 cpus. The funny thing with lga1700 is that after last summer my 12700k would start to pull 200-220w in wz/bf. and that with only 8 p cores at 5.2ghz and 4.8ghz rinbus with ram at 7600/7800mt/s.   The funny thing is that 13900kf I hav only pulls 180w at most at 5.5ghz/5ghz ringbus with the same memory settings. And I am using the same bios, same max unlocked settings but the only thing that has changed is the windows version, drivers and game updates.",hardware,2026-02-22 11:09:25,1
AMD,o6osacu,My first AM4 CPU was the 5800X3D. Though I did buy it launch day sight unseen,hardware,2026-02-21 23:58:11,1
AMD,o6sruwb,"They're making the same mistakes over and over again.    There is no real doubt that the upgrade itself is cheaper, but there are a couple of problems in their takes:  1. They conveniently chose their upgrade times to fit their narrative. An upgrade at a different time would not have been as good of a deal.   2. The difference between the 1700X and 7700k was bigger than between the 14600k and 5800X3D according to their own testing? Why not use the part that was closer in performance?  3. If you bought Zen 1, you bought a 300 series motherboard and slower RAM with it. An AM4 upgrade would also have required a RAM upgrade, negating a lot of what you save.  4. Early 300 series boards, especially B350 had a lot of trouble with RAM and there are a lot of tests that show that modern AM4 CPUs don't run at their full potential and can't use memory that is as good.    You lose out on PCIe 4.0 and 5.0 too and are stuck with horrendously outdated I/O.    Why didn't they test the 5800X3D or 5700X on an actual B350 board?    They're whole argument is based on running a Zen 3 CPU with faster RAM on a different mainboard.   And that's just on their methodology.   The more general ""problems"" with platform longevity:  \- Using AM4 as an example is bad, because it only looks so good because Zen 1 sucked. It was overhyped, buying in on the ""the future will be multithreaded"" narrative and that's it. In gaming, Zen 1 was closer to Ivy Bridge or Haswell than Skylake.   \- They never asnwer the question on how much to factor it in in buying decision  \- They completely ignore that chipsets matter too, AMD only allowed Zen 3 much later on early AM4 boards (most oof which had some performance regression)  \- There is no guarantee in longevity. AMD always phrases it in a way that they're not liable for anything. They made the same promises about Threadripper and broke them thrice. After the first broken promise their new promises were even stronger. Why buy into a promise that the vendor will happily break if it benefits them?  \- There is no guarantee about performance. What if all new CPUs are a Zen 5% scenario all over again? You don't know before.   \- You still pay for it. In theory, a longer platform support is good but do they really think ""intel"" does it because they're greedy and AMD doesn't because they're friend?    In reality, neither AMD nor intel are really affected by that directly, motherboard vendors are. Do people really think that motherboard vendors are happy with having to support a product much longer and provide updates (which costs money) while lowering their own revenue due to that?    No, of course not. And that's why motherboard manufactures cheap out so much, especially on AM5 boards.    For the same price, motherboards are typically much worse on AM5, both in I/O and quality and that's likely also the reason why there are a lot more issues on AMD boards.    And the last point is important. Besides that, it seems to be simply a good thing to have longer socket support, but unfortunately that's not how it works.    If Asrock, Asus, MSI etc etc. sell you a motherboard that they'll have to support for ten years, they have to make the same profit with it as they would if you bought two boards fives years apart, otherwise they would hurt their business.",hardware,2026-02-22 16:50:25,-3
AMD,o6nb6bh,what a low effort karma farming post. no comment or anything.,hardware,2026-02-21 19:08:08,-15
AMD,o6y81wj,Platform longevity is a disadvantage. I'm not interested in a new CPU coupled with an outdated platform and deprecated I/O. I either don't upgrade or I upgrade both anyway.,hardware,2026-02-23 13:47:03,-3
AMD,o7205ak,"Would this mean: AMD for desktop is better for your wallet, while Intel on laptop since there is no upgrade path anyway?",hardware,2026-02-24 01:03:22,3
AMD,o71t9pd,> OEMs have to buy all the various parts anyway so what does it matter if the platform changes yearly.   It does make it easier to maintain compatibility with old stock. If MSI struggled to sell out all their Tomahawks for prebuilds and had leftover stock when a new gen launched then instead of dumping it you do a bios update and sell it with a 10600x. This would be especially beneficial for builders like Dell that like their custom motherboards.,hardware,2026-02-24 00:24:59,3
AMD,o6v6t78,"I think the X3D CPUs are top because theyâ€™re effectively the â€œcapâ€ of gaming CPU costs and theyâ€™re not that expensive when looked at as a percentage of the total system costs. Everyone from the upper midrange to the top (excluding people who just throw money away on 9950X3D, or have mixed use requirements) buys a 7800X3D or 9800X3D.  Previously it was spread around in multiple models in the upper midrange. Lots of gamers used the 12700k, 5800X etc. Not to mention all the non-X SKUs and KS, KF models.",hardware,2026-02-23 00:05:28,4
AMD,o6shzps,"the x3d cpus are the most popular cpus on the biggest hardware site/forum in the Nordic countries, Sweclockers. And that is also the case with 9070xt.  But like in Germany, AMD has always had a very staunch support by the hw/diy community in the Nordic countries.",hardware,2026-02-22 16:06:26,6
AMD,o74mze0,"It's been a bitter pill for me to swallow. When I built my PC, I thought I'd save a lot of money in the long run by upgrading components piecemeal. 6 years later and it's still going strong, and I've realised that (god willing nothing breaks) my next upgrade is basically going to be an upgrade of the CPU+GPU+RAM+Mobo.   Longevity makes sense if you like to upgrade every 3 years or so I guess. For the rest of us, it's not that much of a factor.",hardware,2026-02-24 12:58:34,1
AMD,o6y7i2x,Exactly. I do not even want to carry my old mobo with outdated I/O around because I'm usually keen on some new features and faster I/O.,hardware,2026-02-23 13:43:55,-2
AMD,o6nukv7,"It's far easier to replace a CPU than the entire motherboard. With Zen 6 desktop potentially launching in 2027, that's a 5 year span between Zen 6 and 4. Zen 7 on AM5 or AM6 is an unknown; it'll likely hinge on when DDR6 is commercially available to consumers.  I know I'm not looking forward to replacing my motherboard because of the damn internal USB3 cable connector that is near impossible to remove.  For the prebuilt desktop market, you're right people rarely upgrade. But that market could also be easily serviced with soldered CPUs. Dell, HP, Lenovo and etc often use proprietary CPU coolers (incompatible with ATX boards), boards (e.g. some of Dell's desktop boards has a section that directly connects to the case's front I/O panel instead of using cable connections), PSUs and cases in their design, which already limits upgrade options.",hardware,2026-02-21 20:49:23,30
AMD,o6vsq6z,I've upgraded dozens of AM4 boards. It truly deserves praise as a platform and I wish every socket had that sort of range.,hardware,2026-02-23 02:14:47,3
AMD,o78ilfs,This is specifically addressed in the video.,hardware,2026-02-25 00:10:33,1
AMD,o6vtdqf,"Also people never factor in that their old motherboard and CPU have resell value... A last gen motherboard isn't worthless and thrown away, you can typically get quite a bit of money back on it.   And new chipsets bring new features, so it's not like you're just buying a new socket each time, you're buying upgrades .  I see both sides of the argument, but really don't feel like 1-2 generations per socket is actually a negative.",hardware,2026-02-23 02:18:45,0
AMD,o6t4j0h,"I agree with a lot of what U said/wrote. Zen1 was pretty crappy, and when I bought an 1600 with a b350 board and lpx 3000c15 2x8(sr sticks) I had not issues, becaue the ram was a single rank kit and would go to 3466 without any issues. My friend which is a AMD fan, since waaay back(like me) used his phenom until zen 1 came lunched, and he bought the most expensive 1800x/msi x350 and 3200 dr kit which he could not get to work until like zen+ was a couple of months old.  When I did some head to head comparisons the r5 1600 basically was slower than my 3770k with ddr3, crazy. bf should have been the base case scenario for the zen cpu.  The ram back then was pretty meh, until b-die got recommended as a standard, and it was a fairly costly recommendation from the techtubers, and today the market is filled with only craptastic ddr4 ram. But when zen 3 launched we did not see them push for fast ram on the intel side like they did for am4.  Still remember that the techtubers showed super skewed ram settings, handicapping intel back then, and still the thing up to today, even if 7200from hub is pretty okey, even though that ram was not premium at all back then, was about the same price as my 6000c30 for my amd rig.(same hynix ram basically)  Long socket support is of most importance to me, but not for my friends that no longer are that hw interested, but for sure it would have made it easier for them if that was the case when they upgraded after a few years.  Pretty sure the 5% ""allegory"" or so is what we have to expect now from all cpus in gaming. Dont think where will be any cpu that can push an 5090 at all that much more than we already see. We will basically still see it being bottlenecked at lower res/settings like we do today.",hardware,2026-02-22 17:49:13,2
AMD,o6t4u0f,"Their own testing and data bears out the fact that Intel provided more meaningful upgrades, platform longevity notwithstanding, for anyone who doesn't do incremental updates.   Which would be most people.  If you're going to keep a new CPU you purchased for at least 1 year - Intel is undoubtedly giving you better performance when you upgrade in the 2nd year or later.  Early 2017 7700K -> Late 2018 9900K: 4C8T to 8C16T  Late 2017 8700K -> 2019 10900K: 6C12T to 10C20T  2018 9900K -> 2021 12900K  2019 10900K -> 2022 13900K",hardware,2026-02-22 17:50:36,0
AMD,o6ngsf4,"""karma farming"" with a HWUB video on this sub? good one.  It's mostly a retrospective video looking back at AM4 and how it progressed vs Intel. If you want an earnest comment: It lasted through 4 Intel socket changes and while not winning on performance for most of it's lifetime, it gave extraordinary value and especially so if you adopted early and hopefully it continues with AM5.",hardware,2026-02-21 19:36:38,16
AMD,o6sygsb,For some reason they seem relatively cheaper here vs Intel and Nvidia. Whereas i gather from people posting here that in North America they lag behind Nvidia once performance to price ratio is considered.,hardware,2026-02-22 17:20:32,1
AMD,o6nvu41,"What you spend in cpus another person spends in motherboards. It balances out.Â    If you bought a 8700k then a 12900k, you got the same generational span a ryzen user got, with two mobos instead of one.Â  It's not that significant.",hardware,2026-02-21 20:56:05,-2
AMD,o6puwr5,"5 year span, but it will hardly be worth the upgrade it looks like to me. How much faster can Zen6 be than Zen4? Zen5 was only like a 3-5% improvement. I'd guess most of the silicon budget and node shrink will go to increasing core complex from 8 to 12 cores. I don't know if I'd upgrade my Ryzen 7700x for what will likely be a 10-20% single core performance boost. Maybe to a Zen6 x3D chip eventually. But I'm more likely to wait for Zen7, and see if that still fits, and if not I'll get a Zen6 x3D when it's on sale.",hardware,2026-02-22 04:09:24,1
AMD,o6qrv8g,Yeah that additional 1.5 hours swapping motherboards is a nightmare.,hardware,2026-02-22 08:53:09,-5
AMD,o6tfdrm,>Their own testing and data bears out the fact that Intel provided more meaningful upgrades  >If you're going to keep a new CPU you purchased for at least 1 year - Intel is undoubtedly giving you better performance when you upgrade in the 2nd year or later.  Simply not true:   ######Intel CPUs:   7700k > 8700k = 24% performance increase  or  7700k > 9900k = 43% performance increase  8700k > 10900k = 23.7% performance increase  or  9900k > 10900k = 7.35% performance increase  10900k > 12900k = 22.6% performance increase  So 7700k > 12900k = 88.4% performance increase   Each of these incremental upgrades will require a new motherboard   ######AMD CPUs:   1800x > 3800x = 37% performance increase  3800x > 5800x = 24% performance increase  5800x > 5800X3D = 31.5% performance increase  so 1800x > 5800X3D = 123.81% performance increase over the AM4 lifespan.,hardware,2026-02-22 18:38:24,7
AMD,o6njgaq,so what was hub wrong about? I don't get the videon,hardware,2026-02-21 19:50:34,3
AMD,o6nxnlx,"I spent about $110 in total upgrading from a Ryzen 1600 to a 5600x (sold the 1600 after the upgrade to get the $110 cost), and this was on a $75 Asrock B450m Pro4 board.  My only regret was not waiting another year for 5700X3D to show up. One of my friends bought that CPU for a little over $120 on Aliexpress.  EDIT: I looked back at my old research I did when I initially built my Ryzen 1600 desktop. The Intel build for about the same CPU+board cost was an i3-9100F with a H310 or B360 board, which meant no CPU/RAM overclocking and no cost effective upgrade path. Meanwhile I had overclocked the 1600 on the Pro4 board and ran it with DDR4-3200 (the i3-9100F only supports up to DDR4-2400).  An extra $110 for ""future-proofing"" the Intel CPU would yield maybe an i5-9400F or 9600K (on the same locked down H/B board, because a Z series board easily add +$100 to the cost), both of which put it in direct competition against a Ryzen 3600 + Pro4 build and would lose against an overclocked 5600x and especially the 5700X3D.",hardware,2026-02-21 21:05:47,17
AMD,o6ovsju,"My partner recently upgraded from a 2200G to a 5600 on the same board, as all they had to buy was the new chip. If they'd bought an Intel based PC at the time, they'd be stuck on an 8th/9th gen platform and would have had to buy a new motherboard to get a comparable CPU upgrade. This would have meant that the upgrade wouldn't have happened and they'd still be on an 8th gen i3 because they wouldn't be able to justify an upgrade. It absolutely matters.  Same story for my cousin who upgraded from a Ryzen 1200 8GB RAM, to a 3600 16GB RAM, to a 5800X3D 32GB RAM all on the same B350 board. Massive upgrade each time simply by changing CPU and adding 8GB RAM sticks",hardware,2026-02-22 00:19:53,19
AMD,o6nxc25,">What you spend in cpus another person spends in motherboards. It balances out.   You're under the assumption that Ryzen users upgrade their CPU on every release?  >If you bought a 8700k then a 12900k, you got the same generational span a ryzen user got, with two mobos instead of one.  It's not that significant.   And if you bought a 1700x then a 5800X3D, you got a platform with more performance and didn't have to spend an extra $200+ on a new motherboard, that's the point.  Also, from the video's performance chart: https://i.ibb.co/R46KDTFr/image.png  The 12900k was slower and that's with DDR5 7200 memory. So an extra ~$150+ if you bought it back in 2021.",hardware,2026-02-21 21:04:05,18
AMD,o6qxjv2,"Zen 6 is expected to have a new I/O design to replace Infinity Fabric with a fan-out layout (already used by Strix Halo): https://www.youtube.com/watch?v=maH6KZ0YkXU  The expected improvements are lower power usage by the I/O, including at idle, and much larger bandwidth to remove the bottleneck between the CPU and fast DDR5 memory.",hardware,2026-02-22 09:47:57,10
AMD,o6tnqon,I'm specifically talking about NOT doing piecemeal upgrades. Which necessarily means not caring about what motherboard you buy along with the CPU when upgrading.   In isolation AM4 upgrades look impressive but it is rather due to the base effect of early Zen being pretty crap in gaming.  It is reflected in the graphs too.  8700K -> 10900K gave you the same uplift through one upgrade that you would have to do three upgrades on AM4 (1800X ->5800X).  And to say nothing of the fact that all this is based on hindsight. Any number of things could have turned out differently to cause AM4 upgrade prospects to be different from what they turned out to be.,hardware,2026-02-22 19:17:41,1
AMD,o6wv0hw,You need to put an asterisk next to the 5800X3D as the extra cache does not benefit all programs nor all games equally. Some will barely see any increase while others may see a performance decrease from the reduced clock speed.,hardware,2026-02-23 06:49:54,1
AMD,o6nklhv,"The title was a response to a lot of people making the argument that platform longevity ""doesn't matter"".   The video basically goes over the fact that in the span of the lifetime of a 1700x owner being able to upgrade directly to a 5800X3D and match AND eventually beat the equivalent Intel CPU - the 12900k, Intel went through 4 socket changes.",hardware,2026-02-21 19:56:30,10
AMD,o6u6ifk,"had my brothers first pc (for him personally with basically no income) ryzen 1600 (80 EUR} with 1060 6gb I gave away, few years back upgraded to 3060 ti and last year upgraded to 5600 how many generations later still only cost 83 euros, there's absolutely no way you could ever do that with Intel, best part I don't remember if I ever used a screwdriver.  sure the pc was in a old and with missing parts case with ketchup and mustard PSU cables but he was able to play basically anything he could want with his G29 wheel and the only pricey part was the 3060 ti because he bought it at the mining craze and 1060 blown a fuse",hardware,2026-02-22 20:51:56,5
AMD,o6qvfbd,But also if you bought a 1700x it was on a significantly worse platform and underperformed at anything that wanted ST performance.  For the 5800x3d it released quite late into it's generation so the more direct comparisons can't be made for upgrading/not upgrading at launch as the people buying 12th gen or 5000 series at release wouldn't know anything about it,hardware,2026-02-22 09:27:19,-3
AMD,o6tpext,But the whole point of the video is showing you that you didn't NEED to buy a new motherboard if you had AM4. Not to mention the fact the numbers are a big skewed in later generations (he used DDR5 7200 for 12900k and upwards).   So if you were an early adopter of lets say a 12900k:  $670 MSRP for the CPU   ~$200 for the DDR5 kit (it was very expensive even for lower end kits in the early days)   $300+ for even an entry level Z690 Prime motherboard  All of that extra money JUST to overtake the existing 5800x by 25%. Then 5 months later along came the 5800X3D which was just a drop-in upgrade to your existing hardware and beat the 12900k.,hardware,2026-02-22 19:25:47,-1
AMD,o6tse0x,"It didn't show that.   They didn't use the same motherboard, they used a much newer 500 series motherboard with much faster ram and better ram support.   >$300+ for even an entry level Z690 Prime   there is no need for a Z chipset.",hardware,2026-02-22 19:40:36,2
AMD,o6w00i9,When people upgrade they take into consideration the performance uplift they will get from what they had previously.  They don't think about Intel vs AMD.  And you're acting as if the old CPU and motherboard can't be sold in the used market to recoup some of the costs and put in in the new purchase.,hardware,2026-02-23 02:59:15,0
AMD,o6tuh5m,">They didn't use the same motherboard, they used a much newer 500 series motherboard with much faster ram and better ram support.   They used DDR4 3600 which even a B350 could easily achieve.  >there is no need for a Z chipset.   With a 12900k?? of course there is if you actually wanted to overclock it.",hardware,2026-02-22 19:51:09,1
AMD,o6tvnoy,">They used DDR4 3600 which even a B350 could easily achieve.  Lol, no.   Lot's of test have shown performance regressioon n older boards",hardware,2026-02-22 19:56:58,1
AMD,o6pha1c,"> who say that a CES 2027 launch seems likely for both new CPU generations.  I mean, if Nova lake launches in late 2026, Intel would still 100% use CES to promote Nova lake again.",hardware,2026-02-22 02:36:24,22
AMD,o6p5edp,"I find it hard to believe that Intel wouldn't launch at least the -K series of NVL by the end of 2026, especially since not doing so would mean that they didn't meet investor promises. They have already announced, multiple times, that NVL will be out this year.",hardware,2026-02-22 01:19:17,27
AMD,o6padw7,"If the price tiers don't shift, this actually seems to be a pretty hefty boost in multi-core performance per tier.",hardware,2026-02-22 01:51:35,8
AMD,o6p7zl9,"I wonder how many EU in Xe3 Nova-S.   \> ultra 3   \> 4/4   wait, is that pentium in disguised o_O?",hardware,2026-02-22 01:36:04,2
AMD,o6oxw17,"I thought Intel is not calling it ""Celestial"" and just Xe3?",hardware,2026-02-22 00:32:36,2
AMD,o6pu30o,Is the assumption here that this is an issue with securing N2 production capacity or simply soft demand for desktop chips since nobody wants to upgrade given current RAM prices?,hardware,2026-02-22 04:03:32,1
AMD,o6pbb34,"8, 8, and 6 all under ultra 5 is a bit stinky. Should have raised all the ultra 3 to 6P. Call the 4P pentium or something there will still be a good market for that",hardware,2026-02-22 01:57:29,-1
AMD,o6st5qn,I rather buy from AMD than from Intel.,hardware,2026-02-22 16:56:22,-5
AMD,o6q0q2w,"doesnt matter. ram will be too expensive by 2027, nobody will be able to afford these.",hardware,2026-02-22 04:51:36,-7
AMD,o6plpjm,"Just launch K CPUs then normal CPUs, should be able to generate hype twice",hardware,2026-02-22 03:05:46,14
AMD,o6skp03,CES is also a showcase for the mobile CPU lineup and for OEMs to show the new laptop modells.,hardware,2026-02-22 16:18:09,3
AMD,o6ppfug,There's always a chance of December 14th 2026 like Meteor Lake in 2024.,hardware,2026-02-22 03:31:21,9
AMD,o6phuuq,"It doesnâ€™t have to be NVL-S, it could be NVL-H",hardware,2026-02-22 02:40:14,5
AMD,o6qpfz7,"I wouldn't find it hard to believe. 18a volume is very low. Despite Panther Lake already ""launched"" you still can not buy them. Where are they?   Intel simply does not have the 18a volume to support the mobile and desktop line. At least not anytime soon.",hardware,2026-02-22 08:29:48,-1
AMD,o6ppa0x,"It canâ€™t shift too much, whatâ€™s the absolute ceiling the Ultra 9 sku could be? 699? 799?",hardware,2026-02-22 03:30:15,4
AMD,o6p9snb,"There is no Pentium with that many P and E cores, and the current i3 is 4/0/0, so a 4/4/4 would actually be a pretty significant upgrade (if it even comes)",hardware,2026-02-22 01:47:46,10
AMD,o6piks9,I donâ€™t see how it would be anything but the 4EU tile,hardware,2026-02-22 02:44:55,3
AMD,o6pf83u,NVL will have Xe3P but dont know if that includes NVL-S.,hardware,2026-02-22 02:22:51,2
AMD,o6oy6x7,Where do you see them calling it Celestial?,hardware,2026-02-22 00:34:26,2
AMD,o6qdnli,"Late delays can be some validation issues. Found a bug, need a new stepping, that takes a couple of months.",hardware,2026-02-22 06:39:06,1
AMD,o6piszh,Them even having a -3 tier at all based on the current uArch providing current gen IPC/efficiency is already better than what the competitor has been doing.,hardware,2026-02-22 02:46:25,7
AMD,o6wgyga,Maybe you should buy based on what each company has to offer,hardware,2026-02-23 04:54:27,5
AMD,o6q9h55,"I wouldnâ€™t be so sure. Even if the AI bubble doesnâ€™t pop, thereâ€™s lots of people with DDR5 in desktop systems now. If the performance of Nova Lake is good enough, if the value is there, then people will buy, enough to make it a success.",hardware,2026-02-22 06:02:05,5
AMD,o6urfi9,"opposite, Non K first if they beat Ryzen 9000, K after",hardware,2026-02-22 22:39:03,1
AMD,o6qzh9t,"No one cares about non K CPUs. Heck, do we even know if non K CPUs were affected by the issues Raptorlake K CPUs had?",hardware,2026-02-22 10:06:25,-6
AMD,o6plo6c,"When was the last time laptop parts have launched before desktop parts from the same architecture/family when both lines had desktop and mobile parts?   ARL-S launched before ARL-H, RPL-S launched before RPL-H, ADL-S launched before ADL-H.   And generally in those cases the -H series launched near/at CES, since that's when you get a bunch of new laptop refreshes anyway.",hardware,2026-02-22 03:05:31,14
AMD,o6plu4p,"PTL-H just dropped recently though, itâ€™s hard to believe that they wouldnâ€™t wait at least a year before dropping it.",hardware,2026-02-22 03:06:37,3
AMD,o6tmodo,">I wouldn't find it hard to believe. 18a volume is very low.  Considering it just launched on a new node, this isn't too unexpected. Maybe Intel is reluctant to ramp up tons of volume of PTL with terrible yield until they can improve it, unlike what they might have done in the past with initially poorly yielding nodes.   >Intel simply does not have the 18a volume to support the mobile and desktop line. At least not anytime soon  Well it won't be any time soon, it's almost a year out. And Intel appears to have the 18A volume to support the low end laptops (wild cat lake), and server (diamond rapids and clearwater forest) too, but desktop is too much of a stretch?   And even then, how would the few months of delay between CES and a Q4 2026 launch grant them the volume to then launch a whole new segment that they allegedly didn't have the volume for before?",hardware,2026-02-22 19:12:28,3
AMD,o6qzo2s,Unverified reports,hardware,2026-02-22 10:08:13,-1
AMD,o6qd1m7,I guess positioned against amd -950x pricing. So it is possible to increase a bit on the top end.,hardware,2026-02-22 06:33:31,6
AMD,o6q8zsv,"More. I think theyâ€™ll position the two compute die version as a Xeon/HEDT part, so perhaps something like 699 for the consumer X3D competitor (8/16/4 + bLLC), and 1399 for the full-fat top-end 52 thread beast. Nova Lake is going to be expensive, how could it not be? I might even be understating the prices. I would *love* to be wrong about this, though, and have cheap CPUs. I just donâ€™t think itâ€™s going to happen, unless they get externally subsidized somehow, or are truly desperate to regain consumer market share, and are willing to sell at tiny margins to get it done.",hardware,2026-02-22 05:57:58,3
AMD,o6xg3iu,"If the large cache 52 core part is less than 1000 USD I'll eat a hat.  Or rather, I'll buy it.",hardware,2026-02-23 10:12:53,1
AMD,o6pnr10,"Welp, i thought 4/4 is 4core/4thread based on the slide.",hardware,2026-02-22 03:19:43,2
AMD,o6po5ml,4EU tile accross the entire line-up :thonk:? Man here i hope the new xe3 in nvl-s will outperform rdna3 740m.   gonna see how is xe3 nvl-s vs 610m olympic ridge (doubt amd uses rdna 820m in their non-apu desktop).,hardware,2026-02-22 03:22:30,2
AMD,o6q68jg,"MLID claimed the 12EU tile is coming to desktop, FWIW",hardware,2026-02-22 05:34:54,-1
AMD,o6pyrae,It won't. Only NVL-P has Xe3p.,hardware,2026-02-22 04:36:45,4
AMD,o6p8050,On the article?,hardware,2026-02-22 01:36:10,5
AMD,o6qdu4k,Both AMD and Intel are seemingly affected though.,hardware,2026-02-22 06:40:48,3
AMD,o73whlg,"you also generally NEED to launch non-K first. the K silicon are the more ""perfect"" and high performing bins of the die. Early yields for a new die will be lower at first, getting better with time, producing more high performing binned silicon",hardware,2026-02-24 09:22:23,1
AMD,o6pnnca,"Comet Lake-H landed about a month or two before Comet Lake-S, no? That whole launch is a bit fuzzy, admittedly.",hardware,2026-02-22 03:19:01,7
AMD,o6q9p8q,"Well, they did release Rocket Lake (11th gen desktop) in Q1 of 2021, then Alder Lake (12th gen desktop at least) in Q4 of 2021, so if they did do that it wouldn't be the first time.",hardware,2026-02-22 06:04:01,2
AMD,o6rg9nc,"PTL-H does not target large laptops with dgpus, only has 4 P-cores, and too much silicon is dedicated to the large igpu. I could definitely see Intel releasing a new series with more focus on the cpu.",hardware,2026-02-22 12:37:38,1
AMD,o6tu3s3,"> And Intel **appears** to have the 18A volume to support the low end laptops (wild cat lake), and server (diamond rapids and clearwater forest) too, but desktop is too much of a stretch?   It doesn't appear to have anything because you can't even buy 18A yet. PL should be on shelves at the end of march is what I am hearing. And its a tiny chiplet. The rest is from TSMC. And then we have to see about volume. And then we have to actually see how they will roll out DR and CWF.   Especially you should not be so optimistic about Intel and 18A. I remember you being very sure that 18A would be out by 2025.  https://www.reddit.com/r/AMD_Stock/comments/180vg7n/intel_outsources_cpu_chip_mass_production_for_the/ka9wcnk/",hardware,2026-02-22 19:49:17,-3
AMD,o6r41cv,It is unverified that Panther Lake is still not on the shelves even though it has been launched?,hardware,2026-02-22 10:49:47,3
AMD,o6qce6m,>Â I think theyâ€™ll position the two compute die version as a Xeon/HEDT part  If thatâ€™s the case itâ€™s not running on this 128bit memory bus platform at all and wouldnâ€™t be part of this lineup.,hardware,2026-02-22 06:27:41,2
AMD,o6potvy,4 or cut down version with 2. From Panther lake we know they have either the 12 core tile or the 4 core tile.  Laptops with Panther Lake 4 Xe-3 have been benchmarked already and it actually performs like 7 Xe-2 Lunar lake 130v. Miles ahead of 4CU 840m and actually more like 8CU 860m.,hardware,2026-02-22 03:27:10,4
AMD,o6q6fxa,MILD claims a lot of things,hardware,2026-02-22 05:36:38,4
AMD,o6pftfw,"Where in the article? Call me blind, but not seeing it.",hardware,2026-02-22 02:26:46,1
AMD,o6smi9a,"N2 probably continues the trend of being more difficult to design for than earlier nodes, so it wouldn't surprise me if both AMD and Intel got hit with validation issues.",hardware,2026-02-22 16:26:19,1
AMD,o6slgyt,"Desktop Broadwell (which granted was very limited) was released in June 2015, and desktop Skylake was released just 2 months later in August 2015, so not without precedent either.",hardware,2026-02-22 16:21:37,3
AMD,o6ty4li,">It doesn't appear to have anything because you can't even buy 18A yet.  You definitely can. Well I definitely can at least, there is a model I can go grab at my local bestbuy in the US in 3 days, in my region, if I order it online rn.  >PL should be out at the end of march is what I am hearing.  What are you hearing lol  >Â And its a tiny chiplet.  Larger than AMD CPU chiplets, but it's pretty small, sure.  >he rest is from TSMC.  The 18A chiplet is >2x larger than the iGPU TSMC N3 chiplet, and the PCT tile is built on an external legacy node, not very important.  >And then we have to see about volume. And then we have to actually see how they will roll out DR and CWF  Intel has clearly shown they don't care *that* much about volume for a ""launch"", so what exactly then is preventing them from launching desktop in 2026, especially since rumor is that those compute tiles are external anyway?  >Especially you should not be so optimistic about Intel and 18A.  I was banned from the intel stock subreddit for being too pessimistic about Intel and 18A. Just ask u/Due_Calligrapher_800. It's only in the eyes of AMD stock holders that I can come off as optimistic about Intel lmao.  >I remember you being very sure that 18A would be out by 2025.  In that comment, the very first sentence that you linked:  >Using info given by Intel themselves is propaganda? **It's fine to say it's hard to believe...**  And in that thread, you can also see both you and the person I replied to first in that thread just outright lying about what Intel claims. Such as pretending like Intel said lunar lake was going to use 18A, and then you claiming that Intel said there was going to be a 50% perf uplift between Intel 18A and Intel 3.  You of all people should not be throwing stones at missed predictions though. I still remember your hilarious [Turin 1.5x - 2x the perf/watt as GNR](https://www.reddit.com/r/hardware/comments/1fofwsb/comment/loq3jja/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)... which after it was revealed it was not the case, you tried doubling down, and [I whooped you in the comment sections there again too](https://www.reddit.com/r/hardware/comments/1g0pa1d/comment/lrb7do7/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).  Though also, unlike you, I have [no problem eating crow](https://www.reddit.com/r/intelstock/comments/1nvfkjw/comment/nh8fk15/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) when I had a bad prediction, such about my thoughts on Pat Gelsinger. But in that comment you linked, I pretty clearly said it was okay to doubt Intel's timeline.",hardware,2026-02-22 20:09:14,6
AMD,o6qfsc4,"Iâ€™m not meaning to say that it will literally be a workstation part, but I sure could see them using that as a (attempted) justification for super high prices to consumers. Whether that attempt would succeed is another story. Personally, I think Price-Performance will be worse than Zen 5 (much less Zen 6), but I sure hope Iâ€™m wrong about that too. We need strong competition.",hardware,2026-02-22 06:58:35,0
AMD,o6qbmj6,"Hmm after a short surfing, intel puts the 4 (32EU) Xe3 variant in the entire lineup is kinda a day dreaming for me :D (core ultra 3 with 6000mhz ram + 32EU xe3).   But, if the lowend nvl-s gets 16EU variant that'll be another story.",hardware,2026-02-22 06:20:51,1
AMD,o6ph0mb,The pic of the SKU table.,hardware,2026-02-22 02:34:40,3
AMD,o6pizp1,The intel table documenting the skus and their different breakdowns,hardware,2026-02-22 02:47:37,3
AMD,o6u0opa,"> You definitely can. Well I definitely can at least, there is a model I can go grab at my local bestbuy in the US in 3 days, in my region, if I order it online rn.  Well you can but you can't because not right now because you can... in three days. Ah yes. So you actually CAN'T yet.  Again believe Intel at your own peril.   Oh I can eat crow too. For some comparisons my napkin math prediction was off. See, I too, am extraordinarily humble!",hardware,2026-02-22 20:22:14,-5
AMD,o6qc5mr,Low end probably gets 2 cores because thatâ€™s what lower end Panther laptops use too,hardware,2026-02-22 06:25:32,2
AMD,o6pyx3v,"Ah. Well to answer the original question then, that's not an Intel table at all. Someone just made it based on leaks.",hardware,2026-02-22 04:37:58,7
AMD,o6xbb05,"This is the best kind of ""technically..."" panther lake is not that hard to get, I just pay for 8 of them this morning and they will be delivered to my office tomorrow morning. Technically it's also not ""right now"" by your definition",hardware,2026-02-23 09:26:28,6
AMD,o6us5sv,"My friend literally has a Panther Lake laptop in his hands, it is actually on shelves, just in a (extremely) low volume quantity.",hardware,2026-02-22 22:43:01,3
AMD,o6xp6x2,"Well, I thought it was end of March not February, my bad. That does not matter for the argument though. It is always those kinds of shifting goal posts with intel.  18A by 2023. Well no, by 2024. Well actually 2025. Well no, by the end of. OK by CES 2026. Well not really CES but in a few weeks.  Then it is not: ""Well it is out now and then client and server by this year."" Which this thread wants you to believe.  It is going to be low volume mobile parts. Then until that is ramped we are well into 2026. They may bring out a few token DR, CWF and client parts but not in any volume that matters. And clearly from the independent testing we got 18a is not any better than TSMC N3B. (When you ignore that hand waving with the GPU on TSMC N3).  And that is the point. People always fall for Intels marketing and you can't convince me otherwise like you couldn't have convinced me in 2003 that 18a will be available by 2025.",hardware,2026-02-23 11:36:46,-1
AMD,o6xw8uq,"I do agree with you, you just chose a completely wrong example, because Intel clearly has no trouble with Panther lake so far (whether the yield is good or they simply stockpiled before this launch is a different matter)",hardware,2026-02-23 12:31:56,1
AMD,o6xxmkm,There is a third option. Intel just eats the bad yield and pumps out a lot of defective dies. They own the process after all.  But the problem is not with PL. Its OK for what it is. The big GPU has actual benefits. The problem is 18A.  https://xcancel.com/jukan05/status/2025064438777495845,hardware,2026-02-23 12:41:53,1
AMD,o6e0sh9,"Not a fan of the use of the word ""rebound"" - it implies the prices are going back up after a decline.",hardware,2026-02-20 08:23:47,174
AMD,o6df8iw,not many will buy GPUs when they cant get RAM to build their damn PC lol.,hardware,2026-02-20 05:13:39,61
AMD,o6eguj6,I feel like this article is market propaganda skirting the fact that the retailers were just riding the panic buy wave.   Forced to reduce prices? Thats insulting for some reason. The panic buy is over and they can't sustain the price gouging. I'd frame it as caught red handed being opportunistic greed bags.,hardware,2026-02-20 10:53:19,22
AMD,o6dxrcs,"We did it, reddit, we bullied people into believing that amd software stack is so bad their cards aren't worth it even when same tier nvidia ones are $300 more expensive!",hardware,2026-02-20 07:55:22,31
AMD,o6f7s6a,">reduced demand has forced retailers to drop prices for both the RX 9070 XT and RX 9060 XT 16 GB GPUs by as much as 15% and 20% respectively.  So even with NVIDIA cards selling far over MSRP, no one is buying AMD cards.   This should be a red flag to AMD to improve FSR",hardware,2026-02-20 13:55:18,15
AMD,o6j2d4y,"Prices have definitely dropped here in Australia too.  If it wasnâ€™t for NVEC, DLSS, and RTX Remix mods that I love playing around with, Iâ€™d for sure upgrade my 3080 to a 9070 XT as itâ€™s the best bang for buck gaming GPU on the market right now.  The comparable 5070 is still $450 more expensive here in Australia ($1399 just about everywhere).",hardware,2026-02-21 01:31:24,2
AMD,o6etd7z,'rebound' would suggest prices dropped when that never happened. Prices of AMD GPUs only went up after the first 5-10 minutes after release due to the retailer rebate bullshit AMD pulled. >seen as a viable alternative when NVIDIA GPUs were unaffordable or unavailable  Only card I ever saw that was in such high demand stock couldn't keep up is 5090...,hardware,2026-02-20 12:29:57,4
AMD,o6giykv,Wake up call to the amd echo chamber here in reddit. Literally nobody buys their cards even though Nvidia counterparts are far above msrp.,hardware,2026-02-20 17:41:34,3
AMD,o6hht2h,GPUs are so expensive now that there is no point in going 100$ cheaper AMD route. If you have the money to buy PC you have to splash so much and might as well get superior option in Nvidia.,hardware,2026-02-20 20:23:52,0
AMD,o6ft502,"Clickbait, this is only in Japan, prices have not lowered in other markets",hardware,2026-02-20 15:42:37,-7
AMD,o6e2mz0,Like seriously. Did the author pass middle school English ?,hardware,2026-02-20 08:41:25,79
AMD,o6dkn0e,"would it not be normal for people with older systems to upgrade their GPU's?  As long as their existing cpu and ram works fine, I'd assume getting a new gpu isn't that rare.  Is there any recent survey/data available on how frequently certain parts are commonly upgraded?",hardware,2026-02-20 05:57:25,24
AMD,o6ed102,"The prices arenâ€™t bad considering mooreâ€™s law is dead. Iâ€™ve been gaming since the 90s and even with the inflated prices you can acquire hardware thatâ€™s cheaper and better than what was available 5 years ago. You can build a system with 16gb ram and still play absolutely everything. The low to mid tier cards play absolutely everything. Heck, even onboard graphics is in a better place with more competition there.",hardware,2026-02-20 10:18:59,2
AMD,o6ej1d4,"You speak like you understand basic supply and demand needs.  They expected a red wave - it never materialized, welps back on the discount shelf you go.  Meanwhile NV product are selling like hot cakes, so GOUGE GOUGE GOUGE!!!!",hardware,2026-02-20 11:11:58,7
AMD,o6e60mh,"AMD is unusable, the upscaling and hallucinations look marginally worse in a zoomed in slowmo video of videogames! Better buy the more expensive card of the other mega corp that would sell your kidneys if they could. I don't get why people In this space put up their tents in the camps of corporations and the go around complaining their favourite trillion dollar company is raising prices or stops making products for them, or stopping support for old hardware or somehow engineer the worst connector we've seen in a decade or screw them/us in other ways. Imagine if you go and meet your favourite actor and he slaps you in the face, would you still defend him online?",hardware,2026-02-20 09:13:28,-6
AMD,o6hgejg,"> So even with NVIDIA cards selling far over MSRP, no one is buying AMD cards.  Nvidia 60 and 70 cards were also selling below MSRP for a while.  > This should be a red flag to AMD to improve FSR  I'm sure that's your *real* concern.",hardware,2026-02-20 20:16:56,0
AMD,o6ggndq,"Or hear me out, just build a better performance cards like before DLSS and FSR.",hardware,2026-02-20 17:30:45,-1
AMD,o6g20l5,"RDNA4 cards went well below (~10%) MSRP in Europe before Christmas.  To be fair, so did Nvidia cards. A 5070 Ti would've been a great deal two months ago if only I had need for one.",hardware,2026-02-20 16:23:43,6
AMD,o6h2a23,"I would rather buy Intel's mid range card (if they will ever do that like e.g. AMD's 9060/9070) than AMD. Like I literally lost all my fate in AMD that I would rather bet on Intel who just entered the market.  But yeah it's absolutely crazy that people will rather pay 300$ more for NVidia than AMD, but after my experiences with AMD I'm really not surprised. DLSS is years ahead compare to whatever the fuck is AMD doing, and even FSR 4.0 INT8 looks worse on officially unsupported GPUs compare to DLSS 4.5 which only has performance impact on 2000-3000 series.  It's absolutely wild, but I guess people really got too emotionally attached to their AMD cards, so whenever someone attacks AMD for being straight up dogshit they take it personally LMAO. Also who gives a fk what reddit thinks? Steam literally has NVIdia at 73%.",hardware,2026-02-20 19:08:57,6
AMD,o6g3a2o,Did you even check other markets?  Here in the UK it's been at MSRP or under for going on ~6 months now. Here's a snapshot i just took (and yes they're all in stock):   https://i.ibb.co/wh3HZwHS/image.png  Hardware Unboxed did a pricing analysis too: https://youtu.be/eUOEk2C7M-M?si=e77AB2M_OLrjxsWd&t=616  The Nov > Now price increase of 10% was expected (they did a press release saying this would happen due to the memory shortage),hardware,2026-02-20 16:29:21,6
AMD,o6e8ejx,I think itâ€™s intentional,hardware,2026-02-20 09:36:20,13
AMD,o6htzaj,debound?,hardware,2026-02-20 21:24:08,5
AMD,o6eh6wr,"I've not seen properly collated data about this, though I'd guess for example comparing sales volumes of standalone motherboards vs. GPUs at a single retailer could serve as decent proxy to frequency of upgrades of either.  Overall though, it's a weird market segment to estimate/predict. Vast majority of people in the market for ""a computer"" buy laptops. The remainder that decides to get a desktop, still more often than not goes for various kinds/levels of prebuilts. Only tiny sliver of the market actually buys parts to build a PC - and it's the only sliver where I'd expect significant chunk of customers to consider upgrading individual components.  Purely anecdotal, but even among the people I know, who built their own PCs, I pretty often see no upgrades at all. Just riding out initial setup for as long as it's viable before replacing the whole thing.  That said though, even more limited group of people I know and consider gaming/PC enthusiasts, *do* upgrade their GPUs additional one or two times between platform upgrades. I just think it's a tiny niche within already small niche.",hardware,2026-02-20 10:56:15,15
AMD,o6h0akx,"most people dont know you can lol, like its just an appliance like a TV or fridge and buys prebuilds.  and of the people who do, they likely buy only when something is slow to be an issue (which means FSR and DLSS means you can stretch things far beyond normal), so if the price isn't right why bother  and with things being inflated like this hey, wait a bit more right until the wheels falls off or just buy used.",hardware,2026-02-20 18:59:43,3
AMD,o6h15lq,"Lol exactly... Like RAM is probably the only peace of component that you never upgrade once you buy RAM in your system (ofc. unless it dies), but GPUs? I read tons of people upgrading every gen.  And e.g. I builded my PC in half 2023, yet I only in last Dec I bought a real GPU for this system before that I was still using GTX 1080 that I bought in 2016.",hardware,2026-02-20 19:03:40,2
AMD,o6e4kif,most GPUs are bought in prebuilts,hardware,2026-02-20 08:59:41,4
AMD,o6etn8r,"Yes, the average user is on somethibg like a 6 core 2.3ghz cpu with a 8gb 4060 at 1080p.  There are still plenty of buyers who will upgrade gpu only. Especially amd users as a 9070xt does work well on am4 with a decent am4 cpu.",hardware,2026-02-20 12:31:51,1
AMD,o6ejqx7,Your old CPU is going to hold it back.,hardware,2026-02-20 11:17:58,0
AMD,o6fimnr,When I bought a samsung monitor I was pretty convinced from reading reddit reviews that it had a 100% chance of arriving covered in dead pixels.,hardware,2026-02-20 14:51:55,16
AMD,o6g2vkc,"> Better buy the more expensive card of the other mega corp that would sell your kidneys if they could  All corps would, AMD included",hardware,2026-02-20 16:27:34,5
AMD,o6e7qvm,Fsr4 at pixel peeping is only marginally worse. Thats why Im aÅŸways behind my green multitrillion giant that is powering Palantir and assholes like Peter Thiel and links to Epstein.,hardware,2026-02-20 09:30:00,-15
AMD,o6jr2y0,"AMD is a reasonable buy when you get a tier higher raster performance at your max spend. Nvidia formal and informal marketing has been very successful in convincing people otherwise.  I have a 5070 Ti because I bought it when it was MSRP. If I was buying today my choices would be 9070 xt vs 5070, I would choose the 9070 xt.",hardware,2026-02-21 04:11:53,3
AMD,o6hianm,"man, just say they are the Aston Martin F1 team but for gpus and drop the mic  the 2 first letters even match  :P",hardware,2026-02-20 20:26:15,0
AMD,o6eif7q,"Of all the friends I've made directly (as in locally), I'm the only one who's built their own PC. The rest bought a pre-built.  Of my online friends, I'd say less than 10% built their PC, the rest got a freebie (either prebuilt or custom built) or got a pre-built.  I'm always confused why the DIY crowd some how thinks they have sway/leverage outside of recommendations.  And if my circles are anything like others - I can drop detailed explanations on my recommendations based on their budget needs and all I get for a response is ""what do you think of this one from <insert OEM>?""  r/hardware might think what is happening is a ""travesty"" - as someone growing older and having less interest in mucking around, and looking at other business models like streaming and even digital game shops - the market/majority of consumers are A-OK with the current trends. They've been conditioned into buying over priced tech, hell just look at how the ""gaming"" crowd handled crypto/COVID gouging - they lined up and bent over.",hardware,2026-02-20 11:06:46,6
AMD,o6rt2k5,"â€œNormal person upgradesâ€ usually arenâ€™t a gpu, itâ€™s usually peripherals that are plugged in without opening anything up.   Out of all my friends, usb hard drives are probably the most common and first upgrade. They run out of space and a usb hard drive is plug and play without opening a system. Spinning drives, not usb ssds, because they want a terabyte or two or four for really cheap. It supplements the usually small ssd that people get with their computers.  Covid wfh normalized ultrawides and dual monitors for a lot of people, so monitors became a common upgrade.   Gpus are bought by people who game regularly on their pc which is a subset of pc owners. Consoles sell better than gpus because again, no need to open a pc up to get this gen gaming. Opening PCs and installing parts is a big hurdle for people to overcome. And navigating GPUs skus has always been difficult, compared to going to Best Buy and choosing switch, Xbox, or PlayStation.",hardware,2026-02-22 14:01:15,1
AMD,o6exqt1,"Yeah, Monitors have gotten cheaper. So it makes sense that even those with <5yr old systems would want to upgrade from 1080p gaming.",hardware,2026-02-20 12:58:23,4
AMD,o6ghwnq,Read my comment to its end.,hardware,2026-02-20 17:36:39,2
AMD,o6evzel,You missed the joke,hardware,2026-02-20 12:47:21,0
AMD,o6kitff,"But you can't compare almost MSRP type of GPU to overblowned prices. That's just not fair. You bought yours when it was for MSRP, so did I.",hardware,2026-02-21 08:06:56,1
AMD,o6maqj4,"When looking to buy a GPU you pretty much have to consider the current price. I am pointing out that the idea that Nvidia is the only choice has people buying a whole tier below what they could get versus AMD. The feature differences just do not make that a justified position in most circumstances, especially when there is additionally a large difference in VRAM.  When the prices are closer then I absolutely see additional features being a significant consideration.",hardware,2026-02-21 16:06:51,3
AMD,o6md2ib,I fully agree with your take. I mostly meant it like in my personal experience in December NVidia's prices were at MSRP level. Right now if I would have to upgrade I would do exact same decision like you did. Like there is no fking way I would buy 5070.,hardware,2026-02-21 16:18:30,1
AMD,o42yfzn,"As a 6950 XT owner this is VERY annoying. I'd rather not have to mess with my drivers just to get FSR4 working modded in... but AMD just wants to leave anything pre RX 9000 behind, even their brand new RDNA 3.5 iGPUs it seems. I've done the driver mod on my 6950 XT before in Cyberpunk and while FSR4 is a little more heavy than XeSS, it also looks a decent lot better. Then XeSS is much heavier than FSR3.1 anyway so like, it just makes sense to go with FSR4. AMD needs to release it, no question.",hardware,2026-02-07 14:09:59,95
AMD,o42thn0,Amd dug their own grave with FSR  First it was you dont need upscaling. Then they pretend a sharpening filter was good with fsr1. How open source and working on multiple vendors is good. Then they said you don't need AI to do good upscaling with fsr2. Then they said you don't need dedicated hardware for FSR3 (frame generation).  Every single time they kept digging a bigger hole. All this time people got locked into the Nvidia ecosystem with Dlss.  And now AMD have a hardware locked ai based upscaling and frame generation.,hardware,2026-02-07 13:40:50,230
AMD,o42sd8n,"Put pressure on them, it's the only way to get them to change their shitty decisions.",hardware,2026-02-07 13:34:01,56
AMD,o42vtel,"The real travesty is AMD gaslightingÂ  buyers into thinking RT and AI upscaling were fads, only to finally imitate Turing 5 years too late, and to then blame the fans for listening to them.",hardware,2026-02-07 13:54:37,133
AMD,o42lni5,AMD really screwed over anyone pre 9xxx series. Crap upscaling really makes games look so much worse.,hardware,2026-02-07 12:49:46,90
AMD,o4373qj,Why bother with marketing and image when you are already at capacity with your sales and you probably plan to reduce supply in the near future?,hardware,2026-02-07 14:58:12,10
AMD,o4830in,"Glad they're keeping the pressure on her another dumb AMD decision, but they missed a key point which makes AMD's stance even worse - they are still releasing rDNA 3.5 graphics cards in their APUs. Those same products which are in most need of FSR for gaming.",hardware,2026-02-08 08:53:21,6
AMD,o435ru2,Aw man turns out AMD is a corporation after all.,hardware,2026-02-07 14:51:00,22
AMD,o44xbvi,"This is the same company that said that it was ""impossible"" to put a 5000 series ryzen on a B350 motherboard. Lo and behold.",hardware,2026-02-07 20:09:44,11
AMD,o42m57s,Yeah we really know at this point it's getting tiresome and ppl can already use it with mods anyway. I mean it's not even like RDNA4 is that much better off most games need Optiscaler just for basic FSR4 support so what does it even matter? AMD always relied on the open source community and they already said FSR4 will most likely go open source soon as well.  Seriously company's are not your friends and both Nvidia and AMD are just looking in to making a quick bug with AI atm so dunno what ppl are even expecting.,hardware,2026-02-07 12:53:19,21
AMD,o43llnk,Moved from 7900XTX to a 5090 because of this and many other reasons like games not supporting AMD feature.  But what pissed me the most is how bad the frame gen from AMD is to my eyes,hardware,2026-02-07 16:11:06,5
AMD,o48kj9r,I wouldn't be even fine with an experimental version. Needs to be full,hardware,2026-02-08 11:38:05,2
AMD,o42muev,"The difference in tone and style between when Nvidia does something and AMD is hilarious. Cant take them seriously anymore since the RTX 5000 and AMD 9000 series launch and their obvious bias and borderline misleading coverage when it come to the ""fake msrp"" saga",hardware,2026-02-07 12:58:15,16
AMD,o439mr3,"AMD make quite good GPUs, but they have spent years in denial that the software is a vital part of the consumer GPU product, and an increasingly large part at that.  The Radeon division treat this side of the business as a chore their mom is making them do on a sunny Saturday morning when they'd really really rather be doing anything else - to be done late, slowly, and in as slipshod a manner as they think they can get away with.",hardware,2026-02-07 15:11:38,3
AMD,o44mymn,Only in this sub could you get people accusing the people of creating a negative AMD video of being biased for AMD.,hardware,2026-02-07 19:15:32,6
AMD,o45nxnr,"It's pretty bad that my first thought was ""which one?""",hardware,2026-02-07 22:33:56,2
AMD,o437mew,"As a 6700 XT user, I honestly do not care yet. Do not get me wrong, I am glad big YouTubers are turning up the heat and keeping AMD honest. But my OptiScalar test 2 to 3 months ago was disappointing. FSR 3 Quality was performing close to, or even better than, FSR 4 int8 Performance. quality difference was not worth it  At this point, I would only care about FSR 4 on RDNA2 if AMD can squeeze out an extra 10 to 20 percent. I am not even sure that is a realistic expectation anymore.",hardware,2026-02-07 15:00:59,-4
AMD,o43bjul,The only thing holding AMD up is good processors (only x3d) and cheap graphics cards.,hardware,2026-02-07 15:21:32,-2
AMD,o42lapc,"Here they are with the video to assure their fans they don't give unfair reviews by subtly hinting ""AMD bad""  Edit: I fucking love getting downvoted for this, I am awaiting the comments pointing out my obvious Intel PFP and then proceeding to call me a troll that shouldn't be given attention all while still posting the comment!",hardware,2026-02-07 12:47:08,-39
AMD,o44kgx3,AMD can't release it probably because it can't run on RDNA 3.5 APUs,hardware,2026-02-07 19:02:56,-4
AMD,o43ctih,Same with features for rtx2080ti they donâ€™t release. Rebar for example,hardware,2026-02-07 15:27:57,-2
AMD,o43nzd6,>As a 6950 XT owner this is VERY annoying. I'd rather not have to mess with my drivers just to get FSR4 working modded in... but AMD just wants to leave anything pre RX 9000 behind  And who now has any faith that RDNA4 GPUs won't get left behind when UDNA comes out...?,hardware,2026-02-07 16:22:49,81
AMD,o45ivbt,"Don't forget that the transformer model (4 and 4.5) was even released on the 20 series.  Which supposed to lack that kind of HW performance to do them right, they just said hey, this isn't the best on them and you may need to tweak settings to get the best things out of it, and that if you wished 3.x can still be used just toggle it. And that 4.5 will have more perf issues.  I cannot imagine why AMD won't do the same, like seriously what is going on there, esp if they gave them the extra vram to make them last longer supposedly and now drop them on features that could actually help them last longer.",hardware,2026-02-07 22:05:56,11
AMD,o45oqg9,"RDNA4 1, 2, and 3 owners are just going to have to accept that you bought in at the time that AMD had its head in the sand about hardware for RT and ML upscaling. The cards can't do it because they don't have the hardware, and the cards don't have the hardware because AMD didn't take it seriously on the consumer side. The only options for RDNA2 would be to take a big performance hit to the point where it might not even be worth turning it on, or accept an image quality hit to the point where it might not be worth modifying the model when FSR3.1 exists.",hardware,2026-02-07 22:38:24,-6
AMD,o4303lm,Meanwhile Intel has been quietly doing a good job with XeSS,hardware,2026-02-07 14:19:25,133
AMD,o43qut6,They also promised fsr 3.1 development would continue in parallel with fsr4.,hardware,2026-02-07 16:36:54,19
AMD,o43ddnt,"> Every single time they kept digging a bigger hole. All this time people got locked into the Nvidia ecosystem with D  and you know what the real kicker is ?      If AMD had just shup up early and accepted to be part of the ""Streamline"" project that had Nvidia leadership and Intel participation, every single DLSS2+ game today would also support FSR4.",hardware,2026-02-07 15:30:45,35
AMD,o43cw8z,It's Ok When AMD Does It^TM,hardware,2026-02-07 15:28:20,23
AMD,o43jsaq,"consumers are partly to blame imo, all that hate for dlss came back to bite everyone in the ass. literally everyone in the past few years have been saying they don't need/want upscalers/frame gen until amd makes a subpar equivalent and then everyone praises it like it's the second coming of christ when it's not even close most of the time, it's like they don't want to admit that these features are actually really helpful just because it's made by nvidia.",hardware,2026-02-07 16:02:09,23
AMD,o49sy4k,"Yep. When your *only* selling point it was available to everyone is gone, their isn't any point for devs to implement as lowest denominator. That will be Intel now.",hardware,2026-02-08 16:14:11,3
AMD,o45f6ad,"Amd was wrong from the beginning, Nvidia was right.",hardware,2026-02-07 21:46:03,6
AMD,o43n4ay,IS what why Nvidia users of series prior to 4XXX love to use AMD frame scaling? Because it sucks? LOL,hardware,2026-02-07 16:18:35,-13
AMD,o44gur2,"What were they supposed to say? ""To get good results you need dedicated hardware, which we won't have for at least 4 years, and next/current gen consoles don't have""?  They played the hand they had. 100% normal.",hardware,2026-02-07 18:45:01,-7
AMD,o4356my,"This, people yelling at GN for putting pressure on nvidia please see the upper comment too",hardware,2026-02-07 14:47:47,13
AMD,o434n5s,"My annoyance is that their technology gets used in consoles, thus dictating how games and engines are developed. And because they're so widespread, console users will automatically come into the conclusion that Upscale, RT, and even Framegen are bad because FSR Upscale, Framegen  and AMD's ""at home"" RT implementation is terrible.",hardware,2026-02-07 14:44:46,57
AMD,o43ms1f,"Reminds me a bit of WAAAY back. When shaders first appeared and how 3dfx went under. Yes, novel tech is first slower or inferior than a mature legacy system.",hardware,2026-02-07 16:16:54,18
AMD,o434udg,Its a rope pull game of nvidia trying to justify early adoption selling of RT in Bf5 demo days which fell flat and needed more time in the oven and AMD justifiedly being late to the party on features people demand and mocked they dont care about RT. And a huge gray zone where people draw their own lines.  Because anyone with sense knew path traced rendering will be the future. But if its 5 years or 15 nobody knows for sure thanks to moores law,hardware,2026-02-07 14:45:55,13
AMD,o42xykd,They always lead from behind in everything they do.  AMD has a rich history in copying IP.  From their founding to modern implementation.. everything is unoriginal and 2nd rate.  They are the generic store brand cola of tech.,hardware,2026-02-07 14:07:13,6
AMD,o45fuoo,"AMD always had so bad marketing policy. AMD always says nonsense and then quietly backtracks on it. That's why I haven't trusted AMD for a long time and don't take seriously what they say, what they announce and what they claim.",hardware,2026-02-07 21:49:41,1
AMD,o436vtm,they are the wrong direction (RT and ML fuzz) imo but amd software was so fucking bad before this and now its even segmented so you have even more reasons to just go nvidia,hardware,2026-02-07 14:57:01,-6
AMD,o437796,"It was a fad. It was something that should have been sold to developers, not to consumers. There's a video out there, that people find non-raytraced water nicer to look at because that's what people got used to. Not only they were selling something that didn't look good to the consumer, they did so at a significant price step up and degrading performance. That is the sin of Nvidia. If Nvidia instead gave it for free and only started charging money once it was good, nobody would have complained.  AMD on the other hand, was caught with their pants down because they don't speak to developers as often as they should. They should be begging developers to tell them what tools they think could be good and copying Nvidia's homework on their spare time.",hardware,2026-02-07 14:58:44,-15
AMD,o47y7zx,"They are imitating the RTX 4000 series. RDNA 2 was AMD's Turing in terms of ML tasks. RDNA 3 then improved the AI hardware by adding First gen matrix cores, which currently remain unused even by the leaked Int8 FSR 4 shaders.  The 7800XT has ~3070 Ti level ML performance but with no software to use it.",hardware,2026-02-08 08:08:03,-3
AMD,o42woxy,"Well... it's a plus that Linux has workarounds that get FSR4 running on 7000 series to some degree.  Which shows that AMD's GPU marketing team is just fucking up, as usual. Why do they have to shoot themselves in the face so much? Doesn't it hurt?",hardware,2026-02-07 13:59:49,27
AMD,o44yql8,> Crap upscaling  This is very funny to me because HUB and AMD fans said FSR1-3 was so good.,hardware,2026-02-07 20:17:14,11
AMD,o42swsh,"Native is still there, they're not really screwed",hardware,2026-02-07 13:37:21,-21
AMD,o42xhph,"Most games I played lately (RE4 remake, Metaphor) run at 4k 60fps with no issues on my 6800XT.     I don't feel screwed over by my 4 years old GPU.",hardware,2026-02-07 14:04:32,-17
AMD,o4ma19w,"iGPU without proper upscaling is a crime fs.       But this vid isn't going to change a thing. I suspect it won't happen until Gamescon or later. PS5 Pro gets PSSR2, then Steam Machine FSR4 INT8 or Full FP8 or something else (custom model based off FSR4). By then people will be so pissed that AMD is forced to release it.  AMD needs a public flogging before they do anything. Pressure on them is nowhere near high enough. GN, LTT and many others need to call them out before anything happens.",hardware,2026-02-10 14:21:49,2
AMD,o42r5wo,"Well Tim (in the video) did a comparison between FPS scaling vs image quality and IIRC he prefered FSR4 INT8 Performance vs FSR 3.1 Quality, and the INT8 version performed better at like for like image quality.   Technically if the Game has FSR3, AMD Adrenaline can override similar to GeForce App .dll swapping. So optiscaler isn't as required if the game is relatively recent and already has FSR3.   I'd rather we keep talking about it, but if Nvidia is going to divert GPUs/Memory to AI. AMD can keep focusing on improving FSR Redstone and their 16GB RDNA4 will keep selling above MSRP.",hardware,2026-02-07 13:26:31,19
AMD,o42nc9s,People love to not realize the big 3 don't give a flying fuck about them and AMD is still a shit company with shit morals that just presents them better  Edit: being downvoted for saying that a big tech company has shit morals and doesn't give a fuck about you is peak Reddit,hardware,2026-02-07 13:01:36,69
AMD,o42szrh,Can't tell if you think they're biased towards AMD or Nvidia. They've complained about both heavily over the years.,hardware,2026-02-07 13:37:51,9
AMD,o42sswc,Wtf are you talking about,hardware,2026-02-07 13:36:40,9
AMD,o42v3rx,Can you give me examples such as direct quotes? Cos I donâ€™t see it.,hardware,2026-02-07 13:50:26,0
AMD,o42qean,"years ago Nvidia is angry about them for a negative review, and threaten then not having early review cards for testing. After that they change and become biased.  That's why I'm not watching them anymore, and go to other Youtubers like Gamer's Nexus or anybody else to confirm the news  Edit: here are some sources [https://www.youtube.com/watch?v=wdAMcQgR92k](https://www.youtube.com/watch?v=wdAMcQgR92k) this was 6 years ago  [https://www.reddit.com/r/hardware/comments/1kqrsw3/hardware\_unboxed\_nvidia\_accused\_of\_manipulating/](https://www.reddit.com/r/hardware/comments/1kqrsw3/hardware_unboxed_nvidia_accused_of_manipulating/) this was 9 months ago",hardware,2026-02-07 13:21:38,-6
AMD,o443hqd,"Nvidia 5000 series had melting power connectors and unstable drivers on release. 9000 series had the usual AMD issues, and not much more. Those are simply not the same, so obviously the reaction to them won't be the same either.  Also: AMD released after Nvidia and had the weaker hardware, so they adjusted their pricing accordingly, while Nvidia can just not give a shit about availability and pricing at all. And AMD still got a shitload of flame for pricing better than Nvidia, but not good enough.  And Nvidia has been actively trying to restrict critical reviewers - considering that, I'd actually argue they are actually holding back their animosity.",hardware,2026-02-07 17:39:18,-6
AMD,o43agpu,Bias? Towards who? When both rdna4 and blackwell launched they were very critical of both when it came to prices being way above msrp,hardware,2026-02-07 15:15:59,-7
AMD,o42s23q,Comments which add little  to no value to a discussion are supposed to be downvoted. It's what the button is there for. Maybe if you'd addressed the content of the video your comment would have been valued higher.,hardware,2026-02-07 13:32:05,21
AMD,o42t67x,Downvoted for just being a stupid comment.,hardware,2026-02-07 13:38:56,14
AMD,o42w1ca,"> Here they are with the video to assure their fans they don't give unfair reviews by subtly hinting ""AMD bad""  HU have never taken sides, much like GN ~ AMD has just been simply less awful and anti-consumer than Nvidia and Intel, for whatever reasons, whether lack of monopoly or market power or otherwise, but HU still happily gives AMD plenty of shit whenever they fuck up. Especially when it comes to AMD's absolutely abysmal GPU marketing.",hardware,2026-02-07 13:55:56,8
AMD,o43zmdi,"YEAH, like I wanna upgrade next gen and especially here in Australia Radeon prices tend to be more competitive vs global MSRPs... but how much trust can I have in AMD's support of their GPUs? RDNA2 being the driver of the PS5 and Xbox and Steam Deck hasn't stopped AMD for trying to bury the RDNA2 Radeons to push upgrades or whatever their dumb plan is when it should be to gain marketshare, not annoy what they have.  Like we understand these GPUs aren't as advanced as the competition they had, but FSR4 INT8 is plenty of proof where they stand right now is not the end of the road, as long stuff works on the INT8 path it works.  Like FSR4 INT8 could literally be used on console too you know, not base PS5 which lacks it but Xbox? Yes, Series X and S on paper could use the INT8 model instead of being stuck with TAA and FSR3 and nothing better.",hardware,2026-02-07 17:20:03,29
AMD,o45htgt,They always try to pull this bs. Same with Ryzen 3000/5000 on initial motherboards.,hardware,2026-02-07 22:00:15,10
AMD,o448gc8,"But ""Nvidia bad"" according to reddit.  Meanwhile, 3080 owners enjoying dlss 4.5...",hardware,2026-02-07 18:03:46,28
AMD,o4684j0,"Well UDNA5 will have the FSD Redstone issues fixed, but not ported to RDNA4",hardware,2026-02-08 00:35:38,-1
AMD,o4l3y0u,"20 series is right now twice as old as the oldest AMD GPU that still has driver support, let alone features.",hardware,2026-02-10 09:09:31,3
AMD,o46nrzr,"Love that you still say this while I literally said I used FSR4 in Cyberpunk mate. It works, it's heavier sure but it's also not as bad as DLSS 4.5 on RTX 20/30 series and Nvidia still gave those GPUs the option.   Like 4ms or so isn't a killer of viability, for 60fps or lower it's actually still realistic to get going and beats trashing image quality with FSR3.  Like FSR3.1 vs 4 INT8 is night and day, and what I want out of FSR4 is pretty simple... keep my 4K monitor viable with my 6950XT without either integer scaling 1080p or having to resort to FSR3 which is not great, neither is native 1440p on a 4K panel a good idea.",hardware,2026-02-08 02:13:01,10
AMD,o431pyx,Nvidia always said AI and Hardware Acceleration was needed for the best quality.  Intel said the best quality was hardware acceleration but a weaker version will be available for others.  Intel and Nvidia both stuck to their words and never did virtue signaling for public points like AMD.,hardware,2026-02-07 14:28:37,113
AMD,o439wj3,And now their APUs are legitmate threats to AMD's again...,hardware,2026-02-07 15:13:04,60
AMD,o438e6r,"And a downgraded version is available for other gpus, amd cant even provide it for their own older gpus, which are still produced and sold mind you, and new products based on it are made",hardware,2026-02-07 15:05:07,18
AMD,o433wjx,"Although Intel did initially say XeSS would be open source, which it still isn't.",hardware,2026-02-07 14:40:43,17
AMD,o43bbv9,"It is not open source,  like they said it would be.",hardware,2026-02-07 15:20:25,9
AMD,o45pnhh,"Intel's frame generation is the best of all three, not going to lie. At least it is on Intel hardware, I can't speak for the fallback models. It has frame pacing that's better than AMD and latency handling that's better than Nvidia. Upscaling has fallen behind a bit, but it's still around the level of DLSS 3 and is better than FSR3.",hardware,2026-02-07 22:43:36,2
AMD,o434it5,They are ok on side by side basises to fsr4. But breaking preset parity buy downgrading sample resolution was a big no no,hardware,2026-02-07 14:44:06,2
AMD,o44kpja,"Don't worry, Intel will kill further development of XeSS once they have products with Nvidia GPU tiles on their products a few years from now.",hardware,2026-02-07 19:04:09,1
AMD,o4l4cw3,well there was the rumored FSR 3.2,hardware,2026-02-10 09:13:35,2
AMD,o456x86,"Nah, letâ€™s pay for FSR exclusivity instead",hardware,2026-02-07 21:01:50,8
AMD,o46ve7e,AMD has been given a pass for so much shit it's unbelievable.,hardware,2026-02-08 03:01:11,15
AMD,o44ms12,"yeah, im sure it was ""everyone"" instead of 20 people online",hardware,2026-02-07 19:14:35,12
AMD,o4f2gje,Reddit is not everyone,hardware,2026-02-09 11:45:53,2
AMD,o4l4g2z,">  literally everyone in the past few years have been saying they don't need/want upscalers  not everyone, only a loud minority of trolls.",hardware,2026-02-10 09:14:26,2
AMD,o445fon,"I wonder if it's that these two groups might just be separate?  Oh look, another limitation / issue with AI gaming I can completely disregard. Bliss.",hardware,2026-02-07 17:48:56,3
AMD,o48y5pk,"the hate was well deserved for DLSS 1 on 20 series, it was a shitshow of implementation that was also locked out of the 10 and 9 series with VERY limited game support (battlefield was the only big one, then more came in).  esp coupled with the fact that the 20 series was the first time in a long time when the prices was raised and you gotten down tiered at the same price (not the raw perf ofc, you still got more than before).  that first impression really hurt things, and it wasn't until DLSS 2, and esp with CP2077 where things start to turn around, and beyond that is when new games start to more or less always included some sort of upscaling.  but at that point, 30 series was out.",hardware,2026-02-08 13:23:36,-2
AMD,o49z6ip,"I mean, they still somewhat fix this by overriding FSR 3 with 4, no?",hardware,2026-02-08 16:44:32,1
AMD,o43yedb,it's better than nothing i guess,hardware,2026-02-07 17:13:58,12
AMD,o45727q,Better than nothing doesnâ€™t mean it doesnâ€™t suck.,hardware,2026-02-07 21:02:35,1
AMD,o44v1ba,"AMD 100% knew Nvidia was working on dedicated ML hardware even before the 2000 series was announced. Employees go back and forth between the big 3 all the time, obviously bringing trade secrets with them. They just didn't realize how big of a deal it was until it was *way* too late.  Sure, the RX5000 series was already near-finished by this point so it was a lost cause, but there was all the opportunity in the world to make the clean break with RX6000, implement the ML hardware and then they'd only be one upscaling generation behind instead of 3.",hardware,2026-02-07 19:57:27,7
AMD,o4l4qg0,"Yes, that is exactly what they should have said. To say anything else is fraud.",hardware,2026-02-10 09:17:16,2
AMD,o436lgz,> people yelling at GN for putting pressure on nvidia please see the upper comment too  He's probably in the process of making similar video but first he needs to find a way to shift the blame towards Nvidia,hardware,2026-02-07 14:55:29,41
AMD,o4380t3,"Which pressure? the 4080 12gb? Prices have only been increasing, nvidia has only been getting richer and diversifying immensely. YouTube video isn't putting pressure on Nvidia when Nvidia is driven to maximize profits & current supply issues wont allow them to make as many gpus as they wanted.  The only pressure Nvidia will understand is the competition is even good alternative. For every 1 recommendation GN makes for an alternative 20  nvidia gpus are bought.",hardware,2026-02-07 15:03:08,6
AMD,o46ncfh,"To be fair, AMD's strategy is because they're used in consoles. Every technical decision they've made has been dictated by the cost constraints of developing console hardware. What you get in the desktop was paid for by the console manufacturers and it is as powerful and feature-packed as the console manufacturers were willing to pay.   What I mean is, AMD's strategy with RT was meant to be cheap because it's the way Sony and MS got to have the tech while also not having spiraling hardware costs. Rdna2 is basically the tech those consoles got scaled up and rDNA 3 was rdna 2+.   Rdna4 was basically a rehash of what Sony paid for the ps5 pro. And rdna5 is likely going to be a beef up version of whatever the console manufacturers pay for the 2027 season.Â    AMD doesn't have anywhere near the r&d money nvidia has and it doesn't have the deep pockets Intel has to throw money to a hard problem until they crack it.   I'm going to be the first to complain about their lack of software strategy, but their hardware strategy makes sense when you consider they get a tenth of the market share nvidia gets and for the longest time had revenue much smaller than Intel's and Nvidia's. They make the best use of the tech MS and Sony pay for their consoles.Â    Maybe this time they will use that sweet sweet AI money to flesh out their hardware, but I'm not holding my breath. It's clear they have a hard time convincing buyers to give them a try.",hardware,2026-02-08 02:10:17,5
AMD,o44we8h,">And because they're so widespread, console users will automatically come into the conclusion that texture filtering, z buffering, and even perspective correction are bad because Sony's ""at home"" implementation is terrible.    Time is a flat circle",hardware,2026-02-07 20:04:45,9
AMD,o45g4bn,Yes. The great tragedy is that both major consoles use AMD hardware.,hardware,2026-02-07 21:51:04,5
AMD,o4l54p8,Well EPIC seems to have said fuck it dont care about console capabilities and are enforcing hardware raytracing with UE 5.4,hardware,2026-02-10 09:21:08,1
AMD,o44k899,Framegen is bad. At least the way smooth brains use it to pretend its the same as real frames.  One good thing about consoles being amd I guess,hardware,2026-02-07 19:01:43,-18
AMD,o44lb5t,"As a 5090 owner,  those are bad in nearly every game.",hardware,2026-02-07 19:07:11,-11
AMD,o431xuj,not totally accurate they gave us zen architecture and great consumer CPUs,hardware,2026-02-07 14:29:49,22
AMD,o43bcap,The x64 arch and the first consumer dual core CPU came from AMD... On the graphic side they where leading in the Geforce FX era...,hardware,2026-02-07 15:20:29,14
AMD,o432wwr,"Well their Zen architecture was something actually novel and new, especially on the consumer market. They forced Intel out of their 4 core, 4% incremental upgrade cycle. People were hoping they'd use that momentum to innovate on the GPU side but they haven't really been able to do that.",hardware,2026-02-07 14:35:14,8
AMD,o45jkg6,ATI/AMD was in the lead pushing for new memory tech on GPUs for like a decade. Nvidia ran the strategy of trailing behind both on memory and nodes up until Kepler when they reached parity by launching just months after AMD.  Theres whole memory standards that Nvidia never touched. GDDR4 and HBM1 were never implemented by Nvidia on any product iirc.,hardware,2026-02-07 22:09:45,2
AMD,o43ipnz,"I'd like to see that water comparison video, especially in regards to the way those scenes are presented and interacted with. Also if they compared RT/non-RT within the same scenarios.  There can be many discussions about artistic choices, but RT reflections allow to retain the same art style without the drawbacks of SSR, which was the popular AAA dev choice before RT reflections came along.",hardware,2026-02-07 15:56:54,17
AMD,o43al1s,Amd IS asking devs what tools they think could be good. Why do people think amd is not some 300B corporation? Nvidia is simply able to execute the ecosystem better which led to amd running to catch up.,hardware,2026-02-07 15:16:36,6
AMD,o4l5u6e,There was no need to sell to developers. Metro developers even showed a cool demo where they show designing lighting the old way and via ray tracing. Ray tracing means they would save months of developement time. RT sells itself to developers real fast. Its the consumers that need to buy it.,hardware,2026-02-10 09:28:07,2
AMD,o48tftc,"There are no matrix cores in RDNA 3, just added support for ML instructions.Â  Thus the core runs into contention when having to process graphics at the same time.Â    Â Everything up to the 9070xt has been brute forced. RDNA4 is the first feature complete AI radeon.Â  Making it Radeons Turing.",hardware,2026-02-08 12:51:04,4
AMD,o48ph1i,Why they donâ€™t let rdna2 use FSR4 then?,hardware,2026-02-08 12:20:23,1
AMD,o4l6605,"There are no AI cores in anny of AMD GPUs. No, not even in RDNA4.",hardware,2026-02-10 09:31:21,1
AMD,o42zczb,"> Which shows that AMD is just fucking up, as usual. Why do they have to shoot themselves in the face so much?   AMD's graphics division has historically never missed an opportunity to miss an opportunity.    Not only that, but even AMD's 2006 acquisition of ATI was financially fucked up to the degree in which they had to do [multiple write-downs](https://www.eweek.com/pc-hardware/amd-will-take-900m-q2-charge/) totaling over two billion dollars (out of a $5.4B initial purchase price) because they grossly overestimated how valuable the company and their technology was to begin with.",hardware,2026-02-07 14:15:13,50
AMD,o46rd1q,Anyone with functioning eyes has been calling FSR 1â€“3 unusable since day one. Itâ€™s essentially just aggressive resolution reduction in motion conveniently toggleable with a side of heavy artifacts.,hardware,2026-02-08 02:35:35,6
AMD,o4et2rd,"it IS usable, but when I still had an AMD card I much, MUCH preffered the image quality of XESS (in most, not all games) and since I have seen DLSS in action, there is no comparing these 2 anymore.   Yes FSR is usable, but it's also the worst upscaler currently on the market.",hardware,2026-02-09 10:20:52,3
AMD,o4l6jsm,the less you listen too techfluencer fans the higher braincell count remains alive.,hardware,2026-02-10 09:35:06,3
AMD,o45vowv,They are good. But transformer DLSS is significantly better.,hardware,2026-02-07 23:18:37,-5
AMD,o42tli6,Native TAA looks like crap. Native FSR or XESS is okay but still worse than DLSS quality.,hardware,2026-02-07 13:41:29,34
AMD,o42tw48,Upscaling is being widely adopted as part of the optimization package and default anti-aliasing method.  So... Yeah you're screwed going forward more and more if you don't have access to good upscaling.,hardware,2026-02-07 13:43:14,33
AMD,o42zrxo,"The native argument used to work before DLSS transformer, but now after DLSS 4.5 it's really hard to argue against how fucking good it looks, it's basically just free performance with no downsides at this point, and any imperfections in the image are almost always present in native too.",hardware,2026-02-07 14:17:35,21
AMD,o4l6osg,noone plays at native anymore.,hardware,2026-02-10 09:36:30,3
AMD,o42ua54,"Turn on RT, see how well native runs",hardware,2026-02-07 13:45:34,9
AMD,o42ysqd,Try any of the ue5 titles.,hardware,2026-02-07 14:12:01,19
AMD,o42tf70,1440p quality DLSS is a slight downgrade for better FPS. Itâ€™s worth it every time.,hardware,2026-02-07 13:40:26,18
AMD,o437sd5,"Native is not a magic resolution where everything is correct. This is a statement people keep making that sounds correct if you don't think about it, but it's just flatly wrong.      Tons of geometry and textures have detail that can be smaller than a pixel in games today. The vast majority of effects and basically all the lighting (whether ray traced or not) is initially rendered at lower than native resolution.  TAA is already used in basically every game because of how common deferred rendering is. If TAA weren't used, the flickering and aliasing would be so intense it would make games unplayably bad.    Even if none of that were true, the idea of native being ""correct"" makes no sense. We would have had no need then for antialiasing techniques like SSAA or MSAA.    DLSS and all upscaling techniques today are a form of super sampling. It's just that they trade off spatial resolution and do the super sampling temporally. Using data from past frames is obviously worse than just rendering at a higher resolution in a vacuum. But you need cheap anti aliasing that covers everything anyway. And basically every game needs temporal accumulation (to run effects at sub-native resolution) in order to run fast enough with good enough visual quality anyway. And if you really can create an algorithm that effectively reuses data from past frames, then it can both look better and run faster than native.    Now could there be edge cases where something looks worse? Sure. All you need to do is look at FSR2/3 to see how worse algorithms make a ton more tradeoffs. But with good techniques, there will also be many situations where it can look better.    And if you truly care about quality above all else, then you can turn up graphical settings when introducing some upscaling. You'd have to compare something like ""Native"" with Low settings to DLSS with Medium or High settings (turning up the most impactful graphical settings until the framerate was the same).    Personally, at 4K output,  DLSS is always worth turning on. Your dumb PC gamer gatekeeping is hilarious because PC has by far the best upscaling and the most ways to tweak it.",hardware,2026-02-07 15:01:52,14
AMD,o42sn8c,The beauty of pc gaming is â€œchoice.â€ So..nah keep buying them gpus fellow consumers totally not ceo,hardware,2026-02-07 13:35:43,18
AMD,o42tbda,"omg, I need to trash my 4090 and get a Wii",hardware,2026-02-07 13:39:47,9
AMD,o42s1wi,What a pathetic attempt to gatekeep.,hardware,2026-02-07 13:32:04,19
AMD,o42t38e,"Agreed that it makes it look worse, strong disagree with everything else. Thereâ€™s valid reasons to play at below native res.",hardware,2026-02-07 13:38:25,8
AMD,o42smys,"Bad take, Iâ€™m willing to sacrifice raw visuals for higher frame rates on my 4K 240FPS monitor and Iâ€™m not about to run out and buy a 5090",hardware,2026-02-07 13:35:40,7
AMD,o42rzs1,Naaa,hardware,2026-02-07 13:31:42,0
AMD,o42tbxk,"> optiscaler isn't as required if the game is relatively recent and already has FSR3.  only for fsr3*.1*, which a lot of software misses.",hardware,2026-02-07 13:39:53,10
AMD,o42t5y3,Not to mention that with RDNA2 you also have to mod the driver...,hardware,2026-02-07 13:38:53,7
AMD,o42pf7d,Reddit is so pro AMD itâ€™s hilarious.,hardware,2026-02-07 13:15:23,50
AMD,o42p4sx,"AMD has done a great job pushing their narrative on reddit for years, making people treat these companies like sports teams (*I'm Team Red guys!!1!*). Now there are so many bootlickers defending their 'underdog' from all criticism, completely ignoring that AMD is just another soulless corporation who only cares about profits, exactly like Nvidia, Intel and all the others.",hardware,2026-02-07 13:13:29,35
AMD,o43gs9y,"In 2020, during the RTX 3000 series launch, Nvidia tried to control HU's videos because HU ""did not focus enough"" on DLSS. I believe it was either by refusing to send FE cards, or directly by telling them what to write? Not sure anymore, but after a public outcry and support from LTT, GN, and other outlets, Nvidia reversed their course.  Nonetheless: HU did not take this story well. They were previously indeed focusing on rasterized performance. After this Nvidia stunt, they continued to do that, and it took them until about ~2022/2023 when they finally started properly and broadly testing FSR/DLSS - after it became evident that a) there's enough games that support it and b) there is a demand for coverage from the pubic, because gamers are actually using it.  Even still, a lot of their FSR vs DLSS comparisons did acknowledge that DLSS was better, but the wording was always casting AMD in a better, or at least a good, light. Either FSR wasn't far behind DLSS, or the price differences on GPUs made AMD a better purchase, or whatever other reason they could find, but they had a very clear pro-AMD bias.  Only around the Radeon RX 9000 series release did HU balance their coverage. I believe this was because both companies, AMD and Nvidia, had done so much anti-consumer shit by that point, that the public hated them (almost) equally.  Before this launch, the Reddit majority opinion was ""AMD has shittier drivers, but GPUs are much better value, and we can handle worse FSR for the price difference, also they are much better for Linux and fuck Nvidia for many valid reasons"". After the launch, Nvidia invested into Linux drivers (but had Windows driver issues of their own), AMD did the same shit with pricing, and DLSS pulled away far enough from FSR that the difference became too difficult to ignore. Both companies now have done so much anti-consumer shit in one way or another that they are evenly disliked, imo.",hardware,2026-02-07 15:47:31,10
AMD,o42tka2,Yet it just so happens everything AMD does wrong can get fixed in their eyes by a PR statement and nothing more. Maintenence mode? Oh thats ok because they use vague language to say they might update drivers to support new games.,hardware,2026-02-07 13:41:17,5
AMD,o46kjcz,"If nvidia didn't let DLSS4 work on RTX 20-40 cards but accidentally let slip a working version of it, do you think HUB would describe it as a ""blunder"" or ""anti-consumer behaviour""? Would they be ""annoyed"" or ""outraged"" (and would they frame it as they're outraged or it's objectively outrageous)? Would the thumbnail say ""stop hiding it"" or ""nvidia F*CKS their customers""?",hardware,2026-02-08 01:52:44,2
AMD,o42rdk3,"You don't confirm from a single source, you always go to multiple sources.",hardware,2026-02-07 13:27:51,13
AMD,o42ylzd,"GN is arguably even worse these days, starting from routinely calling stuff like GeForce Now a scam, and ending with blatant misinformation like calling Ray Tracing a proprietary NVidia tech.",hardware,2026-02-07 14:10:56,13
AMD,o4l80cq,usual AMD issues are worse than the worst point in 5000 series drives though. AMD fans are just used to eating shit all day.,hardware,2026-02-10 09:49:25,2
AMD,o449bmy,"Nice whataboutism  â€žAlso: AMD released after Nvidia and had the weaker hardwareâ€œ so?  â€žAnd AMD still got a shitload of flame for pricing better than Nvidia, but not good enough.â€ž Â  Another point: this was only true if you only look at raster which HUBs loves to do because it makes amd look better",hardware,2026-02-07 18:08:01,1
AMD,o43bp2h,"they spend like every rtx 5000 video talking about how the MSRP was misleading and meaningless and will not be reached and they only set it so low to get the good reviews and how it was all fake simply because the cards were not at msrp at the day of the release.   2 months later they were  Meanwhile AMD actually had a fake msrp and it took them weeks to call them out in a single video.   Similar thing for DLSSa and RT, they did not consider those thing that in their value part of the video and only focussed on the Raster performance, why?",hardware,2026-02-07 15:22:15,10
AMD,o4l8a0e,the fact that you believe AMD hasnt been as anticonsumer shows you area already looking at it from an extremely biased perspective.,hardware,2026-02-10 09:52:00,1
AMD,o4mbdls,"Pushing proprietary GameWorks + locking headline feature to latest gen with RTX cards is nothing compared to all the BS AMD has done. Only good thing AMD had going for it was more VRAM but that was irrelevant long term due to other factors.  Look at NVIDIA's track record in terms of Day one game ready drivers, driver support, HW future proofing since 2018 (Pascal wasn't exactly forward looking), DLSS adoption, Frame-gen, dlss support across ALL generations (SR and RR) etc...  AMD pretends to be an underdog but has been scummy ever since Lisa Su took over. Let's not pretend it's still the good old days of Terascale and early GCN.  HUB is biased AF. Look at their historical dismissal of RT, DLSS and in general NVIDIA bad AMD good coverage. u/Strazdas1 is right.",hardware,2026-02-10 14:28:55,1
AMD,o44iaj9,"3080 is going to age well, especially the 12gb version",hardware,2026-02-07 18:52:06,21
AMD,o4l3r8r,2060 owners enjoying DLSS 4.5,hardware,2026-02-10 09:07:40,3
AMD,o44xsf4,"One being ""bad"" doesn't make the other not ""bad"".   I see this fallacy plastered all over Reddit constantly.",hardware,2026-02-07 20:12:11,11
AMD,o44ipr9,"Yes, Nvidia is bad? Its independent to whether AMD or Intel is bad or not.",hardware,2026-02-07 18:54:10,8
AMD,o44uv9c,"I am absolutely over whining about how unfairly poor dear innocent Nvidia are *slandered*, sir, absolutely slandered!  They absolutely deserve ever bit of criticism they get, and more.  AMD being half-assed and shitty doesn't excuse Nvidia one tiny bit.",hardware,2026-02-07 19:56:35,5
AMD,o44nu1z,"Yes, nvidia bad. nvidia very bad. They did the same shit or worse.",hardware,2026-02-07 19:19:56,-8
AMD,o43azsh,"Their CPUs might too giving they are trying to implement bigger cache like 3D v-cache. Add to that more cores for the i5 and below, AMD right now is what Intel was before Ryzen. Difference is Intel was market leader back then, AMD still isn't today.",hardware,2026-02-07 15:18:41,39
AMD,o4571h5,"to be honest, I doubt AMD cares much about consumer personal computing anymore. They are all about AI and enterprise customers.",hardware,2026-02-07 21:02:28,5
AMD,o43ccky,What good does open source really do?  Did nothing for FSR1-3.  Dlss is now pretty much everywhere and you can mod DLSS into games. No open source was needed.,hardware,2026-02-07 15:25:34,14
AMD,o4g2d1q,Still the most prevalent opinion on the internet and not just reddit,hardware,2026-02-09 15:26:44,1
AMD,o49mn4g,"Well my point still stands, if group A is fine without upscaling then they should be happy either way, they can just choose not to use any of the features but having said features is still nice to have, the problem is that group A is so loud and proud about not giving a fuck about upscaling that if you mentioned one bit about how you would like for a better upscaling, they would just tell you to use native because upscaling is ""bad"" and that ""fake resolution"" is not acceptable. Which in turn affects group B who does want those features but can't quite afford or doesn't wanna spend extra money for the Nvidia counterpart.   If amd follow the hate blindly then they'll just be left behind at this point just like how they didn't believe at dlss at first and so did many people including reviewers. But I think that they did learn their lesson from that by quickly implementing frame generation despite the hate it's getting because who knows what frame gen might hold in the future, it might just turn into a fine wine like dlss.",hardware,2026-02-08 15:42:50,3
AMD,o4a52uc,"Only if AMD fold and add it, as standard without users having to jump through hoops. Otherwise FSR will have too little marketshare to justify adding it as an option.",hardware,2026-02-08 17:13:08,2
AMD,o44vyd8,Knowing what your adversary is doing doesn't mean you can counter quickly. Developing and bringing to market dedicated ML hardware takes more than 2 years.  Final hardware specs are decided up to a year before products hit the market.,hardware,2026-02-07 20:02:23,4
AMD,o4iapsa,How dare the company with 90+% market share receive ANY demands from the customer base,hardware,2026-02-09 21:54:58,3
AMD,o44fnqw,"Give the tribalism a rest for once, I'm begging you.",hardware,2026-02-07 18:39:06,-5
AMD,o438rzr,With AI generating 10x profits. You are right consumer side lost all the leverage until this blows over. Even people bough more amd or intel. There isnt enough gpus to satisfy all prebuilt and side markets.,hardware,2026-02-07 15:07:11,-1
AMD,o45o6l0,a good framegen implementation is the best motion blur available.,hardware,2026-02-07 22:35:19,6
AMD,o45qw8m,"That was ATI, AMD had nothing to do with the 9700 series back then.",hardware,2026-02-07 22:50:43,11
AMD,o44bhn2,"> On the graphic side they where leading in the Geforce FX era...  That was ATi though, before AMD acquired them. The first Radeons to be released after the acquisition were the HD 2000 series, which were a massive flop. Post-acquisition RTG never managed to match the Radeon domination of the 9700/9800, x850 and x1950. The closest they came were with the first two GCN generations, HD 7970 GHz and R9 290x. Everything after that has either been shit, too little too late, or at best a compromise.",hardware,2026-02-07 18:18:45,8
AMD,o43czjv,Intel shipped the first consumer dual-core x86 with Pentium D in early 2005. AMD came a months later with Athlon 64 X2.  AMD64 is a licensed architectural extension of intelâ€™s x86 ISA,hardware,2026-02-07 15:28:47,5
AMD,o43d0v3,"> but they haven't really been able to do that.   In 2017 they weren't able to, that's fair enough.  But it isn't 2017, and they have plenty of resources to stop being so disappointing on the GPU side.  Especially as many of the disappointments aren't even due to resource constraints, just poor decision making.  Like the decision the video in the OP discusses.  They've *done* the work to let RDNA2/3 GPUs use FSR4!  All they have to do is add it to the drivers!    But no, they'd rather pull some bullshit notion of segmentation to 'force' people to buy RDNA4 cards, because they're in complete denial of the fact that AMD isn't many people's first choice to start with, and certainly not after they've alienated customers with a move like this.  They keep acting as if they dominate the GPU market, rather than being a very very distant second place in it.",hardware,2026-02-07 15:28:58,11
AMD,o435m6o,"Zen wasnâ€™t some stroke of pure originality. AMD rebuilt the CPU team. They poached Intel and Apple architects, Jim Keller included, scrapped Bulldozer, and went back to IPC first design.",hardware,2026-02-07 14:50:08,6
AMD,o4bnwmt,"AMD often introduced new memory standards first, but NVIDIAâ€™s architectures paired with older, mature memory still outperformed them. From NVIDIAâ€™s perspective, there wasnâ€™t much incentive to follow AMD down those memory experiments when skipping them didnâ€™t cost performance leadership.  GDDR4 and HBM1 are good examples of why NVIDIA didnâ€™t rush to follow AMD on memory transitions  GTX 8800 Ultra (gddr3) > HD 2900 XT (gddr4)  GTX 980Ti (gddr5) > Radeon Fury X (HBM1)",hardware,2026-02-08 21:38:24,2
AMD,o4l5xq0,SSR is basically horrible solution any time you cannot control the camera and force it to look upwards.,hardware,2026-02-10 09:29:06,2
AMD,o46peui,"Reminds me of a TPU forum post of an NV RT demo where dweebs who had clearly hadn't touched grass for a while were preferring super sharp sun shadows over RT soft shadows with variable penumbra, calling it blurry and low res. Those people really need to go outside.",hardware,2026-02-08 02:23:21,2
AMD,o44iob4,"I spent like 5 minutes looking for it on my history on youtube, and yeah, remembered how ass was search on youtube. The gist that I remember is that in a side by side blind comparison, people preferred the non-raytraced. I remember it well because someone commented like something like people probably wanted something that looked familiar, not realistic in the context of a video game, that I disagreed with because I think it was the jarringness of having realistic water with not very realistic, I think it was a log, of a pier. The basis of that is that humans are very good at finding inconsistencies.",hardware,2026-02-07 18:53:59,-2
AMD,o498j4s,"Do you think techpowerup is misleading? They clearly state that rdna3 has matrix cores but maybe theyre wrong?  Also amd themselves advertised rdna4 with second gen ai accelerators, so i wonder what happened to first gen ai accelerators.",hardware,2026-02-08 14:26:57,2
AMD,o48xup7,RDNA 4 is maybe 40% better than RDNA 3 in ML tasks. RDNA 3 AI accelerators exist and were advertised yet never used for anything official.     Don't let the marketing fool you.,hardware,2026-02-08 13:21:36,0
AMD,o48pmos,So people will have a reason to buy RDNA 4.,hardware,2026-02-08 12:21:39,1
AMD,o4mdbep,Then how can the ML TFLOPS/TOPS per CU/SM at iso clock be identical between 40 series and RDNA4?   Doesn't NVIDIA have them either?,hardware,2026-02-10 14:39:15,1
AMD,o42zrz6,Pretty much...  AMD CPU marketing and choices? Amazingly good.  AMD GPU marketing and choices? Amazingly awful.  Just why is there even such a ridiculously dramatic difference between them...,hardware,2026-02-07 14:17:36,15
AMD,o46i4rg,"Nope people loved it, look at r/AMD threads and HUB videos about it",hardware,2026-02-08 01:37:31,-1
AMD,o465cre,Theyâ€™re not good. FSR 1-2 are extremely basic and frankly bad. 3 is a little better but still far from good.,hardware,2026-02-08 00:19:25,4
AMD,o42un8t,Wait native xess/fsr look worse than dlss q when dlss q isn't better than taa all the time?   [https://www.youtube.com/watch?v=O5B\_dqi\_Syc&t=1s](https://www.youtube.com/watch?v=O5B_dqi_Syc&t=1s)  [https://www.youtube.com/watch?v=T86IufvA4qg&t=1101s&pp=ygUfZGxzcyB2cyBuYXRpdmUgaGFyZHdhcmUgdW5ib3hlZA%3D%3D](https://www.youtube.com/watch?v=T86IufvA4qg&t=1101s&pp=ygUfZGxzcyB2cyBuYXRpdmUgaGFyZHdhcmUgdW5ib3hlZA%3D%3D),hardware,2026-02-07 13:47:43,-19
AMD,o42x0gd,"> Upscaling is being widely adopted as part of the optimization package and default anti-aliasing method.  Upscaling as ""optimization"" is a horrific thing ~ but anyone with a few brain cells knew that as soon as that was offered as a feature, it was going to be abused by publishers and studios to have an excuse to do less optimization. When I say ""optimization"" I mean the real thing, not crappy bandaids passed as such.",hardware,2026-02-07 14:01:42,-14
AMD,o42uqxt,Just lower settings & resolution & use fsr/xess native like dlaa,hardware,2026-02-07 13:48:20,-16
AMD,o457s56,"To be honest, the Native argument was already beginning to fall apart after DLSS 2.5.      The Transformer models only just helped seal its grave.",hardware,2026-02-07 21:06:29,9
AMD,o42xivk,rasterization is good,hardware,2026-02-07 14:04:43,-10
AMD,o43bb7a,"And they did it again with ML frame gen, it has to be fsr 3.1.4, so a lot of games that only have fsr3.1 are cut off yet again from the upgrade",hardware,2026-02-07 15:20:19,5
AMD,o4l7fh6,"only for FSR3.1 implementations that use the DLL, not all do.",hardware,2026-02-10 09:43:46,2
AMD,o43bgc4,But amd said it wasnt in maintenance... right....,hardware,2026-02-07 15:21:02,4
AMD,o42wdrg,"This sub often has a weird hard-on for Nvidia ~ I've observed the comment history on here for long enough.  So, your comment is nonsensical.",hardware,2026-02-07 13:57:59,0
AMD,o42t0bs,Hardly.,hardware,2026-02-07 13:37:56,-10
AMD,o42rxk3,Intel is more fun to root for if you hate Nvidia these days,hardware,2026-02-07 13:31:18,-13
AMD,o42xkvw,AMD is one of the worst hardware companies ever. They dont know how to market their shit CPU products with the same 6core Zen5%. RDNA flat out sucks compared to Nvidia and their software stack sucks really. I dont know how can people defend such a company in 2026...,hardware,2026-02-07 14:05:03,9
AMD,o44mb2y,"> Nonetheless: HU did not take this story well. They were previously indeed focusing on rasterized performance. After this Nvidia stunt, they continued to do that, and it took them until about ~2022/2023 when they finally started properly and broadly testing FSR/DLSS - after it became evident that a) there's enough games that support it and b) there is a demand for coverage from the pubic, because gamers are actually using it.  I mean that's a fair attitude no? People might disagree with it, but that's not evidence of bias, it's just an opinion on the value of RT. RT has only really come into fruition in the last few years, before then it was a reflective puddle that halved the frame rate or something. Not focussing lots of benchmarking RT for the perhaps 5 games where it is worth it does make sense.  >Even still, a lot of their FSR vs DLSS comparisons did acknowledge that DLSS was better, but the wording was always casting AMD in a better, or at least a good, light. Either FSR wasn't far behind DLSS, or the price differences on GPUs made AMD a better purchase, or whatever other reason they could find, but they had a very clear pro-AMD bias.  Since DLSS 2 (1 was awful), they've always said DLSS was better than FSR. At times FSR wasn't that far behind DLSS, and when AMD cards were a fair bit cheaper it might be been worth it for some. Pointing that out isn't bias.",hardware,2026-02-07 19:12:11,2
AMD,o46p7fa,"You don't need to come up with hypothetical thought experiments, you can just look at some of their previous video titles to see they don't go easy on AMD.  1. [AMD Throws Loyal Radeon Customers Into The Trash](https://www.youtube.com/watch?v=KsjjFr9mB7w) 2. [AMD Keeps Screwing Up](https://www.youtube.com/watch?v=iLpAinbL8vA) 3. [AMD Fails Again: Radeon RX 7600 Review](https://www.youtube.com/watch?v=Yhoj2kfk-x0) 4. [AMD FSR Redstone Tested - There's Disappointing Issues](https://www.youtube.com/watch?v=LpAZF_-qsI8)",hardware,2026-02-08 02:22:04,2
AMD,o42xkez,yeah you are right,hardware,2026-02-07 14:04:58,2
AMD,o43kmiz,by far the worst video was about Frame Gen... recorded at 30 fps to amplify artifacts. It was hilariously bad and i've lost any respect i had for him after that - he is simply deranged at this point,hardware,2026-02-07 16:06:17,15
AMD,o4mgglo,"In what world is BURNING CONNECTORS better than Windows bugging out drivers occasionally?   When reporting about current hardware, you report about current hardware, not AMD's PAST problems.",hardware,2026-02-10 14:55:35,1
AMD,o44c355,">â€žAlso: AMD released after Nvidia and had the weaker hardwareâ€œ so?  They actively release their GPUs at a value that makes it better than Nvidia. So obviously, they will look better in a direct comparison. When you flame Nvidia, your AMD flame has to be less because that's just how it is objectively.   >â€žAnd AMD still got a shitload of flame for pricing better than Nvidia, but not good enough.â€ž Â  Another point: this was only true if you only look at raster which HUBs loves to do because it makes amd look better  Since when is raster performance pricing? You good, mate?",hardware,2026-02-07 18:21:36,-3
AMD,o45af37,Nvidia did their best to shorten the lifespan of 30X0 serie. 3070 should have had at least 12GB and 3080 16GB for sure.,hardware,2026-02-07 21:20:42,19
AMD,o4tu406,There is rarely as anything in these subs that isn't fallacy. Irrationality and the lack of basic reasoning skills are rampant.,hardware,2026-02-11 17:01:44,1
AMD,o44jw6i,All 3 are bad. Different types of bad. You gotta lick which bad is least bad to you.,hardware,2026-02-07 19:00:02,18
AMD,o450vcz,With software support/features? No. They've done plenty bad and made some poor decisions but they've always done well on this front,hardware,2026-02-07 20:28:52,24
AMD,o44vv79,When,hardware,2026-02-07 20:01:55,6
AMD,o43dg6i,This is why it is important to have Intel around. We need there to be legit competition and Intel needs to rebuild itself which could be good for consumers.,hardware,2026-02-07 15:31:06,28
AMD,o44znn9,Top-end Nova Lake and Zen 6 X3D are rumored to have the exact same amount of cache. I'm interested to see what the independent benchmarks show in mid-2027 by which point both will hopefully be out.,hardware,2026-02-07 20:22:13,6
AMD,o448tcq,I remember AMD being about 10% to 15% slower per core most of the time when Intel was dominant. AMD is absolutely destroying Intel at gaming these days. It doesn't feel even remotely close.,hardware,2026-02-07 18:05:33,-5
AMD,o459ne2,"Even before the AI bullshit, the consumer dGPU was clearly low on the priority list.  5th place at best.",hardware,2026-02-07 21:16:35,8
AMD,o44js39,The AI XeSS frame gen would have been useful for Nvidia cards,hardware,2026-02-07 18:59:27,3
AMD,o43d5yl,"If it were open source then people could port the XMX model to non-Intel cards, which is probably why they changed their mind and kept it closed. Nvidia obviously doesn't need it but AMD cards could have benefitted.  The binary XeSS SDK also only supports Windows, not Linux. If it was open source as originally promised then the community could work on bringing it over.",hardware,2026-02-07 15:29:41,10
AMD,o45dlby,"Especially if you might think (or hope) that what your adversary is doing is ""digging a big R&D hole"". And the issues that DLSS1 and to a lesser extent DLSS2 had may have made AMD feel justified in not picking up their shovels.  But now it seems as though Nvidia's hole has long since struck gold so AMD needs to grow their mining operation quickly. The bet didn't pay off, so Nvidia's the one raking in the cash. Or, as much as they can in the *paltry consumer graphics landscape*.",hardware,2026-02-07 21:37:33,2
AMD,o4l4w3n,if those demands are based on lies and are not realistic then they shouldnt be voiced.,hardware,2026-02-10 09:18:48,1
AMD,o44wzyi,"Never gonna happen when people make money off of it on YouTube. Anyone with eyes can see what video titles are assigned to what company  Case in point, we still get ""this company's MSRP was misleading"" this long after launch",hardware,2026-02-07 20:07:58,16
AMD,o447hmm,"> With AI generating 10x profits. You are right consumer side lost all the leverage until this blows over.  Willingly rolled over and parted their RND cheeks for ""AI Tech"". Everyone who was OK with this shite deserves their $800 RAM kit and 12GB GPU in 2026.",hardware,2026-02-07 17:59:00,2
AMD,o4l5a30,this might be the most convincing argument against framegen ive seen so far.,hardware,2026-02-10 09:22:35,1
AMD,o47xjhf,Motion blur is garbage though.,hardware,2026-02-08 08:01:45,-2
AMD,o45zfjg,"All frames are fake, the end result is what matters",hardware,2026-02-07 23:43:40,4
AMD,o43esde,"The difference between the releases isn't months, but days. The Pentium D was May 25th, 2005. The Athlon 64 X2 was May 31st, 2005.",hardware,2026-02-07 15:37:44,18
AMD,o441per,"Pentium D wasn't a dual core design.   It was a dual CPU design with two separate CPU's on one package.   This was implemented in the same way as dual socket systems of the time, where the CPU's interfaced with each other over the FSB.",hardware,2026-02-07 17:30:23,17
AMD,o449oje,"> Pentium D  I'd call this two-core, not dual-core. They didn't have intra-die comms in them pentiums if I remember correctly.",hardware,2026-02-07 18:09:48,3
AMD,o43j3f4,"Oh no I didn't mean to imply that they are trying and haven't been able to. I think they were trying to catch up with Nvidia with early RDNA, but yes the way they've marketed most of the RDNA tech and the anti-consumer bullshit they've been doing has definitely been disappointing. Bad leadership and bad business practices trying to be seen as slightly less shitty than Nvidia but not providing products that actually compete. I was rooting for AMD for a while just to provide some competition and alternatives to Nvidia's shitty business practices and bullshit but we are stuck with 2 shitty GPU Companies and even if Intel catches up they're also a shitty company.",hardware,2026-02-07 15:58:45,4
AMD,o46dlnk,"They had Jim Keller before either of those two companies, and brought him back on to help with the Zen uarch. All of these companies technically ""poach"" from each other in way. I find it weird to call amd, a cheap generic cola cpu, when Intel pumped out those broken raptor lake cpus not relatively long ago. The reality is, arm probably going to sneak up on both those companies.",hardware,2026-02-08 01:08:50,1
AMD,o4c4l77,"And 4800 series trashed GTX 200 series on price/performance. Much thanks to needing less busswidth to feed the 4870/4890 thanks to GDDR5.   > GTX 8800 Ultra (gddr3) > HD 2900 XT (gddr4)  The 2900 XT was using GDDR3 actually. The first GDDR4 card was the 1950XTX.  And AMD soon after brought out the 3870. Which while still a bit slower than the 8800GT. Still fixed much of the 2900XT issues, and one big thing was taking the bus down to 256 bit thanks to faster GDDR4.  >GTX 980Ti (gddr5) > Radeon Fury X (HBM1)  The only reason they could even compete in that segment was thanks to HBM. Hawaii already had a 512 bit bus.",hardware,2026-02-08 23:07:23,1
AMD,o4l60nw,"To be fair that is one thing i prefered long before RT came around. I like the sharper shadows over software. To me they look a lot more realistic, as in real life i see sharp shadows every day. In Crysis (2007) you could play with soft shadow settings and get them how you wanted, but the sharp ones always just looked so much more realistic.",hardware,2026-02-10 09:29:54,2
AMD,o4b5a3d,"The confusion is that AMD uses specialized matrix cores in CDNA. In RDNA3, the so called AI cores are just the graphics core with added flexibility (executing WMMA instructions.) That's why you aren't getting any AI RDNA3 tasks to run alongside raster, it's meant for non graphics application.  9070xt probably dedicates way more die space for AI instructions.",hardware,2026-02-08 20:05:27,2
AMD,o498sk5,"This article from amds official website literally states dedicated ai acceleration, so they do have something  [rdna3 revealed](https://www.amd.com/en/newsroom/press-releases/2022-11-3-amd-unveils-world-s-most-advanced-gaming-graphics-.html)",hardware,2026-02-08 14:28:29,2
AMD,o48q4jz,Looks like people are buying elsewhere instead.,hardware,2026-02-08 12:25:36,5
AMD,o4mmu0m,ML TOPS is a metric thats hard to compare unless we know precision used and its same in both cases. But its those matrix extended instructions sthat allows regular cores to make ML tasks faster for AMD that achieve this. They certainly made a lot of improvements on it for RDNA4. Its still not dedicated hardware though.,hardware,2026-02-10 15:27:31,1
AMD,o4345tb,"> AMD CPU marketing and choices? Amazingly good.  That really depends on the timeframe in question.  They've only had two eras in which their desktop chips were on top:  * ""Athlon"" / ""Athlon 64"" vs Intel's ""Netburst"" (2000-2006)  * ""Zen 3"" and later vs Intel's ""Lakes"" (i.e. Rocket / Alder / Raptor / Arrow; 2021 to present).  Setting aside AMD's humble beginnings as a ""second source"" manufacturer of Intel's designs in the 1980s and their eternal game of catch-up in the 1990s after Intel discontinued second sourcing of their designs starting with the 486, they were completely noncompetitive between late 2006 and the advent of Zen 1 in 2017 due to their ""Phenom"" and ""Bulldozer"" architectures lagging far behind Intel's ""Core"" CPUs.    Note that even when Ryzen launched it still took three generations just to reach technological parity in 2019-20 versus Intel's then-current offerings (i.e. Ryzen 3000-series / ""Zen 2"" vs Core 10000-series / ""Comet Lake"") as well, so honestly it's *only* been 5+ years of AMD being in the lead so far this time around.",hardware,2026-02-07 14:42:07,18
AMD,o4l6gvv,"AMD CPUs are good, but marketing? have you forgotten that they flat out lied about almost all performance metrics of current generation?",hardware,2026-02-10 09:34:17,2
AMD,o474ec0,"I was just trying to give them some credit, ""acceptable"" might have been the better word.",hardware,2026-02-08 04:00:32,1
AMD,o430bdf,"These are pre-DLSS 4 videos, although I wouldn't make that statement flat-out either, but it's kind of safe to say that just using DLSS 4/4.5 will get you very good results.",hardware,2026-02-07 14:20:39,18
AMD,o430ouh,"I used to be in the same boat when upscaling was kinda crap, it was only good if you really really needed more performance otherwise it's just better to stick to native.       However these days, it's actually so good that I can consider it an optimization, it basically increases performance without you noticing most of the time, isn't that what optimization is? Developers have been running certain aspects of games at lower resolution than the user's native resolution for decades at this point, that was optimization and it was good optimization cause you can't really tell that the cloud particles in the distance are actually only 1/4th the resolution you're playing at.",hardware,2026-02-07 14:22:49,22
AMD,o42y6yw,"Sure, that opinion is largely irrelevant to the reality we are facing though.  90% of people are happy to use it so it will continue to gain ground despite a minority opposition.  That means you can either keep up or fall behind, because this is the direction things are going.  It may be distasteful to you, but here we are.",hardware,2026-02-07 14:08:33,12
AMD,o42y0id,"optimization most of the time will just be devs lowering settings. Dont many will spend a lot of time with lods, unless they're given way more time",hardware,2026-02-07 14:07:31,12
AMD,o4l6wz1,">Upscaling as ""optimization"" is a horrific thing  why? we have been using upscaling as optimization for variuos parts of game rendering since 3D rendering became a thing. There is almost no game out there that has full resolution shadows, for example.",hardware,2026-02-10 09:38:47,1
AMD,o42zvsv,"Why would you lower resolution and then use whatever your preferred TAA solution at the lowered ""native"" render and not just upscale instead?",hardware,2026-02-07 14:18:11,11
AMD,o45kxab,"Yea dlss sr 2.5.1 dll was quite the improvement when it dropped, as it vastly reduced ghosting in a lot of games. And it keeps getting even better with the transformer dlss sr dll 310.x.x presets J/K and now M/L.   Though the overhead keeps increasing as well so from performance stand point it isn't completely free visual upgrades, especially at super high fps, but always the choice to use the older presets/whatever the game came with as well if you want to, instead overriding to the latest.",hardware,2026-02-07 22:17:11,5
AMD,o45xu0u,So we just canâ€™t use any modern features then,hardware,2026-02-07 23:31:23,3
AMD,o43ksou,"This sub has certainly leaned towards nvidia and downplayed AMD. RDNA2 was barely being considered a 2080Ti competitor on the basis of the performance/power of RDNA1 5700XT.   And DLSS didn't magically end up being as good as it is today, despite heavily upvoted comments maintaining how it is 'as good or better' than native for years.    Now it'll be funny if RDNA5 or whatever AMD are calling their next-gen, ends up in consoles and optimizing for its raytracing pipeline is slower on nvidia cards.   https://www.reddit.com/r/hardware/comments/1e8sri7/leaked_rdna_4_features_suggest_amd_drive_to_catch/lea62vy/",hardware,2026-02-07 16:07:09,-8
AMD,o42tdq4,Bruh,hardware,2026-02-07 13:40:11,13
AMD,o43vila,"People on tech subreddits blamed me for driver issues on my 6750XT, driver issues AMD themselves admitted to.",hardware,2026-02-07 16:59:45,2
AMD,o42sqao,"I donâ€™t hate any of them, itâ€™s just hardware to me. Iâ€™ll purchase whichever product gives me the best performance within my budget",hardware,2026-02-07 13:36:13,20
AMD,o46rbod,"4 examples in 3 years is more outliers than a trend, and it doesn't diminish that they brought out the kid gloves here. You won't be able to find them going as easy on nvidia as they are here",hardware,2026-02-08 02:35:21,2
AMD,o43rr62,And then doubled down in comments saying fps has no impact on image quality. It also made me completely shift my opinion on him.,hardware,2026-02-07 16:41:19,16
AMD,o4mnole,"In the world where burning connectors are so rare and easy to replace compared to driver crashes with no fix for months because thats how long AMD usually takes to fix compatibility after software launches.  Its not past problems. Two years ago AMD driver got people banned from multiplayer games by triggering anticheat. Last year many popular games were literally unplayable with AMD cards until months after release. And lets not even go into work tasks, most productivity apps dont support AMD outright due to how bad the driver is.",hardware,2026-02-10 15:31:38,1
AMD,o45t7py,Who the fuck downvotes this,hardware,2026-02-07 23:04:07,10
AMD,o4c5cgo,I remember Nvidia justifying their 10GB card back in 2020 on Reddit,hardware,2026-02-08 23:11:48,1
AMD,o45fauk,Reverse is also true. Letâ€™s not forget Intel was complacent for a very long time taking full advantage of their position for years until they started losing marketshare to competitors.,hardware,2026-02-07 21:46:44,6
AMD,o44ym0j,"During the Bulldozer era it was more like a 30 or 40% difference per core.  In productivity workloads, you saw 4 Intel cores generally beating 8 AMD cores, and in gaming it was a night-and-day difference.  Sandy bridge and its successors were so dominant over AMD that the company nearly went out of business.... and Intel decided to cease nearly all CPU development and improvement for 5-6 years, which eventually bit them in the ass.",hardware,2026-02-07 20:16:33,10
AMD,o449xjk,"maybe i'm crazy for this, but i feel like it's important to recognize workstation performance too when comparing the brands, especially when most people aren't chasing 1% differences. i'd much rather pick up a 14600k over a 9600x just for the fact that it will run the rest of the computer better, on top of being similar for gaming in experience",hardware,2026-02-07 18:11:01,10
AMD,o48frw1,The huge lead in gaming is only for X3D CPUs and intels next gen will also have a huge cache that could close the gap.  In normal single core and MT they are quite close already. Just a few gens back AMD had a huge MT lead of intel (80%+) but alder lake and raptor lake were able to catch up.,hardware,2026-02-08 10:54:15,2
AMD,o4m340t,"https://www.overclockers.com/amd-fx-8350-piledriver-gaming-comparison/  I'll respond to myself with this link for anyone who wants to see proof. Each core back then underperformed Intel's cores clearly. I can't remember AMD's naming scheme for modules and cores. Each set of two cores as we think of them was about 16% slower than Intel's one core. They were about 16% slower at gaming. That's about how much slower AMD was than Intel at gaming. Today, Intel struggles so hard at gaming that sometimes AMD is 50% faster at certain games. I know I know you can buy a cheap Intel that works as a good workstation. Sure if you want something power hungry you can get a cheap Intel. AMD has 16 core CPUs that do excellent as a workstation too. My ultimate point is I think Intel is further behind at gaming than AMD. Yes, it'll be awesome when Intel gets 3D cache. I'm sure that'll give them a giant leap forward. My comparison is talking about the situation today.",hardware,2026-02-10 13:44:11,2
AMD,o4l9ibz,"This is pure delusion my goodness. It was way more than 10 to 15 percent. 13900k still to this day competes very well with the 9800X3D, especially with faster memory, donâ€™t forget that CPU is over 3 years old now.",hardware,2026-02-10 10:03:49,0
AMD,o45hlx8,True,hardware,2026-02-07 21:59:07,2
AMD,o456or4,Doesnâ€™t it run on any card now though?,hardware,2026-02-07 21:00:33,2
AMD,o43ernq,"With expensive xess dp4a is on amd cards, xmx will unquestionably be more expensive.  Fsr4 fp8 and int8 can be modded and ran on pre rdna4 but at high performance penalty than fsr3 or even xess dp4a. Significantly more on the fp8 version.",hardware,2026-02-07 15:37:38,15
AMD,o4lszht,lmaooooooooooooooooo,hardware,2026-02-10 12:42:59,1
AMD,o4l4y7s,Yep. What we should actually do is aim the tribalism to make it illegal not to ban users with clickbait titles in the videos.,hardware,2026-02-10 09:19:22,1
AMD,o4554hi,"I see it more as pro-consumer thing to acknowledge these mega corporations' latest snafu, at least from gn and hub, none of these companies give a shit about  anything but quarterly profit.",hardware,2026-02-07 20:52:03,0
AMD,o4qdwbp,"i like framegen.  it's fantastic motion smoothing.  just not a replacement for frame rate.  eventually monitors with aspirational hz will become common, and framegen will become the default.",hardware,2026-02-11 02:44:05,1
AMD,o45w6nk,"Fun fact, inter core communication on athalon x2 was done through main memory exactly like the pentium D cores. The pentium d would still experience higher inter core latency though because the memory controller was on the north bridge and not the same die as the athalon x2.",hardware,2026-02-07 23:21:30,3
AMD,o4798xr,"It was also not as important at the time. You had two cores mainly to dramatically offload activities off of the main core. Each core would run different and independent tasks, and they'd stick to the core they were assigned to.  It took long years for software to be (painstakingly slowly, one tool at a time) updated to effectively use two cores at the same time.   Nowadays, you've got multi-threaded software that benefits much more from comms and low latency between cores. We've also got insane schedulers having CPUs juggle tasks across threads within nanoseconds. But we took ages to get here, and a good chunk of software still isn't really there yet.",hardware,2026-02-08 04:34:47,3
AMD,o45wcqb,Neither did the athalon. Both cpus had to go out to main memory to talk to the other core.,hardware,2026-02-07 23:22:30,2
AMD,o4l5jk1,>They didn't have intra-die comms in them pentiums if I remember correctly.  Neither did Athlons.,hardware,2026-02-10 09:25:14,1
AMD,o46m7hh,AMD has basically followed this pattern forever:  Intel 8080 â†’ AMD Am9080  Intel 386 / 486 â†’ AMD Am386 / Am486  Intel SSE â†’ AMD adopts it after 3DNow! loses  Intel SSE3 â†’ AMD Venice adds it later  Intel Hyper-Threading â†’ AMD SMT  Intel Turbo Boost â†’ AMD Turbo Core  Intel AVX / AVX2 â†’ AMD adds support later   On the GPU side itâ€™s the same story:  NVIDIA SLI â†’ AMD CrossFire  NVIDIA G-Sync â†’ AMD FreeSync  NVIDIA DLSS â†’ AMD FSR  NVIDIA RTX ray tracing â†’ AMD RDNA2 RT  NVIDIA Frame generation â†’ AMD FSR 3   Generic rc cola,hardware,2026-02-08 02:03:10,5
AMD,o4c59z4,"1GB GDDR4 2900 xt was also released, with memory clocked at 1000 MHz (2000 MHz effective), although performance differences between the two variants were negligible.",hardware,2026-02-08 23:11:24,1
AMD,o4mdvjo,"What about 40 series then? RDNA4 has the same ML compute per CU as 40 series. Compare FP8, INT8 and INT4 numbers.  Besides dual issue for FP16 it's 4X over RDNA3. 8X with sparsity. Same as 40 series.",hardware,2026-02-10 14:42:12,1
AMD,o4l6bw8,they have something. that sometihng is AMDs advertisement department ability to somehow find a way to lie every single time.  What they have is regular graphic cores with expanded capabilities to execute matrtix multiplication instructions.,hardware,2026-02-10 09:32:56,2
AMD,o4mcsq8,"It's just dual issue WMMA instructions through vector ALUs. Only applies to FP16 everything else is basically unchanged compared to RDNA2.  RDNA4 much higher throughput. FP8 is quadrupled, 8X with sparsity. Same for INT8-4.   4X dense throughput compared to RDNA3-2 per CU is a lot.  If this doesn't make sene u/Strazdas1's explanation is easier to understand.",hardware,2026-02-10 14:36:31,2
AMD,o4mvl1b,"They do talk about Matrix cores in this official GPUOpen programming guide: [https://gpuopen.com/learn/using\_matrix\_core\_amd\_rdna4/](https://gpuopen.com/learn/using_matrix_core_amd_rdna4/)  If you search for RDNA3 there are zero mentions of Matrix Cores in relation to that.  In specs sheet for 40 series in the whitepaper PDF Tensor FP8 dense is 8X FP32 TFLOPs. sparse = 16x. FP16 is half that and INT4 is double.   9070XT is the same (AMD product listing on their website). FP8 matrix = 8X FP32, sparse = 16X, FP16 is half that and INT4 is double.  If RDNA4 doesn't have dedicated ML then all RTX cards doesn't have it either.",hardware,2026-02-10 16:08:55,1
AMD,o434x40,"I'm talking post-Ryzen AMD, not their prior struggles, where they were desperate to sell anything. Pretty sad marketing, but at least Sony and Microsoft could market the consoles for them...  We do need to recall that Intel's marketing and choices were pretty dogwater for a good decade prior to Ryzen, because they were just sitting on their monopoly, and no-one had any real choices. Intel could just coast on being the brand with the better performance, even if everything else suffered.",hardware,2026-02-07 14:46:20,5
AMD,o4l6npg,FSR1-2 is not acceptable.,hardware,2026-02-10 09:36:12,3
AMD,o431b4a,Dlss 4/4.5 is better than xess/fsr native? 70%> of the time?,hardware,2026-02-07 14:26:19,-10
AMD,o4324hk,"It is still **not** an ""optimization"". This is exactly the crap logic publishers and studios want you to accept so they don't have to properly optimize and just tell you to use upscaling instead.  You don't seem to understand what optimization actually is ~ getting the same effective quality more efficiently. No, I don't mean using upscaling ~ but the same native resolution giving you more performance because you optimized your rendering technique. Upscaling in that regard is just a hack, not an optimization.  You can't use stuff like particle effects in your argument, as that is entirely distinct from upscaling from a lower final resolution to a higher one. That just says to me that you don't understand the difference.",hardware,2026-02-07 14:30:49,-24
AMD,o42ymhi,"> optimization most of the time will just be devs lowering settings.  That is not ""optimization"" ~ that's just lowering settings.  Optimization proper would getting the same effective quality more efficiently.  > Dont many will spend a lot of time with lods, unless they're given way more time  LODs should not be skimped on, because they offer massive boosts in performance. If devs skip them, they're simply lazy or just think so new hot crap like some UE5 Nanite equivalent will mean they can just skip optimizing. A waste of time and effort to implement compared to just make LODs with some program dedicated to producing them for you, which do exist. (Can't remember names...)",hardware,2026-02-07 14:11:02,-5
AMD,o478rhg,"Most games dont have ""modern features"" and you still enjoy them.",hardware,2026-02-08 04:31:12,1
AMD,o44a1rw,"> This sub has certainly leaned towards nvidia  Fuckin pull the other one pal, there's still some give left in it",hardware,2026-02-07 18:11:36,10
AMD,o4l7q8g,"> RDNA2 was barely being considered a 2080Ti   RDNA2 was an expensive paperweight.  >And DLSS didn't magically end up being as good as it is today, despite heavily upvoted comments maintaining how it is 'as good or better' than native for years.   DLSS was better than native since DLSS3.  >Now it'll be funny if RDNA5 or whatever AMD are calling their next-gen, ends up in consoles and optimizing for its raytracing pipeline is slower on nvidia cards.   just another gen of consoles being so shit it drags entire industry down. Just like last two generations. Thanks AMD.",hardware,2026-02-10 09:46:42,2
AMD,o436l1v,"The easiest way to farm karma is to complain about how biased Reddit is, while in a thread where the exact opposite is happening.",hardware,2026-02-07 14:55:25,-6
AMD,o43c0jv,"As you should. I buy AMD because it offered me the best prices for the performance I was looking for. Also, not having to deal with nvidia bullcrap on linux.",hardware,2026-02-07 15:23:51,6
AMD,o448gwr,"\>And then doubled down in comments saying fps has no impact on image quality.  Okay, I saw original video and cringed at testing with 30fps native, but haven't seen those comments. Steve REALLY wrote that?!",hardware,2026-02-07 18:03:51,9
AMD,o43v5if,> doubled down in comments saying fps has no impact on image quality  yea!,hardware,2026-02-07 16:57:58,8
AMD,o4mrg24,">Its not past problems. Two years ago AMD driver got people banned from multiplayer games by triggering anticheat. Last year many popular games were literally unplayable with AMD cards until months after release.  I gotta need some sources for this. Especially considering I have no run into a single of those issues with any of my AMD cards.  Also, just because certain applications or games are not optimised for AMD, that doesn't automatically make it a driver issue. E.g. if an anti-cheat detects a totally fine driver as a cheat and autobans people for it, in 99% of cases, that is on the anti-cheat, not the driver. If there is some function that they count as a cheat, they'd have to configure their anticheat to just tell people to disable that function instead of just autobanning them without a comment.  Tbh just sounds like you just call everything ""driver issues"", whether it actually is or not.",hardware,2026-02-10 15:49:33,1
AMD,o45ivqk,"Oh absolutely agree. We need more than one, was my main point. I wish we had more than the two on the CPU side like we used to.",hardware,2026-02-07 22:05:59,7
AMD,o48g1a6,"> Intel decided to cease nearly all CPU development and improvement for 5-6 years  They did not cease development, but failed for many years to get their new nodes online. It took many years of stagnation and failure from intel for AMD to catch up.  Hopefully Intel can deliver good yield in 18A and 14A, we need more highend foundrys besides TSMC.",hardware,2026-02-08 10:56:41,9
AMD,o44euls,"when people say amd is dominating perf on desktop, they're probably referring to gaming and x3d chips, specifically.  there's other comparisons to make, but that's the one that stands out",hardware,2026-02-07 18:35:08,6
AMD,o45mm8y,Yes,hardware,2026-02-07 22:26:35,3
AMD,o48xatb,"Just the downgraded dp4a version, not the XMX version",hardware,2026-02-08 13:17:56,2
AMD,o4l4zrb,Thats because you are clearly ignorant of whats actually going on.,hardware,2026-02-10 09:19:46,1
AMD,o4rhdjo,"I like framegen too, but i hate motion blur.",hardware,2026-02-11 07:41:05,1
AMD,o47zdd7,"No, that is not correct.  K8 was the first architecture with an integrated northbridge chipset, and memory controller, as well as two cores on a single die.  Inter-core communications were much faster, since they went over the brand new hyper transport link and stayed on-die   AMD64 x2 systems did not have a front side bus.",hardware,2026-02-08 08:18:46,1
AMD,o45z43x,Did the pentiums have something equivalent to hypertransport? you probably know more about this than I do,hardware,2026-02-07 23:41:42,0
AMD,o47zc5v,Somehow you missed x86-64 / AMD64,hardware,2026-02-08 08:18:27,1
AMD,o4n1r2e,RDNA4 made great advances. They should have come much sooner.,hardware,2026-02-10 16:37:19,1
AMD,o4md9wn,Yeah rdna4 has proper matrix cores. WMMA works with int8 and 4 too tho,hardware,2026-02-10 14:39:02,2
AMD,o435u8s,"> Pretty sad marketing, but at least Sony and Microsoft could market the consoles for them...  They were lucky that MS/Sony wanted to stop cycling through exotic CPU architectures (e.g. IBM's PowerPC / Sony's ""Cell"") in favor of x86, combined with the fact that AMD was the only vendor at the time who could design semi-custom x86 chips with a suitable level of integrated graphics.",hardware,2026-02-07 14:51:22,14
AMD,o432erh,I'm not the one who said that btw. Just that you could probably use another comparison video.,hardware,2026-02-07 14:32:27,15
AMD,o4l6sjz,DLSS 4 is better than native TAA 100% of the time.,hardware,2026-02-10 09:37:33,2
AMD,o43f2dm,"No, this is a bad take. Quality is what I can see. That's the ultimate baseline. If you can render me the same image faster by upscaling it from a lower fidelity image, then it's an optimization.",hardware,2026-02-07 15:39:08,16
AMD,o43fdn9,"Nah, thatâ€™s just you setting your own goal posts. â€œSame effective quality more efficientlyâ€ is basically the textbook definition of what upscaling is aiming to do.  All of graphics is full of these sorts of tricks, like are we going to say a game that has no LoDs and no visual culling is â€œoptimizedâ€? None of what you are looking at on screen is real, artificial purity tests are dumb, if it looks good and runs good itâ€™s good.",hardware,2026-02-07 15:40:40,13
AMD,o431nji,"Depends on how much time the devs are given. All devs know what lods are, they're 30x more knowledgeable about game optimzation than me and you.   But if they're given the same timeframe most of them will rather reduce settings to make sure they hit consoles targets and whatever PCs they have in mind.",hardware,2026-02-07 14:28:15,7
AMD,o4l750n,"lowering settings IS optimization. Theres no magic button that developers press to make things run faster. Its by reducing quality, always.",hardware,2026-02-10 09:40:55,0
AMD,o4l7an7,it is very evident that games that include modern features are more enjoyable because of them.,hardware,2026-02-10 09:42:27,3
AMD,o47a36c,Why not both?,hardware,2026-02-08 04:40:55,2
AMD,o5aj5gr,"what? Were you around when these discussions were happening in the year of covid?    This thread is just one example where even after the PS5 clockspeed reveal, people were downplaying big navi.  https://old.reddit.com/r/hardware/comments/gvwuur/big_navi_on_desktop_will_be_the_first_rdna_2_gpus/",hardware,2026-02-14 05:00:19,1
AMD,o4liafv,"Nah that's prob more so the PS4 and PS5, than PS6. PS4 games still going strong and will prob continue into PS6 crossgen era.    PS5 crossgen will be longer than anything we've ever seen before. 2032-2033 is a safe bet. Could extend to 2034-2035 worst case.   Thanks to stupid handheld, PS6 prob won't even have a chance to really shine before PS7 lands.  >Now it'll be funny if RDNA5 or whatever AMD are calling their next-gen, ends up in consoles and optimizing for its raytracing pipeline is slower on nvidia cards.  u/bctoy the only world where that is going to happen is one where AMD disregards almost ALL the work of their poached talent from Intel, NVIDIA, Qualcomm and Imagination Technologies. I find that extremely unlikely.  Also 50 series is a joke. It's glorified 30 series RT with low hanging fruits and low effort design in the form of OMM+SER+LSS, L2 and quadrupled ray/tri rate. No serious attempt to tackle the bottlenecks. Beating it will be easy.    For someone who actually bothered look at what Imagination latest designs achieved and imagine what scaling that up to dGPU power would do. Lot of AMD's latest design philosophy in their patent filings align with this (not surprising given their RT HW lead is from that company) so it's more likely than not that RDNA 5 will completely annihilate 50 series in RT traversal perf. RT shading perf will be adressed by HW optimization for work graphs and neural shading.",hardware,2026-02-10 11:23:02,1
AMD,o5ajnz0,">RDNA2 was an expensive paperweight.  If not for nvidia's gamble on RT, they were humbled that generation with the 30xx series clocks barely matching the 10xx gen clocks.  >DLSS was better than native since DLSS3.  No, it wasn't, and motion absolutely smothered the image.  Otherwise nvidia would have not bothered with the DLSS4, though you can make a case that they did so for FSR4 competition.  >just another gen of consoles being so shit it drags entire industry down. Just like last two generations. Thanks AMD.  Sometimes the shoe needs to be on the other foot for the people to come to their senses.    nvidia have better RT hardware, but that does not mean that their advantage will continue infinitely as the RT workload increases.",hardware,2026-02-14 05:04:16,1
AMD,o436p5t,For awhile that comment I made up there was absolutely getting downvoted,hardware,2026-02-07 14:56:03,9
AMD,o44ojkj,"Yea, he did",hardware,2026-02-07 19:23:35,5
AMD,o4rehxq,"No, that is 100% of the driver to start injecting games memory like a cheat program. It is also 100% on the driver if the driver crashes because of bugs in the driver itself.",hardware,2026-02-11 07:14:25,1
AMD,o4l4asd,The AI version only runs with Intel hardware.,hardware,2026-02-10 09:13:00,1
AMD,o4njfxa,"Thanks for the ad hominem. I assume you're not speaking from a position of representing a company or as an investor and believe in some position of which I am, as you say, I am ignorant.   Please enlighten me with your superior intellect.",hardware,2026-02-10 17:58:50,1
AMD,o4960xh,But everyone is on the AMD64 standard now and not the Intel one so I guess that would negate it being a cheap clone.,hardware,2026-02-08 14:12:18,2
AMD,o4nb5hk,100%. RDNA4 is what RDNA3 should've been.   Interesting that they state RDNA4 has Matrix Cores here. [https://gpuopen.com/learn/using\_matrix\_core\_amd\_rdna4/](https://gpuopen.com/learn/using_matrix_core_amd_rdna4/),hardware,2026-02-10 17:20:35,1
AMD,o4mejlk,"Yeah they literally state it right here: [https://gpuopen.com/learn/using\_matrix\_core\_amd\_rdna4/](https://gpuopen.com/learn/using_matrix_core_amd_rdna4/)  >AMD RDNAâ„¢ 4 architecture GPUs, which have 3rd-generation Matrix Cores, improved the performance of Generalized Matrix Multiplication (GEMM) operations. The table below compares theoretical FLOPS/clock/CU (floating point operations per clock, per compute unit) to previous generations. However, we changed the VGPR layout for the arguments of Wave Matrix Multiply Accumulate (WMMA) operations compared to the previous RDNA 3 generation \[1\]. Therefore, it does not have backward compatibility.",hardware,2026-02-10 14:45:43,1
AMD,o43ohwh,"> No, this is a bad take. Quality is what I can see. That's the ultimate baseline. If you can render me the same image faster by upscaling it from a lower fidelity image, then it's an optimization.  You are not rendering the same image faster! You are rendering a lower resolution image, and are relying on an upscaling model to fill in the blanks. That is not an ""optimization"" ~ it's a hack. I am talking actual render techniques, not workarounds.",hardware,2026-02-07 16:25:22,-10
AMD,o43o329,"> Nah, thatâ€™s just you setting your own goal posts. â€œSame effective quality more efficientlyâ€ is basically the textbook definition of what upscaling is aiming to do.  You are just changing the goalposts to sneak in upscaling as an ""optimization"" when it is not even in the same category as making more efficient code that does the same task, but better.  > All of graphics is full of these sorts of tricks, like are we going to say a game that has no LoDs and no visual culling is â€œoptimizedâ€? None of what you are looking at on screen is real, artificial purity tests are dumb, if it looks good and runs good itâ€™s good.  You are deliberately conflating different kinds of things ~ upscaling is not even akin to LODs or visual culling! Those are actual rendering techniques to reduce the amount of work happening on both the CPU and the GPU. The GPU isn't wasting time rendering stuff that is never observed.  In contrast, upscaling doesn't do any of that ~ all it does is upscale a lower resolution final buffer to something that mimics a higher resolution output.",hardware,2026-02-07 16:23:20,0
AMD,o432jox,"> Depends on how much time the devs are given. All devs know what lods are, they're 30x more knowledgeable about game optimzation than me and you.   Not all devs are cut equal ~ these days, many devs just lazily lean on game engine defaults given to them by UE5 and Unity, and don't bother to actually optimize, because they have never learned how to. After all, they develop on high-end GPUs, and everyone else just gets screwed, because they were never taken into account.  > But if they're given the same timeframe most of them will rather reduce settings to make sure they hit consoles targets and whatever PCs they have in mind.  Reducing settings is still not ""optimization"" ~ lowering settings is just lower settings. You're not making anything more efficient ~ you're just cutting corners to force something to run not awfully. It's still not efficient or optimized.",hardware,2026-02-07 14:33:13,-4
AMD,o47dyi6,If it removes enjoyment from your games then the modern features are important. If not I rly dont care about rt. Only pathtracing looks nice,hardware,2026-02-08 05:10:06,0
AMD,o5b7iyz,"No I'm saying ""lean to"" is an understatement, large majority of this sub are incredibly charitable to Nvidia and fullthroatedly embrace all the AI crap.  1080ti > 6950xt looking like two of best choices I've ever made more and more.",hardware,2026-02-14 08:38:02,0
AMD,o5gk701,"It wasnt a gamble and RT was not the only benefit Nvidia brought over AMD.  >No, it wasn't, and motion absolutely smothered the image. Otherwise nvidia would have not bothered with the DLSS4, though you can make a case that they did so for FSR4 competition.  DLSS3 in quality mode was. DLSS4 can do it in medium or sometimes even performance modes. The main benefit for DLSS4/DLSS4.5 improvements are reduced ghosting and better quality at higher upscale ratios. Quality mode was already fantastic.  >nvidia have better RT hardware, but that does not mean that their advantage will continue infinitely as the RT workload increases.  are you saying AMD is going to have better RT hardware in the future? Thats possible, especially with patents they have been filing. But i for one am not going to expect AMD to deliver until they actually do. They have been overpromising and underdelivering for too long.",hardware,2026-02-15 04:58:02,1
AMD,o4sj392,">No, that is 100% of the driver to start injecting games memory like a cheat program. It is also 100% on the driver if the driver crashes because of bugs in the driver itself.  ""Trust me bro.""",hardware,2026-02-11 13:01:57,1
AMD,o4rgeyb,"Its not ad hominem. Maybe you should look up the definition of ignorant.  the GN videos of the RAM market has been riddled with errors and outright conspiracies that are clearly not true if you did any research into whats going on and know the basics of economics. They have mislead you, and now you are repeating the ignorance.",hardware,2026-02-11 07:32:04,2
AMD,o4nddgd,"Its quite confusing, techpowerup states that both rdna3 and 4 have matrix cores, yet call rdna4's 3rd gen ai accelerators, so what happened to rdna2's?  Amd themselves call rdna4's 2nd gen ai accelerators, implying that rdna2 doesnt have, but rdna3 does",hardware,2026-02-10 17:30:50,1
AMD,o4oib7o,"A core in this sense is an ALU dedicated to a specific function/instruction.Â  RDNA4 may have 2 or 3 SM dedicated to just these tasks, or they could have bolted on enough transistors that each SM can now do graphics and AI with great speed coherently. Either way it's now a true ""core"".Â    RDNA3 absolutely does not do this. Which is why the AI abilities are barely used. The graphics ALU needs to find time to execute the instructions. So it's not really an AI core.",hardware,2026-02-10 20:39:42,1
AMD,o43p8vg,"To my eyes that's the same image. That's the only part that matters.   If my GPU could render a 1px by 1px black square and then transform that into the appropriate game image, *I don't care* that it's not native rendered. There is no such thing as a ""real"" frame. They're all fake.",hardware,2026-02-07 16:29:01,13
AMD,o43vns2,"LODs however can be readily observed, itâ€™s not â€œthe same taskâ€ itâ€™s quite literally a different task, LESS things are rendered, just like less resolution is in the final buffer. Bad LODs are just like bad upscaling, and good LODs are just like good upscaling, a more efficient task for what ultimately brings you a better looking image with the same amount of hardware.   >	Those are actual rendering techniques to reduce the amount of work happening on both the CPU and the GPU.  >	In contrast, upscaling doesnâ€™t do any of that  Are you even listening to yourself? A lower resolution final buffer = less work happening on the GPU.  Next you are going to say MSAA wasnâ€™t an optimization and all games should have been super sampled.",hardware,2026-02-07 17:00:27,9
AMD,o5bro9x,"ok, then your upvoters misunderstood it as well.",hardware,2026-02-14 11:52:26,1
AMD,o5gt1qm,"20xx series didn't sell well and Jensen was widely mocked for pleading with the 1080Ti owners to upgrade during his 30xx series reveal.  DLSS3Â was mostly about FG, the superres. model was not that improved. I play on 55'' 4k TV as a monitor and DLSS3 quality was the best of upscalers, but still had obvious flaws that would not have been noticeable on smaller screens.     \>are you saying AMD is going to have better RT hardware in the future?     What I meant was different and related to the below post of mine.  [https://www.reddit.com/r/hardware/comments/1e8sri7/leaked\_rdna\_4\_features\_suggest\_amd\_drive\_to\_catch/lea62vy/](https://www.reddit.com/r/hardware/comments/1e8sri7/leaked_rdna_4_features_suggest_amd_drive_to_catch/lea62vy/)     Essentially, while AMD's RT hardware is slower, it will not become slower infinitely against nvidia as the RT load keeps getting heavier.     I had both 6800XT and 3090 at one point, and the latter was 3-3.5x of former in heavy RT games. But then path-tracing updates were done in Cyberpunk/Portal and ratio became absurd like 20:1.       This was obviously due to these games being designed for nvidia hardware, and we could get similar situation in the future where games are instead optimized for AMD hardware in consoles.",hardware,2026-02-15 06:12:04,1
AMD,o4y4eg1,Its clear you do not argue in good faith as an AMD owner you should have been clearly aware of the issues in the first place as they happened.,hardware,2026-02-12 08:03:32,1
AMD,o4rkn6r,"Ah, my mistake in reading your statement of my ignorance was benevolent and I failed to recognize your inate omnipotence. Silly me for not kissing your feet, o ye of divine haughtiness.  -  If I may,  I'd like to point out that you've neglected to provide specific examples of these errors or conspiracies, or what principles of economics that tech Jesus has botched. So far, I'm getting 'because vibes' and,  until better persuaded by objective information,  I'm going to dismiss your claims with the same level of effort (~0).",hardware,2026-02-11 08:11:48,1
AMD,o4ne0j7,RDNA2 doesn't have ML. It's just INT8 and INT4 instructions.   RDNA3 has limited WMMA instructions and dual issue FP16.   RDNA4 is the first architecture that matches 40 series in on paper specs per CU/SM. Massively boosted IN8/FP8 and INT4 perf + sparsity support.  Yeah it's very confusing.,hardware,2026-02-10 17:33:49,1
AMD,o43rta0,"> To my eyes that's the same image. That's the only part that matters.   And yet that doesn't magically make upscaling any more an ""optimization"".  > If my GPU could render a 1px by 1px black square and then transform that into the appropriate game image, I don't care that it's not native rendered. There is no such thing as a ""real"" frame. They're all fake.  There are such a thing ~ real frames are those directly rendered by the engine itself. Just saying ""they're all fake"" is a copout to justify your nonsensical logic.  None of it changes the fact that game developers are relying far too much on them to make up for extremely awful and basically non-existent optimization of their games and engines.",hardware,2026-02-07 16:41:36,-1
AMD,o47a1i2,"> LODs however can be readily observed, itâ€™s not â€œthe same taskâ€ itâ€™s quite literally a different task, LESS things are rendered, just like less resolution is in the final buffer. Bad LODs are just like bad upscaling, and good LODs are just like good upscaling, a more efficient task for what ultimately brings you a better looking image with the same amount of hardware.  Bad LODs are not like ""bad upscaling"" ~ bad LODs are the fault of the developer. Bad upscaling is the result of AMD, Intel or Nvidia's driver-provided solution being awful.  You are just deliberately mixing up different things at this point.  > Are you even listening to yourself? A lower resolution final buffer = less work happening on the GPU.  That's just a hack. You would get the same result by just lowering the native resolution. That's not ""optimization"".  > Next you are going to say MSAA wasnâ€™t an optimization and all games should have been super sampled.  MSAA is entirely different ~ edges are being sampled and scaled, which has a hit on performance.  None of these techniques are the same as upscaling the final buffer ~ they happen before the final buffer is composed.  All in the service of justifying the garbage that is upscaling as an excuse for not optimizing a game.  So many games need upscaling just to have not-garbage performance ~ they're rendering to a lower resolution, which the external upscaling solution then upscales. That's just a big hack, because they apparently don't know how to actually optimize code these days.",hardware,2026-02-08 04:40:35,-1
AMD,o5cws2f,"No they just knew that saying ""this sub leans Nvidia"" is just laughable. Like you are. Take your AI GPU tech and get in the sea with it.  Maybe Jensen will give you an extra 10 mins on the AI cloud machine if you keep this up, good job.",hardware,2026-02-14 16:09:15,1
AMD,o5gvuqi,20xx series sold well and so did 30xx series and so did 40xx series and so are 50xx series.  DLSS3 was when upscaler got really good. FG is just a cherry on top.  >This was obviously due to these games being designed for nvidia hardware  Obviuosly this is not true. Its simply that AMD RT does not use dedicated hardware but rather WMMA instruction set on regular cores so the heavier RT requirements the worse overall performance as you are stealing from regular cores to do your RT.  Its the same when people claimed Witcher 3 is optimized for Nvidia when the reality was simply that AMD was shit at tesselation. In every game.,hardware,2026-02-15 06:37:31,1
AMD,o4yvfkm,"Then you do not understand what good faith means.  You claim stuff on HOW certain bugs work, not just that they exist. That means that there is someone, who has to have analysed those bugs. This is anything but a trivial claim.",hardware,2026-02-12 12:12:35,1
AMD,o4w58iq,FYI not all 9800X3D's can run a +200Mhz offset.,hardware,2026-02-11 23:45:47,165
AMD,o4w29lq,No shit?  If you needed this article to tell you you can close a 2% performance gap by overclocking a little....you need help,hardware,2026-02-11 23:28:57,81
AMD,o4y7u7m,Tomâ€™s AI finally discovers the secret art of overclocking.,hardware,2026-02-12 08:37:24,4
AMD,o4vycm4,"TLDR:    ""We retested the Ryzen 7 9800X3D and 9850X3D with PBO turned on and a positive 200MHz boost clock override for both CPUs, to see how that impacted gaming performance, efficiency, and clock speeds. In short, the Ryzen 7 9800X3D can make up the thin margin between it and the Ryzen 7 9850X3D. AMDâ€™s latest chip, however, has little to gain from even more clock speed, at least in games...Especially below $450, itâ€™s hard to justify the Ryzen 7 9850X3D over the 9800X3D when the latter offers almost identical performance with PBO enabled. And, although the Ryzen 7 9850X3D can climb higher, that extra clock speed doesnâ€™t amount to much in games â€” in our suite, it amounts to 0.9%.""",hardware,2026-02-11 23:07:47,16
AMD,o56kgk9,The best use case of the 9850 is to undervolt them because of the better bins.   9800 performance with lower power and heat.,hardware,2026-02-13 15:52:38,3
AMD,o50lry8,Cause ya idiots canâ€™t keep it under 60c while testing. Put it on a decent water cooling setup or direct die. And itâ€™ll utilize those extra clock headroom. 9xxx series chips start to down lock after 60c. Literally every damn chip.,hardware,2026-02-12 17:40:11,2
AMD,o4xggyu,"The 9850X3D is a 9800X3D with worse leakage characteristics.   That's why it runs hotter, uses higher Vcore and consumes more power than the 9800X3D, all for a measly 125-150 MHz bump in all-core frequency in a workload like Blender, as tested by Gamers Nexus.",hardware,2026-02-12 04:37:38,1
AMD,o4vxiuj,"Hello Antonis_32! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-02-11 23:03:25,1
AMD,o5cyy2a,"no sh\*t - 9850x3D is literally binned 9800x3D - you pay for that, not all 9800x3D's will reach it.",hardware,2026-02-14 16:20:09,1
AMD,o4vz56z,"Arenâ€™t the 9850X3D chips just lower binned 9800X3D chips that have a higher factory OC, as shown with the higher power consumption and thermals in many reviews?",hardware,2026-02-11 23:12:03,-12
AMD,o4w19gm,"*\*me watching a rigged game over the fence from the sideline, refusing to pay quadruple\**",hardware,2026-02-11 23:23:25,-7
AMD,o4xiwgt,"AMD been refreshing CPUs since 7000 series and people still pandering to them: ""in socket upgrade""  Turns out Intel was just as upgradeable in the end, since AMD isn't actually making anything new",hardware,2026-02-12 04:55:44,-6
AMD,o4ydyu0,Itâ€™s most likely an update to prevent asus and asrock from frying your chip from default mobo settings overvolting.,hardware,2026-02-12 09:38:17,-1
AMD,o4wfcpz,AMD and disappointing product launch never fail to disappoint.,hardware,2026-02-12 00:44:17,-13
AMD,o4x66gb,"The timeless classic - ""just overclock (tier low product)""",hardware,2026-02-12 03:26:58,85
AMD,o4y2cvb,No they don't I have bad silicone and max I can set is on my 9800x3D is +100mhz and -15 CO all cores and I was hoping I can but I can't!,hardware,2026-02-12 07:44:04,2
AMD,o4z31fj,so far my 10850k less glitchy and faster in bf6. gotta tinker,hardware,2026-02-12 13:04:40,1
AMD,o4xgqh7,The problem is 9850x3d doesnâ€™t gain anything with overclocking. So the new model is just pre-overclocked old model that you can buy for more money.,hardware,2026-02-12 04:39:36,15
AMD,o4yjbve,"Its like many things, bigger number than your friends so you can brag.",hardware,2026-02-12 10:29:53,1
AMD,o4zgz3p,Doesn't reading an article that tells you something you don't know qualify as getting help for the problem of not knowing it?,hardware,2026-02-12 14:23:53,1
AMD,o4w3ywy,">in our suite, it amounts to 0.9%.  Unless I'm mistaken, they got the math wrong? Shouldn't it be 1.5%? Either way it's probably more important to look at the 1% lows as some of those games are mostly GPU limited. Though I don't know how good their testing is since the 9850X3D came out ahead by 18% in 1% lows in CS2 :/",hardware,2026-02-11 23:38:33,17
AMD,o6a0yjt,"9850x3d over 9800x3d only makes sense if you are upgrading from ddr4 build to a ddr5 build. Like, you are likely spaffing a huge amount of money up a wall anyways, so an extra Â£50 on a Â£3+k build for an extra 2% uplift isn't terrible.",hardware,2026-02-19 17:52:30,1
AMD,o4xmgvb,"Weâ€™d want a pretty broad sample to statistically verify that. What the new CPU does do is run faster out of the box.  We all know that AMD was already near the top of their efficiency curve, so the 9850X3D is simply validated to be pushed a little further - with the accompanying costs.",hardware,2026-02-12 05:23:09,16
AMD,o4xvh2y,"The days when higher leakage chips clocked better than low leakage chips are past. That hasn't been a thing for ambient temperatures since 45nm. If you look at the binning statistics for the 8086K to the 14900KS they were all lower leakage than their regular counterparts. Even under LN2, the best performers haven't been high leakage since 22nm and the 4770K (the 4790K was lower leakage and clocked worse under LN2).  The 9850X3D runs hotter because it has a higher frequency target. If you were to set the 9850X3D to have the same 5225 MHz target as a 9800X3D, it will consume less power. These chips are about 1 GHz higher in clock speeds than their efficiency window.",hardware,2026-02-12 06:40:13,7
AMD,o50e1kl,"So in short, it's a marginally faster CPU for those choosing the highest end parts and cooling, but who also don't wish to tweak PBO and want it to just work.",hardware,2026-02-12 17:03:25,1
AMD,o4wtc1l,Never happen. They probably only released this because they have enough supply of chips that can hit these targets.  AMD likely has no new consumer desktop chips for the rest of the year.,hardware,2026-02-12 02:09:08,18
AMD,o4xmx3n,Worthwhile for what?,hardware,2026-02-12 05:26:49,2
AMD,o4w3wtr,Wouldnâ€™t it be higher binned? The only reason they can even hit those thermals and power consumption while maintaining the higher clocks (even after 9800X3D has been over clocked) is because they have slightly better silicon.,hardware,2026-02-11 23:38:13,32
AMD,o4w3p0y,yes. it's just a factory sanctioned OC like you said.,hardware,2026-02-11 23:36:59,7
AMD,o4xmtuu,"Not really, but most of that rhetoric came from how far behind AMD started out with AM4, so they got a lot of mileage out of the socket while they worked to catch up.  For Intel, theyâ€™ve switched back and forth on new implementations as well as famously struggled with their fabs, which has led to some erratic releases.",hardware,2026-02-12 05:26:05,3
AMD,o4wz2u4,Are you ok? This is about a consumer CPU. NVIDIA is not in that market.,hardware,2026-02-12 02:43:15,7
AMD,o4wz2gb,"Wait, Nvidia is selling x86 CPUs now?",hardware,2026-02-12 02:43:12,5
AMD,o4xtwmy,"Just win the silicon lottery, have the OC knowledge and possibly purchase a tier higher motherboard with sufficient OC support, to run a lower tier cpu above its design while also consuming more. It is trivial for random Joe.",hardware,2026-02-12 06:26:05,54
AMD,o4yreaz,5070ti overclocked is as good as a 4090 is my personal fav.,hardware,2026-02-12 11:41:18,3
AMD,o4yh0tp,"I have a gigabyte b850 aourus elite, I am running -30 all core and +200 MHz.   I might try -35 but I have never seen anyone go that low, I also thought that each chip has a different voltage-frequency curve so my -30 might not be the same as someone else -30 which farther complicates things (could be totally wrong here)",hardware,2026-02-12 10:07:52,1
AMD,o5b9xqz,"-15 is pretty normal for all cores.  After a point, it gets annoying to set popular guides say ""almost every 9800X3D can hit -30, you are likely to go even lower"" and never update their guides after so long, constantly confusing inexperienced people.  Won't be surprised if 99% of those -30 CO 9800X3D chips can't even pass 10 seconds of proper validation.  If anyone's can pass a full suite of proper, multi-day validation, that's truly a golden chip.",hardware,2026-02-14 09:01:26,1
AMD,o4xgve4,Yeh it's basically maxed out from factory,hardware,2026-02-12 04:40:37,11
AMD,o4xm47n,Thatâ€™s not a negative IMO. Kind of like the X versus non-X SKUs AMD has.,hardware,2026-02-12 05:20:21,12
AMD,o51dcyi,"I wouldn't say that's a *problem*. I think what the 9850X was was self evident. But more importantly, it resets MSRP and raises prices of the same chip by 12%",hardware,2026-02-12 19:49:50,1
AMD,o4xo35d,"Yes. The cost in this case being higher leakage.  Higher leakage also lets you bin for higher frequency, but being worse at everything else.  Like the 9850X3D.",hardware,2026-02-12 05:36:19,2
AMD,o4xvowt,>The days when higher leakage chips clocked better than low leakage chips are past.  Tiger Lake H35 says hi.,hardware,2026-02-12 06:42:10,0
AMD,o4xccb7,How often does AMD release these type of gamer CPUs? Every 2 years?,hardware,2026-02-12 04:08:08,1
AMD,o4wyyd8,I donâ€™t even know if theyâ€™re better binned. My 9850x3d runs at absurd voltages out of the box compared to my 9800x3d.,hardware,2026-02-12 02:42:32,3
AMD,o4y2mtp,I have MSI B850m Mortar WiFI and max I can set on my 9800x3D is +100mhz and CO -15 all cores which is max  stable so mobo may help but not always!,hardware,2026-02-12 07:46:43,4
AMD,o54uobd,"> win the silicon lottery, have the OC knowledge and possibly purchase a tier higher motherboard with sufficient OC support  Nah, you just need a good sample that has enough voltage headroom to reach +200. Requires no complicated knowledge. If the silicon quality is not there, mobos and coolers can't do much to help in that regard, due to the way PBO works.",hardware,2026-02-13 09:01:23,1
AMD,o4yittp,It's called silicone lottery and I bit lost it! And about Curve Optimizer from my experience while on R7 7700 this CPU took all I throwed at it +200 CO -30 All cores + -0.500mv Offset on CPU which let my CPU boost all cores 5500mhz while unlocked PBO 160W and while at stock 90W hit 5350mhz full load and in CB r26 beat 7700x at 140W hitting 4885pts and when unlocked it hit 5063pts! But CO -30 All Cores was max I could set before IMC aka Memory Controller was crashing even I could set -125 All Cores so to hit more then CO -30 stable you need Hyper Duper Platinum binned silicone! And if you are stable at CO -30 then you won silicone lottery and you should be easy to set FCLK at 2200mhz which is biggest gains for system responsiveness you can set quite easy! [https://drive.google.com/file/d/12HVXcx1qXOXAhZQNqLUuIazKfmT0pKJU/view?usp=sharing](https://drive.google.com/file/d/12HVXcx1qXOXAhZQNqLUuIazKfmT0pKJU/view?usp=sharing),hardware,2026-02-12 10:25:07,3
AMD,o4yi2wn,Ahh man.. pretty sure I still have my x470 Aorus that ran my 3700x.,hardware,2026-02-12 10:17:56,1
AMD,o4yj76h,"Do you actually hit that frequency and are you actually stable? I have serious doubts that you are. If you are, well done.",hardware,2026-02-12 10:28:39,1
AMD,o5bmd0v,">\-15 is pretty normal for all cores.  Well this could be right for 9800x3D CCD but we don't know what quality of silicone was used in 9950x3D! If 9950x3D CCD behave better then 9850x3D in Curve Optimizer or same we will know much more about it potential!  Ps My previous R7 7700 took everything I throwed at it! CO -30  All Cores +200mhz FCLK 2200mhz without messing with voltages to much! And with this settings 7700 at stock 90W beat 7700x 140W in CB r26 scoring 4885pts and when on the top of it I set manual CPU negative offset -0.500mv power draw dropped from 34W on idle to 26W exactly the same as Zen 5 which shows Zen 4 was overvolted and when I unlocked to 170W it hit 5063pts and all cores boost was 5.5ghz Hmmm  [https://drive.google.com/file/d/12HVXcx1qXOXAhZQNqLUuIazKfmT0pKJU/view?usp=sharing](https://drive.google.com/file/d/12HVXcx1qXOXAhZQNqLUuIazKfmT0pKJU/view?usp=sharing)  >constantly confusing inexperienced people.  Inexperienced people shouldn't touch anything until they read and learn that's how everyone who want to learn and want to know start! And they keep learning every day that's how it works!  But Idiots will always be Idiots talking shit thinking they know and you can read a lot of this shit over here!  >Won't be surprised if 99% of those -30 CO 9800X3D chips can't even pass 10 seconds of proper validation.    Exactly! And coming back to my 7700 I could set CO -125 All Cores and all booted I passed some CB r24 etc but anything above CO -30 brake IMC aka memory controller and anything touching ram was crashing even if I set one core CO -31 so am reading and laughing at people saying they set CO -40 XD  >If anyone's can pass a full suite of proper, multi-day validation, that's truly a golden chip.  Well you can be hardcore about testing but I found if you mess with OC and Curve Optimizer and FCLK easiest way to validate is to use CB r23 30 test as it test AVX2 instructions and CB r26 because it test AVX and Infinity Fabric and other instructions while listening to youtube and if you are unstable it will crash almost instantly and second mark is about 8-9 minute and last is between 18-20 minute and if it pass and your sound is not crackling or completely gone it will be stable 24/7!! I know its kinda short but I didn't found any issues and I use my PC for more then 12h a day!",hardware,2026-02-14 11:03:33,1
AMD,o4xokyi,"Makes me wonder if the voltage curves are the same and the new CPUs just push a little harder, or they really (at a statistically significant scale) are also leakier.",hardware,2026-02-12 05:40:22,1
AMD,o4xw4b2,"Do you mean that Tiger Lake clocked higher than Ice Lake because they had higher leakage? Tiger Lake H35 consumed way less power than Ice Lake at identical clock speeds, mostly due to the better match of architecture and process node.",hardware,2026-02-12 06:46:02,3
AMD,o4xdzgi,"Whenever they release a new series:  * 7000 series had 7800X3D, 7900X3D and 7950X3D * 9000 series has 9800X3D, 9900X3D, 9950X3D, and 9850X3D (refresh of 9800X3D) with rumours of a dual X3D 9850(X3D2?) being a possible release.  So whenever they release Zen 6 and name that series (11000?) will have new X3D chips and a chance to increase the cache size. Odds are the way things are going that Zen 6 either releases very late this year, or most likely early next year.  /note: there is still a chance for more X3D chips in the 9000 series like a 9600X3D. AMD could have some rejects from the X3D chips that they are waiting to build inventory on before release, but that is all they are is rejects from higher tier chips, not real new stuff.",hardware,2026-02-12 04:19:34,7
AMD,o4yk4e4,"On my MSI X870 Tomahawk I can set my 9800X3D to +200, but I can't undervolt at the same time.   And for undervolting without overclocking, I can only set -20, while many YT-people set -30.  So yeah my 9800X3D is about average.",hardware,2026-02-12 10:37:25,1
AMD,o4yj1z0,"I might have gotten super lucky and won the lottery on my GPU as well I can undervolt it pretty low and still get all of the performance with a good 400+ gain in clocks   I didn't want to mess around too much with memory timings, or other voltage stuff since I keep seeing 9800x3d have unknown issues and dying.   I already have cl30 memory from gskill at 6000 I don't really know how much I'll even notice if I can get it to 6400",hardware,2026-02-12 10:27:15,2
AMD,o4yibu4,It's my first time getting a gigabyte motherboard so far so good,hardware,2026-02-12 10:20:18,1
AMD,o4yki0z,"Yup, I was pretty surprised as well. I have run all the usual benchmarks(OCCT, Y-Cruncher, P95, cenibench) it boosts to 5450 but usually comes down to something in 5425ish and stays there.   The only crash I have had in about 2 months was space marine 2 but that turned out to be an overzealous OC on my GPU.   I was going to do per core tuning but see no real issues I haven't needed to do that",hardware,2026-02-12 10:40:57,1
AMD,o4xx4s1,"I'm talking about Tiger Lake H35 vs regular Tiger Lake.   The former is quad-core only.  The SKU names are 11370/11375H for the former(H35) vs the 1185G7/1195G7 for the latter(UP3).  Identical max turbo clocks between the two types, but the former has worse V-f curves than the latter.  I own laptops with both of them so I know it first-hand.",hardware,2026-02-12 06:55:07,2
AMD,o4ydkwt,>So whenever they release Zen 6 and name that series (11000?) will have new X3D chips  It's probably gonna be Ryzen AI 7 580X3D or something like that.,hardware,2026-02-12 09:34:21,2
AMD,o4yksf2,Welcome in the club :D So now best for you is stabilize like +100mhz or +125mhz with -15 or something and try to set FCLK 2200 and this will give you gains!,hardware,2026-02-12 10:43:36,1
AMD,o55knu6,Mine works with +200 and -35 all core. Skalar to 10. Mostly playing games,hardware,2026-02-13 12:41:30,1
AMD,o50yn15,\**silicon* lottery  The **silicone** lottery is at the strip club lol,hardware,2026-02-12 18:39:51,2
AMD,o4yjw72,If its fully stable then yes you won silicone lottery! And if you are on 50 series then that's normal you can set low sub 800mv and on my Palit 4070ti Super lowest possible is 925mv with 2700mhz-2760mhz stable + 2000mhz on memory but I flashed unlocked BIOS from Colourful Vulcan OC 330W and it take OC like a champ which gives me 3000mhz Core +2000mhz mem and its +-10% gain!,hardware,2026-02-12 10:35:14,1
AMD,o4ynfkt,Well me to have G.Skills 6000c30/38/38/96 but I tighten timings c26/34/33/42 which I found give me best gains compared to 6400 with loose timings!   If you want here are my tuned timings [https://drive.google.com/file/d/1j7UOYTg68o2PZSF0zhVdQvpma\_QrY3mo/view?usp=sharing](https://drive.google.com/file/d/1j7UOYTg68o2PZSF0zhVdQvpma_QrY3mo/view?usp=sharing)  And to test what gains you get from memory use Benchmate [https://www.benchmate.org/](https://www.benchmate.org/) and two things PYPrime 2b this is latency test and with my timings I see 7.125s normally and lowest I see was 7.080s and second one is Super PI 32m which test whole memory and throughput and my lowest score was 5 min 49.700s!,hardware,2026-02-12 11:07:29,1
AMD,o4yitt3,May it serve you well.,hardware,2026-02-12 10:25:07,1
AMD,o4ymd3f,Im afraid if you ran CoreCycler you would discover perhaps one or two cores being stable at -30CO but not all of them.   https://github.com/sp00n/CoreCycler,hardware,2026-02-12 10:57:54,1
AMD,o5baoch,"Kinda curious, have you tried AIDA64?  That one specifically is great for validating errors from too low CO, my understanding is specific instruction sets are more prone to errors with too low CO settings, but if you're never gonna use those, some just live with it knowing it can fail AIDA64 within seconds.  Never tested that claim myself though since I wanted mine to be stable at all times in all circumstances.",hardware,2026-02-14 09:08:45,1
AMD,o4xzcmx,"And you're completely certain the 11370H is binned with the exact same criteria as the 1185G7? Because from what I can see, the 11370H has a higher TDP target, meaning it could easily have been binned after the 1185G7, and Intel just allowed it to be binned worse.  Besides, it's not even clocking higher. If your theory were to be correct, the 11390H should be targeting a higher peak boost than the 1195G7. By higher peak boost, I mean something like 5.2 GHz, or even 5.3 GHz (ludicrous for Tiger Lake).",hardware,2026-02-12 07:15:26,5
AMD,o4yovhn,">this will give you gains!  * on a benchmark tool that is completely irrelevant for 99.999% of real world usage  (Snark aside, overclocking is super cool, but I don't want people seeing this and fucking up their PC because they don't know what they're doing and just think ""free performance!"")",hardware,2026-02-12 11:20:02,1
AMD,o5744qw,Thats a golden sample.,hardware,2026-02-13 17:26:29,1
AMD,o5h79jk,Mine does about the same on a Aorus b650 elite. Purrs like a kitten but it is dumb to run it like that daily.,hardware,2026-02-15 08:26:43,1
AMD,o5w1vrp,Never had an AMD CPU that was stable below -10 all core.       Every single one would become instable at low loads/idle if every core wasn't dialed individually at that point.,hardware,2026-02-17 16:33:59,1
AMD,o4yl04c,"I have a 5070ti i have 2 profile set up one can boast to 3000 at 875 I think (I might need to double check) the other is 850mv and 2900.   Both run fine for most games at +2000mhz memory overclock, but I have had issues with space marine 2 and god of war Ragnarok where I had to reduce just the memory down to +1500  I worried about messing with power because power is very expensive where I am, I live in a very warm environment and the connector loves to burn",hardware,2026-02-12 10:45:33,1
AMD,o4yptum,What are the gains in more real world scenarios?   I found memory timing tuning to be not the best in terms of time investment for me because brain smol. So I kinda just skipped it,hardware,2026-02-12 11:28:08,2
AMD,o4yixen,Thank you :),hardware,2026-02-12 10:26:05,1
AMD,o4yplgi,"Anything particular with this benchmark? As in why would this crash when y-cruncher etc. doesn't, not being passive aggressive genuinely want to know.   Stability is more important for me than the ultimate big number",hardware,2026-02-12 11:26:10,2
AMD,o5bbpy6,"No I didn't test it using AIDA64, Honestly it's stable for most of the things I do right now including fitting it with several heavily paralleled and single threaded tasks (I am unfortunately a programmer).   At this point I am mostly past the messing about with stress test/ benchmarking phase, I'll probably retune things in the summer for better heat management ( I live in a very hot climate so having a extreme summer fan/power profile helps and I built this computer in the ""winter"" ) but I am mostly happy with the performance where it's at.",hardware,2026-02-14 09:19:12,1
AMD,o4y8ao2,"No, the UP3 (1185G7) and the H35(11370H) aren't binned the same.   The H35, intended for gaming laptops, also ended up in thin and light systems with cTDP tuned down, but the boost clocks being the same as UP3.  I own such systems.  The H35 always have worse voltage, temperature and power consumption, even after setting identical power limits set through utilities like Throttlestop.",hardware,2026-02-12 08:41:55,1
AMD,o4yygy6,Its clearly not for you then!,hardware,2026-02-12 12:34:19,1
AMD,o5h7ek8,Why is it dumb,hardware,2026-02-15 08:28:04,1
AMD,o60fh5i,"before the 9800x3d, I had a 7800x3d -32 all core stable with no problems. Maybe Im just lucky",hardware,2026-02-18 06:48:38,1
AMD,o4ym98s,This clocks and voltages on 50 series are pure joy you know :)   And yest its smart move to have multiple profiles for games especially for RT games! Tbh I play on 4k TV 60hz so I lock at 60fps + DLSSQ + 925mv - 2500mhz gives me best Power Draw to Performance around +-150W as I try to keep it under 200W because if I have flat 60fps am happy,hardware,2026-02-12 10:56:56,1
AMD,o4zug8e,Well am not gonna write you elaborate ðŸ˜… so I will try to explain what I ment by gains in simple words! ðŸ˜   Look when I said about latency test in PYPrime 2b this test measures how fast Memory exchange data with CPU so theoretically faster ram 6400+ should give you faster data transfer but unfortunately UDIMM's with higher clocks are losing timings and while you do that you're latency growing so as example I use standard EXPO 6000c30/38/38/96 and in AIDA Latency test it get 75.3ns and after all my tuning on my old R7 7700 I get 57.6ns almost 20ns fastery Ram exchanged data with CPU and because of this I get uplift in games especially in 1% Low but Biggest gains I felt and feel is in System Responsiveness like apps like browser file explorer and other apps like these started opening immediately! You click and app is open and ready to go + feeling of mouse how smooth suddenly become! Even likes of Steam opening visible quicker!Â    But having 9800x3D gives you uplift because of extra 64mb cache and this cache too exchanging data with memory so when your extra cache can exchange data faster you get more performance all for free!!   So if you have any more questions and I can try to answer you ðŸ˜,hardware,2026-02-12 15:32:03,1
AMD,o4z31l3,No i understand. I believe everything is explained in the description of the project on github much better than i could explain it.,hardware,2026-02-12 13:04:42,1
AMD,o5bcb5n,"Gotcha, ultimately you do you.  If you are programming though, I do recommend getting AIDA64, just plug in one of the codes you find on Reddit and give it an hour long spin while you eat or something.  Most instabilities are found within seconds to minutes, so it's pretty fast.  Don't want any errors with your programming work that is caused by unstable hardware, so I'll recommend giving it a try when you feel like it one day, though again, plenty of people just live with it.",hardware,2026-02-14 09:25:05,1
AMD,o62dhi7,"Running it too far in the edge daily is dumb, because CPUs degrade over time.    It can run well today, but in a week or in a year, especially after updates, it might not run stable anymore. Such errors are hard to detect, if it happens.   It might also degrade faster when running it on the edge.",hardware,2026-02-18 15:15:46,1
AMD,o60n3sc,Interesting because before 9800x3D I was suing 7700 which  seems Golden One because I was able to set +200mhz + FCLK 2200 + CO -30 All Cores Per Core and it took it as a champ but if I set even single core at CO -31 IMC aka Memory Controller inside CPU stared to crash even I could set like -125! System was booting I was able to pass some testes but IMC no no! And with this settings +200 CO -30 at stock 90W 7700 beat 7700x 140W in CB r26 scoring 4885pts and hitting 5hgz all cores!  But real gains I seen when set negative offset on CPU -0.500mv on the top of CO -30! Right away my 7700 dropped idle power draw from 34W-36W to 25W-26W exactly like 9700x-9800x3D and when I unlocked it to 170W it score 5063pts in CB r26 and all cores boost was 5.5ghz!      So my conclusion is does your IMC work or scram errors!?,hardware,2026-02-18 07:57:09,1
AMD,o4yohzs,My monitor is 200hz 1449 but I have it capped at 144 for similar reasons. I usually use DLAA which limits max FPS for me   I am waiting for the summers to really hit (it gets up to 45C at times here) to kinda do a second pass at all the settings balancing power and heat,hardware,2026-02-12 11:16:51,1
AMD,o671d1i,"Im using my PC like 6 hours a week. I really dont care about degradation. If it fails in like 4 years, I\`ll buy a new one",hardware,2026-02-19 05:41:18,1
AMD,o4zwn9r,You know that playing with DLAA you use full power and DLSS reduce power draw about +-30W but as we can see 50 series undervolt and OC better so as long you happy and all works than who cares ðŸ¤·ðŸ»â€â™€ï¸ðŸ˜ŽâœŒï¸ðŸ‘‰,hardware,2026-02-12 15:42:34,2
AMD,o67yswc,It's more efficient to do this for smaller igpus. Any rt upgrades from rdna4 is wasted area because none of these will run rt at any acceptable perf in real use. You'd rather cut the area for lower cost or increase the wgp count and even with those igpus are still bottlenecked by bandwidth at ~16 cu which covers most consumer segments  Benchmarks ain't gonna like this but it's probably the right thing to do for real use. Consumer markets are goin to shit due to ram prices so enterprise mobile that can withstand the ram shortage better with greater margins and less gaming focus is where shares are gonna be gained  In the end the top igpu pantherlake turned out to be a benchmark pleaser for gamers but not practical for the current market. The rampocalypse that shifts market momentum away from premium gaming to cost focused and enterprise systems is gonna be a coincidental boon for amd's mobile sales.,hardware,2026-02-19 10:47:06,22
AMD,o67juvt,"Makes sense, RDNA 4 was designed to only scale down to 32CUs. Backporting some of the key features to RDNA 3.5 is probably easier than redesigning RDNA 4.",hardware,2026-02-19 08:22:15,11
AMD,o69tar1,"It's a common misunderstanding in ""enthusiast"" forums - that the ""Later Generation"" is just *better* at everything in every way.  For example, if you cut down a zen4 to hit a lower power and area target, at a known acceptable loss of peak performance, it'll probably look a *lot* like a tweaked zen3. So why not just tweak the zen3?  Not every design decision that went into making RDNA4 from RDNA3 likely makes sense for smaller form factors, power and performance targets, or even different environments (like different vram/bandwidth ratios). It may be that backporting the things that *do* make sense is simpler than cutting half of what you just added out of the design again.",hardware,2026-02-19 17:15:37,14
AMD,o6aucvm,> none of these will run rt at any acceptable perf in real use  There are lower RT loads like Lumite's optional use of hardware RT.,hardware,2026-02-19 20:12:05,5
AMD,o69p0up,"yeah while i want the day where apu have good RT performance to be here, it aint anywhere close. giving die area for ML stuff for upscaling and raster for the rest of the games while still having somewhat usable RT for optimized games like TDA is the mission atm",hardware,2026-02-19 16:55:08,3
AMD,o6dqord,"rt in general is a waste. because even now. games are largely really baked in. most effects are either baked in, or approximated anyway with denoising. not to mention it made devs a lot lazier, rely on engine, and now every game looks the same and poorly optimized. a lot of games that should have a distinct lighting and artstyle are now photorealistic because its a lot cheaper than baking in lighting.",hardware,2026-02-20 06:50:21,0
AMD,o69mzm8,"Saves validation time. Frankly, these fuckers exist, for the most part, in case your GPU died or you need to install the drivers for it, so work to validate advanced features is a waste of effort.",hardware,2026-02-19 16:45:31,-3
AMD,o67v7zp,>RDNA 4 was designed to only scale down to 32CUs.  Where did you learn this?,hardware,2026-02-19 10:14:00,28
AMD,o67qbcz,The ISA has nothing to do with hardware designs.,hardware,2026-02-19 09:26:18,6
AMD,o69n4cl,"Not to mention, better to save that time to finish UDNA one and then do a new iGPU arch once that's working.",hardware,2026-02-19 16:46:09,-1
AMD,o6cw7ug,"But why would ya turn that on over higher base presets or a more consistent locked fps at 60? Someone running a midrange igpu would be targeting 1080p 60 raster, not 1080p 40 with rt on. The focus would be on upscaling support and raster capabilities, rt is a distant afterthought  And that's gaming. For enterprise laptops rt capabilities have almost 0 use, it's 99% wasted area. We know that both intel and amd are moving supplies to dc and enterprise. Both lip bu tan and lisa su said so in their most recent earnings call. Amd downplayed the impact of ram prices and said they were focusing on enterprise, intel blamed the consumer market and said they regretted supplying it and are shifting as much as they can to other markets  The story of the next 2years in consumer is kinda ""you're in luck if you can even afford midrange stuff"", forget about premium experiences that push the envelope. Was this really planned by amd? Probably not, they probably cut most rdna4 products to focus on a larger architectural reset with rdna5 and have a gap between rdna3.5 and rdna5 but they lucked out here because of the market conditions.",hardware,2026-02-20 03:02:13,3
AMD,o6a47p7,I game on my laptop Strix Point regularly without a dGPU. For eSports games it's not bad at all.,hardware,2026-02-19 18:07:50,7
AMD,o68dsym,"I think they looked at NAVI44 that has 32CUs and decided there's no lower end chip, because arch doesn't scale well below that?",hardware,2026-02-19 12:44:35,16
AMD,o67yu88,> The ISA has nothing to do with hardware designs.   It never ceases to amaze me how people will state things with complete confidence while being totally wrong.,hardware,2026-02-19 10:47:25,31
AMD,o67rrq6,"Not what they're saying though. First commenter is saying that for AMD it's cheaper to transform preexisting RDNA3.5 igpus into some sort of RDNA 4 lite with FSR4 support, compared to designing a 4WGP RDNA4/5 igpu from scratch while minimizing area usage.  No idea if that's true (the part about not scaling down below 32CUs is definitely sus) but doesn't sound crazy",hardware,2026-02-19 09:40:53,9
AMD,o68kyiw,You should look into the purpose of the decode stage in pipelining,hardware,2026-02-19 13:29:19,0
AMD,o6hma9q,People with iGPUs aren't going to be fussy about performance. They will just be happy if they game runs.,hardware,2026-02-20 20:46:04,2
AMD,o68929w,Please enlighten me how the RDNA 4 uArch inherently doesnâ€™t scale down from 16 WGPs? Which front or back-end limitations it has so that it can't scale.,hardware,2026-02-19 12:11:18,2
AMD,o68j3g4,Welcome to reddit,hardware,2026-02-19 13:18:16,-1
AMD,o67ty6l,"RDNA 3.5 WGP should be smaller than RDNA 4 WGP. From an area efficiency standpoint its preferable to add support for FP8 and INT8 than create a RDNA 4 based iGP from scratch, yes.  As for the scaling, the ISA is independent from hardware design. It can scale up or down as much as wanted, although with limits to what it can be achieved, of course.",hardware,2026-02-19 10:01:58,7
AMD,o68eyi1,"You were the one that made the extraordinary claim there, itâ€™s on you to provide extraordinary evidence to support â€œthe ISA has nothing to do with hardware design.â€  The burden is not on other people to prove the negative of your claim. Itâ€™s on you to prove the positive.",hardware,2026-02-19 12:52:15,18
AMD,o68gpjc,"The whitepaper, as much as the RDNA 4 ISA reference guide are freely available to anyone to read. If you encounter a single mention of the ISA/uArch not scaling down from 16WGPs or how the hardware design of Navi 44 is the minimum of RDNA 4, please feel free to share to everyone.  I'll give you a hint: The smallest working hardware implementation of RDNA 4 continues to be a WGP.",hardware,2026-02-19 13:03:25,11
AMD,o6a7ewl,"I agree that RDNA4 can theoretically scale down, but I completely disagree with the blanket statement that â€œISA has nothing to do with hardware designsâ€.  ISA can and does drive hardware designs decisions and vice-versa.",hardware,2026-02-19 18:22:38,9
AMD,o69tw3s,"""Smallest working implementation"" and ""Smallest *efficient* implementation"" can be pretty different things. You'll probably balance a number of things rather differently in different contexts, different cache amounts, different ratios of compute/bandwidth, different bus fanout widths etc.  And there's a *lot* of low-level optimization to get the area/power/clocks to hit targets that *are* layout specific - it's not just as easy as ""make NUM_WGP=1"" and actually get *competitive* results out the end.",hardware,2026-02-19 17:18:27,5
AMD,o68qosc,"It's because AMD doesn't do it, so obviously there is a technical reason why AMD is doing so",hardware,2026-02-19 14:01:50,0
AMD,o6aclt0,Or a business reason,hardware,2026-02-19 18:46:47,5
AMD,o4e5gt5,"This still remains one of the biggest marketing foot-guns I have ever witnessed.  The biggest benefactor of the FSR4 INT8 release would be the thousands of Handheld PCs where AMD have basically stalled out at RDNA 3.5. All the Steam Decks, Legion GOs, XBOX Allys are all stuck with FSR3 from the AMD camp.  I use it on my Legion GO (via an Optiscaler plugin for Bazzite/SteamOS) and it absolutely dunks all over FSR3.  AMD could really lose the handheld market in a single generation with Intel's B390 release.  Not to mention the fact that AMD users can get a better upscaler by substituting FSR3 for Intel XeSS is hilarious!",hardware,2026-02-09 06:33:35,406
AMD,o4e3kp8,"Considering how fast they put rdna2 on maintenance mode, i don't think they want to put resources in rdna3 which will go into maintenance mode in late 2027.",hardware,2026-02-09 06:17:26,117
AMD,o4fpmes,"I would kind of understand this move if they were focusing only in RDNA 4 and shifting everything into this architecture, but they're still selling and releasing new products with RDNA 2, 3 and 3.5.",hardware,2026-02-09 14:18:59,20
AMD,o4fj10f,"It's going to be bizarre when valve officially implements it for the steam machine launch, and it won't be officially supported on Windows",hardware,2026-02-09 13:40:54,26
AMD,o4ebkr6,"They can't even apply the argument that FSR4 would work bad on older cards now that Nvidia released DLSS4.5 for all RTX cards. Should have taken advantage of  being the 'underdogs' but missing out is their thing, they aren't  even competing on high end consumer end where there's only Nvidia",hardware,2026-02-09 07:28:45,82
AMD,o4f24zb,Why would anyone buy an AMD card going forward if you only get one gen out of their software? Surely they will do it.,hardware,2026-02-09 11:43:14,37
AMD,o4ebnmc,"Gamers will question... Sure - that somehow didn't bother Nvidia, despite far bigger performance hit on older architectures",hardware,2026-02-09 07:29:30,25
AMD,o4e2y3k,"What AMD should've done is actually put non-shader-based AI compute starting from RDNA2.  RTX 20 series' RT and upscaling performance had always been called into question, but that was back in 2018 and their first attempt at this. It came with hardware accelerated BVH traversal and all the fundamentals to stay relevant for a while.  RDNA2 was launched in 2020 fighting with Nvidia RTX 30 series, and by then they already had known that the competition launched some AI capable hardware.  Fast-forward to 2022 and RDNA3 still didn't come with dedicated AI compute hardware that doesn't weigh down shaders. It is only until early 2025 that they introduced RDNA 4 with FSR4 capable dedicated AI compute accelerators, 6 and a half years later, meanwhile their server accelerators had long had these (starting with CDNA 1). What a shame.",hardware,2026-02-09 06:12:07,62
AMD,o4htrh6,"If Nvidia can do it for their previous 3 generation RTX GPUs, then why can't AMD do it",hardware,2026-02-09 20:31:03,6
AMD,o4e8w55,"""Why would we remove an incentive for our customers to buy our newer products?"" - soulless, greed-consumed corporate suits",hardware,2026-02-09 07:04:00,33
AMD,o4lce8k,"Either they won't or they can't, either way it's not good.",hardware,2026-02-10 10:30:47,4
AMD,o4e14tg,I really want to support the Underdog and accept some flaws but with moves like that I want switch to the big player.  You can say what you want but Nvidia did a solid move by cutting off right from the beginning and still supporting the 2000 series cards even if not with all features the 5000 series have.  Switched from my 1070 to 5700XT which aged fine but my 7800 XT probably won't. Let's see how RDNA5 or the RTX 6000 cards will be but this time I will buy what gives me the bang for the buck.,hardware,2026-02-09 05:57:00,68
AMD,o4e5dbt,"9070xt was the ""buy 1st and wait until 5080ti super"" GPU for me. It doesn't seem like it is happening.",hardware,2026-02-09 06:32:45,12
AMD,o4et1b8,"AMD just can't stop themselves from shitting in own yard.. Everyone behind this decision should be fired ASAP. It's massively hurting brand loyalty and trust in future product feature support - especially that Nvidia just did this too with far bigger performance hit on INT8 GPUs.   Not to mention Steam Machine, handhelds and such - all are pretty much DOA - because FSR3 is just garbage.",hardware,2026-02-09 10:20:29,15
AMD,o4hm3d4,"Well, fuck AMD  My rx 7900 GRE will be the first and last AMD GPU I'll ever buy  I went to AMD due to how much NVIDIA was getting on my nerves with low VRAM amounts and high price-performance ratio  They managed to piss me off more than NVIDIA. I would rather pay too much for a GPU than get no proper support for my several hundred dollar investment",hardware,2026-02-09 19:52:06,6
AMD,o4izvvg,"I bought rx6800, for bargain price and looks like it would be my last card from amd. In my country amd are usually cheaper but yeah if that means I wouldn't have a chance to use new technologies like nvidia had, dropped support. I will look for some nvidia equivalnent",hardware,2026-02-10 00:08:29,3
AMD,o59zbv3,Fuck this. I'm never buying a radeon GPU ever again.,hardware,2026-02-14 02:40:54,3
AMD,o4f13bf,Never buy AMD unless you're just looking for Reddit karma,hardware,2026-02-09 11:34:22,8
AMD,o4g4a8r,They only care about out AI and printing money while they can.,hardware,2026-02-09 15:36:17,4
AMD,o4k3c6k,"Im still on 6800 xt, honestly cant justify upgrading to 9700 xt. The performance uplift is not that big + there is no game that I like to play that doesn't run very well on my card yet.",hardware,2026-02-10 04:02:30,2
AMD,o4s23po,AMD has become evil slowly but surely.,hardware,2026-02-11 10:54:38,2
AMD,o4trvb1,Simlple AMD for cpu and nvida for GPUs. unless your ok with getting a subpar fsr support for one gen on a AMD gpu,hardware,2026-02-11 16:51:08,2
AMD,o51p7s2,"I was gonna switch to AMD , but they're doing the same shit NVIDIA is. Just gonna upgrade my CPU / MOBO through some pre-owned channels as I don't want to give these awful companies anymore money.      They want us to not have hardware and be happy.   A life long nvidia user was about to switch over after hearing about the 9xxx series of GPUs, now I'm just not interested. I'm positive Nvidia is pulling the same shit since FSR3 FRAMEGEN works on RTX 2000/3000 cards.",hardware,2026-02-12 20:46:32,2
AMD,o578hhw,Also the expansion of hardware with fsr 4 will also help push more games to adopt.,hardware,2026-02-13 17:47:45,2
AMD,o4e6e0n,maybe it will come out with the Steam Machine?,hardware,2026-02-09 06:41:43,6
AMD,o4f4f0k,"Intel shat the bed so hard they gave AMD a golden opportunity, and they're trying so hard to squander it.",hardware,2026-02-09 12:01:33,3
AMD,o4f9980,To be fair wasn't the performance so bad it didn't provide a tangible benefit over native res?,hardware,2026-02-09 12:38:02,3
AMD,o4ep4t9,"Look AMD, you *do* know that if you don't *want* to be in the GPU business, you don't have to, right?  Really not helping with the constant suspicions that they're only doing it to keep the monopoly regulators off Nvidia's back here.",hardware,2026-02-09 09:42:08,2
AMD,o4e03hh,It has a bigger performance hit on RDNA3 than RDNA4. I can understand why they wouldnt want to release an inferior version of FSR4.,hardware,2026-02-09 05:48:33,-29
AMD,o4ezewi,I may be wrong but I honestly believe they're gonna release it for older gpus when Sony's new fsr4 based upscaler comes out. That may end up being the version they release for the previous gen gpus,hardware,2026-02-09 11:19:44,0
AMD,o4keomk,"My theory is that possibly 7900XT cannibalizes 9070XT, so they mandatory need some exclusive tech. Meanwhile on Linux with Valve RADV driver...",hardware,2026-02-10 05:22:45,0
AMD,o4eieoj,I wonder if AMD has a contractual obligation to Sony/Project Amethyst to not release this.,hardware,2026-02-09 08:34:55,-6
AMD,o4dz6q6,"Hello JohnSteveRom2077! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-02-09 05:41:14,0
AMD,o4ecn94,"What we know so far:  * FSR4 INT8 exists. * It seems to work fine.  Rather than celebrate this, for some reason everybody seems to be dogpiling on AMD.  All because they accidentally released FSR4 INT8 ahead of time.  Sigh.",hardware,2026-02-09 07:39:03,-26
AMD,o4fizoy,Fake OP title that will never be resisted or corrected. Any post even remotely resembling a *neutral statement* about AMD is downvoted into oblivion. All standards and rules have been torn to shreds.,hardware,2026-02-09 13:40:41,-13
AMD,o4e6vro,They don't give a shit about handhelds or any other market besides AI right now.,hardware,2026-02-09 06:46:05,174
AMD,o4fggnb,Yeah I notice in most games that Intel XeSS performs a lot better than FSR3 on my 6950 XT. It's very odd to me.,hardware,2026-02-09 13:25:14,13
AMD,o4eee7h,I would love this for my z13.,hardware,2026-02-09 07:55:41,12
AMD,o4e837o,AMD like many others sadly don't care about the consumer side right now. Hyper focused on AI.,hardware,2026-02-09 06:56:46,48
AMD,o4eckgv,"I was stuck between deciding whether to go the Ally X or the Claw a couple months ago and AMDâ€™s original decision to not release FSR4 for even the Z2E made my decision easier. And now, the Claw has official FG support along with unofficial 4xFG support simply by replacing a few dlls which of course can be translated to most games using optiscaler.",hardware,2026-02-09 07:38:19,23
AMD,o4oxhp1,"So a while ago AMD ran a large survey on GitHub over which GPUs they should add ROCm support for.   Ok so first of all, they all should've had full ROCm support from launch day. Just like how every Nvidia GPU has CUDA support from day 1.   Anyways, after a while the results were tallied, and someone asked AMD what they're planning to do. The response ended up being completely disconnected from the survey results.   The main difference between AMD and Nvidia is that Nvidia sees itself as a software company (CUDA, etc) which makes fast hardware accelerators. AMD still sees itself as a hardware company. Any software AMD releases feels like an afterthought.",hardware,2026-02-10 21:49:58,6
AMD,o4fb52z,I hope AMD loses the handheld market and finally for them realise the error of their way,hardware,2026-02-09 12:50:59,13
AMD,o4edmo0,"If we didn't need them to fight NVIDIA's monopoly (which they don't care about us consumers anymore), I would say they would need to lose profit as they did with bulldozer, just to go back to the real world and stop screwing their customers.  That picture on the article shows that the performance hit from FSR 4 ""emulation"" on RDNA3 is less than half the performance hit of the new DLSS model on a 5090. How is that ""poor performance on older hardware"" AMD? Absolute pieces of crap for doing this.",hardware,2026-02-09 07:48:24,18
AMD,o4h7x40,yeah I wonder what the issues are. are the numbers for am5 CPU/mobo builds less than they expected because of am4 longevity? so they're forcing users to go to 9000series cards because of that?,hardware,2026-02-09 18:44:08,2
AMD,o4efl1d,This sub needs to remember that the handheld market is absolutely tiny.,hardware,2026-02-09 08:07:01,13
AMD,o4fbmfi,"Well, if it helps at all, a lot of those handhelds are running Linux or capable of it. And the open source Linux drivers for AMD cards already have the int8 feature baked into it. On Steam, you just need to add a command to enable it.",hardware,2026-02-09 12:54:12,3
AMD,o5anq66,"Intel has released the new driver that support MFG (up to 4x) for MSI Claw 8 AI, today. I'm using both Intel and AMD hardware but now AMD is slowly losing my favour.",hardware,2026-02-14 05:36:53,1
AMD,o4fh3t0,"> This still remains one of the biggest marketing foot-guns I have ever witnessed.  > AMD could really lose the handheld market in a single generation with Intel's B390 release.  Your entire perception of what the market looks like is wildly incorrect.  edit: 371 upvotes for blatant lies about a market nobody actually cares about, Jesus Christ.",hardware,2026-02-09 13:29:14,-2
AMD,o4j78m4,"To be fair these machines would likely not benefit from FSR4 at all because FSR4 would need so much compute the performance gain of upscaling would be very small. AFAIK, we only have data how FSR4 INT8 runs on mid to high end PC GPUs.",hardware,2026-02-10 00:50:20,0
AMD,o4eg0gx,but they will still be releasing new apus with rdna3 in 2027 :D  hell the shity steam machine will come with rdna3....,hardware,2026-02-09 08:11:08,82
AMD,o4falc4,Where did you get the info that RDNA3 is going into maintenance mode in late 2027?,hardware,2026-02-09 12:47:18,0
AMD,o4h5f6r,"Exactly, not just handhelds and apus/igpus, but also the rx 7700, released just a few months ago, its actually their most recent desktop gpu, newer than any of the rdna4 cards",hardware,2026-02-09 18:32:29,11
AMD,o4jyj7f,This will be the most ironic thing on hardware ever lol.. LINUX EXCLUSIVE,hardware,2026-02-10 03:31:48,8
AMD,o4exutj,"DLSS4.5 still looks the same on all cards AFAIK, it's just the performance that differs.   INT8 FSR looks worse, so they'd either have to live with that, or brand it as ""FSR Lite"" or something.",hardware,2026-02-09 11:05:45,26
AMD,o4elnie,"IRC the main issue is that the older architectures lack the AI features required. In which case the fumbling isn't that they didn't want to port FSR4 backwards, but that they completely failed to implement forward planning with earlier architectures.   Which just seems silly with how popular DLSS was.",hardware,2026-02-09 09:07:02,-14
AMD,o4fifwe,"You can't ""take advantage"" of being the underdog. You are *forced* to be the underdog in the hopes that you will die.  The entire high end is a complete waste of silicon. The xx90 cards shouldn't even exist, but Nvidia doesn't care because they have infinite money.",hardware,2026-02-09 13:37:24,-8
AMD,o4fgjdj,There is a reason their market share dropped below 10%  Most people have stopped buying them.,hardware,2026-02-09 13:25:43,32
AMD,o4firxw,Because this isn't happening. You are describing unreality.,hardware,2026-02-09 13:39:24,-8
AMD,o4f27ka,It's actually mindboggling just how forward looking Turing was. Here are the features from the Turing architecture PDF:  * ML hardware (tensor cores) * ML based upscaler (was trash initially but aged like fine wine) * Level 3 RT HW with BVH traversal HW (RT cores) * RTRT pipeline (initially a gimmick but DDGI and later improvements changed that) * VRS + Content adaptive shading + motion adaptive shading * Various VR gains (foveated rendering + Multi view rendering) * Sampler feedback and texture space shading * Mesh shaders (clean slate rendering pipeline) * 2 x FP16 (I know vega introduced) * L1+Shmem design * Concurrent INT + FP (IIRC AMD already had) * Improved resource management (tier 2) and binding model (tier 3) * Very efficient vs Pascal (perf/mm\^2 vs perf/watt tradeoff) * GDDR6 * Big memory compression gains * Very good encoder  No doubt their largest leap since ~~Kepler~~ Tesla in 2006.,hardware,2026-02-09 11:43:50,44
AMD,o4e7o0g,"Yeah it's actually kind of funny to go back and watch all the shit Turing got at the time from everyone, reviewers on down, when in reality looking back Turing will likely be seen as one of the most influential and important GPU releases of all time.   All three vendors make to Turing's base design of shader/ai/RT cores now, and as you said the fact that it took AMD over 6 years to finally throw in the towel, and really only with outside money from Sony, just shows how unconcerned and uncompetitive they truly are. I really miss ATI :/",hardware,2026-02-09 06:52:59,49
AMD,o4f7b7k,RDNA 4 has the same shader core AI accelerators as RDNA 3. Only CDNA and eventually UDNA do not.,hardware,2026-02-09 12:23:46,3
AMD,o4ffqog,(Meanwhile DLSS 4.5 works on cards from 8 years ago),hardware,2026-02-09 13:20:48,17
AMD,o4f7nlw,"But it looks like they're not even making RDNA4 mobile chips, so they're just making the laptops and handhelds that use RDNA 3.5 or older less appealing",hardware,2026-02-09 12:26:23,7
AMD,o4e3i8u,"There is no underdog imo, just what company is gonna fleece you more and AMD has been riding on folks like you for a long time now. Buy the GPU that has what you want now and not whatâ€™s promised for the future, because who knows how many times AMD is gonna pull this stuff now",hardware,2026-02-09 06:16:52,116
AMD,o4f6cxx,"I swapped my 5700xt to a 5070ti and I have no regrets. I loved my old card but AMD really just falls flat on software features and support. Having a Nvidia card for the first time in a decade and being able to just boot up games and see features supported widely without tinkering is very nice, reflex especially.",hardware,2026-02-09 12:16:36,9
AMD,o4ees79,"> I really want to support the Underdog   Buy battlemage then. AMD is just ""whatever Nvidia does -$50 and with inferior software and RT"" and has always been.",hardware,2026-02-09 07:59:19,49
AMD,o4lfqqa,"> I really want to support the Underdog  why? what is this obsession with underdogs? Underdogs are more likely than not actually worse, which is how they ended up being underdogs in the first place.",hardware,2026-02-10 11:00:54,7
AMD,o4eb6zu,Sadly Nvidia is crap in Linux and I'll never go back to AI 360 CopilotOS,hardware,2026-02-09 07:25:08,-2
AMD,o4emexc,"There is no underdog they're both capitalist as in they exist, and aim for one thing, to generate profits",hardware,2026-02-09 09:14:39,0
AMD,o4e2lif,"I'm not spending nvidia money lol. I'm going to run my 9070xt until the fans fall off, and then maybe I'll buy an intel arc card, or whatever some chinese company makes in the next 10 years.",hardware,2026-02-09 06:09:08,-19
AMD,o4eicqq,"Dude I just got FSR4 working on my 7800XT with a basic DLL swap. Took all of 15-20 minutes once you have the right files/watch ancient gameplays video on it.   I can even share you the whole int8 folder which is all you need.   The 7800XT will absolutely age fine at 1440p due to its 16GB bigger. Can't say that about the 4070/4070TI though with its 12GB in some games.   If you're planning on holding it another 2-3 years, who cares anyway?",hardware,2026-02-09 08:34:23,-8
AMD,o4eet9x,isnt mfg is broken at rtx 3xxx and below?,hardware,2026-02-09 07:59:36,-8
AMD,o4k8lst,Bad news. Nvidia probably wonâ€™t even make gaming GPUs anymore,hardware,2026-02-10 04:38:38,0
AMD,o4goqgx,"Logically it makes no sense to make millions of people wait, so they can launch it alongside a product that will sell thousands. The Steam Machine is looking bleaker and bleaker as days go on.",hardware,2026-02-09 17:13:37,3
AMD,o4h9npa,It's gonna come for PS5 Pro. That's likely what it was developed for in the first place.,hardware,2026-02-09 18:52:15,1
AMD,o4e192h,So does DLSS 4 and 4.5 on older cards but Nvidia at least gives the user a choice.,hardware,2026-02-09 05:57:57,31
AMD,o4e4wi1,"Give the option. Always give the option. Drown it in a menu, hide it behind a couple of clicks, but make it available. There's no excuse. Let people choose their poison.",hardware,2026-02-09 06:28:43,12
AMD,o4e58dv,so is dlss 4.5 for older nvidia gpus,hardware,2026-02-09 06:31:33,14
AMD,o4e1hej,"Ok, and DLSS4 and 4.5 have bigger performance hits on each subsequent past generation down to the RTX 2000s but Nvidia still lets you use it anyway. No excuse for AMD. It's *already optional*. Why not give users more freedom of choice? You know, one of the biggest reasons for PC gaming.",hardware,2026-02-09 05:59:50,56
AMD,o4e0mir,"More than native, less than FSR 3.1. So it's still a net positive which I would take with my 7800XT.",hardware,2026-02-09 05:52:49,26
AMD,o4ec3w4,"Nvidia gave us an option despite the fact that DLSS4 and DLSS4.5 aren't that huge in difference, basically from good to good upscaler. Meanwhile FS3 vs FSR4 can't even compare  in visuals,  performance hit would make more sense in this case with visual gain. I bet 7900 users at least would appreciate it....",hardware,2026-02-09 07:33:51,7
AMD,o4e5dgw,It runs about the same as the new dlss4.5,hardware,2026-02-09 06:32:47,6
AMD,o4e3299,"It's definitely a case of branding issue, they're probably weighing between not releasing it at all or they could brand it FSR 4 Lite.  Or their internal statistics indicate that people buy 9000 series for FSR Redstone",hardware,2026-02-09 06:13:06,4
AMD,o4e0fuf,"They could have released an official version with a disclaimer ""it is what it is, don't expect updates"". But they don't. For exactly no reason.",hardware,2026-02-09 05:51:18,17
AMD,o4e09c9,Eh.,hardware,2026-02-09 05:49:52,10
AMD,o4e392w,"Is it worse than FSR3 on these cards? Thatâ€™s the important question. If it is an upgrade over FSR3, releasing it for these cards:  1. Gives them a meaningful upgrade, and, 2. Increases the userbase of FSR4 cards, reducing market fragmentation and improving the market for FSR. The fact that EVERY RTX card ever released supports the latest DLSS is a big deal.",hardware,2026-02-09 06:14:43,5
AMD,o4o6tag,Nah if Steam Machine hacks it or releases with it that'll be the final nail in the coffin. But it needs massive pushback from big YouTubers otherwise nothing will happen.,hardware,2026-02-10 19:45:55,0
AMD,o4ejw7c,Why would they?,hardware,2026-02-09 08:49:37,8
AMD,o4lg923,Any release after the release of FSR4 is not ahead of time but late with required penalties.,hardware,2026-02-10 11:05:25,7
AMD,o4egl56,"> ahead of time  where does anyone claim, that the fsr4 int8 version was released ahead of time?  at this point the most likely reason for its release was a disgruntled developer ""accidentally"" hitting publish with an mit license, because they worked on it for ages and were pissed off, that amd didn't want to release it, or it was really an actual mistake.  either way there is now many many months later NO reason to assume, that amd has plans to release the int8 version for rdna2 or rdna3. it wasn't an early release by accident. amd clearly didn't want people to know about it. IF they did, then they would have said, that they are working on it before massive endless backlash about it. they have done the opposite and showed people the middle finger.  and you want people to celebrate a middle finger from a giant tech company?",hardware,2026-02-09 08:16:45,17
AMD,o4efcs7,"Because that's what people like to do with AMD. NO matter how good their products are, they're AWFUL.",hardware,2026-02-09 08:04:48,-12
AMD,o4hbto6,"People seem to forget that all this is based on a leaked beta or even alpha stage backport of FSR4. We don't know why AMD didn't release it. Maybe they didn't have the manpower, so they prioritized RDNA4. Maybe they weren't satisfied with INT8 and switched to FP8 later. Maybe Sony is heavily involved with the development and there are contracts in place that forbid a release. Maybe there are some libraries or technologies they have to license and they just don't want to pay the bills. Or maybe they are gonna release it as FSR 3.5 or whatever.",hardware,2026-02-09 19:02:24,-1
AMD,o4ewkca,"Yet Nvidia, the company that always been labelled as ""Don't give a shit about gaming, only AI, don't care about gamer"", still have a whole gaming talk show at CES, introduced new GAMING features like DLSS 4.5 and 6x MFG, not only the BEST GAMING FEATURES in the market with insanely wide game support, but coming to all RTX gpus dated back to 2018, 4 generations ago. They also promote a bunch of games and showcase many new AAA sponsored games with those technologies.  If all those things Nvidia did to gaming are ignored and treated as a given, and still got them labelled as ""Don't care about gaming, only focus on AI"", then what the fuck is AMD, who give literally nothing compared to Nvidia, and on top of that treating their consumers like shit by abandoning all the previous gen",hardware,2026-02-09 10:53:59,127
AMD,o4eg9dh,if they didn't give a shit then they would have no reason to block FSR4,hardware,2026-02-09 08:13:33,46
AMD,o4ggny0,"Yea, new article from last week says intel/amd both redirecting their manufacturing to much more target the datacenter side than consumer.  Get ready for all of them to stop caring about anything but big datacenter.",hardware,2026-02-09 16:35:27,2
AMD,o4lq7k6,The handhelds that would be hobbled by being on RDNA 3.5 (thus no FSR4).,hardware,2026-02-10 12:23:50,1
AMD,o4gi3p5,They have a meaty contract with Sony to produce a handheld more powerful than the Series S which will outsell the Switch weâ€™ve people can play GTAVI on it!,hardware,2026-02-09 16:42:18,0
AMD,o4fe1td,Its not like they gave a shit before AI either. Amd gpu division has been in shambles since many years,hardware,2026-02-09 13:10:06,28
AMD,o4fi3p4,AMD has never cared about the handheld market because they were never allowed to participate in it.,hardware,2026-02-09 13:35:21,-6
AMD,o4lf2lg,Except evidence shows Nvidia cares more about consumers than AMD does.,hardware,2026-02-10 10:54:55,7
AMD,o4j6lfh,"The prevailing wisdom (never confirmed by AMD) is that either RDNA4 doesn't scale down well to a 12/16 CU part suitable for APUs or that they were hoping to have UDNA (which is much more modular for use in dGPUs, SOCs and servers) ready for integration much sooner.",hardware,2026-02-10 00:46:38,3
AMD,o4ek8ip,"NVIDIA's shield market was/is absolutely tiny and yet it has supported the platform for 7 years now including with it AI-enhancement features. Fact of the matter is, NVIDIA for all its faults supports its past products extensively whereas AMD just looks for excuses to drop support which is why the former has built up such a monopoly in the gaming segment. People who have purchased both NVIDIA and AMD cards at some point can see the difference in support between the two and it has affected AMD long term even if people dont like to admit it.",hardware,2026-02-09 08:52:57,60
AMD,o4gh9fv,RDNA 2 and 3 still make up the majority of the AMD users. There's just no excuse.,hardware,2026-02-09 16:38:18,11
AMD,o4gnvae,"That's absolutely true, but it's a new and growing segment. AMD treating it so poorly is such a short sighted move, as carving out the segment is not only good for AMD but for x86 as a whole.  With the way AMD is handling the segment, it leaves the door wide open for Intel to take over, they already have more efficient chips, better iGPUs and XeSS 3 works on all Arc GPUs, whether it's their desktop dGPUs or their oldest and cheapest Arc iGPUs on laptops.",hardware,2026-02-09 17:09:29,4
AMD,o4hqz0j,correction pc handheld is tiny the actual handhelds people want already has nvidia making the chip,hardware,2026-02-09 20:17:18,5
AMD,o4ffey0,"It is very small, but it's also ine of the few sectors that is clearly growing over the last few years.  AMD run the risk if just tossing it away whilst chasing AI.",hardware,2026-02-09 13:18:48,4
AMD,o4eph4k,So is AMD's share of the GPU market.  Because they keep getting all trigger-happy at the sight of their own feet.,hardware,2026-02-09 09:45:38,9
AMD,o4eq5nk,"Around 6 million sold Steam&nbsp;Decks I wouldn't call exactly tiny â€¦  It's at least enough market-meat to fight over, for Intel *and* Microsoft try to engage on that ""absolutely tiny"" market-niche with Intel through the MSI Claw (among others), and Microsoft wanting to refit their whole OS for it.  So it might be a corner of the market, yet there seems to be enough money to be made to make it worthwhile.",hardware,2026-02-09 09:52:20,0
AMD,o4j3q0j,"Yep!  I'm running Bazzite on my Legion GO and use a combination of the Proton variable for FSR4 and Decky/Optiscaler plugin for XeSS.  It's also hilarious to me that the Source release blunder means that, technically, the INT8 code was released under the MIT License.  So they can't even stop people from redistributing it.",hardware,2026-02-10 00:30:10,3
AMD,o4j52yn,"> Your entire perception of what the market looks like is wildly incorrect.  In what way?  The handheld gaming market is fairly niche, but absolutely gaining momentum.  What we know for sure, is that Intel is ready to drop new hardware that is significantly faster than AMD (who have no plans for at least 12 months).  That's a pretty big opening for Intel to grab up market share based on technical superiority alone.",hardware,2026-02-10 00:37:54,12
AMD,o4jkii4,"From what I've seen on my LeGo, it's still worth it.  On the gen1 Legion Go it's actually uniquely positioned to benefit as it has a 2560x1600 screen, so running FSR4 in performance mode nets a better image than a native Steamdeck (1280x800).  The main benefit is that FSR4 doesn't break up horribly like FSR3 does at low resolutions. e.g.  FSR3 performance on most handheld screens is a garbled mess.",hardware,2026-02-10 02:07:47,6
AMD,o4f260j,"Not the first time that they've sold new, unsuported GPUs. I believe that they are still selling laptops with VEGA iGPU thanks to their new name scheme (7x30U models I believe).",hardware,2026-02-09 11:43:29,39
AMD,o4f15l8,Yeah but the Steam Machine is not affected by what AMD does with drivers on Windows,hardware,2026-02-09 11:34:56,0
AMD,o4fmvj7,"thankfully steam machine runs linux, so open source community can manage drivers for gpu",hardware,2026-02-09 14:03:13,1
AMD,o4gyf7n,"Pretty sure they said 2029, also they just released the rx 7700 lol, newer gpu than rdna4",hardware,2026-02-09 17:59:51,1
AMD,o4fgduo,"Rdna2 launched in late 2020 and maintenance and late 2025. So 5 years.  So by that assumption, rdna3 launch in late 2022 + 5 years = late 2027",hardware,2026-02-09 13:24:44,16
AMD,o4lff4x,I now want to see it just to see the reaction in gamer circles. Ill bring my own popcorns.,hardware,2026-02-10 10:58:00,5
AMD,o4ns2yl,"Rumor is that Valve is working on an optimized FSR4 for the Steam Machines, but you can already enable FSR4 using Proton-GE.",hardware,2026-02-10 18:38:07,1
AMD,o4f2z5n,"According to HW Unboxed, the int 8 version of fsr4 is visually better than xess2 dp4a while having a similar performance impact. Itâ€™s an improvement over whatâ€™s already there on the market and AMD refuses to release it.",hardware,2026-02-09 11:50:05,27
AMD,o4f5gjk,"FSR 3.8 is fine; not quite FP8 FSR4, so brand it under FSR3. It can mark the end of the FSR 3.x line.  Not sure why they're being stubborn about FSR4. Honestly, AMD could just use conversion instructions and provide full FSR4 to RDNA3/3.5 and 3.7 (""RDNA4m""). I think, maybe, AMD wants GPUs to support the entire Redstone stack going forward. That's where FSR 3.8 branding makes sense as a standalone upscaler based on FSR4.",hardware,2026-02-09 12:09:45,6
AMD,o4enz06,">Which just seems silly with how popular DLSS was.  I get not wanting to get in at the earliest, but since DLSS 3 in late 2022 it was clear that it wasn't just going to be a phase.",hardware,2026-02-09 09:30:18,18
AMD,o4eu123,"Problem is that it can work, if it couldn't like MFG on older RTX cards than this would have  not been as problematic. But people got it to work after that leak. I read that DLSS5 might not work with some older RTX cards when it comes out but given it's run I would say that would be more fair and square especially with quality of DLSS as it is now",hardware,2026-02-09 10:30:03,7
AMD,o4evnur,"FSR4 can run on RDNA 2 and 3, with slightly lower performance and image quality. But it still beats FSR3 in terms of how it looks.  https://videocardz.com/newz/leaked-fsr4-int8-version-runs-on-rdna2-and-3-with-9-13-lower-performance-image-quality-below-fsr4-fp8-but-still-above-fsr-3-1",hardware,2026-02-09 10:45:36,10
AMD,o4f345k,Both rdna 2 and 3 support wmma instructions and can run the int 8 version of fsr4 with no problems at all.,hardware,2026-02-09 11:51:12,5
AMD,o4g2xl7,"The market decides what is a waste and what isn't, not you.",hardware,2026-02-09 15:29:34,16
AMD,o4g35jq,"Where is FSR 4 on the current gen - 1, of better yet, the CURRENT gen of mobile APUs they currently actively produce, market, and sell?",hardware,2026-02-09 15:30:40,22
AMD,o4fj2e7,"Turing was absolutely forward-looking, but it was also comparatively expensive at launch and didn't offer the big gen-on-gen raster gains everyone had become accustomed to. We know it was the future *now*, but if you were looking at it back then as a Maxwell or Pascal owner it didn't really do much to sell itself.   Meanwhile AMD came out with the 5700XT that offered ballpark 1080ti performance for $399. It's not hard to see why the narrative went the way it did.",hardware,2026-02-09 13:41:07,20
AMD,o4ee5wi,Turingâ€™s super refresh also addressed the value aspect. I donâ€™t know any unhappy 2000 Super buyers.,hardware,2026-02-09 07:53:30,37
AMD,o4er6fn,Even funnier when you look at Intel and Arc. Arc released with RT and ML in mind and I believe their cards take a lower hit from enabling RT than AMD's.   And even XeSS 2 is getting features before FSR,hardware,2026-02-09 10:02:22,27
AMD,o4eyemk,"And funny how RDNA 1 like 5700XT at the time was treated as gold standard compared to Turing, which everyone trashing on, now RDNA 1 is not even supported for driver anymore, can't run many games due to lacking even mesh shader, zero RT hardware so can't run many new games, and even if it could the experience is horrible with absolutely garbage upscaling and zero feature, the 5700XT aged like rotten milk while Turing is more than fine wine",hardware,2026-02-09 11:10:44,19
AMD,o4enp3a,"To be fair ray tracing is still not really a thing that's worth the performance hit, which is what people were saying back then.  DLSS, which is now the real ""magic"" of RT cores, was also ***horrendous*** in its first incarnation, wasn't  *really* needed, and only really got acceptable in late 2022 with DLSS 3.  Doesn't change the fact that you're right about everyone clowning on it but it turned out to be a pivotal tech though... It would just have been so hard to just believe back then given the actual performance.",hardware,2026-02-09 09:27:29,-12
AMD,o4e8m8d,"It's not like this is the first time.  Ryzen 5000 on X370/X470 anyone?  But I agree - buy what you need NOW, don't buy promises and potential things.",hardware,2026-02-09 07:01:31,39
AMD,o4e3tw1,"This just sounds like you don't have a functioning moral compass or worse, just don't want to admit there are greater evils so you never have to give anything up.",hardware,2026-02-09 06:19:32,-75
AMD,o4efxn6,If they would release cards which are actually competitive and not far too late to the party I would. But they don't have a card more powerful than my 7800XT and by the time I will upgrade I won't buy a card with +10% performance,hardware,2026-02-09 08:10:23,5
AMD,o4fgya0,"> AMD is just ""whatever Nvidia does -$50 and with inferior software and RT"" and has always been.  This is fiction, but you'll be upvoted for it because it's the correct dogma, while I'll be downvoted for pointing this out.  edit: Love to predict the future.",hardware,2026-02-09 13:28:16,-9
AMD,o4nrjri,"No they donÂ´t always have to be the reason of their own downfall. AMD was an absolute CPU Underdog up to 2018/19 and built their way up with fair prices, meaningful improvements and big promises like supporting AM4 for a long time. My support was rewarded as I got a cheap upgrade possibility by selling my 3600 and buying a 5800X3D.     I gave them my trust with the GPUs too and my old Sapphire 5700XT Nitro+ did a good job by having an awesome buy and sellprice (bought it for 360 euros brand new in 11/19 and for 160 in 02/24) and a good feature support. Why shouldnÂ´t I give AMD another try? The Underdog did deliver a great value for me.",hardware,2026-02-10 18:35:43,2
AMD,o4f0ki7,"It sucks that their drivers are closed source, but it's far from ""crap"" in Linux.",hardware,2026-02-09 11:29:54,16
AMD,o4ichlz,"People saying this kept me from switching for a long time. And when I did, I discovered that things work fine. My 4090 hums along and I can still use DLSS and RTX features.",hardware,2026-02-09 22:03:55,3
AMD,o4gerdd,"Maybe it's just me, but going from a 7900XTX to a 4090 was a night and day difference, even in linux. 4090 *just worked*. Whereas the AMD card would have constant driver timeouts and crashes in specific games(especially those that didn't like AMD in the first place) despite being RMA'd twice.  Sure, maybe i got unlucky, but i've had pretty much zero problems with nvidia on linux. Maybe it's because i run a rolling release distro though, i can see it being rough if you're constantly on older versions via LTS or something like bazzite which is immutable.",hardware,2026-02-09 16:26:25,8
AMD,o4eqiec,It would be really nice to see Nvidia open up their drivers on Linux because that is a real sticking point for many. We just rely on Nvidia to fix stuff there for us instead of being able to do it ourselves. You would think considering Linux is the primary OS in industry they would support it better?,hardware,2026-02-09 09:55:49,6
AMD,o4e71l5,"You don't have to as your 9070 XT will work fine with FSR4 while my 7800XT will artificially get castrated because AMD sold me Hardware they themselves don't use (""AI Cores"")",hardware,2026-02-09 06:47:30,28
AMD,o4ekzch,"Thatâ€™s great, but if itâ€™s so simple why isnâ€™t it officially supported? If itâ€™s as easy as swapping DLL files, I donâ€™t see why users should have to do that themselves.",hardware,2026-02-09 09:00:16,11
AMD,o4f02pe,"Most gamers won't look into these kinda things as they will only use official drivers from AMD. Workarounds are just that, workarounds.",hardware,2026-02-09 11:25:34,2
AMD,o4ef4uk,MFG is only a RTX 50 series feature but DLSS 4.5 works perfectly on 20 series.,hardware,2026-02-09 08:02:41,15
AMD,o4k9sj3,"If that ends up being the case, which i find highly doubtful, at least on the short-mid term, specially knowing that there's a very real possibility that the AI bubble ends up bursting, there will always be Intel as a secondary option(and even if Intel wasn't there and the AI bubble never bursts, it literally makes no sense for NVIDIA to completely abandon a market where they hold more than 70% of total market sales. What is likely to happen is that NVIDIA puts less and less focus on gaming GPUs, but never stops making them fully)  And if Intel is shit too, i would rather not buy a GPU at all than give money to Radeon again.",hardware,2026-02-10 04:46:57,2
AMD,o4o6bxf,NVIDIA has 4.5-5 billion a quarter gaming revenue. Why would they drop that market that's low risk and guaranteed. Putting all your eggs in one basket is never a good idea.,hardware,2026-02-10 19:43:40,1
AMD,o4e7rj1,"I was honestly shocked when Nvidia gave new DLSS also to older gens. Sure, the performance hit is there and on some models it's not ever worth it, but still.",hardware,2026-02-09 06:53:51,9
AMD,o4e0wlu,"Honestly quality FSR 3.1 on 4k looks almost decent, but with the negativity, I can see why they dont want to release an inferior version of FSR4.",hardware,2026-02-09 05:55:07,-14
AMD,o4e37lz,"Honestly, releasing it with a SPECIAL name would be the best solution. Like FSR4 lite or FSR4 preview name.",hardware,2026-02-09 06:14:22,7
AMD,o4e3dg1,"Most consumers, even if they have a custom PC, will default to ""install the latest drivers"". If that means negative performance, I can see a good reason why AMD would not release such a driver.",hardware,2026-02-09 06:15:44,-2
AMD,o4kn6la,Because then the same crowd wanting this now would then proceed to complain that it sucks balls compared to the old version?  Seems pretty obvious there isnâ€™t any satisfying the outraged gamer crowd.,hardware,2026-02-10 06:32:07,0
AMD,o4e0r3q,"There is a lot of negativity on reddit towards AMD. for upscaling there are two things to consider: quality and performance. If AMD released fsr4 for rdna3, they wouldnt have any advantage over DLSS either in performance nor in quality. (FSR4 quality is good but still inferior to latest DLSS versions).",hardware,2026-02-09 05:53:51,-17
AMD,o4e4bgv,"I mean, FSR4 quality is slower than FSR3.1 quality, but even FSR4 performance still looks significantly better and also performs better than FSR3.1 quality.",hardware,2026-02-09 06:23:42,9
AMD,o4lgeae,Sony supposedly put a lot of effort in themselves to help develop it (according to Mark Cerny). Could be SONY permission is required.,hardware,2026-02-10 11:06:42,1
AMD,o4ekgkw,I don't know. I'm just guessing. Sony seems to have invested in the R&D and may have set some limitations on where the technology can be used.,hardware,2026-02-09 08:55:09,-2
AMD,o4lgb3j,because the last time AMD made a good GPU product was over 10 years ago.,hardware,2026-02-10 11:05:55,4
AMD,o4jq8vn,"I'd say the ""Nvidia research to standard feature pipeline"" has been pretty well established at this point... RTX, DLSS, Framegen, Reflex, Gsync, other AI rendering stuff (RR, NRC, NTC)... sure, they're not the ones who ""invent"" them sometimes (although they do contribute a shit ton of research in novel fields that people typically don't know about until it hits consumer products), but they're clearly the driving force behind modern gaming features.   Part of that might be corporate culture, part of it might just be that Nvidia's 90% of the gaming market, part of it is just gaming adapting to the hardware the market provides (if you're a AAA game dev now or developing any sort of technically ambitious feature, you are building your engine/game from the ground up with features like RT/PT/DLSS/Framegen/NRC whatever else from the ground up- future cards are going to have insane matrix math capabilities to support AI, so we take our current gaming computation problems and turn them into matrix math problems- that's just the general hardware market shaping gaming tech).   There's a feedback loop, but yeah, Nvidia's certainly putting in the work, and I'm sure they *wish* they could make an extra billion or two sellings GPUs to gamers on top of their AI profits- current supply and demand doesn't support that though.",hardware,2026-02-10 02:41:17,9
AMD,o4gb8f3,"The whole ""NVIDIA doesn't care about gaming"" idea has always been absolute bullshit. Obviously no massive corporation ""cares"" in the way we humans might care about something, that is a given. But the idea that NV is somehow *uniquely disdainful* of gamers is childish and facile, and has no basis in observable fact.   If anything, it's the opposite: as someone who has been a PC gaming enthusiast for decades, I think NV has actually done more for me in that position than any other company (except, perhaps, Valve). Obviously they do so for business reasons; but the way they decide to act has been frequently beneficial to PC gaming.  As one massive (for me) example, I remember the first post-CRT decade in the monitor market where 60 Hz was just seen as **the** standard, and that was basically it. Which was very bleak from an action gaming perspective. And what really started to change that whole trajectory was the introduction of G-Sync -- not so much just the technology itself but more so getting the whole stagnant monitor industry to see the value of creating gaming-focused products.",hardware,2026-02-09 16:09:35,41
AMD,o4fxoyg,Slight clarification: MFG 6X/dynamic is RTX 5000 only. But all RTX GPU do get the new transformer model for upscaling.,hardware,2026-02-09 15:02:54,11
AMD,o4hpzfi,amen probably the best comment ive see in a very long time,hardware,2026-02-09 20:12:15,5
AMD,o4kcp2y,AMD is simply irrelevant in the gaming market except for consoles.   Even my own (currently unused) desktop has an AMD CPU with an older Nvidia GPU. I just don't see any reason to buy an AMD GPU or APU at this point.,hardware,2026-02-10 05:07:53,5
AMD,o4fmcwy,Preach!,hardware,2026-02-09 14:00:13,9
AMD,o4foti2,"It's really crazy how horrible AMD is, yet people are *seriously* defending them because I guess people are just that too much into the corpo propaganda that they for no reason at all have to defend the underdog...  I'm really fucking greatful that I did not buy AMD card, and went for RTX 5080 (mainly because of DLSS & RTX as my previous card was GT 1080/RX 6600 as temp gpu). I still ROFL how the only pros of AMD card is higher VRAM like yeah sure bro that is 100% what just sold me LMAO",hardware,2026-02-09 14:14:24,12
AMD,o4u2hlq,Where FG on 30 series? Where mfg on 40 series? A fucking 5050 does both,hardware,2026-02-11 17:41:12,1
AMD,o4gmdzl,"Always someone has to turn into a VS battle, kids man. They are both shitty corporations who don't give a fuck about gamers at the moment.",hardware,2026-02-09 17:02:25,-2
AMD,o4gcbno,"Seeing as how Nvidia just said they're halting gaming GPU production, I'd say neither of them care about us compared to their AI ventures.",hardware,2026-02-09 16:14:47,-9
AMD,o4hpdvv,fr lol,hardware,2026-02-09 20:09:08,0
AMD,o4f87xp,Being broke  Itâ€™s pretty clear AMD just doesnâ€™t have the money to compete with Nvidia now.,hardware,2026-02-09 12:30:33,-14
AMD,o4hct8e,ok but have you considered that amd has better linux support,hardware,2026-02-09 19:07:08,-4
AMD,o4gnfeg,"Thanks for your reminder, Kind Nvidia Employee!!!",hardware,2026-02-09 17:07:23,-7
AMD,o4eix88,"There's an extent to which they care, given this INT8 version even exists in the first place. As for why it's not official yet is anyone's guess. Could be laser focus on AI or deliberate gatekeeping of the model. Probably former since they included a nonsense AI ""bundle"" in their driver stack.",hardware,2026-02-09 08:40:04,38
AMD,o4g00nn,> reason to block FSR4  they are blocking FSR4?,hardware,2026-02-09 15:14:55,-6
AMD,o4qmlbu,outsell the switch? you dont go from a vita to outselling Nintendo handhelds lol,hardware,2026-02-11 03:38:20,1
AMD,o4hfoe2,At least they used to pretend,hardware,2026-02-09 19:20:52,7
AMD,o4lex0y,AMD GPU division has been in shambles since AMD bought it.,hardware,2026-02-10 10:53:33,7
AMD,o4g21bn,Do they need affirmative action like Taylor Swift is the biggest victim on planet earth?,hardware,2026-02-09 15:25:05,1
AMD,o4ll1on,"Yes, but the difference between ignoring completely the consumers and listening to consumers 10% of the time, while being better and your argument being true, in reality it doesn't matter much.  With the crap show that the last 3-4 drivers they released, with one of them completely locking me out of my OS (I had to use my iGPU to nuke the driver), I still stand with my statement.  I'm starting to believe they use AI to code their newer drivers, just like Microslop does with windows updates...",hardware,2026-02-10 11:45:42,0
AMD,o4eon5w,"And why they have such a lock in with CUDA in enterprise and professional users. Imagine buying thousands of dollars of hardware for it to be deprecated in 2 years, or have driver bugs that never get fixed by the vendor, rendering it a paperweight in a year or two.  I work with Blender, and when I was starting off I remember many people in the artist discord I was in buy Vega 64 for its compute compared to the 1080Ti. Within a year or two, the OpenCL backend on AMD slowly started [accumulating bugs](https://projects.blender.org/blender/blender/issues/65924) that blender devs couldn't fix from their end and everyone had to fallback to using CPU rendering, which is magnitudes slower than GPU rendering. Meanwhile you can use a 1080Ti in Blender to this day and everything is supported.",hardware,2026-02-09 09:37:08,40
AMD,o4gbkqg,"7 years?  The OG Shield TV released May 28, 2015 and works the same as the refreshes.  It's at 10 years, nearly 11.",hardware,2026-02-09 16:11:12,16
AMD,o4esvk6,"AMD dropping support is a tale as old as time.  Ask all the folks who bought a ""Grenada"" GPU IE: R9 390 in \~2016 that was totally not a damnable rebrand of Hawaii only for AMD to drop support for it in the ROCM stack two years later when they dropped support for GFX7 Hawaii.  * GFX7 GPUS only had support in ROCM for \~2 years. * GFX8 GPUs (IE RX580) only had support in ROCM for \~2-3 years. * GFX9 GPUs (IE 906, AMD Radeon Pro VII) had support for \~2-3 years.  I think some GFX10x GPUs are now being axed.  I mean this is only probably relevant if you need what parts of ROCM that AMD actually seems to support. I do 3D and AMD has been a non-player in this game for so long, last I remember was them doing some big slapdash song and dance about HIP in Blender only for Intel to come out of left field and kick their legs out from under them with three times the  progress in a single generation.  Meanwhile Nvidia is like ""We have decided to release a CUDA toolkit version compatible with the NV1 so you can do gaussian splats in Virtua Cop for some reason""",hardware,2026-02-09 10:18:56,29
AMD,o4hqv5e,yup,hardware,2026-02-09 20:16:45,1
AMD,o4fhre0,"> NVIDIA's shield market was/is absolutely tiny and yet it has supported the platform for 7 years now including with it AI-enhancement features.   Because Nvidia has infinite money due to being a monopoly.  > Fact of the matter is, NVIDIA for all its faults supports its past products extensively whereas AMD just looks for excuses to drop support  Because Nvidia has infinite money due to being a monopoly.  > which is why the former has built up such a monopoly in the gaming segment.  You have this backwards. Competitor actions do not create monopolies unless there is active collusion (we're talking an Nvidia/AMD merger here... cough Nvidia/Intel cough). Monopolies are enabled by *the public* and by *the government*.  Nvidia isn't a monopoly ""because AMD"", AMD is the underdog because Nvidia.  > People who have purchased both NVIDIA and AMD cards at some point can see the difference in support between the two and it has affected AMD long term even if people dont like to admit it.  Yeah, it's called AMD makes better cards and supports their cards better, but that doesn't matter because people will literally ignore reality in favor of blind dogma.",hardware,2026-02-09 13:33:16,-9
AMD,o4hs0uw,lol I thought that until i found out 6 million is actually the entire pc handheld market not the steam deck that takes up most 4-5 tho,hardware,2026-02-09 20:22:33,2
AMD,o4lyqtx,"> The handheld gaming market is fairly niche, but absolutely gaining momentum.  Do you have any source for this whatsoever?  > What we know for sure, is that Intel is ready to drop new hardware that is significantly faster than AMD  We actually don't know this at all, and Intel has constantly indicated that they are incapable of such a thing, unless Nvidia is about to bail them out massively with their infinite money.",hardware,2026-02-10 13:19:21,-2
AMD,o4fhkvn,"It's baffling that people even give them the benefit of the doubt after that bullshit. If Nvidia had pulled that, people would have, rightfully mind you, lost their minds.",hardware,2026-02-09 13:32:10,32
AMD,o4gyywc,"Just like how they delayed the 500 series motherboards which made lots of people buy 400 mobos, only to then spring up out of nowhere and say that zen 3 will only be supported on 500 mobos, before they even released the b550, to be fair they did reverse it, but their original reasoning for lack of support was bullshit",hardware,2026-02-09 18:02:26,10
AMD,o4f4bru,"the steam machine ships with rdna3. if int8 fsr4 isn't going to be released, then there will be a hell of a lot more work to force it into games through proton and the mesa driver to get fsr4 int8 to work on gnu + linux well enough.  what you want for the steam machine is a full fsr4 int8 release and have it in as many games as possible. that is what you want, because proton and valve will have a vastly VASTLY easier time to get fsr4 to work with as many games as possible with the steam machine.  so yes the steam machine wants an official release very much.",hardware,2026-02-09 12:00:51,23
AMD,o4g2g3v,"Every single mobile product AMD continues to sell this year is on rdna3.5 or older. Including but not limited to laptop processor, handheld processor, and mobile dGPU.   Laptops began to outsell desktops 20 years ago.",hardware,2026-02-09 15:27:10,7
AMD,o4gc0ft,"the hope is much better than for windows machines and as i am on a powerful rdna2 card, i sure as shit want the open source/libre software community to force fsr4 int8 to work on rdna2 and 3 on gnu + linux NO MATTER WHAT, but yeah we'll see.  i guess it is a bonus for rdna 2 and 3 users, that the steam machine uses shity old hardware, because now valve is also of course pushing for fsr4 to come to it lol...  i'm so worried about the steam machine :D they didn't make a custom apu for it, which would have been rdna4 if it would release today of course, they didn't use recent hardware, but old and weak af hardware and the worst thing of all, the thing, that could actually make things truly terrible for gnu + linux gaming perception is, that they only put 8 GB vram in that damn thing :/",hardware,2026-02-09 16:13:17,4
AMD,o4fj0gg,"BREAKING NEWS! (3 months late for you!): **AMD canceled maintenance mode for RDNA2 and RDNA1!**  [https://www.amd.com/en/blogs/2025/continued-support-for-every-radeon-gamer.html](https://www.amd.com/en/blogs/2025/continued-support-for-every-radeon-gamer.html)  Here's a driver that supports all the RDNA generation with game support, bug fixes and all the bells: [AMD Software: Adrenalin Edition 26.1.1 Release Notes](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-26-1-1.html)  Did you just wake up from a coma or what?",hardware,2026-02-09 13:40:48,-15
AMD,o5rrum4,"I hope it will run on my legion go s steamos also, baked into small right menu just like fsr1",hardware,2026-02-16 23:11:23,1
AMD,o4f5byi,"Sure, but it doesn't look like FSR4.    I agree that they should have released it, but likely not as ""FSR4"" (or ""FSR Redstone Upscaling"" or whatever the new name is).   It's confusing enough that XeSS looks different on different GPUs (with the DP4a version not being the same as the Intel XMX version).",hardware,2026-02-09 12:08:46,17
AMD,o4epf4y,"When they release a new architecture to the market, the next one is already practically finished despite being 2 years away, the one after is halfway done, and the one after that is getting its features planned out. Which is all to say that they can't edit architecture features all that easily even 3 years out.   One of the crazy things though is that even new apus lack the newer architectures.",hardware,2026-02-09 09:45:04,5
AMD,o4flzgj,"Well fuck me then, AMD is just screwing the pooch.",hardware,2026-02-09 13:58:03,8
AMD,o4hppul,The prices were high because the chips were huge. A 2060 was 445 mmÂ² almost as big as the 1080 Ti which was 471 mmÂ².,hardware,2026-02-09 20:10:53,13
AMD,o4h0xca,"No more node gains (16FF -> 12FFN is iterative) or easy architectural low hanging fruits after Maxwell withdrew the entire account.   So the only thing left to do was ballooned die sizes. 5070S has larger die size than 1080 TI and 1650S \~40% larger than 1060 etc...  But they should've done Super pricing from the beginning.  TBH most of that forward looking stuff is still being underutilized. APIs and games have progressed frustratingly slow.  Sure 5700XT was great at launch. For anyone who was going to upgrade 4-5 years later anyway  this card was fine, but didn't exactly age well.",hardware,2026-02-09 18:11:34,6
AMD,o4fvp1u,"Yep, especially when previous gen they released 1080 Ti. 2000 series really wasn't that awesome just as you said, and worst thing was that it was during the Cryptoboom...",hardware,2026-02-09 14:52:20,1
AMD,o4f2kdb,Alchemist had thread coherency sorting (TSU) and if it had launched ahead of Lovelace as originally planned Intel would have achieved this earlier than 40 series.  Intel put in the HW investment early while AMD hired bean counters and decided to wait until it was too late.  Massive push on Intel XeSS for sure but they also own the laptop space and this will be an easy way to make AMD irrelevant across most segments if AMD continues to neglect mobile iGPU.,hardware,2026-02-09 11:46:46,20
AMD,o4iygzg,Uh really? Im not surprised that nvidia is so popular in pc gpu then. When you can't play many games. I bought for great price rx6800 but started to look for another nvidia gpu.,hardware,2026-02-10 00:00:34,5
AMD,o4ft9an,"Well reviewers review what features cards offer at the time, not what they might offer in the future.   So when those reviews all came out, there was no dlss, there was no reflex, rt was in nothing and had like a 50% performance hit, and a card like the 5700xt was far better price/performance. This was all true for a number of years, it's not like dlss got wide support all at once.   Turing has definitely aged better though, as a former 5700xt user myself haha.",hardware,2026-02-09 14:39:15,13
AMD,o4htoqe,yup,hardware,2026-02-09 20:30:40,2
AMD,o4fi39r,"If AMD had released Turing and its features instead of Nvidia, all of it would be ewaste by now because AMD would not have have been able to get devs to support its features. Let's be honest, nobody would have fomo'd into proprietary Radeon RT and upscaling and AI, everyone would be saying wow AMD really wasted their time on all those useless Turing features.",hardware,2026-02-09 13:35:17,14
AMD,o4i523w,"Speak for yourself, Path Traced Cyberpunk is absolutely mindblowing.",hardware,2026-02-09 21:26:31,6
AMD,o4edlpi,Didnâ€™t they also try to gate some feature like resizable BAR to AMD CPU + AMD GPU only?  They definitely got a lot of backlash for attempting to gatekeep something platform level to AMD GPUs only.,hardware,2026-02-09 07:48:09,21
AMD,o4e683i,"Yeah, I have no moral compass. You got me buddy. Fucking lmao AMD fans are the worst for real",hardware,2026-02-09 06:40:15,44
AMD,o4e5jcf,"""Greater evil"". What do you think AMD would be doing if they were in Nvidia's position? Handing out giftcards and free GPUs? ðŸ˜‚",hardware,2026-02-09 06:34:12,54
AMD,o4ec4tg,"> Complains about moral compass.  > Look Inside  > KiA poster, Pro-AI and conservative.  lol  Also you probably use Windows, and probably typed this from Chrome. If you're gonna go all ""this company is unethical, so I won't use their products"", be consistent at least.",hardware,2026-02-09 07:34:06,27
AMD,o4ehqvb,"It's companies dude, you're buying a product. What greater evil? Stop watching so many Disney movies",hardware,2026-02-09 08:28:18,10
AMD,o4eici5,> But they don't have a card more powerful than my 7800XT   You don't really need to worry much about upgrade if you already have a RX 7800XT unless you want decent RT. In which case your option is Nvidia RTX 50 series.,hardware,2026-02-09 08:34:19,19
AMD,o4g3q4z,"Pointing WHAT out exactly? What is your point you supposedly ""pointed out""?",hardware,2026-02-09 15:33:30,8
AMD,o4o43yg,AMD CPU =/= AMD GPU.  Really hope RDNA 5 is more serious attempt from AMD because the last ones have been a joke. RDNA4 was somewhat decent but FSR adoption and Redstone has been a huge joke.,hardware,2026-02-10 19:33:16,3
AMD,o4rgse0,"I didnt say they always are the reason. Im saying that supporting something solely because its an underdog is stupid and often wrong thing to do.  AMD almost went bancrupt trying to deliver Zen, and the first generation wasnt great either (a lot of people memory holed the problems here). Supporting a socket is useless to 99% of users who never replace CPU without replacing motherboard. This is nice PR for AMD but has no practical benefit to almost anyone. What it does however do is makes sure AMD is stuck on slow I/O which is why they are unable to utilize fast memory as well as Intel.  >I gave them my trust with the GPUs too and my old Sapphire 5700XT Nitro+ did a good job by having an awesome buy and sellprice (bought it for 360 euros brand new in 11/19 and for 160 in 02/24) and a good feature support.  well now you are just being delusional. 5700xt did not have good feature support.",hardware,2026-02-11 07:35:36,3
AMD,o4f1vfz,It's getting better and they have started to support NVK and promoting more into the open source kernel module.  It's been a slow process but it's not completely without hope.,hardware,2026-02-09 11:41:03,4
AMD,o4i5oe3,"> because that is a real sticking point for many.   0.01% of users is not ""many"".",hardware,2026-02-09 21:29:35,1
AMD,o4e7wcl,I'm sure that at some point in a few years they'll screw over those of us who bought a 9070xt as well. Don't you fear! AMD never fails to disappoint lol.  But I do really wish they'd get their heads out of their butts on this!,hardware,2026-02-09 06:55:03,5
AMD,o4e7z8o,"I hope you do! You can do some pretty darn cool stuff with them! (and then there's all the CUDA stuff, of course). I just don't have a need for any of that at this moment in time.   They are very very cool cards. Don't get me wrong :)",hardware,2026-02-09 06:55:47,4
AMD,o4eoa2y,"Yeah I know, it sucks. I'm not defending them. But just letting you know that it's possible and easy to do yourself if you're worried about being left in the dust. What is especially most egregious though is that the 7800xt didn't even launch until late 2023. So it was less than 18 months old when they basically decided to stop supporting it.",hardware,2026-02-09 09:33:28,5
AMD,o4o690d,What's AI bubble gonna do for that? Demand keeps growing. People just gotta have to start paying or accept adds.   Stock market =/= DC buildout Capex,hardware,2026-02-10 19:43:17,1
AMD,o4lg5br,even steam fans realize that steam machine is a niche product. the highest estimate i saw touted around was 600k units.,hardware,2026-02-10 11:04:30,3
AMD,o4e1sbx,"FSR quality at 4k has always produced a decent image. 2560x1440 is a fair amount of visual input to work with. The issue is artifacts, especially in motion.",hardware,2026-02-09 06:02:20,13
AMD,o4e543m,Bury it in the adrenaline settings with multiple disclaimer pop ups.,hardware,2026-02-09 06:30:32,10
AMD,o4ecb9q,"Nvidia just released new DLSS models that are extremely heavy on older hardware, and even quite heavy on new hardware at higher resolutions. Depending on the game, they also don't play well with raytraced effects. In many situations, it makes more sense to use the older model. But at least they give the user the option to make that choice themselves  This may very well be AMD's reasoning, but if it is it's some pretty dumb reasoning imo",hardware,2026-02-09 07:35:51,11
AMD,o4e2qtt,They already donâ€™t have any advantage over DLSS in either performance or quality.,hardware,2026-02-09 06:10:24,18
AMD,o4e16sz,"Int8 version is already there in the open. It requires third party tools to work. Official release removes that part, at least at most levels.   So. Again. There is no reason not to release and be done with it. With a disclaimer ""whatever"".",hardware,2026-02-09 05:57:26,12
AMD,o4e1pzg,"There's not enough negativity towards their dogshit treatment of their customers. People who have Rdna3 and older cards don't have much of a comparison with DLSS, they can't use DLSS, but they can trade performance for visuals with fsr4 if amd enables it. Though at this point the best course of action for them is to just buy an rtx card instead.",hardware,2026-02-09 06:01:47,10
AMD,o4o56gh,Based on their posturing around Project Amethyst I doubt Sony is strict about it. Could be wrong though.  More likely than not it's just AMD dragging their feet and doing everything they can to motivate their customers to get NVIDIA instead.,hardware,2026-02-10 19:38:19,1
AMD,o4o4cg2,Which one?,hardware,2026-02-10 19:34:22,1
AMD,o4le741,"Nvidia made ""standard"" features before they had such high market share too. Look at Tesselation for example.",hardware,2026-02-10 10:47:10,6
AMD,o4qt8oq,"> (although they do contribute a shit ton of research in novel fields that people typically don't know about until it hits consumer products  Pretty sure Nvidia has a significant graveyard of whitepapers fully of technology which didn't quite hit the mainstream.  A great example is VXGI (Voxel Global Illumination).  Was announced way back with Maxwell/900 series and effectively solved the same problem that RTGI does now, which was to make Global Illumination fully dynamic instead of using static lightmaps.  Didn't really catch on because the compute cost was very high at the time.  I do wonder if VXGI directly led to RTGI being created.  Since they still had to basically calculate ray intersection, just at a much larger Voxel level, rather than per-vertex level.  Although, it sort-of lives on nowadays in Crytek games which their own implementation of voxel cone-traced GI.",hardware,2026-02-11 04:23:21,1
AMD,o4gxl0a,"I think the negative sentiment towards Nvidia primarily has come from their proprietary approach towards most of their new technologies. This obviously misses a lot of the nuance, like how AMD only really has been ""open source"" because they never allocated the resources towards making a strong ecosystem on their own, or how Nvidia's dominance in part came from the much greater level of support they gave to devs in the past.   I honestly wish we had an Intel and/or Nvidia console in the mix this console generation (I don't count Nintendo), competition always benefits the consumer.",hardware,2026-02-09 17:55:54,17
AMD,o4h3elb,I'd point to 3D vision instead of G-sync as a cause for the rise in high refresh rate displays. My first high refresh rate display was the LG W2363D back in 2010 and I think a good number of people started getting them at around the same time. At least people who participated in FPS communities seemed to do it.,hardware,2026-02-09 18:23:05,6
AMD,o4heovx,"Let's not get carried away here.  G-Sync was a proprietary variant of VRR that happened to be first to market. If VESA had been first, we hadn't had the absolute mess that G-Sync spawned.  Making new tech and new standards is fine. Pushing for adaptation is fine. Making needless proprietary variants and creating needless segmentation is not.  VRR wasn't some obscure idea Nvidia had. It was coming either way.",hardware,2026-02-09 19:16:09,-4
AMD,o4pod0w,Dynamic FG would be far too powerful on older hardware. People would be using 3060s into the 2040s.,hardware,2026-02-11 00:12:27,1
AMD,o4gnmmn,"""I still ROFL how the only pros of AMD card is higher VRAM like yeah sure bro that is 100% what just sold me LMAO"" .-- I feel like i lost a few neurons reading this.",hardware,2026-02-09 17:08:20,2
AMD,o4leboi,"But this is not true according to the evidence we have. Nvidia consistently releases gaming-only features, proving they DO care about gamers.",hardware,2026-02-10 10:48:18,6
AMD,o4gqm6w,they never said that.,hardware,2026-02-09 17:22:37,17
AMD,o4fvukj,"This is BS, they have plenty of money.   The problem for us is WE don't have enough money compared to datacenters.",hardware,2026-02-09 14:53:09,11
AMD,o4g1brx,Just p*** off  35 billion record full year revenue in 2025. Even Intel can do proper upscaling on their mobile chip.,hardware,2026-02-09 15:21:32,12
AMD,o4leowg,this argument does not fly when AMD spent 13 billion in stock buybacks.,hardware,2026-02-10 10:51:32,3
AMD,o4fe99r,"Neither the money nor the tech.  My gut instinct is that they *are* working on releasing INT8, but anything they say becomes an absolute promise, so they're staying quiet such that they can pivot products/release dates on a whim.",hardware,2026-02-09 13:11:27,6
AMD,o4ejmxt,"It is possible that the software just needs a bit more polishing and bug fixing and they just could not bother allocating resources on it, since all focus is on AI.",hardware,2026-02-09 08:47:05,22
AMD,o4gkoek,From coming to RDNA 3.5 and earlier products.,hardware,2026-02-09 16:54:23,12
AMD,o4fhukz,"> And why they have such a lock in with CUDA in enterprise and professional users.  > Within a year or two, the OpenCL backend on AMD slowly started accumulating bugs that blender devs couldn't fix from their end and everyone had to fallback to using CPU rendering, which is magnitudes slower than GPU rendering.  Because Nvidia has infinite money.",hardware,2026-02-09 13:33:49,-18
AMD,o4gpblp,"Yea the OG Shield is older, I inherently focussed on the Pro in my comment without mentioning it since thats the one I got and have seen getting regular updates.",hardware,2026-02-09 17:16:27,9
AMD,o4ixh5l,"Uh that sucks. I didn't understand that battle nvidia vs amd. Went amd because it was cheaper and wanted to support opposite, now I know why nvidia is just more popular overall, better and longer support, more features like cuda cores that caused monopoly, and amd instead doing better, shooted into foot.",hardware,2026-02-09 23:55:02,7
AMD,o4fhxt1,> AMD dropping support is a tale as old as time.  Because Nvidia has infinite money.  You have got to understand that none of this is about any decision AMD has ever or could ever make.,hardware,2026-02-09 13:34:22,-14
AMD,o4hrpfa,lol misinfo at its finest and everyone can see it now you cant change reality,hardware,2026-02-09 20:20:58,8
AMD,o4oxmge,"> Do you have any source for this whatsoever?  Depending on the timeline, the increase gets bigger.  I've been into handheld PCs since the first gen GPD Win, so the market exploded since then.  2023 was notably a massive year for it, but 2024->2025 still saw a jump.  For more trend info:  https://www.tweaktown.com/news/108880/xbox-ally-in-great-demand-asus-expects-to-make-up-to-dollars160-million-from-handhelds/index.html  https://www.techspot.com/news/106921-handheld-gaming-pcs-surpass-6-million-sales-steam.html  More to the point, there are more manufacturers than ever in the space, and with multiple models.  MSI, ASUS, Lenovo and Valve are the big names but there's also GPD, Ayaneo, AYN, OneXPlayer etc.  On top of that, pretty much all of them have multiple models in the market.  > We actually don't know this at all, and Intel has constantly indicated that they are incapable of such a thing  What?  We sure do.  B390/Panther-Lake laptop reviews and benchmarks are already out and it's a good chunk faster than AMD perf-per-watt (which is the most important metric for handhelds).  Intel have also confirmed that handhelds are coming out:  https://www.notebookcheck.net/Asus-ExpertBook-Ultra-review-One-helluva-debut-for-Intel-Panther-Lake-X7.1209366.0.html  https://www.notebookcheck.net/Dell-XPS-14-2026-review-Fully-reborn-with-Intel-Panther-Lake-X7.1218670.0.html",hardware,2026-02-10 21:50:35,5
AMD,o4g1pyn,"FSR4 INT8 would be useful regardless, atleast a lighter model like Intel did with the DP4A model for XeSS. But it's worth noting that RDNA3 isn't stellar at INT8 precision matrix multiplication operations either. Comparing it to Nvidia might be obvious but wait.. compare Intel's last gen A770 with RDNA3's best RX 7900 XTX. The A770 does over 2x the INT8 TOPs that of the 7900 XTX.",hardware,2026-02-09 15:23:31,4
AMD,o4f4kbr,"Or you know, FP16 + WMMA support.",hardware,2026-02-09 12:02:44,1
AMD,o4ft9ko,"I like the way you wrote your comment. Especially the part where you (of course, right?) skipped how AMD was the one that announced that RDNA1-2 are in maintenance mode, and just said how they ""misclicked"" after Internet's outrage.   Like sure. This is 100% the thing that I want to see from AMD to do... The fact that NVidia ended in October 2025 after like 9 and something years, while AMD wanted to kill it off after 5 years is absolutely crazy.  AND THE FACT that this isn't for YOU a RED FLAG, but something you try to brag about is just so funny. Like of course...  Also I dunno if you noticed, but this chain pretty much talks how AMD refuses to release FSR4 on older cards, which means that even if they are still receiving driver support that they're indeed in a maintenance mode, otherwise they would release a thing like y'know DLSS 4.5 to all RTX cards?",hardware,2026-02-09 14:39:18,20
AMD,o4fuk3r,"Notice how the blog intentionally left out ""Day 1""  So nothing has changed. Its in maintenance mode branch and will the get the updates depending on market needs.   Rdna 3 and 4 are the ones that will get full and proper day1 support and drivers.",hardware,2026-02-09 14:46:17,15
AMD,o4g9hq1,"FSR 3.5 would have worked then, considering that the ""new"" RDNA 3 hardware on APUs is called RDNA 3.5",hardware,2026-02-09 16:01:15,10
AMD,o4gz8hy,"Wdym it doesnt look like fsr4, based on HUB's testing its almost identical",hardware,2026-02-09 18:03:42,5
AMD,o4l8ull,"My understanding is like this:  DLSS 4.5 > DLSS 4 >> FSR 4 > FSR 4 INT 8 > DLSS 3 > XeSS XMX >> XeSS DP4a >>>>>> FSR 3.1  We're still looking at something better than DLSS 3 and XeSS XMX, albeit noticeably worse than FSR 4 FP8.",hardware,2026-02-10 09:57:33,1
AMD,o4j43ds,Soonâ„¢ï¸,hardware,2026-02-10 00:32:15,9
AMD,o4l9nym,"Die costs were negligible back then. A 400mm^2 die is huge (5080 sized), but it also would've cost like $20 to manufacture. In comparison a 5080 die costs at least $100. This is assuming 100% yields, which is obviously not the case (albeit both were very mature nodes at release), but it still provides perspective.",hardware,2026-02-10 10:05:18,0
AMD,o4h02x6,"Nah that was 30 series.  20 series launched during the post mining GPU glut of 2018-2019. Suspect this was partly why 20 series was priced horribly originally. We saw the same thing with 40 series, but to an even larger extent.",hardware,2026-02-09 18:07:39,6
AMD,o4f6rrg,"Based on previous AMD slides, AMD didn't believe in the value of adding full RT hardware and matrix cores in consumer hardware. They wanted supercomputers to handle the calculations, then inject the data into games via cloud processing. That totally panned out.   (Honestly, this has merit, as RT/PT on-device is taxing, so very complex calculations could be offloaded in future, but on-device hardware should not have been neglected either)  I think Sony and Microsoft pressed them to improve hardware RT and dense geometry performance for next-gen consoles, which were in conceptual development 4-5 years ago, and becoming more finalized (regarding hardware requirements) within the last couple of years. High performing matrix cores were probably a requirement for a high quality temporal upscaler and RT denoiser as well.   Microsoft were known to be on-the-fence regarding AMD partnership for next Xbox, and I'm betting AMD were under pressure to ensure RDNA4 performed, as AMD could show progress to MS and extrapolate performance based on AMD projections of RDNA5. Must have met Microsoft's requirements.",hardware,2026-02-09 12:19:41,8
AMD,o4fazf4,"DLSS1 was terrible, as it tried per-game frame AND object inferencing. So, intead of trees with leaves and branches, you got this clumpy malformation of trees with a bunch of green colors that were supposed to be leaves that looked like something from 10 years prior. The upscaler also wasn't great as Nvidia was trying to make the tensor cores do too many things at once.  DLSS2 was where Nvidia turned the corner. That actually started with shader-based DLSS 1.9 that debuted in Control. This moved to a generalized training algorithm based on temporal multi-frame accumulation (TAA-based). Image softness and ghosting were still issues though, so AMD's contrast adaptive sharpening often provided better results when combined with a basic upscaler.  DLSS3/4/4.5 continued improving upscaler algorithms and models to where we are today. It took years to get to this point because missing pixel information was an issue when upscaling images. AI/ML has helped solve that, regardless of how anyone feels about AI slop.",hardware,2026-02-09 12:49:55,5
AMD,o4etz3w,"It really wasn't any better than what FSR2 was after it, and given that everyone absolute hates that one, 'horrendous' covers it quite well. I bought a 2080 for way too much money when Turing launched and was really disappointed about all those features for the longest time. Ended up playing stuff like Metro Exodus without DLSS because it just looked like ass.",hardware,2026-02-09 10:29:32,4
AMD,o4f1w9h,"No, AMD SAM does only work for AMD CPU+GPU but it's a better version of Resizable Bar which already exists. Other companies don't seem to focus as hard on it. Far as I know they've never gotten backlash for SAM.",hardware,2026-02-09 11:41:15,1
AMD,o4elw5j,Right? Capitalism do be capitalism,hardware,2026-02-09 09:09:26,4
AMD,o4e6kzn,It doesn't matter which company. The first will always rip us off if there is no second or even third. Intel taking far too long to release anything in high-mid to high-end range and AMD looks like it wants to be complicit and do the bare minimum to grab some cash while they can.,hardware,2026-02-09 06:43:26,3
AMD,o4fgnvw,"AMD has been in Nvidia's position on the CPU side for most of the last 26 years. It's been pretty good actually, CPUs are only as good as they are because of them.",hardware,2026-02-09 13:26:29,-7
AMD,o4eye7e,I'll stay for another 1-2 years but it fits with the rumors of new Gen cards in 27/28 and I'll take a close look on promises made and promises fulfilled. Nvidia is on a better track I have to say but there's time,hardware,2026-02-09 11:10:38,2
AMD,o4g57nu,Why are you pointlessly responding to every single post I make in this thread?,hardware,2026-02-09 15:40:51,-3
AMD,o4scxds,"Navi 1 was good from start to end and Navi 2 also but a letdown later on. Navi 3 has been a disappointment and Navi 4 is good again. I too hope V5 will be better and stay that way, I won't be a sucker for another time",hardware,2026-02-11 12:20:57,0
AMD,o4sdg62,"First of all, the AM4 support was for enthusiasts and early adopters not for the normal consumer. I did upgrade and so did many others.  The 5700XT got all the features Navi 2 did get besides Raytracing but that hardware not software. What didn't they deliver with that card? FSR 2, SAM, HAGS, new adrenaline software were given to Navi 1 users",hardware,2026-02-11 12:24:40,0
AMD,o4f2h2r,"The problem I have is that they could lose interest at any time and we wouldn't be pretty much stuffed. But yea it's getting better, you just have to make a report and send it to Nvidia if you want something fixed.",hardware,2026-02-09 11:46:00,4
AMD,o7cf5zx,3.4% on steam and growing,hardware,2026-02-25 15:54:31,1
AMD,o4eyo9n,"That's my point, I know about the injection method and I could do that but why tinker with that stuff when it could be officially supported and everybody be happy?  I'll probably do that when I have more time to tinker but for now I'm good. Thank you anyways for making sure people get to hear about that :)",hardware,2026-02-09 11:13:09,6
AMD,o4ode3m,"I'm not so sure about that, the only ones growing demand are bussineses trying to implement AI tools on their operations, and the first ones that tried to do it are seeing no increases in efficiency (or even slight decreasesin some cases)  The market is also saturated with AI tools, solutions and LLMs for consumers, and demand isn't growing on that side, which is the only one that will continue with the same demand after a hypothetical bubble burst  Demand was increasing as well just before the dotcom bubble, and everybody knows what happened and how it affected both bussineses that were part of the dot com bubble AND their partners, which heavily hindered the implementation of 3G technology on some parts of Europe because ISPs were affected by that burst  Nvidia would be the equivalent of the ISP here",hardware,2026-02-10 20:16:31,1
AMD,o4e2u5j,"I dont see this level of negativity of Nvidia users of a 3090, who spent over a thousand on a GPU , and on the next generation theres a new frame gen feature that they couldnt use. Likewise for users of 4XXX nvidia series who got locked out of multi frame gen.",hardware,2026-02-09 06:11:10,-3
AMD,o4fg97v,"> There's not enough negativity towards their dogshit treatment of their customers.  There is way too much mindless negativity towards a company that doesn't actually do the thing you're claiming here. Refusing to release bad tech that people shouldn't be using is the exact opposite of ""dogshit treatment"".",hardware,2026-02-09 13:23:57,-2
AMD,o4rha9j,When they were calling it after islands in pacific. Tahiti i think.,hardware,2026-02-11 07:40:15,1
AMD,o4le3qj,AMD went open source because they knew they could outsource most of the work to free labor volunteers.,hardware,2026-02-10 10:46:20,10
AMD,o4qlrp8,yeah sadly both console makers chose amd,hardware,2026-02-11 03:33:01,3
AMD,o4hntr9,"That's a very fair point -- and I actually used 3D Vision back then for a while, although on a DLP projector, so I should have thought of it. Of course, it doesn't change the underlying idea.",hardware,2026-02-09 20:01:02,3
AMD,o4qrmm4,"> G-Sync was a proprietary variant of VRR that happened to be first to market  > Making needless proprietary variants and creating needless segmentation is not.  To be fair to Nvidia, the segmentation wasn't necessarily ""needless"" once you know the history and origin of G-Sync.  Basically, G-Sync was born out of a side-effect of using the PSR feature of the eDP protocol as well as the Variable VBLANK flag.  Neither of these features existed in mainline DisplayPort at the time VESA had no intention of doing so.  It took nearly **2 years** after G-Sync was announced for VESA to announce their implementation into the DP1.2a spec and still more time after that because the LFC logic had to be built into the GPU (1st Gen G-Sync actually had LFC built-in to the custom ARM SBC they were using as the Display Controller+TCON).  I actually posted about this 9 years ago:  https://www.reddit.com/r/nvidia/comments/48bsfp/why_are_monitors_with_gsync_so_much_more/d0io99d/",hardware,2026-02-11 04:12:09,6
AMD,o4hqfyd,"VRR was a technology that was on its way to being standardized... primarily because some VESA members wanted to save energy in embedded devices by skipping updates in static display situations.  I doubt that the impact on high end gaming monitors would have been remotely similar without Nvidia's push.  > If VESA had been first, we hadn't had the absolute mess that G-Sync spawned.  If anything, I'd call the early-to-mid-term ""generic VRR"" monitor space an ""absolute mess"". There were **tons** of displays releasing even many years after G-Sync hit the market (with a pretty damn solid initial implementation) that were almost unusable in actual VRR scenarios due to a complete lack of adaptive overdrive.  > Making new tech and new standards is fine. Pushing for adaptation is fine. Making needless proprietary variants and creating needless segmentation is not.  Obviously, all else being equal, an open standard is preferable.  But all else isn't equal. As I said in my previous post, I'm under no illusion that any of these companies do anything for any reason other than to make money. And a company can justify investing a lot more into research, development, and - yes - a marketing push when they think they can subsequently benefit from a proprietary ecosystem.  The other option is to invest less and focus on staying competitive, and sell your products at a cheaper price. I think a healthy market can benefit from both approaches, but as an enthusiast I find the former more exciting.",hardware,2026-02-09 20:14:34,7
AMD,o4ibeh3,Sadly I've had to turn off my g-sync because it causes my monitor to go black randomly from time to time sometimes on a consistent basis.,hardware,2026-02-09 21:58:24,3
AMD,o4ljwrv,"If you completely ignore all the evidence to the contrary, sure. They both only care about AI. Take the green coloured blinkers off dude.",hardware,2026-02-10 11:36:39,-4
AMD,o4gry18,"https://www.pcgamer.com/hardware/graphics-cards/nvidia-reportedly-wont-release-any-new-rtx-graphics-cards-this-year-and-the-rtx-60-series-is-said-to-be-pushed-back-too/  It's not coming from Jensen's mouth directly, but you'd have to be a little dumb to think that this isn't the truth.",hardware,2026-02-09 17:28:58,-17
AMD,o4hggmm,Blocking would be actively preventing the use of FSR4 on the devices. People are using the leaked version just fine using optiscaler.,hardware,2026-02-09 19:24:40,-5
AMD,o4g1odj,They should just quit altogether if their entire existence is just being a victim like Taylor Swift.,hardware,2026-02-09 15:23:17,16
AMD,o4hx0d2,"Plain statements like ""Nvidia has infinite money due to being a monopoly"", ""monopolies are enabled by the public and by the government"", and ""AMD makes better cards and supports their cards better"" are not, have never been, and likely never will be ""misinformation"". These are very basic facts.",hardware,2026-02-09 20:46:56,0
AMD,o4skeje,"> More to the point, there are more manufacturers than ever in the space, and with multiple models. MSI, ASUS, Lenovo and Valve are the big names but there's also GPD, Ayaneo, AYN, OneXPlayer etc. On top of that, pretty much all of them have multiple models in the market.  So... there's a lot of fragmentation. Not any success stories, but a lawless realm of warring states. Great.  > What? We sure do.  This doesn't make any sense with that statement Intel put out a week or so ago. Either Panther Lake isn't as good as is being claimed, or someone is up to no good.",hardware,2026-02-11 13:10:08,-2
AMD,o4gct9s,"i mean technically the performance doesn't even matter, it could be a lot worse than what it is and it still needs to be released, because we are living in a dystopian temporal blur reliant development nightmare.  so to get a less shit experience you need a ""good"" ai upscaler given what a blurry unusable mess it is otherwise. fsr4 and dlss 4.5 is still shit compared to true native.  true native means games properly designed to not have temporal blur reliance and to use proper aa like msaa see half life alyx as an example:  [https://www.reddit.com/r/FuckTAA/comments/1i2li6s/this\_is\_half\_life\_alyx\_it\_uses\_4x\_msaa\_no\_ray/#lightbox](https://www.reddit.com/r/FuckTAA/comments/1i2li6s/this_is_half_life_alyx_it_uses_4x_msaa_no_ray/#lightbox) (no spoilers in the screenshots)  clear crisp extremely clean and artifact free and extremely well running graphics.  that is what true native would mean.  it is a unicorn rarity these days, so even if int8 fsr4 would cut your performance by 20% on rdna2, doesn't matter it may still be by far the best experience sadly.  so yeah absolutely useful regardless indeed and it needs to get officially released as soon as possible.  ideally you want fsr4 int8 all integrated and working fine with proton and integrated into major games already when the steam machine releases, so the sooner the better.  gotta be some weird conversations going on between amd and valve i guess in that regard lol.",hardware,2026-02-09 16:17:08,7
AMD,o4fvb9i,"The thing is, nothing has changed.  If you read carefully they make no mention of day1 support in that blog  Rdna1 and rdna2 are in maintenance mode branch and will get updates depending on the market needs.",hardware,2026-02-09 14:50:18,14
AMD,o4gkyr9,I was replying to a comment that said RDNA2 is in maintenance mode and I proved it's not. Are you ok?,hardware,2026-02-09 16:55:45,-5
AMD,o4h31k0,"You're right, I was misremembering a bit there; it seems to be quite close visually.   OTOH the framerate hit is worse than I remembered... FSR4 INT8 Performance is even slower than FSR3 Quality / XeSS Ultra Quality in many games, and rarely more than tied.",hardware,2026-02-09 18:21:25,2
AMD,o4h3qhr,"> 20 series was priced horribly originally  Exactly. I think if it would be priced like 1000 series it would be much well received. I mean DLSS, RTX for the price of 1000 series? Sign me in! But sadly that wasn't the case cuz Jason needed a new leather jacket.",hardware,2026-02-09 18:24:37,5
AMD,o4g3kto,> AMD has been in Nvidia's position on the CPU side for most of the last 26 years.  ?  What reality do you live in? What market share do you think their peaked with?,hardware,2026-02-09 15:32:47,10
AMD,o4g8ml7,Does that mean you have pointlessly replied to at least just as many people in this thread to begin with?,hardware,2026-02-09 15:57:09,7
AMD,o4se9nb,"Upscaling was already a big deal when Navi 2 launched and the completely fumbled that and ruined the gen. Took almost 1.5 years to get FSR2 out the door.  Good but only due to big discount. DLSS moat is still a thing. AMD needs to speed up their FSR4 adoption and work to get old games updated.   All indications point to nextgen will be excellent on HW side, but I'm not so sure about their FSR team. What they're doing rn with Redstone isn't inspiring a lot of confidence and NVIDIA isn't going to stand still nextgen especially not when AMD's HW is going to be this good. AMD better anticipate this and not fumble it like with RDNA 2.",hardware,2026-02-11 12:30:26,3
AMD,o4y4alz,">The 5700XT got all the features Navi 2 did get besides Raytracing but that hardware not software.  No it didnt, and lets not pretend that Navi was feature rich in the first place.  >What didn't they deliver with that card? FSR 2, SAM, HAGS, new adrenaline software were given to Navi 1 users  Either using old features because news are lacking or workarounds due to lack of hardware support is your examples?",hardware,2026-02-12 08:02:31,1
AMD,o4f8ly7,"At least this time if they do lose interest the community developers won't be as hampered as they were during the GTX 9xx and 10xx days when NVIDIA locked down the power management firmware so those cards will be stuck forever performing like Intel HD Graphics (or worse) regardless of any optimizations the driver can possibly have because they'll be stuck running at 100MHz or whatever the minimum frequency is, while the newer GPUs have the GSP (basically a CPU on the GPU) to run all the secret spice so the open source drivers can talk to that and not have total dogshit performance which is why we have progress again.",hardware,2026-02-09 12:33:25,6
AMD,o7eqzbr,Of which the vast majority is Steam Deck.,hardware,2026-02-25 22:20:03,1
AMD,o4fgehw,DM me if you want the INT8 files for it to work.,hardware,2026-02-09 13:24:51,0
AMD,o4op3ys,We'll see what happens but if model progress doesn't begin to pick up significantly some short to medium term crash is inevitable.,hardware,2026-02-10 21:11:11,2
AMD,o4e3iom,MFG isnâ€™t as big of a game changer as dlss is,hardware,2026-02-09 06:16:59,16
AMD,o4e3dgv,"Do you actually think frame gen is anywhere near as important as upscaling?  I'd say to most people that is a big fat NO. Upscaling actually improves game performance, not to mention it replaces TAA for its own AA. Frame gen is just frame gen, it guesses what goes between frames to make things look smoother. But it doesn't make 30fps have fast inputs like 60fps, or 120fps, and there'll be visual bugs of course, just depends what you notice.  Hell, I basically never use FSR frame gen or anything, which older RTX can use too btw, and XeSS is also getting it. But I do use either the native AA options or quality upscaling in newer games.",hardware,2026-02-09 06:15:45,11
AMD,o4rjy7r,Was it really that good? Sure it competed and beat 680 (ghz edition) but the architecture was subpar compared to Kepler.,hardware,2026-02-11 08:05:13,1
AMD,o4p6sey,">If anything, I'd call the early-to-mid-term ""generic VRR"" monitor space an ""absolute mess"". There wereÂ **tons**Â of displays releasing even many years after G-Sync hit the market (with a pretty damn solid initial implementation) that were almost unusable in actual VRR scenarios due to a complete lack of adaptive overdrive.  Let's not forget that a lot of the ""generic VRR"" monitors also several other issues that were unrelated to overdrive. For example a lot of them had a really, really narrow update frequency where VRR actually worked. With G-Sync it did not matter what your refresh was, it would also be VRR and smooth. It accomplished this by various techniques like doubling frames at the really low refresh rates.   With a lot of the early FreeSync monitors, we had monitors that would only enable VRR at specific update frequencies, like maybe 48-75 Hz and as soon as you had higher or lower FPS than that, VRR would get disabled. It was awful, and monitors didn't advertise their workable VRR-range either so you had to Google and pray that someone had tested it and written about it.",hardware,2026-02-10 22:35:32,6
AMD,o4jz8dw,> randomly from time to time sometimes on a consistent basis.  ðŸ¤”,hardware,2026-02-10 03:36:13,4
AMD,o4llv0x,i guess thats why Nvidia realeases a bunch of gaming-only features last month?,hardware,2026-02-10 11:51:54,4
AMD,o4gsuf2,"They're not halting gaming gpu production, they wont release new rubin cards this year",hardware,2026-02-09 17:33:11,14
AMD,o4h2on7,Do you know what it means to halt production? Delaying the release of a new generation does not stop production of current GPUs.,hardware,2026-02-09 18:19:45,11
AMD,o4fypgx,"No. That's not how this works, and that's not what happened. This is wholly revisionist.",hardware,2026-02-09 15:08:12,-20
AMD,o4vfmq8,"> So... there's a lot of fragmentation. Not any success stories, but a lawless realm of warring states. Great.  One man's fragmentation is another man's competition.  As for lack of success stories, to use your own words:  **Do you have any source for this whatsoever?**  Steam Deck is the clear success story on multiple fronts.  The Legion Go and ROG Ally sold well enough to get new models.  The Xbox Ally is a maybe, since hardware-wise it's fine but the software side is a mess.  But at least it woke up MS enough to bother to create Xbox FSE.  Think about that for a second though, the company that is currently obsessed with shoving cloud subscriptions and CoPilot down everyone's throats actually built something for Windows for the niche handheld market.  Granted, they probably vibe-coded it....  The competition is ultimately good thing as it has at least forced hardware to be iterated through quite quickly.  Notably, very few handhelds are using re-purposed tablet displays (ie Vertical native) now that the market is large enough.  So even display manufacturers are coming to the party.  We're also now seeing things like VRR becoming more or less ubiquitous.  > Either Panther Lake isn't as good as is being claimed, or someone is up to no good.  The benchmarks are there, and for multiple models now so you get a spread of performance setups.  Seems pretty clear to me when looking at the numbers.",hardware,2026-02-11 21:34:06,3
AMD,o4fy6x9,"Yeah I remember reading this. It was that moment when I realized how important driver updates are for your GPUs. Like I dunno who showed it I think GamerNexus a little footage of the game without a driver, and it was literally unplayable. Like I always just thought a less performance, and not a totally broken game.",hardware,2026-02-09 15:05:32,9
AMD,o4i2yi8,You don't know what Maintenance Mode is.,hardware,2026-02-09 21:16:10,10
AMD,o4h4iq6,"The thing is that its sometimes bugged when you do the straight file swap, if you instead use optiscaler, performance is better, i experienced this myself, when swapping files in specifically fsr3.1 games, not fsr4 games, i would get no performance difference from native, even at performance mode, but then using optiscaler with dlss inputs performance got fixed, and was only slightly slower than fsr3.1",hardware,2026-02-09 18:28:18,4
AMD,o4g52gx,"The only reality available, where AMD has made the absolute best CPUs on the market for the vast majority of the 2000/2010/2020s.  edit: It's wild how very basic facts get you downvoted around here, while blatant lies and weird harassment are deserving of praise. Wintelvidia will never die, I guess.",hardware,2026-02-09 15:40:10,-10
AMD,o4gnftr,"No, it doesn't.  I ask again: why are you pointlessly responding to every single post I make?",hardware,2026-02-09 17:07:26,0
AMD,o4f9ed8,Having experienced my 1070 on Linux near release I can confirm it was a shit situation to be in.,hardware,2026-02-09 12:39:02,4
AMD,o4orkwu,"I actually think we are at a point were model progress is likely to end up not making a difference, because the main problem isn't that the AI solutions aren't efficient enough at what they do, but rather that the use cases that companies have chosen for them are de facto wrong",hardware,2026-02-10 21:22:38,1
AMD,o4rkkm0,I dont think better architecture than Nvidia was the goal in the comment i replied to. In that case i would struggle to think of a AMD card that actually was.,hardware,2026-02-11 08:11:07,1
AMD,o4lm56o,"Are you serious? You can't just keep repeating that while ignoring everything else that is happening right now. They're putting 1 dollar in your left hand and taking 100 dollars from your right hand, and you are smiling about it.",hardware,2026-02-10 11:54:04,-3
AMD,o4fzikz,You only need to look at Arc launch to see how important drivers are,hardware,2026-02-09 15:12:21,10
AMD,o4kna4v,I want to know. Please tell me.,hardware,2026-02-10 06:32:59,-1
AMD,o4hg28h,">where AMD has made the absolute best CPUs on the market for the vast majority of the 2000  At the end of the 2000s, sure. They were merely competitive at the beginning of the decade.   >/2010  Bulldozer would like a word. AMD remained entirely uncompetitive the moment Intel launched Sandy Bridge in January 2011 all the way until Ryzen's release *in 2017*.  >/2020s.  Intel started having to actually compete again, and very quickly closed the gap that AMD created with Ryzen.   So out of the 26 years you claimed, AMD was only top dog for maybe 7 of them.  >The only reality available,  [citation needed]",hardware,2026-02-09 19:22:45,10
AMD,o4g977f,">in Nvidia's position  But I thought according to you, nvidia never made the best GPUs? They just have overwhelming market share which amd never ever had anywhere. Now it's about ""best CPUs"". Hmm",hardware,2026-02-09 15:59:51,7
AMD,o4hgqoe,"Probably because you keep posting ""AMD is the greatest thing since the orgasm and they have always been the pinnacle of everything they do"" fluff pieces which aren't even remotely close to reality, and people keep calling you out on it.",hardware,2026-02-09 19:26:00,10
AMD,o4otbat,What should they change it to then? Robotics?,hardware,2026-02-10 21:30:31,1
AMD,o4rq2bv,Yes you're right sorry xD  Yeah prob has to go back to the ATI days.,hardware,2026-02-11 09:03:51,1
AMD,o4lravn,The fuck are you on about? they are not taking any dollars in this allegory.,hardware,2026-02-10 12:31:25,5
AMD,o4g4uxv,"I'm not ""spamming"" anything ""all over this thread"".  You ignorantly declaring untruths doesn't make them true, nor does anyone pretending to ""agree"" with it.",hardware,2026-02-09 15:39:07,-15
AMD,o4vmp4k,"> You still have yet to provide an actual source for any of this.  ???  There's clearly 2 links in my previous post with industry references.  One which has follow-on links to the ASUS earnings call it's based on, the 2nd with links through to the IDC study commissioned by the Verge.  > You strongly come off as someone who's bought stock in some company involved  Great deflection.  Shows how little you're paying attention when my first comment literally tears into AMD for a dumb decision and now you're suggesting I'm in bed with one of them.  And all over a reddit comment chain over handhelds?  Yikes.  Personally I'm much more of a fan of what SteamOS and Linux can do for handhelds and gaming.  But Valve is privately owned so I guess your narrative falls apart there....",hardware,2026-02-11 22:07:49,3
AMD,o4g5unl,"Can you elaborate? I played Arc on RX 6600, and it was was playable.",hardware,2026-02-09 15:43:57,3
AMD,o4m0296,Only getting critical updates. Which is what RDNA 2 is.,hardware,2026-02-10 13:27:02,5
AMD,o4hvyz1,"> At the end of the 2000s, sure. They were merely competitive at the beginning of the decade.  Incorrect. Everything from the K6 up to the release of the Phenom II was unquestionably great releases.  > Bulldozer would like a word.  The Bulldozer gens are good chips surrounded by horrible misinformation invented by gamers, and will never get the respect they deserve. Surprise, AMD was right on this as well.  > all the way until Ryzen's release in *2017*  Aside from this being false, 2017~2019 are still ""the 2010s"". Don't draw lines where there are none.  > very quickly closed the gap that AMD created with Ryzen  Intel has done nothing of the sort, and they're getting *worse*, not better.  > [citation needed]  Imagine proofcalling me when you're the one making ridiculous claims based on nothing at all.",hardware,2026-02-09 20:41:49,-6
AMD,o4gnbfp,"> But I thought according to you, nvidia never made the best GPUs?  Which they don't. They occasionally make really great GPUs like the 1080 Ti, but by and large AMD/ATI is the better buy.  > They just have overwhelming market share which amd never ever had anywhere.  Which has absolutely nothing to do with quality.  > Now it's about ""best CPUs"".  The subject was never changed like you're pretending. Read the post I replied to.",hardware,2026-02-09 17:06:52,-6
AMD,o4hwczd,"> ""AMD is the greatest thing since the orgasm and they have always been the pinnacle of everything they do"" fluff pieces  Nothing I've written sounds anything even remotely like this. Why the useless hyperbole?  > which aren't even remotely close to reality  You've made it very clear that you don't know this at all.  > people keep calling you out on it  You're being really creepy about this. Are you this person's friend or something?  This was one specific account replying to every single post I made for no particular reason, constantly making weird statements and asking weird ""questions"".",hardware,2026-02-09 20:43:44,-2
AMD,o4lrx50,I give up. Keep smiling while they bend you over.,hardware,2026-02-10 12:35:44,1
AMD,o4gc7zp,As in Intel Arc GPUs,hardware,2026-02-09 16:14:18,11
AMD,o4m208k,Did you even read what I posted?! I gave you AMD statement where they canceled maintanance mode on RDNA2 and RDNA1 and gave you a driver that supports day 1 game optimizations and bug fixes for RDNA 1 and 2!  >This is not the end of support for **RDNA 1** and R**DNA 2.**Â  Your Radeon RX 5000 and RX 6000 series GPUs **will continue to receive:**  >**- Game support for new releases**  >**- Stability and game optimizations**  >**- Security and bug fixes**,hardware,2026-02-10 13:38:02,-3
AMD,o4i20ou,">Nothing I've written sounds anything even remotely like this. Why the useless hyperbole?  Incorrect:  >>The only reality available, where AMD has made the absolute best CPUs on the market for the vast majority of the 2000/2010/2020s.  Fucking lol.   >You've made it very clear that you don't know this at all.  Indeed? Remind me again, what's AMD's market cap compared to Intel's right now? How about post-2010 to 2020?  Oh, Intel absolutely dominated during those periods and AMD wasn't even viewed as competition? Damn.   >You're being really creepy about this.  Imagine saying stupid shit on a public forum and then playing the victim card when people point and laugh.   >Are you this person's friend or something?  Ah yes, now it's a conspiracy.   >This was one specific account replying to every single post I made for no particular reason,   You are posting in a public thread, and saying dumb shit. Not only do people *not* require a reason to do anything, they don't require *your* permission to do so.   > constantly making weird statements and asking weird ""questions"".  By ""weird"" you mean ""has called me out and I have no response to them so I'm going to victim card"".",hardware,2026-02-09 21:11:33,11
AMD,o6t25v0,"Everybody that built with Alderlake/Rapotorlake 4-5 years, specially with Raptorlake got decent rate DDR5 at very good prices which they can swap into these new builds.  There is actually enough reasons to upgrade from Raptorlake to Novalake/Zen6 which will be good chunk faster and more efficient and like 4-5 years is a good enough time span.  The RAM situation could actually be great for these people if it translates to deals on CPU/boards.",hardware,2026-02-22 17:38:05,23
AMD,o6qzyd5,A real (not paper) launch of new equipment in the near future is unlikely. TSMC is much more profitable simply churning out chips for AI accelerators while demand for them is still huge.,hardware,2026-02-22 10:10:57,23
AMD,o6qa604,"Literally who cares? It's a few months difference. If I were in the market for either of these product lines, I would prefer them to release when they're ready than get rushed out the door with a bunch of bugs for the sake of a few months.",hardware,2026-02-22 06:08:02,42
AMD,o6rt47d,Neat. Hopefully it sturdy to be good value for indie gamers in 5 years as used.,hardware,2026-02-22 14:01:30,3
AMD,o6q8x2t,With DDR5 prices more expensive than the actual CPU/motherboard combined I couldnâ€™t care less about Zen 6.,hardware,2026-02-22 05:57:19,32
AMD,o6r9idq,Unless the price of motherboards and RAM drops it seems like I will be skipping the whole of AM5. The games I play are still getting absurd framerates at 1440p on my 5950.,hardware,2026-02-22 11:41:28,4
AMD,o7115n0,"Joke's on me, I got a DDR4 Alder Lake board...",hardware,2026-02-23 21:54:22,3
AMD,o7edv8m,"Me. I have been using an i7-12700F for several years, already have DDR5 and can't wait to upgrade this gen.",hardware,2026-02-25 21:18:18,1
AMD,o6tlx5l,"From AMD's side, they need Consumer products to eat up the worst CCDs that cannot fit the server product. I think AM5 Zen 6 products will launch in 26 fall.",hardware,2026-02-22 19:08:50,22
AMD,o76pyuy,"They just can't switch out the CPU process and GPU process easily. They have foundries, facitilites and equipment made for certain procesess,  CPUs and others for GPU/etc. It's not just changing a recipe.",hardware,2026-02-24 18:57:19,1
AMD,o6ufvmo,"Nobody wants a rushed product, but who says they would be rushed? Originally they were said to launch in 2026 anyway so I simply hope that's still the case.  The last couple years have been dead for new hardware launches so it'd be nice to get something new to geek out on. Also I have a 7700X that Stellaris is already maxing out, I need an X3D chip to deal with it.",hardware,2026-02-22 21:39:01,18
AMD,o6s6xto,"9800X3D and 9950X3D released in Nov '24 and Mar '25 respectively, so if the X3D parts of Zen6 are released closer to the vanilla parts, it might not be much of a practical difference for a lot of enthusiasts.",hardware,2026-02-22 15:15:58,15
AMD,o6s1967,we dont want to wait. the longer they take to release the longer it takes for us to get price drops,hardware,2026-02-22 14:46:44,-4
AMD,o6qmcnn,Lots of people already have DDR5,hardware,2026-02-22 08:00:31,45
AMD,o6qdo5l,You are the most important user in the world indeed,hardware,2026-02-22 06:39:14,21
AMD,o6z56bg,"Will people buy them?   I thought the AM4 platform was still outselling AM5, but maybe things have changed. I got my AM4 CPU and motherboard for â‚¬100 new last year and I don't think any AM5 combination comes close.",hardware,2026-02-23 16:34:25,0
AMD,o6ro1oh,Yea but not all of us.,hardware,2026-02-22 13:31:26,20
AMD,o6qwhr3,"I'm hoping mine doesn't die, I paid $350 for my kit, replacement cost is now $800  a lot of people still on ddr4 didn't upgrade because a year ago they said ddr5 was still too expensive and so now they definitely can't afford it",hardware,2026-02-22 09:37:51,2
AMD,o6r9o87,They aren't going to be buying a 3% better CPU in large numbers though.,hardware,2026-02-22 11:42:54,-4
AMD,o6qxk2a,Since it's currently the most upvoted comment they aren't alone with that opinion.,hardware,2026-02-22 09:47:59,-11
AMD,o6roumk,You snooze you lose. DDR5 was real cheap not long ago.,hardware,2026-02-22 13:36:25,-17
AMD,o6rq9i2,RAM usually has lifetime / 10 year warranty,hardware,2026-02-22 13:45:03,3
AMD,o6qyb8z,It was cheap last year tho,hardware,2026-02-22 09:55:17,3
AMD,o6t2mhg,"It is going to be a lot better than 3%. Both of these are huge increase in core count. Specially on Intel side, going from Alderlake to these will be a big upgrade in IPC too.  On AMD side, products like 9900X are not very popular because of their bad price/performance but with Zen6 for example, you may get 12 cores as a Ryzen 7 with all of the cores on one CCX which will be a big performance uplift by itself.",hardware,2026-02-22 17:40:16,9
AMD,o6rabyd,"A lot of people are willing to upgrade from existing Zen 4/5 products due to the larger CCX of Zen 6.  The double CCX CPUs have scaling problems in gaming and some other workloads. But that doesn't mean they cap out at 8/16 cores and threads scaling wise.  So getting 12 cores in a single CCX is the only real upgrade path for them if they want higher core count. Rumors are that the cache sizes are increased as well. So there might be a decent low thread performance uplift as well, at least if cache latency has been kept in check with the increase.",hardware,2026-02-22 11:48:39,9
AMD,o6y6pvb,That's irrelevant if you have an AM4 platform. I really don't know who buys hardware and leaves it in a closet for years.,hardware,2026-02-23 13:39:20,3
AMD,o6sadby,I bought 192gb for 340 last year. You could get good ddr5 for 110 bucks,hardware,2026-02-22 15:32:51,5
AMD,o6ugnfi,"Exactly, though there's no guarantee what they send back will be the same spec. Or even a matched kit. But at least it'll be something that runs.",hardware,2026-02-22 21:42:50,2
AMD,o6s8ww9,"Zen5 non-3D had a disappointing performance increase over Zen4, so if Zen6 is anything like the greater increase from Zen3 to Zen4, yeah I suspect a lot of people might be interested in finally pulling the the trigger. At least people who have for example a 6c Zen4 non-3D and don't need new RAM.  I'm very excited for raising the baseline for cores in the CCX, but I'm not sure most people care except for enthusiasts. We've seen a real stagnation for how many cores are needed, both for games and for regular usage. If you asked me during the Zen3 generation (2020-2021) my guess would have been 8c would be a bottleneck in a very short time. Yet it isn't, even 6c performs quite well with just a tiny penalty in a minority of games. I'll take the win of raising the baseline, as Zen6 will definitely lead to raising the average number of cores for consumers, which will probably lead to more powerful software and innovation in the long run, but I'm not at all convinced most consumers will feel they need the higher core parts next year.",hardware,2026-02-22 15:25:32,6
AMD,o6rdo84,"The IO die and also the way the cores communicate with said IO die have been massive improved. From what we know Iâ€™d expect 20% minimum, Iâ€™m thinking fully tuned we could see 30/40%.",hardware,2026-02-22 12:16:50,8
AMD,o6ui4yl,"I literally bought a 9700X instead of 9800X3D (for 200$ less) a couple of months ago just to upgrade to Zen 6 X3D a bit later, those very fast 12-Cores one 1 CCD are way too spicy.",hardware,2026-02-22 21:50:19,1
AMD,o6y7u7r,"Upgrade while prices are low, itâ€™s a very simple thing to do and with how volatile the market has been since crypto and covid this was always going to happen.",hardware,2026-02-23 13:45:50,-1
AMD,o6uk1j1,"Nailed my use case. Early adopter on a 7700X, really could use an X3D chip to tame Stellaris but Zen 5 wasn't even a proper upgrade from Zen 4 so I'm ignoring the 9800X3D.   Am hoping Zen 6 makes up for four years of development time, along with a much improved non-first generation IMC. Zen 6 has all the potentials going for it to drop a nice performance improvement before factoring in the X3D upgrade. The extra cores & increased cache sizes would just be icing on the cake.",hardware,2026-02-22 22:00:08,4
AMD,o6tsple,"I find this overly optimistic. What do we know that makes you expect 20% minimum? If there was around that percentage of a latency improvement in what we see in strix halo vs zen 5 DT, that could be a good baseline, but we see nothing of that sort.",hardware,2026-02-22 19:42:14,3
AMD,o6uhi15,"Haven't seen anything concrete, but I do expect a substantial improvement. Zen is still using a first-generation DDR5 controller designed five years ago. Even though AMD has already designed multiple generations of DDR5 controllers for EPYC & Threadripper since. First-gen IMCs have always always had low-hanging fruit left on the table, optimizations, and fixes that second gen IMCs benefit from.",hardware,2026-02-22 21:47:05,3
AMD,o6uq1px,Strix halo doesnâ€™t have massively high clock speeds and the ram it uses has poor latency.,hardware,2026-02-22 22:31:34,1
AMD,o6uqmzj,Mem latency isn't [even much different](https://chipsandcheese.com/p/amds-chiplet-apu-an-overview-of-strix) than Strix Point.,hardware,2026-02-22 22:34:45,2
AMD,o6urtg4,But itâ€™s much different to zen 5 with DDR5,hardware,2026-02-22 22:41:09,2
AMD,o6ut3ej,"Fair. That's a good point, I forgot about that.   But if it's also not faster than Strix Point, which uses similar speed LPDDR5....   Also, the fabric links are still only running at \~2GHz on both Strix Halo and Zen 5 DT.",hardware,2026-02-22 22:48:07,2
AMD,o6gb7ay,"The majority of end user computing these days doesn't need high memory bandwidth. Think about who actually buys the computers, it's not gamers or computer enthusiasts who could maybe benefit from more memory channels. It's businesses buying computers for their employees. Work a day in corporate IT and you'll see the vast majority of things are done through websites/webapps which aren't particularly demanding and served perfectly well by midrange modern hardware.   Intel/AMD do offer platforms with more memory channels for those that do actually benefit from that sort of thing, but they have no reason to increase the baseline from 2 which would increase cost for no real tangible benefit for the majority of users.",hardware,2026-02-20 17:05:25,124
AMD,o6gi6ut,"Market segmentation. They want you to pay if you want or need more memory channels. Want or need 4 channels? Get normal Threadripper. Want or need 8 channels? Get Threadripper WX. Want or need 12 channels? Get Epyc.  Sadly the GPU department of AMD is not as forward thinking as they were in the past when they developed HBM or forward thinking architectures like GCN. In addition to Strix Halo they could have released a mini M300A for SP5, which could have had access of up to 6 TB of normal DDR5 memory with 460-614 GB/s bandwidth. The AI bros would have been all over it.",hardware,2026-02-20 17:37:57,26
AMD,o6gmsmk,Bandwidth? I thought latency was **the** bottleneck these days. Does it improve with more memory channels?,hardware,2026-02-20 17:59:01,29
AMD,o6gs4h8,"Modern M-series macs do this of course, with memory bandwidth speeds ranging up to \~819 GB/s for an M3 Ultra.   Strix Halo (e.g. AMD AI Max+ 395 series) APUs do that as well, though only \~256 GB/s. Rumors abound that the 2027 Medusa Halo followup will feature LPDDR6 RAM, with extreme configurations topping out at 384 bits bus width, and speeds that can get close to a sooner arriving M5 Max.  For most tasks, the cpu is perfectly fine with 50-100GB/s of bandwidth, and you're typically better off with more memory, a better graphics card, etc. Computing using huge datasets, scientific computing, AI, graphics, all of these can benefit from more memory bandwidth.  The other reasons not to? Cost, and lack of upgradability. SOCAMM2 modules might be a solution (though will be initially a premium and unavailable), but typically there just isn't the performance reliability with socketed RAM vs soldered. AMD tried with Strix Halo, but concluded they could only offer a solution that involved soldered RAM, like Apple. And cost -- far more bus traces, more complex design, more electrical hardware to stabilize signals, likely more extreme requirements on the physical position of the memory on the mother board... do you want to pay (say) $100 extra at retail for this if you don't really need it? (And that's just the mobo; an APU that makes use of this is going to be huge and therefore extremely expensive, whether it's an M3 Ultra, or a Strix Halo 395+ or a Grace Blackwell in the DGX Spark).  So if you really want it, you can pick up a Framework desktop or a Mac Studio today. But will their relatively high price be worth it if your application doesn't need it?",hardware,2026-02-20 18:22:53,8
AMD,o6gp1e0,"Latency is the issue causing memory bottleneck, not bandwith. If you are doing lots of small random accesses to different memory locations, which is typical in most normal applications, bandwith isn't going to help you. Having more cache would help.",hardware,2026-02-20 18:09:11,17
AMD,o6hfnp3,"IO costs a lot because it's on chip edge, and the vast majority of workloads aren't missing membw but just bottlenecking on fetching random data that additional channels would do nothing for",hardware,2026-02-20 20:13:17,8
AMD,o6gfmpd,On one hand the tasks that the average consumer is busy with actually aren't memory bottlenecked. On the other hand it's also simply product segmentation. Want memory? Buy our 3k CPU,hardware,2026-02-20 17:26:00,8
AMD,o6h8fr0,"It's market segmentation deliberately created by Intel, which AMD copied later on.  The first release of Nehalem (Bloomfield) was triple-channel with plenty of PCIe lanes, fully in line with how the PC was evolving up to that point.    Then they released a crippled toy version (Lynnfield) which had only two channels and a pathetic number of PCIe lanes, retroactively creating the HEDT market segment.  That HEDT segment became progressively more and more expensive, reaching a point where with one platform, your PCIe lanes were crippled unless you spent $1000 on the CPU.  AMD, in the Bulldozer era, still had a respectable number of PCIe lanes, though still two memory channels.  When they made AM4, they adopted the toy computer model with too few PCIe lanes, and still just two memory channels.  Threadripper was then created as a proper PC, in line with how things were going before Nehalem.  More memory channels, more PCIe lanes.  It wasn't priced too badly, either.  But after Intel stopped being able to compete in HEDT, AMD raised the floor of entry way too high.  I see no sign of a proper consumer PC platform being released any time soon.  The E-core spam strategy that Intel is following also suggests they have no intention of competing in HEDT again, either.  Which means AMD is unlikely to fix TR to have a sensible entry point.",hardware,2026-02-20 19:38:14,12
AMD,o6gnwth,"The short answer is cost and low ROI for the majority of consumers. Most consumers donâ€™t benefit much from the additional bandwidth, and if you do, you can get a quad or octa channel HEDT system.   Additionally, large cache (X3D) chips are pretty effective at reducing memory bandwidth demands.",hardware,2026-02-20 18:04:07,6
AMD,o6h9626,"The memory bottleneck is memory latency, not memory bandwidth. Adding more channels does very little to help with latency, and it adds a whole lot of cost.  Bandwidth is very helpful for some workloads, like AI (when running it on CPU), but most workloads don't benefit from it. People who need lots of bandwidth go buy a server grade CPU with more memory channels to fit their workload.",hardware,2026-02-20 19:41:45,7
AMD,o6idwab,They do that.  They charge a huge premium for it that no one wants to pay.  Look up AMD Threadripper and Intel Xeon W. They both support 4-channel on the lower-end models and 8-channel on the higher-end models.,hardware,2026-02-20 23:07:34,6
AMD,o6kk58i,"I don't believe that ""so many"" consumer computing tasks are actually memory bottlenecked.",hardware,2026-02-21 08:19:55,4
AMD,o6iggsb,The cost in silicon space seems significant. https://www.pcmasters.de/system/photos/20393/full/Intel_Core_i9-13900K-Die_Shot.jpg  Though maybe they could shrink the integrated GPU for that or move it off-die?,hardware,2026-02-20 23:22:00,3
AMD,o6mqsyl,Had a couple of old I7-9XX builds that had triple channel memory. No idea if it had better performance than the dual channel versions.,hardware,2026-02-21 17:27:23,3
AMD,o6gbfn6,"Ddr5 is 2 sticks 4 channels, they were solving it before Ai became a thing,  edit to add: the real bottleneck is ram itself, it takes many cycles to store/read data from it.",hardware,2026-02-20 17:06:30,7
AMD,o6glnf8,"It costs more for them to make for a niche market. They don't see an ROI for making a more costly and complex CPU, or they would have done it. It is a niche market.",hardware,2026-02-20 17:53:49,2
AMD,o6ige9q,"Because 95% of the market ain't got a need for it and they ain't gonna pay the extra, period.",hardware,2026-02-20 23:21:36,2
AMD,o6k0gnv,"Because more channels implicitly come with much larger cost. You have basically hundreds of pins on each RAM, you need to have them also on the CPU - so the CPU have to come with ~200 additional connections - larger packaging, more DRAM controllers inside CPU - again additional cost, then you have much more complex connection routing on the motherboards - you need more layer PCBs. So if you would have 4 channels instead of 2, you would basically increase the cost on CPUs and Motherboards by a substantial amount, an most people would probably dont even use all those 4 channels, especially at the current prices of RAM. Also since DDR5 technically each stick have effectively 2 independent channels in them.",hardware,2026-02-21 05:21:57,2
AMD,o6r3t5g,> Since so many tasks nowadays are memory bottlenecked  no,hardware,2026-02-22 10:47:39,2
AMD,o6hsqjj,"GPUs are also instructive as to why not though, they are running highly parallelized and scalable workloads where memory bandwidth is the limiting factor; this is also why the AMD APUs scale so well with ram frequency  The average workload being put on the CPU/memory is random comparatively, you're more likely to encounter bottlenecks related to latency or capacity than the system bottlenecking because peak bandwidth is too low",hardware,2026-02-20 21:17:59,2
AMD,o6gjkea,"DDR5 is 2 channels a stick. So 2 sticks 4 channels. The answer is cost. Ram takes a lot of pins because it's a parallel bus and they are hard to route because serpentine and high speeds. If you add more, it's going to cost more. Who is going to pay more?  And what memory bottleneck? Gigantic monolithic RAM consuming tasks are pretty rare. Except for AI... Most problems that can be solved with throwing more sticks of RAM at something can be solved the same or better by throwing more RAM attached to another CPU and you get clustering benefits. And for when it does make sense, you can orchestrate RAM channels per CPU but then use the CPU to CPU channels like on Xeons to sync memory and tasks. For example look at the old HP DL980s (I think) that had 4 Xeons in them with 160ish cores. Then each had a literal bucket of RAM. You loaded things into the RAM and CPU but if you needed more, CPU1 could use CPU2s RAM, just very slowly. So it did make lots of interesting use cases.",hardware,2026-02-20 17:44:23,2
AMD,o6l9kdi,i've been down this rabbit hole before. the short answer is cost and market segmentation - quad-channel adds pins and motherboard complexity that most consumers won't pay for. what helped me was looking at the [Corsair Dominator Titanium DDR5](https://www.amazon.com/s?k=Corsair+Dominator+Titanium+DDR5&language=en_US&tag=bestdeals202f-20&ref=as_li_ss_tl) or [G.Skill Trident Z5 RGB](https://www.amazon.com/s?k=G.Skill+Trident+Z5+RGB&language=en_US&tag=bestdeals202f-20&ref=as_li_ss_tl) which hit really high speeds on dual-channel. those kits can push 8000MHz+ which helps with bandwidth. also Intel's non-K chips lock memory overclocking which forces people to buy their higher-end stuff. AMD's been more flexible with memory on their AM5 platform. for serious bandwidth needs Threadripper exists but that's a whole different budget tier.,hardware,2026-02-21 12:22:24,1
AMD,o6nelos,"A lot of incorrect and incomplete answers.   The correct answer is we do. DDR5 has two channels per DIMM, one stick of DDR5 has the same bandwidth as two sticks of DDR4 (unless you have an 8gb stick, which only has one channel and is basically high latency DDR4). It's why memory controllers struggle with four sticks of DDR5, it's the same bandwidth as 8 sticks of DDR4. The architecture sacrificed some latency for double the bandwidth. It goes great with the high number of threads modern CPUs have.   It's why latency is also the big bottleneck now.",hardware,2026-02-21 19:25:28,1
AMD,o6rf8hy,Memory bandwidth isn't really a issue for most people. For home uses memory latency is important but its still pretty much fine ddr4 and ddr5 both have good enough latency that most people dont notice. For example their is a basically ps4 pc kit that uses the apu and memory with parts of it disabled that people could buy and while it works it kind of sucks for home applications even though it has about 450 GB/s bandwidth while ddr5 is normally at about 50 GB/s the much worse latency just ruins it for many applications. So most home uses dont really need more than 2 channels of memory and memory channels are really really expensive to build into the chip so its jot reallt worth it for quad or octa channel memory controllers for a chip that 99% of the time is gonna be paired with at most 2 sticks of ram but ia often just using 1. Like if we could only put 4GB of ram on a stick then maybe we would need the ability to put 8 sticks in for consumers.,hardware,2026-02-22 12:29:32,1
AMD,o75ejdd,"I agree. Smartphones, handheld consoles, handheld PCs and standalone VR headsets are in desperate need of higher memory bandwidths. It would greatly help to have at least 280 GB/s for the whole SoC. Unfortunately, 75-110 GB/s for the whole SoC is definitely not enough. This is why Xbox One sucked so much, with its budget DDR3 memory (+very low number of TMUs and ROPs). It was awesome how much faster was the One X! I wish the OG One was already like One X in 2013!  RTX 5090 has really wide memory bandwidth and a substantial amount of memory, but it is very very costly unfortunately.  But also, memory bottleneck quite often really means too high memory latency, instead of too narrow bandwidth. Putting memory closer to the CPU/GPU/NPU definitely helps with latency.      Something is beginning to move in the right direction, because of AMD's Strix Halo, but that was 1 year ago. I've hoped for more devices using it. Intel's Panther Lake is having higher memory bandwidth than ever.",hardware,2026-02-24 15:24:13,1
AMD,o6kfk54,"how many consumer tasks scale with memory bandwidth so hard?  the general question is: do we have enough bandwidth to feet the max core chips on consumer platforms well enough?  and thus far that seems to still be true.  there is ONE task, that is EXTREMELY memory bandwidth limited though, which is apus on desktops.  so if amd would want to sell decent performing apus in a socketed desktop platform in the future, then 2 channels would be very limiting.  more likely however amd won't care about that and enjoy just selling higher performance apus with garbage soldered on and also non ecc memory instead.  and for all the rest we will see another bandwidth doubling with ddr6 coming with... am6 in many years though.  which should be good enough to feed things mostly, which would be 32 core at least zen8 chips.  32 core zen7 on am5 will be interesting however if there would be some bandwidth limitations in tasks happening then with just ddr5 and 2 channels.  \_\_\_  also the chipset doesn't matter. the chipset isn't part of the conversation literally. the cpu communicates directly with the memory. the chipset has no say in it. IN FACT you sell amd cpus without chipsets, as the i/o from the cpu is enough.  the thing, that they'd need to change is the io-die and the motherboards.  and io-dies are a lot cheaper to change than core chiplets, because they are on older nodes generally.  but yeah the one bandwidth issue, that exists is apus and they probably will just show us the middle finger anyways in that regard.",hardware,2026-02-21 07:35:17,1
AMD,o6gr741,"Well, I canâ€™t say that Iâ€™m more knowledgeable but I believe you missed a few key points, thereâ€™s something as memory bus for 2 channels up to 12 channels, also when looking at SoC among the cores there is cache, memory bus(for Apple sheep unified memory), display engine, NPU and many other IP blocks, so it is all about configuration and use case(labeled for home, pro or corporate use).",hardware,2026-02-20 18:18:45,-1
AMD,o6h32uf,Even gamers arenâ€™t bandwidth limited but latency so itâ€™s sorta pointless even on the high end most of the time,hardware,2026-02-20 19:12:42,50
AMD,o6gpf7z,"Yup.  People complain about the base storage options on Apple products too, ignoring who the majority customers are: people who donâ€™t need it.  Apple sells bulk orders of base iPhones to corporations who use them as portable scanners, pos devices, etc. they install a singular app and lock it down so it only runs that app.  Not to mention all the people who literally install WhatsApp, Facebook and thatâ€™s it.  They donâ€™t need 1TB of storage.   The only reason the base amount of storage, memory or memory bandwidth, cpu cores bumped up is because manufacturing changes. You canâ€™t offer less without charging more because youâ€™d need to keep manufacturing processes running for this use case. Cheaper to up the specs and use binned chips than to keep things exactly the same.  If there was an iPhone sold with 128GB storage, people would buy it. Itâ€™s just cheaper to set the iPhone 17 to 256GB and have a tighter supply chain and more volume on specific NAND chips.  People forget how basic so much technology usage actually is.",hardware,2026-02-20 18:10:54,23
AMD,o6glhmk,"Even if you don't use a webside/webapp moving on scales of ""you have some thousands of employees hired"" makes every hardware upgrade a careful consideration. For a dev job running Visual Studio on a 8-core CPU would be ideal, but usually even on a virtual machine every instance gets assigned 4 cores/8 threads. Having any SSD would do wonders for any kind of IT work due to fast responses, but even on VMs they use HDD for storage which is not ideal.  The cost of hardware when a signature means thousands of units it's harder than what most people thinks.",hardware,2026-02-20 17:53:06,2
AMD,o6smjzn,"I have a threadripper (2950x) and while the increased channels and increased threads are theoretically nice to have theyâ€™re problematic for latency critical uses. The ram, CPUs, and pcie buses are bifurcated so thereâ€™s massive latency for anything having to cross over the infinity fabric. Games have massive micro stuttering and crashes, for example.   Intel has done hpc without bifurcation but it cost much more money for the same performance and needed almost 2x the power.   The other problem with quad channel memory is that quad channel kits are inflated. You have to roll the dice and hope two dual channel kits work well.  The threadripper is relegated to the home lab.",hardware,2026-02-22 16:26:32,3
AMD,o6hdfur,"No, it's the opposite. It generally gets harder to keep stable clocks / tight timings as you add channels and DIMMs.   Memory latency has been the major bottleneck since the dawn of computing, but bandwidth can help you hide some of it.",hardware,2026-02-20 20:02:25,28
AMD,o6goj0g,Well the only thing more channels do is transferring more data per cycle.,hardware,2026-02-20 18:06:54,5
AMD,o6h37bx,Probably gets worse in terms of max memory speeds supported   So likely a downgrade for most people,hardware,2026-02-20 19:13:17,1
AMD,o6o95hw,"With Nova Lake, itâ€™s rumored to have 48 PCIe lanes, which is still well below the 64/80/128 of a workstation/Threadripper, but is at least ~double the typical 24 lanes of normal desktop. (Not 100% those are counting the same way though.) Core count / multithreaded performance _should_ increase quite a bit as well with the doubling relative to Arrow Lake.  I _wish_ the dual compute tile part would have a dual channel IMC directly in each compute tile for quad channel total, relatively like what theyâ€™re doing with Granite Rapids or AMD did with Threadripper before they adopted the IO die, _but_ every leak/rumor has been pretty unanimous thatâ€™s not the plan. Wouldâ€™ve been a straight forward way to get something mostly HEDT like.",hardware,2026-02-21 22:06:39,2
AMD,o6s6wdz,"> I see no sign of a proper consumer PC platform being released any time soon. The E-core spam strategy that Intel is following also suggests they have no intention of competing in HEDT again, either. Which means AMD is unlikely to fix TR to have a sensible entry point.  Problem is motherboard costs post PCIe gen 3.  Both gen 4 and gen 5 push the cost of motherboards to the point where people aren't going to be happy paying for stuff they don't need.",hardware,2026-02-22 15:15:46,1
AMD,o6gl6jf,"It's still just 128 bits total, same as it's always been. Also, ddr5's two sub-channels aren't full channels by themselves. It's not exactly incorrect to call it 4 channels, but it's not exactly correct either, and you'll find just as many specs calling it 2 channels.",hardware,2026-02-20 17:51:41,27
AMD,o6gmg7i,But aren't the 2 channels half width. 32 bit on DDR5 vs 64 on DDR4 and previous DIMMs?,hardware,2026-02-20 17:57:27,13
AMD,o6o2bhk,"I don't think that's correct, these channels(or 'subchannels') are half the width.",hardware,2026-02-21 21:30:24,3
AMD,o6oakl8,">Apple sells bulk orders of base iPhones to corporations who use them as portable scanners, pos devices, etc. they install a singular app and lock it down so it only runs that app.  Why would anyone use an iPhone for something like that? Makes no sense. IPhone is a high end consumer device.",hardware,2026-02-21 22:14:23,4
AMD,o6hkjvu,I call bullshit on the   >people don't need 1 TB of storage   Putting little storage in is a strategy to get people to sign up for cloud storage. If people really didn't need 1 TB of storage there would be no market for cloud storage after all...,hardware,2026-02-20 20:37:30,19
AMD,o6l5rps,"If you factor in productivity gains for those thousands of employees, the little bump in one-time cost looks manageable.",hardware,2026-02-21 11:49:57,2
AMD,o6kakrq,"> Memory latency has been the major bottleneck since the dawn of computing   For a long time in early computing the CPU and RAM ran at the same speed and CPUs would receive memory data effectively immediately (compared to their clock speed). It's only in the late 80s that CPU speed started outpacing RAM, beginning the ever increasing hoops that CPUs need to jump through to not be sat around waiting for memory access starting with adding cache.",hardware,2026-02-21 06:48:39,8
AMD,o6hdyaw,"A single burst of transfers on one memory channel is enough to fetch a whole cache line. More channels means you can have more cache line requests going in parallel, which does help with average memory latency but not worst-case latency (because the next bit of data you need may be on a channel that's already busy).",hardware,2026-02-20 20:04:58,13
AMD,o6hexzp,"The top-end platforms right now are running 12 channels of ddr5-8800. Speed and width aren't a direct tradeoff. You can have both.   What tends to hurt speeds is the number dimms per channel. Moving more consumer platforms to 1DPC, or even better IMO, to a single CAMM2 socket, would help bring supported speeds up. The latter would also force dual-channel ram which would probably help the average pre-built with an iGPU a bit.",hardware,2026-02-20 20:09:50,8
AMD,o6omuit,"There are only 24 PCIe lanes connected to the CPU with Nova Lake.  All additional lanes come from the chipset, and have to go through the CPU-chipset link.  It's better to have those than not, but it's not at all the same as having those lanes coming from the CPU.",hardware,2026-02-21 23:24:51,2
AMD,o6gpahf,"Yeah, if we get very technical its a complete mess, and 6 will bring this to 4 per stick.",hardware,2026-02-20 18:10:18,2
AMD,o6gmrpa,Yes.,hardware,2026-02-20 17:58:55,6
AMD,o6pj89j,"Not really, iPhones are also commercial fleet devices. Super common for them to be a standard issue Teams / Outlook / MFA machine for corporate staff. Very common for POS systems.  They use an iPhone because there's a whole ecosystem around it. Easy to centrally manage with something like Intune. Familiar to use for staff. Very long security patch lifecycle. Tons of accessories for them. And fairly cheap all things considered",hardware,2026-02-22 02:49:11,10
AMD,o6hoxoc,"Look at the actual marketing for cloud storage. It's all focused on availability (""Access your files from any device, anywhere."") reliability, and disaster recovery (""Your phone might break, but your memories are safe.""), not the amount of space you can buy.  If the Reddit theory of ""device manufacturers are decreasing storage sizes to sell cloud storage"" were true, we would see companies pushing multi-terabyte plans, not hundred gig plans.",hardware,2026-02-20 20:59:06,18
AMD,o6hllo6,"Nobody is forcing you to buy a small device.  But businesses alone sometimes order 10k iphones at a time to run a single app that takes up maybe 150MB of storage to work as a POS device in a restaurants.  They don't need 1TB of storage that will never be used.  They'd be fine if there was nothing more than 1GB free.  That's all they'll ever use.  And you just proved my point: you're an out of touch hardware enthusiast who can't contemplate that for most of the population, it's a tool and nothing more.",hardware,2026-02-20 20:42:40,15
AMD,o6pitl8,People usually buy cloud storage for backups so they don't lose their pictures and videos if their phone breaks or gets lost.,hardware,2026-02-22 02:46:31,1
AMD,o6lw03w,"It's not like that for this kind of work: it's employee QoL but doesn't translate in huge productivity increases. Most office work isn't heavy computing where the time a program takes to finish is limiting all the time the production. In those cases they buy tha hardware for the task.  It is bursts and the difference is solving eventual bursts with less ms of time: it's mostly about system snappiness or fast responses, but that doesn't mean the worker will finish equally fast. That's also why virtual machines are an acceptable working environment despite there's no way around network latency and tricks the connection programs do to keep bandwidth low. Programming is also a lot more about thinking/reading than typing and doing stuff.  Before even SSD were a thing people would keep everything open all the time just in case they needed, because it would take several seconds to re-open if needed later. You can put an SSD and the person now opens/closes programs as needed because opening them is not longer a huge time sink.  Other problems like compilation time are solved by giving compilers the ability to compile portions of the code base and reusing what's already compiled and didn't change. Or compiling less times.  It's overall more complex and if you look hard enough there's always a solution to the problem that doesn't involve buying more powerful hardware. And that in a business is the way to go.",hardware,2026-02-21 14:50:05,3
AMD,o6kwi1b,"And then consider multi-CPU systems. Now you have to find a way to synchronize the cache not only with main memory but among CPUs, because they might have common memory addresses cached.",hardware,2026-02-21 10:22:22,4
AMD,o6r9hks,"There's a thought - you could presumably a platform that supported RAM in RAID 1, so you've reduced the likelihood of this. Hellishly expensive though, especially at present.",hardware,2026-02-22 11:41:15,1
AMD,o6kwehi,Donâ€™t most gaming focused boards only have 1 dimm per channel ( I only use itx so I honestly have no clue )   Also at the very least more channels means more slots means more places to loose the lottery and longer traces overall,hardware,2026-02-21 10:21:25,1
AMD,o6pn621,Limited storage is probably used to make you need an upgrade,hardware,2026-02-22 03:15:44,3
AMD,o6m0jhl,> But businesses alone sometimes order 10k iphones at a time to run a single app that takes up maybe 150MB of storage to work as a POS device in a restaurants.  Any proof of that? That's a ridiculous waste of money.,hardware,2026-02-21 15:14:57,2
AMD,o6hp6mq,"[Ah yeah running out of storage, a famously ""hardware enthusiast"" only type of experience that normies never get to experience](https://youtube.com/shorts/kFAPM_mfE0U?si=n-HKIckAhb1RkR9z)",hardware,2026-02-20 21:00:20,-11
AMD,o6ykzjm,"That's not my experience. I had a normie come up to me because he could not update his iPhone anymore: ""I don't understand: I upgraded my iCloud subscription to get more storage but my iPhone still says I don't have enough storage to download the latest update""",hardware,2026-02-23 14:57:39,1
AMD,o6nh9vf,"Many do, especially ITX boards, but many also have all 4.",hardware,2026-02-21 19:39:10,4
AMD,o6l515x,iCloud+ in my iPhone was advertised as donâ€™t lose your data (the 1$ plan).,hardware,2026-02-21 11:43:26,-1
AMD,o6p7nuq,">Any proof of that? That's a ridiculous waste of money.  Go to sysadmin sub you can find out similar stories. Also per dollar value of them isnt bad, super depedendable products, software support on things, immediate repairs or replacements",hardware,2026-02-22 01:33:56,3
AMD,o6m1d2u,"Why? Building your own hardware is expensive. iOS is very stable, easy to deploy and unlike android supported with software updates for many years.  Itâ€™s literally the cheapest option beside paper and pencil.  An iPhone sold today will last 5-7 years. And android phone sold today even a flagship will be lucky to get security patches for 5.  iPhones are GOAT for industrial and commercial use. Hence you see them in so many restaurants around the world.  And we havenâ€™t even gotten into battery life and build quality. Even the cheapest iPhone is extremely well made and can handle abuse. Guaranteed even the worst selling model will have replacement batteries and screens sold for repairs. Supply chain is solid. You canâ€™t say that about budget android devices from China. Theyâ€™re disposable.",hardware,2026-02-21 15:19:21,5
AMD,o6hq5w9,"Again, you're pointing to non casual user.  Most people have a handful of apps installed, none of which require much space.  And businesses which buy thousands at a time are MILLIONS of sales.  Your use case is atypical, that's not my opinion, that's a fact.  Just because you can't game on a chromebook, doesn't mean kids can't use them for school either.",hardware,2026-02-20 21:05:15,8
AMD,o6pne2n,"Android devices by Google get 7 year support guarantee.  Their is a major price difference between budget apple and budget android. You don't get parts for budget android because it would be cheaper to just replace the phone.  Your points are based on the market several years ago.   Fm I use both platforms daily. I prefer android for my personal device because I am a Linux user and I am able to do things like initiate a wireless backup through sftp from a cron job on my computer. Being able to run an ssh server on a phone is huge, it means you can ask an AI to write you custom scripts to manage your phone and schedule them with cron or the windows scheduler if you use windows. IOs requires closed source proprietary software and you only get to do what they say you can do. Also, there is no official Linux support 3% of us care....  I use IOs for work because its popularity makes it the primary dev platform on business apps. I feel the nice hardware is underutilized as IOs being locked down  makes it feel like an appliance. You bought it to do these things and that's what it can do.   The other nice thing about apple is if I look up how to do something, it's universal and the steps will be the same for me. On Android, too many phones have custom images (Samsung) and the steps won't be the same. I prefer android over apple, but the only ones I buy are pixel due to being android straight from Google without the bloat from other manufacturers. Android gets a bad rep due to the cheap crappy devices, if you get something in the same class as an IPhone, the experience is great",hardware,2026-02-22 03:17:15,2
AMD,o6m3gaf,"Why do you need software updates on a phone that will only be used as a POS device?  And what kind of restaurant needs 10k POS devices? You only need a few per location, so that would be a very large chain. McDonalds has less than 14k locations in the USA (40k in the world) and they are at the level where they have their own solution for their self ordering kiosks.  EDIT: I was blocked",hardware,2026-02-21 15:30:14,1
AMD,o6m4rcb,"Iâ€™m not sure if youâ€™re playing stupid or actually stupid, either way, stop.  PCI compliance is a thing.  And restaurants donâ€™t do their own IT. Chefs cook, and hire someone to do the other stuff for them.",hardware,2026-02-21 15:36:56,8
AMD,o6r2s92,"Just to answer on the software updates thing - that's exactly the use case that needs software updates (especially security) the most.   A phone or tablet being used as part of a POS system is going to be using bluetooth or NFC (linking to a card reader or reading the cards itself) - possibly both - and is going to be connected to a network that probably shares hardware with a restuarant's guest network. That opens up a lot of potential attack vectors that a machine sat in a data centre doesn't have to care about. The POS systems aren't sat in secured areas, and are constantly interacting with devices owned by the general public (apple/google pay) that could be out of date and compromised by worms or trojans.  If the owner of a POS system isn't keeping their systems updated, they \*will\* be compromised. It's not an 'if' but a 'when'. PCI rules mean that when that happens, the owner of that POS system will be on the hook for every cent that VISA or Mastercards customers had taken from their accounts \*and\* will lose the abilitiy to take card payments. That's more or less a death sentence to any business.",hardware,2026-02-22 10:37:55,2
AMD,o5cdtmp,"Wild the takes I'm seeing. HuB sounded the alarm that price hikes were coming -- internet calls them liars. HuB pulls out the receipts that proves the hikes happened -- people accuse them of ragebait  When the PC building space is being destroyed by the AI bubble driving up prices, there's going to be a negative slant to much of the news. I for one appreciate the reporting.",hardware,2026-02-14 14:27:14,344
AMD,o5f1zgl,"So, whom we hate today?",hardware,2026-02-14 22:54:26,13
AMD,o5gzxf9,"Fun fact: based on inflation from 2017 to 2026, one dollar in 2017 is about 1.32 dollars today. So, the original GTX 1080 Ti price of 699 dollars in 2017 would be roughly 923 dollars in todayâ€™s money.",hardware,2026-02-15 07:15:46,7
AMD,o5c92dk,"Facebook had a bunch of studies leaked that found that posts that garnered a negative reaction to what was being shown got more engagement than any other form of content.   They specifically found that posts that induced more â€œangryâ€ reactions had more engagement.   Therefore, writing a title/thumbnail like this is specifically done to boost engagement because, no doubt, these creators can see the same statistics in their dashboards.   There are issues with the current PC gaming space. However, posting content like this isnâ€™t constructive nor is it beneficial to the whole segment.  Instead, it would have been better to focus on the alternatives available and offer a solution.   The problem is that AMD and Nvidia arenâ€™t going to prioritise a smaller segment of the market. The solution is that Intel ARC is currently available and offers good performance.  Itâ€™s not leading edge, but the emphasis these content creators put on specifically AMD/Nvidia cards as being the â€œBest Everâ€ is specifically the cause of the problem.   How are other brands like Intel going to want to invest more in a higher end product and introduce competition to AMD/Nvidia if content creators donâ€™t actively promote the competition?   If you want proof of this, [he doesnâ€™t recommend the 8GB 9060XT.](https://youtu.be/MG9mFS7lMzU) The 8GB 9060XT card costs the same as the 12GB B580 yet he doesnâ€™t even mention the B580 in that video as an alternative card for people at that price point.  Itâ€™s always easy to state the problems. Itâ€™s harder to offer a solution.",hardware,2026-02-14 13:58:31,22
AMD,o5cfipp,"Intelâ€™s drivers are mostly matured and their Xess upscaling is very good. Once they start offering high-end GPUs, the consumer space will start looking much better.",hardware,2026-02-14 14:37:11,7
AMD,o5bw0rm,How many low effort clickbait doomer videos is AMD unboxed going to make in a row? Fun drinking game,hardware,2026-02-14 12:28:50,-59
AMD,o5i1ssz,"Yeah, because gamers are the only customers in the world, right?  Gamers, including myself, are not entitled to low cost hardware. We've been blessed with it for many, many years. The market conditions are changing though. Learn to adapt.  Do I like spending more? No, but this is not something you can directly control, unless you want to go start your own GPU or memory manufacturing company. All I hear is a bunch of whiny, entitled teenagers complaining about money and the ""eeeeevil companies.""  Go ahead and downvote me; reddit hates hearing the truth.",hardware,2026-02-15 13:03:47,-4
AMD,o5c41u8,HUB became such rubbish,hardware,2026-02-14 13:27:04,-64
AMD,o5gd2zo,I wonder what the r/nvidia sub thinks of this,hardware,2026-02-15 04:03:34,-6
AMD,o5cjluk,"Wow people are weird. I donâ€™t follow HUB but I feel like everyone has been saying price hikes are coming, especially on bapcs and even Linus on wan show has briefly talked about it many times throughout the last couple months.",hardware,2026-02-14 15:00:13,79
AMD,o5cg7gv,People would rather bury their heads in the sand and pretend everything is alright than hear bad news,hardware,2026-02-14 14:41:06,54
AMD,o5cg0p2,> Wild the takes I'm seeing. HuB sounded the alarm that price hikes were coming -- internet calls them liars. HuB pulls out the receipts that proves the hikes happened -- people accuse them of ragebait  No one is calling the price hikes fake?   People have been discussing how 5070Tiâ€™s are now Â£800-900 from ~Â£700 and how AMD is planning on increasing the 9070XT price.,hardware,2026-02-14 14:40:02,24
AMD,o5ga7a9,"this is just one aspect of financialization of life - you are at the mercy of others for things you need to live and want to enjoy, especially from wall street and the equivalents globally.   Stop participating, or at the least minimize your commercial footprint - imho this is being 'woke' and obviously ymmv",hardware,2026-02-15 03:42:25,1
AMD,o5grloh,The issue isnt that they said prices are rising. The issue is that they have made up some conspiracy theory that makes no sense as to why they are rising.,hardware,2026-02-15 05:59:21,1
AMD,o5eg73u,"people don't like being told what to do they are always contradictory; so when HUB says ""prices are going up, if you need new hardware go buy right now asap"", people get mad because they don't like being told what to do. then they get mad again when they realise HUB was right",hardware,2026-02-14 20:53:13,0
AMD,o5fvi2m,"Aye, I understand the hate against the cringe rage hardware price-go-up videos, but this one was just about presenting the concrete data across global regions.   HUB didn't do themselves any favors with that intentional title, though. Vendors and stores are the ones cashing in off gamers more than NVIDIA.",hardware,2026-02-15 02:00:34,0
AMD,o5cslox,Nah fuck HuB,hardware,2026-02-14 15:47:51,-18
AMD,o5fsqop,Does it matter? There wasn't anything we could have done about it.,hardware,2026-02-15 01:42:03,-1
AMD,o5fbbug,Just go check out gamers nexusâ€™ YT channel. Need to start hating Sam Altman and discord right away,hardware,2026-02-14 23:51:30,-6
AMD,o5of0xr,Without the RAM-agadon it would be fine as it would be close to the 5080 price (the 1080 ti was made on the last node where cost per transistor was cheaper)c,hardware,2026-02-16 13:18:00,2
AMD,o5cidut,"The problem is if you already have a top end card, odds are the Arc is a sidegrade at best, but most likely a downgrade.",hardware,2026-02-14 14:53:25,25
AMD,o5cb2vg,"Check the views on their arc videos vs stuff like this or even amd/NV HW in general.   Shit goes both ways, man ðŸ¤·â€â™‚ï¸",hardware,2026-02-14 14:10:47,30
AMD,o5dufzf,Rage bait works for everything   Some charity group was curing tuberculosis in Africa and needed to have a negative title to get clicks.,hardware,2026-02-14 18:57:35,10
AMD,o5cbhpn,"I think overall you're right, but a couple points: the duopoly isn't the result of content creators it's the result of the market just not supporting a bunch of GPU providers. It's a very expensive industry.   The other big point is that while you're absolutely right about negativity boosting engagement, these channels have a major problem. The PC enthusiast industry is in a major lull right now and if not for angry posts, a bunch of these channels aren't going to have anything to post at all the next year or more.",hardware,2026-02-14 14:13:19,18
AMD,o5c9xta,Okay...,hardware,2026-02-14 14:03:50,6
AMD,o5f5hr9,">Facebook had a bunch of studies leaked that found that posts that garnered a negative reaction to what was being shown got more engagement than any other form of content.  >They specifically found that posts that induced more â€œangryâ€ reactions had more engagement.  >Therefore, writing a title/thumbnail like this is specifically done to boost engagement because, no doubt, these creators can see the same statistics in their dashboards.  I don't like being manipulated in this manner.  I actively avoid YouTube channels like the one on this thread. I'm OK with not being a member of their target audience.  I wish we had more ""serious"" Tech YouTubers.",hardware,2026-02-14 23:15:23,1
AMD,o5gs2rn,Which is why it needs to be regulated. Clickbait should be bannable offence.,hardware,2026-02-15 06:03:32,0
AMD,o5cc0lw,"and that's why i will never understand the hard-on that specific group of gpu users have on this creator. now he turns on them, too",hardware,2026-02-14 14:16:29,-6
AMD,o5cfcbe,"It's called ragebait, known for ages now. No need for wall of text",hardware,2026-02-14 14:36:09,-7
AMD,o5cn2wl,They are never going to offer high end gpus. They will continue to invest in integrated graphics which are looking very promising.   They are partnering with Nvidia to add compute tiles to their architecture in the future.,hardware,2026-02-14 15:18:59,29
AMD,o5g7opp,"""mostly""? Heh, that's an understatement.",hardware,2026-02-15 03:24:19,3
AMD,o5ckz5j,"Their* upscaling is fine but it's still noticeably worse than DLSS 4 or FSR 4, even when using the XMX version. I've compared all three of them at 1080p and 4k in multiple games.    Point being that I wouldn't say it's ""very good"" when both of intel's competitors offer much better upscalers right now.",hardware,2026-02-14 15:07:40,15
AMD,o5lug92,Side note:  Xess is usable on AMD cards as well :),hardware,2026-02-16 01:09:56,1
AMD,o5cjlxv,Yeah but their GPU architecture is still not as efficient as Nvidia. Look at the die size of the B580 and the performance it delivers. I am rooting for them cos we really need more alternatives.,hardware,2026-02-14 15:00:14,1
AMD,o5gb2bh,Clueless.,hardware,2026-02-15 03:48:42,1
AMD,o5bzeca,"Clearly they are profitable given this is now how most youtubers operate now. Everything is a disaster, everything is a scam, nothing is worth it, various forces are working against us etc.  Sad truth is people are just looking for negativity.",hardware,2026-02-14 12:54:36,14
AMD,o5c3z5z,"for this reason, i stopped watching videos that resembles that type of clickbait even if it has actual good content. Like post a normal thumbnail dude.",hardware,2026-02-14 13:26:34,1
AMD,o5c5ets,2m old reddit account with 1500 comments. Thats wild to call someone else rubbish when youre either an LLM or terminally online in a bad way.,hardware,2026-02-14 13:35:53,52
AMD,o5h1wat,Whatever the r/nvidia mods tell them to think probably,hardware,2026-02-15 07:34:36,-2
AMD,o5dz32o,Yeah the price hikes have been known to be coming for months now. Hub wasn't lying or anything,hardware,2026-02-14 19:21:22,18
AMD,o5cmz4f,"Many tech creators were sounding the alarm for sure, the writing was on the wall. And yet when HuB and MliD did videos on 5070ti supply about to run out they were dragged in the mud for it.",hardware,2026-02-14 15:18:25,48
AMD,o5gs11w,"Honestly the price increases and low supply were basically forecastable with just common sense without needing any ""deep"" industry knowledge or contacts.  Most GPUs are bought as part of system purchases even in DIY when someone does a complete build. Memory and other sub component prices went up no due to consumer demand this would mean lower overall PC demand and therefore GPU demand.  Core subcomponent costs for GPUs were going up. This means either they raise the price or eat the margins (why?). Higher prices again mean lower demand. You can also hide price increases by reallocating to higher margin SKUs.   How you frame and then ""sell"" that information though as part of the media well you need to use headlines like this thread.",hardware,2026-02-15 06:03:07,1
AMD,o5cqj3j,"Thatâ€™s not really a fair take. There is in fact a position in between not caring, and watching hours of YT content talking about the issues.   Itâ€™s a shitty situation, but it doesnâ€™t mean I need to spend my time lamenting how bad things are by watching YT videos saying the same thing over and over for the likes and subs. Make no mistake, YT like this and GN have popularized the position of â€œbig company badâ€, to give their subs a position to rally around and a common enemy. They are not doing it out of the good of their hearts, theyâ€™re doing it for the YT algorithm and a paycheck.   So excuse me if I dip out of all the 20yo reddit army discussion about the economy and companies and influencers and just play some fucking games with the hardware Iâ€™ve got.",hardware,2026-02-14 15:37:07,43
AMD,o5cnl8q,"I'm unironically doing exactly this. I simply do not have the mental capacity to care about an issue as (respectfully) minute as rising PC hardware prices when I have issues that are *much* worse and hit *much* closer to home nearly every day. Similarly, I do not have the capacity to care that my favorite entertainment medium is now bought by the Saudi Arabian government, for the same reasons. And I very much do not appreciate when my YouTube feed constantly tries to make me even angrier than I already am on a daily basis.  I even started to manually train YouTube's algorithm to avoid videos like that and so far it hasn't helped much. There's far too many of those around to dodge all of them.  So yes, I'm burying my hand in the sand on many important issues, because otherwise I'd legit go into depression from the overwhelming amount of horrible stuff happening to us, especially those we can't control.",hardware,2026-02-14 15:21:41,13
AMD,o5chaih,Sorry were you under a rock when everyone was piling on HuB and MLiD after their 5070ti video?,hardware,2026-02-14 14:47:18,5
AMD,o5ftmuu,Discord has a reason to hate,hardware,2026-02-15 01:48:01,15
AMD,o5pywqi,"I dont think cost per transistor should be your metric of choice, it prob still went down. (12 to 45 billion in this example) you may be looking for cost per square mm of wafer or die?",hardware,2026-02-16 17:55:11,1
AMD,o5cll20,"People have been complaining about the â€œinflationâ€ with the different tiers of cards produced.   For example, a 16GB 9060XT is more expensive as a â€œbudgetâ€ card than the RX 480 from a decade ago which was also a budget card.   Yet YouTubers like HW Unboxed frequently still call the 9060XT a â€œbudgetâ€ card, despite that. In fact [he compares it to the B580 as a budget card](https://youtu.be/xlbNsP5ySmA) despite the B580 being Â£120 cheaper than a 9060XT. Of course people are going to look at those graphs and decide not to buy a B580 as a real budget option.   People canâ€™t complain about the inflation of â€œbudgetâ€ cards if they are basing the â€œbudgetâ€ aspect on the naming scheme of the cards.   A 12GB B580 can be bought for Â£270, around the same price as a RX 480 a decade ago. Thats a budget card. The 16GB 9060XT and the 5060Ti arenâ€™t â€œbudgetâ€ cards yet YouTubers keep saying they are because of the model numbers.   â€œBudgetâ€ should be objectively based on the price of the cards, not the naming scheme assigned to them. Yet he ignores that and inadvertently promotes the worse option for â€œbudgetâ€ gamers.  If you want proof of this, [he doesnâ€™t recommend the 8GB 9060XT.](https://youtu.be/MG9mFS7lMzU) That card costs the same as the 12GB B580 yet he doesnâ€™t even mention the B580 in that video as an alternative for that price point, despite it costing the same and being available for 6 months by that point.",hardware,2026-02-14 15:10:57,14
AMD,o5gs4n3,"if you already have a top end card, why do you get outraged that a 5070ti got more expensive?",hardware,2026-02-15 06:03:59,3
AMD,o5cc4uk,"I donâ€™t disagree it gives them views.   However, itâ€™s hypocritical to be claiming that PC Gaming is now dying or whatever else and at the same time not do anything to try and fix it.   If they are trying to push people off of supporting AMD/Nvidia because theyâ€™re focusing on enterprise segments, whilst at the same time not supporting alternatives that are cheaper, then how do they expect anything to get better?   AMD/Nvidia isnâ€™t going to suddenly start caring about the gaming segment unless there is a threat to it. The only way to create that threat is by promoting alternatives.",hardware,2026-02-14 14:17:12,-15
AMD,o5cct09,But hereâ€™s the thing. Itâ€™s hypocritical because how do they expect PC gaming to get better when they donâ€™t promote alternatives to get off of the Nvidia/AMD duopoly?   The only companies we have at the high end is Nvidia/AMD.   Intel isnâ€™t going to invest heavily into discrete ARC GPUâ€™s if they donâ€™t have a large enough market share. So they have no incentive to make higher end products at a competitive price.   That in turn means AMD/Nvidia arenâ€™t going to feel any pressure to lower their prices anytime soon.,hardware,2026-02-14 14:21:14,-9
AMD,o5f38mp,"I've been of the opinion for a few years now that eventually integrated graphics would consume the marketshare of dgpu. I was expecting still a few more years out but this will probably accelerate that pace. It only makes sense, graphics realistically have been in a rough plateau, only being somewhat reset performance wise by raytracing demands. Even there some integrated gpu are able to run realytime raytraced games like cyberpunk, granted with a tonne of compromises and at like 30fps but as a proof of concept thats crazy.   For the average gamer using like 3060 levels of graphics I don't think most people would care if that came in the form of an integrated chip. Hardware heads, like myself, will likely always have their eye on higher specs but most people would be fine with games running through aggressive upscaling as long as framerates are relatively consistent. If the internal resolution can get to about 720-900p on average I think people will just go with it.",hardware,2026-02-14 23:01:49,2
AMD,o5i69gg,"> They will continue to invest in integrated graphics which are looking very promising.   On that note, I did try running Hogwarts Legacy on my i7 265 (?) laptop, pretty much just for the shits and giggles of watching my Â£850 choke just trying to load the menu. In actuality, it runs comfortably well at default settings (AMD FSR 2 on Quality, fidelity settings all at Ultra, RT on), and easily does 60fps+ with XeSS on Performance and settings at High. Having only ever seen laptops with iGPUs barely capable of the most basic 3D stuff before, it's amazed me to see how well it performs.",hardware,2026-02-15 13:33:39,1
AMD,o5g934s,Do you own one?,hardware,2026-02-15 03:34:18,0
AMD,o5dfxlw,Yeah it's bcz their upscaler is yet to be updated. Current xess 3 is nothing but 3 year old xess upscaling+ multi FG.  It's kinda amazing intel's older upscaler is still holding out well . But I really hope intel update their upscaler soon. Amd and nividia won't wait for them,hardware,2026-02-14 17:45:44,0
AMD,o5cugov,I havenâ€™t compared it with FSR4 but itâ€™s way better than FSR 2. Not much point in FSR4 being better if almost nobody can access it.,hardware,2026-02-14 15:57:25,-2
AMD,o5luzau,With much worse performance,hardware,2026-02-16 01:13:17,2
AMD,o5gc5tr,Do you own one?,hardware,2026-02-15 03:56:47,-2
AMD,o5c0321,"Iâ€™m sure youâ€™re right.Â   Before this video, thereâ€™s the MSI 5090 one, and then before thatâ€¦ FOUR doomer videos in a row.Â   Its even affecting GN. I guess Wendell is the last hope for positivity in the hardware space.",hardware,2026-02-14 12:59:34,-23
AMD,o5c4sjg,"Somewhat true, i hate most yt titles and thumbnails. HWU is not amd sided, i do hate how mucj pc buildingsnd just tech in general is just crying about the biggest company and forgetting that businesses need money and what they are doing is best for them, not for customer unless thatâ€™s also best for the business. Businesses want money, thatâ€™s the point, you arenâ€™t clever for saying nvidia is greedy, crying why the 5080 is 1000$ wonâ€™t get you a free card.     HWU is reasonable with that mostly unlike gamersnexus that just constantly wants some bs drama (even tho they do have good data)",hardware,2026-02-14 13:31:54,-13
AMD,o5c6ffc,These accounts pop up everytime this happens. I wish they are bots rather than terminally elitist corporate bootlickers.,hardware,2026-02-14 13:42:19,41
AMD,o5cpqup,"Yeah, bot behaviour",hardware,2026-02-14 15:33:01,10
AMD,o5gseya,is there a way to see total number of comments? whats mine. Btw i agree HUB has become much worse if you need motivation to look through my history.,hardware,2026-02-15 06:06:29,0
AMD,o5el16l,"I think in the case of MLID thats because he in the past has made several statements about availability that never really panned out. Somewhere during RDNA2 / RTX 3000 era for example, he made statements on how AMD had much more supply in the volume segment (60/600 and certain 70 / 700 Sku's segment) back then, while when looking at market share trends Nvidia was only growing in market share and revenue.   And even now nothing is made concrete by leakers like MLID, when you watch MLID now at times the way he phrases stuff makes it look like Nvidia is hardly supplying / fabbing chips atm for all the Sku's that fall under their gaming division. And while i very much agree that supply is lower than normal, i doubt when we look at market share numbers for Q1 / Q2 later in the year, or earningscalls that it actually is as bad as he phrases it now.",hardware,2026-02-14 21:19:42,18
AMD,o5coqzr,MliD is a lying waste of space who just deletes videos where he's wrong is should be taken about as seriously as a jester juggling fuzzy balls.  HuB got dragged for stating the the 5070 ti was going to be EoL when that was not the case near at all.,hardware,2026-02-14 15:27:49,-14
AMD,o5d1fc1,"There's also a group of people who seen the writing on the wall, and acted accordingly.  I built my system in February and overpaid on my GPU by about $350 to get the hell out of the market before the shit storm made it fully ashore.  This'll either last until the market stabilizes or Bezos gets his wish for cloud computing and I at least spent a few moments with a view from the top  before it was all over.",hardware,2026-02-14 16:32:27,11
AMD,o5erj0c,"> GN have popularized the position of â€œbig company badâ€,   IDK if I'd really call it that? Most DIY PC parts vendors play their customers pretty hard with ""gaming"" stuff lol. They always have. Steve is more Captain Obvious than anything lol.",hardware,2026-02-14 21:55:07,-2
AMD,o5cse18,"I know everythingâ€™s bad, I donâ€™t care or engage in anything. I just donâ€™t buy any pc hardware, and tbh thatâ€™s the most effective way to â€œprotestâ€ this.",hardware,2026-02-14 15:46:46,11
AMD,o5gfmzx,"fun fact, you dont have to be here.",hardware,2026-02-15 04:22:50,-3
AMD,o5grvui,usually if MLiD says something the opposite is true so it does not surprise me at all.,hardware,2026-02-15 06:01:51,12
AMD,o5dtxae,"MLID's track record is pretty bad, someone years ago calculated it and only around 50% of his ""leaks"" were correct. Keep in mind he also deletes his videos were he was wrong so it's much harder to keep track of his record  Here found it  https://www.reddit.com/r/Amd/s/yDgdHdVwKV",hardware,2026-02-14 18:54:57,23
AMD,o5cmg4s,"They were piling on because he stated it was EOL/Discontinued.  They werenâ€™t piling on because of price hikes.   Also, putting MLiD in the same sentence as HUB is laughable.   You should stop watching MLiD, [he has a track record of deleting videos](https://www.reddit.com/r/Amd/s/w1Vk3ThfHO) where his predictions were completely wrong so he â€œlooksâ€ accurate but isnâ€™t.",hardware,2026-02-14 15:15:36,35
AMD,o5w6sdf,"MLiD  I believe in HuB, but really? MLiD?",hardware,2026-02-17 16:59:17,1
AMD,o5h6yzo,At least itâ€™s better than truth social. Canâ€™t get any good hardware discussion there.,hardware,2026-02-15 08:23:52,-2
AMD,o5qfp2p,"cost per transistor itself has been going up node after node, each wafer has been costlier and costlier and while Nvidia has tried to keep costs in check, it's normal that new GPUs keep getting more expensive for their tier. Perhaps when we do GPU chiplets correctly, it will keep costs a bit down but still",hardware,2026-02-16 19:12:06,1
AMD,o5cf8wa,Why are people acting like Intel isnt 200B+ corporation lol? Intel doesn't need teletubers making vids on them to increase supply of their gpus. They have enough marketing budget to make sure they get exposure but Intel would either focus on DC like amd/nvidia & everyone else atm. Intel is not a saint,hardware,2026-02-14 14:35:36,23
AMD,o5cte6m,you cannot possibly believe youtubers are responsible for the gpu market lol,hardware,2026-02-14 15:51:57,22
AMD,o5cegjl,There is no reasonable path out of the current duopoly. A handful of YT channels are not going to begin to move the needle here.   In the overall GPU market this is irrelevant. It simply doesn't matter in any way.,hardware,2026-02-14 14:30:57,18
AMD,o5cg108,What alternatives are there to AMD/Nvidia? Intel only has budget GPUs because like you said they don't want to invest heavily and most folks want something stronger than the b580 on par with the 5070ti/9070xt so they're not gonna buy Intel if they have no options there. The only way to push these companies to change then is to dredge out their dirt and hope it hurts their bottom line enough since we have no real alternatives.,hardware,2026-02-14 14:40:05,6
AMD,o5ef3gp,"You think it's the responsibility of content creators or hardware reviewers to [jawbone](https://en.wikipedia.org/wiki/Moral_suasion#Jawboning) the prices of graphics cards, down?",hardware,2026-02-14 20:47:15,2
AMD,o5q1sl8,The writing's been firmly plastered on the wall in terms of unified architecture supremacy since the apple silicon M1 came out and they started busting down the door with more memory channels in consumer computers. There is just too much to gain from unifying everything. All the memory bandwidth you can cram in can benefit both the CPU and the GPU (and anything else that's in there). Maybe we'll get another round or two of viable traditional PC architecture but I think it's possible even for the next iteration of strix halo (medusa halo) to render all but top end gaming obsolete.  Even a 5090 which is difficult to even find a way to closely fully utilize from most gaming software in existence (due to being optimized for more modest hardware) is a baby next to the training and inference optimized nvidia hardware now... I look forward to good healthy competition between intel/nvidia APUs and AMD APUs. Surely some design will be forthcoming that can give good PCIe bandwidth to connect a top end discrete GPU and it will be a nice compact workstation powerhouse platform.   Let's not discount the value that can come out of locally running huge LLM models resident in huge unified memory and efficiently inferenced at good speed by onboard NPUs and tensor cores.,hardware,2026-02-16 18:08:24,1
AMD,o5hun7h,Yes. As soon as you stray from the most popular games all bets are off. I got an Arc A380.,hardware,2026-02-15 12:07:26,1
AMD,o5i644b,"Intel has stated themselves that they are targeting the midrange and low end consumers, which only eats away at AMDs shares.",hardware,2026-02-15 13:32:44,1
AMD,o5dpvs8,Yeah I imagine they're working on a transformer model implementation. They're ahead of AMD on Frame Generation tech already so that's a good sign.,hardware,2026-02-14 18:34:53,1
AMD,o5m5guh,"Sure, but better on older AMD cards than older FSR.",hardware,2026-02-16 02:20:26,0
AMD,o5c3eqg,">I guess Wendell is the last hope for positivity  I'm sorry but there's no positivity when it comes to recent PC hardware related news, if you want to exclude doom posting, your best bet would be limiting yourself to hardware articles from sources like techpowerup, which are just reviewing hardware without extra emotions and personal bias.",hardware,2026-02-14 13:22:48,33
AMD,o5c2hh3,Have you checked prices recently?,hardware,2026-02-14 13:16:32,15
AMD,o5c5t3f,"Any hardware youtubers making positive content around GPUs or building PCs is either an idiot or a liar, tbh. Because the current situation is absolutely disastrous.",hardware,2026-02-14 13:38:24,12
AMD,o5elv7t,> Its even affecting GN  I'd say he was the origin of it all. Guy has been making this shit for years more than most of them.,hardware,2026-02-14 21:24:18,0
AMD,o5ex558,"Low supply for nvidia is like handling 20K instead of 50K  While AMD is only supplying 5K and that there best.   The market share number would still be high because people still buy Nvidia.  For the earning Nvidia and AMD are selling more AI cards, so earrning would be up even if ""Gamers"" card are lower they would still be winning",hardware,2026-02-14 22:26:30,9
AMD,o5d797o,Bro here in Germany the 5070 ti IS effectively EoL. Don't think it's gonna be any different where you are from very soon.,hardware,2026-02-14 17:01:36,12
AMD,o5d5vof,**EFFECTIVELY** EOL.,hardware,2026-02-14 16:54:42,16
AMD,o5exq0j,"His channel has evolved over time from hardware reviews, then some studio/infra upgrades that saw him pivot to validating companies marketing claims on tests/capabilities, which turned into â€œsuch and such lied! Their fans are 43dB, not 41dB!!1â€, and from there became progressively more belligerent as they pivoted into â€œinvestigative journalismâ€ because they noticed their fan base was really into calling companies out in a loud voice.   Call it what you want, he may not have truly started that approach, but heâ€™s certainly done his part to add fuel to the fire.",hardware,2026-02-14 22:29:52,0
AMD,o5d85e6,"The most effective way is to avoid purchasing overpriced hardware, and advocate for alternative means of gaming. Retro, consoles, anything else to drive business away.",hardware,2026-02-14 17:06:10,7
AMD,o5d7vhk,"5070ti is effectively EoL here in Germany. The supply is quickly drying up, prices are going crazy reaching 1000â‚¬.",hardware,2026-02-14 17:04:46,2
AMD,o5fdrg0,"It is effectively becoming EOL. Supply is becoming severely restricted and pricing is rising because of it. I went and talked to a bunch of microcenter employees and they basically confirmed demand hasnt changed, its just supply.",hardware,2026-02-15 00:06:52,4
AMD,o5gjrd2,A thread from 5 and a half years ago! Talking about threads from 6 and a half years ago! Referencing videos from 7 years ago!  Jesus. I'm impressed.,hardware,2026-02-15 04:54:39,-9
AMD,o5j3jwn,Yeah no one asked about truth social bud.,hardware,2026-02-15 16:31:34,6
AMD,o5i1xgn,"I wish more people would use Truth Social. It's actually a really nice platform, even more user-friendly than X, in my experience.",hardware,2026-02-15 13:04:42,-4
AMD,o5ciqsq,"Theyâ€™re not a saint, but neither is AMD/Nvidia currently.  Let me put it this way, people have been stating that the price increases for these â€œbudgetâ€ GPUâ€™s has been an issue.  For example, a 9060XT is ~Â£400 and a 5060Ti is ~Â£450 whereas a budget card a decade ago with the RX580 was ~Â£300.   Yet content creators still push the 9060XT and the 5060Ti as â€œbudgetâ€ cards.   They should change their definition of â€œbudgetâ€ to be done by actual price. Because if theyâ€™re complaining that â€œbudgetâ€ cards by AMD/Nvidia are increasing in price, Intel still sells a real budget card for Â£270-300.  â€œBudgetâ€ is an objective measure based on price to performance. Yet content creators still push cards that are objectively worse for those figures.   HW Unboxed made a video a month back comparing the 9060XT and the B580 as â€œbudgetâ€ cards. The B580 is Â£120 cheaper than the 9060XT, how is that even a comparison?",hardware,2026-02-14 14:55:26,-5
AMD,o5ckxxo,"People have been complaining about the â€œinflationâ€ with the different tiers of cards produced.   For example, a 9060XT is more expensive as a â€œbudgetâ€ card than the RX 480 from a decade ago which was also a budget card.   Yet YouTubers like HW Unboxed frequently still call the 9060XT a â€œbudgetâ€ card, despite that. In fact [he compares it to the B580 as a budget card](https://youtu.be/xlbNsP5ySmA) despite the B580 being Â£120 cheaper than a 9060XT. Of course people are going to look at those graphs and decide not to buy a B580 as a real budget option.   People canâ€™t complain about the inflation of â€œbudgetâ€ cards if they are basing the â€œbudgetâ€ aspect on the naming scheme of the cards.   A B580 can be bought for Â£270, around the same price as a RX 480 a decade ago. Thats a budget card. The 9060XT and the 5060Ti arenâ€™t â€œbudgetâ€ cards yet YouTubers keep saying they are because of the model numbers.   â€œBudgetâ€ should be objectively based on the price of the cards, not the naming scheme assigned to them. Yet he ignores that and promotes the worse option for â€œbudgetâ€ gamers.",hardware,2026-02-14 15:07:29,-3
AMD,o5c3swq,Good advice tbh. Techpowerup is great. Iâ€™ll do just that.,hardware,2026-02-14 13:25:26,9
AMD,o5em7ol,"Panther Lake has been pretty great hardware news. Some have chosen to almost completely ignore it though... I get it in some way, laptops aren't very exciting but it is at least a good new product that many of us will use.",hardware,2026-02-14 21:26:11,3
AMD,o5em0zz,Are you not bored of the same shit?  I appreciate they have to make something for their upload schedules but they're all pumping out the same everything is shit shit multiple times per week.,hardware,2026-02-14 21:25:11,5
AMD,o5f3v97,"Yep, and that is kinda my point. Its what i think also, Nvidia's supply is lower, but nowhere near 0 as some leaks make it out to be.   AI cards aren't in the gaming segment of the earnings though, those are either in in the Professional segment (RTX Blackwell Pro and the likes) segment (very small, much smaller than gaming), or in Datacenter (much bigger that gaming).",hardware,2026-02-14 23:05:38,8
AMD,o5d8h6w,"His exact words before pulling back after being corrected to state effectively after both NVIDIA and ASUS said wtf no were ""The RTX 5070 Ti is now an end of life product"".  He decided that an ASUS employee saying they can't give him samples because of how supply constrained everything that relies on ram is becoming and describing a ""dismal supply chain"" meant that a product that was clearly not going to be EoL was EoL if it meant he could get a good bit of views rolling around.",hardware,2026-02-14 17:07:49,-11
AMD,o5facmm,"> such and such lied! Their fans are 43dB, not 41dB!!1â€,   I don't.. think they've ever exaggerated like that?",hardware,2026-02-14 23:45:25,8
AMD,o5ls7hm,"Yeah man, a billion YouTubers sucking the shaft of corporations is fine but GOD FORBID some dude starts calling out corporations out for their bullshit.  Every single  dystopian sci-fi novel worth its salt saw the corporate hellscape coming since the mid 20th century.  We're right in the middle of it",hardware,2026-02-16 00:55:59,1
AMD,o5pbxec,I forgot the /s tag,hardware,2026-02-16 16:08:40,0
AMD,o5pc28t,"It needs to be, based on the demographic.",hardware,2026-02-16 16:09:17,0
AMD,o5csz5a,"The b580 is currently Â£10 cheaper than the 9060xt and has 30% less performance. You're right, how is that even a comparison?",hardware,2026-02-14 15:49:48,14
AMD,o5fep8c,"> For example, a 9060XT is ~Â£400 and a 5060Ti is ~Â£450 whereas a budget card a decade ago with the RX580 was ~Â£300.  That's directly in line with the cost of inflation in the UK during that time period. 300 pounds in 2015 is 420 pounds in 2026.",hardware,2026-02-15 00:12:38,3
AMD,o5h1sxg,I'd be interested to know what your reaction would be if the video was about real massive price decreases.,hardware,2026-02-15 07:33:42,-1
AMD,o5decg5,"The only thing questionable about their video was the original title and thumbnail. The actual video itself was very clear about what was happening. Nvidia was reducing supply of the 5070 ti to the point where brands like ASUS would no longer be bothering to produce them. The ASUS rep used the words ""end of life"", and only walked it back once after the whole thing blew up and Nvidia was a obviously asking them to ""clarify"" what they meant (notice ASUS never claimed that hardware unboxed misquoted them).",hardware,2026-02-14 17:37:42,9
AMD,o5damt8,"Funny how I understood exactly what he meant the first time around, because its not the first time Nvidia has done that..... now when it happens again, you don't have to be confused.",hardware,2026-02-14 17:18:49,7
AMD,o5lsd1d,That some dude is sucking the shaft of YouTube at your expense,hardware,2026-02-16 00:56:55,1
AMD,o5d1w5t,"Youâ€™re comparing the 8GB model. The Intel B580 has 12GB of VRAM. The 8GB model already gets saturated even at lower resolutions, you can see this in benchmarks comparing the 8GB model to the 16GB model.   [Overclockers have a B580 for Â£270.](https://www.overclockers.co.uk/sparkle-intel-arc-b580-guardian-12gb-gddr6-graphics-card-gra-spk-03945.html)  [The cheapest 16GB 9060XT on Overclockers is Â£379 and thatâ€™s discounted from Â£419.](https://www.overclockers.co.uk/sapphire-pure-radeon-rx-9060-xt-gaming-oc-16gb-gddr6-pci-express-graphics-c-gra-spr-05180.html)  Hardware Unboxed himself states that [he doesnâ€™t recommend the 8GB model.](https://youtu.be/MG9mFS7lMzU) Instead he recommends the 16GB model as a â€œbudgetâ€ card instead of the B580 which is actually a budget card. Nowhere in that video does he suggest that the B580 might be a better value at that price point.   Thatâ€™s what I mean by my comment.",hardware,2026-02-14 16:34:47,-1
AMD,o5ibvhw,And then people complain that wages havenâ€™t kept up with inflation so whatâ€˜s your point?,hardware,2026-02-15 14:07:50,0
AMD,o69wmgw,rx 6600 ig.,buildapc,2026-02-19 17:31:34,1
AMD,o69wuw0,Whatâ€™s your price range,buildapc,2026-02-19 17:32:42,1
AMD,o69ww78,"if you can find a dirt cheap 1660ti or super. you could go better if you want to spend but then it begins, new mobo, new processor, oh need more cooling now, new cpu cooler....drat doesnt fit need a new case....youre starting basically where i did and this is what ive done lol",buildapc,2026-02-19 17:32:53,1
AMD,o6aki9b,Start with a budget.  Buy something that fits the budget and can work with your PSU.,buildapc,2026-02-19 19:24:21,1
AMD,o6d19mb,"Depends on your budget, maybe the intel arc?        https://www.amazon.com/ASRock-Intel-B580-Challenger-Graphics/dp/B0DNV4NWF7",buildapc,2026-02-20 03:34:32,1
AMD,o6a8x1o,"I found one at a rather affordable price, this is Nvidia though... I have always had team red GPUs (No real reason; they simply happened to be the right specs each time). Anything I can expect of Nvidia that I should know?",buildapc,2026-02-19 18:29:42,1
AMD,o6a9y2j,"nothing super fancy since its a pretty old card. someone else suggested a 6600 which is better but would def be more money than a 1660 but would probably give you more time before yould have to buy a better gpu once youve upgraded everything else.  I dont use the ""nvidia experience"" app. its better than an rx570 and an rx580 tho.",buildapc,2026-02-19 18:34:29,1
AMD,o6acafn,"I donâ€™t really plan to make a big budge movement - in fact, a couple of years ago I was using literally a previous generation build (DDR 3...) so I am really only looking to replace the GPU for the time being. Helps I donâ€™t necessarily play the latest AAA stuff and I am not picky with graphics either (Just 1080P and 60FPS and I am happy, donâ€™t need stuff like Raytracing or 4K or all that fancy stuff).     Yes, I looked it up, and the 1660 is defenitively a step above my RX570. It actually seems to be an almost perfect matchup for my Ryzen 3 3100- the RX570 was seemingly holding it back a tad. Thatâ€™s really what I need, just a nice little upgrade but within my current build, since to go even further, I would have to start checking to upgrade the CPU as well as you said and yeah, havent got the budget for that atm.  rx 6600 is also clearly better, but just a tiny bit pricey for my tastes. I think I might just go 1660. Thanks",buildapc,2026-02-19 18:45:17,1
AMD,o6acsus,i ended up buying a ryzen 5 3600. its still a decent cpu. i play fortnite and cs2 just fine. not cranked up but decently.  if you want to go any furthar than that youre gonna need a better mobo,buildapc,2026-02-19 18:47:42,1
AMD,o7ek269,"I wouldn't spend close to $300 on a B850 mobo just for a POST code display. A B850 closer to $200 or less should be just as reliable and performance is going to basically be identical.  You could likely save money with slightly slower ram, such as CL36. Even 5600 Mt/s CL40 ram isn't that much slower for gaming. [https://www.techpowerup.com/review/ddr5-memory-performance-scaling-with-amd-zen-5/17.html](https://www.techpowerup.com/review/ddr5-memory-performance-scaling-with-amd-zen-5/17.html)  The Saphire Pure is not worth it's significantly  higher cost over other RX 9070 XTs, even for the warranty. I'd just get the cheapest white RX 9070 XT you can find, if color matters, otherwise you can get a RTX 5070 ti for less than the cost of the Pure.",buildapc,2026-02-25 21:46:56,9
AMD,o7ep4xp,I built a 9800x3d with a 9070 red devil with a b650 mobo. It works flawlessly. You wont be disappointed,buildapc,2026-02-25 22:11:04,1
AMD,o7goboi,Good build. Go for it.,buildapc,2026-02-26 04:57:56,1
AMD,o70bhk1,How is your CPU lacking? What issues are you experiencing? What difference are you hoping an upgrade will make?  Worth it for most people means spending hundreds of dollars will make a noticeable difference to them. This is subjective and situational.,buildapc,2026-02-23 19:49:57,8
AMD,o70bu8m,"Micro Center or New Egg bundles are available with some great (all things considered) deals.  If you are upgrading platforms, I would go the bundle route.",buildapc,2026-02-23 19:51:35,3
AMD,o70c1pd,"Do you reliably hit the fps of your monitor?      If yes, then there's no need to upgrade.      If no, then you can look at upgrading your system.      I currently run a 5070TI with a 5800x and I consistently stay at 144 fps without any dips at 1440p. I have my games capped at 144.",buildapc,2026-02-23 19:52:33,3
AMD,o70dnja,"Honestly, I see little point in upgrading, unless you are seeing a major deficiency in your FPS performance.  Switching to AM5 means you will need a new CPU, motherboard, and RAM.... and RAM prices right now are crazy.  If you still want to upgrade, you might want to consider going for a Ryzen 5800X3D.  That way, you can get a meaningful boost in FPS performance in some games, and go from 6 cores/12 threads on your Ryzen 5600X to 8 cores/16 threads on the Ryzen 5800X3D, without needing to replace your existing motherboard or DD4 memory. It would be a much cheaper upgrade compared to switching to AM5.",buildapc,2026-02-23 20:00:04,3
AMD,o70bxdo,I'd just keep going with what you currently have. Why do you want to upgrade?,buildapc,2026-02-23 19:51:59,2
AMD,o70ff2j,"Absolutely do not upgrade unless you are extremely rich. Giving up on a good CPU, 32 GB DDR4 RAM which is already very expensive and an entire motherboard is not worth it in any way sort or form. If the CPU is not good enough for you then consider getting a Ryzen 9 5900X or 5950X or similar.",buildapc,2026-02-23 20:08:32,2
AMD,o70ftn7,"I wouldn't upgrade, your AM4 build is still strong.",buildapc,2026-02-23 20:10:30,2
AMD,o70dr5w,"I started with your specs 5 years ago except for a different GPU and stuck with AM4, but Iâ€™ve swapped out for a 5800X3D and upped to 64GB of RAM.  I have no plans to get into AM5 now that AM6 is on the horizon and everything is overpriced these days.  5600X is a great processor that overlocks decently well.",buildapc,2026-02-23 20:00:33,1
AMD,o70drw9,"Depends what you are trying to achieve. An example, I upgraded from a 5800X3D to 9800X3D. I game at 1440p 240Hz and 4k 120Hz. Trying to push 240fps for 240Hz in games like BO7 & BF6 the upgrade was absolutely worth it. However, when aiming for 120fps for 120Hz it would not be worth it all.",buildapc,2026-02-23 20:00:39,1
AMD,o70f9vo,"The 5600X is still pretty good, and with current prices I would try to hold onto it for as long as you can.  The only downside is the 6-cores but that may also be fine. You could look at buying a used 5700X or 5800X or similar and then selling your 5600X. Depends on used market conditions in your area.",buildapc,2026-02-23 20:07:50,1
AMD,o70g873,"Maybe price out a 5950x CPU and an adequate cooler to go with it  Otherwise  Any 7000 series CPU + a decent cooler  Look for ""open box"" B650 motherboards from Newegg  DDR 5 RAM is going to be the item that hurts your pocketbook the most",buildapc,2026-02-23 20:12:27,1
AMD,o70gspq,"5800X3D could be your solution if you are on tight with money, but its hard to get. And only used one, amd stopped production year or two ago.  Otherwise am5 and 7800x3d..but prices of RAMs arent in your favor. Im sorry.",buildapc,2026-02-23 20:15:11,1
AMD,o70e53z,"Biggest thing for me is the CPU for editing and playing games (star citizen, etc) just window shopping right now and looking for suggestions on what would be a good build for Amd 5 where I can just sit there for years like I have with Amd 4.",buildapc,2026-02-23 20:02:24,1
AMD,o70gmoq,"I have looked into the 5800x3D but they don't make them anymore and they are hard to come by, even tried to get one from GameStop. Honestly I'm fine with waiting for a few more years to get AMD 5 but I want a build that I can work towards while I wait for the market to settle down, if that makes sense?",buildapc,2026-02-23 20:14:24,2
AMD,o70em2b,"A better CPU, but I'm just looking for suggestions on a build, not that I am building it right this second, but just for the future",buildapc,2026-02-23 20:04:39,1
AMD,o70ir8f,"Noted, thanks for your comment, I'm not rich unfortunately, so it's window shopping until then, I'll definitely take a look at those CPUs.",buildapc,2026-02-23 20:24:36,1
AMD,o70iwrk,"Yes not yet, but window shopping for when the market finally calms down, I wanna have something thought out before then.",buildapc,2026-02-23 20:25:20,2
AMD,o70hbmy,"Yeah I agree, if the 5800X3D was produced still and easier to come by then I wouldn't have any issues upgrading to it, not to say my current build is horrible but I do want to know what I have to work towards in upgrading to AMD 5, especially after the market calms down and the Amd 5 builds look cheaper.",buildapc,2026-02-23 20:17:43,1
AMD,o70fr5j,"I play on 1440p and my system works, I even do editing, but my CPU is the weakest link and I wouldn't mind knowing what I have to do in order to upgrade to AMD 5 with a good CPU that works with my current GPU, for the future because the market these days is putting me off on making the leap from Amd 4",buildapc,2026-02-23 20:10:10,1
AMD,o70jese,"Thanks for the suggestion, I plan on waiting a bit for the prices to go down, so I'm just getting the build idea together until then.",buildapc,2026-02-23 20:27:44,1
AMD,o70hr81,"Yeah I tried to get the 5800X3D, wish they still made them, but as far as the Amd 5  goes, I'm just looking for a build suggestion for the future, something to work towards for when the market finally stops being how it is.",buildapc,2026-02-23 20:19:47,1
AMD,o70f0dx,"I mean, if a 5600X has been fine for you, then I would assume 7600X would be fine as well, or any CPU above that. It entirely depends on what you want and your opinion on the resulting performance.   You can use benchmarks to compare expected performance in various workloads, then decide for yourself if that difference is worth your money. It's ultimately your opinion, rather than anything objective or factual.",buildapc,2026-02-23 20:06:34,2
AMD,o70jb6w,"If you are looking to upgrade in a few years, then I would suggest waiting for Zen 6 to come out, which is slated for 2027.",buildapc,2026-02-23 20:27:15,3
AMD,o70eu8x,A 7800X3D would be a big upgrade.,buildapc,2026-02-23 20:05:45,1
AMD,o70m879,"Nice. Those two specific CPUs are not mandatory at all, but they are very good. I can also recommend the Ryzen 7 5800X3D, as long as itâ€™s AM4 lol!",buildapc,2026-02-23 20:41:21,2
AMD,o70hmiw,Does your cpu actually prevent you from achieving the fps you want? If I had 1440p 144-165Hz and therefore a 144-165 fps target the upgrade I did wouldnâ€™t have benefited me. What is it specifically you want your system to do better? Generally upgrades only feel beneficial when a component is actually lacking for your needs.,buildapc,2026-02-23 20:19:10,1
AMD,o70i43f,"Appreciate it, yeah hopefully the market calms down eventually, till then, it's window shopping at this point.",buildapc,2026-02-23 20:21:30,1
AMD,o70ko62,"Dang, 6 is already coming out that soon? That's great news, hopefully the prices of Amd 5 will seriously go down, because all I can do in the mean time is wait.",buildapc,2026-02-23 20:33:46,2
AMD,o70idw8,"Oh yeah, hopefully big enough to last me until Amd 7 or Amd 8 haha, but gotta wait for the market to settle down",buildapc,2026-02-23 20:22:50,1
AMD,o70ozvl,I wish I could get my hands on a 5800X3D haha,buildapc,2026-02-23 20:54:24,1
AMD,o70k6bx,"It's a bottle neck on some stuff like editing or gaming, like star citizen or any CPU intensive games, it's not an upgrade I need right now because it doesn't prevent me from doing those activities, but I want to know what would be a good build for Amd 5 in which I can just sit there for years until something else needs upgrading, in the future",buildapc,2026-02-23 20:31:23,1
AMD,o70lr2o,"I have two Zen 2 systems (Ryzen 3600 and Ryzen 3900 non-X) and one Zen 3 system (Ryzen 5600X).  I am holding off on replacing my existing PCs until Zen 6 comes out.  It is only a year or so away, so I figure it does not make sense getting Zen 5 nowadays.",buildapc,2026-02-23 20:39:02,1
AMD,o70syoh,"Why is that? 5800X/XT is also a good alternative, otherwise the only ones Iâ€™d consider are the R9 5900X/5950X or the 5800X3D.",buildapc,2026-02-23 21:15:18,2
AMD,o70ouir,"Yeah especially with the AI bubble that makes everything expensive, one day hopefully",buildapc,2026-02-23 20:53:44,2
AMD,o73zu08,"At that price difference, go with 9060xt.",buildapc,2026-02-24 09:54:36,23
AMD,o741ozu,"9060XT 16GB $580: [https://www.centrecom.com.au/powercolor-reaper-radeon-rx-9060-xt-16gb-gddr6-graphics-card](https://www.centrecom.com.au/powercolor-reaper-radeon-rx-9060-xt-16gb-gddr6-graphics-card)   5060TI 16GB $730: [https://au.pcpartpicker.com/product/zWrp99/zotac-twin-edge-oc-geforce-rtx-5060-ti-16-gb-video-card-zt-b50620h-10m](https://au.pcpartpicker.com/product/zWrp99/zotac-twin-edge-oc-geforce-rtx-5060-ti-16-gb-video-card-zt-b50620h-10m)   9070XT $950: [https://www.centrecom.com.au/powercolor-radeon-hellhound-rx-9070-xt-16gb-gddr6-oc-edition-graphics-card](https://www.centrecom.com.au/powercolor-radeon-hellhound-rx-9070-xt-16gb-gddr6-oc-edition-graphics-card)  At those prices I think the 9070XT is the best option, 5070TI performance in most areas for $400 less. Although the hit in Blender will be substantial.",buildapc,2026-02-24 10:11:45,14
AMD,o74bl8u,"Depends how much you care about blender.  If you're primarily gaming then with those prices go for the 9060xt 16gb.  Very close to the 5060ti 16gb with a much lower price.  If you care a lot about blender, don't go AMD, they're just plain awful for it.  The 9070xt often performs worse than a 5060.",buildapc,2026-02-24 11:38:11,8
AMD,o748jn1,"Highjacking OPs post. I got my eye on a 9060XT 16GB for 450â‚¬â€¦. Looking at historic price, some months ago it was at 400â‚¬â€¦ Best decision is probably grab it now at 450â‚¬ right?",buildapc,2026-02-24 11:12:54,2
AMD,o757g5u,"if you had the money, go with 5070 or 9070, no need XT or Ti, just normal GPU.  You buy once, u can play without needed to worry ""will this game run on my gpu?""",buildapc,2026-02-24 14:50:12,2
AMD,o741v2w,Blender on amd is beyond fucking horrible. A 9070xt is between a 5050 and 5060   [https://nanoreview.net/en/gpu-compare/radeon-rx-9070-xt-vs-geforce-rtx-5060-ti-16gb](https://nanoreview.net/en/gpu-compare/radeon-rx-9070-xt-vs-geforce-rtx-5060-ti-16gb),buildapc,2026-02-24 10:13:14,2
AMD,o740zrb,"JCâ€¦9070XT for $1,000 already?",buildapc,2026-02-24 10:05:22,2
AMD,o75gdva,Holy crap; I got my 9070 for $650 in December. Glad I took the plunge then!,buildapc,2026-02-24 15:32:52,1
AMD,o75hkse,"What is your main priority with the PC? Gaming, Productivity, Both?  Here is a quick gaming performance & value overview based on the [Tom's Hardware GPU Hierarchy](https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html) (more fps & higher fps/$ = better)  GPU| Price | 1080p |  value| 1440p |  value |  ---|---|----|---- | ---- | ----  RX 5060ti 16GB| $830 AUD| 84.4fps | 0.102 fps/$  | 62.3fps |  0.075 fps/$ |  RX 9060 XT 16GB| $650  AUD| 80.3fps | 0.124 fps/$ | 59.4fps |  0.091 fps/$ | RX 9070 XT 16GB| $999 AUD| 119.9fps |  0.120 fps/$ | 98.3fps |  0.098 fps/$ |  And here is the blender productivity performance based on [PCMag](https://www.pcmag.com/comparisons/amd-radeon-rx-9060-xt-vs-nvidia-geforce-rtx-5060-ti-16gb-which-is-better) (more points = better):  GPU| Price | Monster|  Junkshop| Classroom|  Combined | Value ---|---|----|---- | ---- | ----  | ---- RX 5060ti 16GB| $830 AUD| 2142 points| 1168 points | 1218 points|  4528 points| 5.33 p/$ RX 9060 XT 16GB| $650 AUD| 791 points|421  points| 381 points|  1593 points| 2.45 p/$ RX 9070 XT 16GB| $999 AUD| 1569 points|835 points| 756 points|  3160 points| 3.16 p/$  So looking at the performance in productivity and gaming: For gaming, the value of AMD is 33% more than Nvidia. But for Blender Nvidia has a 68.7-117% more value than AMD. It will really come down to how much you value blender performance over gaming performance. I don't really know how you would judge the value of rendering time over gaming performance. It is very subjective.  But what resolution would you game at? Because if both the 5060ti and the 9060ti meet your performance standards in gaming, then you could consider the additional $180AUD spent on an Nvidia GPU to be an 'investment' in improved Blender performance and the question would be, is that $180AUD worth 117% more blender performance? But if neither the 9060xt or 5060ti really meet your performance standards in gaming, then it is more of a dilemma of gaming vs blender.",buildapc,2026-02-24 15:38:26,1
AMD,o77qk5f,9070 xt OR 7900 xt,buildapc,2026-02-24 21:45:55,1
AMD,o78vpom,I upgraded mine from 1660 super to 5060 ti 16gb. Still awesome! Never regret it.,buildapc,2026-02-25 01:21:58,1
AMD,o74t3dt,5060 ti is like biggest gpu scam to ever release.,buildapc,2026-02-24 13:34:31,0
AMD,o745w9g,$830 for a 5060TI? Holy shit I got mine for $500 back in like December. Crazy times,buildapc,2026-02-24 10:49:51,0
AMD,o74lg9h,"Not sure what country youâ€™re in, but if itâ€™s the US then $1k is overpriced for a 9070 XT given that you can get it for $729. When picking which 9070 XT you get, use the filters on PCPartPicker to filter by the 9070 XT, then sort by price, and the pick whatever is the cheapest. Same goes for any other GPU such as the 9060 XT, B580, 5060, etc.  https://pcpartpicker.com/product/h7RnTW/gigabyte-gaming-oc-radeon-rx-9070-xt-16-gb-video-card-gv-r9070xtgaming-oc-16gd",buildapc,2026-02-24 12:48:53,0
AMD,o748nrc,9070xt easy. Upgraded 3070 to 5060ti with barely any performance improvements,buildapc,2026-02-24 11:13:53,-3
AMD,o74f66d,"Thank you so much! I've been reading more into Blender and AMD, I'm a little uncertain, but not 100% with sticking to Nvidia yet. I appreciate you looking for cheaper options too!",buildapc,2026-02-24 12:05:18,2
AMD,o74emw0,"Thank you so much! Reading the comments here and other threads, it is a little disappointing, but I think I'll stick with Nvidia. My main focus is definitely Blender, the most recent AAA game I've played is from 2018. Would the 5070 with 12 GB be worth it, compared to the 5060 Ti 16 GB?",buildapc,2026-02-24 12:01:19,2
AMD,o74m69g,"Thatâ€™s (probably) a good deal, but it depends what the price of other cards around it, and what you want to do with it. Res? FPS target? Type of games?",buildapc,2026-02-24 12:53:30,1
AMD,o74ep5c,"Thank you! Sounds like sticking with Nvidia is worth the extra couple $100 now than regretting it later, as much as I was hoping that wouldn't be the case",buildapc,2026-02-24 12:01:47,2
AMD,o741fm7,AU.,buildapc,2026-02-24 10:09:24,6
AMD,o74bhzl,My guy one look and you could have seen it's in AUD.,buildapc,2026-02-24 11:37:28,5
AMD,o79iv0g,$360 black Friday deal,buildapc,2026-02-25 03:33:51,1
AMD,o74evou,"Thank you! Is that for gaming or Blender? I'll play a few older gen games here and there, but my main focus is Blender for this build",buildapc,2026-02-24 12:03:08,1
AMD,o76wdia,"One thing to note is that Blender 5 or 5.1 or something, not sure of the exact version, is supposed to have some RDNA4 optimisations to help 90XX GPUs perform better. It won't bring them near the performance of Nvidia in Blender, but it should help at least a little bit.",buildapc,2026-02-24 19:26:23,2
AMD,o74fb8x,"The 5070 is about 45% more performant and has more cuda and rt cores, so unless you're working with huge high poly count environments constantly, I'd say it is.",buildapc,2026-02-24 12:06:21,4
AMD,o74f8bo,"Yes, it's better price performance compared to the 5060ti 16gb. But you need to think about how important vram is for your blender workload.",buildapc,2026-02-24 12:05:44,3
AMD,o74p4l4,"Resolution 1440p, but I donâ€™t have any FPS target or requirement. I want to play any AAA, but Iâ€™m not by any means a hardcore gamer  The only req Iâ€™m looking for is 16GB ram, but for what I see, all other cards with 16GB are more expensive",buildapc,2026-02-24 13:11:43,2
AMD,o74hqgc,"Honestly just gaming.   The 5060 16gb AI features are, very well done. I personally am just more of a raw throughput kinda guy, I donâ€™t mess around with upscaling or dlss.   I guess, and Iâ€™m ignorant on what blender needs, if the question is the software advancements > raw throughput get the 5060ti 16gb, if youâ€™re looking for raw power I think the 9070xt is the way to go.   Again, Iâ€™m ignorant on this so hopefully someone more educated can chime in and correct me for ya.",buildapc,2026-02-24 12:23:45,0
AMD,o74t8xw,"Do you have any workstation use for the VRAM or are you only gaming on the PC?  If itâ€™s only gaming 16gb VRAM is definitely not needed for 1440p.  Even the 3070 8gb is faster than the 9060xt 16gb at 1440p.  In my opinion youâ€™d be better off prioritizing a more powerful card with less VRAM, and/or Nvidia to get use of the better upscaling tech.   It honestly just really comes down to how much you could get other cards for in your area. Iâ€™d rather go with a 5060ti or even a used 4070/4070 super. The 4070 12gb is ~28% faster avg at 1440p.  All depends on cost.   Check out what games you want to play: https://www.techpowerup.com/review/sapphire-radeon-rx-9060-xt-pulse-oc/34.html",buildapc,2026-02-24 13:35:22,1
AMD,o7fz6rs,The 9850x3d bundle is cheaper,buildapc,2026-02-26 02:22:25,1
AMD,o7g0f5a,Cheaper than $730? I didn't notice that one.,buildapc,2026-02-26 02:29:22,1
AMD,o57jvbg,"Just bought one to replace my 3600xt, and so far itâ€™s going great.  Itâ€™s paired with a 3070.  Solid performance in cyberpunk, all my colony sim games, and competitive titles go way above 200fps.  I play in 2k No idea for video editing though",buildapc,2026-02-13 18:41:59,7
AMD,o57na1s,Youâ€™ll be fine. Have a 5600x and 5070TI playing in 2k no problem.,buildapc,2026-02-13 18:58:05,4
AMD,o57kbnq,I have a Ryzen 5700X paired with a RX 9070. Absolutely smooth in every game and I think it will carry on for many years.,buildapc,2026-02-13 18:44:09,3
AMD,o57m498,Am I missing something or is that a great deal??  Will be great for gaming and production work. MUCH better than your 9400f.,buildapc,2026-02-13 18:52:38,3
AMD,o5b30ft,"Just so you know R5800X gets hot very fast, stays hot and it's normal for it. Either have really good cooling, and I mean really good, or prepare for lifetime with your fan whirring all the time.  I returned my 5800X same evening it was delivered to me. Got 5700X instead and I'm happy with it, as it has normal operating temps and is just a tad bit slower.  But maybe it's just me hating fan whirring noises, so for you might be ok.",buildapc,2026-02-14 07:54:22,3
AMD,o57k50l,"Gaming at 1440p will be perfectly fine. In 1080p 5800x will run out of steam. Source : I have laptop with ryzen 7 7435hs and rtx 4060. It's pretty much the same cpu, and in cpu bound games I'm getting better fps in 1440p. Battlefield 5 for example.",buildapc,2026-02-13 18:43:16,2
AMD,o57l6bu,My 5800x absolutely flies. I use a Peerless Assassin to cool it.,buildapc,2026-02-13 18:48:09,2
AMD,o57z0ca,[5000-series CPUs are still holding up great in 2026](https://youtu.be/RijAyVshtok?si=FJ2fTCrh-QJ9AKuM)  5800X is about as good as it gets without getting one of the much-desired X3D CPUs,buildapc,2026-02-13 19:55:44,2
AMD,o5beq08,Bought a used 5800x for 100 euro. Very capable cpu,buildapc,2026-02-14 09:49:12,2
AMD,o6c3cp8,"i highly \*100%\* suggest getting an FW 360x AIO (frozen warframe) -you need your own fans, its 50 bucks on amazon. keeps my 5800x cool and thats with pbo pushing to 5ghz, plus prime95, occt, ycruncher/etc cant push the cpu past 80c.  on a decent air cooler id see 90c in no time and specific games id see 80c most the time. (before someone says thermal paste, i double checked) it was the cpu just being hot af lol. anyways with the FW360x ive had no issues with high temps since. for the record, it never crashed or anything from temps. i just didnt like 80c playing games at 1440p/4k.  p.s. my 5800x is paired with a 4070ti, and 32gb of ram. have yet found a game i couldn play at 1440p or 4k.",buildapc,2026-02-20 00:07:17,2
AMD,o57m7ya,"I have 5800x and 3080. Fantastic for 1440p gaming. I play arc raiders, helldivers 2 and cyberpunk. Looks great and plays smooth. I don't do video editing, but I do photo editing in Lightroom and have no complaints.",buildapc,2026-02-13 18:53:07,1
AMD,o57nbwu,"Yup. Good shout. It's a significant enough improvement on the i5 and if you bought sufficiently fast DDR4 for your old rig, you can re-use it.  That's a decent price in the local market BTW - very good - make sure it's all working before you committ...",buildapc,2026-02-13 18:58:20,1
AMD,o584rp2,"that's a hell of a deal, i would jump on that. those cpus are still very good!",buildapc,2026-02-13 20:24:32,1
AMD,o591ws8,"It would be better than your I5.  Really this is more about how much are you willing to spend to squeeze out more performance.  For gaming there will already be situations where the 5800X would hold back a 4070Ti at 1440p, but would you be willing to spend double or triple the money for another 10% fps?  Also for many games the 4070Ti would be the bottleneck anyway.  For editing having 8 cores and 16 threads should be noticeable over your current 6 cores and 6 threads.",buildapc,2026-02-13 23:14:43,1
AMD,o57k0fo,Yeah I play 1440p too so that takes a little stress off the cpu ig,buildapc,2026-02-13 18:42:40,2
AMD,o57kiy1,"Hm, Iâ€™m wondering if anyoneâ€™s got experiencing w the editing/workflow side of things",buildapc,2026-02-13 18:45:07,1
AMD,o57mcft,Yeah I thought it was pretty good too tbh,buildapc,2026-02-13 18:53:42,1
AMD,o57ma6x,I was contemplating whether to use a fan or AIO,buildapc,2026-02-13 18:53:25,1
AMD,o585idr,"im getting a 5600x and was considering a peerless assassin, but wasnt sure what the benefit over the stock cooler (prism wraith) is?",buildapc,2026-02-13 20:28:15,1
AMD,o596o9n,Honestly to me this seems like an amazing price to performance ratio,buildapc,2026-02-13 23:43:06,1
AMD,o57m54h,"I've been running a 5600X and 6750XT for 1440p gaming since July 2023. For gaming you will have plenty of oomph, but for productivity it won't be as good as 5900X but for 100 pounds it's a solid base.",buildapc,2026-02-13 18:52:45,2
AMD,o57mizd,"I would consider just the CPU for that a decent deal already, plus a free mobo is nice.",buildapc,2026-02-13 18:54:33,1
AMD,o586afz,"totally upto you. AIO look good, while air is usually better at cooling.",buildapc,2026-02-13 20:32:11,1
AMD,o586djq,Probably nothing that would be noticable tbh.,buildapc,2026-02-13 20:32:38,1
AMD,o57mmgv,Tbh I didnâ€™t rlly feel like tryna get that price lower it seemed fair so I js took the first price,buildapc,2026-02-13 18:55:00,2
AMD,o58a2ru,might do it anyway since it's not pricey and might be quieter/reusable..,buildapc,2026-02-13 20:51:17,1
AMD,o58bik6,It will definitely be quieter than the prism cooler. It's a larger heatsink with 2 fans so the fans have less work to do.,buildapc,2026-02-13 20:58:29,1
AMD,o5biq1p,"I currently use the assassin spirit 120 evo for my 5600x, itâ€™s does the job perfectly and is both half the cost and smaller than that one is. Definitely would recommend it.",buildapc,2026-02-14 10:28:22,1
AMD,o73i985,"An Intel arc B580 runs about 350 new and you can game with 2k resolution on that. You could look for one of those used. Or try and save another $100 and buy one new... If I were you I'd aim for this.   But whilee you're looking around, if you see a 7700xt, or 3070, 2080, nab any of those if you can find them at 250. But it'll be rare.  Edit: added additional info on GPUs",buildapc,2026-02-24 07:08:45,7
AMD,o73xrx3,6600 xt,buildapc,2026-02-24 09:34:48,2
AMD,o74c6d5,3060 ti should be just in your budget if you shop around. It's really an excellent card,buildapc,2026-02-24 11:42:50,2
AMD,o73ia6e,"Well, it also depends on the monitor resolution you are going to use. For example, if it's a 1080p monitor, I would choose something around the \*\*60 tier of gpus, like 5060 or 9060xt, possibly in their 16gb vram variants, but with that budget in CAD the rose of candidates just shrinks to even lower tier ones, like the arc a580, a380 or the 3050 6gb. Otherwise, you can go on the used market in order to find a good offer, like on a good 3060ti, or an rx 6600, or even something like a 2070 or 2080 super, or a 3060 12gb you can find around those prices on eBay CA.",buildapc,2026-02-24 07:08:59,1
AMD,o73kb97,I had a 6650 xt but recently got a 3070. Same CPU.  Both run good but the 3070 is much better. I'm playing in 1080p. Games like Helldivers 2 and Arc Raiders run at about 120-150fps on the 3070. And about 90-120 on the 6650.,buildapc,2026-02-24 07:27:29,1
AMD,o73p51c,I'm using my 5600X with rx 9070xt at 3440x1440p.,buildapc,2026-02-24 08:12:02,1
AMD,o73tzb6,"With your budget, you're forced to get a used GPU.Â    I suggest getting a used RTX 2080 Super (https://bestvaluegpu.com/en-ca/history/new-and-used-rtx-2080-super-price-history-and-specs/) since every used RTX 30-series and newer Nvidia GPUs that are worth buying costs more than 250 Canadian Dollars as of time of writing this comment.",buildapc,2026-02-24 08:58:09,1
AMD,o73uykn,"For 250 CAD, I'd look at an RTX 2080 Super or 2070 Super.",buildapc,2026-02-24 09:07:37,0
AMD,o73noxo,"B580 is a solid card for the price, but it basically requires SAM/Re-BAR to function as intended. Not sure if OP's CPU & board are capable or not. Just throwing this out there.",buildapc,2026-02-24 07:58:38,1
AMD,o73nxup,"That's good to know, I didn't know about this SAM/Re-BAR, I'll have to give it a good. Thanks man.",buildapc,2026-02-24 08:00:54,2
AMD,o73upw1,"The 5600 supports ReBAR, a few 300 series motherboards and OEM ones don't AFAIK but they're few and far between.",buildapc,2026-02-24 09:05:17,2
AMD,o73z494,"If op doesnt, the 3060ti is very similar preformance, possibly go with a 8gb 5060 if u want something new",buildapc,2026-02-24 09:47:49,1
AMD,o74a7ll,Sounds awesome and congrats on the build. This post is giving me inspiration and courage to tackle my own PC build in the future!,buildapc,2026-02-24 11:26:55,2
AMD,o74axj9,"Thank you!  From a knowledge perspective I don't think there's ever been a better time to build a PC, it's simple as searching the motherboard name on YouTube, you will usually get a selection of high quality guides - the one I used was by ""FIRE WOLF TECH"" for example.   Still nerve wracking, given the expense, but miles better than it was even in 2019 when I last built",buildapc,2026-02-24 11:32:52,2
AMD,o705o90,"I use the Thermalright Assassin X120 Refined SE CPU air cooler. It has ARGB and at $17 on Amazon, it is cheap but effective for my needs.  Also, I avoid liquid coolers.... a friend of mine had his liquid cooler leak, ruining a bunch of components, so I try to avoid adding stress to my life by sticking to air coolers.",buildapc,2026-02-23 19:22:44,2
AMD,o7062hj,Assassin Peerless is a great option,buildapc,2026-02-23 19:24:34,2
AMD,o70888s,"Thermalright peerlees assassin, don't buy anything else",buildapc,2026-02-23 19:34:40,2
AMD,o70mxdy,Thermalright peerless assassin.,buildapc,2026-02-23 20:44:41,2
AMD,o704x8u,AK400 would be good.,buildapc,2026-02-23 19:19:13,1
AMD,o704xwh,"Any thermalright single tower cooler is probably the 'cheaper' option, and will handle your 7700 easily.",buildapc,2026-02-23 19:19:18,1
AMD,o7eat1w,"Update the BIOS now. Then look for a used 5600/5600X. For a used GPU, something like a 6700/6750 XT. Make sure your PSU can handle it.",buildapc,2026-02-25 21:04:09,3
AMD,o7ecnzq,what exactly is your budget also your power supply wattage,buildapc,2026-02-25 21:12:45,1
AMD,o7ebyhd,"Thank you, Iâ€™ve had a quick search and I think I can get both",buildapc,2026-02-25 21:09:29,1
AMD,o7ee1cu,"I think Â£300-Â£400 is the most I can really justify to myself, maybe more if Iâ€™m buying from a retailer and can use credit. I definitely want to spend less but if itâ€™s worth the upgrade Iâ€™ll do it.   Power supply is 700W",buildapc,2026-02-25 21:19:05,2
AMD,o7ecmr5,Nice :).,buildapc,2026-02-25 21:12:36,1
AMD,o7eebfl,do you know the specific psu,buildapc,2026-02-25 21:20:22,1
AMD,o7efvfn,It is a CIT 700w FX Pro,buildapc,2026-02-25 21:27:33,1
AMD,o7emvrn,gpu. a used rx 6700xt is great  cpu: ryzen 5 5600,buildapc,2026-02-25 22:00:08,1
AMD,o7enbtp,"Thank you, Iâ€™m going to get on the marketplace",buildapc,2026-02-25 22:02:16,1
AMD,o6vki87,"Just build it. It's so worth it, you save money along the way and you create a sentimental bond with the PC that in turn makes you take better care of it. I would rather build my own PC than be payed to have someone else build it.",buildapc,2026-02-23 01:25:03,6
AMD,o6vk66a,"build it.  read and keep your manuals close by, google and youtube will be your friends.  but also $100 isn't outrageous to build it, as that most likely also comes with cable management and OS installation/setup.",buildapc,2026-02-23 01:23:00,10
AMD,o6vkup0,"That case seems to be pretty easy to build in, and I wouldn't blow $100 to have someone else build it. What exactly are you worried about?  You can watch [this guide](https://youtu.be/MctiQFUqQc0?si=g-OjbhpQjKMOu3WM) to get an idea of how the build will go.",buildapc,2026-02-23 01:27:08,2
AMD,o6vrzlc,"Do build it yourself. Take your time, and read the manuals. It is an experience that will be rewarding for you. If you get stuck, walk away, decompress. When you can breathe again take a go at the manuals for your answers, You also have YouTube, or here for answers.",buildapc,2026-02-23 02:10:16,2
AMD,o6wjtxz,You can build it. There's a million tutorials on YouTube.  A few tips.  DO NOT drop the cpu. Make sure it is aligned correctly with the socket before you try to insert it.  Put everything on the mother board that you can before you install it into the case.   Make sure the case is ready. Remove any I/O shield and any PCi.E slots that need to be uncovered.  In general just be firm but gentle with your components.,buildapc,2026-02-23 05:16:05,2
AMD,o6vlk5n,"You're probably right, it is something I wanted to learn how to do anyways.",buildapc,2026-02-23 01:31:30,1
AMD,o6vnzyo,"Yeah honestly that's not an insane price, especially if the builder shows you how to do it along the way. It can be very helpful watching someone do it and being able to ask questions  However, there are a lot of resources online that are free. Like this sub   That $100 could be 1gb of ddr5 ram down the line",buildapc,2026-02-23 01:46:20,4
AMD,o6vlnbx,"I would like it to look very clean, so its tempting.",buildapc,2026-02-23 01:32:02,1
AMD,o6vu39e,Yeah 100$ pretty alright..i saw 200-1000$ depends on build system,buildapc,2026-02-23 02:23:03,1
AMD,o6vlh8z,"I just dont wanna break anything. I spent alot lol.   Sweet, ill check it out, thank you.",buildapc,2026-02-23 01:31:00,1
AMD,o6vs9bn,"I think thats the move. That dose lead me to a question, I bought my PSU used and it didnt come with screws. Should I buy more screws?",buildapc,2026-02-23 02:11:55,2
AMD,o6x10em,"Hey so I have a question. I just put in my cpu, ram and ssd's and was just about to put in my cooler but the screw holes dont match up with the holes around the cpu. And the plastic bit is not in the motherboard box or the cooler box... what do I do?",buildapc,2026-02-23 07:45:22,1
AMD,o6vna7e,"Its surprisingly hard to break anything as long as you're being careful.  A few bits of advice - when you insert the RAM and GPU, you may feel like you are putting too much pressure on them, but just keep applying firm pressure straight down until the little plastic latches snap into place. As long as everything is lined up, you will be fine. On the other hand, when you put the CPU in the socket, dont push it at all. Just set it on the socket and let it fall into place, otherwise you might bend some pins. It's also advisable to install everything you can on the motherboard BEFORE you put it in the case (RAM, CPU, CPU cooler, SSD, etc.) - besides the GPU obviously.",buildapc,2026-02-23 01:42:00,5
AMD,o6vtxlf,"Buy a set of extra screws for building computers, it comes in handy. If you have room in the budget for a small set of computer tools? It beats striping out screw heads.  Edit; JOREST 40Pcs Small Precision Screwdriver Set from amazon",buildapc,2026-02-23 02:22:06,1
AMD,o6yepjn,Are you sure? Your cooler should have some kind of bracket or standoff you install in the motherboard. It's usually two metal brackets that say AM5 on them. You'll probably need to remove the plastic bracket that came with the motherboard.   Don't forget to remove the plastic stickers from the cool and the SSD heatsink covers,buildapc,2026-02-23 14:24:23,1
AMD,o6yfaam,This unboxing video shows the parts that should have been in the cooler box  https://youtu.be/DyG-gTrmS5o?si=OPaMXeZwgrUdHfMG,buildapc,2026-02-23 14:27:32,1
AMD,o7e3jsy,honestly with that cpu bottleneck i'd go with teh rx 550 for now. the 570 would just be wasted on that athlon and you'd probably run into power issues anyway with that sketchy 324w psu. gta 5 might still struggle a bit but it'll be way better than integrated graphics from the stone age.,buildapc,2026-02-25 20:30:16,6
AMD,o7e8vcq,I'm not sure if a GPU upgrade would even let you play GTA 5. Your CPU and ram don't meet the minimum requirements. It might be a stuttery mess even with a better GPU.,buildapc,2026-02-25 20:55:11,6
AMD,o7e4aws,"go for Rx 570 if your budget allows it. For gta 5, I would at least look for 8gb ram. 4gb is too low.",buildapc,2026-02-25 20:33:49,3
AMD,o7e5b7b,"> For my ""goal,"" is the 550 okay?  What is your timeframe for building a new PC? It might be worth slapping a new power supply and the better GPU in that system and then migrating them to a newer build.  As for GTA5 at 60 FPS, on that PC, the verdict is: impossible. This test with an Athlon II X4 at Low settings showed the CPU at 100% usage and the game running around 40 FPS: https://www.youtube.com/watch?v=heOf3rWZZDU. Chop a quarter of the cores off and you've got some dismal performance outlook.  You will need to upgrade from your seventeen-year-old platform to get decent game performance.",buildapc,2026-02-25 20:38:34,2
AMD,o7efeyr,You don't have $100000000 for a new PC? Are you poor?,buildapc,2026-02-25 21:25:26,2
AMD,o7efter,The legendary 324 watt PSU,buildapc,2026-02-25 21:27:17,2
AMD,o7gm1ll,"There's not much you can do that will make an Athlon X3 run modern games adequately, especially with just 4 GB RAM. You're better off trying to get a somewhat more modern office PC for not much money (or, ideally, free) and putting a GPU in the RX 570 range or better in it.  Having said that, you \*can\* put a decent GPU and it \*will\* do wonders, there's videos out there of GTA V running acceptably, the RX 570 is a decent, cheap choice. But make sure you also get your RAM to at least 8 GB for modern Windows.",buildapc,2026-02-26 04:42:01,2
AMD,o7ed4dc,"I would spend the money that you would be putting into this into something older but still decent from marketplace. For a few hundred dollars, you could score something that will be better than this will be if you gave it upgrades.",buildapc,2026-02-25 21:14:51,1
AMD,o7ef7a5,"Just slap RX 550 2GB in there, good very low budget and runs GTA 5 on max setting pretty fine. (l know because it was my card) You might not be able to match my setting due to the rest of your system but yeah. RX 570 4GB is what l have and is overkill for that system and also you would have to worry about extra 8 pin that your PSU probably doesn't have. Running adapters risks burning the PC. Good luck! Also when you get bit better PC just go for RX 580 8GB or GTX 1070 8GB, later one being much better, but might be much more expensive depending on where you live, for my case personally its 10-20 bucks difference.",buildapc,2026-02-25 21:24:27,1
AMD,o7eoowo,"Is this a system you have or something you are planning on doing?  I dont see how this is more practical than picking up a dirt cheap used office PC. This is not a system you can really upgrade, the CPU is almost 2 decades old and will be your limiting factor.",buildapc,2026-02-25 22:08:56,1
AMD,o7f2lbi,"Even with an RX 550, I'm pretty sure that CPU won't even hit 60 fps on GTA V. Do you think you'd have the budget for a Ryzen 5 3400G, B550 motherboard, and a 2x4GB kit of DDR4? It would give you a platform worth upgrading in the future and can hit 60 fps in GTA V with a few tweaks to the settings",buildapc,2026-02-25 23:19:47,1
AMD,o7g1bui,"Sorry to say you have an amazingly bad pc.   A few years ago I bought my 80-year-old dad a used office computer for Â£100. It was a haswell i5 from about 2014. At least double the speed of your computer, with twice the memory.  If you were to add a cheap used GPU with low power consumption to a similar PC, then you could maybe play baldurs gate 3 on minimum graphics.  For a more powerful GPU you would also need a better PSU  This is going to sound harsh, but my dads previous, unuseably slow PC was an AMD athlon x4, which I threw away at the rubbish dump.",buildapc,2026-02-26 02:34:30,1
AMD,o7e3wi9,"I'm a little wary of that power supply, but you say I can leave the 550 there ""permanently"" since, as I said, my goal isn't a super PC by any means.",buildapc,2026-02-25 20:31:56,2
AMD,o7ebur4,"I know, I specified that everything needs to be upgraded, but for obvious reasons, the video card is the first goal. Right now I was looking into buying two 4GB RAM sticks so it can more or less move from the Stone Age to the Industrial Age lol",buildapc,2026-02-25 21:09:01,1
AMD,o7eguuh,"That's what I plan to do, gradually improve it. The most important things are the video card and the RAM, which are what I'm about to buy. Thanks for the information.",buildapc,2026-02-25 21:32:10,1
AMD,o7egdgb,"Simply horrendous, supposedly it's 500 but it's not certified and I've seen on the web that it delivers approximately that much in reality and I continue",buildapc,2026-02-25 21:29:53,1
AMD,o7eehy7,"I don't understand, the motherboard costs me around $70. I looked for cheaper ones, but I couldn't find any. I even looked for used ones on trusted sites, and they weren't any for less than $80-$90 either.  I couldn't find anything cheap and of good quality; that was really my idea from the beginning to do what you said.",buildapc,2026-02-25 21:21:12,1
AMD,o7eg5b4,"Very good recommendation, I think to avoid problems I'll go for a 550 which is more than enough for what I want, thanks for the recommendations.",buildapc,2026-02-25 21:28:49,1
AMD,o7ersu3,Obviamente lo voy a cambiar el cpu Pero principalmente necesitaba saber si la placa de video 550 era decente,buildapc,2026-02-25 22:24:07,1
AMD,o7f38yq,"We went from $70 to almost $200 lol, I'll keep improving that over time. The goal now is to get a video card and forget about it.",buildapc,2026-02-25 23:23:22,1
AMD,o7g1vmp,Okay lol I know it's really bad that's why I came to see what people recommended,buildapc,2026-02-26 02:37:36,1
AMD,o7eayfa,"Hi there! Thanks for the comment.  We ask that posts and comments be in English so they can be understood by as many people as possible. Translations on Reddit are client-side, and not all apps or browsers support auto-translate. Currently many users (and moderators) arenâ€™t able to read your {{kind}.  Could you please submit a new comment in English?  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",buildapc,2026-02-25 21:04:50,1
AMD,o7fyaan,"If you're going to upgrade, the 570 makes more sense. You'll see less improvement than you should compared to the 550, but it'll help out a lot when you get something vaguely modern.  Power-wise, you should be fine - you're looking at maybe 250ish for the entire system combined, the 570 is a pretty low-power card.",buildapc,2026-02-26 02:17:18,1
AMD,o7gldsi,"Old motherboards are significantly more expensive than what they offer, and aren't a good buy unless you're trying to build a period-piece PC.",buildapc,2026-02-26 04:37:26,1
AMD,o7g47x0,"It was not my intention to make you feel sad, I really want to help you.  You are spending time and money trying to resurrect a museum piece  I just looked on FB market, there is a Fujitsu i5-6400 - 8GB DDR4 with Win11 Pro for sale in my town for only Â£70   Add a used RX560 and you have a useable games machine",buildapc,2026-02-26 02:50:47,1
AMD,o7gm7gc,"It was translated incorrectly, 550 is showing up as 70 for me. Sometimes the translator is way too unreliable lol",buildapc,2026-02-26 04:43:10,1
AMD,o7gmcc9,"That's a great deal lol, here on FB market a machine like that is 90-120 usu the market here is kind of ""broken"" people ask for much more x general I was looking there too But nothing good.   I appreciate the advice and I imagined you didn't mean it in a bad or offensive way.",buildapc,2026-02-26 04:44:07,1
AMD,o7gn2ns,"That's also way too much, even a 570 shouldn't be more than 50. For $70 you should be looking at an RX 580 8 GB at least, and probably even something better.",buildapc,2026-02-26 04:49:12,1
AMD,o7gn9t7,Welcome to Argentina The price markup on things that come from abroad is like that; a 570 costs around 80-90 US dollars.,buildapc,2026-02-26 04:50:34,1
AMD,o7gntxo,"Ah, that would explain it. So basically about double. In that case, better spend 80 for a 570 than 70 for a 550.",buildapc,2026-02-26 04:54:29,1
AMD,o7fnjjb,Watch this video   https://youtu.be/4Gik-_0_YTk?si=b4kaF7jlYaBQH-ot,buildapc,2026-02-26 01:15:41,11
AMD,o7fp7bc,A YouTuber made a video about your build lmaoo  [https://www.youtube.com/watch?v=4Gik-\_0\_YTk](https://www.youtube.com/watch?v=4Gik-_0_YTk),buildapc,2026-02-26 01:25:15,6
AMD,o7fuugy,https://youtu.be/4Gik-_0_YTk?si=gp_HzsjX15YvKBIV,buildapc,2026-02-26 01:57:38,5
AMD,o7g1xhk,Watch Daniel Owens video on your list. His build is much much better,buildapc,2026-02-26 02:37:54,5
AMD,o7finp3,"Iâ€™d spend a bit more on a current gen GPU, I wouldnâ€™t buy a low end previous generation card for that price. Should be feasible to get a 9060xt 16gb for 450. If youâ€™re set on Nvidia you can potentially find a 5060ti 16gb for 550. Just for longevity I would recommend 16GB vram variant of the 5060ti. If not you can get a 5060ti 8GB for the same cost as the 4060ti 8GB, which I see no reason not to do.   CPU wise bhphotovideo sells the 5600xt for the same price. Not convinced you need to spend on an air cooler, but 25$ doesnâ€™t exactly make or break your budget.",buildapc,2026-02-26 00:48:06,3
AMD,o7eslmh,"Pretty solid, you'll likely get 200+ fps the majority of the time but fps will drop slightly during fights/heavy effects due to the cpu.",buildapc,2026-02-25 22:28:06,1
AMD,o7evwyd,"That's a very nice pc build you got in mind! I would change one thing though. You have the rtx 4060 ti 8gb as your gpu. That's previous gen hardware. You can get a 5060 (ti) that has better performance. But still, a very nice pc build!",buildapc,2026-02-25 22:44:40,1
AMD,o7f3en1,People still play Fortnite?,buildapc,2026-02-25 23:24:14,-6
AMD,o7fosx7,"Yes, he goes over all the reasoning as well, saw that and came here lol",buildapc,2026-02-26 01:22:55,4
AMD,o7fxp0f,"Here's also a couple of used GPU options if you want more VRAM than the $379 5060 Ti 8GB.     $371 3080 10GB (750W PSU minimum) https://www.zotacstore.com/us/zt-a30800k-10plhr-r   $440 4070 12GB (600W PSU minimum) https://www.zotacstore.com/us/zt-d40700e-10m-o      You can compare their performance here: https://www.techpowerup.com/gpu-specs/geforce-rtx-5060-ti-8-gb.c4246      The 5060 Ti has great performance, I just wish the 16GB models were at more reasonable prices.",buildapc,2026-02-26 02:13:56,2
AMD,o7f9s7e,"Hello, your comment has been removed. Please note the following from our [subreddit rules](https://www.reddit.com/r/buildapc/wiki/rules) :  **Rule 3 : No piracy or grey-market software keys**  > This includes suggesting, hinting, or in any way implying to someone that piracy, or violation of license agreements is an option.   > If a license key is abnormally cheap (think $5 - $30), it is probably grey market, and thus forbidden on /r/buildapc.  ---  [^(Click here to message the moderators if you think this was in error)](https://www\.reddit\.com/message/compose?to=%2Fr/buildapc&subject=Querying mod action for this comment&message=I'm writing to you about %5Bthis comment%5D%28https://www.reddit.com/r/buildapc/comments/1rero6i/is_this_a_good_pc_that_i_made_in_pc_parts_picker/o7f2wzr/%29.%0D%0D---%0D%0D)",buildapc,2026-02-25 23:59:35,0
AMD,o7exs98,oh ok which cpu do you recommend for me to get more fps?,buildapc,2026-02-25 22:54:18,0
AMD,o7ff0xz,great game btw,buildapc,2026-02-26 00:28:21,0
AMD,o7g797h,4070 sounds like the play here,buildapc,2026-02-26 03:08:19,1
AMD,o7fmdig,"5800x3d > 5700x3d > 5800x > 5600x > 5600.   (5700x would only really benefit in multitasking, not so much pure gaming fps)   Any of those options are fine, just depends what you want to spend. Still fine to use the 5600, just want you to have realistic expectations of it.",buildapc,2026-02-26 01:09:04,0
AMD,o7f4pdt,Not who you asked but I imagine a 5600x would work,buildapc,2026-02-25 23:31:19,-1
AMD,o7fszom,Idk man. It WAS great. And now it just sucks lol. But hey if you enjoy it good!,buildapc,2026-02-26 01:47:00,1
AMD,o6o5033,"The Asrock Pro 750g PSU is only $55 on Newegg, the 850watt version is $65.  Could sell your 16gb ram for $50 and buy a used 32gb kit off eBay for $125-150.  Could sell the 3600 for $50-60 and buy a used 5700x, 5800x, or 5800xt off eBay for $150. Whichever you can find in that price range. Just be sure to upgrade bios before you remove the 3600.  The rx580 will sell for $40-50 on fb marketplace or around $50-60 on eBay minus fees. Sell it and buy a 6600xt for around $150-175. If you have more money to spare consider the 3060ti for $200. If more you can buy a 4070 or 7800xt for $400 used.  All these upgrades will put a lot of life in your PC, especially the better GPUs will ass years of life!",buildapc,2026-02-21 21:44:31,3
AMD,o6o5fh4,"If you want a literal ""plug and play"" upgrade just get the 9060XT. Probably wont even need to get a new psu with how low the power draw is. 16gb vram and is like twice as strong as your current GPU. Mine is on a 650W but iirc the recommened is just 450. So you should be literally one upgrade away from a solid rig again.",buildapc,2026-02-21 21:46:47,3
AMD,o6o4flz,Was this after a Windows update?   The RX 580 also got an update...v26....,buildapc,2026-02-21 21:41:30,1
AMD,o6o5jjg,Thank you thatâ€™s very very helpful!  Would you recommend I upgrade all aspects of my pc?   I know Iâ€™ve been needing to do RAM and GPU,buildapc,2026-02-21 21:47:24,2
AMD,o6o5ok9,Oh perfect thank you!   I was hoping my PC wasnâ€™t a lost cause - I know the GPU is a pretty sad sight though,buildapc,2026-02-21 21:48:08,1
AMD,o6o5tm9,Worth noting i also have the same cpu but 16gb more ram. Overall i have been very happy with my purchase.,buildapc,2026-02-21 21:48:53,1
AMD,o6o5c1z,Nah Iâ€™ve had this issue for a while and I canâ€™t update windows anymore.   My PC wonâ€™t allow me to get windows 11 - apparently I donâ€™t meet the specifications for it,buildapc,2026-02-21 21:46:17,1
AMD,o6o65lh,"I personally would do all of those but if you are strapped for cash then I would focus on the GPU and PSU first,  ram 2nd, then CPU last since it's still not a terrible CPU ðŸ‘ðŸ‘",buildapc,2026-02-21 21:50:40,1
AMD,o6o5zxi,The CPU has definitely done me good. I havenâ€™t changed a thing about my build excluding the monitor in almost 7 years.  So itâ€™s definitely done well by me,buildapc,2026-02-21 21:49:50,1
AMD,o6o6jid,That is weird. The 3600 is Win11 compliant. Are your UEFI and secure boot settings on in BIOS? Is your boot drive MBR or GPT?,buildapc,2026-02-21 21:52:42,1
AMD,o6o6dl7,Sounds good to me! Time to start saving!   Thanks again! :),buildapc,2026-02-21 21:51:50,2
AMD,o6o70qf,Seconding the cpu. As somebody also on a 3600 i genuinely dont think that theres an affordavle upgrade thats worth the money unless you can find a good deal. At 1440p the 3600 gets basically no bottleneck from my 9060XT and the only REAL upgrade that i think would be worth it is any X3D cpu. Otherwise youre paying twice the price for like 10 extra frames.,buildapc,2026-02-21 21:55:15,2
AMD,o6o6rga,"I wonâ€™t lie, I have no idea what that means.  Someone once told me BIOS can break my whole PC so Iâ€™ve always been anxious to go in there.   Iâ€™ve done some tinkering here and there for specifics but never perused and looked at things   I just assumed my GPU was so ass it wouldnâ€™t let me go to windows 11",buildapc,2026-02-21 21:53:52,1
AMD,o6o7u1g,"Woot! I recently did a large upgrade on my wife's PC, I bought a used 5800x from ebay, new mobo and Psu combo on Newegg, but she is still using 16gb ram since 32gb is expensive and her games don't require more right now.   Anyways cheers and best of luck on getting your upgrades!",buildapc,2026-02-21 21:59:35,1
AMD,o6oep88,[Checking partitions](https://www.google.com/search?q=how+to+check+mbr+or+gpt&oq=&gs_lcrp=EgZjaHJvbWUqGwgFEAAYQhiRAhi0AhjqAhiABBiMBBiKBRjnBjIbCAAQABhCGJECGLQCGOoCGIAEGIwEGIoFGOcGMhsIARAAGEIYkQIYtAIY6gIYgAQYjAQYigUY5wYyGwgCEAAYQhiRAhi0AhjqAhiABBiMBBiKBRjnBjIbCAMQABhCGJECGLQCGOoCGIAEGIwEGIoFGOcGMhsIBBAAGEIYkQIYtAIY6gIYgAQYjAQYigUY5wYyGwgFEAAYQhiRAhi0AhjqAhiABBiMBBiKBRjnBjIGCAYQRRhA0gELMTg4NTg3NGowajmoAgawAgHxBTYUyJbG3Iz_&sourceid=chrome&ie=UTF-8)...Â   [Checking UEFI.](https://www.google.com/search?q=how+to+check+uefi+firmware+settings&sca_esv=da81959cc233b65d&ei=9HaaaYWsE767kPIP0MHm6Ak&biw=1280&bih=646&ved=0ahUKEwiF9uWMk-ySAxW-HUQIHdCgGZ0Q4dUDCBM&uact=5&oq=how+to+check+uefi+firmware+settings&gs_lp=Egxnd3Mtd2l6LXNlcnAiI2hvdyB0byBjaGVjayB1ZWZpIGZpcm13YXJlIHNldHRpbmdzMgUQABiABDILEAAYgAQYhgMYigUyCxAAGIAEGIYDGIoFMgsQABiABBiGAxiKBTIFEAAY7wUyBRAAGO8FMgUQABjvBTIIEAAYgAQYogQyBRAAGO8FSLkvUOwFWNwUcAF4AZABAJgBb6AB1wOqAQM0LjG4AQPIAQD4AQGYAgagAp0EwgIKEAAYsAMY1gQYR8ICBhAAGAcYHpgDAIgGAZAGCJIHAzMuM6AHoCeyBwMyLjO4B5EEwgcFMi0xLjXIBzOACAA&sclient=gws-wiz-serp)..,buildapc,2026-02-21 22:37:23,1
AMD,o6o827b,"I was always scared of buying used but it seems it might be the way to go!   Hopefully RAM will treat me good, although I do love modding and competitive games.",buildapc,2026-02-21 22:00:46,2
AMD,o74jbsq,What would i do with these?,buildapc,2026-02-24 12:34:45,1
AMD,o6o90dn,Thankfully I've never been stiffed on  used PC parts ever! I just look over profiles and eBay has a fantastic buyer protection refund policy!   Does your mobo have 4 ram slots or 2?,buildapc,2026-02-21 22:05:52,1
AMD,o754bwy,May explain why Win11 refuses to install. It requires GPT and UEFI secure boot.,buildapc,2026-02-24 14:34:25,1
AMD,o6od3za,Yeah I have 4 slots! So could always get 2 more 8GB,buildapc,2026-02-21 22:28:28,2
AMD,o7dyqck,"After installing my os my computer was stuck in a boot loop, I had to press the reset button on the board, it then booted right into the os.   Hope this helps",buildapc,2026-02-25 20:07:33,1
AMD,o7erwi8,Faulty motherboard.,buildapc,2026-02-25 22:24:37,1
AMD,o7c5c7i,"Newer computers are better at actually using all the resources available to them. Depending on how you're converting, you could set a limit on processor speed or amount of cores/threads to use when converting. If that's not part of the program you're using, you can go into task manager and set the program to not use all of your processor cores from the details tab",pcmasterrace,2026-02-25 15:08:28,2
AMD,o7c9xa9,">\- shouldnt the new computer be many times faster, to be able to perfom better then the old one?  That hasn't been true for quite a while because even though the technology has improved we really don't need all that performance.  It's kinda like the megapixels in digital cameras.  In the early days every additional megapixel added to the sensor made a huge difference in quality.  But once we started going beyond 6 megapixels (don't quote me on the exact number) the image quality did not have as much of an impact.  The difference between a 50 and 100 megapixel camera is only beneficial to applications that can take advantage of it.  Just like a computer with 24 cores on the processor are mostly useless until you run an application that can take advantage of all 24 cores at the same time.  >\- wondering, does a descrete graphics make such a huge impact in a pc perfomance?  Similar answer as above as you need an application like a game to truly see the impact.  Otherwise merely browsing the web, checking email, and watching Youtube ain't gonna make a difference even if it's a discrete vs integrated GPU.  There are many reasons why your newer system could have felt slower than your older system, but have you ever compared the time it takes to convert between the two systems on the same OS?  For all you know the conversion time on the newer system might be faster than the old but you have to run them both on the same OS and do your time comparisons.  It could be the way the software is configured on the old system that allows it to take advantage of hardware acceleration while the new system isn't set up properly.  Also your old system got to use those drives while they were young and fresh while the new system has to use the same drives while they're being grandpas.  You are trying to compare a server processor to a desktop processor and servers do excel at multiple I/Os.  That said the technology gap should've rendered this argument moot.",pcmasterrace,2026-02-25 15:30:14,2
AMD,o7c8ude,IGPUs are trash at basically everything except browsing the internet and doing productivity tasks.,pcmasterrace,2026-02-25 15:25:12,1
AMD,o7c9cxj,"Use Task Manager to drop the priority of the processes doing the conversion.  Media encoding is a very intensive multi-threaded multi-core operation. When you moved to the new PC, you probably updated your software too, which would have brought more highly efficient video codecs like AV1 or HEVC, which you wouldn't have been using before.  If you have a profile to use the GPU's video encoder, VCN2.2, use that. This is likely what you were doing on the GTX 1050. GPU encoders aren't as efficient but they are much faster than CPU encoding.",pcmasterrace,2026-02-25 15:27:37,1
AMD,o7ca26m,"that will be very helpful, thanks",pcmasterrace,2026-02-25 15:30:52,1
AMD,o7cecri,"interesting explanation, thanks",pcmasterrace,2026-02-25 15:50:49,1
AMD,o7ca0hj,"its what i always thought too  its hard to know before hand as all reviews point to gaming, there is pretty much nothing on productivity performance tests",pcmasterrace,2026-02-25 15:30:39,1
AMD,o7ggnll,Some are excellent encoders,pcmasterrace,2026-02-26 04:05:44,1
AMD,o74p2n9,"Thank fuck for OptiScaler... wow. I got a 9060xt during last black Friday, and I was getting heavy buyer's remorse with it until I installed OptiScaler a few weeks later.",pcmasterrace,2026-02-24 13:11:23,136
AMD,o74lp9d,"But amd is just a small mom and pop company, you can't expect them to achieve what some nerds do in their free time.",pcmasterrace,2026-02-24 12:50:28,434
AMD,o7519yo,"im convinced AMD has like, 2 guys working in the driver development side",pcmasterrace,2026-02-24 14:18:34,20
AMD,o74vc8m,"FSR4 working great in both Doom TDA and India Jones using vk to dx bridge in OptiScaler 0.9 test build. Thanks OptiScaler devs. FSR4 looks so much better than TAA or XeSS ultra quality and it's really inexcusable that AMD hasn't released FSR4 for vulkan yet, especially with the actual upscaling quality being so damn good. How stupid to have something so good and not just release it.",pcmasterrace,2026-02-24 13:46:49,26
AMD,o74say9,"The obvious move is to make Optiscaler part of the actual driver package for AMD cards, because its the only way for an AMD user to be able actually leverage the feature set of the cards.",pcmasterrace,2026-02-24 13:30:02,21
AMD,o74ql0j,Nvidia shifting to AI should be a all hands on deck moment for AMD. They have a huge opportunity here to capture gaming market share. Wake up AMD.,pcmasterrace,2026-02-24 13:20:12,54
AMD,o74yoe3,imagine the backlash if there was no such thing as optiscaler.,pcmasterrace,2026-02-24 14:04:50,6
AMD,o74o4bt,meanwhile dlss 4.5 works on any rtx card. amd are a joke lol,pcmasterrace,2026-02-24 13:05:34,38
AMD,o76qv9q,"At this point even fanboys need to admit it: AMD is not supporting FSR4 on older GPUs, not because itâ€™s difficult, but because they donâ€™t want to! (hope of people buying newer cards for FsR4 maybe?!)",pcmasterrace,2026-02-24 19:01:18,4
AMD,o74u4c1,AMD Radeon is addicted to bad PR. Another Marketing Disaster! ðŸ”¥,pcmasterrace,2026-02-24 13:40:12,14
AMD,o76fmq6,"As if AMD can do anything right, all they do is ignore the issues, even the issues that is known among most users that post here.  They good at something tho making bad decisions.",pcmasterrace,2026-02-24 18:11:45,3
AMD,o74sc2h,Bought aprebuilt with 9070xt. Realized pretty fast just how little support fsr 4 has. Didn't want to continue using shitty fsr 3.1 so I just splashed on a 5070 ti. After revisiting some games it's insane the quality difference between fsr 3 and DLSS 4.5.,pcmasterrace,2026-02-24 13:30:13,14
AMD,o79t1wz,Amd to busy to find a way to make it a next gen card exclusive,pcmasterrace,2026-02-25 04:39:02,2
AMD,o74sz8j,"AMDâ€™s lack of support for older GPUâ€™s is whatâ€™s gonna do Radeon in. With them discontinuing driver support for RDNA 2 itâ€™s going to be a difficult ask to buy into their GPUâ€™s. RDNA 3 is likely to be unsupported this year, and RDNA 4 will be unsupported presumably once UDNA comes out.   Unless youâ€™re using Linux, there is little reason to go for AMD if you care at all about card longevity. NVIDIA may be more pricey, but youâ€™re also buying years and years of support along with it. And if Radeonâ€™s going to stop supporting their GPUâ€™s the moment a new generation releases, they might as well stop making GPUâ€™s. Itâ€™d save them the bleeding millions later on they could spend on making Ryzen the best it can be.",pcmasterrace,2026-02-24 13:33:53,8
AMD,o74pr91,The todd howard method of feature introduction. Very based,pcmasterrace,2026-02-24 13:15:25,4
AMD,o74sozu,they can do this because it's a hacky solution. A proper solution would be for the game devs to incorporate it into their games. AMD can't force them to do that,pcmasterrace,2026-02-24 13:32:16,5
AMD,o75a9mw,Optiscaler singlehandedly carrying amd. If it weren't for them their shitty ass software suite would be completely annihilated by nvidia's. Now it's only getting buttfucked every wednesday.,pcmasterrace,2026-02-24 15:03:58,3
AMD,o753int,"Itâ€™s a shame we have to rely on Optiscaler, AMD sure as hell isnâ€™t going to give us INT8 FSR 4 so have to do it on our own.",pcmasterrace,2026-02-24 14:30:11,2
AMD,o76i0wc,"Is this now on windows? It's been available for a while  through Proton on Linux, for every game that has fsr as an option.",pcmasterrace,2026-02-24 18:22:18,1
AMD,o78lb1s,How does FSR work fine on Linux where everything runs through Vulkan but not on Windows? AMD really slacking on this field tbh.,pcmasterrace,2026-02-25 00:25:13,1
AMD,o758y3q,AMD a joke fr,pcmasterrace,2026-02-24 14:57:33,2
AMD,o75gtni,Okay but what about DLSS4.5? have you seen how it is #NVDA good?,pcmasterrace,2026-02-24 15:34:56,0
AMD,o77seui,I just bought a 5080 to replace my 9070XT. Got it on launch and gave them a year to impress me but alas they dropped the bomb again. If anyone is looking for a 9070XT hellhound w/ MC 3yr warranty hit me up :),pcmasterrace,2026-02-24 21:54:31,0
AMD,o75oed6,"What a disgrace, without optiscaler 9000 series would have been a failure.",pcmasterrace,2026-02-24 16:09:26,-1
AMD,o74q4yn,What is optiscaler?,pcmasterrace,2026-02-24 13:17:37,18
AMD,o74np1r,"Nobody buys their GPUs, how could they possibly fund much more than this? Being the hardware partner for the PlayStation, Xbox, Steam Deck, and the modern go-to for CPUs? They're basically flat broke.",pcmasterrace,2026-02-24 13:02:57,163
AMD,o74vxxi,"Their GPU division is probably 4 dudes and an AI modeled after Ruby, the old ATI mascot.",pcmasterrace,2026-02-24 13:50:04,14
AMD,o7ahykd,The way r/Radeon talks about them sometimes you would unironically think AMD is some small scrappy startup in someone's garage,pcmasterrace,2026-02-25 08:00:41,2
AMD,o76b8lk,Ngl was feeling the same way with my 3070 but man optiscaler saved my vibe,pcmasterrace,2026-02-24 17:52:18,1
AMD,o76su53,"obviously no, but they still have finite resources and are fighting up uphill battle against Nvidia. just because they could doesnâ€™t mean it makes sense for the business to.",pcmasterrace,2026-02-24 19:10:13,1
AMD,o75whg2,Jimbo's only an intern.,pcmasterrace,2026-02-24 16:45:36,8
AMD,o79ntx4,One of them is just the buildings janitor.,pcmasterrace,2026-02-25 04:04:19,3
AMD,o79p76l,Amd is a hardware company as large as Nvidia with like software as a hobby project.   Nvidia has a software company as large as their hardware company making their drivers.   Unfortunately amd keeps trying to make unified architectures and then abandons them when they're not ideal.,pcmasterrace,2026-02-25 04:13:11,1
AMD,o754ouz,FSR4 looks fine but I can see why even game developers aren't moving on it when amd gpus are like 10% of the market.,pcmasterrace,2026-02-24 14:36:17,1
AMD,o74rj17,"Why would they care lol, gamers aren't buying double digit billions of GPU like data centers are.",pcmasterrace,2026-02-24 13:25:39,75
AMD,o74skst,"They are also shifting to AI, from a purely monetary perspective they can sell AI GPUs for substantially more profit than consumer gaming GPUs. AMD is currently investigating heavily in trying to bring down power consumption for AI GPUs.",pcmasterrace,2026-02-24 13:31:36,17
AMD,o74samq,AMD is also trying to be an AI company eventhough nobody takes them seriously in that space. Have you seen the AI bloat in their software?,pcmasterrace,2026-02-24 13:29:59,13
AMD,o75hsyn,AMD is just as guilty as Nvidia. They're all on board on A.I.,pcmasterrace,2026-02-24 15:39:29,5
AMD,o75losl,They want the ai money too.,pcmasterrace,2026-02-24 15:57:10,4
AMD,o74tlgp,Nvidia is shifting to AI but they still have a dedicated Geforce Team. AMD is just so lost.,pcmasterrace,2026-02-24 13:37:18,6
AMD,o751pvk,Amd just signed a deal with meta for AI gpus. They donâ€™t give a fuck about gaming. Data centers are way more lucrative.,pcmasterrace,2026-02-24 14:20:52,1
AMD,o753vtu,It's more profitable for AMD to sell for AI-stuff with 5x the margin.,pcmasterrace,2026-02-24 14:32:05,1
AMD,o760kmw,I canâ€™t believe gamers think AMD is their friend,pcmasterrace,2026-02-24 17:03:49,1
AMD,o74xr6z,"Most of DLSS 4.5 works on any RTX card, you can't use Dynamic MFG on anything except 50-series... but can use other features from 4.5",pcmasterrace,2026-02-24 13:59:50,1
AMD,o7512fa,"Amd and meta just signed a deal for AI gpus. Amd didnâ€™t give a flying fuck before, but now I doubt weâ€™ll see any movement regarding fsr4. I have a 4090 rig and a 9070xt rig, itâ€™s a shame because RDNA4 is solid hardware but the software still isnâ€™t there.",pcmasterrace,2026-02-24 14:17:29,4
AMD,o74u9i6,IMO the fact that DLSS can be upgraded by simple DLL swap (or even simpler DLSS swapper or Nvidia app) is one of its biggest advantages over FSR,pcmasterrace,2026-02-24 13:40:59,16
AMD,o750wx6,"You clearly never had a 9070 XT. A 9000 used can enable FSR 4 override in Adrenalin to upgrade almost every FSR 3.1 game to FSR 4, **exactly** like you got DLSS 4.5 in DLSS games via NVAPP.",pcmasterrace,2026-02-24 14:16:40,20
AMD,o74yx14,You can upgrade fsr 3.1 to fsr 4 through the driver if on rdna 4,pcmasterrace,2026-02-24 14:06:08,10
AMD,o79jz1o,I've had a 9070XT since launch day and the Windows drivers got FSR3.1->FSR4 support pretty quickly after my card arrived.,pcmasterrace,2026-02-25 03:40:32,1
AMD,o7519kn,AMD didn't discontinue driver support for RDNA 2.,pcmasterrace,2026-02-24 14:18:30,3
AMD,o75ccks,RDNA3 is 2 years newer than RDNA2 and its ISA has more in common with RDNA4 and than 3.  There's no way it's ending this year.,pcmasterrace,2026-02-24 15:13:58,1
AMD,o751nl1,"I was all set to buy a 9070 XT until they decided to no longer support even the previous generation cards for FSR4.Â   After that came out I bought a 5070 FE instead, have a 5070 Ti on the way, and have a B580 being delivered today.Â   All of those cards could have been AMD cards if they hadnâ€™t taking a huge dump on owners of their previous generation cards.",pcmasterrace,2026-02-24 14:20:32,1
AMD,o75hxm4,It just works.,pcmasterrace,2026-02-24 15:40:04,2
AMD,o74wtae,"If a ""hacky"" solution works this well it means that AMD could add an option to force enable FSR4 directly in through driver.   Devs don't have to do anything.",pcmasterrace,2026-02-24 13:54:45,12
AMD,o77i3nu,Yeah,pcmasterrace,2026-02-24 21:07:12,1
AMD,o74re5y,"A way to force different upscalers through an external program manually placed into the game code. Very useful for AMD and Intel gpus, as it can unofficially force a newer version of an upscaler in older or unsupported games.   Itâ€™s typically used to put fsr4 support in many games, seeing as the supported games list is still pretty small. In this case, thereâ€™s now a way to force fsr4 into vulkan api based games, which even AMD have been unable to do so far.  Mind you, itâ€™s risky to try this in multiplayer games with anticheat, as the anticheat could notice injected files made by optiscaler and consider it hacking.",pcmasterrace,2026-02-24 13:24:52,75
AMD,o74xw7k,"> OptiScaler  if a game has DLSS only, OptiScaler can be used to replace DLSS with XeSS or FSR 3.1",pcmasterrace,2026-02-24 14:00:36,2
AMD,o74qnpy,Software that allows to get a fraction of DLSS superiority for amd users,pcmasterrace,2026-02-24 13:20:37,-38
AMD,o74tjkw,Itâ€™s not like the entire Linux gaming community loves their GPUs or anything. Poor little AMD,pcmasterrace,2026-02-24 13:37:00,69
AMD,o74xiu4,r/amd says this unironically,pcmasterrace,2026-02-24 13:58:35,17
AMD,o75vl25,Thatâ€™s true nobody buy there gpus only if have no choice due budget reasons since  Nvidia wanna discontinue a lot of it gpus for ai only got amd gpus as budget ends,pcmasterrace,2026-02-24 16:41:36,1
AMD,o78levd,"Game developers are moving onto it though, the issue is entirely on AMD not having official Vulkan support yet for FSR4.",pcmasterrace,2026-02-25 00:25:47,3
AMD,o74t9ap,Exactly this. They also know the gaming market will be there after the bubble bursts (sooner the better),pcmasterrace,2026-02-24 13:35:25,14
AMD,o74zcef,"Nobody takes them seriously? AMD got an unprecedented YoY growth for 2025.    For a company that no one takes seriously they're making quite a lotta money, it seems.    Hell it even ramped in Q4, it was about 3-3.5 billion for the data centre segment in Q2 and almost 6 billion in Q4.    Almost doubling the revenue in 6 months is insane.",pcmasterrace,2026-02-24 14:08:26,10
AMD,o761bpv,> they still have a dedicated Geforce Team  The guys who sold GPU's with Reflex 2 on the box 13 months ago and then forgot? :D,pcmasterrace,2026-02-24 17:07:15,0
AMD,o74zlwp,I can use Mfg and i dont..dont see the point on anything more than fg2x,pcmasterrace,2026-02-24 14:09:50,-2
AMD,o74wx6y,You can also do that with FSR for a while now it just doesnâ€™t work with older versions cause they used a different approach.,pcmasterrace,2026-02-24 13:55:20,14
AMD,o755rzm,the Nvidia App can update any version of DLSS not just a specific version which the game might not have.,pcmasterrace,2026-02-24 14:41:48,8
AMD,o751m1p,People on this sub have been caught hook line and sinker by Nvidia. There's no point arguing with them.   And I'm not saying AMD is perfect before any of you come at me.,pcmasterrace,2026-02-24 14:20:18,8
AMD,o757s7r,Never saw an override button. Either way DLSS is still clear of fsr regardless of version. So happy with my switch. Nvidia frame gen is also a lot clearer. Pluss I have nvidia hdr. This is coming from a longer amd user. Not just an nvidia fanboy.,pcmasterrace,2026-02-24 14:51:51,-4
AMD,o77as8b,AMD always grabs defeat from the clutches of victory. There is very little doubt in my mind that they discontinue RDNA 3 driver support or put it in the bullshit RDNA 1 & 2 branch that definitely wonâ€™t quietly enter maintenance mode.,pcmasterrace,2026-02-24 20:33:19,1
AMD,o75iiq5,SIXTEEN TIMES THE FRAMES,pcmasterrace,2026-02-24 15:42:45,3
AMD,o750aem,"AMD says the game dev has to and there is more to optiscaler than just a switch to enable that's making that happen   Serious question here. I really am curious. Are we talking about 3 games that are older than the GPUs? If so, why would we need any upscaling/fg for these games on these gpu's?    It just seems to me that it's just not worth the time and effort or that I am missing something here. I'd rather be proven wrong and be right going forward than continue being wrong",pcmasterrace,2026-02-24 14:13:24,2
AMD,o76f1wq,"they can't because of the legality of a different company replacing Nvidia's DLL would look VERY bad. Thats the main reason why optiscaler can do it and AMD cant. if something breaks officially under AMD, AMD takes all the heat. If something breaks under optiscaler, its YMMV because its an unofficial mod and their not obligated to do anything specific because its not a company.",pcmasterrace,2026-02-24 18:09:13,1
AMD,o74zlfm,"It doesn't work well, tho. It works okay. And hacks don't cut it in actual development.",pcmasterrace,2026-02-24 14:09:45,1
AMD,o74xb4i,"Oh, thanks for the very detailed explanation, my buddy has an AMD GPU, I will tell him about it",pcmasterrace,2026-02-24 13:57:25,12
AMD,o751lzu,It was great for boarderlands 4 uising FSR4 on my 7900xtx,pcmasterrace,2026-02-24 14:20:18,8
AMD,o74uv9p,Couldn't be more wrong,pcmasterrace,2026-02-24 13:44:16,15
AMD,o76ediz,"You make it sound like Nvidia users don't use it. For example, Ampere users can freely use it to enable frame gen for gpus abandoned by nvidia for said feature, whether one likes it or not.",pcmasterrace,2026-02-24 18:06:14,1
AMD,o74zyxf,"I use it with every DLSS game on Nvidia, because it's the only way to unfuck DLSS.",pcmasterrace,2026-02-24 14:11:44,1
AMD,o76nk2n,"That's an unfair comparison, Linux users don't have to use AMD's drivers. They get to use RADV instead, which has a grand total of 0 AMD engineers working on it.",pcmasterrace,2026-02-24 18:46:42,12
AMD,o74umex,"As a non-profit little org, they should have all the help they can get from the community.",pcmasterrace,2026-02-24 13:42:57,16
AMD,o758lmz,"I wouldn't say love, but I do feel called out now.",pcmasterrace,2026-02-24 14:55:51,5
AMD,o7560l7,They also know most folk will flock back to Nvidia the moment the bubble bursts also.,pcmasterrace,2026-02-24 14:43:02,7
AMD,o74zolk,It's more to do with Nvidia not having enough capacity than AMD being better. Nvidia is still the goto for Graphic Cards.,pcmasterrace,2026-02-24 14:10:13,5
AMD,o78xcdy,The same one that released DLSS 4.5. Much better than whatever the hell AMD is doing which is just sad.,pcmasterrace,2026-02-25 01:31:21,2
AMD,o750c6b,"MFG, and Dynamic MFG are not the same...  Yes,  you can use MFG, and use 2x or 4x etc... but what you can't do is set a target FPS, then have D-MFG only generate frames in-between when needed to hit the target FPS, rather than a set multiplier...",pcmasterrace,2026-02-24 14:13:40,2
AMD,o75algi,"If I'm not mistaken, DLSS1 can't be overridden, but it is such a small number of games now and those titles are old enough that it really doesn't matter anymore.",pcmasterrace,2026-02-24 15:05:33,4
AMD,o75bod1,"Only in games Nvidia whitelisted. If they didn't, like Diablo 2 Resurrected, you have to completely uninstall Nvidia App to make override work via Profile Inspector, else NVAPP reverts settings on each launch and you're stuck with DLSS 2 maxing out at ""Quality"".",pcmasterrace,2026-02-24 15:10:49,3
AMD,o75atx3,"Well, not really 3, more like around a 100 with FSR2 and 3, seeing how seemingly optiscaler is able to switch out both with FSR4.   Mind you, a lot more of the games listed on [the wiki](https://www.pcgamingwiki.com/wiki/List_of_games_that_support_high-fidelity_upscaling) where I got my rough count from has a lot more DLSS games that can be ""updated"" with Nvidia latest, than there are games with AMDs fsr games for FSR4.",pcmasterrace,2026-02-24 15:06:43,3
AMD,o76fz1c,>they can't because of the legality of a different company replacing Nvidia's DLL would look VERY bad.  That is not the issue here. Optiscaler replaces FSR2 and FSR3 with FSR4. AMD could very well do the same through its own official drivers.,pcmasterrace,2026-02-24 18:13:15,1
AMD,o74zfwm,"Definitely let them know, itâ€™s a pretty handy tool coming from someone with an rx 9070xt and a few Rdna 3 based gpus. In my experience, Iâ€™ve found quite a few games with shitty FSR implementation, none at all, or awful anti aliasing, so having something to improve it is a godsend.   Mind you, it does impact performance a bit- especially on non rdna 4 gpus- so their milage may vary.",pcmasterrace,2026-02-24 14:08:57,8
AMD,o753ty7,"I personally use it for Deep Rock Galactic and the demo for a new indie game, Far Far West (ironically, both unreal games). Works as great anti aliasing when FXAA and TAA are the only options. Also, cause I have it swapping with DLSS, which is the only supported upscaler in FFW.",pcmasterrace,2026-02-24 14:31:49,3
AMD,o74vnrn,"Ok, for non-rdna4 amd users",pcmasterrace,2026-02-24 13:48:34,-17
AMD,o76tb4h,*Mesa,pcmasterrace,2026-02-24 19:12:23,1
AMD,o75b608,"Does it matter? AMD is still making several times yearly gaming GPU revenue in one quarter. The only reasonable option is going ham for data centers.Â    Btw AMD datacenter GPUs with same TFLOPS at half price of Nvidia still make more money than gaming GPUs.Â    I hate it, but data center is the way.",pcmasterrace,2026-02-24 15:08:20,2
AMD,o74zycd,"It's really not. You can't just change your entire software stack on a whim like that. That works for normal consumers, not for extremely hardware restrictive research. This was planned in advance.",pcmasterrace,2026-02-24 14:11:39,2
AMD,o750q6x,dynamic mfg is still to be released tho ?,pcmasterrace,2026-02-24 14:15:42,3
AMD,o75cjrx,According to PCGamingWiki the games with DLSS1 are:  Anthem (cant even play it anymore)  Battlefield V  Final Fantasy XV  Metro Exodus (irrelevant cause the Enhanced Edition has DLSS2)  Monster Hunter World  Outpost Zero,pcmasterrace,2026-02-24 15:14:55,5
AMD,o75cd9i,"Yeah, it can't work for DLSS 1 because it was AI image upscaling. Starting with DLSS 2, DLSS is a TAA(U).",pcmasterrace,2026-02-24 15:14:03,2
AMD,o75eoq4,"I'd like to see AMD get it working. If fsr4 wasn't so much better than anything previous then it wouldn't be a big deal. I definitely saw the difference between 3 and 4 in Hogwarts Legacy (with the dll swap, not optiscaler). FSR3 gave a lot of fps but the hair was horrible and distracting. Then I noticed everything else also looked way better with fsr4   However, using optiscaler, wouldn't it still be better to use dlss4(4.5) instead?",pcmasterrace,2026-02-24 15:24:56,1
AMD,o76gzob,did AMD not release the FSR override option for the DLLs with the redstone update?  > Optiscaler replaces FSR2  FSR2 doesn't use a DLL. the DLL was added in AFAIK during FSR 3.1. You can't swap something that doesn't exist.,pcmasterrace,2026-02-24 18:17:44,2
AMD,o755qep,"So I'm trying to understand. Amd said performance would not be good and choose to not release fsr4 on older GPU just the 9xxx series. Hacker man's on the internet got it working on older amd GPUs, but it does impact performance like amd has said. So how far back of a GPU can we go before performance goes unplayable??   Also completely unrelated, but I do remember a time where people didn't want dlss or fsr, they wanted real frames and games to be optimized...I'm showing my age now xD.",pcmasterrace,2026-02-24 14:41:35,5
AMD,o76yom7,"RADV is a component of mesa. AMD does have engineers on Mesa, they author most of the OpenGL driver. I specified RADV to note that AMD does not have anyone helping the Vulkan driver.",pcmasterrace,2026-02-24 19:37:06,5
AMD,o75aumg,"> dynamic mfg  officially not yet no, April they said.... :)",pcmasterrace,2026-02-24 15:06:49,1
AMD,o76muw7,>did AMD not release the FSR override option for the DLLs with the redstone update?  Only works on 9000 series cards.  Optiscaler works even on 6000 series.,pcmasterrace,2026-02-24 18:43:40,1
AMD,o75cr8l,">So how far back of a GPU can we go before performance goes unplayable??  FSR 4 on RDNA 2/3 is times faster than DLSS 4.5 on RTX 2000/3000, so AMD really had no reason to not release it.",pcmasterrace,2026-02-24 15:15:54,6
AMD,o758d8j,"No clue, but Iâ€™ve seen a few reports of people using it on RDNA 1 and older. So, pretty much the limitation is whether or not the hit to performance is worth it, like you asked.  As for your other point, I agree, for the most part. Unfortunately, a lot of games are now relying on TAA now, which almost always looks absolutely awful at native, and having the feeling of higher fps with lower end hardware isnâ€™t awful. That being said, if I donâ€™t need either Iâ€™m not using either lol.",pcmasterrace,2026-02-24 14:54:42,1
AMD,o6k0rv8,"[https://www.asus.com/motherboards-components/motherboards/tuf-gaming/tuf-gaming-b650e-e-wifi/helpdesk\_qvl\_cpu?model2Name=TUF-GAMING-B650E-E-WIFI](https://www.asus.com/motherboards-components/motherboards/tuf-gaming/tuf-gaming-b650e-e-wifi/helpdesk_qvl_cpu?model2Name=TUF-GAMING-B650E-E-WIFI) apparently, yes.",pcmasterrace,2026-02-21 05:24:25,2
AMD,o6nv80a,Weird you would come here instead of going to ASUS's website. I would think the people that made the board would know,pcmasterrace,2026-02-21 20:52:49,2
AMD,o73l40x,Laut Handbuch lÃ¤uft der 9950X auf diesem Board. Allerdings gibt/gab es das LAN Problem beim Asus B650E. Ob das bereits gefixt wurde ?,pcmasterrace,2026-02-24 07:34:53,1
AMD,o6o4o3a,Your prob right lol,pcmasterrace,2026-02-21 21:42:44,1
AMD,o7621oh,"Seems more than reasonable.  It's also going to have DLSS and all that jazz.  DS1 ran amazingly well on PC, so I have confidence that DS2 will be good as well.",pcmasterrace,2026-02-24 17:10:31,9
AMD,o767pae,https://preview.redd.it/oyjksvvpahlg1.png?width=1080&format=png&auto=webp&s=6c4afff855884ff32b822bef5fc61cf04000de37,pcmasterrace,2026-02-24 17:36:24,2
AMD,o763u2g,Is that requirements with dlss or native?,pcmasterrace,2026-02-24 17:18:39,1
AMD,o765na0,Not bleeding edge tech in this from here. Makes me hopeful that it will run pretty decent out of the box. Probably just Beetlejuiced by posting this......,pcmasterrace,2026-02-24 17:26:56,1
AMD,o7632d3,I think the trend this year will be 16GB of RAM more than anything thanks to AI!,pcmasterrace,2026-02-24 17:15:10,2
AMD,o7651n2,I think that they are for native resolution because otherwise they would mention it explicitly!  [Source](https://blog.playstation.com/2026/02/24/death-stranding-2-pc-specs-detailed-out-march-19/),pcmasterrace,2026-02-24 17:24:11,2
AMD,o766naw,I too am hoping for good optimisation more than anything since this is one of my most awaited games of the year on PC!,pcmasterrace,2026-02-24 17:31:31,1
AMD,o765977,Cool my 2070 should be ok with dlss then.,pcmasterrace,2026-02-24 17:25:09,2
AMD,o765mhx,Yes more than enough!,pcmasterrace,2026-02-24 17:26:50,1
AMD,o7b1bzy,"Still basing port quality on pre-launch sheets?    These specs are extremely similar to spider man 2, look how that game performs   Just wait for the launch and testing",pcmasterrace,2026-02-25 10:59:59,153
AMD,o7b0564,"Its Kojima Productions, they care at least a bit, believe it or not",pcmasterrace,2026-02-25 10:49:35,81
AMD,o7b1z63,"Game I want : Worst PC port of 2024, mouse not supported, movement on arrow keys (hard coded into the engine, can't even be changed by mods), 55 FoV with no slider, heavy vignette and motion blur (also hard coded)  Game I have literally zero interest in: Perfectly optimized, photorealism on GTX 1060 3GB in 240 FPS on max settings, personally signed letter of lead game designer with an autograph",pcmasterrace,2026-02-25 11:05:30,36
AMD,o7bemm6,"Well, this is the bare minimum I'd expect, considering the entire game is set in a barren wasteland with no NPCs. If the minimum specs were any higher I'd definitely consider this a bad port.",pcmasterrace,2026-02-25 12:41:19,7
AMD,o7b1iso,"Nixxes's ports have been quite notorious for requiring a lot more hardware to reach PS5 level performance. I would not be surprised if the recommended settings are in fact what PS5 achieves, however hitting 1080p at 60fps on a 3060 at decent levels of visual quality is a good thing.",pcmasterrace,2026-02-25 11:01:36,11
AMD,o7b91i7,Shit so bad they had to start trying,pcmasterrace,2026-02-25 12:02:20,6
AMD,o7b966f,Nobody can answer that from a chart like this ffs.,pcmasterrace,2026-02-25 12:03:19,7
AMD,o7azeqt,"Really good looking system requirements indeed, though I canâ€™t find any sources explicitly specifying whether these presets are using upscaling or not, only that upscaling options will be available. If these presets require the use of heavy upscaling then this is a lot less impressive but hoping thatâ€™s not the case.",pcmasterrace,2026-02-25 10:43:08,7
AMD,o7b4yjq,"okay, so on medium@1080p, they ask for a 3060, specifically the 12gb one. so can we assume any card under 12gb vram will need to run the game on sub medium?",pcmasterrace,2026-02-25 11:30:40,9
AMD,o7b8ax3,Gonna have to wait and see,pcmasterrace,2026-02-25 11:56:51,3
AMD,o7ccby3,150GB? In this economy,pcmasterrace,2026-02-25 15:41:29,3
AMD,o7badqy,https://preview.redd.it/eupiks0otmlg1.png?width=1080&format=png&auto=webp&s=2a690c072a27f101c641cb9e9a073f799db63641  This is the recommended specs of Monster Hunter Wilds,pcmasterrace,2026-02-25 12:12:11,5
AMD,o7b8mw9,The Decima Engine is quite reliable on PC performance.,pcmasterrace,2026-02-25 11:59:20,3
AMD,o7c3qew,The game hasn't been released yet how can anyone judge on whether it is a good port?,pcmasterrace,2026-02-25 15:00:29,1
AMD,o7c6hel,"They coulda done it years ago, just took shortages to make it reality.",pcmasterrace,2026-02-25 15:13:58,1
AMD,o7c81vf,will connecting to pisstation account be obligatory?,pcmasterrace,2026-02-25 15:21:27,1
AMD,o7ckuik,We can see after the release to see how hard it crashes,pcmasterrace,2026-02-25 16:20:22,1
AMD,o7cpbd2,how are those good ? this is with dynamic resolution scaling and upscaling at who knows what preset.,pcmasterrace,2026-02-25 16:40:37,1
AMD,o7dvctg,"It's still better than Indiana Jones, which requires RTX and sometimes fails to run with 8 GB VRAM.",pcmasterrace,2026-02-25 19:51:53,1
AMD,o7etbly,First one ran smooooth af.  So pretty solid speculation for this to be great as well.,pcmasterrace,2026-02-25 22:31:44,1
AMD,o7ez9vk,">A good PC Port?   Two things:  1) its beyond dumb to trust in a requirement table.  2) just because the previous port was ok, that doesnt mean (sadly) that this one is too.  Hopefully this is a good port, but guessing that it will be ok based solely on this is dumb.",pcmasterrace,2026-02-25 23:02:01,1
AMD,o7fgdba,Specs look reasonable! High requires a 3070....,pcmasterrace,2026-02-26 00:35:36,1
AMD,o7bc6ad,![gif](giphy|JymRvCWcasbdMVXcVA),pcmasterrace,2026-02-25 12:24:49,1
AMD,o7b5cf2,good? is it out?,pcmasterrace,2026-02-25 11:33:50,0
AMD,o7ba6kj,"First game port was a deception, lack of settings, AA was constantly reseting, upscalers make the game crash, stutter, fps drops and they never cared to fix it due to the stans gaslighting/vilifying  everyone... Also graphics quality wise was meh outside of cutscenes.   I hope they dont have their head shoved up their ass this time and make a good port. Still gonna wait for heavy sales because f that price and story gonna be bad enyway.",pcmasterrace,2026-02-25 12:10:42,0
AMD,o7b3ba4,Aah my RTX 3060 will be fine ðŸ˜Š,pcmasterrace,2026-02-25 11:17:02,0
AMD,o7bsnlt,150 GB... not size optimized.,pcmasterrace,2026-02-25 14:03:16,-2
AMD,o7bn9rb,"I mean i would hope so, its a ps5 game so equivalent to about 5-6 year old pc specs which lines up well with the 3060 and 3070 mentioned",pcmasterrace,2026-02-25 13:33:39,-1
AMD,o7bd0uf,"I hope Kojima likes weak sales numbers, releasing on the same day as Crimson Desert.   Easiest choice of my life.",pcmasterrace,2026-02-25 12:30:32,-9
AMD,o7d990g,First death stranding was optimized great no reason this shouldn't be,pcmasterrace,2026-02-25 18:11:13,8
AMD,o7blucu,The insomniac engine was utter doghit tho. The decima engine is world aparts in peformance.,pcmasterrace,2026-02-25 13:25:32,20
AMD,o7b3ga7,This is being handled by Nixxes who did the HZD and HFW ports on Decima which were not as good as the DS1 port done by Kojima. We shall see but Im just cautiously optimistic,pcmasterrace,2026-02-25 11:18:12,22
AMD,o7e5pq5,I love the atmosphere of their games but the literal 30+ minute cutscenes kind of kill it for me. I tried to play the first one 2-3 times. Not my cup of tea unfortunately.,pcmasterrace,2026-02-25 20:40:27,1
AMD,o7b3f6t,What games is the former one?,pcmasterrace,2026-02-25 11:17:56,17
AMD,o7bi0rh,This game wonâ€™t hit 30 fps 1080p low settings on a 1060. According to this chart,pcmasterrace,2026-02-25 13:02:53,-6
AMD,o7g97t9,I mean the previous Death Stranding ran pretty well on pc from what I remember.,pcmasterrace,2026-02-26 03:19:45,1
AMD,o7b14g4,"Nixxesâ€™ specs in the past assumed that you used dynamic resolution scaling to hit the fps targets. So it wonâ€™t be native all the time, but I wouldnâ€™t expect it to regularly drop to performance upscaling either.",pcmasterrace,2026-02-25 10:58:07,3
AMD,o7b7spx,Kind of doubt it. The RX6600 is recommended for the same level of performance and thatâ€™s an 8GB card. The 8GB version of the 3060 is pretty notably slower than the 12GB version VRAM aside so that seems like the more likely reason it was chosen.,pcmasterrace,2026-02-25 11:53:05,4
AMD,o7b5o5f,"Notice they have a 9070XT comparable to a 4080, where realistically the 9070XT is on par with the 4070 TI (they trade blows often).  I suspect that the amount of RAM your card has is very important in this game.",pcmasterrace,2026-02-25 11:36:29,1
AMD,o7ei688,gta 6 better come with a complimentary SSD with the trend of game sizes lol,pcmasterrace,2026-02-25 21:38:15,2
AMD,o7bltjg,"1080p60 medium settings, WITH FGEN ENABLED.... rofl",pcmasterrace,2026-02-25 13:25:25,4
AMD,o7bmtrj,WHO?,pcmasterrace,2026-02-25 13:31:08,-3
AMD,o7ddxog,"Nixxes have a mixed history of ports, as does the decima engine   It's likely it'll be good, many reasons why that might not happen",pcmasterrace,2026-02-25 18:32:11,0
AMD,o7bnit4,"Spiderman remastered and miles morales are fine, Rift apart and 2 aren't. So on PC insomniacs engine is 2/4    Zero dawn had many issues at launch, still has quirks.Â Forbidden west is fine, overall good but has issues. Death stranding 1 is a good port   So decima is 1-2/3 so far",pcmasterrace,2026-02-25 13:35:05,13
AMD,o7b4yes,"Nixxes' recent efforts have been disappointing unfortunately, Part II and Spider-Man 2 are not well optimized and suffer from stutter, and like you said their Decima titles weren't spectacular either, let's hope this is better.",pcmasterrace,2026-02-25 11:30:38,13
AMD,o7b5t9b,And the latter?,pcmasterrace,2026-02-25 11:37:38,13
AMD,o7bpv0e,"Almost like it was a game designed to run at 30/60fps on a PS5, which has a 2080. A 1060 is an incredibly old card to expect 60fps in a 2026 game, itâ€™s literally a 10 year old GPU.",pcmasterrace,2026-02-25 13:48:03,3
AMD,o7din5r,wow  a decade old midrange gpu cant run brand new games?  i for one am shocked,pcmasterrace,2026-02-25 18:53:07,1
AMD,o7be0rm,you're right. i totally forgot that.,pcmasterrace,2026-02-25 12:37:16,1
AMD,o7b5x1q,"it seems so, yes.",pcmasterrace,2026-02-25 11:38:29,2
AMD,o7eidlh,Iâ€™ll just install the game on hdd out of spite,pcmasterrace,2026-02-25 21:39:12,2
AMD,o7by5py,">Zero dawn had many issues at launch  To be fair, ZD port was done by Virtuos, not Nixxes. The latter came in later and took over.",pcmasterrace,2026-02-25 14:32:22,6
AMD,o7bz5rw,"Though I'm inclined to give Zero Dawn a pass, since it was developed as an exclusive. Don't know how much in advanced they knew that it would come out on PC, but I don't think they planned on 4 years. Forbidden West was developed with an eventual PC port in mind. So maybe that plays a role in that, too.",pcmasterrace,2026-02-25 14:37:31,1
AMD,o7d8t3a,"I just recently tested rift apart, it still has an awful memory leak, if you keep changing settings, vram just continues rising",pcmasterrace,2026-02-25 18:09:13,1
AMD,o7d986h,"Spider man is so weird, insanely cpu heavy rt",pcmasterrace,2026-02-25 18:11:07,1
AMD,o7bbbqr,"Forbidden west runs pretty good on pc I dunno where that comes from. DS2 ran like dickhole on the ps5 as well considering it's an engine designed for super fast movement and massive robot battles, and yet DS2 puts the ps5 on its knees fighting 4 little mech dudes as a geriatric. all while still looking worse than the ps4 version of HFW.  I'm ride or die that Decima is a gift Kojima did not deserve, and has grossly under-utilized. Still this port is going to be a banger no matter what since I'd bet within the first day the trench coat will be modded in.",pcmasterrace,2026-02-25 12:18:55,-6
AMD,o7c2gqy,Don't tell this sub that choosing to upgrade once every decade will have consequences to their gaming experience.,pcmasterrace,2026-02-25 14:54:13,8
AMD,o7eiycv,"50,000 floppy disks and a dedicated and fast helper",pcmasterrace,2026-02-25 21:41:51,1
AMD,o7cerby,"Nixxes also did Spiderman 2 and that's a shit show to this day   It's not a given they do good ports these days, regardless of engine",pcmasterrace,2026-02-25 15:52:40,1
AMD,o7d90n9,Same with tlou1 right,pcmasterrace,2026-02-25 18:10:10,1
AMD,o7cei02,As was Spiderman remastered,pcmasterrace,2026-02-25 15:51:30,1
AMD,o7dbgc2,"Part II was even weirder since it was a ps4 game, it made no sense how cpu limited that game was on PC in certain areas.",pcmasterrace,2026-02-25 18:21:02,1
AMD,o7bpewn,"Did we play the same game, lol? I played 90hrs of death stranding 2 on ps5 and never once noticed it dropping below 60fps",pcmasterrace,2026-02-25 13:45:37,2
AMD,o7crwz5,What do you mean my goddamn kepler GTX 770 can't run the new spiderman 2?  It ran the first spiderman 2 on the ps2 emulator just fine brother,pcmasterrace,2026-02-25 16:52:24,5
AMD,o7ci7fn,SM2 and TLoU2 were ported back to back in a short amount of period in the beginning of 2025. I guess Nixxes were spread thin by Sony.  This is Nixxes' first port since TLoU2 (almost a year later) so hopefully they deliver a good port.,pcmasterrace,2026-02-25 16:08:22,2
AMD,o7d5z3d,"Sm2 is a shit show? It was rough at launch but it is fine now,  no?",pcmasterrace,2026-02-25 17:56:37,1
AMD,o7brb3a,"Lol same, never noticed performance issues that pulled me out of the game, it runs great for how amazing it looks.",pcmasterrace,2026-02-25 13:55:57,1
AMD,o7c7vwv,"You don't notice it because you're playing the slowest moving protagonist of the last decade and the game is saturated in a sickening amount of blur effects, which were again, designed for much faster paced games.  The fps and resolution tank pretty hard in the handful of fights where you fight a bunch of the little orange guys, and all the sequences with copy paste mads mikkelson guy. Every BT fight you're either standing in the desert or staring at the skybox the entire time.  It's not like performance even matters on such a chill game, and it's not even the tip of the iceberg of what the poor aspects of the game are, but people claiming any of the hundreds of people Kojima takes credit for are using Decima effectively, let alone half as well as Guerilla does is a mental take.",pcmasterrace,2026-02-25 15:20:40,-2
AMD,o7cv8bs,Curse those pesky devs! Leaving potential customers behind SMH ðŸ˜¡,pcmasterrace,2026-02-25 17:07:38,5
AMD,o7ek8d3,"I used to think quite highly of them but yeah, their PC peers have left a lot to be desired.  I believe thereâ€™s still crashing issues on ratchet and clank that havenâ€™t been resolved",pcmasterrace,2026-02-25 21:47:44,1
AMD,o7d7o0c,"Nope, still pretty shit   If you have a relatively low framerate cap with a fast CPU (E.g. 60FPS with a 5800x3D) it's mostly smooth, but good luck capping above that",pcmasterrace,2026-02-25 18:04:08,1
AMD,o7d5v9g,Bro this pc looks good!!!,pcmasterrace,2026-02-25 17:56:09,2
AMD,o7dnqpl,"Most of the time Magenta / Pink / Purple (choose what you see) with ""active"" effects like wave effects. I used to have this color on my previous PC, which was full black too with tons of RGB parts. But actually as I have this new full white PC, I'm il full RGB setup and I like it. It's so bright and so good to look at.  https://preview.redd.it/2g89hmehxolg1.jpeg?width=3000&format=pjpg&auto=webp&s=14ec2049eb7e81535fc11918c62252893d44fcf4",pcmasterrace,2026-02-25 19:16:28,2
AMD,o7drt8k,Right on,pcmasterrace,2026-02-25 19:35:35,1
AMD,o7druc2,Dope color,pcmasterrace,2026-02-25 19:35:44,1
AMD,o7azk9h,Your specs should last 10 years for competitive games even in 1440p.,pcmasterrace,2026-02-25 10:44:29,5
AMD,o7aznrm,This is a great system and will do you well for at least the next 5 years.,pcmasterrace,2026-02-25 10:45:21,2
AMD,o7b014w,Yeah they're fine,pcmasterrace,2026-02-25 10:48:35,1
AMD,o7b6k8e,"thanks for the replies everyone, decided on buying it.",pcmasterrace,2026-02-25 11:43:37,1
AMD,o7bhmon,Let us know how it runs!,pcmasterrace,2026-02-25 13:00:27,1
AMD,o7azmfh,This is a good mid-range to high mid-range spec and you won't have any problems in the coming years.,pcmasterrace,2026-02-25 10:45:01,0
AMD,o7b0oap,look pretty fine to me. make sure your RAM is 3000/CL30.,pcmasterrace,2026-02-25 10:54:11,0
AMD,o7azojr,that's good to hear. appreciate you for responding.,pcmasterrace,2026-02-25 10:45:32,2
AMD,o7b6l74,Sounds great! thanks,pcmasterrace,2026-02-25 11:43:49,1
AMD,o76zssj,"Lian Li 216, Lian Li 217 are the best air flow cases on the market even with dust filters equipped.   Antec Flux Pro hands down is amazing but no dust filters.   Any well know brand Gold/Platinum, Corsair.   And fans? Noctua is good but expensive and worth it if you value quiet operation.   Arctic for value.",pcmasterrace,2026-02-24 19:42:14,1
AMD,o7702ho,"Corsair RM850 / 1000, Thermalright Phantom Spirit 120, Fractal Torrent",pcmasterrace,2026-02-24 19:43:27,1
AMD,o7ftd6h,"need more info op, whats the cooler and whats ur bios settings?",pcmasterrace,2026-02-26 01:49:11,1
AMD,o7ftmzg,"cpu cooler is id cooling a620 , about bios , i dont change anything",pcmasterrace,2026-02-26 01:50:45,1
AMD,o7fuprb,"An a620 for a r7 7700 is more than enough.Â  Im sure it will keep it cool under higher loads or benchmarking, so no issue!",pcmasterrace,2026-02-26 01:56:53,2
AMD,o72abb2,Did you install the WiFi driver?,pcmasterrace,2026-02-24 02:02:40,1
AMD,o72cu00,What distro are you on ?,pcmasterrace,2026-02-24 02:17:08,1
AMD,o73r6uy,"All you know is ""neither of them worked"" and then you watched some YouTube.   Find out more than that. For example:  Is it in Device Manager?  Is it in the right M.2 slot?  Is it a USB adapter?  Is it even a WiFi adapter or have you taken a mouse dongle and write ""wifi"" on it with sharpie?  Is it on fire?  Has the WiFi adapter turned into an orange cat and ran away?  You don't know any of this, we don't know any of this.",pcmasterrace,2026-02-24 08:31:34,1
AMD,o79ts3d,"If you bring up Device Manager, do you have any ! or ? entries listed there? It can usually be resolved by installing the AMD Chipset Drivers for X870, if you haven't already",pcmasterrace,2026-02-25 04:44:02,1
AMD,o72ifj5,Yes I installed all the necessary drivers,pcmasterrace,2026-02-24 02:49:31,1
AMD,o72igbu,What is that?,pcmasterrace,2026-02-24 02:49:39,1
AMD,o76j8x2,"It's hidden in the device Manager  it's in the available PCIe 2 slot  it is not,  https://preview.redd.it/mpgtrrkvjhlg1.jpeg?width=2448&format=pjpg&auto=webp&s=8ce8f69f5a90855221d716517a90288f996a3526",pcmasterrace,2026-02-24 18:27:42,1
AMD,o7djx3t,No it doesn't have any ! Or ? Entries listed. And I've already installed the AMD chipset drivers,pcmasterrace,2026-02-25 18:58:50,1
AMD,o717zb6,"Man, I miss the days of those silly aliens, monsters and mascot girls plastered all over the coolers. Simpler times.  The Radeon HD 2000 series was terrible though.",pcmasterrace,2026-02-23 22:28:07,14
AMD,o71afnh,Actually one of the faster AGP cards you could get. Nice find.,pcmasterrace,2026-02-23 22:40:48,4
AMD,o734rwp,https://preview.redd.it/vrl9byysmdlg1.jpeg?width=6000&format=pjpg&auto=webp&s=d5a41b215df654fce1f04cd4358f5a0a3d8d52a7,pcmasterrace,2026-02-24 05:17:01,3
AMD,o72ro0p,Literally the only thing I didn't like about AMD buying out ATI was that they got rid of all the neato cooler art.,pcmasterrace,2026-02-24 03:45:44,1
AMD,o71tghe,"Okay, hear me out...",pcmasterrace,2026-02-24 00:26:00,1
AMD,o71c7wi,That's unfortunate but at least it looks nicer than a lot of the newer stuff,pcmasterrace,2026-02-23 22:50:11,5
AMD,o71c5jz,I thought so based on the specs listed. I didn't see if it's a 256 or 512mb version but thats what I expected from it.,pcmasterrace,2026-02-23 22:49:51,1
AMD,o71ds2c,"Yeah, shame the H.264 decode didn't work on AGP cards. I tried to use one in an old system as a media centre build.",pcmasterrace,2026-02-23 22:58:32,1
AMD,o733mdq,And has the S-Video out so you know itâ€™s their fancy version,pcmasterrace,2026-02-24 05:08:28,1
AMD,o73mf4r,I really don't think that had anything to do with ATi and AMD.  The board partners got rid of those designs around the same time on ATi/AMD as they did on Nvidia cards. It was just a trend coming to an end.,pcmasterrace,2026-02-24 07:47:02,2
AMD,o75879c,AMD have already switched to primarily AI last year.,pcmasterrace,2026-02-24 14:53:54,1634
AMD,o75b9j7,![gif](giphy|l2JeeOTlKw7UQC58I)  my 5070ti will last me for eternity,pcmasterrace,2026-02-24 15:08:48,700
AMD,o75b4da,my 9070 xt better live forever,pcmasterrace,2026-02-24 15:08:07,520
AMD,o757vx3,![gif](giphy|WmKrOMrTFFhPW),pcmasterrace,2026-02-24 14:52:22,518
AMD,o759egp,"Wonder what this will mean for pricing, AMD has already signed a deal to provide GPUs for PlayStation 6, it would be very hard to justify massive price hikes when fundamentally the same technology is in a console being sold for less than $1k.",pcmasterrace,2026-02-24 14:59:44,253
AMD,o75che6,![gif](giphy|GjB41rKHBnOkE),pcmasterrace,2026-02-24 15:14:36,47
AMD,o75canq,AMD and NVIDIA are part of the AI circlejerk right now. Allocating hypothetical future stock based on a hypothetical market demand that is subject to change is just plain stupid.,pcmasterrace,2026-02-24 15:13:43,219
AMD,o75klhf,Feels silly to have ever been on team red or team green. These companies will slit your throat if it boosts shareholder value. That's all you need to know about them.,pcmasterrace,2026-02-24 15:52:13,30
AMD,o758n17,Maybe this will send a message to games execs that optimization for a requirement. Not optional.,pcmasterrace,2026-02-24 14:56:03,151
AMD,o75snag,Before this they offered 10% of their stock to scam altman as long as OpenAI promises to keep buying their GPUs.  They lost the plot the moment they did that. This is just the cherry on top.,pcmasterrace,2026-02-24 16:28:31,55
AMD,o7593m0,Every time I see these news I do wonder wouldnâ€™t flops be a better measurement of GPU compute capacity than watts?,pcmasterrace,2026-02-24 14:58:16,66
AMD,o75zurw,Intel could do the funniest thing right now and take the market by storm.,pcmasterrace,2026-02-24 17:00:34,43
AMD,o75acgu,"Here's to hoping intel will really focus on the consumer market to fill the gap. (lol, lmao even)",pcmasterrace,2026-02-24 15:04:20,53
AMD,o757ubp,good thing my 7900xtx will serve me for years,pcmasterrace,2026-02-24 14:52:08,103
AMD,o76rrrf,6 gigawatts is like 3 Vega GPUs so shouldn't affect much.,pcmasterrace,2026-02-24 19:05:22,12
AMD,o75cv36,"I was going to make a meanspirited joke that that's be giving meta shares. Then I read the news....   ""Meta will buy 6 gigawatts of computing power from AMD in a multi-year agreement announced by the two companies early Tuesday. The chip maker has also agreed to give Meta a performance-based warrant for up to 160 million AMD shares""",pcmasterrace,2026-02-24 15:16:23,11
AMD,o76xim1,![gif](giphy|ve5NDutaZXDepgyeCN),pcmasterrace,2026-02-24 19:31:39,12
AMD,o75eyjt,"Honestly I thought I was making a frivolous purchase when I bought a 4090 for my last build, but it might turn out to be a great purchase after all.",pcmasterrace,2026-02-24 15:26:13,20
AMD,o77v71e,This timeline is a fucking joke.,pcmasterrace,2026-02-24 22:07:38,7
AMD,o75ug5g,"Both Nvidia and AMD are going to make some GPU's available for gaming but neither company, both publicly traded with a duty to shareholders, is going to avoid all this guaranteed revenue from AI and datacenters.   If they did their shareholders would revolt and vote the C-suite out.",pcmasterrace,2026-02-24 16:36:33,21
AMD,o75yhqk,"Consumer market is over. Time to revert overclocks and buy UPS's for our pcs, because they're the last hardware we get at home.",pcmasterrace,2026-02-24 16:54:30,22
AMD,o75iwz4,7900 XT donâ€˜t even think about dying on me now,pcmasterrace,2026-02-24 15:44:34,5
AMD,o76dyq3,AMD are the good guys so this is impossible,pcmasterrace,2026-02-24 18:04:24,7
AMD,o75bxt1,I should buy a 9070 soon but idk i dont feel the need rn to let go of my 1080Ti.,pcmasterrace,2026-02-24 15:12:03,23
AMD,o7590kc,GDP Just multiplied by the deal announcement /s,pcmasterrace,2026-02-24 14:57:52,13
AMD,o76t1r4,"I will simply continue to play old games until my hardware dies, then I will just pick up new hobbies. Itâ€™s been a good run.",pcmasterrace,2026-02-24 19:11:11,5
AMD,o75ea40,Omg no waaaayâ€¦Iâ€™m gonna be honest. I donâ€™t get why folks uphold loyalty to what company your GPU is made from after buying the GPU. Both are garbage companies with investors in mind first and do not care about you or the GPU you already paid for. Itâ€™s why we keep old ass GPUs alive as long as we can because none of us here want to support these kinda BS. Sadly doesnâ€™t mean the majority will share that sentiment,pcmasterrace,2026-02-24 15:23:00,20
AMD,o7600k6,there's 0 business reasons not to.     They make shovels during a gold rush.  i don't know why people thought AMD was immune to money.     just hoping the bubble pops sooner than later,pcmasterrace,2026-02-24 17:01:18,6
AMD,o76tcug,THE SHAREHOLDERS DEMAND IT!  lol,pcmasterrace,2026-02-24 19:12:36,5
AMD,o773ydl,"Wait, are AMD GPUs for consumers even on the same nodes as their GPUs for datacenters? I'm pretty sure it makes sense to still make consumer GPUs if only because there's so much cutting edge node supply you can get. Plus they already did chiplet sharing in Epyc + Consumer that worked out well for us (when CPUs occupied the datacenter position that GPUs do now), so is there any reason to think that their upcoming MCM solutions and UDNA won't still give us options?  Idk, taking into account GPU chiplets or differentiated nodes where we don't get shared chiplets, I'm pretty sure it's totally possible to maintain consumer GPU supply if they want to.  The bigger concern IMO is memory chips. VRAM has been a huge limiter on consumer GPU development (which came to a head with \~2022-2024 8GB woes), but I'm pretty sure datacenter demand for memory will drive memory manufacturers to get more efficient and higher volume, which could have economies of scale benefits for us towards \~2029-2032?",pcmasterrace,2026-02-24 20:01:20,5
AMD,o77n10e,"Source? Also, how is a ""gigawatt"" and unit of ""chips""?",pcmasterrace,2026-02-24 21:29:40,5
AMD,o783btw,Bro we already said goodbye to whole PC gaming. If my rig dies then that's it. Can't afford a new PC lol,pcmasterrace,2026-02-24 22:47:58,4
AMD,o75wsg3,Ai isnâ€™t even that great. All I get from my managers is an email response generated by Ai which then I have to ask Ai how to respond to that email.   Itâ€™s Pointless,pcmasterrace,2026-02-24 16:46:57,15
AMD,o75o53n,"Good thing they went AMD, 5 gigawatts barely powers 10 intel cpus",pcmasterrace,2026-02-24 16:08:16,9
AMD,o76dj49,guess we can say goodbye to pc building,pcmasterrace,2026-02-24 18:02:28,4
AMD,o76pa50,Remember: No Friends.,pcmasterrace,2026-02-24 18:54:17,5
AMD,o775kqh,So back to my neglected woodworking hobby it is.,pcmasterrace,2026-02-24 20:08:53,4
AMD,o78qkqm,"""No, consumers arent being ignored, they can rent pc's!""  Cant wait for that line",pcmasterrace,2026-02-25 00:53:13,4
AMD,o75fd6v,"This is possibly even worse than Nvidia as Nvidia is primarily desktop, some signage/professional, and Nintendo (fuck Nintendo), but AMD is in everything from Steamdeck to PS5 and Xbox. Get ready to say goodbye to any new gaming device for the next 4 years. I'm fine but I hate it for everyone else because this is complete horseshit for a handful of people's personal gain when they decide to trash the stonks when the house of cards come tumbling down and it's possibly already starting to show the cracks. The only positive here is this sounds to be an 'agreement' but that doesn't directly mean it's a required sale.",pcmasterrace,2026-02-24 15:28:07,11
AMD,o75lr0t,"I won't cry after them as a product. They somehow works to everybody, except myself. I never had less stable GPU.",pcmasterrace,2026-02-24 15:57:26,3
AMD,o769yz5,Doesnâ€™t change anything. The bottleneck was TSMC and DRAM/HBM already. AMD having a deal with someone has no influence on that at all.,pcmasterrace,2026-02-24 17:46:42,3
AMD,o76aobc,"I enjoy the sub for memes but what is with these silly doomer posts? AMD makes products for console, laptop, datacenter which has recently expanded to include AI. As long as they properly forecast (lol nvidia) and it shouldn't be an issue. Generally their instinct GPU use a more advanced node than their consumer GPU...so if anything instinct will be contesting their main cash cow epyc for manufacturing space. I don't think they'd risk one for the other.",pcmasterrace,2026-02-24 17:49:50,3
AMD,o76euc8,I'm going to put my 9070 XT warranty in a secure vault.,pcmasterrace,2026-02-24 18:08:18,3
AMD,o76nomk,"Remember when AMD was, â€œFor the Gamersâ€? I memberâ€¦",pcmasterrace,2026-02-24 18:47:15,3
AMD,o77q5nn,"I mean, they have always been the budget GPU and only became a CPU leader became Intel tripped, fell, then started punching it self. Then got back up and set it self on fire , refused help and now is blaming everyone. For not helping, AMD needs to do this to be able to grow.",pcmasterrace,2026-02-24 21:44:02,3
AMD,o77ssy6,I will hold on to my 32GB of RAM and 6950XT until I die at this rate.,pcmasterrace,2026-02-24 21:56:19,3
AMD,o77wwi4,Oh goody. Here's hoping my brand new Nitro+ 9070XT lasts the next 30 years. Thank god my GTX 1080 still works like a champ.,pcmasterrace,2026-02-24 22:15:53,3
AMD,o7863d5,"Should either of those fu\*ks ever crawl back, ignore them. Now lets see what the Chinese or intel makes.",pcmasterrace,2026-02-24 23:02:12,3
AMD,o78h8dc,"So hereâ€™s a question regarding the gaming market, if everything is going to be 3 or even 4 times more expensive from now on,how are the gaming companies (which is a huge industry) planning on making money out of this. Gamers will not be able to buy the new consoles which are going to be way too expensive,which will result to less and less games being made,are they really planning on destroying every single industry just to feed their hypothetical data centers that they are â€œplanning on buildingâ€ just because they want to hoard everything? This is going to burst in a way worse way than the dotcom bubble.",pcmasterrace,2026-02-25 00:02:59,3
AMD,o7acqmq,Stay strong my 3080 8gb friends ðŸ™,pcmasterrace,2026-02-25 07:13:19,3
AMD,o7ajjg4,"Mark my words: Something new is gonna hit the market hard and replace GPUs as dedicated AI processors, and gamers will reclaim the GPU market making them dirt cheap.   Itâ€™s going to be glorious",pcmasterrace,2026-02-25 08:15:15,3
AMD,o75bb18,Ethical capitalism simply does not exist.,pcmasterrace,2026-02-24 15:09:01,15
AMD,o75e2px,"But I thought AMD would never? This sub told me AMD = good and ""Ngreedia"" = bad  Maybe one day people will understand that no corporation is their friend.",pcmasterrace,2026-02-24 15:22:03,9
AMD,o75o6a7,"""GPU's"" ?   GPUs",pcmasterrace,2026-02-24 16:08:25,2
AMD,o761p5l,They'll still make consumer GPUs but they'll never be high end and probably never on bleeding edge nodes unless they figure away to make modular chips actually work so they can have a universal compute chiplet and graphics engine chiplet when they want to humor the lowly gamers or the bubble bursts,pcmasterrace,2026-02-24 17:08:56,2
AMD,o762368,A gigawatt is a unit of electrical power.,pcmasterrace,2026-02-24 17:10:42,2
AMD,o76f16x,No shit,pcmasterrace,2026-02-24 18:09:08,2
AMD,o76m4k3,6 gigawatts... as in TDP? Is that including PBO? Without a link we may never know. Maybe you should make sure you understand the information given before passing it on.,pcmasterrace,2026-02-24 18:40:29,2
AMD,o76soi5,Will intel arc gpu save us?,pcmasterrace,2026-02-24 19:09:30,2
AMD,o77dd9o,"I'm out of the loop. What is Intel up to these days? This would be a prime opportunity to gain market share before the bubble bursts, but I somehow doubt that that's the plan.",pcmasterrace,2026-02-24 20:45:32,2
AMD,o77gy83,I just can't even put into words how thankful I am that I got my own 5080 at retail,pcmasterrace,2026-02-24 21:01:57,2
AMD,o79yk95,I know Nvidia and AMD are the only real players in the GPU field but I really wish it backfired on them.,pcmasterrace,2026-02-25 05:18:03,2
AMD,o7agbd2,"Time to invest in retro gaming, manga/comic and books.",pcmasterrace,2026-02-25 07:45:43,2
AMD,o75gmyd,![gif](giphy|6fMZvAOjzM9t6),pcmasterrace,2026-02-24 15:34:03,5
AMD,o75x76a,Please nobody use Ai for anything. Sick of this shit,pcmasterrace,2026-02-24 16:48:46,3
AMD,o75bof6,"Already have 9070 XT, so unless they make something better than that I am good for a long time.",pcmasterrace,2026-02-24 15:10:49,3
AMD,o75co03,"That's not very much though, considering it's multiyear.",pcmasterrace,2026-02-24 15:15:28,2
AMD,o75f8cf,The kernel Will continue to provide,pcmasterrace,2026-02-24 15:27:30,1
AMD,o75hu2k,Bloody hell.,pcmasterrace,2026-02-24 15:39:38,1
AMD,o75i1gh,I really need to get on repasting my 7900GRE if it needs to last another 8 years.,pcmasterrace,2026-02-24 15:40:34,1
AMD,o75j73z,Link the article,pcmasterrace,2026-02-24 15:45:52,1
AMD,o75lvaj,Wonder how long my 3070ti will last.,pcmasterrace,2026-02-24 15:57:59,1
AMD,o75nwza,"Really, who even thought they wouldn't jump into the AI wagon? It's too much money involved. Sure, we as consumers are fucked, it's a bleak future, but we can't really do anything so far",pcmasterrace,2026-02-24 16:07:15,1
AMD,o75q1is,I thought that was obvious after that CES from earlier this year.,pcmasterrace,2026-02-24 16:16:52,1
AMD,o75tedi,Fuck  ![gif](giphy|1AxP4HW17LhsKH22uB|downsized),pcmasterrace,2026-02-24 16:31:49,1
AMD,o75u1sg,"I'm in a 3080 FE 10GB, I had bought it second hand at the time cos I shelled out double what it's worth during covid to be able to have a rig at all. I've accepted the time for me to buy bi yearly gpus is well and truly gone, at this point with the prices so high, I haven't even kept up with the market to know what my next upgrade would even be and that's just cos the answer to all of the options now is I can't afford it. I'm grateful for the 3080 I have and feel like it's gonna have to do me for another few years.",pcmasterrace,2026-02-24 16:34:44,1
AMD,o75zoiz,"Competition is what keeps the market healthy, with Intel being a more or less non-competitor, it does make me concerned that Nvidia cards will have a monopolistic hold on the market if AMD leaves. Without competition innovation stagnates and prices go up.",pcmasterrace,2026-02-24 16:59:47,1
AMD,o76193c,Donâ€™t hate the player hate the game. They tried to hold off as long as possible itâ€™s Microsoft,pcmasterrace,2026-02-24 17:06:55,1
AMD,o7631au,no,pcmasterrace,2026-02-24 17:15:02,1
AMD,o764krx,Fuck! I was saving for a new one. Rip gaming for me.,pcmasterrace,2026-02-24 17:22:03,1
AMD,o7651ov,Gigawatts of chips?,pcmasterrace,2026-02-24 17:24:12,1
AMD,o7667ut,Thank God I just picked up a ryzen 7 7800x3d thanks to microcenter bundle,pcmasterrace,2026-02-24 17:29:33,1
AMD,o768k3g,I hate everything,pcmasterrace,2026-02-24 17:40:19,1
AMD,o76d4an,This was already in the works,pcmasterrace,2026-02-24 18:00:40,1
AMD,o76ejyu,Man am i glad i upgraded my pc when i did.,pcmasterrace,2026-02-24 18:07:01,1
AMD,o76fl9s,"So glad I got my setup when I did, what cost me 4500 dollars is now over 7k",pcmasterrace,2026-02-24 18:11:34,1
AMD,o76rfp2,"Saddle up lads, let's go gpu hunting while we can. Crack that pot of gold now.",pcmasterrace,2026-02-24 19:03:52,1
AMD,o76rvo2,I pulled the trigger and got 9070XT yesterday. I guess this is last breath of home computing. Decentralize to centralize everything again,pcmasterrace,2026-02-24 19:05:51,1
AMD,o76wr41,So even less of a reason of sticking with my XTX and just switching to Nvidia whenever they finally release a new generation,pcmasterrace,2026-02-24 19:28:07,1
AMD,o7756be,Whelp hereâ€™s to hoping my 7700XT lasts eight years likemy old system,pcmasterrace,2026-02-24 20:07:00,1
AMD,o779nqo,Glad I already got my Ryzen 5 9600X I dont plan on upgrading anytime soon,pcmasterrace,2026-02-24 20:28:01,1
AMD,o77b2bl,has anyone looked at the stock market today? We will be absolutely flooded by cheap gaming GPUs rebuilt from data center AI chips within a few years.,pcmasterrace,2026-02-24 20:34:39,1
AMD,o77bbpy,That's the reason why the software updates for their gaming GPU's have been shit,pcmasterrace,2026-02-24 20:35:53,1
AMD,o77btue,Why goodbye? I think we're already know that every company is chasing where the money goes. Practically a business 101,pcmasterrace,2026-02-24 20:38:18,1
AMD,o77c0o4,Let's all go outside and touch some grass and / or snow. Might as well get used to not gaming,pcmasterrace,2026-02-24 20:39:11,1
AMD,o77dm1e,"....6 ""gigawatts"" of chips...???",pcmasterrace,2026-02-24 20:46:39,1
AMD,o77g6a3,oh no...,pcmasterrace,2026-02-24 20:58:24,1
AMD,o78kd60,"Just because they signed an agreement doesn't mean it's without clauses allowing one or both companies to walk away without incurring penalties. From phases, performance per quarter, convenience clauses, etc. it allows either company to get away.  There was some other company recently that signed a similar agreement.",pcmasterrace,2026-02-25 00:20:12,1
AMD,o78kyfp,"Played for 10 years with my 980ti, hopefully my 7900 XT can perform the same time span",pcmasterrace,2026-02-25 00:23:21,1
AMD,o78loth,"Have you seen their new driver bundles? AI up the ass selected by default if you don't go into custom installation -_- even some garbage that takes screenshots and sends it to AMD ""with your permission"", cool but why is it installed by default tho?",pcmasterrace,2026-02-25 00:27:15,1
AMD,o78lyou,Just another Tuesday.,pcmasterrace,2026-02-25 00:28:41,1
AMD,o78rwdz,Glad I splurged when I did back during Black Friday.,pcmasterrace,2026-02-25 01:00:26,1
AMD,o79hg64,Honestly why not milk the ai bubble for R & D funds seriously doesnâ€™t mean they will stop gaming GPUs. I Think deep down nvidia knows itâ€™s a bubble just like crypto mining was and are just milking the profits from those poor saps,pcmasterrace,2026-02-25 03:25:27,1
AMD,o7a01gr,"This would have been THE opportunity for amd to monopolize the gaming market. once the ai bubble bursts, they could have been the biggest gpu company aswell as cpu if they focused on consumers rather then companies.",pcmasterrace,2026-02-25 05:29:02,1
AMD,o7a1pdc,I'm fully anticipating Rdna 3 and probably 4 to get put in maintenance mode prematurely because of their ai focus. The open source community are the only group who really give a shit about Radeon because God knows amd themselves don't. The linux RADV drivers lap the adrenaline ones in everything except raytracing and even that won't be the case forever so I wouldn't be shocked if they just give up and hand the keys to the open source community and valve.,pcmasterrace,2026-02-25 05:41:50,1
AMD,o7a23jq,"6 gigawatts is pretty tiny tbh, maybe one data hall in a modern data center. To put into perspective most new data centers are 50+ gigawatts each and thereâ€™s dozens of those being built every year.",pcmasterrace,2026-02-25 05:44:52,1
AMD,o7a4a8j,Its not a new turn of events.,pcmasterrace,2026-02-25 06:01:54,1
AMD,o7aj1d7,I like how all these companies are all in for AIs.  Make a quick cash grab and fuck the long term.,pcmasterrace,2026-02-25 08:10:36,1
AMD,o7bo60i,"If performance is measured in Watts, efficiency can go out the window.",pcmasterrace,2026-02-25 13:38:41,1
AMD,o7dbvko,6 Gigawatts of Graphics cards chips?!  The like 6 Milliion Tonnes of Dreams  Or 200 Thousand miles per Air.,pcmasterrace,2026-02-25 18:22:55,1
AMD,o7edx2y,Well it's been a good run,pcmasterrace,2026-02-25 21:18:32,1
AMD,o7eedue,"I guess the only real benefit of AMD hardware would be that the drivers are open source. Somebody could feasibly continue to make 3rd party drivers for CPUâ€™s and GPUâ€™s, but that would be a massive undertaking and would require constant maintenance.  Intel ARC is potentially starting to seem like the best option. Time will tell though.",pcmasterrace,2026-02-25 21:20:41,1
AMD,o7ejxxy,they already signed a deal with openai a little while ago,pcmasterrace,2026-02-25 21:46:23,1
AMD,o7f8014,what awful times we live in. i just want to play video games :(,pcmasterrace,2026-02-25 23:49:42,1
AMD,o7f83v8,Theyâ€™re gonna lose so much market shareâ€¦  ![gif](giphy|XHeLeuirRbwptHhSWd),pcmasterrace,2026-02-25 23:50:19,1
AMD,o7fdyn5,"Difference is that Nvidia isnâ€™t owned by anyone. If AI goes tits up in any way, AMDâ€™s gonna be caught in the fallout of Meta trying to recoup their investment. Nvidia regardless of the industry will be fine as theyâ€™re not coupled to a service, they just provide the hardware for it.",pcmasterrace,2026-02-26 00:22:36,1
AMD,o7g87t2,The future of gaming and consumer hardware is ARM SoCs. x86 CPUs and discrete GPUs won't be an option in 10 years.,pcmasterrace,2026-02-26 03:13:55,1
AMD,o75bz6h,I already said goodbye to Radeon a while ago after 20 years of experiencing the shittiest drivers ever made.,pcmasterrace,2026-02-24 15:12:13,0
AMD,o76z31i,"Why anyone would choose AMD over Nvidia at this point is just beyond dumb imo. I mean they literally tried to kill support for the 6000 series last year, only the massive pushback they got stopped them. But you can see how much they want to just abandon their cards, wonâ€™t surprise me if the 7000 series is cut off by 2027.",pcmasterrace,2026-02-24 19:38:57,0
AMD,o75tw6j,It is so fucking strange that a computer I built a year ago is now worth way more today than it was then. Strange times folks.,pcmasterrace,2026-02-24 16:34:02,1
AMD,o75ydxg,"Everybody look on the bright side, when the AI bubble bursts, the market will be flooded with high end GPUs.",pcmasterrace,2026-02-24 16:54:02,1
AMD,o76fgse,And then Intel corners the consumer GPU market and becomes bigger than Nvidia after the AI bubble breaks.,pcmasterrace,2026-02-24 18:11:01,1
AMD,o76jq6c,I have owned quite a set see dddfddfew AMD based GPUs and they have always worked great.  I just upgraded to the 9070XT the REAPER BY POWER COLOR itâ€™s been great and works well at 4K.  My other card was an XFX RX6800 non-XT.  It worked great and I only upgraded because I wanted to get a new GPU.  No issues just plain wanted to spend money.  I did get the impression that graphics cards were going to become scarce.  I was kind of worried that if my graphics card broke I would not be able to get a new one.  So I pulled the trigger and purchased the POWER COLOR REAPER 9070XT and it works perfectly fine.  No issues at all just works no issues at all even with me running it at 4K.  It is a smaller card but itâ€™s still pretty large.  Not as long as my last monster.,pcmasterrace,2026-02-24 18:29:49,1
AMD,o76x8p3,"There just can't be this much money in AI can there? No way every single corpo will make billions off the same damn thing.   In fact, the more corps that jump on the bandwagon, the more sure I am that it'll come crashing down (to a certain extent, AI is here to stay).",pcmasterrace,2026-02-24 19:30:22,1
AMD,o785b57,This just in: Publicly traded megacorp isnt your friend and never cared about you.    More shocking news at the top of the hour.,pcmasterrace,2026-02-24 22:58:09,1
AMD,o75lvkf,9070XT EoL update next year lmao and no new GPUs to come either. Hate Ngreedia all you want but at least they toss a bone or two. Well at least until 2027 when it all comes crashing down,pcmasterrace,2026-02-24 15:58:01,0
AMD,o75u45m,Rest in piece AMD.,pcmasterrace,2026-02-24 16:35:02,0
AMD,o75hslr,Good thing my mom is a chicken nugget,pcmasterrace,2026-02-24 15:39:26,0
AMD,o75mgwh,I cant believe we rely on Intel now  https://preview.redd.it/28cri0bntglg1.jpeg?width=500&format=pjpg&auto=webp&s=a814f4a7082486a230198e0b36ba0a1772e1c056,pcmasterrace,2026-02-24 16:00:41,0
AMD,o75ttai,My 1080Ti HODLing,pcmasterrace,2026-02-24 16:33:41,0
AMD,o75yw7v,"AMD has always followed their competitors lead. Whole ""for the gamers"" marketing last year was farcical.",pcmasterrace,2026-02-24 16:56:17,0
AMD,o764kox,The silver lining would be more optimized games going forward I guess?,pcmasterrace,2026-02-24 17:22:02,0
AMD,o76dj4a,That will basically make every GPU in existence go up in value. Even a 2060.,pcmasterrace,2026-02-24 18:02:28,0
AMD,o7797gx,I said goodbye to AMD GPUs forever ago lol.,pcmasterrace,2026-02-24 20:25:54,0
AMD,o77plza,So stupid. Could have taken higher market share of the gaming market to go for something that will fail in the next few years. Hell Meta is losing so much money it is ridiculous.,pcmasterrace,2026-02-24 21:41:30,0
AMD,o7aq6zb,I just bought a 9070 xt what the fuck man,pcmasterrace,2026-02-25 09:18:01,0
AMD,o75ob8s,Nvidia is still much worse so I assume you won't be buying any new GPUs from now on?,pcmasterrace,2026-02-24 16:09:02,-3
AMD,o78f4ju,Didn't Nvidia say they were going to stop producing consumer GPUs? Does this mean that in the next generation of GPUs will just be Intel by itself? Oh god...,pcmasterrace,2026-02-24 23:51:15,-1
AMD,o7647oh,Intel now has a _very_ good chance to gain market share in the GPU space.,pcmasterrace,2026-02-24 17:20:23,-2
AMD,o75hznd,"AI vibecoding is my new gaming, so I kinda think its ok. ðŸ˜ Keep them clouds juiced, I wanna code!",pcmasterrace,2026-02-24 15:40:20,-16
AMD,o75gp8y,I'm guessing that they'll mostly abandon their software for gaming GPUs. Nvidia still works on it and maintains their track record but AMD gives zero fucks.,pcmasterrace,2026-02-24 15:34:21,445
AMD,o75n41l,Just look at amd.com. They are not a PC parts company anymore.,pcmasterrace,2026-02-24 16:03:37,25
AMD,o75cies,we're in the same boat,pcmasterrace,2026-02-24 15:14:44,76
AMD,o7667gy,Exactly why I went with a Ti instead of a regular one.  That 16GB gotta last *awhile* I'm afraid.,pcmasterrace,2026-02-24 17:29:30,23
AMD,o762ouw,Here's hoping,pcmasterrace,2026-02-24 17:13:27,3
AMD,o76xs5d,Me except my 7900 xtx lol,pcmasterrace,2026-02-24 19:32:52,2
AMD,o77a8oe,This is my first PC- but my PC also has a 5070ti. Seems like I made the right decision,pcmasterrace,2026-02-24 20:30:44,1
AMD,o7bc4nc,"My 3080 shall ride with me to Valhalla, shiny and chrome.",pcmasterrace,2026-02-25 12:24:31,1
AMD,o76gpjx,And I'll be waiting here when it collapses.,pcmasterrace,2026-02-24 18:16:30,0
AMD,o75loyw,AMD will drop driver support in a 2 years. Bet,pcmasterrace,2026-02-24 15:57:11,234
AMD,o75lppc,Same bro,pcmasterrace,2026-02-24 15:57:17,7
AMD,o76omcj,12-pin go burrrrrr,pcmasterrace,2026-02-24 18:51:20,1
AMD,o758zsm,Now you just need to post this meme 5 times,pcmasterrace,2026-02-24 14:57:46,106
AMD,o75dhpt,![gif](giphy|0itiANRUpOXGRYQHfL),pcmasterrace,2026-02-24 15:19:20,26
AMD,o75l4ek,![gif](giphy|ITUJNUnX25T7q),pcmasterrace,2026-02-24 15:54:35,19
AMD,o75ammk,"like they have to justify anything, they dont care bud prices will hike and people will pay. it fucking sucks but its the truth",pcmasterrace,2026-02-24 15:05:43,225
AMD,o75fmwn,Whatever Nvidia's flagship is minus like Â£100 or something,pcmasterrace,2026-02-24 15:29:22,21
AMD,o762voj,Bold of you to assume consoles will remain under a grand lol.,pcmasterrace,2026-02-24 17:14:19,11
AMD,o75ie38,"The gpus in consoles are more like integrated graphics then a real gpu. As in, they don't provide the Vram etc. Their deals with Sony and Microsoft won't save the pc gpu market in any way.",pcmasterrace,2026-02-24 15:42:09,6
AMD,o75k2wp,"Lol they will up the prices and tell you, you either buy them at these insane prices or you're not getting anything.",pcmasterrace,2026-02-24 15:49:53,4
AMD,o75lnmd,"Think of it like this. Their biggest question any company has is, how much profit does this make? If your thing isnâ€™t at the top of that list, youâ€™re shit out of luck..",pcmasterrace,2026-02-24 15:57:01,4
AMD,o75omqm,"I think Sony will just have to deal with it, which is why they are delaying or are rumoured to delay the PS6. It wouldnâ€™t be completely out of the question that they heard about this deal which is why we started getting leaks about a delay because so he just knew they wouldnâ€™t be able to compete with Meta.",pcmasterrace,2026-02-24 16:10:29,5
AMD,o77knod,They can't hike the prices.  Which is why Nvidia stopped supplying hardware for most consoles.,pcmasterrace,2026-02-24 21:18:49,2
AMD,o75g0c8,No it wouldnâ€™t lol.,pcmasterrace,2026-02-24 15:31:06,1
AMD,o796nix,"Great. Were going to be ""shucking"" consoles to build PCs in the future.",pcmasterrace,2026-02-25 02:24:29,1
AMD,o7azl92,"AMD does not care about regular customers. With these six-gigawatt deals with Meta and OpenAI, AMD is going all-in on the AI data center. If everything works out, AMD is selling out their own shareholders to get Meta and OpenAI to figure out the software support for AMD's Instinct GPUs. Meta and OpenAI each get 10% of AMD in the form of new stock.",pcmasterrace,2026-02-25 10:44:43,1
AMD,o762679,"The thing is,if this bubble ever bursts whatâ€™s going to happen to all of them. Are they going to be bailed out,or are they going to be left out to rot,and if that does happen,whatâ€™s going to happen to the chip production industry globally. Either way I see it,we as consumer are ultimately fu***d",pcmasterrace,2026-02-24 17:11:05,48
AMD,o76qcis,"That hypothetical stock is being bought by real dollars, though. AMD is happy to sell out their future cards for money today. Especially at the current market prices. I'm not trying to be an asshole, but they know exactly what they are doing and it's far from ""just plain stupid"".",pcmasterrace,2026-02-24 18:59:00,9
AMD,o7773uw,I hope they all burn. Fuck em.,pcmasterrace,2026-02-24 20:16:03,3
AMD,o76bkio,I bet blacksmiths who made horseshoes felt the same way about the automobile.,pcmasterrace,2026-02-24 17:53:47,1
AMD,o76f7sh,Yeah 100s of millions of users is hypothetical demand ðŸ¤¡,pcmasterrace,2026-02-24 18:09:56,-4
AMD,o758tic,hahaha nice joke.,pcmasterrace,2026-02-24 14:56:55,128
AMD,o75hfis,"More than likely studios will focus even more on optimizing for console, and leaving PC users to eat shit. This were swinging back the other way for a while there.",pcmasterrace,2026-02-24 15:37:45,34
AMD,o75kh94,We are maybe just going back to shitty pc ports at this time.,pcmasterrace,2026-02-24 15:51:41,6
AMD,o76kr7a,Lmao,pcmasterrace,2026-02-24 18:34:25,3
AMD,o77tf4t,*finger curls on the monkey's paw* extreme ultra performance FSR/DLSS setting is now hardcoded into every game.,pcmasterrace,2026-02-24 21:59:12,2
AMD,o76pxmn,we can only hope that AI somehow will learn how to optimize for so cheap that super greedy managers will actually use it,pcmasterrace,2026-02-24 18:57:10,1
AMD,o78tipi,Not even remotely.  Graphics tech will just stagnate for a few years.,pcmasterrace,2026-02-25 01:09:28,1
AMD,o75r1qy,"It already did. Offsetting costs was cheaper when you could buy a new GPU every five years for peanuts, but now it's become unaffordable for many people. Devs are aware of it.",pcmasterrace,2026-02-24 16:21:24,0
AMD,o75xgts,That's fucked. Fucking hate that guy.,pcmasterrace,2026-02-24 16:49:56,24
AMD,o7b0915,"Yeah, 10% to OpenAI. Now 10% to Meta. 20% new stock will dilute the power of the current shareholders if AMD's plans work out. Why stop there? AMD should give 10% to Alphabet to get them to buy Instinct GPUs. Why not? The shareholders do not appear to oppose these actions.",pcmasterrace,2026-02-25 10:50:30,1
AMD,o75bcd6,Less impressive to investors who dont know what the hell a flop is.,pcmasterrace,2026-02-24 15:09:12,53
AMD,o759frt,Of course it would.,pcmasterrace,2026-02-24 14:59:55,13
AMD,o75fim2,But you don't build a datacenter based on flops. The facility is designed to handle a certain amount of power and the resulting waste heat and whatever the tflops value that comes out is what you get.   The compute capability can also change significantly over time as CPUs and GPUs are upgraded while the total power and heat remain the same. For a multi year deal this might mean a completely new generation of accelerators being installed on year two or three vs at the start of the build out.,pcmasterrace,2026-02-24 15:28:49,13
AMD,o75li6h,"it's just one GPU that sucks up 6 gigawatts, we'll be fine",pcmasterrace,2026-02-24 15:56:19,6
AMD,o76wxvy,Flops per watt improves every GPU generation so instead they talk about the max power consumption. Meta is planning a datacenter that can get up to 6 gigawatts. In 5 years the chips could be 2x more efficient than today,pcmasterrace,2026-02-24 19:28:59,4
AMD,o75fdp2,"Honestly I have no idea. But if I were to guess based on the assumption that the tech companies know tech stuff, I'd say it's got something to do with how efficient a processor is at turning electricity into heat. With the same architecture, I can see how power consumption would be a very good indicator of the total output a chip can produce.  On the other hand, what if as part of the revolution we're currently in the middle of, they find out that decimal calculations aren't the most important thing to optimise for? While power consumption will always be a decent indicator of a chips output, the type of operation could drastically change.  Alternatively, maybe companies want to optimise their contracts, and if power consumption from the grid or a private farm is the governing feature, then they would want their chips to consume that amount regardless of amount or type of output.",pcmasterrace,2026-02-24 15:28:12,1
AMD,o772ugk,They won't,pcmasterrace,2026-02-24 19:56:12,16
AMD,o772gsp,Where GDDR6? ðŸ‘€,pcmasterrace,2026-02-24 19:54:28,2
AMD,o75qtgl,"I do, and would take the opportunity if it presented itself.",pcmasterrace,2026-02-24 16:20:21,1
AMD,o75h9m3,Intel can and will provide affordable gpus to gamers.  ![gif](giphy|DwWA0wHJIhusb6YwFu),pcmasterrace,2026-02-24 15:36:59,36
AMD,o75958b,At this rate they will deprecate its driver support 2027. AMD has been taking self made L's from left and right.,pcmasterrace,2026-02-24 14:58:29,92
AMD,o75eci6,"I have the smaller xt brother, but yeah, they will last a while :)",pcmasterrace,2026-02-24 15:23:19,3
AMD,o7bmy1x,My 6700 XT will serve me just fine for the forseeable future. Upscalers are good to the point of me playing Cyberpunk 2077 in 4K with mostly High/Ultra detail at 60 fps.,pcmasterrace,2026-02-25 13:31:49,1
AMD,o758wzd,Glad I bought one a few years ago. It has been a great GPU.,pcmasterrace,2026-02-24 14:57:24,1
AMD,o75h479,Too bad no FSR4 support.,pcmasterrace,2026-02-24 15:36:18,1
AMD,o758ao3,7900xtx monster,pcmasterrace,2026-02-24 14:54:21,2
AMD,o76xkuu,"Ha, this one made me laugh out loud. :D",pcmasterrace,2026-02-24 19:31:57,1
AMD,o75ir4d,This is me with my 6950xt i hope it lasts me at least 7 years like my 1060 did,pcmasterrace,2026-02-24 15:43:50,6
AMD,o762iv2,"I'm looking into ways to jailbreak/root the ""cloud machines"" they'll force down our throats, instead, so that I'll install Linux on them and link them together into a computing farm of some sort.  It'd be useless for gaming, but at least I should be able to run some programs on them.",pcmasterrace,2026-02-24 17:12:41,6
AMD,o75wof0,https://preview.redd.it/a-finally-honest-upgrade-list-v0-3bdli0cwbrae1.jpeg?width=640&crop=smart&auto=webp&s=b1b2c5744e7e15779e0ff3e5082bb9d759f75b19,pcmasterrace,2026-02-24 16:46:27,9
AMD,o75gmbl,AMD is worse tbh their prices are Nvidia - 50 while their feature set isnt even comparable.,pcmasterrace,2026-02-24 15:33:58,7
AMD,o77vl7v,Assuming combined power consumption of the chips?   Just my best guess lol.,pcmasterrace,2026-02-24 22:09:31,5
AMD,o76hs5z,Its provides the business case for AI. See people using AI.  Complete garbage,pcmasterrace,2026-02-24 18:21:14,6
AMD,o77fcg1,The problem is that they had already invested so much that they cannot back off,pcmasterrace,2026-02-24 20:54:36,2
AMD,o76swhw,"Lot of people don't want to communicate or have fear of giving responses, reacting. AI is very welcoming for them; they don't have to 'deal with that anymore'.",pcmasterrace,2026-02-24 19:10:31,2
AMD,o77fwkl,"This is there whole plan, they want people to sub cloud pcs for gaming",pcmasterrace,2026-02-24 20:57:09,4
AMD,o77bne9,I'll probably get a new library card,pcmasterrace,2026-02-24 20:37:26,3
AMD,o76ubca,"Glad Ä° invested in Xbox few years back, several. Don't get me wrong, Ä° also have a gaming rig from 2014 that still holds up for me, and also 2 fuck Nintendo's. How could we have now we were getting quadruple whammied before Skynet took over.",pcmasterrace,2026-02-24 19:16:59,4
AMD,o76rlia,"First frame it, then put it in the vault.",pcmasterrace,2026-02-24 19:04:35,2
AMD,o77j0hs,"The amount of mentally challenged and smooth brain AMD shills that exist in this space/community is insane.   No, my AMD would never be anticompetitive. No, my AMD would never price gouge. No, my AMD would never jump on the AI hype train. No, my AMD would never exhibit corporation type behaviours.   Corpo bootlickers are bad for every consumer.",pcmasterrace,2026-02-24 21:11:23,2
AMD,o77lhbm,"People downvote you but you are right. People that unironically use ""team red"" or ""team blue"" or ""team green"" should be shunned",pcmasterrace,2026-02-24 21:22:36,3
AMD,o76448p,Yeah I am confused lol,pcmasterrace,2026-02-24 17:19:57,1
AMD,o76orv6,Thats like 7.5 million of their high end commercial GPUâ€™s,pcmasterrace,2026-02-24 18:52:02,1
AMD,o77hmr3,Why would they? The moment Intel GPUs could compete against a wet paper bag theyâ€™ll jump into the AI space too,pcmasterrace,2026-02-24 21:05:03,3
AMD,o7643f5,Hope their deal with Disney bankrupts them.  Disney recently signed a deal with OpenAI allowing them to exclusively generate Disney character slop using their AI.  I have without doubt money will flow to Disney each time Disney Slop is generated. Wonder how much it would take to make OpenAI owe the mouse one trillion dollars.,pcmasterrace,2026-02-24 17:19:50,3
AMD,o76nfuu,"I stopped using it an year ago, fuck it, humans don't know it at all, corpo evils are simply trying to get rid of their work force through it, ai agents suck at communication, they simply don't have a soul, only a human can understand a human, and I know for a fact that AI, call it agi, super human AI whatever, will never ever take place of human because it won't be ever a sentient being, for a sentient being to come alive, it will need biological drive to survive and thrive under the pressure of evolution, and humans pretty sure won't create a super human if it does, it won't listen to human beings or feel a thing for them, cuz it will be a totally different sentient being, so at the end of the day, AI will simply be buncha codes, if comes sentient it will see humans as threat to its own existence, humans won't probably want that at all.",pcmasterrace,2026-02-24 18:46:12,1
AMD,o77ff1r,Enough to send a DeLorean back to 1955 nearly 5 times,pcmasterrace,2026-02-24 20:54:56,2
AMD,o75kjqo,It's crazy how vast the PC world can be. I have had 2 AMD cards (RX580 and now a 6900XT)  and have not experienced any problems with drivers.,pcmasterrace,2026-02-24 15:52:00,7
AMD,o75g8du,Been using AMD for 6 years now on multiple cardsâ€¦zero issues.  Most people are lazy and blame AMD when they switch cards and not all the old conflicting software they left on their pc. I always do a full reinstall of windows when I change major components.  Buy a new build with AMD and youâ€™re unlikely to have those issues.,pcmasterrace,2026-02-24 15:32:10,4
AMD,o763hl2,If you lack the ability to spend 5 minutes trouble shooting an issue AMD isnâ€™t for you. Just pay the premium and get nvidia. Or if youâ€™re purchasing the Pc for someone who isnâ€™t knowledgeable with computers. Iâ€™ve had exactly one issue with my 6950xt since purchase. Took all of 5 min to fix by updating drivers. If you arenâ€™t tech savvy then I get it though.,pcmasterrace,2026-02-24 17:17:06,-1
AMD,o78d7cf,">Why anyone would choose AMD over Nvidia at this point is just beyond dumb imo.  Linux support. AMD's GPU drivers have better driver support there than on Windows. You can even run raytracing on pre-RDNA2 cards.  Nvidia's drivers suck on Linux, and aren't open source.",pcmasterrace,2026-02-24 23:40:25,1
AMD,o76daeh,You will be more concerned with foraging for food than buying GPUs,pcmasterrace,2026-02-24 18:01:24,4
AMD,o770oz7,honestly i will probably buy one. theres no way im paying what nvidia is asking right now.,pcmasterrace,2026-02-24 19:46:19,0
AMD,o77g14y,I said goodbye to Nivida GPUs forever ago lol,pcmasterrace,2026-02-24 20:57:44,0
AMD,o79vjkz,Why? That the worst market. Thing margins,pcmasterrace,2026-02-25 04:56:24,1
AMD,o79eoa0,Nvidia already said nothing new this year,pcmasterrace,2026-02-25 03:09:10,1
AMD,o75z4tl,That's as much coding as AI playing games for you,pcmasterrace,2026-02-24 16:57:21,3
AMD,o75hrvn,Exactly they were already going to deprecate support for RDNA3/2. FSR4 not coming to these products. Restone actually being very bad.  I hope RDNA actually is supported after the so called UDNA releases.  We should not ever support such companies. Can believe that we have this kind of competition after so long. Intel will be AMD's replacement.,pcmasterrace,2026-02-24 15:39:21,118
AMD,o75tk82,AMD produced the Chips for Sony Playstation and Microsoft XBOX. They are going to improve the software.,pcmasterrace,2026-02-24 16:32:34,28
AMD,o75umub,This is dumb and there is zero reason to think so. AMD has released new features and updated their software many times over the last couple years.,pcmasterrace,2026-02-24 16:37:22,179
AMD,o76adgp,"AMD have hired loads of software engineers in both departments in recent years. It makes little sense to redeploy people to something they have no experience nor desire to work in.  The AI people will continue doing AI, and gaming will continue as is. Itâ€™s also important for console and semi-custom, which are both important business units.",pcmasterrace,2026-02-24 17:48:31,10
AMD,o763idu,I fucking knew AMD would do this. Yeah NVIDIA aren't the best but I paid more for my 5070Ti when I could've gotten an AMD card for cheaper but I knew they'd turn to dogshit.,pcmasterrace,2026-02-24 17:17:12,10
AMD,o76fqth,AMD has always been very behind Nvidia on the driver side to the point that Nvidia was a better buy despite the cost and power consumption going back forever.,pcmasterrace,2026-02-24 18:12:16,3
AMD,o764foo,This isn't completely true. I have a 50 series GPU and have never seen Nvidia cards have so many issues related to drivers and not to mention the new 12v connector that likes to catch fire. The quality in drivers has suffered heavily with multiple times last year where it wasn't best practice to be on the latest driver.,pcmasterrace,2026-02-24 17:21:24,2
AMD,o78cm0q,"Spot on, already their software support is crap. No fsr 4 on rdna 3 despite it being pretty obviously implementable. Drivers are absolute ass and so many games have bugs/crashes. Canâ€™t play watch dogs 2, a game that came out in 2016, without it crashing on a 7900 xt. Absolute joke of a company amd isâ€¦ not to say that nvidia is better, but at least when it comes to supporting shit long term they seem to be doing a better job.",pcmasterrace,2026-02-24 23:37:08,1
AMD,o78gh1e,That wouldnâ€™t be much of a changeâ€¦,pcmasterrace,2026-02-24 23:58:45,1
AMD,o7d266h,ðŸ˜‚ sure keep saying that to yourself,pcmasterrace,2026-02-25 17:39:36,1
AMD,o77hz2s,Bullshit. Nvidia drivers have been shit for over a year now.,pcmasterrace,2026-02-24 21:06:37,1
AMD,o77whc2,And nvidia already stated their focus will be AI so who's left?,pcmasterrace,2026-02-24 22:13:50,1
AMD,o75k2e7,Hell yeah. Same specs brother.,pcmasterrace,2026-02-24 15:49:49,23
AMD,o77qvzl,"Buying that 5070 Ti back in December was my last chopper out of 'nam, for sure",pcmasterrace,2026-02-24 21:47:27,11
AMD,o762v66,"We are both set, ram cpu gpu.",pcmasterrace,2026-02-24 17:14:16,4
AMD,o77agzb,thatll still last ages tbh,pcmasterrace,2026-02-24 20:31:49,1
AMD,o75pxd4,Which is so fucking weird to me because then they go ahead and support a CPU chipset for 5 generations,pcmasterrace,2026-02-24 16:16:21,113
AMD,o75qk61,hopefully not,pcmasterrace,2026-02-24 16:19:11,5
AMD,o77konq,"I do not depend off of AMD, my driver is Valves RADV.",pcmasterrace,2026-02-24 21:18:56,4
AMD,o79a8ph,"don't worry, I'll just use open source driver",pcmasterrace,2026-02-25 02:44:27,4
AMD,o766s8l,Didnâ€™t they stop support on the Z1E? The chip thatâ€™s powering most PC handhelds with many of them coming out in the past few months?,pcmasterrace,2026-02-24 17:32:09,1
AMD,o7bg2kq,Then just use Linux.,pcmasterrace,2026-02-25 12:50:39,1
AMD,o77tzzh,2x8 go brrt.,pcmasterrace,2026-02-24 22:01:55,3
AMD,o76pcrw,"I undervolted it, takes about 220-250W, should be safe.",pcmasterrace,2026-02-24 18:54:36,1
AMD,o75cvw6,![gif](giphy|c2C2RyuXSizTO),pcmasterrace,2026-02-24 15:16:29,52
AMD,o75lc32,![gif](giphy|c23OB3KN3lnKo),pcmasterrace,2026-02-24 15:55:33,12
AMD,o7b8lnu,A electric output of  RMBK-1000 nuclear reactor at usual working day.,pcmasterrace,2026-02-25 11:59:05,1
AMD,o75fto2,Depending how high prices go they might lose a big chunk of customers. We all have a threshold how much we are willing to pay including irresponsible spending.  In this economy especially.,pcmasterrace,2026-02-24 15:30:15,56
AMD,o77hqr4,I donâ€™t think the console market will â€œjust payâ€. Those prices go up by a significant amount I think the market will shrink by a similar amount,pcmasterrace,2026-02-24 21:05:34,1
AMD,o76mp4p,"Haha, one has to remain hopeful! I think the PS5 Pro pricing will be what the standard PS6 goes for. It was Sony testing the waters for a more expensive machine.",pcmasterrace,2026-02-24 18:42:58,1
AMD,o7980ds,There are alternative ways of obtaining goods.,pcmasterrace,2026-02-25 02:32:07,0
AMD,o75t2de,"Sonyâ€™s deal predates this one by a significant amount of time, I am sure the lack of availability of RAM impacts them but AMD isnâ€™t going to change Sonyâ€™s already agreed price and volume.",pcmasterrace,2026-02-24 16:30:21,4
AMD,o77or92,"Right, I meant for their PC components. Sony signed a deal for $30 billion in 2024 for the PS6 so those are set in stone.",pcmasterrace,2026-02-24 21:37:31,1
AMD,o768mwk,"they will 10000000000000% be bailed out to some degree. They are officially 2b2f from a corporate investor's eyes, which are the only eyes that matter",pcmasterrace,2026-02-24 17:40:40,69
AMD,o76aoth,"I am starting to speculate, but I think the old excess datacenters will be used to build massive surveillance networks, just like old crypto mining computing power was initially recycled to train AI cheaply. There isn't that much demand for anything else that requires such an vast amounts of computing power and the hardware that those datacenters will be obsolete for AI in 3-5 years tops.   The trend for state wide Surveillance has been trend for few years and legislators have tried to increase it in globally for past decades at this point. Pair that computing power, AI and face recognition with mandatory age verifications (Like Discord tried) and you will have dystopian surveillance state. Governments will try to justify it with child protection or war against drugs, like always.   Well at least consumer hardware will be cheap for a while, due to excess production capacity.  Edit, spelling",pcmasterrace,2026-02-24 17:49:53,25
AMD,o76xmv5,"I highly doubt the chip production industry would collapse. In the PC space, only Intel owns their own chip fabs, almost everyone else rely on TSMC. Nvidia, AMD, Qualcomm and Apple all rely on TSMC. If the x86 space collapses, I can see the market adjusting to adopting ARM architecture more, with Apple, Qualcomm, Mediatek etc. buying up more TSMC production to fill the void.",pcmasterrace,2026-02-24 19:32:12,1
AMD,o785d6k,"Yeah but cars were tangible, horseshoes were tangible. Imagine instead agreeing to kill all the horses on your farm so you can build a car factory, but no one gets a car from the factory and none of the factories that exist yet are making a profit.",pcmasterrace,2026-02-24 22:58:27,5
AMD,o76rlie,Donâ€™t forget your makeup ðŸ’„,pcmasterrace,2026-02-24 19:04:35,4
AMD,o758vxx,Let me dream,pcmasterrace,2026-02-24 14:57:15,34
AMD,o75js13,Traditional consoles are dying. Or changing. Xbox and Playstation are much smaller relative to the gaming market and PC gaming.,pcmasterrace,2026-02-24 15:48:31,20
AMD,o76jp7o,"No, dont you know that everyone wants to play In THe ClOuD???!?",pcmasterrace,2026-02-24 18:29:42,4
AMD,o7ae8c4,Ahem *ff16*,pcmasterrace,2026-02-25 07:26:46,1
AMD,o75rbz7,"And PR hates the term, because it has negative connotations.",pcmasterrace,2026-02-24 16:22:41,13
AMD,o7bnqot,Hopefully the whole thing flops before it's too late to go back to normal.,pcmasterrace,2026-02-25 13:36:19,1
AMD,o76k74r,And you don't sell a GPU based on watts. You sell a number of GPUs.,pcmasterrace,2026-02-24 18:31:56,1
AMD,o7c5b8h,In all the AI data centers dude... Even GPU RAM is in a crisis rn...,pcmasterrace,2026-02-25 15:08:19,1
AMD,o75i1vc,"B770 is nowhere to be seen, b70 released",pcmasterrace,2026-02-24 15:40:37,1
AMD,o75bv9t,"on Windows\*  On Linux, HD 7000 is still getting drivers.",pcmasterrace,2026-02-24 15:11:43,38
AMD,o75e3yk,amd cards age like fine vinegar..,pcmasterrace,2026-02-24 15:22:13,8
AMD,o75g20w,"If they aren't making new GPUs to sell to consumers, what would be their motivation towards bricking their older GPUs?",pcmasterrace,2026-02-24 15:31:19,-2
AMD,o759m0t,"only if we have new generation of gpus by 2027. or what, do you think they would stop supporting their last high-range gpu and just focus on low and mid range 9060's and 9070's?",pcmasterrace,2026-02-24 15:00:46,-6
AMD,o75b1ol,"That's great to know! I have the gen before and i find it's also great.  Mostly 1440 ultrawide, 144, high settings but no Windows overheads.",pcmasterrace,2026-02-24 15:07:45,0
AMD,o75hhni,"this may be a problem if I ever decide to start using it, so far amd fluid motion frames are enough",pcmasterrace,2026-02-24 15:38:01,3
AMD,o766yeg,Does the leaked DLL not work?,pcmasterrace,2026-02-24 17:32:57,2
AMD,o75ndsu,"Iâ€™m not really pressed about it. Itâ€™s shitty how amd handled it, but the card is already a work horse.",pcmasterrace,2026-02-24 16:04:50,1
AMD,o76hcop,Optiscaler,pcmasterrace,2026-02-24 18:19:19,1
AMD,o76qmjm,"lol...good luck on ""jailbreaking"" the ""cloud machines"".",pcmasterrace,2026-02-24 19:00:14,14
AMD,o75yk5n,"honestly, i have pretty big backlog of games that the 1080Ti would absolutely demolish at 1080p Ultra. I'll upgrade in 2030 or sum shi.",pcmasterrace,2026-02-24 16:54:48,13
AMD,o76t9g6,"That's not even remotely my point.   I extrapolated the most likely meaning of what OP copy pasted, but we have no way to know if that's what they wanted to say, let alone if it's true.",pcmasterrace,2026-02-24 19:12:10,1
AMD,o76scl4,We all now what the cylons did and how it ended eventually.,pcmasterrace,2026-02-24 19:08:00,1
AMD,o774n6f,"As much as I hate Nvidia, my current GPUs might be my last AMD  products.  AMD seems to be dropping support for products that are perfectly capable. FS4 seemed to make huge improvements and was close to on par with DLSS, then the new DLSS dropped and showed how far behind AMD really is.  You might pay a premium for Nvidia but they seem to be supporting their older stuff a lot better than AMD does.  The current performance of my 7900XT and my 9070XT are great but what happens later this year when the 7900XT turns 4 and AMD decides it's a dead platform. Nvidia moved the 900 series GPUs to maintenance mode 11 years after release.  AMD can't be bothered to support a CPU for that was still being released less than 2 years ago.",pcmasterrace,2026-02-24 20:04:32,1
AMD,o79o13t,And Linux is such a small percentage of people that it loops back around to being irrelevant lol.,pcmasterrace,2026-02-25 04:05:36,-2
AMD,o75utxe,"Intel is deciding whether or not to break itself up and sell itself off to competitors, Intel is NOT going to be anyone's competitor in the GPU space.",pcmasterrace,2026-02-24 16:38:15,97
AMD,o75j963,Intel is not making new gaming cards.,pcmasterrace,2026-02-24 15:46:07,24
AMD,o75rizs,"Its kinda weird what their intentions are, when they split the drivers they stated that they were working on features for both rdna3 and 4, yet they now wont provide the version we know works",pcmasterrace,2026-02-24 16:23:32,9
AMD,o796q0u,Imagine coping for Intel lmao,pcmasterrace,2026-02-25 02:24:53,1
AMD,o762v0l,"As a 7900xt owner, what new features",pcmasterrace,2026-02-24 17:14:14,191
AMD,o763uzd,"You can ask the 7900xt,xtx or any 7000 gen buyers. They are basically abandoned by amd, while modders can easily make new fsr work on these cards.",pcmasterrace,2026-02-24 17:18:46,57
AMD,o7657hb,"Fuck them, let um change. Out of thr ashes and 5 years time we will have new players and more competition as a result.  We should be happy about this and when they come back, not buy their junk.  Ea pulls the battlefront 2 sense of accomplishment bs and ive havent bought a single ea games since.  Vote with your money.",pcmasterrace,2026-02-24 17:24:56,2
AMD,o77bkn9,Lmfao you vsnt honestly believe that and if you do man amd bros got a screw loose,pcmasterrace,2026-02-24 20:37:04,1
AMD,o76pn55,"lmao their rollout of FSR Redstone has been a dumpster fire, FSR 4 still lacks Vulcan support, RDNA 2/3 still havenâ€™t got FSR 4 despite us having ample proof the INT8 version works better in performance/ultra performance than FSR 3 does in quality",pcmasterrace,2026-02-24 18:55:52,1
AMD,o76lz7p,"Ah yes, like their own version of Nvidia broadcast that they bricked ""accidentally"" a couple driver updates ago and it's been 3 months with no fix. Is that the new feature you're referring to?",pcmasterrace,2026-02-24 18:39:51,0
AMD,o76vfza,"Like FSR Redstone? Doesn't seem to be doing that great, about... What, a year(?) later. Support includes two games.",pcmasterrace,2026-02-24 19:22:08,-1
AMD,o77ghsh,"Amd already sucks real hard, and that is with 100% focus on desktop",pcmasterrace,2026-02-24 20:59:52,-1
AMD,o768t7p,well yeah anyone with a brain can figure this out. no company is our friend. they're all aiming to stuff more and more money into their pockets as much as they can. never understood that whole nvidia vs amd BS when it's more like nvidia + amd vs avg. consumer,pcmasterrace,2026-02-24 17:41:28,6
AMD,o78u756,"Nvidia are the best. That's why they own most of the market share and have the best products. If money were not a problem, the only people that would buy AMD gpus are people that need it for Linux.",pcmasterrace,2026-02-25 01:13:18,1
AMD,o77zy9i,I think the 5070Ti consumes less power than the 9070XT especially with undervolting.,pcmasterrace,2026-02-24 22:30:58,2
AMD,o77qfk3,Yet myself and four other friends all have 50xx and no problems.,pcmasterrace,2026-02-24 21:45:19,2
AMD,o792t3x,"That was true until 2025, Nvidia does have a track record of intentionally fucking the performance on a previous gen to make their new shit look good, but the drivers ever since 50 launched have been obscenely shit on everything.  566.36, 576.8 and 581.8 are the only functional drivers in the last 15 months on 10-40 gen. I had to fall back on them last week because it was their fucking shit drivers making my monitor dim all the time. I only found out because the last 5 driver patch notes have had ""trust me bro it's really fixed this time"" on all of them.",pcmasterrace,2026-02-25 02:02:45,1
AMD,o764sgi,"yooo, wassup twin",pcmasterrace,2026-02-24 17:23:01,4
AMD,o7ae3g0,Praise brother,pcmasterrace,2026-02-25 07:25:33,1
AMD,o75q4s6,Not anymore. They're dropping support for z1 extreme lol. AMD are entering their enshitification era.,pcmasterrace,2026-02-24 16:17:17,95
AMD,o77e8g3,"They absolutely didnt want to, but the consumer outcry â€œforcedâ€ them to (ez marketing win + excuse to keep making cheaper CPUs/sell off AM4 stock)",pcmasterrace,2026-02-24 20:49:30,1
AMD,o7ayw3w,"AMD had no choice because AM4 was a brand new chipset with unknown prospects. No one is going to buy into new platform without some incentives. AM4 and Ryzen was their last chance. If they failed, AMD would go bankrupt. In 2017, first gen AMD Ryzen offered six and eight core CPUs on a mainstream socket, which was a big value proposition, and long-term support for AM4 was another selling point. AMD had a robust timeline of next generation CPUs. Zen+ (2018), Zen II (2019), and Zen III (2020) released on time and delivered solid performance gains to catch up to Intel.  Right now, we are in a whole different ball game. AM5 will live on because AMD is focused on data center and AI. Last year, AMD unofficially hinted on ending support for AM5 with Zen V as the last CPU generation, but after the backlash, they decided to extend support to 2027+. Recently, AMD's Zen VI is rumored to launch at CES 2027 alongside Intel's Nova Lake, so AMD appears to be extending the cadence of CPU launches,",pcmasterrace,2026-02-25 10:38:31,1
AMD,o7674l2,Not really. Even Open Source drivers for AMD GPU require a huge binary closed-source blob to be loaded. If they stop support weâ€™re still stuck.,pcmasterrace,2026-02-24 17:33:44,7
AMD,o76qtdt,"The chip is three years old in may though. Three years its not a lot, but its not like it was dropped immediately.",pcmasterrace,2026-02-24 19:01:04,1
AMD,o77xvpk,3 of em in my case,pcmasterrace,2026-02-24 22:20:43,2
AMD,o79h3a7,My 3090 with 2x8pin is on track to know how a B-52 feels when it gets yet another life-extension upgrade for its 80th birthday.,pcmasterrace,2026-02-25 03:23:19,2
AMD,o75f1tm,\[wth is this Sepia ass filter in my gif\]  ![gif](giphy|149gVqjyvMnV72),pcmasterrace,2026-02-24 15:26:39,30
AMD,o75hq6f,"Bottom of the fiat ponzi can not fall out soon enough. They are ""buying"" all this and forcing higher prices upon the consumer by taking out loans that have zero consideration. They are quickly building the case that they never planned to pay back this debt debasement tryst, for their motive appears to be more and more a propagation of fraud unchained.",pcmasterrace,2026-02-24 15:39:08,29
AMD,o75iedk,"i get that and ofc agree with you but looking at it they 're going for the quick buck here and us the ""regular customer"" didnt cut it so every brand that can will go to the ai crap because theres where is at. again i fucking hate it but we're not the priority anymore our threshold is meaningless to them",pcmasterrace,2026-02-24 15:42:11,1
AMD,o7ap9gy,"50 % of consumer spending is now driven by 10 % of the people. The sad reality is that most of the ""middle"" class simply doesnt matter anymore to these companies.",pcmasterrace,2026-02-25 09:09:14,1
AMD,o77ik04,"the same market who paid thousands in the release for a ps5? or the one who pay 90usd for nintendo re-re-releases? maybe the market who enter raffles for the privilege of buying a gpu, dunno man...",pcmasterrace,2026-02-24 21:09:17,1
AMD,o7apvft,"IMO, itâ€™s not a test. Itâ€™s the reality before all the storage and memory increases. And now imagine that with the storage and memory increases.",pcmasterrace,2026-02-25 09:15:00,1
AMD,o7ax0al,That was before the AI boom.,pcmasterrace,2026-02-25 10:21:18,1
AMD,o75tlk7,"Eh, only if Sony had done their mass order of chips for the PS6 at the time.",pcmasterrace,2026-02-24 16:32:43,3
AMD,o77rgxb,It's an APU.  You likely won't see any price hikes on Strix Point.,pcmasterrace,2026-02-24 21:50:10,1
AMD,o7781np,Effectively,pcmasterrace,2026-02-24 20:20:26,8
AMD,o7720so,Time for clothing/prints that poison the recognition software.,pcmasterrace,2026-02-24 19:52:25,7
AMD,o79rnwv,"When you fill up your car, do you specifically enter the number of liters or gallons of gas that you want? No you just know that you have a 30 gallon tank and that you want to fill it up, or that you have $50 to spare so you punch that in. The number of liters of gas you buy entirely depends on how much more your car's fuel tank can accommodate, or what the gas budget was.  Same thing with datacenters. A company the size of Facebook isn't going to order individual GPUs piece by piece, or maybe even individual racks like AMD Helios or Nvidia NVL.   They just know how much capacity their latest datacenter has and go to a company like HPE (or direct to AMD/Nvidia) and ask for a quote for like 10MW worth of rack scale systems. Money would have been the other constraint, but apparently there is infinite money available for AI so these deals are being specified in watts rather than saying Meta is buying $100M of Helios racks.",pcmasterrace,2026-02-25 04:29:34,1
AMD,o75cx93,Is that AMD making those drivers or community made drivers?,pcmasterrace,2026-02-24 15:16:40,19
AMD,o75clal,Doesn't change the fact that AMD's recent track record for product software support is abysmal. They seriously need to improve and stop beating themselves in the head.,pcmasterrace,2026-02-24 15:15:07,13
AMD,o75suy0,">still getting **better** drivers  I've a 5700XT that on Windows absolutely craps itself once a day, sometimes corrupting the drivers so I have to reinstall them.  On Linux, 0 issues.  Same pc, same games ðŸ™ƒ",pcmasterrace,2026-02-24 16:29:27,10
AMD,o75gyn0,You mean without driver updates after they deprecate RDNA3/2 support with no FSR4.,pcmasterrace,2026-02-24 15:35:35,4
AMD,o75gg1h,Go ask them. But quite obviously to save costs. Not like the old ones are earning them any income.,pcmasterrace,2026-02-24 15:33:09,6
AMD,o75ba80,"They already tried to kill the support for rdna 1 and 2 (5000 and 6000 series) last year. They still haven't made fsr4 available to older cards, even 7000 series, even though the leaked version has shown that it works pretty well on those. Now there's rumours that they are killing support for one of their most popular handheld chips, z1e.   So yes, I wouldn't be surprised if they try to kill 7000 series next year.",pcmasterrace,2026-02-24 15:08:54,17
AMD,o75czdm,"I mean they're already doing that. They're focusing on Redstone and it's technologies, only the 9000 series gets it. Besides they're both mid range cards. The 7900xt and 9070xt performance in rasterization is almost equal within 10% sometimes the 9070xt is a bit faster, sometimes the 7900xt is, depending on the game and setup.  The difference is big in ray tracing performance though up to 30% in favor of the 9070xt",pcmasterrace,2026-02-24 15:16:56,5
AMD,o76hezz,It does,pcmasterrace,2026-02-24 18:19:36,1
AMD,o76sieq,"I meant jailbreaking whatever device they'll let us use directly, not the remote computers themselves which are obviously impossible to access or modify legally.",pcmasterrace,2026-02-24 19:08:44,6
AMD,o7a2zgw,If ands or butsâ€¦.  I donâ€™t try to speculate and just focus on buying the most reasonable components for my use case whenever my old parts no longer keep up with the games I play.,pcmasterrace,2026-02-25 05:51:46,1
AMD,o7d7gf7,"yep, but you said ""anyone"" buying amd is dumb, so.....",pcmasterrace,2026-02-25 18:03:11,0
AMD,o76863k,"unless ARM for gaming or RISC-V somehow breaks through we're going to need some form of intel to maintain SOMETHING, consumer electronics as a whole is getting cooked right now in favor of big data",pcmasterrace,2026-02-24 17:38:34,27
AMD,o75t53v,Lol that's corporate speak for idgaf for people who supported me those peasants be blessed that AMD is still in the ring with Nvidia not knowing Nvidia is just 50 dollars away according to the launch MSRP.,pcmasterrace,2026-02-24 16:30:41,-2
AMD,o763tvd,"I have the same card, i had to revert a couple updates cause the latest one was so broken games woudnt play or i had enormous lagspikes and FPS drops. F That fixed itself by reverting to the past update lol.",pcmasterrace,2026-02-24 17:18:38,59
AMD,o76atd4,Like FSR 4 which works perfectly fine on 7000 series but is gatekept by AMD .,pcmasterrace,2026-02-24 17:50:27,55
AMD,o76g6u1,"Have you tried clicking the ""what's new button?"" The fsr versions specifically.",pcmasterrace,2026-02-24 18:14:13,1
AMD,o76ctmg,I have a 7900 XTX and literally zero issues with it,pcmasterrace,2026-02-24 17:59:20,1
AMD,o76o7l2,"why are you not being downvoted? the last time i said almost exactly the same things that were true, i got only downvotes...",pcmasterrace,2026-02-24 18:49:33,-2
AMD,o77lh2b,"Because of FSR 4?   Â ðŸ™„  They didn't enable it because the performance is not very good for FSR 4 on RDNA 3, which the modders have found out.  They never promised FSR 4 would work on RDNA 3 GPUs.   It's crazy work to say those products are abandoned because they didn't get a feature they were never promised. I have an RX 7900 XT in one of my rigs at home (I have both Nvidia and AMD GPUs) and it's been a great card with great features and performance. It still gets updates as well.",pcmasterrace,2026-02-24 21:22:33,0
AMD,o76krkf,I like your enthusiasm but if you think that a new company is going to come out of the woodwork and provide you with a new GPU I have news for you stop dreaming because the only player in this field is INTEL and they have not expressed any desire to save the gaming world.  Then writing is on the wall the existing companyâ€™s are going to switch production to satisfy AI demand and we are going to get the screwing of our lives.,pcmasterrace,2026-02-24 18:34:27,9
AMD,o77k54h,"I am one of the first to bang the ""vote with your wallet"" drum but you have to be joking.   If no one bought an AMD GPU for gaming they would still completely sell out their supply due to the data center demand.   There are only two serious competitors in that space and both are following the same general trends.   Now if you are telling me that AMD is doing consumer unfriendly things and therefor you recommend buying Nvidia than you are either a troll or a fanboy.",pcmasterrace,2026-02-24 21:16:31,3
AMD,o781zwd,Itâ€™s too late to vote with our money. Gaming makes up less than 10% of revenue for both NVIDIA and AMD. We could all walk away and they wouldnâ€™t care because theyâ€™ll sell that market share to enterprise for more money anyways.,pcmasterrace,2026-02-24 22:41:14,1
AMD,o78tz2g,"This needs more upvotes. The Inevitable ""but XYZ"" from people below you will be interesting to read.",pcmasterrace,2026-02-25 01:12:01,1
AMD,o77i06f,"Um... what? I work in support and work with both Nvidia and AMD products on a regular basis, what kind of work do you do with AMD products?",pcmasterrace,2026-02-24 21:06:45,1
AMD,o7co48c,How so?,pcmasterrace,2026-02-25 16:35:12,1
AMD,o7ad0ta,"UV is the great equalizer. My current setting allows me to roughly reduce the power consumption down to 260w with my 9070xt, while only losing roughly 1% of FPS.",pcmasterrace,2026-02-25 07:15:50,1
AMD,o77kret,https://preview.redd.it/5fuqlznheilg1.jpeg?width=800&format=pjpg&auto=webp&s=38148e68ae0764796cdc1a251ec65a6fbf0dd1c2,pcmasterrace,2026-02-24 21:19:17,7
AMD,o776i32,![gif](giphy|pHb82xtBPfqEg),pcmasterrace,2026-02-24 20:13:13,1
AMD,o75u1kv,"I meant AM4, which allowed me to jump from a Ryzen 7 2700X to a 9 5900X. Is that AM5?",pcmasterrace,2026-02-24 16:34:43,32
AMD,o7a32jo,No they arenâ€™t.  The â€œsourceâ€ for that was some random customer support rep who didnâ€™t say something properly.  Media outlets took it and ran with it because it works as excellent engagement bait for redditors who donâ€™t read the articles.,pcmasterrace,2026-02-25 05:52:26,1
AMD,o77z2na,I got the Asrock Challenged edition lol,pcmasterrace,2026-02-24 22:26:34,1
AMD,o78eorq,![gif](giphy|WmKrOMrTFFhPW),pcmasterrace,2026-02-24 23:48:47,1
AMD,o75ilgg,"i sure hope sal altman and all his ai nonsense fall, miserably",pcmasterrace,2026-02-24 15:43:06,31
AMD,o77rsmv,"Yeah that market! :) I dunno ps5 sales are down compared to ps4 (and given the expectation of â€œgrowthâ€ in these things, over that timeframe, to be _down_ is significant), and the main reason people point to is the higher price point. Letâ€™s see what happens with the steam machine and its dramatically higher price point, thatâ€™s probably what will settle this.",pcmasterrace,2026-02-24 21:51:40,1
AMD,o75uebj,"They did, AMD signed a contract for $30 billion.",pcmasterrace,2026-02-24 16:36:19,3
AMD,o77t3mg,AMD could absolutely raise the tray price of Strix products.,pcmasterrace,2026-02-24 21:57:43,1
AMD,o779rpz,![gif](giphy|EaEWuES5SDSpcnOlRt),pcmasterrace,2026-02-24 20:28:32,7
AMD,o7afnk6,"> When you fill up your car, do you specifically enter the number of liters or gallons of gas that you want? No you just know that you have a 30 gallon tank and that you want to fill it up  I don't, but the gas station sure does. The refinery isn't selling ""eh idk fill me up"" amounts. They are selling exact amounts. And that's still a terrible analogy because you can't buy 100W of a GPU. You buy 1 GPU. A set amount of GPUs has to be loaded into a set amount of servers and loaded into a set amount of racks and hooked up to a set number of circuits. AMD is loading a set number of GPUs onto a truck. They're not just pouring GPUs into a tank until it fills up the gigawatts.",pcmasterrace,2026-02-25 07:39:42,0
AMD,o75eqhk,"AMD does most of the work on amdgpu, there have also been contributions from valve and the linux community",pcmasterrace,2026-02-24 15:25:09,13
AMD,o76qi8f,"AMD does the hardware level stuff but the software stuff like Vulkan is a joint effort between the community, Valve, Red Hat and AMD.",pcmasterrace,2026-02-24 18:59:43,1
AMD,o75vgkr,6800xt constantly crashes and I have to disable and enable it through device manager every time.  The hotkey to restart the driver doesnâ€™t work.  Itâ€™s ridiculous.,pcmasterrace,2026-02-24 16:41:02,6
AMD,o760egs,Same. I have a 7900XTX that would randomly glitch out and then cause the system to reboot on Windows. It's super stable on Linux.  But that had only made me uninstall windows from that PC tho. Now I mostly run Linux because of that.,pcmasterrace,2026-02-24 17:03:02,2
AMD,o76tgbp,"This is what makes me hesitant to buy an AMD gpu. Right now my 3060Ti is starting to show artifacts when playing games, it might kick the bucket soon. I'm torn between a 9060XT 8gb and rtx 5060.   From what I've seen the 9060XT is better than the 5060 in almost every way, except upscaling, DLSS is much better. It has x16 lane so it won't be gimped by PCIE 3.0 and 4.0, it has slightly better performance than the 5060, a bit cheaper too. Only thing making me hesitant to buy it is the driver issues.   Before buying the 3060Ti that I have now, I was using a RX 470, and it was just nonstop problems on windows 10, driver timeouts, Adrenaline software deleting itself when i wake my pc up from sleep. Dogshit windows 10 updating the drivers automatically, causing crashes and imcompatibility issues. It was a nightmare to deal with. Looking up solutions online gives you dozens of solutions with none of them fully fixing it.  I don't wanna relive that experience.   My experience with 3060Ti was smooth sailing in comparison. I do think it's mostly a Windows problem, since I had little to no problems with AMD drivers back when I was still using Windows 7. I used 7 for damn near a decade, and didn't have any amd driver issues all those years. It's only after installing windows 10 that the problems began.",pcmasterrace,2026-02-24 19:13:03,1
AMD,o75jkm2,"You're talking like a conspiracy theorist.  The existing 6000-series GPUs continue to function just as they always have, even if they don't have the ""latest features.""  Who could have imagined that a certain generation of hardware wouldn't support the newest hardware?",pcmasterrace,2026-02-24 15:47:35,-4
AMD,o75e32w,They would I used to work there and there were plans to shelve RDNA4 when RDNA5 launches.,pcmasterrace,2026-02-24 15:22:06,3
AMD,o761i67,"Linux users would be feasting tho. Linux often get cool stuff that isn't going to official drivers. For example, a group of eggheads managed to implement RT support on Navi1, Vega and Polaris cards by repurposing their compute units. So on Linux games that requires RT can run on these cards as if they supported RT all along. This is not going to happen on windows because AMD and Microsoft would rather you buy a new PC and a new GPU. Same eggheads also managed to get ReBAR working on Polaris and Vega cards when the official stand is these cards don't support ReBAR, granting them a slight performance boost over windows.  I won't be surprised if someone manages to implement a version of FSR4 for pre-navi4 cards in time.",pcmasterrace,2026-02-24 17:08:04,1
AMD,o75dpk1,i have 7900xtx not 7900xt,pcmasterrace,2026-02-24 15:20:20,-1
AMD,o76tg1x,Ah...got ya.,pcmasterrace,2026-02-24 19:13:00,1
AMD,o7eijkf,Yeah and? People using Linux just to be different and them having some weird hate for windows is dumb lol.,pcmasterrace,2026-02-25 21:39:59,1
AMD,o76hxs6,"I know itâ€™s not the done thing to talk about here, but I have a handheld android device with a snapdragon 8 Gen 2 CPU. Iâ€™m amazed the games which will run well on it.",pcmasterrace,2026-02-24 18:21:56,7
AMD,o796xqk,Msrp is completely irrelevant as evidently seen.,pcmasterrace,2026-02-25 02:26:05,2
AMD,o75tq9i,"""We wanna give every radeon gamer the best experience"" bullshit  https://www.amd.com/en/blogs/2025/continued-support-for-every-radeon-gamer.html",pcmasterrace,2026-02-24 16:33:19,1
AMD,o76wp5n,Which version did you revert to? I've been getting random crashes to desktop playing Space Marine 2 :(,pcmasterrace,2026-02-24 19:27:52,4
AMD,o764k4w,Not to mention that amd noise suppression just hasn't turned on in very versions of the drivers,pcmasterrace,2026-02-24 17:21:57,5
AMD,o771fk2,"No issues on XTX so far, in fact one random crash got fixed for me. Unlucky I guess",pcmasterrace,2026-02-24 19:49:42,1
AMD,o76eh0y,"Works on 6000 series too, using it in a game as I'm writing this.",pcmasterrace,2026-02-24 18:06:39,35
AMD,o7buqnd,You can literally use Optiscaler to inject it into games and enable it that way.,pcmasterrace,2026-02-25 14:14:27,1
AMD,o77lva4,"The performance of FSR 4 on RDNA 3 chips isn't as good, which is the reason they haven't released it for RDNA products.",pcmasterrace,2026-02-24 21:24:22,-2
AMD,o76fjlc,It works but youre not officially getting the newest features even though its totally feasible,pcmasterrace,2026-02-24 18:11:22,32
AMD,o77lowk,"They are trying to argue that because AMD didn't release FSR 4 for RDNA 3 chips, which they never promised they would and the modders have found out the performance of it isn't great on RDNA 3, that AMD has ""abandoned"" those GPUs.   It's crazy and I'm thinking that person is just a fanboy.",pcmasterrace,2026-02-24 21:23:34,4
AMD,o76xlhw,"depends on which group of bots is online currently and what trigger words you hit, I guess",pcmasterrace,2026-02-24 19:32:02,-1
AMD,o76vucb,Defeatism looks bad on anyone.  You dont think a boutique graphics card company wouldnt get built in to fill the gap?  Just doing a 5 second search i was ableto find this new startup  https://bolt.graphics,pcmasterrace,2026-02-24 19:23:57,-6
AMD,o77mw9w,"Im in thr camp that ill go without instead of continue imho a stupid cycle.  Both these people leave, then i keep my 3060ti and ride it into the sunset.",pcmasterrace,2026-02-24 21:29:03,1
AMD,o78z9ly,"Nah, its going to slow down people upgrading and you will see people optimizing current builds for new games. We will get a cycle of devs not chasing graphics. I'm interested.",pcmasterrace,2026-02-25 01:42:26,1
AMD,o75wwli,It's the chip for all handhelds. Like legion go and ally,pcmasterrace,2026-02-24 16:47:28,18
AMD,o78er7a,"Okay, now we just need .16 of a gigawatt",pcmasterrace,2026-02-24 23:49:10,1
AMD,o75om84,"It's easy to point to AI when talking about technology prices. Honestly, I feel like it's something way more pervasive and insidious.   COVID turned out to be a catalyst for the plutocrats of the world to seize their opportunity to further consolidate wealth and power. Despite the increasing ability to leverage globalization to benefit all of humanity, literally every facet of life has become increasingly expensive or unobtainable. I'm disheartened by the fact that everything appears to be a money filter to stuff the coffers of the wealthy elite. Prices never ""return to normal"" like they have in the past. Is it sustainable? Probably not. How much longer are people going to suffer just to provide basic necessities? Within this context, It's a luxury to be in this sub complaining about technology prices. We're going to lose a lot more before things come crashing down, but not before the top have solidified their taking. My only hope is that someday they'll be forced to pay back their share for the greater good.   Sorry about the rant. Like most people, I'm starting to feel the pressure and I'm getting extremely pissed off.",pcmasterrace,2026-02-24 16:10:25,39
AMD,o75j7sm,For sure.,pcmasterrace,2026-02-24 15:45:57,1
AMD,o77wduh,i forgot about the steam machine! and yes it will settle this,pcmasterrace,2026-02-24 22:13:21,1
AMD,o75uunm,Was that confirmed to be for all the ps6 SOCs? Thatâ€™s great if so. Then a delay is probably entirely contingent on memory pricing. Especially if the plan was 32 gigs or somewhere around there.,pcmasterrace,2026-02-24 16:38:20,2
AMD,o77tthm,"They could, yes.  But I doubt they will.  They're already absurdly expensive.",pcmasterrace,2026-02-24 22:01:03,1
AMD,o75hokg,Sure but amdgpu is the kernel driver not what 99% of people care about for playing video games. The Vulkan userspace driver RADV is mostly a Valve show at this point with very little input from AMD.,pcmasterrace,2026-02-24 15:38:55,19
AMD,o76uhx0,"6800xt never crashed on me, I had it until recently switched to 7900xtx. never restarted drivers with any hotkey too, installing drivers, done properly should be done by removing old ones with driver removing software such as ddu in safe mode, and installing new drivers a fresh.  if you had crashes, chances are the drivers for your previous gpu were still installed and they caused the problems. of course reinstalling the OS accidently fixed that problem.",pcmasterrace,2026-02-24 19:17:50,0
AMD,o76x4mu,"don't buy an 8gb card, buy the 16gb variant at least   AMD has better driver support on both windows and linux and offers same quality for lower price. It's just better. You can find some anecdotal reports of amd drivers not working, but nvidia drivers break far more often on average, just check the respective subs for what the people are talking about.",pcmasterrace,2026-02-24 19:29:51,0
AMD,o75jy6r,That's why it said they TRIED. They backtracked pretty damn fast after the backslash.Â    https://youtu.be/-PQ7G7l4KAk,pcmasterrace,2026-02-24 15:49:17,9
AMD,o75eqkp,"https://preview.redd.it/fib6djzanglg1.jpeg?width=960&format=pjpg&auto=webp&s=7da5ca19a96c3f948117284f4572ed8daba58628  Still not seeing the big difference, not enough to call it high end.",pcmasterrace,2026-02-24 15:25:10,5
AMD,o76jpxr,"Qualcomm has SDX chips that are for laptops, they have middling performance at best. If they can make real leaps and enter as a real alternative to x86 land then we might see something shift",pcmasterrace,2026-02-24 18:29:48,7
AMD,o7d4fmp,Well AMD is also irrelevant at the prices they sell at regardless of MSRP anyways.,pcmasterrace,2026-02-25 17:49:47,0
AMD,o78l9vn,I haven't been able to use anything past 25.9.2 with my 7900xtx,pcmasterrace,2026-02-25 00:25:03,3
AMD,o79l4sk,"i had similar problems on my 6900xt. if youâ€™re all out out options, maybe trying lowering your RAM speed. I turned mine down from 6000hz to 5800hz and games stopped crashing",pcmasterrace,2026-02-25 03:47:33,3
AMD,o779v50,25.1.1 is what i use. it works well for me.,pcmasterrace,2026-02-24 20:28:58,2
AMD,o76yxlu,How can I do it?,pcmasterrace,2026-02-24 19:38:15,2
AMD,o77q0o1,"The 6800 xt, 6900 xt amd 6950 xt are all still very relevant GPU's.",pcmasterrace,2026-02-24 21:43:23,1
AMD,o78702b,"Performance isnt as good as the 9000 series sure. But the upscaling from FSR3 to 4 is far superior and worth the performance difference. Plus FSR4 is still more performant than native so your point is moot. The customer should have the choice and all AMD needs to do is add a disclaimer that FSR4 runs better on 9000 series or call it a ""beta"" feature for 6000 and 7000 series.",pcmasterrace,2026-02-24 23:07:02,2
AMD,o77y5vm,I literally could not care any less for new fsr.  I have a 24Gb vram card that plays the hell out of 4k on my 4k monitor and runs like a dream.,pcmasterrace,2026-02-24 22:22:06,4
AMD,o76xhfk,"you are getting newest features, except fsr. fsr is just one feature.",pcmasterrace,2026-02-24 19:31:30,-16
AMD,o78d712,"Bro the performance gain is very reasonable, performance fsr 4 looks a lot better than ultra quality fsr 3 lol while still performing the same if not very slightly better. I know this because I tested it myself this past weekend on avatar frontiers of pandora, cyberpunk, and a bunch of other games.",pcmasterrace,2026-02-24 23:40:22,1
AMD,o76y162,"You do realise their claims have yet to be proven, right? You do realise they have yet to have a single physical chip produced and is literal vaporware.  And the question is, which fab is manufacturing this supposed ""most powerful chip""? Notice how this isn't mentioned anywhere?",pcmasterrace,2026-02-24 19:34:02,6
AMD,o77ne5x,Dumb name for a company imo,pcmasterrace,2026-02-24 21:31:18,2
AMD,o778464,You are totally wrong!  Defeatism is not me by any means.  I only told you what I feel is the truth.  And you take the opportunity to verbally abuse me.  I could verbally assault you in ways you would not recognize but that would not serve any useful purpose.  If you want to believe that some new company is going to rise out of the ashes left by the companyâ€™s that have abandoned you well I have a bridge for you itâ€™s only a couple of years old and it will provide you with passive income for the rest of your life.  Let me know if you are interested in purchasing it I will finance it for you at very reasonable % rate.  Have a great day and donâ€™t lose that positive attitude it will carry you far.,pcmasterrace,2026-02-24 20:20:45,0
AMD,o75x2um,Oh i see. Didn't follow mobile chips,pcmasterrace,2026-02-24 16:48:13,7
AMD,o773bgj,Wait what? It's not the chip for the Steamdeck which is a pretty huge one to leave out.,pcmasterrace,2026-02-24 19:58:24,3
AMD,o75ppq1,no no dont be sorry thank you for your insightful response. it certainly is a luxury to rant about tech taking into account the reality of the daily life problems you're totally right and i share your view too.  i have no hope life is hard enough and will never become easier again sure we can (try to) power through but whats the point? im disapointed with our species as a whole gaming and tech is just the last straw. pd:sorry if my english is not perfect im not a native speaker,pcmasterrace,2026-02-24 16:15:23,14
AMD,o76pnyt,"they wont be forced by anyone or anything, not a single one of them have ever paid back for all the wars they have started...",pcmasterrace,2026-02-24 18:56:00,2
AMD,o797rc2,They will only be forced to if we make them.,pcmasterrace,2026-02-25 02:30:43,2
AMD,o76cbq2,Tge issue with delsying the ps6 is that the hardware gets more outdated the longer they sit on it while oroduction costs and R&D costs stay the same. Meaning its value per dollar gets worse tge longer they dekay it.  Meanwhile nintendo's going to take full advantage of this current predicament witb tbe wombo combo of being the cheaoest system in the market + running current gen ganes witg a bit of optimization + being on a somewhat older samsung node with low demand.,pcmasterrace,2026-02-24 17:57:09,3
AMD,o76kq5y,"It was confirmed, the deal was signed in 2024. If the PlayStation 6 is releasing by 2030 its design is effectively already finalized.",pcmasterrace,2026-02-24 18:34:17,1
AMD,o77ucyj,"Okay, well AMD canâ€™t raise prices on the PS6 chip as they are under contract for $30 billion dollars.",pcmasterrace,2026-02-24 22:03:38,1
AMD,o75jn5b,"the kernel driver is a critical part of the stack though, mesa wonâ€™t give you what you want without it  AMD is actually a heavy contributor to RADV again as of last year since discontinuing AMDVLK. Valve and Red Hat do also contribute significantly",pcmasterrace,2026-02-24 15:47:54,9
AMD,o76uzsm,Never had to DDU anything with my nvidia cards. Went from 570 to a 680 to a 980ti and all of them never had no safe mode no special boot crap no wipe nothing.  Iâ€™ve used DDU like a million times on this machine and the crap doesnâ€™t work.,pcmasterrace,2026-02-24 19:20:05,3
AMD,o77hozw,"16gb cards are out the question for me unfortunately since prices have skyrocketed. A 9060XT 16gb costs about $550-$570 now, almost the price of a 9070XT at MSRP, the 9070XT is now around $770-$800+. It's just dogshit value at the moment. The cheapest 9060XT 16gb i can find is used and it costs $462, which is still an awful price, way above msrp.  The 8gb models haven't gone up in price as much, cheapest is about $308 -$330. I would love a 16gb gpu but it's just a really bad purchase right now. I know 8gb is shit too, but the extra cost for 16gb is way too high.",pcmasterrace,2026-02-24 21:05:21,1
AMD,o75fq4n,"it was for its generation, they didn't make any high end card for 9000 generation at all",pcmasterrace,2026-02-24 15:29:47,2
AMD,o76nfhr,"Itâ€™s not about how good they are relative to what we are used to, which is to say a decent CPU and GPU combo. It is about a viable alternative that is affordable and allows people to play their games.",pcmasterrace,2026-02-24 18:46:09,0
AMD,o7d4qhv,Okay? I fail to see why I would care about that lol,pcmasterrace,2026-02-25 17:51:08,1
AMD,o79wvg2,I've had no issues with any updates recently,pcmasterrace,2026-02-25 05:05:52,3
AMD,o7719y7,Optiscaler.,pcmasterrace,2026-02-24 19:49:00,6
AMD,o771mdz,"Just put the right dll into the game folder, replacing the older FSR dll. Not sure if I can link it here.  Though, I think on Windows it also requires you to use an older driver version. No such problem on Linux.",pcmasterrace,2026-02-24 19:50:33,1
AMD,o773iry,What features? FSR Redstone doesn't support any card below the 9000 series. FSR 4 officially does not either. AMD have already said that pre RDNA 3 cards are no longer getting any features going forward and have also said on record that there is no current plan to support Redstone or FSR 4 on any card below the 9000 series.,pcmasterrace,2026-02-24 19:59:20,10
AMD,o7cmteb,"When modders first got it working and I checked out the performance it was a lot worse on the RDNA 3 chips than the RDNA 4.   If that's changed that is interesting, of course I prefer native where possible and most of the games I play these days don't require anywhere near the amount of gaming hardware I have anyway.",pcmasterrace,2026-02-25 16:29:22,1
AMD,o77mf7u,Im not arguing for that company. Im saying that if amd and nvidia move out it would bestupid to not think someone else wouldnt try and step in,pcmasterrace,2026-02-24 21:26:53,0
AMD,o77mk26,I didnt read all that past the first sentence. I hope your day gets better.,pcmasterrace,2026-02-24 21:27:31,-1
AMD,o77vzcx,No the steam deck will keep getting driver updates luckily. All other chips on windows are fuuuuuuucked,pcmasterrace,2026-02-24 22:11:25,1
AMD,o75sb4l,"thanks for the good additions guys. this issue is for the informed to discover. and this time they made the mistake of messing with savvy technophiles who also at least in part know a bit about financials.  the ""money"" that they want us slaves to use is debt based that can be created out of thin air with a simple promise to repay. these people/persons(corporations) have no intent to repay this and it is directly effecting us now.  in our opinion there is no reasonable explanation for the borrowing/capex expenditure that results in higher consumer electronics prices for(the trade) data centre expansion other than to produce a panopticon which is meant to enslave us. ai is not the answer unless it provides good honest governance which cannot be provided by corrupt politicians and in that same reality does not require the same kind of expenditure as we are now seeing.",pcmasterrace,2026-02-24 16:27:02,7
AMD,o798heh,"I'm going to power through because it's the human condition. I can only cling on to the very values that I try to instill in my own children and with those I have the pleasure of interacting with...  I still believe people are innately good.  I also believe that if you can do more to make this world better, then you're obligated to do so. I'm just trying to figure out what that means for me in moments like this.",pcmasterrace,2026-02-25 02:34:44,3
AMD,o798lo5,I intend to do what I can.,pcmasterrace,2026-02-25 02:35:23,2
AMD,o76d9x6,"Sure, but I donâ€™t think it matters. Realistically, out of date hardware has always been part of the console equation. Because first of all they are using AMD and Nvidia are generally delivering the cutting edge in gaming. Secondly, a console is expected to last for seven years or whatever so the technology will be updated at the end if that anyway. Thirdly PC gaming seems to not be getting much cheaper. I think itâ€™s unlikely that we will see the PS6 sell for less than thousand dollars. Especially if they have to release it anytime soon. If they are able to wait a while, then maybe they can push the prices down because it will be an old process node at that point with an old architecture which is exactly what Nintendo has done with the switch 2.",pcmasterrace,2026-02-24 18:01:21,4
AMD,o76kzja,"I feel like that makes sense, honestly. Doing the Nintendo thing and just releasing quite a while after tape out when the architecture has assured and prices have gone down is not a bad idea especially if they want to target anything below $1000",pcmasterrace,2026-02-24 18:35:27,1
AMD,o75kvtd,I'm very aware it's an important part of the chain but the reality is that gaming performance and the side of the coin that this community cares about is pretty much driven by Proton & RADV.  I'd be interested to see the stats on RADV commits from AMD staff before and after they shuttered AMDVLK. As someone who checks the commit tree pretty regularly I haven't seen much activity from them at all.,pcmasterrace,2026-02-24 15:53:30,1
AMD,o76vskt,"I had to ddu when I installed my current gpu, my computer didn't detect it even though it was connected. Imagine my shock when I booted the safe mode and looked at it and it turned out that despite uninstalling and updating drivers through the app in normal mode, I still had not only the drivers for 6800xt installed, but also 6650xt, which I have used for a while too.  After properly cleaning them and installing 7900xtx drivers, it worked great and never glitched on me.",pcmasterrace,2026-02-24 19:23:44,1
AMD,o78ieth,What about second-hand? 6800XT or something from the 7000 series?,pcmasterrace,2026-02-25 00:09:32,1
AMD,o76pvlb,"that includes cloud computing lmfao, which sidesteps my entire initial point. processor architecture differences being what they are and causing some to fall behind in terms of efficiency, ease of use, and compatibility means we need them to fall in line with or exceed x86 otherwise they aren't performant.",pcmasterrace,2026-02-24 18:56:55,4
AMD,o7d5wlv,No one asked you to care about that anyways. At least now I know who the peasants supporting such companies are.,pcmasterrace,2026-02-25 17:56:19,0
AMD,o7a1r44,Yeah it's just The Finals I have major issues with in newer updates. I may have had problems with BF6 as well but I don't recall atm,pcmasterrace,2026-02-25 05:42:12,1
AMD,o7751qm,https://preview.redd.it/zso7ukfh1ilg1.png?width=1121&format=png&auto=webp&s=15329226477a50061152d49e792ac3c0d8a49f1d  like these,pcmasterrace,2026-02-24 20:06:25,-5
AMD,o77wlke,There is nobody else. Intel's GPUs are still two tiers below the amount of silicon suggests and Chinese GPUs are competing with cards from 6+ years ago while having terrible drivers.,pcmasterrace,2026-02-24 22:14:25,3
AMD,o788od2,My day is going fine.  I just did what you did and took a cheap shot at you just like you did to me today.  Itâ€™s a shame you didnâ€™t read the whole thing as the end is the best part.  I have a feeling that you are a child that got lose on your parents computer.  I am so done with you please do me a favor and stay off your parents computer.,pcmasterrace,2026-02-24 23:15:51,2
AMD,o78mm79,"Uh, aren't there all of the chips prior to the Z1 Extreme? There were all the Z series chips that were also used in the handhelds prior to Z1 Extreme that just used off the shelf laptop chips. Those are also still getting updates.  So it's literally just a narrow band of a couple of handhelds losing driver support. It's not even most of the gaming handhelds that have ever released.",pcmasterrace,2026-02-25 00:32:08,3
AMD,o76h9k4,"The msin problem i see with this is specifically the competition, the switch 2. Sony's going to lose a LOT of market share the longer tgey delay the ps6 since most devs realising that they can just port their games there too. Tge ps6 would rewiden tbe tech gap. RN there isnt really one. Delaying the ps6 means devs will target switch 2 level fir that much longer, and delays next gen games that much longer. It also means they'd be giving more time for nintendo to catch up on the piwer level for whatever thir next device is too",pcmasterrace,2026-02-24 18:18:57,1
AMD,o76mj02,"They also may want to wait for lithography changes, a console is usually targeting around 200W at most so the longer they wait, the more performance they can fit into 200w. PS5 Pro is already built on 4nm so they may want to wait for 2nm since the jump from 4nm to 3nm may not be large enough.",pcmasterrace,2026-02-24 18:42:13,1
AMD,o79f351,"Prices for those are crappy too, but even if it's good, I refuse to buy it since AMD has shown they're effectively abandoning those 2 generations of gpus.  They refuse to include FSR 4 support, they tried to end driver updates for the 6000 series. It's just not a good buy right now. I'd rather buy a shitty RTX 4060 over any of those AMD cards, despite the subpar performance. At least, I'd be assured that Nvidia will continue providing driver updates and support for a good amount of years. Can't say the same about AMD.",pcmasterrace,2026-02-25 03:11:34,1
AMD,o78cc48,Performant at the high end but most donâ€™t require high end.,pcmasterrace,2026-02-24 23:35:37,1
AMD,o7d6p2q,"I'm not supporting a single company in this world, what are you even on about dude   I'm just pointing out that nvidia gpus sell for 50% above msrp and that it therefore doesn't matter in the slightest what kinda magic msrp number they shit out.  Why are some of you guys so weird..",pcmasterrace,2026-02-25 17:59:48,1
AMD,o777u8q,"None of these are ""new"" or ""newest features""",pcmasterrace,2026-02-24 20:19:28,9
AMD,o7cbz11,Welp i guess we are properly fucked then. Your world view is sad.,pcmasterrace,2026-02-25 15:39:49,1
AMD,o79cjod,"Yeah that's why it's so damn weird. They dropped driver support for z1E. It's insulting you buy a 500+ machine and in 3 years there's no more support. But the older ones are still being used in laptops, or rebranded as newer ai chips.",pcmasterrace,2026-02-25 02:57:08,2
AMD,o76ipdf,"I just donâ€™t see that. The switch 2 is still much weaker than the PS5. If players are mostly using the performance mode on the current generation consoles, they will not be happy having to move back to 30 FPS on the switch 2 mostly for big titles. Most devs can port games there but they are compromised.  Also, it doesnâ€™t really change anything is even if they release the PS6 it will likely be much more expensive, especially if they have to release it in the current economy.   The target will likely keep being the PS5 and for the low end the switch 2. Thatâ€™s not going to change even if theyâ€™ve released the PS5 because people wonâ€™t have the money to buy a new $800+ console.   Sony releasing PS6 with current ram prices thatâ€™s supposed to be roughly 5070 TI level is going to be even with mass production discounts and an aggressive subsidy well north of $1000.   They wonâ€™t be grabbing any market share realistically with that either.",pcmasterrace,2026-02-24 18:25:18,3
AMD,o785znm,"Even if they released the PS6 tomorrow I doubt we'd see many PS6 exclusive games. Devs would continue targeting the PS5 for years to come, like they did with the PS4 in this generation.",pcmasterrace,2026-02-24 23:01:39,2
AMD,o76okl7,I think thatâ€™s basically not going to happen. Because I just donâ€™t think thereâ€™s money for it. These newer chip architectures are super expensive. I cannot see any universe where a cutting edge chip is going into one of these gaming consoles maybe with the exception of whatever Microsoft is cooking up because clearly Microsoft has kind of moved away from being price conscious.,pcmasterrace,2026-02-24 18:51:08,1
AMD,o7ambfi,"Have to say I'm also disappointed with the short driver support. On the other hand, it's not like the old drivers won't work anymore, right? We're only missing out on some game optimization, I think? Still no excuse.  Advocate of the devil (because for me, nvidia is still the larger evil): why do you even want AMD if their support is so short?",pcmasterrace,2026-02-25 08:41:33,2
AMD,o77kh4y,Anyone who has used a handheld (such as a steam deck or rog ally) could tell you that they are able to run more games than most people would expect. If you don't need high resolutions or are willing to compromise on settings then depending on the game you can absolutely have a decent experience.,pcmasterrace,2026-02-24 21:18:00,1275
AMD,o77lhcc,"Try overclocking your RAM! The Ryzen 8000 iGPUs (all iGPUs, really) are heavily bandwidth-limited by the RAM and if you can increase your memory frequency by 30% you might see a near-30% performance increase in some tests.  The Ryzen 8000 memory controller is also fabricated on much better silicon than the 7000/9000 IOD, so you can hit much higher frequencies much more easily. 10GT/s should be achievable on good sticks. I'm always irritated when I see 8700Gs benchmarked on DDR5-4800 when they can literally handle double that. Not to mention that the Infinity Fabric can go significantly faster too, I've heard 2400MT/s is possible. Timings arn't very important, it's mostly a bandwidth thing.  The 8700G is too expensive to be worthwhile, but yeah, the 8600G is a little slept on IMO. Having only 16MB L3 does make gaming kinda stuttery, and it doesn't boost very high because it's mobile silicon.  Note: the bandwidth limitations of iGPUs is exactly why Intel used to have ~128MB of L4 cache on some mobile chips (and the i7-5775C), which would catch a lot of the iGPU bandwidth. It required a massive cache controller, though. IIRC Apple M-series chips share the main L3 with the iGPU, which helps in the same way.",pcmasterrace,2026-02-24 21:22:36,283
AMD,o77k63d,The AMD Radeon 890M igpu is a beast for what it is.,pcmasterrace,2026-02-24 21:16:38,164
AMD,o77ly2s,"To be fair, I think iGPUs get a bad rap because you most often encounter them in laptops where you're extremely power constrained. On desktop chips, things really start to open up and they're decent for midrange gaming...as long as you're not running heavy UE5 titles that is.",pcmasterrace,2026-02-24 21:24:43,98
AMD,o77mooi,"Sure but mind you Forza is really optimized, but I get your point I also think it's nice that they gave very decent performance",pcmasterrace,2026-02-24 21:28:06,30
AMD,o77od5s,"I've always had an appreciation for iGPUs and low power dGPUs.  My favourite GPU ever was the 1050Ti 4GB because it was a 75W card that rocked at 1080p for most games.  My favourite iGPU was the one built-in to the Haswell series from Intel. I played a lot of Minecraft and Fallout 3 on my Lenovo ThinkPad with Haswell CPU. It ran Fallout 3 much, much faster than when I played it originally on Xbox 360, which is still a great little console even today.",pcmasterrace,2026-02-24 21:35:44,24
AMD,o77l7oe,"My wife's pc has a 5700G in it and she plays everything she wants at 1080p. I told 3 different coworkers to buy pcs for their kids for Christmas with iGPUs and every one of those kids is able to play the games they want.   People on reddit just love to think you need the max stats to play anything.   Its like comparing a 130hp Ford focus to a 700hp Ford mustang. Both of them will get you back and forth to work, but some people just love to spend money on overkill.",pcmasterrace,2026-02-24 21:21:21,94
AMD,o77oq0n,Testing a GPU performance on Forza Horizon is cheating,pcmasterrace,2026-02-24 21:37:22,37
AMD,o77n75p,"On a sub like this you have mostly enthusiasts, so it's not surprising really that there is not much talk of low end hardware. You're right though, iGPUs are very impressive these days when you consider that they're basically free.",pcmasterrace,2026-02-24 21:30:26,14
AMD,o77w852,Forza horizon 5 is really not a demanding game. It's actually more of a testament how playground games make that series good looking and optimized. Run any modern benchmark game and you'll see the igpu limitations. However it is still servicable and the fact that it can rival last gen consoles is a feat on its own,pcmasterrace,2026-02-24 22:12:35,17
AMD,o77mu12,"The next generation is CPU GPU and RAM all integrated vertically on one chip, 2030 will be the year highend gaming PCs are the size of a raspberry pi or jetson nano.",pcmasterrace,2026-02-24 21:28:46,26
AMD,o77pd3m,"OP is LLM-generated, with the minor tweak of asking the LLM to make every first letter in each sentence lowercase. Report and move on.",pcmasterrace,2026-02-24 21:40:21,32
AMD,o77qde8,"I have an 8600G, too, but I can't use it because it was part of a bad batch with a defective iGPU.   As soon as I try to install the proprietary AMD drivers, Windows crashes and goes in a BSoD loop.   Once I'll change OS I might try it again if the Linux drivers of my GTX 1650 somehow won't work.",pcmasterrace,2026-02-24 21:45:02,6
AMD,o77mo95,"I have a Core Ultra 7 255H in my Gram Pro. Itâ€™s lighter than a macbook air, runs a 60w usb-c brick thatâ€™s basically a phone charger and can play Cyberpunk at 1080 50-60fps medium. Itâ€™s the ultimate travel laptop IMO if weight is your primary concern. I like it so much, I might buy the latest one with the 358H to have some extra power on tap for newer games and running more stuff at native resolution but yeah itâ€™s pretty insane what modern igpus are capable of.",pcmasterrace,2026-02-24 21:28:02,6
AMD,o77rola,mf just discovered igpuâ€™s,pcmasterrace,2026-02-24 21:51:08,8
AMD,o77xccv,"The new Intel ARC iGPUs are absolute monsters, i'd actually say that you could just buy an ultrabook with a Core X7 with an ARC B390 iGPU & get by for gaming. Even the AMD Radeon 660/760/860Ms are pretty solid too.  I honestly think that my next PC may just be a mini PC using an iGPU at the current rate of things",pcmasterrace,2026-02-24 22:18:03,9
AMD,o77lwii,"include the AI 395+ beast iGPU in this conversation, but even on my 780m I feel it  https://preview.redd.it/bmq8o9hmfilg1.png?width=1280&format=png&auto=webp&s=11cbc8a75162433fd288087e7bd41d72053f95e9",pcmasterrace,2026-02-24 21:24:32,3
AMD,o77o5cm,"I used to have a 780M, played a bunch of games at high settings fine. iGPUs nowadays are incredible.  Anyways problem with that is availability, laptop manufacturers using the Ryzen AI line will only go up to an AI 7 350 with an 860M which is even worse than its older counterpart the 780M on the 8845HS. Desktop I mean SFF cards fit almost everywhere so people still use GPUs. Fun thing laptop side is that Intel is competing good with their iGPUs like the 140T on the 255H which is available under 1.5K, for AMD who makes better ones you'll need to go with an AI 9 365 and better to get something good and they're on the more expensive side. There's also the 8060S but it's only on pretty expensive ROG ultrabooks that aren't great, it performs between a 4060m and 4070m but at that price (something 3K) just get a dedicated gpu in a better built laptop.",pcmasterrace,2026-02-24 21:34:44,7
AMD,o77rr1n,Went from a 2070 mobile to a 780m and I actually gained performance due to the 2070m always thermal throttling.Â    Igpus are the replacement for gaming laptops (minus the better gaming laptops of course.) Absolutely love my 780m.,pcmasterrace,2026-02-24 21:51:27,6
AMD,o7afy9e,People on this sub and techforums at large are absurdly delusional when it comes to hardware requirements. I bought a 4K monitor in 2016 and was happily gaming on it using a GeForce 970. Nowadays youâ€˜ll get screamed at for suggesting running a 1440p display with anything less than a 5090. Not to mention all the idiots recommending heavily overpowered machines for the most basic of workloads; itâ€˜s ridiculous.,pcmasterrace,2026-02-25 07:42:25,5
AMD,o77k8ej,"Because its all thanks to DDR5, which nobody who needs to rely on iGPUs can even afford.",pcmasterrace,2026-02-24 21:16:56,9
AMD,o780i20,An igpu is still the punchline bro. Dunno what youâ€™re smoking.,pcmasterrace,2026-02-24 22:33:44,2
AMD,o78t5ua,"If your expectations previously were set by 4 years old midrange GPUs then by all means, iGPUs can do wonders for you in the future.  The problem is that PC is an enthusiast platform, with a lot of enthusiast users, who moved beyond 1080p medium a decade ago. And so for them, iGPUs are still not solving anything unless they sell all their enthusiast hardware alongside the rest. They don't talk about them, because it offers nothing of value to them.",pcmasterrace,2026-02-25 01:07:28,2
AMD,o798y7k,"I saw reviews of those AI Max 395+ PCs and was amazed at how well they can game. I'm curious to see how iGPUs develop, especially since graphics card prices are through the roof right now.  I'm especially curious to see how well this might work with a Lossless Scaling dual-gpu setup, if you got a slightly more powerful but still cheap dGPU and paired it with the iGPU for either upscaling / frame generation",pcmasterrace,2026-02-25 02:37:19,2
AMD,o79he4v,I had a dedicated streaming PC last year that ran solely off a 7700X and jt had no issues. A great time for iGPUs for sure.,pcmasterrace,2026-02-25 03:25:07,2
AMD,o7a2x36,https://preview.redd.it/k5twd4erxklg1.jpeg?width=1320&format=pjpg&auto=webp&s=92a94877261d7c04f1056bdf41fdb31a9198ac23  I own a 4080 pc but my rog ally z1 is best purchase when i travel to my home country,pcmasterrace,2026-02-25 05:51:14,2
AMD,o7ap015,> $300 AMD mini PC  Where can you buy this 8600G PC for $300?,pcmasterrace,2026-02-25 09:06:43,2
AMD,o7b0box,">girlfriend using a Ryzen 8600G... expecting disaster and this thing just ran Forza Horizon 5 at 1080p medium without even blinking  Even my 7800X3D with its two compute units of RDNA 2 is surprisingly powerful. Enough to run Forza Horizon 5 at 720p if you really had to (there are benchmarks in one of my reddit posts). Most excellent for powering my 1080p second monitor with all of its browsers, Steam, Discord, and even the Windows GUI itself all while leaving my DEDICATED graphics card DEDICATED to a single game on a single monitor...  Meanwhile people just seemingly by default disable their CPU's graphics in BIOS throwing away this extra compute power. This is what really isn't talked about enough: the proper way to configure a multi display setup in Windows for gaming and I have the Reddit post to send you in the right direction if you have a CPU with graphics and use a second display",pcmasterrace,2026-02-25 10:51:08,2
AMD,o7ba89e,"They are, not to beat around the bush, overpriced. I bought an HX 370 laptop (HP Omnibook Ultra) without a dGPU and it cost more than one with a better dGPU.",pcmasterrace,2026-02-25 12:11:04,2
AMD,o7d5f6o,"https://preview.redd.it/00v19hjqiolg1.png?width=690&format=png&auto=webp&s=de6b7127236b6f4583e87e5062276cea23e384e3  I thought this was ironic maybe, or whatever, but i got a chuckle out of the placement.",pcmasterrace,2026-02-25 17:54:10,2
AMD,o77nitl,"iGPUs/APUs have been pretty solid even going back to the Ryzen 3200g, though they've greatly improved even since then because faster ram really helps them.   And you're right about those Minisforum and Beelink mini PCs, they are honestly amazing value. I recommend them all the time for people who just need a computer, even older family/friends who won't do any gaming at all. They're cheaper than the standard desktop tower at Best Buy or wherever, they're smaller, quieter, produce less heat and they're extremely capable. Plus they can game too, they're fantastic.",pcmasterrace,2026-02-24 21:31:53,1
AMD,o77oq79,I bought a mini pc on Amazon for my wife for like $300 and honestly I was surprised at what you could do with it. I have a ~1k prebuilt from 2021 and I've never had any issues playing anything I want with high graphics. The one thing I really couldn't do was 4k cyberpunk but even that was okay to a point. You dont have to spend all your money to play AAA anymore for sure,pcmasterrace,2026-02-24 21:37:23,1
AMD,o77pvpg,"I remember all i had forever was an old hp laptop that i donâ€™t even remember the cpu it had, but I could play any game from 2016 and before with pretty much max settings.",pcmasterrace,2026-02-24 21:42:45,1
AMD,o77qwe4,"I feel the same when saw minecraft running in a tablet surface 4, was like, OMG,  that used to be a heavy game and is running smooth in a windows tablet !",pcmasterrace,2026-02-24 21:47:30,1
AMD,o77r2k9,"To be fair, they're already amazing for a long time. Like, when the first Ryzen APUs appeared, the hype was unreal, especially with GPU prices as they were in 2017/2018 those things were the best choice for budget builds. Even the Athlon APUs kicked ass back then.",pcmasterrace,2026-02-24 21:48:17,1
AMD,o77rnc9,Yeah while waiting for a steam machine I got a 8745hs mini pc and itâ€™s running a lot of stuff really well even at 4k,pcmasterrace,2026-02-24 21:50:59,1
AMD,o77sqpg,You also got to remember that developers are now optimizing for them via handheld pcs as well as the tech being better now.,pcmasterrace,2026-02-24 21:56:02,1
AMD,o77t708,"Yep, included in most cpu reviews these days",pcmasterrace,2026-02-24 21:58:09,1
AMD,o77tsqx,"APUs are a great budget option, but they arenâ€™t for everyone or every game.  You also need to remember this is an enthusiast sub.",pcmasterrace,2026-02-24 22:00:58,1
AMD,o77tw04,"Yeah, in what 15-20 years dedicated graphics cards will be remnants of the past?   Your tower today will probably be the same size as an office buildings server rack in 50 years.",pcmasterrace,2026-02-24 22:01:23,1
AMD,o783nqk,Have you had any issues with the 760M drivers? I just built a mini pc and I'm getting artifacting when I enable the igpu through device manager.,pcmasterrace,2026-02-24 22:49:39,1
AMD,o785e0v,"I see these discussions on here and in a few podcasts and my problem is all the numbers and letters get legitimately very confusing. Sorry, I'm just not tuned into the ecosystem and naming conventions.  If I want fair 1080p performance with decent visuals and a price that isn't going to just end up rivaling a GPU, what number ranges and suffixes should I me looking for? I'm assuming AMD will be the bang-for-buck sweet spot but I'll listen to anything.  Would love to have a second machine to share with my wife. And the water cooler for my 2060 Super is making noises....",pcmasterrace,2026-02-24 22:58:34,1
AMD,o785heb,A friend was running star citizen and modded valheim acceptably on an igpu until i came over to visit and noticed that performance wasnt quite as good as it should be on the rtx4080 system i sold him. Of course he had the monitor plugged into the wrong port...,pcmasterrace,2026-02-24 22:59:03,1
AMD,o78638s,In 2013 I was stuck for weeks in a hotel for work and I played the entirety of Dishonored on a first gen Surface Pro hooked to a hotel TV with a controller.,pcmasterrace,2026-02-24 23:02:11,1
AMD,o786don,"I'm like 90% sure OP is an AI bot and if they are they're a pretty damn good one. This post reads like an advertisement and all of their comments are very similar   Search for ""author:(freecurfew_9)"" if you're curious  Text really seems like it's trying to emulate ""internet talk"" by being perfectly consistent in its usage of lowercase for almost everything but all the abbreviations, punctuation and brand names are absolutely flawless. They also replaced the em dash with the hyphen when writing their comments",pcmasterrace,2026-02-24 23:03:44,1
AMD,o786otx,"The Intel Ultra igpu is insanely good for processing video for my plex server. Iâ€™ve seen it transcode multiple 4K streams down to 1080p without a sweat.   I do have to add- if someone gives you access to their plex and has 4K content, try not to do that lol. Just stream in direct quality if you can.",pcmasterrace,2026-02-24 23:05:23,1
AMD,o787y6r,"I bought a Beelink SER8 for my father a year ago to replace an aging desktop losing Windows 10 support. As I was setting it up I gave a few games a  try just to see what the 780M was able to do. I was expecting 720p at low setting hitting 30 fpsâ€¦ However, much to my surprise it was hitting 60 fps on medium setting at 1080p. Itâ€™s ended up impressing me much more than I expected. Tech is neat these days!",pcmasterrace,2026-02-24 23:12:01,1
AMD,o788aqf,"2 points: First of forza 5 is good looking but not anywhere near comparable to modern AAA titles. Secondly, who thought Minecraft didnâ€™t run 5 years ago on apus? You may not realise this but that was when zen3 came out and the 5700g was goated already xD 15 years ago yeah, different story thoughâ€¦",pcmasterrace,2026-02-24 23:13:51,1
AMD,o78brn7,"Was rocking the 5700G and now a 8700G in those AsRock Desk mini things, 32GB or more ram, these things are flipping blazing fast!!  It wonâ€™t run your game on ultra, but nearly everything is simple just hella fine and playable.   Ye sure, upgrading to a 9060XT was very noticeable, but without we could do.",pcmasterrace,2026-02-24 23:32:29,1
AMD,o78bxyn,Because most people in the PCMR community aren't enthused at playing games at 1080p low/medium.  We're enthusiasts.,pcmasterrace,2026-02-24 23:33:27,1
AMD,o78e9o3,They have been really good for a few years now. But they only pair the highest end igpus with the highest end cpus meaning that they end up costing way too much for what you get.,pcmasterrace,2026-02-24 23:46:25,1
AMD,o78fh8c,"Remember a lot of these interest groups are not just filled with experts. There's people at all levels, including those who think anything less than a OC 5090 is shit for Terraria.",pcmasterrace,2026-02-24 23:53:12,1
AMD,o78gxqw,"iGPUs in MiniPCs are the best for gaming, GMKtec is one of the best, their MiniPCs with iGPUs are the way to go if you want a pre-built PC.",pcmasterrace,2026-02-25 00:01:19,1
AMD,o78jete,I have a small asus 13 inch laptop that has an nvidia card but an  AMD CPU with built in graphics... Games legitimately run way better on the amd integrated one. Seems odd but very cool. ðŸ˜…,pcmasterrace,2026-02-25 00:15:01,1
AMD,o78lpbd,"I like them for servers, and managing stuff like that on headless things.",pcmasterrace,2026-02-25 00:27:19,1
AMD,o78of9j,"Yeah, it's absolutely bonkers. Especially the new Intel Core Ultra 9 Series 3 CPUs with the Arc B390, like the X9 388H. I've been scouring the web trying to find some laptops with this chip, but it seems like the only people who can get their hands on it at the moment are Youtubers/Tech reviewers. Dawid Does Tech Stuff just recently made a video about it, and LTT made a video on the Panther Lake chips at CES.  I bought an MSI Prestige 13 with an Intel Core Ultra 7 258V a while back, and I was flabbergasted by how it handled several of my Steam games.  I was playing Wukong last night on my SFF Ryzen 8700G build (AsRock DeskMini X600W, Noctua cooler, 64GB DDR5 CL40 (SODIMMs), Corsair MP600 Pro XT), 1080P, low settings. Pretty smooth with 60-70 FPS. But the AMD Adrenaline software had me baffled. I can't figure out how tf to configure everything.  Are iGPUs friggin awesome? Hell yes! Are they going to replace discreet GPUs? Hell no.",pcmasterrace,2026-02-25 00:41:44,1
AMD,o78pnj0,"Just because you donâ€™t talk about doesnâ€™t mean no one else doesâ€¦and maybe not gauge it by who on â€œpcmasterraceâ€ talks about it  Didnâ€™t know this subreddit was the only outlet for goingâ€™s on  I have a 8600g in a Chopin pro, sitting on the kitchen table, built it, just sits there. Been sitting there for months  People who really know about PCs arenâ€™t in the â€œmaster raceâ€ posting about it",pcmasterrace,2026-02-25 00:48:16,1
AMD,o78sek5,"My cheap intel laptop that's around 5 years old consistently surprised me with what it could run on its integrated graphics. Of course, it's nothing too exciting-the likes of Skyrim or Fallout 3, but I didn't even pay 300 for it and I was able to play a surprising amount of stuff on it.",pcmasterrace,2026-02-25 01:03:15,1
AMD,o78tbsu,"I played most of Elden Ring on release on a Ryzen 3 3200g onboard. It wasnâ€™t a great experience but I was still able to play, it was incredible in that sense.",pcmasterrace,2026-02-25 01:08:23,1
AMD,o78uljt,"To be fair, fh5 is incredibly well optimized and incredibly old, but it is impressive how little juice you need to run that game well. I was able to get 55fps in 4k ultra settings with my b580 but decided that 70fps with high settings were better because it stuttered a bit in ultra.",pcmasterrace,2026-02-25 01:15:33,1
AMD,o78uxai,"I recently bought a relatively cheap laptop with an iGPU. Did upgrade the RAM from 8GB to 16GB, and surprisingly it is able to run Doom 2016 to an okay degree. Not great, but just about playable enough that you could actually play it if it's the best you got.",pcmasterrace,2026-02-25 01:17:25,1
AMD,o78whg1,"I dont need it at all so I havenâ€™t, but Iâ€™d love to give fsr4 (int8) a whirl on a 860m or 890m",pcmasterrace,2026-02-25 01:26:25,1
AMD,o78zpqm,I built my wife a PC with an 8700G and that thing is a little beast for sure!,pcmasterrace,2026-02-25 01:45:00,1
AMD,o792ox4,"It's genuinely impressive, yeah. Doesn't surprise me that iGPUs are at this point. Anybody here remember the 2200G and 2400G, and how big a deal those were lol?",pcmasterrace,2026-02-25 02:02:05,1
AMD,o794dfb,"since everyone is talking about ryzen, let me talk about intel.  intel IGPUs post meteor lake are actually pretty good, with them backporting a lot of developments from their new DGPU line. any meteor lake IGPU can run most games at low settings at least.",pcmasterrace,2026-02-25 02:11:40,1
AMD,o79qfqm,"I got a laptop with an AMD A10-4600m back in 2012 and it could run a bunch of games. Mainly got it for being a cheaper laptop mainly for classes but also being able to play games. Had Skyrim as the benchmark there at the time, ran it on high settings no problem.",pcmasterrace,2026-02-25 04:21:17,1
AMD,o79yzj5,"hey man i really don't sleep on them at all. as a matter of fact there are some really good use cases for them. it's just that when you are looking to spend $800-1000 dollars on a full system (inc monitors, peripherals, maybe a desk, and a windows key) that i would recommend them. anything above that (at least before the insane AI and tariff inflation we are currently seeing) i would always recommend something with at least a xx60 nvidia card or an x600 amd card in it.",pcmasterrace,2026-02-25 05:21:10,1
AMD,o7a3cgq,My kid is playing fine minecraft and btd6 on a 2400g igpu on a lenovo thinkpad minipc.,pcmasterrace,2026-02-25 05:54:36,1
AMD,o7a6npt,"This is why we need more computers with ryzen AI max (strix halo) AMD seems to have only released like 3 skus: 395, 392, 390. Unfortunately it might be a bit because they basically require a decent amount of soldered RAM. We all know how that market is currently.",pcmasterrace,2026-02-25 06:21:08,1
AMD,o7ajwt6,"Glad it runs it but Forza 5 is super optimized, is 5 years old, and 1080p at medium isnâ€™t that good (unless you go into it thinking iGPUs literally crash out of games on launch).  Theyâ€™re not talked about because their performance is unimpressive compared to dGPUs. That doesnâ€™t mean itâ€™s bad, just that there are better options to recommend.",pcmasterrace,2026-02-25 08:18:43,1
AMD,o7aqczp,"1060 6 gig here,  only the last year or two new games that I may want to play have had requirement above my hardware.",pcmasterrace,2026-02-25 09:19:35,1
AMD,o7baqac,I had a mid 2015 MacBook Pro in school and couldnâ€™t get a windows so I installed windows on the mac and removed macOS entirely. It was a 4th gen i7 with integrated graphics and nothing else .  I played cs valo gta 5 remnant minecraft hitman 1 and hitman 2 .  I mean I could run anything on that thing I would spend a whole day just on tweaking the settings I even undervolted the cpu on it. Most of the time it was 40 ish fps and 720p but my god was it fun. I played on it till the battery bulged during Covid. It also developed a few issues after the battery bulged so I had to retire it.  Upgraded to a 3070 after that but I still miss those days.,pcmasterrace,2026-02-25 12:14:41,1
AMD,o7bf478,"I remember I built my first PC in 2018, at the tail end of the first (maybe 2nd?) mining boom. Couldnâ€™t afford a dGPU so I got an R5 2400G with integrated Vega 11 graphics. The best iGPU at the time. I donâ€™t think it was quite as strong as what would eventually be the Steam deck, but it could play all the indie and older titles that I liked. LoL, Terraria, and L4D4 ran like a dream.  Now I have a laptop with a 4070 and a 7940HS. When I still gamed, the 780M in the 7940HS was often enough to play a lot of my older games just fine. Really remarkable.",pcmasterrace,2026-02-25 12:44:31,1
AMD,o7c51zb,"I meanâ€¦.its not really a new concept lol. What do you think game consoles do? They donâ€™t have a dedicated GPU, they are just APUs.   I mean yeah, technically speaking CPUs with integrated graphics arenâ€™t AS good, but they are technically the same idea. APUs just have a slight edge, probably always have tbh. I assume thereâ€™s just some architectural differences that allow the APU to have a bit more oomph when it comes to gaming specifically.   Then you have the steam deck and all these other handhelds emerging, essentially doing what youâ€™re talking about, but in the palm of your hand.    Totally get what you mean though, if you can sacrifice and go 1080p and be okay with it, you can definitely get much more affordable gaming now. Itâ€™s kind of crazy.   Idk, I still think consoles will had a slight edge though. Itâ€™s kind of a bummer Nvidia doesnâ€™t push out APUs in devices like how the switch two does, because DLSS would be incredible on a cheaper rig. I just donâ€™t think they want people to have easy access to itâ€¦.",pcmasterrace,2026-02-25 15:07:03,1
AMD,o7cr0ao,"I have a lot of early ryzen APUs, due to upgrading and making ""secondary"" PCs, like a mini ITX build for my HTPC.  While the vega 8 graphics on most of them won't run anything recent at 1080p medium/high, they still have potential when running older games, and are great for emulation, while being cheap and compact.  Of course, my GTX 1660 super and now my RX 6700XT are in a different category, but seeing integrated GPUs evolve to the point that they're capable of 1080p gaming is great, because it means that at one point I might not need a dedicated GPU anymore.",pcmasterrace,2026-02-25 16:48:16,1
AMD,o7djzty,The real issue is 8600G is not cheap while DDR5 pricing is also not helping.  And with a old RDNA3 uArch it will be out of support soon.,pcmasterrace,2026-02-25 18:59:11,1
AMD,o7fcokv,Panther Lake can run cyberpunk with RT pretty wellâ€¦That is very dope to me,pcmasterrace,2026-02-26 00:15:36,1
AMD,o7fsl8t,I have a rog ally x non xbox as my companion device for my main rig.  Running a trim down version of Windows 11 Pro.  The Z1 extreme is based around the 780M GPU  The system has 24 gigs of RAM that I have divided up 16 GB for the system and 8 gigs for VRAM.   Generally shocked what games this thing is able to play with a small amount of FSR upscaling   Frame generation is not even needed most of the time.   Obviously You have to turn down some settings but that's totally fine being able to play games anywhere I want without having to look around a laptop I fucking love it and I'm sold lol  The YouTube twitch streaming people really don't do a good job showing off the performance of these things and knock them more than they should or they just hate anything that's not a steam deck running ShitOS,pcmasterrace,2026-02-26 01:44:44,1
AMD,o7gc2f7,"Don't mind me over here, happily gaming on a Ryzen 5 2500U.",pcmasterrace,2026-02-26 03:37:07,1
AMD,o7gckk7,"I used the Rog Ally Z1E for a year and a half while playing on board a cruise, I managed to play every single game out there, the only game it didn't allow me to play properly and forced me to use mods to lower quality is Clair Obscure, but KCD2 at 80ish fps with Frame Gen on a damn handheld is crazy",pcmasterrace,2026-02-26 03:40:10,1
AMD,o7gtkm2,if it canâ€™t run 1080p 144/240 i donâ€™t really care for it,pcmasterrace,2026-02-26 05:36:27,1
AMD,o77tibe,"iGPUs are great if you don't game.  iGPUs are great if you only play 10 year old games or if you want to play more current titles but are totally cool running minimum graphics.    But right now, iGPUs suck at gaming.  Any 900 series card is going to beat the performance of an average iGPU.  On top of that, iGPUs use your system ram affecting overall performance.  Seriously, 12 year old budget GPUs that you can find used for $35 on ebay will outperform most iGPUs today.  The 10 year old 1060 cards can beat nearly any iGPU on the market.  They are fine if you don't need much.  They are fine if you are on a razor thin budget.  But pretending like they are capable of any real performance is just not real.  Find a friend that likes to build PCs and take any old junky GPU they have lying around and it will be an improvement.  Trust me, they will more than likely have 2 or 3 old GPUs gathering dust somewhere.  Now, I don't believe this will be the case for much longer.  Nvidia is looking to do some wild stuff, and intel and amd will have to respond in some way, but i'd also expect CPUs with much better iGPUs will also cost considerably more than current iGPU CPUs.  In 5-10 years, the entire landscape around GPUs might shift considerably.  I'm not hating on iGPUs, just being honest about what you can expect from them.",pcmasterrace,2026-02-24 21:59:36,2
AMD,o77py0p,"The problem is memory bandwidth needs have increased dramatically in the same short time frame that IGPUs have got good.   Yes, its amazing that you can play almost all pre 2010 AAA games on a iGPU at 1080/60, most pre 2016  AAA games at 720/60 and the odd well optimised pre 2020 AAA game at 720/60 with upscaling.   but everything past that is a blurry mess even at the lowest settings and with upscaling.   I get that for some, that is more than enough but as the owner of a 8600G, a Z1 extreme Rog ally and a MSI claw 8 aI, I just keep running headfirst into the limitations of the hardware when I attempt to play anything thats less than a entire generation old.   There are obvious exceptions - The forza games are incredibly optimised, as is Doom eternal and funnily enough, Cyberpunk, but alot of games simply dont scale down to iGPUs, even as capable as they are.",pcmasterrace,2026-02-24 21:43:03,3
AMD,o7bis5b,"You are absolutely right. This sub suffers from a deep elitism associated with an unwillingness to learn how things work in the real world. The result is that you often see 3080s being recommended for 1080p setups, or a setup like my current one (Ryzen 5500, Rx 6600, 16GB RAM) being considered outright â€œweakâ€ or ""outdated"" without asking what my uses are (games on high at 1080p FHD) and if I've ever had any limitation issues with it (never).  Everyone wanted to have a 5090, but you are absolutely right to point out that we need to know the available options, especially in these times of ridiculous prices, before discarding things that are below a 5070 Ti Super as if they were trash.",pcmasterrace,2026-02-25 13:07:33,1
AMD,o79pf0b,RDNA iGPUs are very impressive. I've used some laptops with the radeon 680m and 780m in the past and was very impressed with how well they could handle my steam library.,pcmasterrace,2026-02-25 04:14:36,1
AMD,o79tcpt,I recommend iGPUs all the time when the budget is low. But most people here live in countries with relatively good economies where GPUs are better priced.  The bad repnis pure bs from people who can afford to go overkill.,pcmasterrace,2026-02-25 04:41:05,1
AMD,o7a2t0u,"Got a framework desktop, Strix Halo's iGPU is a monster, honestly making me question dedicated GPUs in general...",pcmasterrace,2026-02-25 05:50:21,1
AMD,o77nxgr,Igpu AMD good. Igpu Intel ass.,pcmasterrace,2026-02-24 21:33:43,-2
AMD,o786mfx,"I just got my hands on a intel core ultra x7 laptop. And while I wouldnt by any means call it budget. Its integrated graphics are running everything i try on it. At 1080p high.  Ran sekiro at max settings 1080p. Ran nioh 3 at 1080p medium settings, BF6 did 1080p high with 100+ fps but that one used frame gen and upscaling.   Killer instinct went through 1080p max settings and never dropped a frame during its performance test.   2xko runs at 120 frames 1080p medium.   Only had it since saturday and im just having a blast trying to find what it cant do.   Edit: just wanted to add that price to performance is abysmal by the way. Ive grown very into igpu gaming and love to see what theyre capable of. I could have spent less and gotten a much much much more powerful gaming laptop lol but still, it will run all these games on battery and thats just insane to me",pcmasterrace,2026-02-24 23:05:01,0
AMD,o788nq8,"Probably because hardcore gamers have a bias towards dedicated graphics and normies would rather just plug and play. That leaves integrated graphics in an awkward spot.  At least Panther Lake looks promising. With dedicated GPUs from Intel you can finally expect proper driver support across dedicated and integrated graphics. With this gap in Intel's product lineup filled you can get a decent baseline graphics performance from both Intel and AMD iGPUs. Casual gamers might actually have something to look forward to, even if they're unable to appreciate it.",pcmasterrace,2026-02-24 23:15:46,0
AMD,o79taso,"The newer panther lake CPUs with arc B390 iGPUs score timespys roughly around a 1660 super and 2060 desktop GPU, which allows for decent 1080p medium gaming these days still when you factor in XeSS.",pcmasterrace,2026-02-25 04:40:43,0
AMD,o77vfuf,Donâ€™t forget to download Lossless Scaling. That will help performance a bunch on that igpu as well. Itâ€™s a must have for handhelds also.,pcmasterrace,2026-02-24 22:08:50,-1
AMD,o77mv09,"Nah 8500G can barely drive vscode and Diablo 2 Remastered before you get screen tearing because it cant deal with raster,  it's a bad example",pcmasterrace,2026-02-24 21:28:54,-5
AMD,o77leca,The decks get away with it because the screens are tiny. So 720p is perfectly fine.,pcmasterrace,2026-02-24 21:22:12,367
AMD,o77qwh9,"I just bought a handheld and I'm really surprised with it so far. I was playing God of War: Ragnarok at 1080p at 70+ FPS on it...  Granted, not that hard of a game to drive, but still.",pcmasterrace,2026-02-24 21:47:31,18
AMD,o7ayyrn,I played BF6 on my Xbox ally X on a monitor at 1080p with upscaling and it wasn't a shitshow. 40 to 60 fps.  Most games run super smooth on such a small screen with controller input.,pcmasterrace,2026-02-25 10:39:10,3
AMD,o77w0ai,"To add to this, install steamOS on your pc and they run amazingly well as little gaming boxes",pcmasterrace,2026-02-24 22:11:32,2
AMD,o78b59i,Chuck in some lossless scaling and fsr and my Xbox ally x can literally handle almost anything at 1080p. Sure I'm not running max settings but I can get 100fps in most games no problem. It's kinda nuts. Or I turn down the power and get crazy efficiency. On low power mode I can play something like hollow knight at full 120fps for ~6 hours on a single charge. I genuinely use it more than my desktop pc now because it's so convenient and easy to fit into my daily life.,pcmasterrace,2026-02-24 23:29:04,3
AMD,o794b75,ya it's surreal how powerful the Ally X is.,pcmasterrace,2026-02-25 02:11:19,1
AMD,o78yltq,I've got an original Legion Go. 800P with integer scaling looks great and runs nearly everything at 50-70 fps.  Performance is decent at 1200p in most AAA if you turn down some settings.  1600p is mostly unusable in anything besides Indies and much older titles.,pcmasterrace,2026-02-25 01:38:36,1
AMD,o77v8oc,"I would advise not trusting handhelds, especially cheap ones, too much. I too was under the impression that handhelds like the Steam Deck were just good enough.  I recently bought a Legion Go S Z2 Go, which is slightly better than a Steam Deck, and man my expectations gor immediately put to bed. I am not getting 30 fps in Elden Ring even on the lowest settings and low resolution. Not even stuff like Lossless Scaling helps.  Please be cautious, always check your individual games on handhelds. I would advise buying a handheld for streaming if you have already good computation power. Or if you buy a handheld, buy an expensive one, like the Claw or Rog Ally X. Don't cheap out.",pcmasterrace,2026-02-24 22:07:51,-13
AMD,o792lrs,Steamdeck has an APU though. Not exactly the same as igpu,pcmasterrace,2026-02-25 02:01:34,-10
AMD,o77phm1,They benchmark them at 4800 bc thats all most mini pc's can handle,pcmasterrace,2026-02-24 21:40:56,68
AMD,o78417x,Sadly getting fast/expensive ram these days costs more than a decent GPU,pcmasterrace,2026-02-24 22:51:35,37
AMD,o79jq70,"Yeah it's truly awful how slow DDR is compared to GDDR.  Now with LLMs being a thing maybe we'll see a slot spec for GDDR and MMU support for it in Intel and AMD CPUs.  The PS5 and XBox have had an AMD CPU talking to GDDR for 2 generations now so it's not impossible, PC motherboards just don't use it for some reason. Apple's been shipping with GDDR ever since the switch to ARM.  The latency might be worse but most heavy CPU work these days involves chugging through data in tasks like decompression and newer video codecs or feeding data to a GPU over PCIe. It really feels like the DDR spec has been lagging behind what computers typically do these days.",pcmasterrace,2026-02-25 03:39:04,8
AMD,o77td3f,If people actually played games in this sub you would hear much more praises for this chip,pcmasterrace,2026-02-24 21:58:56,129
AMD,o77ylou,"I gifted my Steam Deck to my nephew and bought a refurbished Ryzen AI 9 laptop to replace it. Slapped on Bazzite and dedicated 8GB of its DDR5 to the video memory.  The fans get loud quick on such a thin little device, but it's a champ with performance!",pcmasterrace,2026-02-24 22:24:16,9
AMD,o77o4qd,"yeah, it's basically RX 480 performance in an iGPU",pcmasterrace,2026-02-24 21:34:39,29
AMD,o77oh68,Mine has run anything at medium 1080p I could throw at it and some titles at low 2440Ã—1620. Absolute beast of an iGPU. Especially if I dedicate 12GB of RAM to it.,pcmasterrace,2026-02-24 21:36:14,9
AMD,o797j5y,"Yep, exactly this. A lot of the time laptops end up constrained to some lowly 15-20w TDP unless you get something with reasonable cooling. Which usually means paying a premium for something with Integrated Graphics, anyways. So no mainstream laptops that you find at the big box store, typically. When you have the CPU and iGPU fighting for the TDP limit, something has to give. Usually the iGPU gets shafted, then the CPU starts throttling as the machine heat soaks.  My personal laptop is in that boat. The AMD Ryzen 2500U in it has a configurable TDP. It's set to 15w because the laptop's VRM and cooling can't handle the chip at the max TDP. But the chip itself can perform much, much faster. The BIOS also artificially caps the performance lower when on battery, with no real way to override it.  Given how it runs at 15w, it would be very competent at 1080p gaming if it could go a little faster!",pcmasterrace,2026-02-25 02:29:28,12
AMD,o7a356o,"Even on laptops they can be a bit of a surprise. A while back I was traveling and wanted to try path of exile 2 when it came out and I had my laptop with me, which was a 6900hs CPU with like 6700s GPU or some such. So I plugged it into the 4k tv, booted the game (keep in mind this was a game that was getting a ton of complaints about bad performance), turned everything down to low and turned on some upscaling, and it was doing ok. Like, 30 fps ish, but with the reputation the game was getting, I wasn't too annoyed.   After playing for a while I was curious if I could get it to run better, so I went to check the temps. And the GPU was still cool... It was running this all on the stupid igpu the whole time, and I was just thinking that it's a heavy modern game at 4k.",pcmasterrace,2026-02-25 05:53:00,5
AMD,o7ar7xe,"It's the same performance that you get out of a $20-40 used GPU from ebay. It's cool and all, but you really have to see it in this context.",pcmasterrace,2026-02-25 09:27:44,3
AMD,o7b5pms,I still play on that 1050Ti and can confirm still runs most games fine on 1080p.,pcmasterrace,2026-02-25 11:36:49,1
AMD,o77p227,"I built a pc for a spoiled kid of a friend of mine. Had a 4090 in it. When I finished building it and asked him what he planned on doing with his new pc, he said he was just going to play Minecraft.",pcmasterrace,2026-02-24 21:38:55,34
AMD,o77pc5m,"There's a lot of dick swinging it both communities. I get shit at work for my old V6 F150 with 200hp, but guess what? Nobody else races their 5.0s or Raptors. They commute on the same roads I do. I've had sports cars. They're overkill for 90% of tasks.  I got by just fine for ages with a 1660Ti and my SO had my first gpu, a 1060 3gb, until last year. Both did what we wanted them to just fine until just recently. Buy what works for you. Flexing is dumb",pcmasterrace,2026-02-24 21:40:14,8
AMD,o77s0j0,It's too optimized...,pcmasterrace,2026-02-24 21:52:40,23
AMD,o796l6b,Optimised games should be the standard.,pcmasterrace,2026-02-25 02:24:08,9
AMD,o77pobn,I wonder if we'll see coolers the size of itx cases when we start seeing systems like that?,pcmasterrace,2026-02-24 21:41:49,13
AMD,o77u4lv,"Well maybe not 2030 but point taken. Canâ€™t wait until we can stop worrying about dedicated graphics. Love what Apple is doing in the GPU department with their Silicon graphics which are getting better with each iteration. I wish companies cared to experiment to give iGPU a chance to thrive and develop. It would give Intel a fighting chance at building more advanced GPUs. If Apple could pick up gaming share we can rely less on Nvidia right there. I know lots of people that would love to play games on their MacBooks that could easily handle most games but just cannot due to absolutely no Apple support. Proof that games can thrive elsewhere away from Nvidia and AMD dGPUs should be priority to show gamers there are options. Looking forward to the Steam Machine doing that, hopefully",pcmasterrace,2026-02-24 22:02:31,6
AMD,o7aad3a,"That seems kind of neat but also insanely expensive, and if one part gives out on you, the whole thing is cooked so I dunno if I like that. I once saw GPUs with a dedicated slot for M.2 SSDs so that they used your GPUs heatsink and fans to cool and saved space which seemed neat and I thought would become more common than it has. I think it had something to do with using too much of the power of the PCIe slot so it was relegated to weaker GPUs.",pcmasterrace,2026-02-25 06:52:34,1
AMD,o77v5hk,How can you tell,pcmasterrace,2026-02-24 22:07:26,4
AMD,o78746w,"I'm with you on that. This absolutely reads like an ad and we're fucking losing our organic internet   Their comments also all replaced the em dash with hyphens but still use them in exactly the same way. Even ""internet savvy"" people stop recognising this shit",pcmasterrace,2026-02-24 23:07:37,3
AMD,o780hft,broken into 2 sentence chunks is not ai,pcmasterrace,2026-02-24 22:33:39,-2
AMD,o792w56,Can you get a replacement from AMD? I believe it's a three year warranty,pcmasterrace,2026-02-25 02:03:15,3
AMD,o77serp,"Sorry to ask, but you seem knowledgeable on the subject, are you saying a ryzen 7 8845hs is a good iGPU? I have one in my 4050 laptop but have never really explored its capabilities other than some small programming tasks.",pcmasterrace,2026-02-24 21:54:30,3
AMD,o7ee7z6,Holy misinformation,pcmasterrace,2026-02-25 21:19:55,2
AMD,o7c7rr3,It does depend on the game you are playing. UE5 games like Hellblade 2 barely run on my laptop 3060 on low settings.  My advice: People always joke about their backlogs. Do something about those instead of hyperfocusing on new games.,pcmasterrace,2026-02-25 15:20:06,1
AMD,o7c4x89,"Does the portable monitor have an inbuilt battery? If so, how long does it last? I'm also considering getting a portable monitor for my steam deck for travelling..",pcmasterrace,2026-02-25 15:06:24,1
AMD,o77q887,"I have a bunch of gmktek k8+, those things are very solid for gaming and proxmox clusters",pcmasterrace,2026-02-24 21:44:22,1
AMD,o7ab4t0,You might be onto something!,pcmasterrace,2026-02-25 06:59:11,1
AMD,o77wsjf,"Not really, they're quite comparable. Amd being better in igpus was about 6-7 years ago",pcmasterrace,2026-02-24 22:15:21,2
AMD,o787kgb,"I canâ€™t speak to their gaming use, but for transcoding video the Intel Ultra CPUâ€™s are just hand down better than any of the competition.",pcmasterrace,2026-02-24 23:10:01,2
AMD,o7c8xuo,I would encourage you to look up the benchmarks for Panther Lake. The ball is now in AMD's court.,pcmasterrace,2026-02-25 15:25:39,1
AMD,o7a3dzs,Issue is you need like 40 fos for lossless to work as intended,pcmasterrace,2026-02-25 05:54:55,1
AMD,o77rwko,8600g is miles ahead of 8500g. 8500g got only 2 full cpu cores. Other 4 are just like intel e cores. 8600g got full 6 cores and far better graphics,pcmasterrace,2026-02-24 21:52:10,2
AMD,o78mpzo,"I often use my Steam Deck on my 65"" 4k TV. It looks great in 4k with the game itself in 1080p. A surprising amount of the ""high res feeling"" you get, actually comes from UI elements. The 3D rendered game itself is actually not that noticeable being only 1080p.",pcmasterrace,2026-02-25 00:32:41,24
AMD,o77n4qe,"Nah my deck does 1080p all day, cyberpunk only ever crashes after 2+ hours of non stop play lol",pcmasterrace,2026-02-24 21:30:08,148
AMD,o781zud,That's just a matter of screen size versus distance.Â    Put the larger screen at the equivalent distance for the size and it looks the same.   https://www.rgb.com/sites/default/files/articles/viewingdistanceresolutioncomparison.jpg,pcmasterrace,2026-02-24 22:41:14,13
AMD,o79br0q,Deck screen is tiny sure. But also the fact that they are portable and battery powered limits their potential. If pushed to the limit iGPUs are much more capable than 720p. Ie Z1 extreme on the ROG Ally.,pcmasterrace,2026-02-25 02:52:41,2
AMD,o79mw9f,"2560x1600 on a ""handheld"" with igpu here, and it definitely has no issue cranking the settings.",pcmasterrace,2026-02-25 03:58:28,0
AMD,o77wyvx,"Elden ring is really popular on deck, was the Go running windows?",pcmasterrace,2026-02-24 22:16:13,5
AMD,o77wwb7,"That seems weird?   The Steam Deck is able to do Elden Ring at 1080p 30fps using an iGPU that probably is around Radeon 760M performance, the Z2 go's 680M like iGPU should have little issue.",pcmasterrace,2026-02-24 22:15:52,11
AMD,o789vgz,"You are doing something wrong. Most of the time people arenâ€™t using proper upscaling techniques to get the most out of their handheld gaming experience. Personally, I just integer scaling which looks decent and provides a massive fps boost. Additionally this can be improved further by using frame gen. The problem is that each game requires tinkering until you find the optimal experience.",pcmasterrace,2026-02-24 23:22:10,1
AMD,o77zlof,"Can confirm, Ally X is a champ, just wish I could find a 32GiB Legion 2 anywhere. My Ally X RAM is on a 16/8 split, Iâ€™d love to get a Z2E and have a 16/16 split.",pcmasterrace,2026-02-24 22:29:13,0
AMD,o782vhq,"Despite having a pretty decent PC, I don't buy games if they don't run well on the Deck because I do 90% of my gaming on it. I'll get the games I miss out on sale and play them on an eventual Steam Deck 2. Don't feel like I'm missing much in the last few years anyway.",pcmasterrace,2026-02-24 22:45:42,0
AMD,o79tfib,It's exactly the same.   An APU is just a CPU with integrated graphics (igpu),pcmasterrace,2026-02-25 04:41:37,9
AMD,o789sv9,"I recently overclocked my old DDR4 instead of paying for an upgrade, it was super easy and completely free, highly recommend.",pcmasterrace,2026-02-24 23:21:48,19
AMD,o7bjos5,"GDDR has a big latency hit, and a lot of workloads are sensitive to latency",pcmasterrace,2026-02-25 13:13:00,2
AMD,o78xh3h,That chip is attached to strix point apus which are ridiculously expensive. Youâ€™re talking 1kUSD for something that is still a igpu at the end of the day.  No one talks about it because no one has one.,pcmasterrace,2026-02-25 01:32:06,32
AMD,o78hp39,Yup instead they spend thousands on overpriced gear and play the same shitty games everyday. Or they just watch YouTubeâ€¦,pcmasterrace,2026-02-25 00:05:34,11
AMD,o7avay3,"At 30w, those chips start to rock.",pcmasterrace,2026-02-25 10:05:34,5
AMD,o7eim7z,Would it be possible to build a gaming PC with this and then switch to a dGPU later on?,pcmasterrace,2026-02-25 21:40:19,2
AMD,o7bmtvj,"I remember starting to play Doom 2016 on the 1050Ti.  I did end up finishing it with a 1070Ti, but that 1050Ti was a little ripper.",pcmasterrace,2026-02-25 13:31:09,1
AMD,o77qy04,Enough mods and he'll get great mileage out of it tbh,pcmasterrace,2026-02-24 21:47:42,52
AMD,o77rrcp,Minecraft with shaders will make good use of a 4090.,pcmasterrace,2026-02-24 21:51:29,23
AMD,o77sz24,â€œJust going to play minecraftâ€ he saysâ€¦â€¦.,pcmasterrace,2026-02-24 21:57:07,10
AMD,o78tiii,"I changed GPUs recently and went for a 9070XT from a 3070. In my experience, playing a minecraft modpack with 427 mods plus shaders will make my baby **work.**",pcmasterrace,2026-02-25 01:09:26,5
AMD,o78cyll,"But my more expensive car is more confortable for the same road we take. Â«Â Not going for the best is enoughÂ Â» is a take from someone who canâ€™t buy more or has no use. But I need that more confortable car because I would off myself if I were to do 2h of driving everyday in a â€˜99 corolla. Same speech if I play everyday some game other than PVP ones. Of course buying a 5090 only to play Rocket League is useless, but you wont have to tinker every settings on more demanding games in the future because you got a Â«Â just good enoughÂ Â» GPU. Still, people buy what they want and can or need, but sometimes it seems like a sin to buy a future proof or more specs-confortable hardware if we listen to this sub.",pcmasterrace,2026-02-24 23:39:03,1
AMD,o790e8s,I wish we could say this about more games lol,pcmasterrace,2026-02-25 01:48:54,14
AMD,o7ad9q0,"Car racing games are very cheap to render, you don't need a lot of optimization in the first place.",pcmasterrace,2026-02-25 07:18:02,2
AMD,o77s5yk,"Yeah I mean the laws of thermodynamics are a bitch but the nice part about vertical integration means more 3d space to fit a capillary network for small molecule sovlents like novek to be used for cooling directly through the chip, even so a 2 stage cooler with pumps radiator and fans is gonna take up a small pelican case at least.",pcmasterrace,2026-02-24 21:53:22,9
AMD,o77t2vy,Just look (as reference) at the size of the Steam Machine cooler and case.,pcmasterrace,2026-02-24 21:57:37,3
AMD,o7c7ch1,The problem is you will get dumb SKUs like Strix Halo. You only get the full fat GPU with 16 CPU cores. That's complete overkill for gaming. Thankfully AMD has now launched a more reasonable 8 CPU cores SKU after a year.  But the point stands. You'll end up buying i7/i9 level chips when an i5 would be more than sufficient.,pcmasterrace,2026-02-25 15:18:04,1
AMD,o77vxbl,"I'm honestly glad apple isn't in the gaming market, they would just increase the price point on everything by virtue of existing in the space, the opposite effect that any other major company jumping into the market would have, because they would over price anything they would put out for gaming.",pcmasterrace,2026-02-24 22:11:08,-1
AMD,o7ad697,"Yeah they currently are working on getting them cheaper and the biggest issue is ram burnout, a modular approach is what they keep wanting to fall back to but it creates so much latency by moving the ram even millimeters away from the cpu/GPU chip, best current solution is using modified camm2 surrounding the processor. The other route is making them significantly cheaper and ""disposable"" so when the ram in the chip calls it quits you either replace the chip or the entire unit. Extremely wasteful but potentially more profitable.",pcmasterrace,2026-02-25 07:17:10,1
AMD,o7b72bu,"The complexity of a CPU is so extreme that it can't really be considered ""one part"" with regards to what you said about ""if one part gives out on you"". A CPU has hundreds of component modules, so by packing everything into one package it's more that you're taking something from a few hundred to a few more hundred modules in that package. To say that increases the chance of failure is like thinking modern CPUs are more failure prone because they have more component modules than older ones. That's one aspect, the other aspect is that the economy of scale of building it all into one chip will mean even if you have to replace the whole thing if it breaks, it will probably cost you less than replacing just a GPU now because there will be less highly specific products that need to exist, resulting in increasingly generic and mass produced solutions.",pcmasterrace,2026-02-25 11:47:30,0
AMD,o77x6kj,">anyone else ... ?  1. Pretty common closing from AI  >is still sleeping  2. Common from AI  3. Notice how proper nouns are all properly capitalized in a pretty formal way (particularly things like ""dGPU""), which makes no sense when the first letters of each sentence are not.  >without more conversation  4. This is the kinda half-intellectual rhetoric style you see a lot of AI, since a lot of them are trained on Reddit data and weighs liberal-ish, high-brow ish, high education comments more heavily during training (because it's associated with higher quality in terms of content). It's kinda part millennial, part therapy-speak, part intelligentsia-speak... It's hard to describe, but you'd get a feel for it if you prompt AI enough and look at what they spit out. It's basically peak liberal Reddit style. (And I don't mean politics, I mean the writing style).  >felt like a win  5. Another Reddit-ism. AI loves writing like this.  6. User has a hidden profile.  As a side note, it used to be easy to bypass the hidden profile feature by searching their username. But Reddit blocked that so now detecting AI is harder. It is what it is, but I just instinctively distrust anyone with a hidden profile now.",pcmasterrace,2026-02-24 22:17:15,30
AMD,o77wjvz,"""Trust me bro""",pcmasterrace,2026-02-24 22:14:10,2
AMD,o7988i3,Does it apply regardless of where I got it from?,pcmasterrace,2026-02-25 02:33:23,1
AMD,o77mnal,"Pricey, but ridiculous. I got one of these [https://www.amazon.com/dp/B0F53XL9DP](https://www.amazon.com/dp/B0F53XL9DP)",pcmasterrace,2026-02-24 21:27:55,1
AMD,o794woz,It is good but probably not nearly as good as a 4050,pcmasterrace,2026-02-25 02:14:43,1
AMD,o7aylo1,"The 8845HS is a CPU with the 780M for iGPU. Yes itâ€™s a good iGPU (~1650m level), but not as powerful as a 4050m. For the wattage it takes though and the fact itâ€™s integrated with the CPU itâ€™s good.",pcmasterrace,2026-02-25 10:35:52,1
AMD,o7eergv,No joke. 2070m was hitting 96c when I tried to game on it. Tried to repaste but the asus rog zephyrus s15 is hard to open. Ended up selling since 3 hours battery wasn't good enough for school either.   Actual 780m performance is around a GTX 1650.,pcmasterrace,2026-02-25 21:22:25,0
AMD,o78fvth,Just gaming,pcmasterrace,2026-02-24 23:55:28,1
AMD,o77nis4,"I think what they mean is that since the screen is small, 720p is a fine resolution to play at",pcmasterrace,2026-02-24 21:31:53,170
AMD,o78j3xg,it's an 800p screen.,pcmasterrace,2026-02-25 00:13:22,10
AMD,o78g1ol,"While you're right your chart shows the wrong end of the spectrum, we need the plot for screen size 8""-12"" at a 1-2ft distance",pcmasterrace,2026-02-24 23:56:22,8
AMD,o785m31,"It is running Windows, but even from YouTube videos, people advise you to use frame gen to get above 30-40fps on 800p on the deck. There's countless videos showing you how to use Lossless Scaling to use frame gen, which is idiotic cause the input lag destroys a game like ER.  It goes to show, Steam Deck can't run modern stuff properly, and people need to tone down their expectations. It is foolish to trust fanboys and biased reviewers, especially now where this iGPU power from Z2 Go and Steam Deck is aged.",pcmasterrace,2026-02-24 22:59:43,2
AMD,o782hky,I remember Elden Ring being smoother on the Deck than other PCs at launch because of Proton's shader compilation process.,pcmasterrace,2026-02-24 22:43:43,1
AMD,o785157,"It can go above 30fps with max power but in certain areas it doesn't stay above 30 fps. Even at Gatefront, the starter area, it dips below 30fps depending on the angle.  It's either Windows or Lenovo bullshittery that is maybe causing some issues maybe.  Regardless, the amount of Steam Deck dickriding is crazy. I want someone to seriously tell me the Steam Deck handles Elden Ring at all times above 30fps. I guarantee you, if you go into a heavy area, like Nokstella, it won't.   The Steam Deck is old and has limitations. You need frame gen + upscaling (or force upscaling using Optiscaler) to get 30fps+ in some games. And Optiscaler can easily get you banned in many online games. I don't see how you can't make a disclaimer like this without beint downvoted.",pcmasterrace,2026-02-24 22:56:44,-1
AMD,o78aypq,"I think you underestimate what type of user I am. Integer scaling is something Lossless Scaling can do and I have used that. It literally does not improve performance, how can you say to me ""I'm doing something wrong"", you mean I'm not able to press ""Scale"" in the application or what? Or just flick the setting in AMD Adrenalin? Do you want me to record this?   On the Z2 Go, in Balanced power, you dip below 30 fps, period. You can play with LS or without, it doesn't matter. The only thing that boosts frame rate is LSFG. In Performance mode, you can get 35 fps but this is not consistent.  Indeed, the consistent low frame rate is strange. I am surprised by this too. Only power modes affect the performance. But this is beyond my scope. I am literally using all the tools people claim to make the Deck viable, on better hardware, and it still isn't as good as it seems.",pcmasterrace,2026-02-24 23:28:04,1
AMD,o7865yx,"No offense but I don't think you're being smart about this. Unless you have horrible internet or no internet at all, you should use your PC for streaming with apps like Moonlight. You have hardware that's dozens of times more powerful, it makes no sense not to use it. You should buy whatever you want, and if it runs poorly natively on Steam Deck, stream the game from your PC.",pcmasterrace,2026-02-24 23:02:35,3
AMD,o7cybtu,"APUs dedicate a massive portion of the chip to gpu, unlike igpu, APUs share share memory more efficiently than igpu which shares system ram, etc, etc. They are similar in principle but different in design. So not the same thing. Love getting down voted for speaking truthfully.",pcmasterrace,2026-02-25 17:22:00,-1
AMD,o78bsnx,Only worth it if you are using iGPUs or if it is slower than 3600MHz. Even then you want to keep it simple because stability testing for ram takes forever.,pcmasterrace,2026-02-24 23:32:37,11
AMD,o7chesh,"Some are and some aren't, but it seems to me like demand has been shifting to higher bandwidth, higher latency applications like games and databases/web backends for at least a decade now  Applications that would suffer from higher latency on many disparate small reads are like browsers, email, etc and are already fast enough. The latency sensitivity in these applications could also be papered over by larger 3D cache technology, most of them don't access large randomly spaced datasets that would thrash cache anyways.",pcmasterrace,2026-02-25 16:04:44,1
AMD,o7a843a,"You got confused, the 8060s is the iGPU of the Strix Point cpus",pcmasterrace,2026-02-25 06:33:22,8
AMD,o78t4lj,I swear most of them only run benchmarks,pcmasterrace,2026-02-25 01:07:17,13
AMD,o7fja4t,"It's possible, but the hardest part is getting PCI-e lanes. My personal laptop lacks Thunderbolt, and the best it has for peripherals are 5Gbps USB ports. Everything else is going to NVMe Storage or Wi-Fi/Bluetooth.",pcmasterrace,2026-02-26 00:51:34,1
AMD,o77tcey,I can bring my 4090 to tears in Minecraft if I want and it can also be the best looking game I own,pcmasterrace,2026-02-24 21:58:51,14
AMD,o782oae,"Iâ€™m not so sure about that anymore. The Apple silicon kind of turned that model upside down after the Intel era. RAM has always been expensive but they havenâ€™t been raising the price (yet)  Now that everythingâ€™s on one chip weâ€™ll see what they do. Theyâ€™re not reliant on a million vendors for their parts. The performance has made their hardware quality/performance ratio a good value IMO.   Also! All their RAM is part of the same chip. So if you have 128GB RAM, you immediately have 128GB VRAM. So itâ€™s great for AI and stuff that you can use lots of offline modeling on your own. Iâ€™ve used a lot of AI stuff offline and itâ€™s pretty cool. My gaming PC with a 4090/24GB RAM is WAY faster at offline AI workloads but I still use my M1 MacBook Pro with 64GB RAM because itâ€™s more convenient to load in models that are much bigger. The models can spill into swap too which is cool.   Iâ€™m not an expert about gaming APIs but it seems to me there are some unique advantages to the Apple platform for gaming. Chiefly, convenience for most people I think. Who are sitting there with their M4 MacBook Pros with a full fledged GPU inside them essentially, and they know it, but have nothing to do with it.   Anyway I know this is PC master race but in the past few years I think this sub has become interested in other forms of hardware too. I think weâ€™d all love Nvidia and their AI lackeys to get their boots off our necks!",pcmasterrace,2026-02-24 22:44:40,3
AMD,o783l2z,"As an Apple user I agree completely. Generally competition creates better and cheaper products, then you have Apple. They donâ€™t want to be low cost, or even care about cost. Theyâ€™d raise the ceiling then every other company would start to increase prices to get close but remain cheaper (but still more expensive than what they were).",pcmasterrace,2026-02-24 22:49:17,5
AMD,o7885fy,"> As a side note, it used to be easy to bypass the hidden profile feature by searching their username. But Reddit blocked that so now detecting AI is harder. It is what it is, but I just instinctively distrust anyone with a hidden profile now.  Search for ""author:(freecurfew_9)"" in the reddit search bar and you'll see everything. Good luck on fighting a fight we already lost",pcmasterrace,2026-02-24 23:13:05,15
AMD,o79fsqi,I believe it's any boxed CPU from an authorized retailer. You can try looking up your serial number and see what it says:  https://www.amd.com/en/resources/support-articles/faqs/SNV-LUT.html,pcmasterrace,2026-02-25 03:15:44,1
AMD,o77o84f,Ah yeah that tracks,pcmasterrace,2026-02-24 21:35:05,46
AMD,o78bzn8,This exactly. 720p at that tiny resolution looks fine and allows lots of breathing room on the iGPU performance.,pcmasterrace,2026-02-24 23:33:43,5
AMD,o799pdj,Docked it pushes 1080p on tons of games with no issues whatsoever.,pcmasterrace,2026-02-25 02:41:28,2
AMD,o7b41jl,"Absolutely not my experience on the Deck. Medium settings, 720p. Roughly 57 hours into my play through.  It hasn't dropped below 30fps a single time. Not once, occasionally hitches when loading a new area but that's it.  Didn't drop from 30fps even during the dragon fight with all the fire close to the point you exit into limgrave, which is notorious for tanking fps.  I constantly have the fps showing due to being quite the performance whore, main rig has a 5090 and a 9800X3D and I can't stand unstable framerstes. I monitor it very closely.   Elden Ring on the Deck is arguably my favourite way to play it.",pcmasterrace,2026-02-25 11:23:09,2
AMD,o78doo7,Watch a video on configuring integer scaling and set it up via adrenalin. Don't bother with LS.,pcmasterrace,2026-02-24 23:43:07,1
AMD,o7fg078,"Because you're not speaking truthfully. You're just straight up wrong lol.   You can look up APU and see that it just means a CPU with an integrated GPU (iGPU)   >APUs dedicate a massive portion of the chip to gpu, unlike igpu,  Incorrect. Some might. Look AMD's a4 APUs for example. There are varying levels of iGPU performance in these, and you seem to be under the impression that the higher end ones are APUs while the lower end are not.  It's not hard at all to look this up, buddy.",pcmasterrace,2026-02-26 00:33:37,1
AMD,o7baznd,The 890m is on the 370 and up  https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-9-hx-370.html,pcmasterrace,2026-02-25 12:16:32,5
AMD,o7g49c3,"No, they're right. 8060 is strix halo",pcmasterrace,2026-02-26 02:51:00,1
AMD,o78573m,"Agreed people are looking elsewhere but walking away from one villain into the arms of an old one seems like a foolish but common mistake. If we want to get away from the shameless and greedy mega corps we need to move towards modular and open source, which means sacrificing power and speed for principals and solidarity.   I'm just a robot mechanic, but I know there are people from every tech field that want the big tech corps to choke on their profit margins. Maybe an online movement to create a collective of actual tech laborers, not the c suite bros, to build a company that could provide quality hardware without gouging the consumer, tighter margins would mean slower growth, but the point is disrupting the monopoly not becoming the new one.  Then again that sounds like a lot of work, and I'd rather play some games than get beat up by pinkertons IRL",pcmasterrace,2026-02-24 22:57:34,2
AMD,o788tiw,"Nice, it works. Looks like OP might not be a bot though, cos the activity is so low. But a lot of LLM tells in their history. I've seen some real people using LLMs to comment for them, which is just sad",pcmasterrace,2026-02-24 23:16:37,3
AMD,o77qnyz,"This is a special purpose PC, and with 128gb of ram in 2026. I use this almost exclusively for local AI. (Also I paid about 2000\~ for this on release)",pcmasterrace,2026-02-24 21:46:25,0
AMD,o77puzx,Baseball huh?,pcmasterrace,2026-02-24 21:42:39,44
AMD,o7bcmxt,"What can I say man, either I'm missing a pair of eyes or SteamOS gives that much of a performance boost. Let's hope it's the latter. Otherwise there's no explanation.",pcmasterrace,2026-02-25 12:27:57,1
AMD,o7bd0le,"Sorry forgot to ask, what power mode? I assume the maximum available. What do you get when you're at balanced?",pcmasterrace,2026-02-25 12:30:29,1
AMD,o788dr2,"I do think you are right. Looking at both options is a good way to do it. Right now, Nvidia losing both gamers AND AI clients is the best option IMO, as they've been trying to squeeze both.  Beyond that I agree, open source is the best way to go. But nobody is going to build truly high performance hardware right now while Nvidia caps everybody else's needs. Unless somebody experiments with other high performance ecosystems, IMO, it's impossible for anybody to get excited for change.   Linux is starting to gain in popularity now that people are seeing the results of what Microsoft has been setting up for years. Valve is coming in and disrupting the status-quo. If Apple comes in and disrupts the status quo, that's another good start. Anything that starts triggering a new era of competition and innovation is the only way that will allow Intel to start competing which IMO is our only hope of getting a new 3rd high performance competitor. Once the floodgates are open with a 3rd high performance competitor, it will allow people to start experimenting with new diverse APIs and perhaps more open source software that can start bridging ecosystems better that gives everybody more choice  But I'm fantasizing, we'll be chasing Nvidia cards forever",pcmasterrace,2026-02-24 23:14:17,2
AMD,o78a282,"Or this is how the good AI advertising bot of the future works. If I had to make instructions for a bot with my knowledge about how well reddit users might recognise AI I could literally not make it better. Posts regularly but randomised the time between posts by a few days, is active only once on some subs but active multiple times on others to simulate real interest, waits a while before making the actual advertisement posts to pass all karma and account age checks...  The only real things I can see is that the insistence on lowercase for everything except brand names and abbreviations seems like it's trying a bit too hard to emulate real internet writing and that some of their comments use the hyphen (instead of the em dash because they probably already realised that the em dash is to obvious) way too predictably",pcmasterrace,2026-02-24 23:23:10,2
AMD,o79sbqm,Baseball huh,pcmasterrace,2026-02-25 04:34:02,3
AMD,o77wmgv,That's SIDs,pcmasterrace,2026-02-24 22:14:32,7
AMD,o7bjaok,"Steam Deck has a chip optimized for running at 8-15W, your handheld likely has an off-the-shelf rebranded laptop chip meant for 20-30W and limiting it to lower wattage would leave Steam Deck faster",pcmasterrace,2026-02-25 13:10:41,1
AMD,o78cw8j,"Yeah Linux is definitely a big part of the path forward, but while diversity of competition and ideas and methods is great, our current economic system means excess waste, incompatibility, and short lifespan are all rewarded. Our biggest issue in my opinion is designed incompatibility, we should further standardize ports and components for maximum compatibility, open source all protocols and architectures give people the chance to use what documentation exists currently locked behind patents to truly innovate, as a species our progress is being hobbled by the worship of profit. Maybe Apple would tip the scales and disrupt things, but we need everyone as individuals to reexamine what we are even doing this all for, and maybe look for a better method to accomplish our goals.",pcmasterrace,2026-02-24 23:38:41,2
AMD,o77yu48,"I hope not, although it's performing somewhere in between a 4060 / 4070 which for me is good enough, but the real strength is having a GPU with access to all that 8000mhz RAM.",pcmasterrace,2026-02-24 22:25:24,1
AMD,o7ecjb6,Sudden Infant Death Syndrome..? ðŸ™Š,pcmasterrace,2026-02-25 21:12:09,1
AMD,o78okjm,lol we've lived long enough for the villain to become the hero and the hero to become the villain,pcmasterrace,2026-02-25 00:42:30,1
AMD,o6qqq0c,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-02-22 08:42:15,1
AMD,o6qvkzf,"They never even provide a driver to Z series owners.  Regular laptop chips are officially supported by both OEM and AMD so they can use drivers from both side, but all Z series chip cannot install drivers on [amd.com](http://amd.com) without an inf hack, and the official driver distribution is done solely by OEM instead. This helps AMD reduce QA cost in exchange for lower chip pricing.",AMD,2026-02-22 09:28:51,135
AMD,o6sexjs,"So repeat of the mobile chip driver disaster for laptops up to around 2011.     If you bought a laptop, the laptop manufacturer had to provide the driver.   Do you know what they did not provide? Updates.   Some laptops got 1 update after 12 months or so, but if you had any problems afterwards, you had 3 options:   1. live with the problem (how about no?)   2. buy a new device (what they want)   3. download the inf-modified drivers like NimeZ or Radeon Legacy Modded   This mess ended around 2011 with official support from AMD and Nvidia, but history repeats itself.",AMD,2026-02-22 15:53:40,36
AMD,o6sahgd,The GPU driver situation on the ROG Ally devices that use this chip was already annoying as instead of just getting the latest updates directly from AMD you had to wait for ASUS to push them (which happened sporadically at best) and now it seems it won't be happening at all. And to think the first Ally came out less than three years ago.,AMD,2026-02-22 15:33:22,10
AMD,o6unmkx,"Driver Updates need to be standard for the average lifetime of the device, it's not asking alot to at least provide generic drivers that are reasonably up to date or allowing people to at least be able to install the normal drivers from amd.com. Trying to ditch driver support after just 2 years is not on and while I'm not 100% at least in the EU that might raise some red flags around creating ewaste by refusing to update a device after just a few years from launch. 5 years at least is the minimum a device should be supported.",AMD,2026-02-22 22:18:49,8
AMD,o6w5qj3,"AMD needs to stop doing this. Create a standard driver, then have custom vendor blobs on top. Standard driver comes from AMD. Vendor updates their specific blobs separately and are encouraged not to tie customizations to any one version of driver. Nothing should break if base driver is updated without a blob update, in other words.  This is not terribly difficult. Laptops are the same way and it's annoying due to little vendor-specific customizations, like display timing adjustments or backlight control (like smooth granular increase/decrease instead of jarring fixed steps) and other little things that can affect power consumption. I often get the best battery life on vendor-specific driver for my HP laptop vs generic driver.",AMD,2026-02-23 03:36:06,6
AMD,o6t63bq,Steamos or linux. Anyway.,AMD,2026-02-22 17:56:13,8
AMD,o6sti4v,Amd is surprisingly the best choice on Linux / steamos and the worst on windows.,AMD,2026-02-22 16:57:57,14
AMD,o6qyqzc,"Correct me if I'm wrong, but doesn't AMD Adrenalin just ""magically"" get the relevant drivers for this, along with other RX 6000 chips? And OEMs will already need to provide drivers for the joysticks/buttons/lights/etc so I should think this isn't too much of an issue. And on SteamOS it should be part of the kernel",AMD,2026-02-22 09:59:27,6
AMD,o6t5gs7,Isn't it just a 8840U in a trench coat?,AMD,2026-02-22 17:53:31,2
AMD,o6twcws,"Dont worry, radv on linux makes the dirty job for amd",AMD,2026-02-22 20:00:23,2
AMD,o6s66sm,Good guys AMD!,AMD,2026-02-22 15:12:10,2
AMD,o6y9rdh,This is why direct Intel driver support for the MSI Claw is a godsend. Learn the hard way with ROG Ally.,AMD,2026-02-23 13:56:43,1
AMD,o77cgul,I would not have guessed that my gen 1 MSI Claw would have better support from Intel than an AMD based chip. Wild.,AMD,2026-02-24 20:41:17,1
AMD,o6spqm5,So some Lenovo rep that allegedly talked to some bozo on Reddit is trustworthy news?,AMD,2026-02-22 16:40:56,-4
AMD,o6vfczo,I'm fine with this. But what I'm not fine with is they don't allow us to install official 7840U driver or not in a straight forward way. Like come on they are the exact same cheap just stop limiting driver options.,AMD,2026-02-23 00:54:07,-3
AMD,o6r3sz4,> and the official driver distribution is done solely by OEM instead.   Any product that does it that way these days should just be avoided.  MSI does the same for Intel GPU drivers on the iGPU on some models. Installing Intel ones kills switchable graphics on some models and only MSI ones will work (which are never updated).,AMD,2026-02-22 10:47:37,69
AMD,o6rfbdz,Lenovo dropped support too so maybe they stopped OEM level support too,AMD,2026-02-22 12:30:10,11
AMD,o6swvsz,"There's a reason why business prebuilts/laptops are sometimes more expensive, the support lifecycle is guaranteed to be 3-5 years.  Example: I bought a ThinkPad E585 and BIOS update lasted a little more than 5 years wheres the CPU equivalent consumer model like Acer Nitro 5 AN515-42 lasted just 2 years.  >   Some laptops got 1 update after 12 months or so, but if you had any problems afterwards, you had 3 options:  There was this huge PITA before AMD unblocked driver installation on laptop chips, as vendors were lazy towards supporting AMD(then) solutions. Severely outdated vendor GPU drivers hampered usability back then. Thank god AMD did the good and allow users to use generic driver releases.",AMD,2026-02-22 17:13:07,8
AMD,o6wjfga,"Back then it was due to technical limitations of the old driver model, and now it's more like a maintenance cost reduction. I guess in the end we just choose something that is officially supported by SteamOS instead of messing with all these Windows stuff.",AMD,2026-02-23 05:12:58,3
AMD,o6sta0s,"Lenovo-Korea announced the Z1E Legion Go will no longer receive drivers, or BIOS updates.",AMD,2026-02-22 16:56:55,7
AMD,o6tmo9q,"That's not AMD being the best on Linux, they're open source drivers it's technically AMD doing the least amount of effort.",AMD,2026-02-22 19:12:27,20
AMD,o6rbcy5,Those chips have different identification code and an installation file from AMD will not work unless you modify an inf file.,AMD,2026-02-22 11:57:30,12
AMD,o6zgweo,7840 / 8840 with the NPU disabled and a larger range of cTDPs.,AMD,2026-02-23 17:29:07,1
AMD,o6raacj,And how are Z series handheld owners supposed to do that when amd never provided them in the first place?,AMD,2026-02-22 11:48:16,13
AMD,o6rhlet,"Yeah obviously that's the problem. OEMs don't commit to support the device for a longer period, and that's way worse than Valve's Steam Deck.",AMD,2026-02-22 12:47:35,10
AMD,o6toxc9,Who wrote those drivers and still supports them?,AMD,2026-02-22 19:23:23,-10
AMD,o6ruszp,"Install any Linux distribution with a modern kernel and it will work out of the box, no drivers needed.",AMD,2026-02-22 14:11:14,20
AMD,o6raj9m,You avoid the product to begin with?  Not buying something is always a option for products like that.  You wont die without your hand held.,AMD,2026-02-22 11:50:27,9
AMD,o6rl6ri,"That's a different OS with a completely different driver model, so it's not a fair comparison. These Z line chips will continue getting generic RDNA2/3/3.5 updates for a long time, that has nothing to do with OEMs.  One more reason to install Linux on those machines ay?",AMD,2026-02-22 13:12:57,-4
AMD,o6tpkrw,"A collection of unpaid nerds of legend, not AMD",AMD,2026-02-22 19:26:35,14
AMD,o6s4zh5,A lot of people continue to use Windows on these devices despite the poor handheld experience on Windows so they can play Gamepass games.,AMD,2026-02-22 15:05:53,11
AMD,o6vjtz1,Installed CachyOS on my laptop due to this.,AMD,2026-02-23 01:20:56,4
AMD,o6rfgq6,Haha. Ok.,AMD,2026-02-22 12:31:20,-6
AMD,o73wwzu,"Most of these nerds of legend are actually contracted by Valve.  Practically everywhere you look, it's Valve.    For example, DXVK started because the creator wanted to play NieR:Automata on Linux, then Valve started paying them to just keep doing what they were doing.  Sometimes Valve contracts the people directly, sometimes it's through a consulting firm like Collabora or TechPaladin, but wherever you look in the Linux ecosystem, you'll probably find a lot of the work being done is funded by Valve.  ""It's all Valve?""  ""Always has been.""",AMD,2026-02-24 09:26:30,2
AMD,o6tpyxv,Pretty sure AMD is gonna be the top contributor to their open source drivers with paid employees.,AMD,2026-02-22 19:28:31,-8
AMD,o6sjvwb,If you bought a windows handheld you get you deserve.,AMD,2026-02-22 16:14:34,-10
AMD,o75emfn,"Amd writes the amdgpu Linux kernel driver, which is the largest driver in the entire Linux kernel.  The user space components do to be maintained by valve, red hat, and others tho.",AMD,2026-02-24 15:24:38,3
AMD,o6trogn,"So the most contributed is the most good driver, I didn't realize programming worked like that, thanks Elon",AMD,2026-02-22 19:37:00,11
AMD,o73wd92,"Actually, it's Valve.  More accurately, people Valve contracts.  People don't realize how much Valve has been carrying the entire Linux gaming stack from top to bottom over the last decade.",AMD,2026-02-24 09:21:13,1
AMD,o6uxuww,"Ah yes, Lets blame windows for AMDs shitty support.",AMD,2026-02-22 23:14:23,8
AMD,o7akf6h,"Even amdgpu sees a solid amount of work from non-AMD sources (e.g. look at all the recent work being done by a Valve contractor fixing up support for very old AMD GPUs in the amdgpu driver), though AMD is probably the primary contributor to the kernel portion of the driver, unlike the Mesa side where it's Valve contractors being the primary contributors.",AMD,2026-02-25 08:23:35,1
AMD,o6vmllk,Microsoft could support gamepass on Linux.,AMD,2026-02-23 01:37:51,3
AMD,o6we5lh,And amd could also support their own shit in general.,AMD,2026-02-23 04:33:56,9
AMD,o6j4wa5,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-02-21 01:47:26,1
AMD,o6lcjj2,I just want 24 cores on desktop platform to speed up linux compilation. I'm still on my 5800X3D since it doesn't make that much sense to move up with 9070XT,AMD,2026-02-21 12:45:56,71
AMD,o6jabhv,They should just stop making normal vs x3D parts and just make x3D the standard. Clearly most everyone wants it.,AMD,2026-02-21 02:21:17,115
AMD,o6kltu4,Not a surprise given N2X not in full production yet,AMD,2026-02-21 08:36:37,5
AMD,o6p8yh2,is the igpu still rdna2?,AMD,2026-02-22 01:42:22,3
AMD,o6l90ho,Will this be am5?,AMD,2026-02-21 12:17:48,5
AMD,o6tb87n,ZENG,AMD,2026-02-22 18:19:29,1
AMD,o6yosmw,Another year passes... sad.,AMD,2026-02-23 15:17:02,1
AMD,o6l0lkd,"Fuck, i was looking to indulge myself in autumn and replace my 7000 series cpu!",AMD,2026-02-21 11:01:58,2
AMD,o6mzu3i,There is HEDT for this usecase. Threadripper is always there for you to grab.,AMD,2026-02-21 18:12:45,24
AMD,o6of6qt,They probably won't do that for the same reason Nvidia gimps the vram of their 80 series gpus. It's to push you to buy the more expensive option.,AMD,2026-02-21 22:40:07,7
AMD,o6p0za4,"If AMD brought some defective Turin Dense (Zen V) with 12-core CCDs to AM5, that would be a cool niche product. A two-CCD 24-core psuedo-HEDT on AM5 would be a game changer for a cheaper option to Threadripper.",AMD,2026-02-22 00:51:24,2
AMD,o6pfpft,Literally same build. Iâ€™m on AM4 yet Iâ€™m Able to game at 1440p on OLED at 240 hz on pretty much max everything. Itâ€™s been an amazing experience on the 5800x3d. The 1080 TI of CPUâ€™s,AMD,2026-02-22 02:26:02,1
AMD,o6yeu56,"I assume you mean 9800X3D here.  Since, by your hardware specs, you already did move up to a 9070XT",AMD,2026-02-23 14:25:05,1
AMD,o6jqebo,Itâ€™s much more expensive to produce. Honestly if the ipc lift is good over 9000 then having non-x3d parts at a cheaper price point is a win overall,AMD,2026-02-21 04:07:01,125
AMD,o6jvjoe,"I think people have no idea how AMD's CPUs are made. They make full wafers for enterprise consumers and bin them for power efficiency at a certain performance level. Not every chip can pass this threshold so they get pushed down to the general consumers. Then they get selected to be 8 cores or 6 cores. These chiplets are then joined through infinity fabric to make 12 and 16 cores.  The entire wafer of silicon is paid up front, so them not selling the rejects is wasted money. If you want AMD to only have x3D chips be ready to pay $1000+ for an entry level CPU.",AMD,2026-02-21 04:44:32,81
AMD,o6jmrvo,Everything is about yields.,AMD,2026-02-21 03:42:08,19
AMD,o6ledvr,No. Not all computers are for games only.,AMD,2026-02-21 12:59:40,8
AMD,o6k1vxn,"Adding the extra cache has a cost, there's likely a chance the die fails, and not everyone needs the extra cache.",AMD,2026-02-21 05:33:24,15
AMD,o6k4tyx,Not everyone wants it.....,AMD,2026-02-21 05:57:51,16
AMD,o6lkqkq,"""clearly""?  Please provide citations for that claim.",AMD,2026-02-21 13:42:36,6
AMD,o6m0oqj,"If you're not a gamer and only use your computer for productivity, then the x3D variant performs worse, with no benefit at all for the higher price",AMD,2026-02-21 15:15:44,2
AMD,o6n17ku,"AFAIK the process to join the CPU die with the cache die is whats the limit in manufacturing quantities. So not selling the non x3D CPUs would not make them sell more x3D CPUs, those all sell anyways.  You are also very biased with people who actually know PC parts and care for them. Most people that have a device with an AMD CPU will not even know that fact. Even most gamers don't know what hardware they used and those are already WAY more interested in this topic. Only the faction of gamers or nerds that do build their own PCs will actually know this.  Some will buy a prebuild pc that has the best GPU according to some benchmarks, but most will just buy some machine in their price range, the CPU will surely just work is the maximum thought most people put into it.",AMD,2026-02-21 18:19:27,2
AMD,o6jitrq,Especially with how close the clock speeds are between x3d and non x3d now,AMD,2026-02-21 03:15:33,3
AMD,o6lsnk6,"I don't want an x3d, and I don't want what you Jusr described them too do, that woudk jack up prices for not reason.",AMD,2026-02-21 14:30:47,1
AMD,o6rznrm,"Isn't x3d bad for most use cases when there are multiple chipsets, since the cache is only attached to one of those chipsets and makes it unbalanced?",AMD,2026-02-22 14:38:24,1
AMD,o75ow7b,No thank you; The X version is better for some of us.,AMD,2026-02-24 16:11:41,1
AMD,o6uz3re,"Given it is there just to output to a display, I don't think it matters much...",AMD,2026-02-22 23:21:27,4
AMD,o6lcv2w,Yes,AMD,2026-02-21 12:48:25,7
AMD,o6to64o,Yep which is why I love amd. Just got a 9850x3d and x870 and will upgrade in a few years to a new chip if needed,AMD,2026-02-22 19:19:45,2
AMD,o6l3s4c,Waiting 4 months more isn't gonna kill ya. Also on a 7000 series.,AMD,2026-02-21 11:31:57,13
AMD,o6lqowf,"7000. That's a dream for me! Try X470, 5000, 16GB and SSDs so full they make squeaking noises. My new $2k build is now $4 or $5k - at best. Sucking suckety sucks.",AMD,2026-02-21 14:19:16,6
AMD,o6n5tke,"Yeah, but I'd prefer to keep both of my kidneys for the time being. I might work in IT but my whole PC didn't cost as much as 9960X + mobo, not even considering 4 channel ram",AMD,2026-02-21 18:41:54,34
AMD,o6yf2nl,"Why would you assume that? I play my games at 4k, 5800X3D is still perfectly viable and I'm still mostly GPU constrained.",AMD,2026-02-23 14:26:22,2
AMD,o6kph3b,">Itâ€™s much more expensive to produce.  and more importantly x3d straight up doesnt do shit a lot of the time, so its not even a case of ""it costs 100$ more but is always 15% faster""",AMD,2026-02-21 09:12:47,68
AMD,o6jtvup,Doesnâ€™t mean itâ€™s a win over 9000X3D nor cheaper to produce than the N4 9000X3D (that likely also runs on slower cheaper memory),AMD,2026-02-21 04:32:09,-15
AMD,o6l4zwu,"I think the low end 8/6 core can avoid 3d part, but 12-16 part need to have only 3d chip. Any chip that cannot pass 3D cache can be sold as 8 or 6 core lw end variant.   I think AMD need to simplfy the SKU.",AMD,2026-02-21 11:43:07,-6
AMD,o6krcbb,"But arent the x3D components on the wafer? You'd have entire wafers of x3d chips and other wafers for normal chips. The lower and higher bins you are talking about here would be akin to something like a 5700x3d vs a 5800x3d, then a 5600x3d for the bits with defective cores.",AMD,2026-02-21 09:31:40,-3
AMD,o6k6lhy,"It honestly does feel like most people think AMD just waves a magic wand and V-Cache just gets added to any given chip, for free, with zero downsides. You'd think 9700X beating or meeting 9800X3D in productivity benchmarks with half the max power draw would at least raise an eyebrow but... apparently not.",AMD,2026-02-21 06:13:06,11
AMD,o6m2ggy,not to mention just a simple limit to production probably,AMD,2026-02-21 15:25:07,2
AMD,o6lz48t,Here's an easy one to find. https://www.techpowerup.com/332104/amd-ryzen-7-9800x3d-cpu-accounts-for-almost-90-of-zen-5-sales-rest-of-9000-series-in-trouble?amp,AMD,2026-02-21 15:07:19,-4
AMD,o6m5jqo,https://youtu.be/-InE1Nebnj8?si=JUWC_A602OfkgAXz  Except they don't anymore. End of the video compares them and the x3D variants consistently match or exceed their non3d parts despite lower clocks. 9850x3d showing that they can now reach very similar clocks means the gap widens,AMD,2026-02-21 15:40:53,4
AMD,o6kmfde,I was thinking the main difference is X3D performance when using slower memory as memory is so expensive,AMD,2026-02-21 08:42:32,2
AMD,o6s2kmy,Not really no. The 3d chiplet works great for games an still works really well for productivity. Basically one clocks higher than the other but doesn't have extra cache. The difference is minimal and ends up giving the same or better performance than a non 3d cache cpu would. 9950x and x3d perform the same outside of gaming,AMD,2026-02-22 14:53:31,1
AMD,o75ig30,My old desktop systems become servers once I upgrade. Would be nice to have something with updated hardware encode/decode for transcoding on the cpu for plex. Only reason I regret not going with Intel tbh.,AMD,2026-02-24 15:42:24,1
AMD,o7gnzo2,"Yeah, it's only 2 CU's atm, although I heard a rumor it may go up to 4.",AMD,2026-02-26 04:55:36,1
AMD,o6lboim,"Well it wont, but its not the neccesity why i want it, just looked to please myself with something i would want :-) and end of the year is already far out. Hope at least the price wont be off the charts, i mean i an going for the 24 core part, and while i wont be surprised if its more expensive than 7950x, if its 2x as much, then f them.",AMD,2026-02-21 12:39:17,-2
AMD,o6yh2fo,Because replacing a 5800X3D with a 9070XT doesn't really make any sense.,AMD,2026-02-23 14:37:12,2
AMD,o6lt13c,you need to understand gaming is only 10-15% of the market. making all gaming chips is silly,AMD,2026-02-21 14:33:00,11
AMD,o6llgbb,"AMD needs MORE SKU options, not less.  That's been Intel's secret to success for years - always a product at every price point, with products sitting between every price point...  Build a system, find out you're in too deep financially, but a cheap azz compatible CPU for now, get the better one later.  That's the DIY market.  OEM market is even more focused on saving $10 or $20 of BOM.",AMD,2026-02-21 13:47:04,9
AMD,o6nejfs,Most buyers who want 12 or more cores for their work don't need X3D caching but DO want high clock speeds to maximize the multi core output.,AMD,2026-02-21 19:25:09,3
AMD,o6kunjs,"The highest quality X3D chips are reserved for enterprise (EPYC). For example, Zen III X3D's best dies went to EPYC Milan-X CPUs with the lower bins to 5800X3D, and Zen IV X3D's best dies went to Genoa-X CPUs while the rest went to 7800X3D CPUs. For the 9800X3D, AMD revealed that the bottom stacked 3D cache was only for Ryzen, so if this is true, the recent 9850X3D has the best Zen V X3D dies. In May 2025, AMD released the EPYC 4585PX, which uses Zen V X3D. There is no information if the cache is on the bottom like the 9800X3D/9850X3D or on top of the die like previous generations.",AMD,2026-02-21 10:04:16,18
AMD,o6ne56l,"X3D chips were shaved to expose the special contacts and then bonded to a separate cache die manufactured on a memory-optimized process library. They also bonded a spacer over the portion of each CPU that the cache did not cover, to maintain even contact with the IHS. This is why the early processors were voltage-sensitive and more difficult to cool.  The updated design places cache beneath the CPU to allow for improved cooling. I think they have also made improvements for independent cache voltage and clock speed to try to make X3D processors more reliable.",AMD,2026-02-21 19:23:09,7
AMD,o6kykmp,"Not all X3D caches pass, some are faulty. Every chip cannot be an X3D.",AMD,2026-02-21 10:42:36,5
AMD,o6nl59a,"AFAIK the v-cache is manufactured separately and attached to the compute dies via TSVs. So not only is it more expensive to make because of the additional die, it also adds manufacturing steps.",AMD,2026-02-21 19:59:19,3
AMD,o6kd3a5,"> You'd think 9700X beating or meeting 9800X3D in productivity benchmarks   This is actually just completely wrong. The 9800x3d is faster than the 9700x in all of productivity benchmarks and sometimes significantly faster. In applications like blender and corona, the 9800x3d is roughly 20% faster than the 9700x. In adobe photoshop it's about 10-12% faster. Cinebench it's roughly 8-10% faster. The 9800x3d being slower than the 9700x is just wrong.",AMD,2026-02-21 07:11:47,16
AMD,o6km8rt,>9700X beating or meeting 9800X3D in productivity benchmarks  Me when I lie:,AMD,2026-02-21 08:40:42,12
AMD,o6m0b2i,At one retailer geared more towards gamers. That doesn't mean anything about overall sales,AMD,2026-02-21 15:13:42,8
AMD,o6mkboc,"Most of CPU made arent even goin into consumer products anymore. Mindfactory is DIY retailer, which is an extremely minor volume. Can't even be extrapolated to other retailers.",AMD,2026-02-21 16:54:43,2
AMD,o6n2zf6,It comes at a lot higher powerdraw too tho. The performance per W is a lot lower for the x3D CPUs in certain workloads - its better in other tho. It always comes down what you do with it.,AMD,2026-02-21 18:28:05,2
AMD,o6kmxkz,Oh yeah true   I always forget just how different the impact of ram is on x3d,AMD,2026-02-21 08:47:31,2
AMD,o75pqka,Fair enough...,AMD,2026-02-24 16:15:29,1
AMD,o6yixrj,I never said I want to replace my CPU with a GPU.,AMD,2026-02-23 14:47:02,3
AMD,o6n1np4,If you include server CPUs (which we should because AMD uses the same dies for server & desktop CPUs) gaming surely is even less.,AMD,2026-02-21 18:21:36,6
AMD,o7ag2ka,"well, amd already has the cheap ass CPU which you can upgrade later on, on every platform too. Not mentioning that the cheap one on AMD smokes the cheap ones on Intel...",AMD,2026-02-25 07:43:30,1
AMD,o6nu6md,">The highest quality X3D chips are reserved for enterprise (EPYC).  Presumably binned for power, not necessarily ""highest quality""   >Â In May 2025, AMD released the EPYC 4585PX, which uses Zen V X3D.  1kU pricing for this is the same as the MSRP for the 9950x3d. I think AMD would rather sell this to DIY than servers.   >There is no information if the cache is on the bottom like the 9800X3D/9850X3D or on top of the die like previous generations.  I doubt AMD has designed several variants of X3D for Zen 5 like this. And for what purpose would they move the V-cache back on top?",AMD,2026-02-21 20:47:15,3
AMD,o6lhez9,It used to have merit for Zen 3 first gen 3D V-Cache but it's outdated information,AMD,2026-02-21 13:20:55,5
AMD,o6n289j,"Yea it was true for the first 2 generations of x3D CPUs because they had lower clocks compared to the non x3D CPUs (and no OC capabilities).  But it isn't true for the current gen and won't be true for the coming gens either is my guess. Maybe in extreme overclocking scenarios, but we talk out of the box with PBO.",AMD,2026-02-21 18:24:23,4
AMD,o6kpk7l,*This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*  roof alive sulky rinse sheet imagine violet nail squeeze treatment,AMD,2026-02-21 09:13:39,-5
AMD,o6kskq9,Me when I'm beating in the meeting. ðŸ«¨,AMD,2026-02-21 09:43:58,2
AMD,o6n2pxd,It shows the interests in the DIY market but like you said thats about it.  No surprise people that are passionate about a thing are more comfortable with getting the better product but paying a premium for it. I bet in the time that Mindfactory sells one CPU Dell or other system sellers use 100 CPUs and only like 5 of them are x3D.,AMD,2026-02-21 18:26:48,1
AMD,o6zli9o,I however definitely inferred that based on how that last sentence is written.  My apologies if that was not the implication.,AMD,2026-02-23 17:50:40,3
AMD,o6ozu93,"For enterprise, power efficiency and performance per watt would be their main goals to reduce total cost of ownership, so those qualities are the ""highest quality."" For PC desktop enthusiasts, higher clock speed at worse efficiency is acceptable. I would prefer to undervolt a higher quality die to achieve comparable/near-equivalent performance to stock.  Enterprise customers are AMD's priority because they can sell more units and/or have long-term supply contracts, which retailers and DIY cannot match. AMD prioritizes EPYC over Ryzen.  Historically, AMD keeps costs low, so it's likely Zen V X3D has one unified design with V-Cache on the bottom for EPYC and Ryzen.",AMD,2026-02-22 00:44:23,2
AMD,o6tjl33,">Enterprise customers are AMD's priority because they can sell more units and/or have long-term supply contracts, which retailers and DIY cannot match. AMD prioritizes EPYC over Ryzen.  This may not be the case for the specific X3D server sku you listed vs consumer stuff. While in general this holds true, the 4585PX is an ultra low end server chip that is literally just the consumer chip rebranded, unlike AMD's main server lineup that at least use the server IOD if not N3 dense Zen 5 chiplets.",AMD,2026-02-22 18:57:36,1
AMD,o6m43rc,if youâ€™re not getting top bins then not really worth it,AMD,2026-02-21 15:33:36,46
AMD,o6kxaaj,Do Thermalright heatsinks fit on delidded AMDs?,AMD,2026-02-21 10:30:08,9
AMD,o6xl43l,"I understand that is only for the oc people to get the maximum out of their chips.  But damn, i can remember how bad these athlon xp cpu back then without headspreader where. You just had to look at them in the wrong way and their die broke. Never would i use that for my normal pc ðŸ«£",AMD,2026-02-23 11:00:17,3
AMD,o6zbb3m,"I delidded my own 9800X3D with the Thermal Grizzly delid tool. Wasn't all that bad, just a little stressful as it was my first time doing it. It does suck knowing one bad move and its toast.",AMD,2026-02-23 17:02:49,3
AMD,o72y5a2,Waiting for the 9950X3D2,AMD,2026-02-24 04:29:18,1
AMD,o6kywz4,can buy a 9950X3D for 650 euro,AMD,2026-02-21 10:45:53,-13
AMD,o6rp0df,Ripoff,AMD,2026-02-22 13:37:23,-7
AMD,o6lmfns,"It's cheaper to buy their delid tool and do it yourself. Based on a video I saw on JayzTwoCents, it seems pretty easy to do.",AMD,2026-02-21 13:53:13,-12
AMD,o6tfqlg,When it dies you have no warranty,AMD,2026-02-22 18:40:00,-6
AMD,o6t72ts,RIP Silicon Lottery,AMD,2026-02-22 18:00:40,15
AMD,o6le86x,"You'll need a direct-die cooler for delidded CPUs. Thermal Grizzly also offers a heatspreader which would allow you to mount standard coolers and AIOs, but they do disclose that there may be compatibility issues depending on the mounting kit of the radiator.",AMD,2026-02-21 12:58:32,21
AMD,o6ldfv8,Good question.,AMD,2026-02-21 12:52:44,-3
AMD,o7b5xyu,Same. Still waiting. I really hope this thing will drop Q1 and isn't a tide over for Zen 6 and drops Q3,AMD,2026-02-25 11:38:42,2
AMD,o6lk5wx,Yea but thats not really the point of this product.,AMD,2026-02-21 13:38:55,36
AMD,o6pkj6h,"It is "" easy"" todo.  It's also ""easy"" to break the cpu this way.",AMD,2026-02-22 02:57:51,22
AMD,o6mlsqx,Is the risk also free?,AMD,2026-02-21 17:01:58,18
AMD,o6qqa9k,"""Easy to do"" and you have never done it lol. As someone who has done it the process its stressful af and you can easily fuck it up. I would happily pay extra for someone with better tools and knowledge doing it and gives me 0 risk of being left with a $600 paper weight.",AMD,2026-02-22 08:38:01,3
AMD,o6tjywf,Thermal Grizzly is actually providing their own warranty on the delidded CPU's which I am sure is part of the reason the prices are high.,AMD,2026-02-22 18:59:24,16
AMD,o73le8j,"Thermal Grizzly is a German company, EU means 2 year compulsory warranty.",AMD,2026-02-24 07:37:34,1
AMD,o6u08oz,Seriously tho,AMD,2026-02-22 20:19:58,4
AMD,o6xamqc,"Hey OP â€” Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2026-02-23 09:19:44,0
AMD,o6vo7yg,Excuse me? I already have the absolute best consumer PC money can buy. My backup PC is a 9800x3d with a 5090 FE. Tell me again how I'm broke son.,AMD,2026-02-23 01:47:40,-5
AMD,o6rxopz,"Have you ever used the tool they sell to do it? Thatâ€™s what Iâ€™m referring to. I didnâ€™t say it was easy from experience either. I simply pointed to a video of someone using the tool and said it *seemed* easy to do *based* on that video.   I was simply suggesting an alternative option for people that might want to save some money and are open to some risk. I also pointed to a video, which would suggest some research is required.   I was furthering the discussion, which is what we should be doing on Reddit.",AMD,2026-02-22 14:27:28,1
AMD,o74h03h,Yea but do they cover the amd 9800x3d failure issue?,AMD,2026-02-24 12:18:37,1
AMD,o74qnr9,"They have to cover any issue, that they cannot prove the customer is responsible for.   If it's a known product issue, that is not commonly known to customers they also have to cover them beyond the 2 years.",AMD,2026-02-24 13:20:38,1
AMD,o6ws8o7,Who are you? I said a product was a ripoff (it is) and someone calls me broke? Ad Hominem.,AMD,2026-02-23 06:25:24,-2
AMD,o702wcz,"I am your dad, Jim.",AMD,2026-02-23 19:09:50,1
AMD,o7091p4,Reported,AMD,2026-02-23 19:38:31,1
AMD,o6f5weu,![gif](giphy|djdyvnKHZEpwLAo4Qa),AMD,2026-02-20 13:45:17,129
AMD,o6gjucc,yet my 7900xt gets a driver timeout at 100% load...,AMD,2026-02-20 17:45:38,76
AMD,o6i5uol,I know it's pretty damn unstable but it'll be nice to see even a single benchmark at those clock speeds,AMD,2026-02-20 22:24:21,8
AMD,o6fsbac,4.8 ghz? I can do that with my CPU ... oh wait,AMD,2026-02-20 15:38:43,15
AMD,o6etkii,Nice,AMD,2026-02-20 12:31:20,7
AMD,o6eo0wn,AMD *designed* the card that broke the world record; AMD did not overclock the card themselves.  > overclockers Bill Alverson â€œSampsonâ€ and Splave.,AMD,2026-02-20 11:51:51,36
AMD,o6l28r9,"From what I can gather this is likely caused by problematic vBioses that are running on the edge, barely giving the card enough voltage, some drivers also seem to try and ""overboost"" the card beyond the frequency range, some claim it's because the drivers think it's a 9700XT or something, not sure how true that is however.   But yeah, the 7900XT was a shitshow.",AMD,2026-02-21 11:17:35,14
AMD,o7dc5mq,Same brother. Itâ€™s infuriating. Tried so many things and still nothing.,AMD,2026-02-25 18:24:09,1
AMD,o6htsch,find the source of the problem rather than making a pointed effort to tell everyone that you haven't diagnosed sufficiently enough to find it.,AMD,2026-02-20 21:23:12,-12
AMD,o6esgxm,Bill Alverson works at AMD,AMD,2026-02-20 12:23:50,75
AMD,o6f1rs5,"Bill Alverson is an AMD employee, with Splave, an AMD-affiliate assisting, at an AMD event.  So yes, AMD did in-fact beat the world record.",AMD,2026-02-20 13:22:28,74
AMD,o6jw3jk,"Bro, drivers shouldn't be crashing regardless.   Edit: Here's the reply he deleted:  https://i.redd.it/tp89zfm28skg1.gif",AMD,2026-02-21 04:48:42,14
AMD,o6hxxrm,"man its just a joke...  but how are you so sure that I haven't been tried to diagnose the issue?   -I've returned to base settings on the CPU and GPU.    -I've reset  to default driver setting. I've used DDU and checked 5 different driver versions for stability.  -I've run progams in compatability mode, varying display and limiting game refresh rates.   -Ive turned off Instant Replay, Game bar and Gaming mode in windows.   -Ive run from full screen to windowed mode.   -Ive changed power supplies, removed cable extensions and repasted and re applied thermal pads.   The only thing that had was worked is running a -100mhz offset from stock settings.  Please, do not tell me to ""find the source of the problem"" because I have tried.",AMD,2026-02-20 21:43:45,20
AMD,o6g8qh6,"Indeed, we did :-)  hopefully more to come!",AMD,2026-02-20 16:54:04,52
AMD,o6i6wmx,"for a joke you sure got serious quick.  If you have do any of those things, it's already apparently not a driver issue and most probably a hardware issue. I've personally witnessed a customer return their gpu twice for replacement and their 3rd one was still throwing timeouts, only to finally convince them to return it as well and their 4th gpu has been problem free since.  Considering DDR5 failure rates, instability (yeah doesn't matter if you're running stock/failsafe settings) this is predominantly the majority of people's issues, I'm currently seeing nearly 40% of the 100+ kits i've received as being a problem, though most don't exhibit a memtest failure or OCCT issue, weirdness, specially a timeout at all, has been traced back to the ram, luckily i have a half dozen kits i've been able to validate, but it's amazing how many kits i've tested and back to back repeated issues, doesn't necessarily have to be timeouts, game crashes that don't really crash, just CTD with no error even, swap to good kit, presto, alls well. Nevermind the other hardware, but even the cases where people overlook their surge protector as the cause or worse, their own home wiring being the root cause.  Everything is a variable, under no circumstances is a timeout ""expected"" behavoir short of intentionally doing something that is certain to cause it.",AMD,2026-02-20 22:29:55,-20
AMD,o6kqt9w,"I chatted to some folks on the AMD Discord and got to convince them to get their launch 9060XTs to \~3.75GHz in Heaven Bench with like 55C core temp + 220W + OCing. That GPU clocks like crazy, but the voltages required (or well, power draw despite the lower voltage cap vs last gen...) isn't stable for 24/7. Still hopeful for next gen being the ""World's First 4GHz GPU"" as is AMD tradition (hi 7870 GHz edition, had a 7850 at \~1075-1125 MHz lol).  Totally within reason for ""Stock"" I feel but gonna have to wait, have fun in the labs while you do that before all of us, without LN2! :P",AMD,2026-02-21 09:26:21,7
AMD,o6hrmoy,"I love stuff like this, you have a blog or channel?",AMD,2026-02-20 21:12:32,4
AMD,o6i8bch,Yea how dare they expect the consumer device they purchased to do what it's supposed to! Some people are so entitled,AMD,2026-02-20 22:37:22,17
AMD,o6l2hlk,"This is hogwash, there have been substational amount of AM4 users who reported the issues as well.   Most seem to have resolved the issue by downclocking the card, which seems to suggest an ""ambitious"" vbios voltage curve.",AMD,2026-02-21 11:19:54,1
AMD,o6k9neu,I think something like this is in my near future. stay tuned,AMD,2026-02-21 06:40:14,9
AMD,o6jwo1q,"Yeah, HOW DARE a company produce a product, and it fail to work when the product is installed in a system that already has a fault.  I mean, if you dropped a brand new expensive transmission into a vehicle where the engine was already misfiring, or the differential was already -\_-ed, OF COURSE it's the transmission fault, and any REASONABLE consumer would know that it's the new parts fault.  my gaud the reasoning and logic failure of you people, reinforcing the stereotypes and brain rot of reddit.",AMD,2026-02-21 04:52:55,-11
AMD,o4vpgk1,Time for me to upgrade from 7900x to 9800x3d...,AMD,2026-02-11 22:21:37,13
AMD,o5601ad,I bought my 9070XT a week ago :(,AMD,2026-02-13 14:10:30,4
AMD,o5btiad,does it make sense to upgrade from 7900XTX to 9070XT ??,AMD,2026-02-14 12:08:02,2
AMD,o5xm8a3,"I purchased my new 9070 XT last week, and no code was given. I'm currently in the middle of a dispute with the seller and Amazon. The seller states that AMD is responsible for providing the code, but AMD's official terms clearly state that it's the seller's responsibility. Clearly, there is a breakdown somewhere between these companies in providing the bundle. I have to wait and see what happens with my dispute.",AMD,2026-02-17 21:01:26,2
AMD,o4yg7hw,I just hope it runs good on UWQHD.,AMD,2026-02-12 10:00:04,1
AMD,o5bn123,Any link for me to apply for this?,AMD,2026-02-14 11:09:55,1
AMD,o5ddvf0,What gaming laptops are they talking about lol,AMD,2026-02-14 17:35:18,1
AMD,o5z2lg9,"Ordered a new pc with latest AMD cpu (the 3D one) on 31th of January, got it on 13th of February with no code. CSL computer said that I am not eglible because I ordered before 10th February.  Thatâ€™s how early adopters are punished.",AMD,2026-02-18 01:33:10,1
AMD,o66bjo3,Does anyone know how can I get it ? No retailer from my country is mentioned in the list.,AMD,2026-02-19 02:49:52,1
AMD,o6dm5m8,"I ordered a brand new Lenovo Legion 5 AMD Gen 10, 32gb with rtx5060 from Lenovo's website directly and i haven't received the code. I tried calling today and the customer service agent had no idea what i was talking about and was trying to refer me to the gamepass on microsofts store. After explaining it to him over 5 times, he put me on a 15minute hold to ask his internal team about the code, and came back saying he has no idea what i'm talking about. Anyone have experience purchasing a lenovo laptop from laptop that qualified and get a code?",AMD,2026-02-20 06:10:23,1
AMD,o79u235,time to bought a 9800x3D and a RX 9070 OC 16gb!!! 2 keys\*,AMD,2026-02-25 04:45:58,1
AMD,o4z10gi,Perhaps...,AMD,2026-02-12 12:51:33,3
AMD,o5663aw,"dont know if it still works, but ive seen some people change the dates on their purchase proofs to get rewards",AMD,2026-02-13 14:42:19,3
AMD,o67cv74,Depending on the retailer you may be able to ask for a key.,AMD,2026-02-19 07:17:21,1
AMD,o5gaozb,thats called a sidegrade.,AMD,2026-02-15 03:46:01,8
AMD,o5f7orn,Thinking about doing the same for Resident Evil or CD since Iâ€™m getting them anyway. Might just get another Red Devil,AMD,2026-02-14 23:28:52,1
AMD,o5g4p1j,"Unless you really want better RT and native FSR4, it's not worth it. Effectively the same performance, if not a little worse. You'd have to ask yourself why you didn't go NVIDIA if those were features justifying an upgrade.",AMD,2026-02-15 03:03:22,1
AMD,o73ff0c,Same happened with me,AMD,2026-02-24 06:43:51,1
AMD,o6zdu55,The ones with an AMD CPU and *Nvidia* GPU,AMD,2026-02-23 17:14:44,1
AMD,o6o3vth,[https://www.amdrewards.com/claim-reward](https://www.amdrewards.com/claim-reward),AMD,2026-02-21 21:38:38,1
AMD,o50k9kd,Ordered from Amazon and received.,AMD,2026-02-12 17:32:56,1
AMD,o5brpud,"Probably not, cause you need to ask a the retailer for the Key, its not redeem with AMD directly, so, even If you change the date, of course the retailer know the exacatly date they sold this to you.",AMD,2026-02-14 11:52:49,5
AMD,o6zj182,ROG Zephyrus Duo 16 (2023) with 7945HX and GTX 4090 175w here reporting for duty,AMD,2026-02-23 17:39:16,1
AMD,o6pi16f,Did you receive the coupon code needed to redeem from the seller ?,AMD,2026-02-22 02:41:22,1
AMD,o53nrpb,I ordered a 9070XT from amazon and never got a code emailed to me so I contacted customer support. They acted like they had no idea what I was talking about so they refunded me $78.44 so I can just buy the game.,AMD,2026-02-13 03:17:55,3
AMD,o5bttlr,"Yeah, can't complain much because pc part prices kinda died in my country, the free game rn is barely any consolation",AMD,2026-02-14 12:10:43,1
AMD,o55koc1,Bummer,AMD,2026-02-13 12:41:35,0
AMD,o55qs1i,"Oh no, fungible money!",AMD,2026-02-13 13:19:15,3
AMD,o566dri,Ended up getting the code anyway lol. For once something worked in my favor.,AMD,2026-02-13 14:43:49,3
AMD,o6bj9d4,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-02-19 22:14:23,1
AMD,o6av7zw,"I can see why AMD would do this, rather than use RDNA 4 for lower end mobile products. RDNA 4 is a big leap in terms of transistor count over RDNA 3; to compare two 32 CU GPUs the RX 9060 XT has 29 billion transistors vs the RX 7600's 13 billion.  A lot of those extra transistors went to ray tracing hardware + a side of supporting higher clock speeds.    Mobile iGPUs aren't going to be fast enough for heavy ray tracing games even if they included the RT hardware, and obviously clocks are limited on mobile platforms for power use reasons.  On the flipside though, better hardware upscaling is appealing for low end GPUs.  So adding just FP8 & a few other things to RDNA 3.5 was probably easier than trying to cut back RDNA4.",AMD,2026-02-19 20:16:19,23
AMD,o6d5jte,Oke FSR4 support for RDNA3/2 when ?,AMD,2026-02-20 04:03:06,10
AMD,o6akwvl,I could have sworn we exorcised ISA from personal computers a couple decades ago.   /s,AMD,2026-02-19 19:26:18,5
AMD,o6aeuxy,"They did the amount of changes they needed to RDNA 3.5 to enable FSR 4 and no more.  Instead of having to sit out this generation, mobile users now get frankenstein 3.75 or whatever it will be called by the community.",AMD,2026-02-19 18:57:19,7
AMD,o69yxjq,makes sense. the rt improvment of rdna4 wouldnt make much if any difference for an apu. fsr4 will though.,AMD,2026-02-19 17:42:52,3
AMD,o6d4xjp,"They better support FSR4 with this, given how competitive Intel has become in the mobile graphics dept. I wish they had added the fp8 matrix support in RDNA3.5 itself. At least, they're doing it now it seems.",AMD,2026-02-20 03:58:51,2
AMD,o6dpfr3,AMD cancelled most of the RDNA4 line in order to focus on RDNA5 which will have top to bottom chips. So this could be the stop gap for them to get FP8 FSR working on mobile sooner rather than later. This would have been decided on as the stop gap once they decided to cancel most of the RDNA4 line.,AMD,2026-02-20 06:39:09,2
AMD,o6dzk51,"Could this mean Ryzen 300 and 400 series could also get FSR4, and maybe down the line, RX 7000 and 6000 series?",AMD,2026-02-20 08:12:09,2
AMD,o69lhmj,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-02-19 16:38:17,1
AMD,o6dbman,"But isnâ€™t it interesting a <8w sustained power phone soc can use rdna4, but itâ€™s too much for 15-45w class mobile APU? What does Samsung Exynos do that AMD itself doesnâ€™t do right?",AMD,2026-02-20 04:46:22,12
AMD,o6d4yfi,It's a shame they didn't add (mx)fp4 though. For hardware releasing in 2027 this really shouldn't have been a problem.,AMD,2026-02-20 03:59:01,3
AMD,o6evdc1,Weâ€™re not getting it theyâ€™ve basically given up on support,AMD,2026-02-20 12:43:27,6
AMD,o6hhewx,You need hardware to support that. Emulation performance is horrible and int8 quantization sacrifice quality.,AMD,2026-02-20 20:21:55,-3
AMD,o6dw9ff,Also my first thought.,AMD,2026-02-20 07:41:29,1
AMD,o6avknb,"RDNA 3.5 is very dense for the performance it delivers, you're not giving it enough credit. RDNA 4 would most likely be slower at the same transistor budget.  Although RDNA 3 had scaling issues, it was very good and RDNA 3.5 did fix some of the original issues. Adding FSR4 support would indeed prolong it's lifespan even more.  An iGPU is not a high-end GPU. It's not that long ago that an iGPU was a desktop only device. Look at what we get now, almost everything is playable on these little GPU's.",AMD,2026-02-19 20:18:01,20
AMD,o7bxgaj,> They did the amount of changes they needed to RDNA 3.5 to enable FSR 4 and no more.   Wouldn't it just be easier to make INT8 official rather than modify existing hardware?,AMD,2026-02-25 14:28:40,1
AMD,o6dw9ht,"Cancelled implies they were ever annouced to the public, which they were not. AMD may or may not have had some other designs they decided not to pursue, just like they have for every other product generation.",AMD,2026-02-20 07:41:30,2
AMD,o6l7m22,Every other generation is a stop gap for Radeon.,AMD,2026-02-21 12:05:55,1
AMD,o6iobxf,"> <8w sustained power phone soc can use rdna4  XGFX4 is ""based on"" RDNA4, it's not full RDNA4.   Also limited to 980Mhz.",AMD,2026-02-21 00:07:38,6
AMD,o6ybsi3,"Phones are 1-2W. 8W is closer to a Macbook Air. Hell even Switch 2 measured 8W on total battery usage, meaning 5-6W on SOC.",AMD,2026-02-23 14:08:16,1
AMD,o7dnyav,"Its supported and proven by YouTubers, them not releasing it is them just being childish.",AMD,2026-02-25 19:17:26,1
AMD,o6hnghy,int8 still looks supperior than any other alternative,AMD,2026-02-20 20:51:47,7
AMD,o6j2ym4,Meanwhile in the real world from dozens of public demos on YT....,AMD,2026-02-21 01:35:12,4
AMD,o6dbpyg,Samsung didnâ€™t get the memo when they decided to use RDNA4 in the Exynos 2600 I guess? Or they know something we donâ€™t?,AMD,2026-02-20 04:47:08,2
AMD,o6dwyrw,"It was all leaked awhile ago way before RDNA4 was announced by AMD. We knew that AMD only do a mid range RDNA4 way before AMD said anything officially. Cancelling does not imply they were announced, it just means they scrapped their plans for a full RDNA4 range of products.",AMD,2026-02-20 07:48:00,1
AMD,o6lhhu7,Yes it called capitalism. Buy the next big thing!,AMD,2026-02-21 13:21:28,2
AMD,o6jwah5,What details do we have about the Exynos 2600 and its Xclipse 960 and how it differs from â€œfull RDNA4â€?,AMD,2026-02-21 04:50:07,1
AMD,o6yjrkw,"This is absolutely untrue. Itâ€™s a shame anandtech is now gone, so we lack a proper written review site of mobile SOCs beyond shallow mainstream benchmarks and no power measurements. But hereâ€™s Geekerwanâ€™s review of the Apple A18, watch from 5:00 onwards to around 16:00 :Â   https://youtu.be/QK_t1LfEmBA  A single P core under load can actually result in board power as high as 8w and a Geekbench or 3Dmark run can consume well more than 10w at peak. Even sustained they can be anywhere from 5w to 8w as you can see during the gaming test even after half an hour of running.",AMD,2026-02-23 14:51:21,2
AMD,o7drl7c,The only thing I know is that Iâ€™ll never buy another AMD product,AMD,2026-02-25 19:34:31,1
AMD,o6i15hw,It looks noticeably worse than FP8 and still cost significantly a lot of performance. RDNA3 have no Int8 matrix acceleration anyway so that is quite obvious not intended to run on RDNA3.,AMD,2026-02-20 22:00:01,0
AMD,o6edgh5,"[to achieve higher score at ray tracing benchmark](https://www.gsmarena.com/exynos_2600_inside_galaxy_s26_posts_impressive_ray_tracing_score-news-71590.php), probably. let's wait until they released Exynos 2600.",AMD,2026-02-20 10:22:55,2
AMD,o6jlf06,"Can't get consumers to buy newer GPUs if you don't artificially lock features to the newer generation, especially with the NAND shortages.",AMD,2026-02-21 03:32:59,1
AMD,o6k25ls,"Samsung licensed it, but we don't know the exact differences or changes they made to the design to best fit mobile. They could have added our removed things. Just like MS and Sony do with their custom APUs.Â    Also, just because you can do something doesn't really mean it is worth it. Sure, Samsung might want to look good on RT benchmarks, but why? RT in Mobile is very meh and just a waste of die space. No one is buying a phone because it had good RT.Â    Radeon 9060 is 20% faster that 7600. Some of that will be due to cache and clock increases. Rdna 3.5 likely closes the gap some with arch changes. Very little added cost design and won't double the transistor budget of the GPU side.",AMD,2026-02-21 05:35:37,1
AMD,o6e719t,"> Cancelling does not imply they were announced   It does in this context. We do not have any verifiable information to support the fact they ever existed to be cancelled, only ""leaks"" or rumours more accurately. You can say they may have scrapped plans for some other products because that happens all the time, you cannot say they verifiably cancelled certain products given we don't know they existed in the first place.",AMD,2026-02-20 09:23:14,0
AMD,o6lp0j9,They only have one big thing per decade so theyâ€™re not capitalisming very well I fear,AMD,2026-02-21 14:09:08,1
AMD,o6kq9mq,https://videocardz.com/newz/samsung-exynos-2600-xclipse-960-gpu-said-to-use-amd-rdna4-derived-architecture,AMD,2026-02-21 09:20:45,1
AMD,o71izl7,Sustained means running for hours under unfavorable conditions itâ€™s not the same as peak.,AMD,2026-02-23 23:27:21,0
AMD,o7e21m4,Alternative is just as bad but in a different way as long +12vphwr connector exist which already burned some AMD cards as well because vendors like to experiment with planned obsolescence.,AMD,2026-02-25 20:23:04,1
AMD,o6i80fg,"So? Let consumers decide rather than making the decision for them.  I'd much prefer FSR 4 be properly supported on my 7900 XTX than the current state. The int8 model is miles ahead of FSR 3. It's easy enough to use it on Linux anyways. But, I'd rather it have just been supported than have to take the time I did to learn how to set it all up.  This wasn't exactly a cheap GPU either. AMD really needs to get it together on details like this...",AMD,2026-02-20 22:35:46,6
AMD,o6j32xd,It looks \*slightly\* worse than FP8. But considerably better than FSR3.1 and at decent speeds.   Stop talking shite.,AMD,2026-02-21 01:35:58,6
AMD,o6eel3k,So there is indeed a business case for such architecture in low power premium device regardless right?,AMD,2026-02-20 10:33:14,1
AMD,o6jy1r5,Itâ€™s not like many people are buying their newer gpus anyways.,AMD,2026-02-21 05:03:12,1
AMD,o6k40k3,"9060 has lower core count than 7600. With same core count 9060XT is more like 30-40% faster than 7600.  >Â Rdna 3.5 likely closes the gap some with arch changes.Â   I have seen no meaningful consistent improvements in rdna3.5 vs rdna3 by comparing 880m vs 780m even with slightly faster memory in laptop reviews Iâ€™ve looked at. Even the 890m is barely much faster because bandwidth bottled as fuck even with 8000 mem.  >Â Sure, Samsung might want to look good on RT benchmarks, but why? RT in Mobile is very meh and just a waste of die space. No one is buying a phone because it had good RT.Â   RDNA3.5 is used even in strix halo currently sold in $2000+ laptops. And there are games with mandatory ray tracing where low end gpus can otherwise still run after upscaling and frame gen.",AMD,2026-02-21 05:50:53,2
AMD,o6ee6s9,"Its all words cancelled\\scrapped whatever, but the leaks said they scrapped at least the higher end RDNA4 to focus on RDNA5, and were only going to go forward with a midrange chip, and thats exactly what we got, this was way before AMD said anything about it. You can dismiss the leaks all you like, but it was spot on to what we got in the end. Its not hard to come to the conclusion that AMD decided that focusing on RDNA5 ASAP made more sense wince it is also whats being used in consoles.",AMD,2026-02-20 10:29:36,3
AMD,o6m1rq4,Lol yeah sure ok.,AMD,2026-02-21 15:21:30,2
AMD,o6kqkqe,There is zero technical info lol,AMD,2026-02-21 09:23:55,1
AMD,o71n1fs,"You just pulled that definition out of you behind, not that it matters because they will run at that indefinitely with power source anyways.",AMD,2026-02-23 23:50:18,2
AMD,o6k4cbv,It runs around 2ms slower than FSR3. So quite heavy for a 7900XTX and will be unusable for smaller RDNA3 implementations.  I guess AMD donâ€™t want to cut a feature to only the higher end of their generation. And plus it is a int8 model indicates this may never planned to run on RDNA3. It maybe just an early version of PSSR2.,AMD,2026-02-21 05:53:38,0
AMD,o6k3mpf,"Why buy a new GPU from a manufacturer that consistently overhypes and under delivers, drops support for fairly new cards, and now doesn't utilize new software features on new GPUs for whatever reason when the opensource community proves it works just fine.  AMD isn't doing themselves any favours. They love shooting themselves in the foot when it comes to GPUs.",AMD,2026-02-21 05:47:41,1
AMD,o6krpao,"Here is their source   https://www.thelec.kr/news/articleView.html?idxno=50232   If you have any more, feel free to share with the class.",AMD,2026-02-21 09:35:18,1
AMD,o6kpq7e,"i run int8 on god damn rog ally and it makes games look supperior while running better than native , i allways prefer int8. Im on linux too",AMD,2026-02-21 09:15:21,5
AMD,o6l7i1v,According to that link it is said compute is increased 2x and ray tracing 50%Â   Does not appear to support the claim by some commenters that Samsung only used fsr4 â€œfor ray tracing benchmarksâ€. There is also no mention of whatâ€™s supposedly stripped down from rdna4.,AMD,2026-02-21 12:04:58,-2
AMD,o6ri5ml,Contact cheese and chips blog or asianometry yt channel via email. Send us the results.,AMD,2026-02-22 12:51:41,1
AMD,o6rq7s0,?  What does that have to do with whatâ€™s written in the source the other person posted?,AMD,2026-02-22 13:44:46,1
AMD,o6bj9ew,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-02-19 22:14:24,1
AMD,o69mmjo,"Now, if only RAM would become affordable by the time these release.",AMD,2026-02-19 16:43:47,250
AMD,o69m20u,I hope the rumors of more cores are true. It is damn time to move forward with more cores. This and a expanded cache would work wonders for performance.,AMD,2026-02-19 16:41:00,111
AMD,o69odqb,"I'm curious how they'll do 16 and 20-cores with 2 CCDs. It's almost certainly going to be symmetrical, but they could have a pseudo-X3D thing if they keep as many as they can on one CCD instead.  16 could be 4+12, and those first 4 cores would have the same amount of L3 to access as the other 12. That's the same 3:1 L3 ratio as single-CCD X3D chips get over their non-3D counterparts.  It's not as great for the 8+12 20-core, but at 16 it's an exact fit.",AMD,2026-02-19 16:52:06,12
AMD,o69r1p5,"Ryzen 3, Ryzen 5, Ryzen 5, Ryzen 7, Ryzen 7, Ryzen 9, Ryzen 9",AMD,2026-02-19 17:04:47,12
AMD,o69nip4,"Holy title gore!   >the single-chiplet options would include **6, 8, 10, and 12 cores.** The dual-chiplet parts listed are **16 cores(8+8), 20 cores(10+10), and 24 cores(12+12).**  Dividing the single and dual core configs IMO makes more sense.",AMD,2026-02-19 16:48:02,43
AMD,o69ta8p,"Zen 6 X3D will be legendary, both for gaming but also good enough for the vast majority of 'multi-tasking'/thread tasks.",AMD,2026-02-19 17:15:32,23
AMD,o69jz2d,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-02-19 16:31:01,5
AMD,o69peq8,Iâ€™ll probably get the *800x3d version just before Ryzen 7 comes out to replace my 9700x.,AMD,2026-02-19 16:56:58,3
AMD,o6b1iir,they come in bundle with 4 gb ram,AMD,2026-02-19 20:47:13,4
AMD,o69rvyw,A 12 core unified architecture x3d would make me switch from my ailing 13700k,AMD,2026-02-19 17:08:49,3
AMD,o6b1ty1,Wonder if there will be a 1 ccd 12 core x3d on am5. That would be nice.,AMD,2026-02-19 20:48:47,3
AMD,o6b5aat,"and if it were a 16 core ccd, then we may see 6, 8, 10, 12, 14, 16, 20, 24, 28, 32  stating the obvious.....",AMD,2026-02-19 21:05:28,3
AMD,o6cm3tr,Show me a 10950X3D!!,AMD,2026-02-20 02:00:11,3
AMD,o6e3xce,"Zen6 will be DOA if...not because it is bad far from it! But due to techbro AI bubble gobbling up all ""RAM and NAND"" out from the consumer space.",AMD,2026-02-20 08:53:36,3
AMD,o6ameo8,"I'll be good for a long while, but I'm always excited to see progress.",AMD,2026-02-19 19:33:30,2
AMD,o6apqn9,"10-core / 20-thread single CCD X3D could be the sweet spot between gaming and productivity, without going too high in core count.",AMD,2026-02-19 19:49:28,2
AMD,o6avs1j,is the ram bandwidth still shit on single ccd,AMD,2026-02-19 20:19:01,2
AMD,o6e1gn5,What a weird name for a piece of hardware,AMD,2026-02-20 08:30:06,2
AMD,o6easjg,"I for one am looking forward to (hopefully) single-CCD hexacore R3s, single-CCX twelvecore R7s and whatever cache shenanigans they might additionally think of.",AMD,2026-02-20 09:58:21,2
AMD,o6fhzt4,this is like 100 year old news lol,AMD,2026-02-20 14:48:43,2
AMD,o6c7hsz,It's hard to get excited for new CPUs when you can't afford RAM or storage.   Fucking AI...,AMD,2026-02-20 00:31:19,3
AMD,o6a0g4m,"But why. Seems like a lot of extra products for no reason. Just give us 8, 12, 16, 24.",AMD,2026-02-19 17:50:07,3
AMD,o69t3yl,"I hope we will have a few retail-available six-core Ryzen 3 model on AM5 with Zen 6, assuming Ryzen 5 models get promoted to 8 cores and Ryzen 7 models get promoted to 10-12 cores.   Of course, it will only be good if a Ryzen 3 model gets a launch price in line with a Ryzen 3 model i.e. below that of the lunch prices of the 7600(X) and 9600(X) (which I imagine will see price cuts themselves before launch).",AMD,2026-02-19 17:14:42,1
AMD,o6a7bsq,"I hope this means the return of Ryzen 3, can't keep selling 6 core parts as Ryzen 5.",AMD,2026-02-19 18:22:14,1
AMD,o6aurhy,"Wonder how the segmenting will be, will they finally bring back Ryzen 3's and increase the core-count of Ryzen 5's/higher, will they just segment the higher cores more into the 7's and 9s, or maybe even just add more segmentation to the higher end.  I'm mostly wondering if 6 cores will become Ryzen 3's, 8 cores Ryzen 6's, and how they'll segment the 10 and 12 cores as, and maybe even if they'll add a Ryzen 11 for the 20/24 count.",AMD,2026-02-19 20:14:06,1
AMD,o6aya86,"Number of cores is the less important thing when will come to N14 production node vs actual N4. It's been rumored about 2x iopc and higher frequencies and maybe even less TDP...  Looking forward for a 11950x3dv2 with 24 cores/48 threads, though ..",AMD,2026-02-19 20:31:16,1
AMD,o6enr4t,"Bamboo can grow up to 50inch overnight. This speed is possible because all of the bamboo's growth bodies grow simultaneously: the cache, core, and infinity, all change at the same time.",AMD,2026-02-20 11:49:51,1
AMD,o6fopn7,"Sounds like we'll be getting Ryzen 3 again! That 6 core CPU is for sure gonna be a Ryzen 3.  I guess 8 is Ryzen 5. 12 must be Ryzen 7 then. 10 could be either one, probably 7 though.",AMD,2026-02-20 15:21:35,1
AMD,o6fse2v,I feel like I fell out of love with AM5. 2 memory channels and 28 PCIe lanes is just not enough any more.,AMD,2026-02-20 15:39:05,1
AMD,o6fy0ld,"Cache memory sold separately, you have to solder it to the die yourself.",AMD,2026-02-20 16:05:30,1
AMD,o6g7tnr,"im not interesting in more cores, im intrested in clock speed and pcie lanes",AMD,2026-02-20 16:49:52,1
AMD,o6ir9n7,"More PCIe lanes Is what I really want, would be great for my Plex server setup",AMD,2026-02-21 00:24:25,1
AMD,o6ll0m8,"Everybody's talking about gaming, but I want as many cores as possible for Linux compilation",AMD,2026-02-21 13:44:21,1
AMD,o6nu329,12 cores in same CCX is good. People can have more FPS and more stable FPS.,AMD,2026-02-21 20:46:43,1
AMD,o6oqp31,"I want to see what AMD have done with the interconnect. Of course, Strix Halo was a preview, but CCDs still need parallel operations with that package. I really want to see the CCDs and overall architecture act in a more cohesive way. CCDs make sense for server/datacenter, since those operations are exceedingly parallel, almost to a fault.  Gaming, not so much. There are thread spawns with dependencies that can't be on a different CCD currently.  AMD may have to move to grouped CCDs where two to four form a Super Compute Cluster (SCC). These can be tightly linked dies that can communicate and act as one previous CCD. To link them, they simply need to be rotated to where the edge communication connect is (two sides for server or one side for consumer). It can be decently high bandwidth, matching at least local L3 bandwidth. The great thing here is that this also works for server/datacenter operations, but the real winner is us: consumers. Of course, SCCs can accept V-Cache underneath with clever routing.  The cluster can then accept thread spawns with dependencies across any core with the only cost being latency on farthest cores (some firmware and thread schedulers can handle that). A hardware thread scheduler would be nice, but only necessary for mixed core or hybrid silicon. So, if we have 2x standard 12-core CCDs in a single SCC, it'd operate like a large, unified 24-core CPU.  I wouldn't expect this until AM6, sadly.",AMD,2026-02-21 23:48:26,1
AMD,o69n0d3,As MLID predicted.\ ***LONG*** ago.,AMD,2026-02-19 16:45:37,-5
AMD,o69zhtf,No one's bothered when's there's no ram. You can't even buy a steam deck due to the shortage.,AMD,2026-02-19 17:45:35,1
AMD,o6alsx4,How can I be excited when I can't afford memory for this?,AMD,2026-02-19 19:30:35,1
AMD,o6aw3iy,"I heard they may even come in 28, 32, 36, 40, 44, 48, and 52-core configs.",AMD,2026-02-19 20:20:33,1
AMD,o6cis9w,"Well, my 9950x is now useless.",AMD,2026-02-20 01:39:53,1
AMD,o69l020,Surely they would be doing 6+6 cores for cheaper 12 core CPUs.,AMD,2026-02-19 16:35:55,-2
AMD,o6a8s7h,"It should have at least 48 lanes, like Intel. Otherwise, I'll switch from AMD, having bought it for three series. 28 lanes is unacceptable.",AMD,2026-02-19 18:29:03,-1
AMD,o6a29op,"I want that 24 core, preferably with X3D cache...",AMD,2026-02-19 17:58:37,0
AMD,o6e9ykw,Iâ€™m planning to buy the Ryzen 9 9950X3D. Should i wait for these new CPU? and the question is when do you think they are planning to release them? I have already bought all parts of the PC but the mobo and cpu,AMD,2026-02-20 09:50:46,0
AMD,o69qabd,Does it come with built-in ram too? /s,AMD,2026-02-19 17:01:07,-1
AMD,o6dzllt,24 - $1100   20 - $850   16 - $700   12 - $550   8 - $400   6 - $320 (OEM Only),AMD,2026-02-20 08:12:32,-1
AMD,o6a1rny,It's a crazy time we live in where ram cost more than your cpu. I have a 64 ram kit I got on a sale and it's over a 1000 bucks now. Genuinely insane.,AMD,2026-02-19 17:56:17,103
AMD,o6a0cv5,Just dont use AI and the Market will crash. If you need AI just run it locally.,AMD,2026-02-19 17:49:41,21
AMD,o6bjwil,"No worries!  RAM will be widely available in 6, 8, 10, 12, 16, 20 and 24 month financing options!!!",AMD,2026-02-19 22:17:42,7
AMD,o6ca8m9,"At the moment, I have no arms or legs left.",AMD,2026-02-20 00:47:23,2
AMD,o6f2b00,CPU sales have dropped drastically   It would be in the best interest of Intel and Amd to return to reasonable ram prices   But what do I know,AMD,2026-02-20 13:25:32,1
AMD,o6bnr3u,"Sorry, not possible.  You'll have to settle for CPU prices coming up to match the RAM prices... /s",AMD,2026-02-19 22:38:04,1
AMD,o6cmbgu,This is why I bought extra right as I saw the market go insane.  Got a 96GB kit coming Saturday that has been backordered since the end of October.... $271 w/tax and shipping.,AMD,2026-02-20 02:01:28,0
AMD,o6afx3a,What we need is software to catch up to these cores especially for games,AMD,2026-02-19 19:02:20,61
AMD,o6a2a5a,"Y'all are gonna be disappointed when you find out more cores on a single CCD isn't really a big bottleneck to more performance in gaming.    Plenty of game devs have talked specifically about how managing the CPU workloads across more than 8 cores is just supremely difficult and often a self-defeating ambition.  Not necessarily because of chiplets or chip-to-chip communication, but because of the inherent types of things CPU's do in games that you cant necessarily parallelize and how it's still beneficial to have certain primary tasks being handled by a single core or two, while there's not always a ton left to distribute across the rest of the cores.  Like, you could theoretically distribute some things across even more cores, but it's pointless when you're not actually coming close to saturating any of them.    Of course, if what you want is just more max multithread performance for other sorts of workloads with a 24 core variant or whatever, I get it.  Not super exciting for most people I dont think, though.",AMD,2026-02-19 17:58:41,26
AMD,o6a1w6r,https://imgur.com/5TfVv  But good this time,AMD,2026-02-19 17:56:52,8
AMD,o6aer85,If I can get a 1 CCD 12 core X3D processor to replace my 7950X3D that'd be an instant buy for me.,AMD,2026-02-19 18:56:50,9
AMD,o69nsah,Honestly we need more cache and more memory bandwidth/channels rather than more cores.,AMD,2026-02-19 16:49:18,55
AMD,o6c5f7q,It's time for more cores but they won't help. I can't keep my 8 cores busy as it is. Only very briefly on very specific workloads will it hit 100%. I'm going from a ryzen 5700g to a ryzen 9600x. 8 cores to 6. But up to 45% increased single threaded performance which is what will be noticed. And overall those 6 cores are stronger than the 8.,AMD,2026-02-20 00:19:12,3
AMD,o69wsyy,I donâ€™t know how well games will use more than 16 cores. Amdahlâ€™s Law is strongly observed there already.,AMD,2026-02-19 17:32:26,8
AMD,o69pxlm,"Yes, but I want 32 or more.",AMD,2026-02-19 16:59:26,3
AMD,o69nd4r,Its true,AMD,2026-02-19 16:47:18,2
AMD,o6a2f23,"The rumors are true, check out ""Moore's law is dead"" youtube channel, he's the most accurate on leaks, he knows people on the inside.",AMD,2026-02-19 17:59:19,2
AMD,o6cs1vv,Its fun to speculate but people shouldn't get carried away with it. Wait for Benchmarks as they say.,AMD,2026-02-20 02:36:36,2
AMD,o6e3x08,"16+ cores on a normal consumer level high performance cpu would be an INSANE deal for workstations honestly. From way back we got 2cpu mobo's with xeons in them for the render work pc's. They have a whopping 32 threads lol, high for when we got them, we got some 64 thread ones aswell.",AMD,2026-02-20 08:53:31,2
AMD,o6qrcmx,It is impossible to use more cores for gaming. We only need like 4-6 cores with high frequency and this will essentially be the ideal gaming processor.,AMD,2026-02-22 08:48:12,2
AMD,o6b4hnq,Dude is talking out his ass and does not know jack shit about multithreading,AMD,2026-02-19 21:01:37,1
AMD,o69ocuu,what are you doing that you need that many cores?,AMD,2026-02-19 16:51:59,-5
AMD,o69ozvy,20 cores one  will probably be wors ebjnned 2x12 with some cores disabled.,AMD,2026-02-19 16:55:01,7
AMD,o6a6f87,"interesting idea, but I doubt they would have all that many chiplets with more than 6 defective cores to make that 4 core CCD. Such ""high L3, few cores"" chiplets could also be used in specific server loads",AMD,2026-02-19 18:18:04,2
AMD,o69yy5l,"The four cores would be to few to be useful for gaming though, even with the X3D level of cache ratio. Is there a low thread, high cache workload that's common enough people would pay for that?",AMD,2026-02-19 17:42:57,1
AMD,o6avpwo,"Imagine if they actually create the Ryzen 1, Ryzen 2, Ryzen 4, Ryzen 6, and so on... ðŸ’€",AMD,2026-02-19 20:18:44,2
AMD,o69yapp,"Naming that many configurations is going to be tough, especially on the low end. There are four possible single CCD configurations, but only three product families currently used for those. We might end up in a situation where there's a ""good"" Ryzen 5 and a ""bad"" one",AMD,2026-02-19 17:39:47,9
AMD,o6a33sg,What would be the point?,AMD,2026-02-19 18:02:35,1
AMD,o6c81cq,"My prediction is 12 cores for the full chip, and 10 for disabled chips. This gives us 24, 20, 12, and 10 cores. I don't see how a 12 core chip will be cut down to 6 cores. It just isn't happening. Unless they are also producing 8 core chips at the same time with 6 cores for the defective chips. What this means is there won't be any cheap chips. The ryzen 9600x starts at almost 300 Canadian. I'm not sure about the 7600, but that will be the entry point into AM5.",AMD,2026-02-20 00:34:29,1
AMD,o6c8w4d,"I'm waiting on zen7 myself. I plan on buying a 9600x anyday now and upgrading to zen7 x3d in the future. It will come with another die shrink, so it should be good.",AMD,2026-02-20 00:39:28,1
AMD,o69tds6,"Me too, I specifically stood by 9700X to have a better reason to upgrade to Zen 6 X3D.",AMD,2026-02-19 17:16:00,2
AMD,o6ca521,"There should be. And it should be the number one selling chip, just like the ryzen 9800x3d is today.",AMD,2026-02-20 00:46:48,1
AMD,o6h9s1i,"The rumors are that they will be doing different packaging this time, so it's very likely the link between CCD and I/O will be much faster.  Similar to how Strix Halo is packaged right now.  Direct fanout rather than SERDES over package wires.",AMD,2026-02-20 19:44:43,1
AMD,o6amdju,"Usually they \*are\* trying to give us something like 8, 12, 16, and 24, but due to the nature of silicon they end up with the 6s, 10s, and 20s. if they tossed the weaker bins the expensive ones would cost more to cover for that",AMD,2026-02-19 19:33:21,4
AMD,o6ek5vw,Bad dies,AMD,2026-02-20 11:21:26,1
AMD,o6c9f2m,The zen6 chips will cost more than the 9600x. But we should see the 7600x and 9600x fall in price slightly. Overall AM5 will remain an expensive platform to get into. Unlike AM4 with the 5600 costing a little over 100 Canadian.,AMD,2026-02-20 00:42:33,1
AMD,o69thc8,Whereâ€™s the 7ghz zen 6 then? The 9955X6D? The 9090XT he made multiple vids on nearly a year ago.  As predicted my ass.,AMD,2026-02-19 17:16:29,13
AMD,o6a3ydt,Plenty of people had rumored a move to 12 core CCD's.  MLID 'just throwing shit at the wall' approach.  Eventually some common sense guesses will come true. smh,AMD,2026-02-19 18:06:37,6
AMD,o69njyv,"What is with you and simping for MLID? You did the exact same thing in the intel subreddit a couple days ago.  And you do realize other leakers have said the same thing, ***LONG*** ago too, right?",AMD,2026-02-19 16:48:12,7
AMD,o69q5h0,It's good to get confirmation. Projects get cancelled or delayed out of the blue all the time.,AMD,2026-02-19 17:00:28,2
AMD,o69mfuq,That would only make sense if the yields were trash and most dies were close to being half defective.,AMD,2026-02-19 16:42:53,17
AMD,o69lrmz,"No, why would they? It would be like making an 8 core CPU with 2 4-core CCDs with current CPUs, probably even worse of the yields are not too bad.  Dumping the few CCDs they need to disabile that much on the server market makes way more sense.",AMD,2026-02-19 16:39:36,15
AMD,o6a4nc8,They are small chiplets.  Their yields will overall be pretty great.,AMD,2026-02-19 18:09:50,2
AMD,o69nnw5,"Maybe as an OEM only SKU? Trying to sell 2 different 12 core SKU's with different core layouts at retail is probably not worth the confusion. Your average desktop user wouldn't care about the differences, but for gaming the 12 core SKU would be dramatically better.",AMD,2026-02-19 16:48:43,2
AMD,o69qf5w,They might release a variant later in product life or to select market for parts that failed binning late. I'd expect low volume.,AMD,2026-02-19 17:01:45,1
AMD,o6awsvx,6 of the 12 cores? They'd need some fairly bad binned chips for that to be worth it and I doubt that'd be the case.,AMD,2026-02-19 20:23:59,1
AMD,o6dvol8,"That would be entirely pointless because it would be worse than a single CCD 12 core due to the added latency, not to mention that they would be better off selling their limited 6 core bins as budget CPUs instead to give them something in the low end.  8 + 8 makes far more sense as most of their 8 core yields will be 8 cores and it gives AMD a cost effective creator SKU that should still be solid for gaming in the same way a 7950x or 9950x is.",AMD,2026-02-20 07:36:09,1
AMD,o6hbztq,"I don't think that would actually be cheaper.  The market for a 24-core chip is much smaller than for the 12-core chip.  Yields should be quite good by now, so there will be many 12-core dies.  Supply of 6-core binned dies will be fairly low, and something with that many defects is likely to also have worse performance characteristics, coming from the edges of the wafer.  It's also more expensive to package two dies than one.  There may come a time down the road where a new limited SKU is created to use up particularly good 6-core bins in a 12-core dual-CCD product, but it will take time for those to accumulate, assuming they even choose to hold them back like that.",AMD,2026-02-20 19:55:23,1
AMD,o6hat6c,Why are you counting chipset lanes for Intel but only CPU lanes for AMD?,AMD,2026-02-20 19:49:41,1
AMD,o6c9kbl,I do to but that many cores is useless. Nothing will be able to use it. Very niche applications only.,AMD,2026-02-20 00:43:25,1
AMD,o6c4pae,>ram cost more than your cpu  Yeah. I'm looking at buying the ryzen 9600x and 32 GB DDR5. The ram costs over 500 and the processor under 300. Just wild. My harddrive and video card will also cost more than the processor.,AMD,2026-02-20 00:15:02,19
AMD,o6dy8sw,"I think when I bought my 486DX2 66 CPU many years ago, 1 mb of ram was about 70 bucks. And I had 8mb.  Wonder if we are heading back to those days.",AMD,2026-02-20 07:59:53,6
AMD,o6de829,"A year ago I bought a 96gig kit for $200, can now sell it for a new pc",AMD,2026-02-20 05:05:52,1
AMD,o6enii5,And the gpu im looking at is 269$ amazing time to build a pc.,AMD,2026-02-20 11:48:03,1
AMD,o6rens6,"64GB is more expensive than what I paid for my 9070 XT, and the worst part is that itâ€™s only going to get worse before it gets better. Weâ€™re in for some tough times.",AMD,2026-02-22 12:24:52,1
AMD,o6aj4yv,I just started ordering parts when the prices went through the roof. Now im looking for a combo with 64gb ram and 9800x3d or 9850x3d and mobo ðŸ™„,AMD,2026-02-19 19:17:50,0
AMD,o6a34re,"Sorry, they added it to my work office suite. I'm trying though",AMD,2026-02-19 18:02:43,20
AMD,o6en3s3,"Sorry, but local AI is dogshit.",AMD,2026-02-20 11:44:58,2
AMD,o6avbgz,Itâ€™s not consumers asking for ai. Itâ€™s the military and enterprises,AMD,2026-02-19 20:16:47,2
AMD,o6buu69,What if I find it useful?,AMD,2026-02-19 23:17:35,-2
AMD,o6eanj3,that's like pc cases go up and you stocked 800 of them in your basement. What are you gonna do all this ram.,AMD,2026-02-20 09:57:05,1
AMD,o6agebq,"As always. Hardware first, then software!  If we don't get 12+ cores at mainstream prices we won't get the software for it.",AMD,2026-02-19 19:04:37,37
AMD,o6o2u00,Totally agree! Parallelism and multithreading should be a default for modern programming environments and design methods. I used long ago Occam on a transputer (with many cores) and it was just very natural to use it like that. Somehow serial programming became the way to go.,AMD,2026-02-21 21:33:06,1
AMD,o6c0jqw,"One benefit with 12cores is that it brings with it 50% more L3 cache too, all available to the 1-2 main cores doing the work.",AMD,2026-02-19 23:51:17,4
AMD,o6af4in,"That's cool. What I want is a single CCD with 12 cores. If the games max out on 8 cores that's great, it leaves 4 more cores for the rest of my PC to do the work on other stuff in the background without effecting my game threads.",AMD,2026-02-19 18:58:35,10
AMD,o6ak1pm,Some of us use our computers to make shit and not just play games.,AMD,2026-02-19 19:22:08,13
AMD,o6axhov,Do you feel the latencies between the 2 CCDs in daily use?,AMD,2026-02-19 20:27:23,4
AMD,o6b4lva,What do you use it for,AMD,2026-02-19 21:02:11,2
AMD,o6fupci,Same can't wait to replace 7950x3d with the 12 core x3d,AMD,2026-02-20 15:49:58,1
AMD,o69q9t8,"depends on the task and they know it, trying to create a balance. the cache thing though is only became apparent because of zen5â€™s reuse of io die of zen4",AMD,2026-02-19 17:01:03,29
AMD,o69qhs1,"Why not both? Haha. More cores will improve multithreading in games and apps, too, once they become the new normal.  Just like we used to be stuck with 4 cores, now 8 cores has been mainstream for way too long.",AMD,2026-02-19 17:02:07,18
AMD,o6anwr2,"Epyc customers are first, we get the cpu dies that aren't as energy efficient.   Enterprise wants and gets more cores, we get more cores.   They definitely need to improve the the IO dies and/or the cpu chiplet to IO die communication for the Ryzen lines.  That said, if they are going to 12 cores per CCX and 48mb of cache (keeping to the 4mb of L3 cache per core) then that's a decent bump.   Potentially x3D chiplets could have 144mb of L3 cache each!",AMD,2026-02-19 19:40:45,8
AMD,o6b6i8c,Iâ€™d also shill for more pcie lanes on the consumer. Would love to run multiple NVMEs in system at full capacity without having to bifurcate the top x16 slot. Would love the opportunity to run 2x16 full or 1x16 and one 1x8 if you want 2 additional nvmes.,AMD,2026-02-19 21:11:23,8
AMD,o6c6afa,"For sure, but there are a lot of tasks that work better with more cores, even gaming adjacent ones, like compressing or decompressing isos, emulation, etc.   Again, let the new cores and more cache come first, the software will follow later on.   Just think, for gaming, how raytracing was available since 2018 but it's been only a year or two since mainstream games started using raytracing as the baseline for their games (think Indiana Jones or Doom Dark Ages).",AMD,2026-02-20 00:24:17,3
AMD,o6jpw39,"Being at 95% on all cores doesn't mean you won't benefit from having even more cores, too.",AMD,2026-02-21 04:03:25,2
AMD,o69yz0x,"Games used to only need a single CPU core way back when. Life...uh, would find a way.   Not saying it will be instantaneous, and it really depends on the genres. Just bought Dyson Sphere Program and the devs are testing it on a 64-core monster and really excited for the potential performance increase.   Also, I can totally see complex open worlds using as many cores as you can throw at them, in the future. Imagine having dedicated cores for systems, like, say, the pedestrian simulation takes a whole core by itself. Which would be impractical today, but if you have, say, 32 Cores at your disposal, it's just another tool in the shed.",AMD,2026-02-19 17:43:04,8
AMD,o6a0mm9,"Though my 8 core 9800x3D already does all of this without problem, I run a lot of random crap on other monitors so more core better.",AMD,2026-02-19 17:50:57,2
AMD,o6aest0,Zen 7,AMD,2026-02-19 18:57:02,4
AMD,o6db4cz,Youâ€™re still stuck with 128bit memory bus on consumer platform.,AMD,2026-02-20 04:42:43,1
AMD,o6bnefg,You are like literally a sock puppet account of his?  Your glazing of this conman is embarrassing.    Remember this is the guy who said that Zen 3 would have 4xSMT. lol,AMD,2026-02-19 22:36:10,1
AMD,o6csftx,"Imagine if it ends up being Zen 6%, lol.",AMD,2026-02-20 02:38:58,1
AMD,o69r6ri,4 core users always asked the same thing,AMD,2026-02-19 17:05:28,23
AMD,o6ake6m,Encoding video,AMD,2026-02-19 19:23:49,3
AMD,o69rydu,"I mean, some games can take advantage of more cores very well. Cyberpunk is a good example, there was a bug where it would only use 6 cores on Ryzen and when I did a mod to enable the extra 2 cores I had a significant bump in performance",AMD,2026-02-19 17:09:08,5
AMD,o69p5rv,Yeah I'm curious too.,AMD,2026-02-19 16:55:47,1
AMD,o69p7ow,"Yeah like I said, they're almost certainly just binned identical CCDs, 8+8 and 10+10. But they don't have to be and there are reasons they could do something weird.",AMD,2026-02-19 16:56:02,4
AMD,o6a8pc9,"Yeah I can definitely see those chiplets being rare as hen's teeth. If all the L3 is good, but 2/3 the cores are bad, something funny happened to that die. It's just a fun thought that is now finally (technically) possible if they're doing a full 1.5x of a typical CCD for Zen6.",AMD,2026-02-19 18:28:41,2
AMD,o6a0xir,4 cores is not too few to be useful in a gaming workload. There are still a number of Esport titles that really only load 1 or 2 cores in any significant way. Chasing high FPS in those titles comes to mind.,AMD,2026-02-19 17:52:22,4
AMD,o6a11yj,"lower core counts would be useful for handhelds. For sub 15w gaming, you want as much of the power budget you can go to the igpu.",AMD,2026-02-19 17:52:57,1
AMD,o6engoz,Let's just call CPUs by EAN code at this point.,AMD,2026-02-20 11:47:41,2
AMD,o703ihy,Ozempic ridge,AMD,2026-02-23 19:12:40,1
AMD,o6a1n3q,Yeah unless they add more ryzens   3 would have to be 6 core  5 would have to be like 8-10 and maybe 12  7 is either the beginning of dual CCD or 12 cores  9 is whatever is above 7.  I think the 6 core 100% makes sense as R3 cause the ryzen 5 has always been a half of a dual CCD ryzen 9 so lowest dual ccd 16core/2= 8 core R5,AMD,2026-02-19 17:55:41,8
AMD,o6crbha,"Why not keep waiting until Zen 8, by then the node shrink will be tapped out and be mature. Or wait until Zen 9, then Zen 8 will go on sale. Personally I'm waiting for Zen 10 myself, that may be endgame of AM6.",AMD,2026-02-20 02:32:08,11
AMD,o69ywxe,I'm doing the same. I got a solid MOBO and RAM ~10 months ago and paired it with a used 7700x just for this moment. Can't wait for these to start rolling out!,AMD,2026-02-19 17:42:47,1
AMD,o6adlot,"Pretty much, I also got a significant discount at time of sale. The 9800x3d was around $200 more, not worth it.  It will be good though, msi x870E board at the moment.",AMD,2026-02-19 18:51:26,1
AMD,o6an2hd,Hence why I mentioned 8 and 16 core. If they use 12 core ccds they would disable some cores for the 8 and 16 variants.,AMD,2026-02-19 19:36:41,1
AMD,o6chbf0,"> Overall AM5 will remain an expensive platform to get into.  DDR5 wise, yeah. CPU wise, we have 7500F, 8400F or 9500F.",AMD,2026-02-20 01:30:40,0
AMD,o6c9pnc,"> Whereâ€™s the 7ghz zen 6 then?  AMD hasn't even announced a Zen 6 product yet.  > The 9090XT he made multiple vids on nearly a year ago.  That was clearly labelled as speculation on his part, IIRC.",AMD,2026-02-20 00:44:17,1
AMD,o69tzvk,"ALL of them might have easily been in the plans and cancelled the last moment.  That was especially obvious with 9950X3D2, which was practically announced by OEMs.  For a **LEAK** source, it's not a mortal sin to occasionally be wrong.",AMD,2026-02-19 17:18:57,-11
AMD,o69o1tv,">What is with you and simping for MLID? You did the exact same thing in the intel subreddit a couple days ago.   Just reacting to irrational hate towards him.\ AFAICT he is by far the best source for leaks.\ If there are better ones, I'd like to know.  Credit where credit is due.",AMD,2026-02-19 16:50:33,-3
AMD,o69nspx,Show me at least one before him.,AMD,2026-02-19 16:49:22,-1
AMD,o6hbllf,"If I buy an Intel K490 CPU and a Z990 motherboard, I'll have 48 lanes. If I buy an AMD 10950x CPU and an X970e motherboard, how many lanes will I have? If I still have a total of 28 lanes, I won't buy the AMD.",AMD,2026-02-20 19:53:28,1
AMD,o6e730x,My C compiler will happily use as many cores as I throw at it.,AMD,2026-02-20 09:23:42,0
AMD,o6c9mea,Please don't jinx it.  AMD can always raise the price to a line to their Threadripper prices.,AMD,2026-02-20 00:43:45,12
AMD,o6o1nij,Great times! Those had the build-in copro right? Wonderful time to start studying and working. I started with C end of the 80s (and later C++ and assembler). Between all the management now and then still programming in C# and C++ (and simd with AsmJit) on a new AMD. Still love all the new developments ( AVX512 is wonderful).,AMD,2026-02-21 21:26:50,1
AMD,o6av7q1,If you are going for an x3d cpu I assume itâ€™s for gaming. There isnâ€™t a game in the world that needs 64GB ddr5. Why spec it?,AMD,2026-02-19 20:16:16,0
AMD,o6a51kc,same here but i give even less effort and if something is wrong (which often is the case) i just tell them i used the AI tool they forced me to use. Many of my coworkers also do this and management are thinking of removing AI.   You can do the same.,AMD,2026-02-19 18:11:40,18
AMD,o6dyrk2,"Our City IT just sent email that they are adding CoPilot to our office apps, I will be disabling all features if at all possible. My laptop already has issues running without slowing down as it's on the wrong side of the 3-5 year swapout period.",AMD,2026-02-20 08:04:45,3
AMD,o6fa2j4,Nah. You just have an obsolete computer.,AMD,2026-02-20 14:07:27,1
AMD,o6de5un,Then you aren't allowed to complain about RAM prices.,AMD,2026-02-20 05:05:24,5
AMD,o6fa6p7,"That's nothing like it at all.... That's the dumbest comparison I may have ever heard, actually.",AMD,2026-02-20 14:08:04,0
AMD,o6aipmd,"Bleh, not really. Usually programmers can ""just"" design an app to dish out parallelizable tasks, which scales to any number of cores. This is already the case with video encoding, code compilation, browser processes.   It's much rarer in games because this kind of software design is hard and requires a fatass brain.",AMD,2026-02-19 19:15:47,12
AMD,o6avy31,We don't have games that use 8 cores still,AMD,2026-02-19 20:19:50,24
AMD,o6deqg8,Very true.,AMD,2026-02-20 05:09:47,2
AMD,o6c8in6,50% increase in cores with 50% increase in L3 is a wash.  But that's also why potential 8 core variants with 48MB of L3 could mean great gaming performance improvements.  That's a nice improvement in L3 cache-per-core.,AMD,2026-02-20 00:37:18,6
AMD,o6apvfh,The extra 16mb of shared L3 cache is a nice bonus too,AMD,2026-02-19 19:50:07,10
AMD,o6aijgk,If the other 4 cores are working on background tasks then why do you care if they're on the same CCD?,AMD,2026-02-19 19:14:58,7
AMD,o6atxue,"The thing is that when expanded L3 cache is mentioned, it's almost always in the context of gaming because it's basically the only workload that takes advantage of it. High core counts are definitely useful in a ton of workloads, but having expanded L3 cache on all cores of a high core count chip isn't all that valuable.",AMD,2026-02-19 20:10:00,5
AMD,o6bllqu,"Wow, it's almost like I addressed that in my post.",AMD,2026-02-19 22:26:37,1
AMD,o6j1jms,No.,AMD,2026-02-21 01:26:11,1
AMD,o6a37tu,"Both might be ideal. Afaik most game engine still actually do most of the work on 4 threads, we didn't go far on that front, actually I would say a lot of recent games use a huge amount of cpu for basically nothing as physics, npc ai etc didn't improve much over the years. I would say 6 cores are now mainstream, not 8 cores.",AMD,2026-02-19 18:03:07,13
AMD,o6aumxx,"Not all workloads can be scaled linearly over more cores. In fact, basically every workload will have diminishing returns with core counts at a certain point. Even with the 6-8 core norm we have now, the majority of a gaming workload doesn't scale beyond 2-4 cores.",AMD,2026-02-19 20:13:27,9
AMD,o6clnke,">Epyc customers are first, we get the cpu dies that aren't as energy efficient.  >Enterprise wants and gets more cores, we get more cores.  AMD is starting to differentiate their server vs client chiplets with the introduction of the dense lineup. Zen 6 is rumored to have the dense chiplets be the mainstream server offerings too now with the dense chiplets getting the full 4MB of L3 per core like the standard stuff get.",AMD,2026-02-20 01:57:24,3
AMD,o6cegca,They sadly wonâ€™t do this because it would cut into TR and Xeon sales,AMD,2026-02-20 01:12:57,4
AMD,o6bfzcz,"These new chiplets allowing denser core configurations are 100% targeting servers, not gamers.  Speaking as a coder whose specialty is performance optimization. Amdahl's law is a simple math that can tell you much performance improvement you can get by adding more cores, and can help us understand diminishing returns.  Lets say 90% of an app's work will multithread perfectly. That is, if you run it on an 8 core machine, that work'll be 8x as fast as on a 1 core machine.  That 10% that can't multithread perfectly is dragging you down \*a lot\*. On an 8 core machine, your overall app will be running only about 5x as fast. On a 32 core machine, it'll run about 8x as fast.  Bump that number up to 99%: by 32 cores you are still only going 24x as fast. Games don't get near even 90%. Most software doesn't get near 90%.  So, outside of some very very specific games with embarassingly parallel workloads, a 24 core CPU will not really help gamers over a 16 core CPU. Pumping all your TDP budget into making 8 cores run as fast as possible with X3D is the real way to go for games and I don't see this changing in the next decade.",AMD,2026-02-19 21:57:45,6
AMD,o6a31uw,"It's really not gonna be like that.  Today's games still dont even tend to scale well past 6 cores.    There are fundamental problems with trying to distribute CPU gaming workloads across loads of cores.  They just aren't that easy to parallelize, especially some of the heavier logic tasks that you want to keep pinned on a main primary core.  Stuff that is kind of fundamental for the game to function properly.",AMD,2026-02-19 18:02:19,16
AMD,o6c67ls,I can wait. 5950X will do fine. I had an i7 3770 from 2012 before. I think it will even take longer than Zen 7.,AMD,2026-02-20 00:23:50,2
AMD,o6e2gvn,"That was when he had just started out, but anyway he's been more accurate than anywhere else, I'm not sure why you'd say he's a con man ðŸ¤·â€â™‚ï¸  It's pretty much agreed that zen 6 is going to be 12 core ccd's, just come back here after it's been confirmed by AMD, I bet you won't.",AMD,2026-02-20 08:39:49,1
AMD,o69y4dh,And we used to have a single core until the mid aughts. More parallel performance is always good.,AMD,2026-02-19 17:38:56,4
AMD,o69rr5v,Does not answer my question,AMD,2026-02-19 17:08:10,-1
AMD,o69rgg7,So nothing? Got it.,AMD,2026-02-19 17:06:45,-7
AMD,o69svg1,I can see use cases where from 6 to 8 cores makes differences. But I just dont know many use cases that going from 8 to 16 for example makes a meaningful difference for the price jump.  There is a point that the hardware is not the limitation. Its the software/developer that is the bottleneck. Multithreading is not a trivial thing in the development side.,AMD,2026-02-19 17:13:33,1
AMD,o6a2m5u,"Hmm. 4 esport cores and 12 left over for streaming encode? Maybe, but I wonder what demand would be like?",AMD,2026-02-19 18:00:14,1
AMD,o6a1phl,"Right, but they're not going to pop a 16 core Ryzen 9 into one of those. A purpous built APU would be used",AMD,2026-02-19 17:56:00,3
AMD,o6a4oe9,"They're not throwing a full CCD into a handheld outside of something like Medusa Halo. A dedicated handheld chip (Z3 from Ryzen 500 series) would probably be used instead.  On the handheld front I expect a 6-core CPU, likely 3+3, and an 8-12CU iGPU and an Extreme SKU at 8-12 cores on the CPU (4+6 seems very doable) and a 16CU iGPU. Those GPUs should be RDNA4m but given how long they seemingly want to cling to RDNA3.X, it could also easily be that.",AMD,2026-02-19 18:09:59,2
AMD,o70fq9b,For Age-related macular degeneration (AMD)?,AMD,2026-02-23 20:10:02,1
AMD,o6a90gc,It's not even adding. It's just reintroducing an abandoned category.,AMD,2026-02-19 18:30:10,10
AMD,o6d05ek,"Nah it'll be a different naming scheme and it has to be confusing so they'll do this:  It will be ~~Intel~~ AMD ~~Core~~ Ryzen P Ultra G85X (G = Intel current gen + 1) and ofcourse P for performance tier  For example (And i feel like they'll do something worse)  AMD Ryzen 7 480X for 8 Core  AMD Ryzen 7 485X for 10 Core  AMD Ryzen 9 Ultra 490X for 12 Core  AMD Ryzen 9 Ultra 495X for 16 Core  AMD Ryzen 9 Ultra 497X for 20 Core  AMD Ryzen 9 Ultra 499X for 16 Core  Then for the 3D we'll have to add Ultra to Ryzen 7s too if they have a 3D version, Example:  AMD Ryzen 7 Ultra 480X3D for 8 Core",AMD,2026-02-20 03:27:15,3
AMD,o6avuyl,"100% at most Ryzen 7's will be the full 12 cores, I could even see them maxing out at 10 cores just for Ryzen 7 so they can leave the full 12 cores as a Ryzen 9 just so they could price it higher, I could also even see them introducing the 20/24 cores as Ryzen 11's.",AMD,2026-02-19 20:19:24,2
AMD,o6ekr62,Ryzen 5 10750XHX3D4X4GTi Turbo Edition it is.,AMD,2026-02-20 11:26:17,1
AMD,o6ady4t,I bought a 7700x at the start of AM5 and with the idea to upgrade to the latest X3D that my mobo will support eventually. I'll have to be patient. I'm also not going to be buying on release but with its cheaper. Just like many people bought 5800X3D after 7800X3D was released.,AMD,2026-02-19 18:53:03,2
AMD,o6aovpm,"and when some of the 8 cores fail to meet spec they end up with 6 cores like the 5600x3d, that can keep happening up the stack  A 24 core dropping down to a 16 core because 4 cores failed just feels like mismanagement to me.  Everyone would be paying for those 8 lost cores, yet they could have subsidized every other chip by selling it as a 20 core",AMD,2026-02-19 19:45:23,1
AMD,o6a449d,"Or, and most likely, he just makes shit up.  He always does this.  He claimed at one point Zen 4 would have a 25%+ IPC increase! lol  And dont forget all the massive hilarity he put out with his Radeon predictions.  His RDNA3 ones were particularly laughable.",AMD,2026-02-19 18:07:23,5
AMD,o69ulc9,â€œA 7ghz cpu was being plannedâ€  ![gif](giphy|LBb735fuQwRKAVzN23),AMD,2026-02-19 17:21:48,-1
AMD,o6a4cb0,His hate is entirely deserved.  Dude basically lies for a living.,AMD,2026-02-19 18:08:25,7
AMD,o69rfrz,"It's not irrational. Remember how Pantherlake was ""Pathetic lake""",AMD,2026-02-19 17:06:39,2
AMD,o6c9vr3,Nah. Intel is priced competitively at the lower end. They need to keep up the competition.,AMD,2026-02-20 00:45:16,7
AMD,o6b0o6i,X3d for gaming but I use it for other things as well hence the higher RAM amount. Otherwise yeah.. I'd settle for just 32.,AMD,2026-02-19 20:43:07,10
AMD,o6mfarj,"not entirely true, Star Citizen, does on many occasions cause over 40gb Ram use with nothing but Discord in the background (at 1440p). I bought 64gb back in 2024 specifically because of Star Citizen, back then I paid $200 for the 64gb Gskill Flare x5 6000 cl30.   \[Imgur\](https://i.imgur.com/JcNp5Jb.png)",AMD,2026-02-21 16:29:44,1
AMD,o6afoz4,"I really like that approach, thank you. They are really pushing AI and it's been nothing but a detriment.",AMD,2026-02-19 19:01:14,7
AMD,o6afp20,This is the way.,AMD,2026-02-19 19:01:15,2
AMD,o6fa9fy,4090 is obsolete?,AMD,2026-02-20 14:08:29,3
AMD,o6ilsod,you do you bro I'm just asking what use is 96gb ram.,AMD,2026-02-20 23:52:59,-1
AMD,o6ak5or,"Why would you design a game for a hardware that doesn't exist? In practical terms, hardware always comes first in gaming.",AMD,2026-02-19 19:22:40,11
AMD,o6hrkny,"It highly depends on the workload if it can be parallelized or not, or to what extend. Thread synchronization has a cost, so if a certain workload needs a lot of it, there's a point where adding more thread makes performance worse.  Code compilation is single threaded, BTW. What you can do is compile several files simultaneously. Build tools generally do that automatically, assigning one file compilation to each CPU core.",AMD,2026-02-20 21:12:16,2
AMD,o6i5rv4,"I think the last ~5 years have proven that games are indeed very hard to scale. During Zen3 generation a lot of people extrapolated need for more cores for games but 8c have stayed just fine. Next gen consoles are also rumored to be 8c/16t. While I'd really have liked to see more headroom (10c or 12c) just in case since console generations are likely getting even longer than the usual ~7 years, it seems we're stagnating at 8c indeed being 'fine'.",AMD,2026-02-20 22:23:56,1
AMD,o6cejvh,Quite a few do. Iâ€™ve seen Europa Universalis V use all 64 of my threads,AMD,2026-02-20 01:13:33,8
AMD,o6jownl,We've had games that use 8 cores since 2012! Black Ops 2 will use 8 cores in Zombies.,AMD,2026-02-21 03:56:34,2
AMD,o6c8mnw,Except we do ? BF6 won't perform well unless your CPU has 8 cores at a minimum.,AMD,2026-02-20 00:37:58,-6
AMD,o6c9ck1,"Only a wash if the game uses all twelve cores.  But since most games is limited to 1-2 cores for their main threads, then those 1-2 cores goes from 32mb to 48mb L3 cache.   Same benefit as the 8 core 48mb L3 variant.",AMD,2026-02-20 00:42:09,6
AMD,o6aq52p,Requires an OS that can intelligently keep threads on the right CCD or use of a 3rd part solution like process lasso,AMD,2026-02-19 19:51:24,11
AMD,o6chadr,"We can turn off SMT for a boost in frequency and %1 lows, or less power consumption at the same frequency.",AMD,2026-02-20 01:30:29,3
AMD,o6iajrj,"12 core CPUs not all on the same CCD are 6+6, not 8+4, so it's not the same thing.",AMD,2026-02-20 22:49:15,2
AMD,o6blw3q,Exactly.  Wont make much difference there.,AMD,2026-02-19 22:28:07,1
AMD,o6bls3v,">Â it's almost always in the context of gaming because it's basically the only workload that takes advantage of it.  Not really true.  AMD wouldn't have made Vcache a thing if it was purely for gaming, I dont think.  They use it for Epyc CPU's, which are absolutely NOT doing any gaming. lol",AMD,2026-02-19 22:27:32,11
AMD,o6au97s,Can you show me where in this comment chain or article title that L3 cache is mentioned,AMD,2026-02-19 20:11:34,-4
AMD,o6boseh,You did but you downplayed other uses. Like content work. Servers. That's half the reason people want more cores. This sub thinks the whole computing world revolves around gaming but we're realizing these companies are more than happy to disregard that whole segment to make more chips geared towards data center.,AMD,2026-02-19 22:43:41,4
AMD,o6k9cua,Mine was a rhetorical question. Then...,AMD,2026-02-21 06:37:34,1
AMD,o6ckk54,"> Even with the 6-8 core norm we have now, the majority of a gaming workload doesn't scale beyond 2-4 cores.  I so agree, I went with 6 cores years ago to stay lower on power draw. I would be willing to go to 8 or 10 core for the next build if that happens to be in the sweet spot price and power wise. But I'll let price and power decide. I can stay with 6 if that is the deal.",AMD,2026-02-20 01:50:43,3
AMD,o6kvih1,"There's probably a market for an X3D chiplet + dense chiplet part, in fact the Zen5c 16 core die isn't terribly dissimilar in area to the regular Zen5 chiplet IIRC? if they wanted to they could drop a 24 core Zen5X3D x8 + Zen5C x16 SKU right now largely using silicon they're already making (would probably need a new PCB?) - If nothing else it'd solve the scheduler issue of having two sets of cores that are both the fastest or the slowest depending on context.",AMD,2026-02-21 10:12:42,1
AMD,o6cixdv,"All true.   But the extra cores can allow for more simulations, more systems, more things beyond what there is now. Rather then chase diminishing returns speeding up what there already is, more cores can allow the possibility of more.  But hardware always has to come first, or its not worth coding the rest, caz no one will be able to run it.",AMD,2026-02-20 01:40:45,3
AMD,o6a9l7n,"I know it's not easy but the main motivation right now to improve things is the base hardware... which is the 5 years old PS5 and Xbox Series consoles. They don't have more than 8 cores, hell, for the games themselves they only have like 6 cores available, as some of it it's reserved for the O.S.  I have read interviews of John Carmack, who is one of the smartest game devs in history, and he said multicore gaming is super hard. So, I believe him.  But also, once most PCs and game consoles have, say, 16 strong cores, I just don't see the devs not trying to make use of all that power at their disposal.   Like, we are normalizing 8 cores for gaming these days? That was science fiction in the 2000s.",AMD,2026-02-19 18:32:51,5
AMD,o6ak4sl,"you don't buy tomorrow's CPU for today's games, you buy it for tomorrow's games.  all it takes is Unreal Engine 6 releasing with real multithreading support and suddenly every new release is going to benefit from these cores",AMD,2026-02-19 19:22:33,1
AMD,o6a37b2,But not all workloads can be parallelized.,AMD,2026-02-19 18:03:03,6
AMD,o69ww3l,"Anything beyond 8 cores isn't really intended for gaming. 3D modeling, virtual machines, rendering etc will make use of way more cores than games. I can get my 5800X3D's 8 cores 16 threads to 100% usage VERY easily, if I just open a few virtual machines and something like a DaVinci Resolve or a Blender project at the same time.",AMD,2026-02-19 17:32:52,3
AMD,o6a1lox,"There are some games that will gladly take advantage of the extra cores. Dyson Sphere Program has been putting out dev logs about overhauling their threading system to take full advantage of the entire CPU, no matter what CPU you have (one person tested with Threadripper). More simulation type games like that will definitely want threads to handle ticking all the independent bits and pieces.",AMD,2026-02-19 17:55:30,2
AMD,o6e9ygz,"You could still use the other cores for gaming just those 4 cores would be really fast. The new leaked Xbox also has 3 zen6 cores and I think 8 zen6c cores. So only 3 full power cores. Like the commenter above you said, only the first few cores are really important for gaming, the rest can be slower.",AMD,2026-02-20 09:50:45,1
AMD,o6cr03x,Damn budget level CPUs have been dead for so long people forgot there were R3s back in the day.,AMD,2026-02-20 02:30:11,2
AMD,o6aptwb,Where are the 14 core chips then? What if only one core per ccd is bad on a 9950x configuration?,AMD,2026-02-19 19:49:54,1
AMD,o6a9xvz,Did he specifically claim IPC or performance? Because the latter definitely increased by around that amount.  As for RDNA3. Even AMD got the expected performance wrong. I can give a lot of leeway for that since the architectural issues only seemed to be found much later in development. RDNA3 was meant to be a lot better than it turned out to be.,AMD,2026-02-19 18:34:28,1
AMD,o6a8bm2,"Eh, more often it seems people have great trouble with setting expectations from a leaks channel bringing out news for products that are still years out.",AMD,2026-02-19 18:26:53,1
AMD,o69tn79,"Yes, he is sometimes wrong.\ Which is not a death sentence for a **LEAK** source by any means.",AMD,2026-02-19 17:17:14,0
AMD,o6nv6nu,"Theyâ€™ll both go up. Weâ€™re getting cheaper luxuries to distract from the fact that weâ€™re getting priced out of our necessities, and even if manufacturing costs go down after the AI crunch, MSRPs wonâ€™t. Seems very cynical when just looking at RAM prices but it became apparent that computing as a whole is something common folk are getting priced out of when I saw the exact same Samsung 870 QVO 2TB SSD going for $380 right now that I bought for $180 in September of 2021. Itâ€™s not even an NVMe drive, itâ€™s a 2.5â€ SATA drive ffs ðŸ¤¦â€â™‚ï¸  Source: ask gun guys how much ammo prices ever went back down after 2020 lol",AMD,2026-02-21 20:52:37,4
AMD,o6btats,"48 also exists too, i feel like people forget about it",AMD,2026-02-19 23:08:45,4
AMD,o6iwxoc,Yes. Only 24 gb of VRAM in 2026? Bruv youâ€™re cooked,AMD,2026-02-21 00:57:36,2
AMD,o6g3mp5,Just one of them certainly is for running modern workloads.,AMD,2026-02-20 16:30:57,1
AMD,o6iysaz,AI and virtualization.,AMD,2026-02-21 01:08:59,2
AMD,o6eajzm,"Think about it this way: making a game *not* scale with single core performance but with multicore performance is next to impossible atm. There is always the â€žrenderâ€œ thread.  Now the moment your game scales linearly with core count you donâ€™t need to do anything to utilize 8 cores instead of 6, or indeed 24 cores and 48 threads. The issue is we still have the render thread and lots of other threads for AI, physics and whatever. And the only thing we can do with more cores is run more AI and more physics and whatever. So they can make games more detailed or realistic with more cores, but not render faster. In fact, often enough the middle of the road core counts clock a little higher and have better performance *per core* compared to the highend ones.",AMD,2026-02-20 09:56:12,4
AMD,o6hsic6,">Code compilation is single threaded, BTW. What you can do is compile several files simultaneously.  This is the ""dish out parallelizable tasks"" part I mentioned. I don't know how this is different from what I said.  Not everything is parallelizable, I thought that much was obvious.",AMD,2026-02-20 21:16:52,1
AMD,o6kp38f,"""Use"" as in put some 5-15% load on all cores, or use as in putting a 80% load on all cores?",AMD,2026-02-21 09:08:54,3
AMD,o6kqmp1,"Allocating some work to more than 6 cores doesn't mean there's anything significant scaling. Disabling SMT/hyperthreading is still overall a performance boost for gaming purposes to this day.  https://www.pcgameshardware.de/Ryzen-7-9850X3D-CPU-281833/Tests/Benchmark-Review-1490440/2/  An 8-core has 33% more performance than a 6-core, how many games can you find where the difference is half that at 16%?",AMD,2026-02-21 09:24:28,2
AMD,o6js3m6,"Very minimal amount of games can effectively use 8 cores, u cant really be serious. Also cod is now trash",AMD,2026-02-21 04:19:10,1
AMD,o6cxykw,>BF6 won't perform well unless your CPU has 8 cores at a minimum.  wtf are you talking about? [I've been playing BF6 with my 6 cores Ryzen 7600x very well.](https://youtu.be/nA72xZmUSzc?t=488),AMD,2026-02-20 03:13:09,12
AMD,o6c9y9t,"Yea, that's true enough.  The increase of L3 to 48MB would easily be the most exciting aspect of all this.  At least in terms of gaming.",AMD,2026-02-20 00:45:41,2
AMD,o6aw8k9,Or for you to just have the correct AMD drivers installed that already do this for you?  > AMD 3D V-Cache Performance Optimizer Driver,AMD,2026-02-19 20:21:14,0
AMD,o6avz9a,The top of this comment chain.  >I hope the rumors of more cores are true. It is damn time to move forward with more cores. This and a **expanded cache** would work wonders for performance.,AMD,2026-02-19 20:19:58,10
AMD,o6db0oi,And that would be relevant in a thread about epyc and/or Threadripper and not consumer ryzen.,AMD,2026-02-20 04:41:58,1
AMD,o6c0a2s,I didn't downplay anything.  I even started out my post making it very clear I was talking mainly about those concerned with gaming.,AMD,2026-02-19 23:49:42,1
AMD,o6d4v9i,"and specifically, the HW of the average consumer needs to come first, so basically the $200 range",AMD,2026-02-20 03:58:24,2
AMD,o6bjqqf,"Multicore anything is hard. People finally need to get off C++ for their multi threaded engine parts and ship that component in Rust. This is how Firefox finally managed to do multicore layout.Â    Before they haven't tried that, there's still room to optimize. Yes, some tasks can't easily be parallized but there's still a lot of room left before you run into marginal performance increases.",AMD,2026-02-19 22:16:53,6
AMD,o6bmx5d,">But also, once most PCs and game consoles have, say, 16 strong cores, I just don't see the devs not trying to make use of all that power at their disposal.  Again, Carmack isn't talking bullshit.  If he says it's hard, then what do you think the vast majority of game developers are gonna be thinking?  There has to be a worthwhile payoff for the work done, and that's just not at all clear here.  You can easily break a lot of things trying to parallelize CPU workloads in games, it's not just a matter of 'using more cores'.  The entirety of engine programming and how games work as a whole are still built heavily around the more demanding game logic work being pinned to a single core because that's how you get the most reliable and performant results.  It will take like a whole paradigm shift for all of game development for this to change, and I dont see where the motivation for that will come from.  Especially when game development is already so damn hard as it is, and developers hamstrung for time.",AMD,2026-02-19 22:33:36,5
AMD,o6i9bkv,"I agree about gaming and 8c. But for how long though? 5 years? Probably fine. There would need to be a paradigm shift. But the next gen launches in 1-2 years, and will then last for the typical ~7 years, possibly longer considering the stagnation and cost these days. That could be ~10 or more years in total. I would be more hesitant about putting money on that long of an era.",AMD,2026-02-20 22:42:43,2
AMD,o6b4xrg,"People have been saying this for more than decade. That day just never comes. Like UE5 is almost completely bottlenecked by ST performance on PC, once you move past 4 threads.",AMD,2026-02-19 21:03:47,8
AMD,o6bm56j,We've had 'real' multithreading support for a very long time now.    And my point is that you cant just keep scaling and parallelizing CPU workloads in games like you can GPU's.,AMD,2026-02-19 22:29:26,3
AMD,o6a1m50,"Yeah that's what I mean, I use Da Vinci to edit videos but honestly, CPU is definitely not the bottle neck for what I'm doing haha.  My impression is just that ppl just ask for more and more stuff that doesn't actually make difference on what they are doing.  I'm not saying more cores is useless. But I would be surprised if more that 5% of the customer base actually need 16+ cores for example.  IMO there is more stuff that's is a higher priority right now than core count.",AMD,2026-02-19 17:55:34,3
AMD,o6ar1wn,"It may be impossible for only one core to be bad, but given that you're already wanting less SKU it seems understandable that they would attempt to minimize SKUs in a way that each bin has one or two lower tier models instead of every possible combination",AMD,2026-02-19 19:55:47,1
AMD,o6c1ips,"Literally claimed IPC specifically.  >As for RDNA3. Even AMD got the expected performance wrong.  Bud, MLID clown was trying to say that the top RDNA3 part would be 250% faster than the best RDNA2 GPU and would trounce anything Nvidia had. lol  Wild horseshit.",AMD,2026-02-19 23:56:48,1
AMD,o6bntk1,"No, the guy pushes some obvious fucking whoppers.  The guy is LYING most of the time.  Of the time he's not, it's either common sense shit any half informed person could guess at, or he's repeating leaks he's seen elsewhere and trying to act like he has some personal, separate confirmation of those leaks(or just hopes other people dont know about those other leaks saying this stuff first) and on rare occasions he does get some legit info.  A channel telling lies is not 'news'.  They are a tabloid and deserve to be regarded as trash.",AMD,2026-02-19 22:38:26,2
AMD,o6an9wb,"call it what it is, a **YAP** source.  sometimes that is leaks sometimes that is lies.",AMD,2026-02-19 19:37:41,1
AMD,o6okovp,Fortunately zen6 is coming with a 6 and 8 core variant. Those should be cheap enough and similar in price to the ryzen 9600x and 9700x. I was worried that 10 cores was going to be the entry point for zen6 and AMD was going to bring the prices way up. They probably will at the high end. I won't be surprised if the 24 core chip ends up costing 1200 to 1400 Canadian. By comparison the ryzen 9950x3d costs 950. I think with zen6 AMD is manufacturing both an 8 core and a 12 core CCD. There is no way the 6 core and 8 core parts are cut down 12 cores. Fortunately those massive core chips are practically useless and unnecessary since using that many cores is very difficult. We only need the entry level 6 core chips because those bring massive single threaded performance. The 12 core x3d chip is going to be very nice as well. The successor to the ryzen 9800x3d.,AMD,2026-02-21 23:11:53,2
AMD,o6zcmoo,"Not enough competitors in the market does this. America and Europe need their own factories for making Dram among other chips I believe for prices to stabilize but I fear that unless some good person in Government says no way to these insane prices they don't have to so they won't lower the price, (they are making money).  You want to win America and Europe as a whole then you have to beat the competition, America has all it needs to produce Dram from the Earth right now and the chips Do Not have to be built with rare earth's if the process is changed and the concept changed as to its design for any country looking to start their own factories.  Hope everyone has a wonderful day/night",AMD,2026-02-23 17:09:03,1
AMD,o6kql42,60% load while loading the game and in certain strenuous mapmodes such as the road mapmode,AMD,2026-02-21 09:24:01,3
AMD,o6npn1j,"Do you have an article that's not in German? Also, are you really trying to say it's just better to not have more cores? What if people do more than just gaming, such as edit a video of a game with the game itself open?",AMD,2026-02-21 20:23:02,1
AMD,o6kyd2k,"So now that your earlier point has been disproven, you dismiss the truth by saying that game X is trash? You realise this makes you sound like a moron, donâ€™t you?   Many games support high core counts, and hardware improvements always precede software improvements.",AMD,2026-02-21 10:40:35,3
AMD,o6jvo9t,"A minimal amount of games? Every game I regularly play will use all 16 cores on my 3950X: BeamNG drive, Helldivers 2, GTA Enhanced Online, Universe Sandbox and Satisfactory. I'd be VERY surprised if that was the entire list, considering how old some of them are.",AMD,2026-02-21 04:45:28,1
AMD,o6chh1d,Most exciting is the improved IO die and ram speeds as thatâ€™s currently the bottleneck for zen5.,AMD,2026-02-20 01:31:39,5
AMD,o6dgrk9,"Content work means photo, video, music, streaming, etc. So uhhh no people are indeed doing that with consumer hardware. These chips aren't just for gamers, sorry to burst everyone's bubble.",AMD,2026-02-20 05:25:31,1
AMD,o6bln9w,"This is straight from Tim Sweeney stating it's the biggest limitation of UE [https://www.youtube.com/watch?v=477qF6QNSvc&t=5211s](https://www.youtube.com/watch?v=477qF6QNSvc&t=5211s) and that UE6's goal is to solve that  and given that he predicts UE6 preview is 2-3 years out, I can at least believe that's a realistic timeline.  I don't recall hearing Tim suggesting UE5 was intended to solve this but perhaps he did say that in the past",AMD,2026-02-19 22:26:50,6
AMD,o6bshza,"you certainly can keep scaling games just commonly used engines like UE5 do not support this natively yet  and it's only a matter of time for games to begin utilizing more cores once the next console generation sets the next floor for headroom.  sure if you're someone buying a new CPU every generation there's probably no gaming benefit to anything above 8 core, but someone wanting to use a PC for 5+ years should probably consider a 10 or 12 core next gen so they can still keep up with console gaming.",AMD,2026-02-19 23:04:10,5
AMD,o6ns0ul,"Here's a test done by Techpowerup: [https://www.techpowerup.com/review/amd-ryzen-7-9850x3d/17.html](https://www.techpowerup.com/review/amd-ryzen-7-9850x3d/17.html)  Techspot: [https://www.techspot.com/review/3082-amd-ryzen-9850x3d/](https://www.techspot.com/review/3082-amd-ryzen-9850x3d/)  I'm not saying it's better to have fewer cores, but if the choice stands between having some fast cores, or a larger number of slower cores, a consumer should always pick the fast cores.  For CPUs with multiple types of cores, there's also a need for proper scheduling. It's working mostly fine on Intel CPUs since the thread director provides necessary information to the OS for proper scheduling, but AMD's 79X3D and 99X3D chips show some issues.",AMD,2026-02-21 20:35:39,1
AMD,o6p1gfp,"it has not been disproven go ahead and google yourself, single core speed is still the most important for gaming",AMD,2026-02-22 00:54:22,1
AMD,o6k7fsj,"that is a minimal amount of games are you kidding, single core is still the preferred metric for most games",AMD,2026-02-21 06:20:26,1
AMD,o6dirdw,"Most of these either want more memory and memory bandwidth than consumer platform can provide, or they are very lightly threaded. Why do you think even mid to low spec macs do so well in the creative space? A mixture of software and extremely high single thread performance. A consumer cpu going from 16 to 24 core with only minimal increase in memory support (just ram speed) and no difference in pcie lanes isnâ€™t going to make or break it.Â   Thatâ€™s why people are asking for more memory channels and pcie lanes in the comments.",AMD,2026-02-20 05:41:41,1
AMD,o6c36dl,"I agree with that but it's interesting to note that Tim has wanted to do this [since 2009](https://www.highperformancegraphics.org/previous/www_2009/presentations/TimHPG2009.pdf) (see the slide on Software transactional memory) so it's not like this will be their first attempt at this, but hopefully now they have the resources/talent it can be done.",AMD,2026-02-20 00:06:17,6
AMD,o6c04xa,>you certainly can keep scaling games just commonly used engines like UE5 do not support this natively yet  Game developers I've heard talk about this seem to believe otherwise.  But I'm sure you know better.,AMD,2026-02-19 23:48:52,2
AMD,o6nzoce,"Yeah, I'm definitely on board with having faster cores for sure. A perfect example of cores VS speed is AMD VS Intel in the Bulldozer days. That's partly why I'm actually hesitant to see AMD add more cores if they are just Zen C cores. But if we could have the best of both worlds, that'd be great.",AMD,2026-02-21 21:16:25,1
AMD,o6qog6z,X3D CPUs have shown for a few years that that isnâ€™t the case anymore.   How behind are you?,AMD,2026-02-22 08:20:14,1
AMD,o6noypw,Do you really expect me to play more games on a regular basis than that? Those are just the ones that I play regularly.,AMD,2026-02-21 20:19:29,0
AMD,o6c1agi,Well as I said in another post I'm just stating this based off Tim Sweeney's quote specifying a 16 core CPU in his example for describing UE6's goals of multithreading  [http://youtube.com/watch?v=477qF6QNSvc&t=5211s](http://youtube.com/watch?v=477qF6QNSvc&t=5211s)  Seems odd to specify 16 cores if that's impossible to achieve,AMD,2026-02-19 23:55:30,2
AMD,o6p1mf5,im talking about most games everyone plays not just what you play lmao what kind weirdo are  u,AMD,2026-02-22 00:55:22,0
AMD,o6c81st,It'd be worth more if Tim Sweeney wasn't a washed clown.  He hasn't been involved with actual game programming for DECADES. lol,AMD,2026-02-20 00:34:33,1
AMD,o6nx8c2,"And GCN lives on, amazing.",AMD,2026-02-21 21:03:32,32
AMD,o6r6h5d,"Another day, another Linux Banger",AMD,2026-02-22 11:12:45,12
AMD,o6rknrz,Is it time to switch from windows?,AMD,2026-02-22 13:09:23,9
AMD,o6snnfg,![gif](giphy|46hMzlIbVpWPJAWdUY),AMD,2026-02-22 16:31:35,1
AMD,o6lyitw,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q1 2026, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1q1efc5/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-02-21 15:04:06,0
AMD,o6s49rr,It's getting close to critical mass for user adoption and package software development.    But I think the cohesiveness of windows will keep a hold of the consumer market for a long time.  Linux distros are just too fragmented for everyday consumers to make a choice and use one.,AMD,2026-02-22 15:02:13,16
AMD,o6tzlam,You can try linux with a live image.,AMD,2026-02-22 20:16:40,3
AMD,o702f1e,Not if you have an Nvidia card.,AMD,2026-02-23 19:07:36,3
AMD,o6v5xr2,"Year of the Linux, bonafide.",AMD,2026-02-23 00:00:32,1
AMD,o7b8g6b,"Im not saying its perfect right now, but i made the switch to CachyOS a month ago and regret not trying it sooner. If your main games work i'd say you should give it a try",AMD,2026-02-25 11:57:57,1
AMD,o6us5lu,"IMO, the idea that you *have* to choose the ""right"" or perfect distro to start with is what's doing more harm than good. Pick a distro that seems to come pre-packaged with what you want and go from there. At the end of the day, you'll still be able to run the same desktop environments and software. Don't like the desktop environment your distro came with? Install something else -- you don't have to reinstall the whole OS to swap if you don't want to.  If someone doesn't know where to start, just pick one that seems popular if for no other reason than it'll likely be easier to find solutions to problems you'll run into learning the new environment. Or ask for recommendations based on what they want to do. For someone on this subreddit, I would almost always recommend something like Bazzite for most cases just due to the gaming focus and image-based nature. But if they wanted to run Fedora, Ubunutu or even Arch, they can still do all the same things, even if it means installing a few more packages out of the box.",AMD,2026-02-22 22:42:59,5
AMD,o6u0ema,I think I will go with Ubuntu just because it is the most popular,AMD,2026-02-22 20:20:48,2
AMD,o7atonh,"it really doesn't matter much anymore, except for a few distro's, but they will probably not be on the radar for beginners. there are still differences ofcourse, but they're only important once you get 'under the hood', which most people probably will not (need to) do.",AMD,2026-02-25 09:50:55,1
AMD,o6usc1h,I agree. But thats not going to be the way normal consumers look at this.,AMD,2026-02-22 22:43:58,3
AMD,o7cusnt,Most people don't want this. Especially Devs. They want a computer they can startup and code on without having to figure out the OS or interact with it too much. Some devs are techies that find that stuff interesting. Most see a computer as a vehicle to accomplish something. That's why Windows and Mac will always be the first choice for desktop OS for most Devs. That's just not the Linux desktop experience. Great for servers though.,AMD,2026-02-25 17:05:37,2
AMD,o73zhvh,"Not sure why you are being down voted. Ubuntu is the defacto standard in general.  For me personally, there's a few easy reasons.  Xilinx targets Ubuntu for their releases specifically.  Steam does too.  Jetbrains packages their software for Snaps.Â   They support LTS versions for 10 years. I still use Ubuntu 22.04 and Â it is my default target.",AMD,2026-02-24 09:51:25,5
AMD,o6ut36z,"Yeah, that's true, but that just means it's up to us to educate and help people get over that distro choice paralysis and just start using *something* (within reason).",AMD,2026-02-22 22:48:05,2
AMD,o7f2g52,"This is certainly a take, I'll give you that.",AMD,2026-02-25 23:19:00,1
AMD,o7f8zyi,"Computer enthusiasts are in the minority. Especially people that want to be tweaking their OS. Most devs aren't computer enthusiasts. They are people that went to school for computer stuff because they were decent at math and thought it would pay well. There are lots of devs that are computer enthusiasts of course (I'm one), but most just want something that works for them to code on.",AMD,2026-02-25 23:55:15,2
AMD,o7f9hwu,"This feels like a chatgpt response that's heavily influenced by someone who enjoys vibe coding. Or, at the very least, someone who isn't at all familiar with what we're talking about.   Please stop.",AMD,2026-02-25 23:58:00,0
AMD,o7fg63i,Two responses where you can't say anything substantial as to why you disagree and you've already devolved into insults. I'm sorry for whatever I said that upset you friendo. I hope your day gets better.,AMD,2026-02-26 00:34:30,1
AMD,o7cyfif,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q1 2026, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1q1efc5/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-02-25 17:22:29,1
AMD,o7gcl97,Giving 20% of AMD to Mark Zuckerberg and Sam Altman is certainly a choice.  What's next? 10% Larry Ellison? 10% to Elon Musk?  Lovely bunch of people here.,AMD,2026-02-26 03:40:17,29
AMD,o7ge89y,I thought Nvidia bagged Meta recently?,AMD,2026-02-26 03:50:24,5
AMD,o7gnugj,Gigawatts seems an odd choice of measuring business deals. Why not some monetary unit?,AMD,2026-02-26 04:54:35,5
AMD,o7gloe8,RIP AMD,AMD,2026-02-26 04:39:28,6
AMD,o7funu6,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q1 2026, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1q1efc5/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-02-26 01:56:34,1
AMD,o7gsg2p,Does the OpenAI deal still stand if Meta boost the stock?  Does OpenAI still need to first buy the first gigawatt?,AMD,2026-02-26 05:27:57,1
AMD,o7gsppe,ðŸ¤¡,AMD,2026-02-26 05:29:58,1
AMD,o7go4e2,"Each of these people is both good and bad, like all humans. Except Sam Altman, he is a con artist.",AMD,2026-02-26 04:56:31,0
AMD,o7gsnmh,The ceo did say each gigawatt was worth double digit billions,AMD,2026-02-26 05:29:32,1
AMD,o7gv15g,"> Except Sam Altman, he is a ~~con artist~~ demon wearing a skin suit.  Even inside the group of those four... *no-one* gives me the cold-shiver creeps like he does.",AMD,2026-02-26 05:47:37,1
AMD,o6p5qis,"Steve very briefly alluded to something that I want to bring up because I think some people have really short memories, and this certain something really bothered me.    AMD initially did not want and did not allow Ryzen 5000 series cpus to work on the 300 series chipset AM4 boards.  A lot of people praise AMD for the AM4 platform, but I do not think this is really deserved.    AMD has to release a bios ""blob"" to the motherboard manufacturers, who use that to make bios updates for their boards.  When Ryzen 5000 cpus were released they would not work on the A320, B350, and X570 boards.  BIOS updates for those did not exist, and it was AMd's fault.  AMD was giving the most loyal customers of theirs, the early Ryzen adopters, a big middle finger.  They wanted people to upgrade motherboards instead.    Ryzen 5950X review from HUB was on  Nov 5, 2020  [https://www.youtube.com/watch?v=zsfvRw74h30](https://www.youtube.com/watch?v=zsfvRw74h30)  BIOS updates for the 300 series chipsets was announced in March of 2022 and started being available in May 2022.  [https://www.techpowerup.com/292955/amd-brings-official-ryzen-5000-support-to-300-series-chipset-motherboards-circa-2016](https://www.techpowerup.com/292955/amd-brings-official-ryzen-5000-support-to-300-series-chipset-motherboards-circa-2016)  That was a huge gap in time.  Was it really a technical issue?  I don't think it was.  AMD wanted people to upgrade boards, like what Intel does.  I think AMD's reversal of their initial decision was not done out of the goodness of their hearts.  The people who had 300 series chipset boards who wanted a cpu upgrade had to upgrade their board.  If you had to upgrade your board, why not consider all possibilities, as in why not also consider Intel?  LGA 1700 cpus were faster for gaming, so why not get one of those?  I think AMD realized people were jumping ship to Intel so they finally released the bios update to manufacturers.  The point is that they were not trying to do the right thing for their customers.  In my opinion they don't deserve so much credit for the long lifespan of the platform as everyone gives them.",AMD,2026-02-22 01:21:26,215
AMD,o74y8r3,I went from a 1600 to a 5800x3d. The performance jump was insane and I didnt even have to change my old B350 Mainboard,AMD,2026-02-24 14:02:27,21
AMD,o76akx4,"Upgraded from a 1700 to 5800x (missed out on the 5800x3d when it was at non scalp pricing) on the same msi x370 mobo 7.5 years later lol, blessing they decided to update the bios on the older boards to be compatible with 5000 series.",AMD,2026-02-24 17:49:25,4
AMD,o6onlea,"It's nice, but if you bought a 1700x over an 8700k, you've just spent years with some of the worst performance possible.",AMD,2026-02-21 23:29:22,26
AMD,o752r2h,Francesco Totti,AMD,2026-02-24 14:26:14,1
AMD,o73ueme,Is Ryzenâ€™s hidden feature self destruction? Currently only active on Asrock mobos I suppose lol.,AMD,2026-02-24 09:02:15,-5
AMD,o6nfwx7,"Soo, this is just ""Grrr AMD, everyone raise your pitchforks"" video, without admitting the reason why AM5 wont last long.   PCIe.   AMD admitted when they first released AM5, they didn't see the rise of NVME being important. AM5 only supports 24 PCIe lanes maximum. Thats 1 GPU and 1 NVME drive and the motherboard normally takes 4 lanes for its own useless gimmick and your done.   Thats it, no more expansion. Diddly Squat.  AM6 will focus on expandability again. With PCIe 6 just around the corner as well as DDR6, for AMD to support those, they will need the create new socket. Also rumours of a PCIe x32 slot going to exist.   Clueless idiots feel free to comment below.",AMD,2026-02-21 19:32:09,-39
AMD,o72u18y,"Steve is even worse than Steve.   At least Steve is good at his job, when he is not wasting everyone's time with BS.",AMD,2026-02-24 04:01:14,-13
AMD,o6plkdt,"People have a short memory, and refuse to believe that corporations aren't our friends.",AMD,2026-02-22 03:04:48,146
AMD,o6q97wr,"It's a fact that boards with smaller BIOS capacities could not support all AM4 processors, which means to update those boards to support Zen 3 required removing support for other processors.  Basically none of these low-cost boards had support for updating the BIOS without a processor installed.  This creates a support problem that AMD probably wanted to avoid.  I doubt the board partners were complaining about the prospect of people being forced to buy new motherboards, either.  In the end, what AMD wanted to avoid wasn't what their customer base wanted, so they had to take on the support burden to allow Zen 3 to run on those old AM4 boards with low BIOS capacity.",AMD,2026-02-22 05:59:53,97
AMD,o6pwk4x,"Fact is, we don't know what reasons there were behind the scenes. We do know that the limited BIOS size has meant that support for newer CPUs needed dropping support for older CPUs. It is very possible that there were all of technical, marketing and sales reasons for the initial lack of support Zen 3 on older motherboards.",AMD,2026-02-22 04:20:43,22
AMD,o73pngj,This is a bit of revisionist history. AMD didn't want to allow mobo manufacturers to remove support of older cpus to allow the new cpus. This was 100% because of small bios chips that couldn't support every cpu. Eventually the backlash forced them to allow it.,AMD,2026-02-24 08:16:53,23
AMD,o732ora,You have to remember that AMD also had to keep their board partners. If they release new CPUs that donâ€™t require a mother board upgrade then the board partners get upset because AMD gets more sales and the board partners donâ€™t.   Obviously I as much as anyone donâ€™t like this. Iâ€™d rather just upgrade my CPU if my board supported all the functionality. But businesses also need to earn money to stay in business.   Not that any of it matters anymore. I canâ€™t afford a desktop computer and havenâ€™t had one for 6 years now :(,AMD,2026-02-24 05:01:35,5
AMD,o72zlno,"Im sure motherboard manufacturers like msi, asrock, asus, and gigabyte had no input on this matter...",AMD,2026-02-24 04:39:29,4
AMD,o73yjm9,I think you mean the X370 boards. X570 boards had Ryzen 5000 BIOS updates basically day one. Iirc even a handful of 400-series boards got left out for a bit due to also not having sufficient space but can't recall which ones.,AMD,2026-02-24 09:42:17,4
AMD,o73w0x0,You make lots of assumptions. Then again the opposite opinion is rooted in similar assumptions about amd's good will as well. Therefore I choose not to take either side as the definitive fact on this matter and so should you unless you have some concrete evidence to support your claim.  What you're doing is that you choose to believe what you want to believe.,AMD,2026-02-24 09:17:55,3
AMD,o73x5nl,As someone now on a B550 and upgraded from a B350 I have no complaints with this and waiting awhile to get 5000 support for the B350 series just made perfect business sense.   It's not like AMD would really profit much for forcing users to upgrade their motherboards anyway so I don't think it was necessarily something malicious in intent from AMD,AMD,2026-02-24 09:28:51,3
AMD,o74zns2,fun fact: AMD even attempted to [force stop beta bios release](https://reddit.com/r/Amd/comments/mzzuth/amd_is_going_out_of_its_way_to_prevent_further/) to disable unofficial 5000 cpu support,AMD,2026-02-24 14:10:06,3
AMD,o71vycx,"i guess i'm imagining the plethora of asrock boards that had bios updates made available that allowed the 300 series boards to have 5000 support essentially at/prior to launch... which kinda throws a wrench into the overall claim, leaving a combination of it being ""amds fault"" paired with board vendors.  there's something that needs to be addressed, and either willfully omitted or simply attempting to gaslight perhaps.... not sure, but if you're going to go to the extent of elaborating, odd that you'd make a point of neglecting to mention the variables at play and insisting on painting the purely negative picture you want people to ""remember"". One should be aware that as i mentioned, there were several bioses that launched in tandem with the ryzen 5000 series that were then immediately pulled, the reason, well of course it was entirely amd's fault, let's not consider the variables at play. Even for a small business doing deployments, things have to be validated and verified, a task that can take months to years depending on the context and circumstances. The same limitations and restrictions that applied to past sockets that allowed for significantly newer cpus that weren't initially even thought of had similar limitations and delayed bios/product support out of the box, some boards never to receive it because they never completely supported it. So let's play out the realistic circumstances of those variables, and this isn't in defense of a company that so many will immediately cast aside as fanboism or something, but the logic and reasoning for imposing a restriction in order for curb failures or catastrophic issues due to lack of proper testing and validation, or failing to perform up to the standards expected, nothing like getting hauled into court, potentially a class action lawsuit due to the newer cpus failing to operate properly as advertised on old initially released am4 boards, or worse, discovering that many were turned into paperweights, possibly loss of data among other hardware for blindly supporting it out of the box. It really bothers me that there's always the group of people, with the insane mentality that everything has a nefarious purpose and reason, that there isn't any logical or reasonable explanation beyond their own narrow mindset that it's always for the dollar, that there is nothing else making the decisions overtly. Do board vendors want to sell newer boards, definitely, no differently than a laptop manufacturer wanting to sell more laptops, the desire is for some form of obsolescence to occur, be it support or lifespan due to failure or whatever. NO one is discounting that as a significant variable, but the paint the entire premise that it's EXCLUSIVELY this is either pure ignorance at play, or willful intellectual dishonesty.",AMD,2026-02-24 00:39:39,8
AMD,o74nd8g,"""e point is that they were not trying to do the right thing for their customers. In my opinion they don't deserve so much credit for the long lifespan of the platform as everyone gives them.""  Usually, i'd give some push back but you are 100% correct.  I was one of them with a AMD 5 1600.  I wasn't upgrading my mobo to get the 5000 series.  But they they are still doing the same shit.  They put their  RDNA2 (my poor rx 6800) out to pasture already, I immediately jumped ship to Nvidia, at least they will SUPPORT their shit.    AMD will only support their products it their is a short term gain. Fuck AMD, I need someone that will ride with their product and not just dump it after they look at something shiny.",AMD,2026-02-24 13:00:55,2
AMD,o7csz5f,"i'm going to give the credit as long as they keep delivering, zen 6 will be on AM5 so that's three generations of cpu on one motherboard,  whereas nova lake will be on a new socket, and arrow lake's LGA 1851 was a one and done. AMD appears committed to supporting AM5 until DDR6 rolls around at the very least. I do believe in letting companies have credit when they do things better regardless of their motivation in doing so, and mercilessly taking it away when they stop, never fanboy.",AMD,2026-02-25 16:57:13,2
AMD,o6r0ifq,"I got shafted by this, I will always remember.  I felt that I had to buy a suboptimal Zen2 CPU instead of waiting for the X3D since AMD for quite some time had everyone convinced Zen2 is the last upgrade for early AM4 boards.",AMD,2026-02-22 10:16:17,2
AMD,o7424ny,What why it wasn't working on x570 lol,AMD,2026-02-24 10:15:36,1
AMD,o742cf4,"It was smart on AMD's side to permit B320/350 chipset to be used with 5000 series. RAM speed was limited to 2966mHz at best, PCIe was limited to PCIe 3.0 and people would have worse performance than on B550 chipset.     I put 5700x3D on B350 chipset and performance was not that good, after upgrading to B550 motherboard and 3600 mHz RAM my PC was working as intended and no issues at all (except stupid USB ones but oh well, can't have them all)",AMD,2026-02-24 10:17:33,1
AMD,o75je91,But we did complain about it and they did course correct and it wasn't even that hard. So now the end result is very consumer friendly. So they at least get partial credit.,AMD,2026-02-24 15:46:46,1
AMD,o77wkb9,"One actual issue was the capacity of the BIOS for many of those older boards. The compromise was to remove support for some of the older APUs. Motherboard manufacturers had to add ***DO NOT UPGRADE IF YOU USE THESE CPUS*** to the bios downloads on their website, which I'm sure AMD was wanting to avoid.  Honestly, it's a weak reason to deny a new generation of CPUs to an otherwise compatible chipset. I'm glad community push back convinced AMD to change their mind.  However, the trade off was obviously worth it! My friend is currently running a 5800X on my old X370 board.",AMD,2026-02-24 22:14:14,1
AMD,o7cny9k,"Edit: my post might read a bit more angry as I wanted too :) it's not meant that way.  Oh man ... they had a good reason for it, that makes a lot of sense, if you see it from a company perspective. And no, it was not about money or being evil.  First of all, they never guaranteed how long the socket and chips would be supported. They wanted to try at least 3 gens or so, but as this was a whole new platform, nobody knew.  And here comes the kicker and the real issue, why it didn't work with the older boards - or better said, why AMD didn't want to support it on older chips.  When Ryzen launched in 2017, AMD suggested 16 mib memory for the UEFI - or at least didn't enforce more. When the 5000 series came upon us, this resulted in an issue, as they couldn't foresee at the time in 2017, that 16 mib are not enough to support the new CPUs. Or that they would run AM4 for so long with so many different CPU types. Mind you, they were almost bankrupt and Ryzen saved their ass.  So they had two options. Cut the old / first Ryzen boards and only support newer - with larger roms. Or risk a shit storm and support hell, if they support the older boards - BECAUSE PEOPLE CANT / WONT READ UPDATE INFORMATION. And just ask yourself who would've been blamed for it...  After the community forced AMD to give the old Chipsets the new CPU support, it happened as I said. People didn't read, upgraded their Mainboard and couldn't use their old 1800x anymore, because they ignored the warning, that the bios dropped support and there are two different bios. One for the new CPUs, one for the older ones.  Or they bought a new CPU, sold the old one already and couldn't boot to upgrade the Mainboard for the right firmware.  Don't you guys not remember the threads here about it? And I still remember how pissed some were because of it and what a shit show this was, from their perspective.  And AMD even had to resort to an RMA system, where they sent out older CPUs for people to upgrade the boards or newer ones, to downgrade. And this was all because of this issue.  AMD did not try to fuck over anyone or saw the possibility to screw people over. Yes, it's a company and not a charity. Yes, they need to make money,  enforced by the shareholders.  But please get the facts straight. Even back then everyone was raging about it and didn't even for one second think about it objectively and in regards of a company.  And yes, it also impacted AMDs reputation, as I still read comments from time to time, that AMD is bad, because you need different CPUs to get the Mainboard runnings. From people not even knowing what or why it happened.",AMD,2026-02-25 16:34:28,1
AMD,o7cvqyw,A lot of the early 300 boards were terrible and had horrible memory supports. It's a miracle zen3 even works on them.,AMD,2026-02-25 17:10:04,1
AMD,o6p88ar,"To be fair the main draw of the 5000 series was the X3D lineup, and those weren't released until the end of April 2022. Sucks for early 5000 series adopters but I don't think many 1000/2000/3000 users were particularly in a rush to upgrade their less than 3 year old CPUs at the time the 5000 series released.",AMD,2026-02-22 01:37:39,0
AMD,o70wb4b,"That's true. I've ended up with b550, because I've needed an upgraded CPU despite having a frigging x370 Crosshair Hero. It all ended well and the board with new CPU (originally it was running 1700x) is back in service in my sons PC for some time now. But man, how pissed I was!",AMD,2026-02-23 21:31:14,1
AMD,o7621es,I went from a 1600X to a 5800XT (waited too long to get a cheap 5700X3D) on my early-enough-to-avoid-issues Asrock B350 mobo.  Insane is right.,AMD,2026-02-24 17:10:29,6
AMD,o7cw33w,Did that work well ? Old 300 boards had a lot of problems at launch.,AMD,2026-02-25 17:11:36,0
AMD,o78svcp,How noticable is it in games?,AMD,2026-02-25 01:05:51,1
AMD,o72stcy,"The 1700 came out and it was cheaper than the 7700k.   The 8700 was probably a better choice when it came out later. Intel was selling 4 core CPUs as high end parts though. Which was great in 2007 and moronic in 2017.  And yes, Intel was winning at 1080p low gaming with a high end video card. That's always seemed like a weird thing to strive for.",AMD,2026-02-24 03:53:09,31
AMD,o6orbg9,"You are kidding right? If you used the potential of the extra cores, the performance was really good.  Hell that first year i had my 1700x, just mining etherium on the spare cores when i was not using them paid for my system.  I was strongly gpu limited anything faster would not have mattered. The extra cores were great.",AMD,2026-02-21 23:52:13,33
AMD,o6qivhy,"1700X is around the Skylake era, so the 8700K didn't even exist yet.",AMD,2026-02-22 07:27:29,8
AMD,o6ni2tv,AM5 supports 28 lanes with 4 to the chipset leaving 24 available. Thats one x16 and two x4 at full speed depending on motherboard support.,AMD,2026-02-21 19:43:24,48
AMD,o6ninb4,It's 28 PCIe lanes. 4 of which go to the Chipset. You're left with 24 for everything else.,AMD,2026-02-21 19:46:23,26
AMD,o6ngiiv,uh it doesnt just support just 1 nvme drive wtf are you smoking,AMD,2026-02-21 19:35:13,26
AMD,o6nj5ar,"my motheroard supports 4 nvme and a 16x pcie lane, x870 tomahawk wifi",AMD,2026-02-21 19:48:59,9
AMD,o6nhkv4,"You dont really need 16 lanes on gpu if you have enough vram.  If you dont have the money to get decent vram, then you dont have the money to buy several nvmes.",AMD,2026-02-21 19:40:46,8
AMD,o6sfpzc,"They aren't, but acting like the underdog is, helps them against the top dog, making the competition better.",AMD,2026-02-22 15:56:56,35
AMD,o79cm5a,But Valve is my best friend /s,AMD,2026-02-25 02:57:31,3
AMD,o7dblqp,">corporations aren't our friends.  Businesses exist to make as much money as they can away with and that goes quadruple for corporations (bcuz shareholders)  I wish people would stop preaching that businesses are the perfect solution to every problem  Thing is, money is not the only metric of value.  People care about and help other people in ways that are very real and important but don't make economic sense  That's where govt has to step in .  FUN FACT:  every social welfare program had its origin in the failure of the market to provide a good or service society believed important",AMD,2026-02-25 18:21:43,2
AMD,o73l4uk,"Oh look, a reasonable and non-conspiratorial response that uses facts to explain a complex situation.   Companies aren't our friends, but we've known there's a limit on how much CPU info you can pack into the motherboard's brain since these processors released.   There was a small rumble about people who updated their BIOSes and found that they couldn't boot because they had upgraded out of supporting their current CPUs.   People upgrading to a newer CPU that had additional support that removed support for their existing CPU had to upgrade in the hopes that it would actually work lest they're left with a bricked motherboard and two CPUs they couldn't use.   It was a bit of a nightmare.",AMD,2026-02-24 07:35:05,41
AMD,o7af9xq,"> Basically none of these low-cost boards had support for updating the BIOS without a processor installed.  MSI had motherboards that *specifically* had larger BIOS capacities to support Zen 3 (the MAX series). AMD still had a blanket ""no Zen 3 on B450 and X470 boards"", even for those models. Not to mention the ""AM4 support until 2020"" nonsense they pulled.  Edit: Any reason for the downvote? I literally bought a MAX series motherboard because of the bigger BIOS size, then AMD tried doing a rug pull.",AMD,2026-02-25 07:36:12,2
AMD,o73ryyi,Then why did AMD allow specs that they knew were not going to work? Manufactures have to follow guidelines set by the companies.   Those guidelines stated that xyz was enough memory for bios. Then there going to use that amount.,AMD,2026-02-24 08:39:05,-13
AMD,o73nf80,"Not only did it require dropping support for older CPUs, they also needed to reduce the space that was reserved for board partners to implement their own bios features, sometimes requiring extensive changes.Â    My friends msi b350 carbon had the entire mouse driven GUI removed when zen 3 support was added to make room. And my own asus b450 board the fan curve GUI replaced with a much less intuitive purely text based menu for example.",AMD,2026-02-24 07:56:09,17
AMD,o748o0n,"Not only did they have to remove support for older CPU's, they also had to reduce the space that was reserved for board partners to implement their own bios features to make room for zen3.  MSI had to remove their entire mouse driven GUI and my own Asus b450 board had the fan curve GUI removed and replaced with a much less intuitive purely text based interface.",AMD,2026-02-24 11:13:57,9
AMD,o7gmayo,"Yeah, in theory, if AMD pulls support, some users would buy a new motherboard and CPU from them and they'd benefit.  But many more users who would have done an in socket upgrade with a new AMD CPU will now not do that. Or they might even upgrade their motherboard/CPU to Intel.  I don't really understand how AMD would benefit from pulling support.",AMD,2026-02-26 04:43:51,2
AMD,o7cymgm,"Yeah, for a good reason. Just think about it someone flashing a beta bios with the wrong CPU because they can't read and brick the system.  And yes, this happened a lot later on and one of the reasons AMD didn't want the support with two different bios versions, depending on the CPU.",AMD,2026-02-25 17:23:22,2
AMD,o7afjb6,"> i guess i'm imagining the plethora of asrock boards that had bios updates made available that allowed the 300 series boards to have 5000 support essentially at/prior to launch... which kinda throws a wrench into the overall claim, leaving a combination of it being ""amds fault"" paired with board vendors.  That was after all the backlash",AMD,2026-02-25 07:38:36,1
AMD,o7gncjz,Wonder if they'll extend AM5 support further due to the RAM shortages.,AMD,2026-02-26 04:51:07,1
AMD,o6psfzl,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-02-22 03:52:06,1
AMD,o6qe3eo,No that was history revision. The 5000 series pre X3D already got immediate rave reviews at launch. Well before anybody outside of AMD labs knew what an X3D even was. AMD only pulled out X3D after alder lake which came an entire year after 5000 seriesâ€™ original launch.,AMD,2026-02-22 06:43:09,24
AMD,o6u2p3j,Ryzen 5000 (non-X3D) was a massive step up over 3000 series. AMD went from worse than the i5 8600K in gaming performance to i9 10900K equivalent performance. There were a ton of 1000/2000 users considering that upgrade and it was a big enough gap that 3000 series owners would too if they were buying a brand new RTX 3070+ GPU.,AMD,2026-02-22 20:32:30,8
AMD,o7cy6u3,"The 5800x and even more so the 3D are way faster than the 1000 series. The first Zen CPUs where a bit slower on single core then Intels CPUs at that time - like the 7700K or later 8700K.  But they were faster with multi core, because they simply had more. Just the windows scheduler didn't work right with them for a way to long time, also decreasing the performance. Generally it took AMD until Zen2 to overtake Intel in single core and multi core in most scenarios, Zen3 was even better and the X3D moped the floor with them. And before someone chimes one - you would've needed to delid your 13900 or whatever it was at that time and OC it to go against the 5800x3d. With a power consumption of like 300W vs. 100W. Dunno about the real numbers right now.",AMD,2026-02-25 17:21:21,5
AMD,o7f6xju,It was very noticeable especially in cpu intensive games but 5800x runs super hot so get a better cooler if you do plan on getting it. I had a h100i 240mm on it and it would go past 100c on prime 95 (even with undevolting). I suspect the pump may have been failing so I got a PS120SE and itâ€™s been much better now.,AMD,2026-02-25 23:43:48,1
AMD,o73lf33,"1700 also OCed pretty well. I clocked mine somewhere between 1700x and 1800x performance level, on a basic B350 board. Great value.",AMD,2026-02-24 07:37:47,10
AMD,o73l1q9,That's a very very stupid way to say Intel had the faster gaming processor.,AMD,2026-02-24 07:34:16,-6
AMD,o6otbm0,People usually mean gaming,AMD,2026-02-22 00:04:39,18
AMD,o6ov983,"As someone who played a bunch of MMOs at the time, the 1700x shat the bed so hard. Years ago I made a post how bad it was. The 3600 was getting twice the fps. 1700 was never really good value for gaming.",AMD,2026-02-22 00:16:33,19
AMD,o6qskql,"In gaming, the 8700K was a far better CPU. On some titles, you can even see the 8700K outdo a 3950X & if OC'ed, a 8700K can rival a 10900K on some titles at 1080p as well.",AMD,2026-02-22 08:59:48,18
AMD,o6ynxnf,"Ryzen 7 didn't make sense to me for pure gaming, however back then picking Ryzen1600 over whatever i5 seemed like a no-brainer for me.",AMD,2026-02-23 15:12:43,7
AMD,o6qm5rq,"8700k released 7 months later, both in 2017.",AMD,2026-02-22 07:58:43,4
AMD,o6ntl7z,"Yeah, and that assuming you got drives that can saturate 5.0 x4. Many people are still running 3.0. Sure they shouldâ€™ve been more forward thinking but I mean storage was getting cheaper. I watched 4tb nvmeâ€™s go down to more reasonable prices until recently. For the average consumer the direction things were heading in was just larger drives getting more accessible. Now weâ€™re just fucked no matters what you do unfortunately with this pricing.",AMD,2026-02-21 20:44:05,9
AMD,o6nsl41,"This feels like why I donâ€™t have many usb-c ports, because to fully back them it needs pcie lanes.",AMD,2026-02-21 20:38:41,1
AMD,o737wj3,It technically does for X870/E as 4 lanes are reserved for the usb4 Asmedia controller. For every other chipset on AM5 it's 16(GPU)+4+4 as far as what most users would utilize the lanes.,AMD,2026-02-24 05:41:30,2
AMD,o6nl7d1,"Pedantic point:  Some of those PCI-E lanes go to the cost, others go to the CPU. The limits people were talking about were those going directly to the CPU socket.   Pedantics aside, I'm with you. There are plenty of motherboards with plenty of connectivity. The concern about PCI-E Lane limits might technically exist, but not in a way that impacts my experience and use of the PC.",AMD,2026-02-21 19:59:37,8
AMD,o6nqdf9,"If that's an AM5 board, I'm pretty sure you won't be able to populate all of those nvme slots without your GPU running at most X8.   Edit: Downvoting facts because?",AMD,2026-02-21 20:26:54,-3
AMD,o6nt81w,"Read your motherboard manual. Just because your motherboard has all those connections, doesnt mean you can use them all at once. This is why AMD is ditching AM5 soon.  Edit: I did the hard work for you, here are your motherboards Asteriks  ``` * PCI_E3 slot will run at x2 speed when installing device in the M2_3 slot. You can switch PCI_E3 slot to x4 in the BIOS, but this will disable the M2_3 slot ** The M2_2 slot will be unavailable when using Ryzenâ„¢ 8500/ 8300 Series processors. *** USB 40Gbps Type-C ports on the back panel and M2_2 slot share PCIe 5.0 x4 bandwidth. Both run at PCIe 5.0 x2 when a device is installed in the M2_2 slot. You can switch M2_2 to PCIe 5.0 x4 in the BIOS, but this will disable the USB 40Gbps Type-C ports. The USB4 host controller supports up to PCIe 4.0 x4. **** PCI_E3 slot will run at x2 speed when installing device in the M2_3 slot. You can switch PCI_E3 slot to x4 in the BIOS, but this will disable the M2_3 slot. ***** Please refer to the manual for M.2 SSD heatsink restrictions ```  If you use M.2 slot 2, you cant use USB-C. If you use M.2 slot 3 or PCIe slot 3. they disable their counterpart.",AMD,2026-02-21 20:42:07,-8
AMD,o6nsvyu,"Actually better question, which motherboards offer x8 slots, I thought they would automatically offer x16 and then x4.  So where does the x8 flexibility even come from?",AMD,2026-02-21 20:40:20,1
AMD,o6pz5ko,IIRC even a 4090 lost only 1-2% of performance on PCIe 3 x16 compared to 5 x16,AMD,2026-02-22 04:39:45,1
AMD,o6nilsf,"Classic HWUB: use every New Tech Thing as the litmus for how good a product is, even if only a fraction of a percent of users in very specific scenarios can/are willing to fully utilize it, and say every product without it is just not good.",AMD,2026-02-21 19:46:10,-4
AMD,o6vo9pl,"Exactly many people forgot the Zen5% nonsense the RDNA launch debacle, Redstone being really non starter. RDNA2 in maintanance mode.  Wonder why they keep doing this becuase we are not calling out these corporations more.",AMD,2026-02-23 01:47:58,19
AMD,o73s60i,it was just a bit of copium.   common sense says AMD gives out specific specs that manufactures have to follow minimal set of guidelines. They said XYZ amount of memory was the baseline. Then companies should be fine using XYZ as the baseline.   AMD just did what AMD does it over promises under preforms then ways for people to say how its not there fault.,AMD,2026-02-24 08:40:57,-18
AMD,o7gljnl,"I believe that was a case of AMD wanting to avoid confusion, since it isn't immediately obvious without looking up specs which boards even had 32mb bios chips. Max was MSI's branding/series rather than part of the AM4 spec.  It's similar to the PCIe 4.0 on B450 situation. Some B450's could initially be set to use PCIe 4.0, but AMD had mobo makers release a bios update to disable this since it only could only be done on some boards but not others.  For the record, I also used a max board for my friend's PC build at the time.",AMD,2026-02-26 04:38:33,1
AMD,o749h27,"What do you mean ""knew were not going to work""   They just underestimated how much the size of CPU support on the bios would increase. they aren't clairvoyant.",AMD,2026-02-24 11:20:46,4
AMD,o768vap,"Exactly, I got an msi gaming b450i mobo for my 3600 and was surprised at the text based B&W bios, while the advertisements and manuals showed a colorful mouse gui. Later I learned updated bios had to drop the mouse gui so that newer series CPUs could be supported. Eh, you win some lose some.",AMD,2026-02-24 17:41:44,4
AMD,o7debkr,"no they only do this specifically for 300 series chipset, 500 and 400 series chipset mobo bios (which support 5000 cpus) are all released as beta as first and come with warning notes (mostly the one having less than 32mb bios chip for supporting multiple cpu), I have a couple of AM4 board since the early day and have gone through this  Asrock got the cease and desist note is because they are the only one doing full lineup beta bios support before AMD allowed them to",AMD,2026-02-25 18:33:53,1
AMD,o7akqi9,"No it wasn't, Asrock has several bioses available within the month of the ryzen 5000 launch. I had a few customers in which moved from their initial ryzen 1000/2000 cpus to 5000 with those x370/b350 boards by the end of the month which was around when i could actually get my hands on some of those cpus due to the ""covid"" situation that was delaying a lot of availability. The backlash happened AFTER asrock conveniently ""pulled"" the bios updates, but by then, it was too late, for essentially exclusively the asrock boards, fully functional ryzen 5000 support remained as 3rd party websites hosted the bioses, readily available.",AMD,2026-02-25 08:26:31,1
AMD,o6qrh56,"> The 5000 series pre X3D already got immediate rave reviews at launch  The point I'm making is that few, if anyone, was going to buy a new CPU when they literally *just* bought a new CPU less than 3 years ago. I'm not defending AMD or anything, just pointing out that the average user's CPU upgrade cycle is slow enough that motherboard compatibility is almost never a problem, which is why this was never considered to be a practical ""advantage"" AMD chips had over Intel's.",AMD,2026-02-22 08:49:22,1
AMD,o7705d8,it also had a pretty good air cooler included,AMD,2026-02-24 19:43:49,2
AMD,o73nwim,"AMDs $300ish 8 core desktop cpu on a $80 motherboard was at performance parity with Intels 8  core 6900k at $1000 on a work station board.   The 7700k was at the time a low end part with only 4 cores.   Yes, the CPU, which was basically rebadged to a low end i3 two product cycles later, happened to be faster in a few niches. Most of these niches didnâ€™t matter to 99.9% of people. Want to game? Didnâ€™t matter unless you had a top end gpu.   Heck even for gaming the i5 (4C/4T) was generally not as good at gaming as one r5 (6C/12T).   Intel had one favorable match up if you squinted really hard and disabled background tasks which caused lag spikes on 4C CPUs. The i3-i5 line wasnâ€™t competitive. The high end work station parts were matched at a fraction of the cost.   And yes, the 8000 series, which had 50% core bump, rectified these issues later the same year.",AMD,2026-02-24 08:00:34,6
AMD,o6p8svo,Meanwhile the 8700K is still somewhat usable today.,AMD,2026-02-22 01:41:21,17
AMD,o737cvs,A 8700K beat stock Zen3 by a fairly decent margin if you actually OCed it fully (CPU+RAM). in games it was a 5600x with worse stock performance and limited to pcie3.,AMD,2026-02-24 05:37:08,6
AMD,o7865tt,"Yes, i had a 1700x system week 1 of release. For the first ~9 ish months, it was profitable to mine on just the cpu. And that was profitable after paying for power, so im not ignoring power cost. My power was not the highest, but it was above the national average.  The extra cores even allowed me to mine while i was playing games as well. Games didnt use the 8 cores, and depending on the game i could run 4 or 6 threads mining while gaming with little impact to frame rate. I would run however many threads would not impact gameplay. When i wasn't using the system i would use 8 threads.  I should also clarify. At the time, the value of the entherium was about the value of the just the 1700x cpu. So about $400 profit in 2017 dollars if i had sold then, but i did not. I sold the etherium at the end of last year at ~$4700/coin, and the cpu portion of what i mined was sold at a price that more or less paid for the system, sans peripherals.",AMD,2026-02-24 23:02:34,1
AMD,o6qn19j,"Which required a new motherboard to Sky Lake / Kaby Lake. Despite they are the same socket, and the fact that people have managed to hack motherboards to work with Coffee Lake.",AMD,2026-02-22 08:06:55,11
AMD,o6om4le,ah understandable. The guy was trying to say you can't use multiple nvme with a gpu running at 16x which is false,AMD,2026-02-21 23:20:32,3
AMD,o6pz283,"Probably downvoting because the extra NVMe drives go through the chipset which just means that only one of them can operate at full speed at a time, but most of the time you aren't saturating PCIe 5 on consumer HW anyway. Games rarely benefit much from faster than 3.0 speeds.",AMD,2026-02-22 04:39:04,8
AMD,o6sjsts,"No, it's more usual that a bunch of those NVMe share bandwidth through the chipset, but that hardly matters for the vast majority of consumers. Very few people on the whole are trying to run all their SSDs at full speed all at the same time.  And even for the boards where you do go GPU x8, it doesn't matter, there's no GPU consumer GPU where PCIe 5.0 x8 or x16 makes any noticeable difference.",AMD,2026-02-22 16:14:12,3
AMD,o6olxcc,">If you use M.2 slot 2, you cant use USB-C. If you use M.2 slot 3 or PCIe slot 3. they disable their counterpart.  This is incorrect, USB-C is disabled if you use genx5 speeds on m.2 alot 2. 4xgen speed is fine with usbc in use",AMD,2026-02-21 23:19:18,8
AMD,o6ntyff,There is 0 performance difference between pcie 5.0x16 vs 5.0x8 on 99.99% tasks if you have enough vram.,AMD,2026-02-21 20:46:02,3
AMD,o6ntkfh,Uhhh the 4060 and the b580,AMD,2026-02-21 20:43:57,1
AMD,o6onigi,Many boards have M.2 connectors which steal lanes from the first slot if used.,AMD,2026-02-21 23:28:53,1
AMD,o6skc6z,"They are still physically x8, but there's several boards out there that depending on what you have installed in other PCIe slots will change the used configuration to x8.",AMD,2026-02-22 16:16:35,1
AMD,o6nj23a,">a fraction of a percent of users in very specific scenarios can/are willing to fully utilize it  ""People that need"" 32gb vram, 16 gpu lanes and 3+ nvmes use more than 1 gpu.",AMD,2026-02-21 19:48:31,5
AMD,o73rv6v,"People aren't calling out AMD more, the only thing AMD truly has is there processors. All it take to upset this balance is for Intel to catch up and surpass.   My first computer I built was a AMD Duron. Then I built a Athlon Barton. I used AMD for years. Honestly AMD isn't the same value company they use to be. There just as expensive as Intel who was considered the Premium with Pentium 4's and Hyper Threading.   Also AMD has a very good track record of shooting themselves in the foot every chance they get. You mentioned recent things.   Damn AMD was the first X64 processor produced that works with windows. Instead of capitalizing on that and pushing there tech. We had them buy ATI and push Bulldozer. That almost bankrupted them.",AMD,2026-02-24 08:38:04,19
AMD,o7cupa9,"There was no copium, those were facts. Maybe you might read how AMD fared before 2017. Ryzen saved their ass in 2017 and it was a whole new arch nobody had ANY experience with so far. And they always said, they will try to support AM4 for 3 gens or years at least, maybe more.  Back in those days, they couldn't anticipate, how many years that support would go and how many different CPU SKUs they would release. Spoiler - it was a lot more than they could ever imagine. And that UEFI memory is not cheap and the MB makers would go with the lowest for some boards - and even higher priced ones, my 370x also only has 16 mib.  That was not missing foresight, bad planning or whatever. In 2017, 16 mib looked plentiful. But after more and more SKUs and additional fixes for sideband security issues (remember meltdown?), it wasn't enough after all. Not to mention the MB makers blowing up the UEFI with so much crap...",AMD,2026-02-25 17:05:11,3
AMD,o76jvxs,cause if I manufacture something and know how much space xyz takes. Then know keeping support for xyz number of years. Then I know about what I need for storage. I know its hard to fathom a small startup like AMD having the ability to think ahead. Specially with all the copium being huffed.   example. I know on average my bio software takes 20mb of space (made up number). I plan to keep it for 5/6 generations. Then I know the bare minimal my specs should suggest is 160mb. This way there room for the manufactures software and my bios information.   Crazy right.,AMD,2026-02-24 18:30:32,-1
AMD,o7apckd,"Ah, the ""leaked"" BIOS files - I'm aware of those, I meant the officially released ones.",AMD,2026-02-25 09:10:02,2
AMD,o6r0c1r,"Went from a B350 MSI Tomahawk (that died thanks to lots of BIOS flashing, first-gen was buggy) with a Ryzen 1700 to B470 Asus Prime-Pro mobo, still in my system. Got the 5600X and later 5800X3D. I just saved 4-600 dollars on mobos. Which payed for one of those CPUs. Not an advantage? What are you smoking, man?",AMD,2026-02-22 10:14:36,4
AMD,o7gmo4l,"The benefit is more in the years after. As new CPU's come out, old ones are forced down in price which forces the used market down even further. And that makes for very cheap in socket upgrades for everyone. On the Intel side, the best CPU's of the last supported generation per platform spike in price on the used market.  I bought a 5600 non-X for 130 brand new off Amazon in 2023 for example.",AMD,2026-02-26 04:46:23,1
AMD,o73p5ms,Man this brings back memories. Fanatics peddling lies like the cpu didn't matter unless playing at 1080p low with a $1000 gpu. I ate that shit up at the time to save a few bucks.,AMD,2026-02-24 08:12:11,-6
AMD,o6qn8ni,1700 also required a new motherboard lmao,AMD,2026-02-22 08:08:50,8
AMD,o6nu2zs,"Right, but how do you even run in x8?",AMD,2026-02-21 20:46:42,1
AMD,o6ntxrx,Those are low end cards.  They donâ€™t even run on large memory like the person I was responding too says you can use to reduce the number of lanes required.,AMD,2026-02-21 20:45:56,2
AMD,o6nknaq,Not necessarily,AMD,2026-02-21 19:56:45,0
AMD,o73u16a,"""All it take to upset this balance is for Intel to catch up and surpass.""  Which going by the last 10 years seems to be an impossible task for Intel.",AMD,2026-02-24 08:58:38,2
AMD,o7csnx6,"ATI saved their ass with bulldozer. And the reason they couldn't win vs Intel was simply, because Intel made some shady backroom deals to keep them out of the OEM and Server market. And that was important, as this is where the money comes from. Back then AMD still had their own fabs and building new nodes was becoming expensive. With one the gamer and enthusiastic market as support, they didn't have the funds - thanks to Intel - to actually go toe to toe.  And this is not some conspiracy stuff, this happened. Mind you - at that time, Intel had more profit as AMD had sales. Intel was larger in sales as the next 5 companies in the same sector combined.  AMD got lucky, that TSMC steamrolled Intel and Intel fucked up their node long term. This gave AMD the upwind and time they needed to come back and overtake them. Showing people that they can deliver.  But in the bad decade with bulldozer, the GPU part saved their ass, but also bleed dry. They also had some strategy mishaps IMHO. But one of the main reasons, their GPU part is worse than Nvidia is, that they bleed so much knowledge and top tier engineers and devs, that it was impossible for them to keep at a high level.  And especially in this kind of sector you can't throw money at problems. It takes time to get devs and engineers back and them up to speed. Coding GPU drivers is everything, but not easy.  Also - many tend to forget - even today AMD is TINY compared to Nvidia or Intel. In sales, profit and employee numbers. And this was way worse in 2017, when Ryzen hit. Imagine half the employees of Nvidia and maybe 1/4 of Intel. And they had negative cash flow at that time, for years. And still doing GPU, CPU AND SOC for consoles.  Honestly, it's a miracle that they survived this till Ryzen hit and even could overtake Intel and get some serious foot into the server market now.  The only downside to them is, that they lack a good GPU lead with a solid vision and a larger software development team.  Also their PR team is the worst on the planet. They take too long to respond, tell way too much bullshit and so on.  They need some serious community managers in social media that can give solid answers, replys (fast ones) without clearing everything with the law department. They need to be way more active here and actually talk with people about issues. And even if it's only "" we are looking into it, it's a bad to find issue and we will take a lot of time to fix it"". Would be better then the current state.",AMD,2026-02-25 16:55:48,0
AMD,o74555n,"Processors more like a joke when an Apple M3 destroys  both AMD and Intel while Panther Lake Ecore still besting AMD Pcores in IPC. Plus even with the ram crisis ongoing their processors are still expensive...  Edit: For all those downvoting see Geekerwan's video. Even though PTL loses to AMD in this form factor, PTL core arch when scaled up will beat whatever Zen6% AMD has already.",AMD,2026-02-24 10:43:11,-7
AMD,o7d8dc0,"they were ""officially"" released, they were just often referred to as leaked since they were pulled soon after. They were beta bioses dropped on the download pages of the various boards.",AMD,2026-02-25 18:07:16,1
AMD,o6r71nq,"With that kind of upgrade path? Sure, of course it's great. It's just not a very common one, that's all.",AMD,2026-02-22 11:18:11,4
AMD,o747b19,"CPU matters if itâ€™s being choked. It barely matters if youâ€™re above 100fps 99% of the time already.   A 4 core CPU had relatively little headroom for background tasks. Any anti virus spin up? Thatâ€™s a lag spike. Activity on a web browser. Lag spike. System update. Lag spike.   4 cores was overkill in 2007, solid in 2012 and questionable in 2017.   The cores in kbl were individually better than zen 1 cores for games. When they werenâ€™t choking. Even 4C/4T without background tasks started to choke on just a game relative to the r5. So yes, 7700k if there were no background tasks, at 1080p, with the fastest video card at the time was the champion. Until it choked on real world use. Like a software update in the background. If you ignored everything else. And didnâ€™t want to be very very disappointed a few months later by that much better CFL chip thatâ€™s actually still somewhat usable today.",AMD,2026-02-24 11:02:13,1
AMD,o6qnjom,So did Sky Lake.  The 2700 3700 and 5700 did not. Oh and the 9th gen Intel were also Coffee Lake. So you get literally 0 IPC upgrade on the new board you got for the 8700K. Then Intel required a new board for 10th gen again.,AMD,2026-02-22 08:11:44,3
AMD,o6nv3qs,"Well, cpus like 9800x3d have 24 usable pcie lanes, while my 8700g only has 16, so i cant run x16 + my m2 at the same time.  If in the future i upgrade my 780M 16GB to something like 9060 XT 16GB or 9070 16GB, then i will have to use it on pcie 4.0x8.",AMD,2026-02-21 20:52:11,2
AMD,o6ntmnq,"I'd rather have cheaper products with good performance. The majority of gamers will only have one, maybe two SSDs, and they certianly don't need all that bandwidth. People who do need to run ridiculous workloads that require 128Gbit/s SSDs operating concurrently and more than one GPU should not be what the consumer platforms are built for. Cost is more important.   For 99% of people, one x4 SSD and the x16 GPU is more than enough. Everything else going through the chipset is perfectly fine.",AMD,2026-02-21 20:44:17,3
AMD,o74d28q,"If the new laptop chips are any indication, it might not take as long as you think.",AMD,2026-02-24 11:49:37,8
AMD,o76jbxs,We said the same thing for bankrupt AMD.   There is hope though its marginal. I love to see the two compete with each other trade blows. Cause that's how we truly win.,AMD,2026-02-24 18:28:05,2
AMD,o7ds6o7,TLDR but the first sentence makes it pretty obvious its either copium or rage bait GG.,AMD,2026-02-25 19:37:21,1
AMD,o749zr2,>and Intel while Panther Lake Ecore still besting AMD Pcores in IPC.  Everything in your comment is questionable but this one is just laughably incorrect.,AMD,2026-02-24 11:25:07,2
AMD,o6vooci,Yeah I dont know why people keep on hyping that kind of upgrade path. AM4 design sucks with little lanes and that's on AMD to not have a future proof design...,AMD,2026-02-23 01:50:26,-4
AMD,o7aap98,"Zen 1 or kaby lake was like did you want your turd sandwich with mustard or ketchup. When many people were building, near the end of the year, coffee lake came out. i5-8400, 8500, 8600/k, i7-8700/k. Those were all great options for cpu performance.   If you only focused on big AAA titles with ultra settings, maybe it would seem like it didn't matter much what cpu you used. But for the people who actually used these processors, it mattered whether it was paired with a GTX 1060 or 1080 Ti. With all the attention on 1% lows as a measure of smoothness, even Zen 1+ can't maintain 60 fps. Coffee lake outperformed it by massive margins.   https://www.eurogamer.net/digitalfoundry-2020-intel-core-i9-10900k-core-i5-10600k-z490-motherboard-review?page=2",AMD,2026-02-25 06:55:29,1
AMD,o6qpo4m,"In 2017, you had a choice. Buy the 1700x with a motherboard, or buy a 8700k with a motherboard. Then, as a normal person, you don't touch your pc components. You use it for years, enduring any performance, good or bad.   People don't upgrade every gen. Usually they use it until something breaks. It's great that you don't need to get another motherboard, and sell the old one, but was it really worth years and years of the worst performance? I'd say 100% no. Moving forward, Intel and AMD are close enough in performance for motherboard longevity to matter more.",AMD,2026-02-22 08:32:03,7
AMD,o6nwpls,"I didnâ€™t realise they sold CPUâ€™s on a platform that couldnâ€™t even utilize the full spec.   Thanks for the insight.  Yeah I started looking this up and it comes down to how the motherboard is configured to handle this.  Which in your case auto negotiates down to x8 when you add the m2 drive.  Thatâ€™s probably similar behaviour for even 9800x3D on boards with multiple NVMe slots and USB4 controllers.  I read about some other motherboards where they have a secondary full slot that only runs x8 natively so you can put the graphics card in that instead of the main slot if you want to force x8.  Some where they offer more NVMe slots but as you populate some it deactivates others etc. Itâ€™s a good point that maybe AMD didnâ€™t see the issue as PCIE-5 has a lot of bandwidth so we arenâ€™t close to saturating it for GPUâ€™s.  I think PCIE 5 NVMe drives can though, but they only use 4.",AMD,2026-02-21 21:00:45,2
AMD,o6okgg6,"The original assertion is just completely false. Hes saying people that have a 5090 and 3 storage drives have multiple dGPUs.  What in the world is that based on? People will often add drives as time goes when they want more storage rather than upgrading their main drive, where price per TB has diminished returns.  Why would they assume people with 5090s and 3 drives *also* run multi-GPU setups?",AMD,2026-02-21 23:10:29,2
AMD,o77u90k,Its true tho. Zen 5 was never good at IPC as raptorlake even matched it. Early alderlake 12900K easily beats the 7700X in avx-512 in IPC when 8 cores vs 8 cores.,AMD,2026-02-24 22:03:06,1
AMD,o74iatn,Yeah laugh alld you want Geekerwan already did the comparison and Zen5% loses to Ecore PTL.,AMD,2026-02-24 12:27:39,0
AMD,o7ahdu7,"Iâ€™m not arguing against CFL.   Iâ€™ll still say that Zen 1 was a solid leap in usability and it would have been even more marked if it released on time in 2016 (and without the cache latency issues, which were only partially fixed run zen+). And the only thing really affected was games whichâ€¦ ehh, my own experience was that it was good enough and day to day use was much better vs an i5 ivb chip.",AMD,2026-02-25 07:55:28,1
AMD,o6qr3uk,"People not upgrading every gen is exactly why AM4 was better, you can go straight from a 1700X to a 5800X3D. You only get to ""upgrade"" on Intel IF you upgrade every gen. In the case of 8/9 gen Intel didn't even bother changing the code name.   You had a choice of the 6700/7700k vs 1700X at launch. With double the cores the choice isn't as simple as you think. Also the 1700X was inexpensive, Intel was still selling quad cores for more.",AMD,2026-02-22 08:45:52,10
AMD,o6q0zj6,"The key word i said is ""need"".  Gamers with a 5090 dont really need 32gb vram + pcie 5.0x16 + 3+ nvmes.  If you really need to use all that power, then you are running AI or something, like the guys with 2 or 4 5090s or 6000",AMD,2026-02-22 04:53:35,2
AMD,o7dloio,"It did well relative to the crap i5 at the time if it could leverage extra cores and threads. But for many of the most popular games at the time and even now, MMOs that rely on single cores, it shat the bed. For 2017, Zen 1 and Kabylake wasnâ€™t the right choice",AMD,2026-02-25 19:06:55,1
AMD,o72rzff,So from 1700x to 5800x3d. Did you upgrade your motherboard too? ðŸ¤·â€â™‚ï¸,AMD,2026-02-24 03:47:47,3
AMD,o6qry12,"> Also the 1700X was inexpensive, Intel was still selling quad cores for more.  And you got what you paid for. They weren't exactly performing better than Intel quad cores as far as gaming was concerned. This was such a problem for AMD fanboys that they had to invent new metrics to get a leg up on Intel, such as testing ""smoothness"" with ""blind tests"" because objective tests didn't reveal jack shit in most cases.",AMD,2026-02-22 08:53:53,8
AMD,o6qscrl,"1700x msrp for $400, it wasnâ€™t that cheap. Maybe there were sales when the 8700k was released. And maybe you have a point if it was the 7700x vs the 1700x. But hands down, the 8700K was a complete no brainer for those who had to choose between the two",AMD,2026-02-22 08:57:43,3
AMD,o6q4fjz,">Gamers with a 5090 dont really need 32gb vram + pcie 5.0x16Â   You didn't say Gamers. You said ""People""...And people buy graphics cards for *more* than just gaming. Not only AI, but Video editing, 3D modeling, etc.   They *bought* the 5090 for the VRAM. And there's clearly a lot of demand for it.  >3+ nvmes  I don't know why ""need"" is the argument here? People may want additional storage  >   If you really need to use all that power, then you are running AI or something, like the guys with 2 or 4 5090s or 6000     *Plenty* of people do local AI on a 5090. A single 5090 is hugely popular in the AI hobbyist scene. You can easily handle models like Wan2.2, Flux 2 Klein, etc. on a single 5090",AMD,2026-02-22 05:20:20,1
AMD,o74y1qb,Not OP but I've gone from 1700 -> 3600 -> 5800x3D on the same X370 board that I am still currently using. Looking back I am super happy with how it turned out.,AMD,2026-02-24 14:01:25,4
AMD,o78fswr,Upgraded my buddies system from a 1700x to a 5600x all on the same b350 mobo. He was playing xcom with a bunch of mods and the fps improvement was massive. From around 5fps to 20fps on the same gpu and ram.,AMD,2026-02-24 23:55:00,3
AMD,o6qsz9h,"The 8700K was launched Q4 2017, by April 2018 there is the 2700X for $329. Zen+ has a significant clock speed increase and minor IPC increase due to lower cache latency. So except for a small window of time, the 8700K is not competing with the 1700X.",AMD,2026-02-22 09:03:36,5
AMD,o6q7dr1,"They dont really NEED all that full power.  I can sneak on one of those computers (normal users like gamers, editors, etc) and config the lanes to be x8 instead of x16 and they wont perceive any difference in the performance.",AMD,2026-02-22 05:44:25,3
AMD,o6qw5kl,"This model of OEM only drivers is so outdated it even makes Android look decent.  You have the PCI IDs, just include the custom changes in the goddamn public driver and let users update regularly.  Handhelds are the only market where AMD still keeps some sense of relevance and dominance, but you have to go and ruin it too.",AMD,2026-02-22 09:34:30,89
AMD,o6r6ifq,"This is the reason why no other handheld is a true competition for the steam deck! Proper longterm support is so much more important than raw computational power. (Yes I know, by installing SteamOS you get longterm support through Valve. It's still pathetic for Lenovo)",AMD,2026-02-22 11:13:05,211
AMD,o6r8vhp,Lenovo/ Motorola does this on the phone side as well.  It'll either take them a while for OS updates or don't expect them at all.,AMD,2026-02-22 11:35:39,23
AMD,o6slv2m,Just like laptops they barely gets 2 years of drivers update after they act like the laptop is eol,AMD,2026-02-22 16:23:24,8
AMD,o6s44vj,It will keep working just fine under Bazzite/CachyOS,AMD,2026-02-22 15:01:30,14
AMD,o6wz01p,Aren't they still currently selling the Z1e in the Legion Go S?,AMD,2026-02-23 07:26:14,4
AMD,o6ss1a5,"Just install regular Radeon 780M drivers, they work fine",AMD,2026-02-22 16:51:13,6
AMD,o6yogen,This whole thing is based on one conversation with a support rep on chat. So it should be taken with less than a grain of salt.,AMD,2026-02-23 15:15:21,3
AMD,o6vtswe,Classic Lenovo move,AMD,2026-02-23 02:21:18,1
AMD,o6ylwvt,This isn't great but I don't have many options as I prefer a bigger screen handheld for PC games.  Also I can't say I have any problems with my current driver's right now so it's not something that will affect me for a while I think.,AMD,2026-02-23 15:02:26,1
AMD,o6zdzyx,Pathetic,AMD,2026-02-23 17:15:29,1
AMD,o7dzsny,Install Bazzite on itâ€¦. profit?,AMD,2026-02-25 20:12:33,1
AMD,o7ecsdy,There was a post today on the Legion go subreddit where someone withing Lenovo said that was not true and there would still be supported. I'm not saying it's not true but just not to completely trust rumors.,AMD,2026-02-25 21:13:19,1
AMD,o6y4cnw,Bazzite awaits you. We donâ€™t have problems like that under Linux,AMD,2026-02-23 13:25:12,1
AMD,o6sgkeq,Can I install it on ROG Ally?,AMD,2026-02-22 16:00:22,0
AMD,o6tzliw,"Yup, typical turdware: just push out as much shit as you can as quickly as possible, then forget about it. We know this from GPD, Asus, Ayaneo and the likes, but seeing this from Lenovo is really quite bad.",AMD,2026-02-22 20:16:42,33
AMD,o6v9iub,"What updates do you need from Lenovo on a handheld? AMD maintains the chipset, Steam/Microdick/Linux contributors maintain the OS. And anything that works today (offline) will work in 20 years assuming no OS support.",AMD,2026-02-23 00:20:50,17
AMD,o6teofy,"Definitely true, even if the Steamdeck isn't as powerful, long term support is great and I'd rather support Valve over any other company for this reason.",AMD,2026-02-22 18:35:15,24
AMD,o76ch2k,Lmao steam deck is niche as well,AMD,2026-02-24 17:57:49,2
AMD,o6rzfjf,i have it and it is my steam deck.,AMD,2026-02-22 14:37:10,1
AMD,o6rnmdb,Both are important tbh,AMD,2026-02-22 13:28:44,-1
AMD,o6uomfk,How does this work if AMD is stopping driver updates? Does the steam OS not need driver updates from the APU manufacturer?,AMD,2026-02-22 22:24:01,0
AMD,o6zu6lh,On the consumer side sure. If you buy the business focused stuff you'll have a better experience. My Thinkphone has had prompt android version updates and monthly security updates. Hasn't been late once.,AMD,2026-02-23 18:30:23,3
AMD,o6sit8f,CachyOS is buggy on the legion go unfortunately.  Very unlike the desktop version ( it has been by go to for 3 years).,AMD,2026-02-22 16:09:56,11
AMD,o79g73o,"Yes, since that uses the Linux driver, which is not artificially blocked from using the standard AMD GPU driver (on Windows).",AMD,2026-02-25 03:18:03,1
AMD,o6x69j7,Yes.,AMD,2026-02-23 08:36:23,3
AMD,o79s2qs,"you have an entirely different set of new, lovely, exciting issues!",AMD,2026-02-25 04:32:20,1
AMD,o6wk9px,Amd doesnt provide the gpu driver for it.,AMD,2026-02-23 05:19:28,14
AMD,o6rp4pw,I would rather take my device being updated than having a powerful device that can't be optimized for new games.,AMD,2026-02-22 13:38:08,10
AMD,o6vo5oc,"Deck uses Mesa's driver stack for the usermode driver and linux's amdgpu kernelmode driver, both of these have more than just AMD employees on em so it's going nowhere",AMD,2026-02-23 01:47:17,8
AMD,o7dz4fe,"dont know about legion go but on linux AMD is much more stable and faster than on windows I have ROG ally, and issues I no longer face are , random crashes, texture issues, tearing when frame gen is on. random driver crashes. Games loosing half of FPS simply by turning on adrenaline software. On linux I gain easy to access FSR4 with goverlay. Faster and smoother system. It barely uses 2gb ram when windows does 8...I dont play multiplayer games so Im OK with that",AMD,2026-02-25 20:09:22,1
AMD,o79zjoi,I have Bazzite on my ally x and I have yet to hit an issue other than a game bug that was existing on windows too as it was game specific. So I have no idea what you are talking about,AMD,2026-02-25 05:25:21,1
AMD,o6xh94o,Linux is pretty much a neccesity for long term viability of semi-custom hardware.,AMD,2026-02-23 10:24:02,11
AMD,o6v9l90,Optimized by Lenovo? How? The chipset and the OS are out of their control.,AMD,2026-02-23 00:21:13,1
AMD,o6xhq0g,"Valve employees have been doing great work upstream that keeps many legacy AMD GPUs viable for much longer.   The more owners of old AMD cards that can still use them for modern games on Linux, the more adressable market they have.",AMD,2026-02-23 10:28:33,4
AMD,o79zrx3,"never seen a Linux thread without the good ol' ""I have not noticed issues so therefore anyone who is having them is lying"". have a good day   edit: thread about the legion go which is known to be buggy in Linux, you can't make this shit up",AMD,2026-02-25 05:27:03,0
AMD,o6xm2ws,okay... what does that have to do with my comment? Amd still doesnt provide gpu driver for z1e on their website,AMD,2026-02-23 11:09:12,-1
AMD,o6vntwc,"OEM driver usually still has some stuff being tuned by the OEM (duh), but rather than something more intricate like shader compiler or whatever, it's usually related to power delivery and thermals.",AMD,2026-02-23 01:45:19,3
AMD,o6xj4lo,"I still remember way waaaay back when Valve announced testing of their ACO compiler for Mesa, I got so excited that it works on my shitty ahh A8-7410 APU",AMD,2026-02-23 10:41:57,1
AMD,o6xmuxz,"On Linux you don't have to download it from them, it's upstreamed to the Kernel. For chips they call semi-custom or off roadmap, Windows is a gamble.",AMD,2026-02-23 11:16:15,8
AMD,o79fy8n,None of that is true for the GPU driver in this case.,AMD,2026-02-25 03:16:37,2
AMD,o6xo4jr,How is z1e a semi-custom if it is basically a 7840u? All amd did was edit the microcode and rename it.,AMD,2026-02-23 11:27:28,1
AMD,o6xyr5p,AMD called it a semi-custom design in an interview when launching it.   That made it a no-go chip for me. Waited for a decent device built on the HX370 instead.,AMD,2026-02-23 12:49:37,3
AMD,o6ysdvt,"it doesnt matter, unlike Nvidia and its proprietary driver, AMD has no control of what devices are supported by the OSS driver.      since this is just a rebrand(eon) mesa has full support of it.",AMD,2026-02-23 15:34:35,2
AMD,o79ftbt,"It's not ""semi-custom"" (unlike Van Gogh, which was custom-developed for a different purpose and later repurposed for Deck); it's a rebranded laptop APU, sold cheaper initially by AMD to manufacturers, with the catch of having to pay for ""bespoke"" driver support, which is not bespoke, just artificially barred from using standard AMD drivers.",AMD,2026-02-25 03:15:49,1
AMD,o6vnxa4,I dunno why amd didn't release 9070 gre globally. It's perfect to fill that huge gap between 9060xt and vanilla 9070.,AMD,2026-02-23 01:45:53,28
AMD,o6tu72p,Waifu adds +5% performance right out of the gate,AMD,2026-02-22 19:49:44,64
AMD,o6xlz3o,"Not my cup of tea but more unique designs are great. More of the ASUS gundams, AMD 6900XT Halos, GTX Titan Star Wars, and less of the boring red and black rectangles.",AMD,2026-02-23 11:08:14,15
AMD,o72uws3,**TECH JUDAS**,AMD,2026-02-24 04:07:07,2
AMD,o7dnuhg,can someone send me one for freee pleaseeeeeee,AMD,2026-02-25 19:16:57,1
AMD,o7fcw41,Why do all the good designs get locked out of the USA :(,AMD,2026-02-26 00:16:46,1
AMD,o6wvjyx,"I still have no idea what I'm buying when it comes to Radeon. Their naming is 100% confusing. Nvidia makes sense, but then they silently downgrade their cards, but keep the same name (this started way back with the GTX 1060). Ugh. I'm sick of hardware these days. WAY overpriced for the performance you get & expect.",AMD,2026-02-23 06:54:44,-5
AMD,o6wdd65,"Because they can't produce enough of them. They're based on the same die as the 9070XT, they'd need a whole lot of defective 9070XT's that also failed to become 9070's to produce enough 9070GRE's to supply globally.",AMD,2026-02-23 04:28:14,37
AMD,o6yuamm,Ironically I run my waifu rig at -30% power cap to keep the noise down.,AMD,2026-02-23 15:43:45,7
AMD,o723drq,"My asus 3080 Gundam was so damn pretty that it single handedly made me not wanna upgrade for so long (til I finally caved and got a zotac 5090 last month, thank god... just before all the price surges and even worse supply constraints)",AMD,2026-02-24 01:22:17,4
AMD,o7gew1q,"meh, Once it's in my case I never look at it again",AMD,2026-02-26 03:54:30,1
AMD,o6x6hwr,"How is it confusing? It's exactly the same as with Nvidia. First Number is the generation. Numbers after that the performance tier.   XT and XTX are ""upgraded"" versions just like ""Ti"" used to be with Nvidia. The GRE is a special China only case, just like Nvidia did when the whole ""you can't sell your top chips to china"" thing started.   Granted, they kinda fucked up with the XTX, XT was enough. But you generally have ""high number = better"". (Assuming you are inside one generation).",AMD,2026-02-23 08:38:38,14
AMD,o6zlijx,"> Nvidia makes sense  I mean, from a company that does, Super, Ti, Ti Super, and even at times releases a gpu thats cut down but doesn't change its name, I wouldn't say Nvidia is outright ""easy"".  It even gets more convulted on mobile when you introduct the Max-Q varients.  While im not remotely championing that AMD naming scheme is easy, Nvidia is far from easy.",AMD,2026-02-23 17:50:42,10
AMD,o795n2s,"They use the same exact name scheme as Nvidia, though..  > this started way back with the GTX 1060)  Way before that. AMD also occasionally has confusing models, but never 7 different products named ""GTX 1060""",AMD,2026-02-25 02:18:52,0
AMD,o6y6y37,"Because otherwise weâ€™d never get to see them, Avi.",AMD,2026-02-23 13:40:41,0
AMD,o6x65rl,"They don't need defective dies. In most cases unless you have a very immature process the amount of defective dies needed to supply the demand is never reached. Plus you can't really control the defects to only happen where it's convenient for you. So you may end up with a good bunch of defects, but all in places which can't be salvaged.   The truth is, those are perfectly fine 9070xt dies which have a fuse blown to disable certain perfectly working parts. In most cases. There will be defects and those can be reused, but the vast majority are chips that could have been 9070XT's.",AMD,2026-02-23 08:35:22,2
AMD,o6y810r,I donâ€™t  know why you getting dislikes itâ€™s true,AMD,2026-02-23 13:46:55,-4
AMD,o73tu7b,"I feel you, its not as pretty but I have a 3080 white strix in my closet, I just canâ€™t get rid of it. Besides, its useful to have a semi modern backup gpu in case of 5090 explosion anyway I suppose",AMD,2026-02-24 08:56:48,3
AMD,o7590st,"more X in the name, more performance :D",AMD,2026-02-24 14:57:54,0
AMD,o75npzj,"AMD laptop skus are the most confusing naming scheme on the market, after Monitor names. But gpus are easy, agree on thar",AMD,2026-02-24 16:06:22,0
AMD,o76i3ki,skill issue,AMD,2026-02-24 18:22:37,-2
AMD,o6xcrhl,"that may have been true ages ago, but TSMC 5nm only has 80% yields and the chips they produce can and do have salvage die.  there is a reason why in HW design you see ""spare"" sets of logic gates per each cluster to try and cut down on this as much as possible, but there are certainly a lot of that   Like, where do you think the microcenter exclusive 5600X3D or the nvidia 2080 TI black edition that was evga exclusive comes from? just that it seems that there are enough salvage die to supply a whole nation / region rather than just one store / brand in those cases.   as we go into smaller and smaller nodes, these kinds of tricks are commonly used because the yields are not fully there and is mean to make some of the costs back",AMD,2026-02-23 09:41:06,10
AMD,o7aj59s,"Bruh if you complain about AMD naming, you haven't know enough Intel CPU. At least AMD naming are mostly numbers",AMD,2026-02-25 08:11:36,2
AMD,o75q833,Those i think are primarily made by the manufacturer tho? Well... unless AMD has some Naming Scheme requirement.   What is true is that mobile CPU's aren't the same as Desktop ones and you can't compare a 5th gen mobile with a 5th gen Desktop. But aside from that.,AMD,2026-02-24 16:17:42,0
AMD,o6xea3f,Doesn't change the fact that even with 80% yields you cannot supply a whole SKU alone.   I was not aware of the low'ish yields with 5nm.,AMD,2026-02-23 09:55:46,0
AMD,o7aqkxt,"Amd has laptop cpus that look like same gen called 7xxx but theyâ€™re Zen 3 and 4. Its rly confusing for non enthusiasts, i'm not informed about Intel naming on laptops",AMD,2026-02-25 09:21:37,1
AMD,o766nrz,You can when anything you box and label sells above msrp anyway.,AMD,2026-02-24 17:31:35,1
AMD,o6af2xv,"I remember prepordering my 9070xt fromÂ  memory express, they wanted to price gouge me $1400 + taxs + extented warrantys. i refuse every time, they call me. Im still waiting til it hit 900Canadian dollars.",AMD,2026-02-19 18:58:23,75
AMD,o6dwgnd,doesn't seem to be the case for EU though... at least for 9070XT,AMD,2026-02-20 07:43:20,19
AMD,o69rlbu,Price gouging not working anymore?,AMD,2026-02-19 17:07:23,59
AMD,o6dpqo0,"This should be a good thing for buyers, who always comes on here to complain that AMD should be blamed for when demand is higher.",AMD,2026-02-20 06:41:52,12
AMD,o6jl0kh,I basically traded my old 7800xt for my 9070xt.  Holiday deals were insane if you were willing to jump through hoops for cash back!,AMD,2026-02-21 03:30:18,3
AMD,o6dwz1z,"Stores trying to sell above MSRP because if your new rig is already a 1000 more than it should be, a little bit more for the GPU won't matter   Yet those having the money for not to care just get a 5070/5080 anyway   For everyone else, 400 more for RAM means something else needs to be cheaper to fit the budget   And than there are still the majority of people who don't need something new but would just buy if it is cheap and price gouging doesn't work in a market where the actual demand is rather low.",AMD,2026-02-20 07:48:04,5
AMD,o6jspib,Still too expensive for a midrange GPU with missing feature parity,AMD,2026-02-21 04:23:33,1
AMD,o6f4vd5,Same way you could easily buy Radeon in the COVID chips shortage while NVIDIA was impossible.   Thereâ€™s zero demand for Radeon. Zero. Nada. Nobody wants to buy their GPUs. If the price raises you can be well assured itâ€™s just price gouging which will bounce back when shop owners realize nobody is buying anyway.,AMD,2026-02-20 13:39:46,-1
AMD,o6e8g5w,Have they tried the Nintendo way? I heard a lottery system will make things sell faster if someone from AMD gets to buy one. /s,AMD,2026-02-20 09:36:44,-1
AMD,o6kjfxc,"Claw 8's XeSS 3 blows past AIMAX395's FSR 3.1 in quality, at 1/3 the priceâ€”pure joke on the pricier 395.",AMD,2026-02-21 08:13:00,-2
AMD,o6bt4q9,You mean 9070XT?,AMD,2026-02-19 23:07:47,20
AMD,o6fcva4,"The reaper is $960 on Amazon right now, I know it isnâ€™t $900 but itâ€™s significantly cheaper than the others, and you get a free code for crimson desert and 3 months of Amazon Music when you buy it. Not a terrible deal imo.",AMD,2026-02-20 14:22:16,6
AMD,o75dn6v,I feel like I hit the lottery.  Got one for $529 after tax yesterday at microcenter,AMD,2026-02-24 15:20:03,1
AMD,o6dzg7t,"yeah i just checked, i bought the sapphire pure 9070xt for 607 euros around October and its listed for 750 currently",AMD,2026-02-20 08:11:07,14
AMD,o6eqzy0,"Same for 9060XT 16gb, it was under 400â‚¬ late 2025, it's around 450â‚¬ since january",AMD,2026-02-20 12:13:32,4
AMD,o6ch0yw,Still working for RAM and storage. Less PC building -> lesser sale for AMD.,AMD,2026-02-20 01:28:53,23
AMD,o6e8ldw,"AMD always just follows market price. When Nvidia can't supply enough cards given they have 90+% of the market, of course everyone's prices are going to go up. This is just one of the downsides of a monopolistic market: Nvidia controls the tap.",AMD,2026-02-20 09:38:05,13
AMD,o6d4qm6,"Only works if you at top and aren't hated for having bad drivers, especially early on on each new generation.",AMD,2026-02-20 03:57:32,-15
AMD,o6f0bmm,"Have you seen their production numbers!? Of course they should be blamed!  But AMD knows even when they make <10% of quarterly shipments, they'll still sit on shelves.  AMD isn't new to this rodeo, they learned to not flood the zone and will just milk their loyalist.",AMD,2026-02-20 13:13:57,1
AMD,o6f5658,Oh no the demand for the GPU with sales so low it doesnâ€™t appear in the hardware survey is too high!,AMD,2026-02-20 13:41:23,-1
AMD,o6hxvvf,Where were you getting Radeon chips in covid? Because the last time I remember the 6800 XT was launched at $650 and was at least $1800 during COVID and crypto boom,AMD,2026-02-20 21:43:29,8
AMD,o6dc292,No they are reminiscing the good olâ€™ day of â€˜02 with the ATI Radeon 9700,AMD,2026-02-20 04:49:39,46
AMD,o6jb14i,"For perspective, It was $579 at Microcenter for most of December.",AMD,2026-02-21 02:25:47,6
AMD,o6eld4i,Bought the acer bifrost 9070xt for 620â‚¬ almost 3 months ago. Now it's listed for 869â‚¬.,AMD,2026-02-20 11:31:11,4
AMD,o6dzuoi,This current generation NVIDIA was the one f-ing up the drivers at launch. It  took them almost 6 months to get their stuff together. Every tech publication was talking about it and forums were full of complaints.,AMD,2026-02-20 08:14:58,13
AMD,o6enpz0,"Had a 4080 super and now a 5090. Have problems with many games crashing on me and it is always a driver problem. And the games are not even unknown shit, Forza horizon 5 and hogwarts legacy just to name a few.",AMD,2026-02-20 11:49:37,2
AMD,o6ezrib,Holy crap - I'm old!  Bonus point if you got the right 9500 back then and soft modded it to a 9700!,AMD,2026-02-20 13:10:40,11
AMD,o6iwh1c,That was one sick card. Prolly the best value of the generation,AMD,2026-02-21 00:54:47,3
AMD,o6inhnn,I just bought an R9700,AMD,2026-02-21 00:02:46,1
AMD,o6jh85u,Thatâ€™s a great price. Unfortunately we donâ€™t have that in Canada lol. I paid $630 USD for my XFX swft 9070 on Amazon just a month ago. The XT I just couldnâ€™t justify the extra cost.  Same exact card is now $750 USD on Amazon. Wild,AMD,2026-02-21 03:05:08,3
AMD,o6ekk3z,Just this gen because AMD has been doing it every generation......,AMD,2026-02-20 11:24:39,-6
AMD,o6hyo34,"Interesting that this has happened multiple times in the history of Radeon. Other examples that are top of mind for me: 5700 â€”> 5700xt, Vega 56 â€”> Vega 64.  I canâ€™t think of any nvidia examples",AMD,2026-02-20 21:47:25,4
AMD,o6j8ycc,"I bought into ATI / AMD every time this was an option, owned all 3 of those over the years, but I also had the R9 290 -> 290x softmod.",AMD,2026-02-21 02:12:47,1
AMD,o70aq8m,The RX 460 could also be bios modded to unlock additional shaders,AMD,2026-02-23 19:46:23,1
AMD,o6v50vt,"Hey OP â€” Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2026-02-22 23:55:19,-6
AMD,o6v520s,"Hey OP â€” Your post has been removed for not complying with rule 9.   Discussion of Politics and/or Religion, including topics closely associated are not allowed on /r/AMD.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2026-02-22 23:55:30,-5
AMD,o419xp2,"only +23% in Linux feels like bad drivers, when the B390's getting 2x performance on Windows",Intel,2026-02-07 05:33:34,23
AMD,o41k2xt,Good. AMD has been rehashing same crap lately. This is why competition matters,Intel,2026-02-07 07:01:00,21
AMD,o426cd8,AMD 14nm moment,Intel,2026-02-07 10:36:23,10
AMD,o42ncwr,"Nice, I'm tired of AMD rebranding old APUs and charging for them as if they were brand new.",Intel,2026-02-07 13:01:43,6
AMD,o420dq7,What about 8060s? Though?  Who fucking cares about 890M? Itâ€™s made for business laptopsâ€¦,Intel,2026-02-07 09:37:29,0
AMD,o3zmils,I want Intel to win but sadly they only win against the HX370 which is a generation behind the 395+. So intel is still behind. They better price it well below the HX370 to regain market share. AMD lost the plot in terms of pricing and HX370 handhelds are well over 1300. If they can partner with MSI and produce a handheld for under 1000 they have a winner. Anything above that is DOA.,Intel,2026-02-06 23:14:29,-25
AMD,o41ay0s,considering sr-iov and passthrough on linux are completely busted in the xe drivers for the B50 and that has been out for months it wouldn't surprise me if these are rough as well.,Intel,2026-02-07 05:41:39,7
AMD,o421h3i,RDNA 3.5++++,Intel,2026-02-07 09:48:29,15
AMD,o45uzpk,"Yes since they have this new loser as the head of client they don't build anything new. They just recycle old shit with new marketing name. I am sure he will kill the business in 2 years. Saving grace is AMD is untouchable right now for desktop, and Intel sadly is full of useless VPs that spend their days in meetings.",Intel,2026-02-07 23:14:29,1
AMD,o42u6kw,"8060s still comfortably beats out the B390, but it's designed for a different power class and draws a lot more power.",Intel,2026-02-07 13:44:59,11
AMD,o45a3to,"The 890M is in the Z2 Extreme. It's not just for laptops, it's their prime handheld GPU chip.  The 8060S only starts beating the B390 around 33 W, making it unfit for a handheld device where battery life is very important",Intel,2026-02-07 21:19:03,3
AMD,o3zrsvz,the 395+ is a different segment of product imo. They start at $2200+ while the intel chips start around \~$1200 so far. The battery life also isn't even close.,Intel,2026-02-06 23:45:02,40
AMD,o3zrwwh,Panther lake and 395+ canâ€™t be compared the 395+ is a much bigger die and with double the bus theyâ€™re completely different tiers,Intel,2026-02-06 23:45:41,26
AMD,o40cvka,"Strix Point and Strix Halo are the same generation, one is just much bigger and more expensive and for an entirely different performance tier. You're coming into a 5050 vs B580 comparison and saying that a 5090 is faster than the B580. Duh, of course it is.",Intel,2026-02-07 01:50:11,10
AMD,o5d0wok,"I saw reviews that b390 beats 890m only when 890m is power limited, and when 890m APU is allowed to draw its full PL2 power of 54w it beats b390 by ~29%",Intel,2026-02-14 16:29:55,1
AMD,o40lyhs,The price of panther lake is gunna be expensive too,Intel,2026-02-07 02:46:26,-3
AMD,o424dp4,"Exactly. I was really disappointed with their igpu tactics. Rehash, rehash, rehash.   I do think we need DDR6 for next major igpu scaling, as memory bandwidth is a problem too.",Intel,2026-02-07 10:17:09,7
AMD,o4trmyr,The 16C+40CU strix halo sucks down power.  I want to see the 8C+40CU or 8C+32CU halo's compared,Intel,2026-02-11 16:50:04,3
AMD,o40emvu,Which laptop is releasing at $1200 with the B390,Intel,2026-02-07 02:00:58,2
AMD,o45uscn,"Yes agreed, but if the machines offering them are in the same ballpark then it is a failure on intel. A machine sporting the B390 should be cheaper than an equivalent machine with the HX370. We need handhelds with B390 below the 1000 mark for example.",Intel,2026-02-07 23:13:16,1
AMD,o4ejoec,Is LPDDR5X/DDR5 bottlednecked already?,Intel,2026-02-09 08:47:30,1
AMD,o40fa3x,"[https://www.bestbuy.com/product/hp-omnibook-x-copilot-pc-16-2k-oled-touchscreen-laptop-intel-core-ultra-x7-358h-2026-32gb-memory-1tb-ssd-meteor-silver/JJGW34X2K5/sku/6665780](https://www.bestbuy.com/product/hp-omnibook-x-copilot-pc-16-2k-oled-touchscreen-laptop-intel-core-ultra-x7-358h-2026-32gb-memory-1tb-ssd-meteor-silver/JJGW34X2K5/sku/6665780)     there's this $1400 one. there's also only like 5 available series 3 laptops. if this 16"" is starting at this price, I'd expect the 14"" laptops to be a little cheaper.",Intel,2026-02-07 02:04:52,12
AMD,o4emvxo,For igpu purposes? I am sure it is. Compare to bandwidth dGPUs require. the difference is stark.,Intel,2026-02-09 09:19:22,1
AMD,o41rwbo,This seems like a significant outlier from what I can see and it's unavailable so unclear if it's an error.Â    I also can't see this on the hp website.   Guess we'll have to wait and see. From what I've researched the x /h processors are significantly more than the base,Intel,2026-02-07 08:14:15,2
AMD,o416ch9,it's unavailable .....possibly it's a subsidized product.....,Intel,2026-02-07 05:05:43,-4
AMD,o428rdn,"yep, this  basically all the new B390 laptops lists around a same ballpark price as the 395+ ones  just over 2k USD",Intel,2026-02-07 10:59:32,2
AMD,o41au1b,"it ran out of stock a couple days ago. There's a few open box ones you can still pick up though. most of the panther lake laptops just aren't released yet. They might go up in price due to ram, but everything else will as well unfortunately.",Intel,2026-02-07 05:40:45,6
AMD,o45ul90,"That is my point. Not worth the price if it barely beats the HX370. And just the fact we need to have this conversation shows how Intel Marketing miserably failed. If the B390 devices are cheaper, please make sure you plaster it everywhere and communicate it well. Even the people interested in it like me do not know or believe it.   Again if anyone at intel reads this, you better price this lower than the HX370 or you have no chance. I do not like AMD but sadly they have the best overpriced products right now.",Intel,2026-02-07 23:12:08,1
AMD,o45vxqk,this sounds about right.  Intel just said that the margins are PTL are below corporate average. which means yields on 18a are abysmal.  Expect prices to be high and supply to be low until yields improve (which the CFO head said wont be till 2027. But they should gradually improve through 2026.  AMD and intel are in a race. Intel need to improve 18a yields. AMD needs to get zen6 out. Whoever gets there first will be the winner. Until then it kinda looks like the laptop market is just a continuation of what it was before. AMD gets a slight pickup from the 400 series. Intel gets a slight bump from PTL.,Intel,2026-02-07 23:20:02,1
AMD,o431u16,well why don't they restock it ??,Intel,2026-02-07 14:29:14,1
AMD,o5j4oky,"I would not expect them to care much about the DP4a version moving forward. It was a way for them to leverage themselves into the install base and get people to try it, but there's no real reason to keep investing in it.",Intel,2026-02-15 16:37:00,32
AMD,o5j81z7,"DP4a wont get any more updates, every major gpu manufacturer moved on to their ML-based hardware solutions.",Intel,2026-02-15 16:53:10,20
AMD,o5jfy3r,Why is there an image of XESS4? I believe the latest XESS version is 2.1 at least back in July 2025?,Intel,2026-02-15 17:31:45,2
AMD,o5vehco,This is not true. XESS-SR was updated to version 2.1.,Intel,2026-02-17 14:37:24,1
AMD,o5jajbr,XeSS is still abysmally dogshit in ghosting as it received zero major model updates since 1.3.1. Intel might surprise us with a transformer model when celestial releases.,Intel,2026-02-15 17:05:10,-1
AMD,o5oura0,Okay? They are also not releasing XMX upscaler updates,Intel,2026-02-16 14:44:26,1
AMD,o5jiitj,Never say never,Intel,2026-02-15 17:44:32,-2
AMD,o5lwc5u,DP4a is ML based it just runs the instructions on GPGPU hardware.,Intel,2026-02-16 01:21:54,3
AMD,o5jifqr,"It's XeSS 3 right now. At least everyone started to write XeSS 3 in the title, when they were talking about the recent MFG update",Intel,2026-02-15 17:44:07,4
AMD,o5wk2bm,The main focus of XeSS 2.1 was 2x FG for non-Intel gpus. For upscaler they had only small bug fixes and nothing else,Intel,2026-02-17 18:02:46,1
AMD,o5wkabl,XeSS SDK 2.1.0 Introducing XeSS Frame Generation with Xe Low Latency support for non-Intel GPUs:  - Requires Shader Model 6.4 support - Standalone XeLL is not supported on non-Intel GPUs  XeSS Super Resolution Vulkan API Enhancements:  - Improved error reporting - Improved Vulkan validation layer support - Bug fixes and stability improvements,Intel,2026-02-17 18:03:46,1
AMD,o5jk4cn,"The thing is that there isn't market for it.  DLSS has most people covered, Intel is doing their own solution, and AMD is trying pass the page on RDNA2 and 3 (which is a minuscule market to begin with).  Its a development that benefits no one.",Intel,2026-02-15 17:52:18,14
AMD,o5m2cyw,You are correct yes but everyone knew what i meant. i think.,Intel,2026-02-16 02:00:36,2
AMD,o5lw6ge,"> The thing is that there isn't market for it.  That's short sighted. There is a huge swath of radeon owners that try XeSS, realize it's better than FSR, and then plan to get an Intel GPU for their next upgrade cycle (or sooner).  Especially since FSR4 isn't actually being put IN games, when XeSS has a back catalog and new games coming.",Intel,2026-02-16 01:20:53,3
AMD,o5m2j4l,"Sure, but you dont need to actively develop to get that effect.  FSR2 and 3 are booty and AMD is actively trying to sunset all older cards, so no future generations are subject to that phenomena.",Intel,2026-02-16 02:01:43,2
AMD,o2chc3i,Whatâ€™s insane is the IPC of the Darkmont E cores in Pantherlake seems to be better than AMDâ€™s zen5 P cores.,Intel,2026-01-29 03:26:27,54
AMD,o2k7huk,"Now they just need to put this in a desktop package, slap some BLLC (Big Last Level Cache) on that mfer and we're talking. And by the time they've done that, Zen 6 is here. So we'll see. The end of the year will be interesting.",Intel,2026-01-30 06:53:26,11
AMD,o2gkb7q,"I'm very hopeful for Intel on this gen and Nova Lake I'm very excited about.Â    Intel in benchmarks though was always hit or miss due to cache issues and lack of avx 512. Canned benchmarks they did good on, but a lot of apps were starting to get avx512 optimized more and they would lose HARD there. Not only that but there were certain games where it was just catastrophic, and not due to windows e core issues.Â    Look at Homeworld 3 CPU benchmarks (I know it sucks).",Intel,2026-01-29 18:48:07,13
AMD,o2fk8dr,Give me barlett lake for lga 1700. Its all I care about.,Intel,2026-01-29 16:06:35,11
AMD,o2xo1k6,"Great to see good competition, we need to wait for Zen 6 for more equal comparison (18A vs 2nm cores)",Intel,2026-02-01 07:37:01,1
AMD,o2e7a5q,"AMD doesn't really have P cores  Great job Intel though, this is what we like to see",Intel,2026-01-29 11:40:54,23
AMD,o2dxto2,It's Mobile Zen 5 Cores that have lot less cache,Intel,2026-01-29 10:21:41,9
AMD,o2egfae,"Why? AMD still uses 5/4nm, that is a tech from 2020.  They can move to 3nm, and their performance will be in pair. They just did it to make as much profit as they can.",Intel,2026-01-29 12:44:46,-8
AMD,o2ksewg,Yeah. We gonna have a 28 core with bllc cache with novalake. They also making a 28 core with no bllc cache.,Intel,2026-01-30 09:59:36,3
AMD,o2fwqez,"Intel has this golden opportunity to just rule the new DDR4 world while AMD plays pretend like people are still buying DDR5, but instead Intel wants to play pretend, too.  It's sad how stupid Intel's strategy people are.",Intel,2026-01-29 17:02:11,8
AMD,o2llp44,"This. As a early adopter of the 12900K, all the shait we had to go through with Intel/W11 software just to make it work and not to mention 13/14th gen issues.. I mean sure, now 12900K is superb, but still. We deserve Bartlett Lake K.  Would love to OC that juicy chunk of a CPU.",Intel,2026-01-30 13:33:01,1
AMD,o2h0i6b,still much bigger than darkmont E core,Intel,2026-01-29 20:04:06,14
AMD,o2e5xiz,18mb vs 16mb so a pretty fair comparison.,Intel,2026-01-29 11:30:21,18
AMD,o2h9ta3,The e-cores have no micro-op cache.,Intel,2026-01-29 20:48:58,0
AMD,o2em3lb,"Except it's not as easy as cramming more transistors into the same space... You can increase perf/W that way, but not really an IPC, that's all about architecture.",Intel,2026-01-29 13:19:10,31
AMD,o2esiez,"> They can move to 3nm, and their performance will be in pair.   A pure node shrink doesn't give you any IPC benefits. IPC is dictated by the underlying architecture. It might help their boost clocks and energy efficiency, but it doesn't improve IPC.",Intel,2026-01-29 13:53:52,20
AMD,o2g16jj,Except we are running out of ddr4 stockpile and the prices are starting to reach parity unfortunately.,Intel,2026-01-29 17:22:20,17
AMD,o2jj1t8,"TF? AMD literally just released 5900XT which is basically a renamed 5950X not too long ago and still selling. Also, don't bet on DDR4 as those prices are hiking too now.",Intel,2026-01-30 04:00:00,4
AMD,o2egwrm,He's saying mobile zen 5 has less ipc than desktop zen 5,Intel,2026-01-29 12:47:54,12
AMD,o2idkwe,Panther Lake is also Mobile. So itâ€™s a fair comparison.,Intel,2026-01-30 00:09:02,9
AMD,o2kcbrx,Yes I agree but does ipc differ between intle mobile and desktop with the same core?,Intel,2026-01-30 07:33:55,0
AMD,o2ken0q,Spec is dependent on Cache size and Memory Latency outside the core IPC,Intel,2026-01-30 07:54:12,5
AMD,o2kep89,Yep,Intel,2026-01-30 07:54:45,3
AMD,o1wy1s0,"Let's just wait for products to hit, maybe Intel is going to hit it out of the park, but they have a long way to go before I will trust their marketing slides.   AMD is no better in that regard, I'm not just shitting on Intel.   In all honesty, I'm pretty happy Intel is doing better on the iGPU side, cause it was so bad for so very long.   My laptop is currently an i7-1185g7, not their best showing, but was a step in the right direction on the iGPU side.   Not anything I would have bought brand new, but being a refurbisher in my spare time has perks.   My laptop is always 3-6 years old.",Intel,2026-01-26 23:15:08,5
AMD,o1xyg0n,Intel's been down this road before. It's not worth it. 14900HX is the only reminder you need. How fast does it need to be before we start talking about just putting a GPU in? If I'm getting a laptop why would I get a power hog collapsing star? It's called mITX.,Intel,2026-01-27 02:26:55,13
AMD,o1yqu4g,Spinning a delay of building a competitor into this? Good try,Intel,2026-01-27 05:18:16,2
AMD,o23ohno,"To completely compete with Strix Halo (really just the graphics side) would require intel to increase the die size just like AMD did. In practice, even in gaming, it isn't worth it. Yes, you get between 20-30% more performance with Strix Halo up to 65W, and then pushing 80w+ Strix Halo opens up further to closer to 40-50% better performance. Thing is, it just doesn't make sense to create a large APU with those power requirements for full performance in today's market... especially at the prices of Strix Halo products. You can just get a solution with a dGPU at those prices.  I just wish Intel would push for Linux compatibility. Panther Lake is being held back by the suck that is current Windows 11 imo.",Intel,2026-01-27 22:14:03,1
AMD,o2mvyy4,"Hmm, that's a shame.  I really like Strix Halo.  They're niche, no doubt, but with NVIDIA having DGX Spark, and AMD releasing a ""competitor"" later this year, kind of surprised Intel is going to ignore the compact AI development market.",Intel,2026-01-30 17:11:27,1
AMD,o1wzz6y,"From what I've seen today on reviews, they really trade blows at different TDP ranges. I'm yet to see benchmarks that come out with the 70 something percent gains Intel claimed.",Intel,2026-01-26 23:24:58,-8
AMD,o1yy03n,"LPDDR tends to be more power efficient than GDDR, especially when shared with the CPU. Not to mention you lose less power to communication. In other words, an iGPU will always be more power efficient than a dGPU.",Intel,2026-01-27 06:11:47,11
AMD,o1ywlnk,It still might not be a competitor because it's going to be expensive as hell. If one wants the bleeding edge chip they have to fork it out.,Intel,2026-01-27 06:00:59,5
AMD,o1xi2af,The 70% claim is against Strix Point which is the similarly sized offering from AMD. It only trades blows with Strix Halo because that's a far larger (and expensive to manufacture) chip whose gpu portion is as large as a desktop RX 7700. More cores at lower clocks means better efficiency.,Intel,2026-01-27 00:57:44,20
AMD,o1xhl0p,iGPU?,Intel,2026-01-27 00:55:13,9
AMD,o245brf,"Itâ€™ll use less power, and itâ€™ll do less work per watt too.",Intel,2026-01-27 23:36:54,2
AMD,o27qbi3,"By ""this"" I meant the quote. As much as Pantherlake is great, I don't believe they were able to pull an AX launch off in time but simply didn't",Intel,2026-01-28 13:56:02,1
AMD,o1xxwb1,It only trades blows at low power. Once you get higher wattages in it does not. But efficiency wise I think id trade my strip halo for an extra hour or battery gaming even with the lesser performance as I won't feel it much in the type of games I play. Panther Lake is a strategy gamers dream chip. Thin and light that can play the games at 1080p for 3 hours on battery.,Intel,2026-01-27 02:23:52,4
AMD,o24uoot,This is more based on die size than work per watt.,Intel,2026-01-28 01:48:07,3
AMD,o28kg6w,Nah,Intel,2026-01-28 16:17:34,1
AMD,o1yfxob,"I intended to say its large size was what allowed Strix Halo to also be competitive at certain low power ranges, at higher ones it completely dominates, you are absolutely right.",Intel,2026-01-27 04:06:44,5
AMD,o24vwrb,"Die size is going to limit channels more than anything though, right?",Intel,2026-01-28 01:54:40,2
AMD,o1ufz7y,Only 6yrs later than the M1 launch and about 9.5yrs after Intel learned that Apple was developing their old SoC independently.,Intel,2026-01-26 16:38:04,47
AMD,o1wjkyo,"dear god pls give me a zenbook with an OLED screen and all day battery life ðŸ™ðŸ™ I don't need m5 performance, I just need to be able to have my terminal and browser open all day without needing to plug it in twice a day.",Intel,2026-01-26 22:05:48,22
AMD,o1u7jyg,Nice! They will be sweet to run Linux. 30h battery life?,Intel,2026-01-26 16:01:54,12
AMD,o1uctja,"If only Windows could catch up with Apple MacOS  Panther Lake development was chaotic and last year, they were not sure if 18A was going to be ready, but was worth the risk  I am more excited about Nova Lake ( wont be on Intel node)",Intel,2026-01-26 16:24:35,17
AMD,o1ughit,"Panther Lake is very impressive, but this title is clickbait. Apple still is the leader overall, in both performance and efficiency.",Intel,2026-01-26 16:40:12,8
AMD,o1uhyjv,And itâ€™s deleted before i read ðŸ¤£,Intel,2026-01-26 16:46:25,2
AMD,o1uqg9h,I get a 404 error clicking that link. Is it pay-walled? Iâ€™ve searched for it on the main page and it doesnâ€™t come up.,Intel,2026-01-26 17:22:49,2
AMD,o1z60rh,"So a high-end 16 core Intel chip beats out Apples lowest end chip with 10 cores in multitasking, is that really an answer? Apples chip is 50% faster in single core, and they have two tiers of chips above their low end.   Intel is very far from where Apple is, if these metrics are a good example of where they stand.",Intel,2026-01-27 07:17:17,2
AMD,o1vuxkh,QCOM and their pipe dream of Windows on ARM are done for. There's literally no reason to buy a Snapdragon X chip,Intel,2026-01-26 20:16:11,3
AMD,o1vte62,"No it wonâ€™t. At least until the manufacturers get their shits together. I was looking for a mobile and somewhat capable, second laptop and every single brand has some serious shit. Especially related to usb-c ports and mainboards.",Intel,2026-01-26 20:09:23,1
AMD,o1w1k0z,"They have to, otherwise they will be out of business bc arm",Intel,2026-01-26 20:45:33,1
AMD,o1wzkmp,Iâ€™m more curious about ultra low power CPUs and how theyâ€™ll perform. I really want a low power truly fanless laptop for light tasks.,Intel,2026-01-26 23:22:54,1
AMD,o1zpf3q,"lol, its way behind even m4 and under desktop can't beat ryzens. I do wish them good luck though, getting bored with AMD(slacking a lot) becoming intel..",Intel,2026-01-27 10:14:54,1
AMD,o2687wm,"No, it's not. ðŸ¤”",Intel,2026-01-28 06:56:23,1
AMD,o28grp0,My laptop is slow as hell and has a hard drive 8gb ram Intel i5 8th gen and a integrated graphics card how I make my laptop fast,Intel,2026-01-28 16:01:42,1
AMD,o2dnbq7,"I honestly can't recall the last processor(s) I was anticipating as much as this, except perhaps Lunar Lake and the Snapdragon X Elites. Those were primarily focused on battery life and Panther Lake and The X2 Elites are the processors we really wanted the prior gen to be. I believe this will be the year of lightweight, long battery, performant notebooks with incredible screens, that you can game on. It's quite awesome. I expect this to go down as probably tbe biggest jump in compact performance ever.",Intel,2026-01-29 08:44:12,1
AMD,o4aqlk5,@ title     plus it gamesâœŒï¸,Intel,2026-02-08 18:53:53,1
AMD,o1v0ce7,"lol this is like the 3rd time Iâ€™ve seen headlines calling something the windows answer to apple silicon. First it was the latest windows ARM, then it was AMD Strix Halo, and how its intel Panther lake. Donâ€™t get me wrong, Iâ€™m stoked for Panther Lake, but it is diminished by their reliance on Windows for the operating system.",Intel,2026-01-26 18:05:58,1
AMD,o1vv3cn,I very doubt,Intel,2026-01-26 20:16:53,0
AMD,o1uh7xf,Eh as per the article the only thing that stands out is the iGPU and that's it. The rest is on par with a mid-range Ryzen AI 7 350. And you pair that with a 5050 and boom: Double the FPS of the Intel.,Intel,2026-01-26 16:43:16,-6
AMD,o1vlq7a,Better late than never. It's a very solid response. I do hope they keep pushing as we really need CPU and GPU competition.,Intel,2026-01-26 19:36:03,19
AMD,o1xa9il,"Move slowly, but surely",Intel,2026-01-27 00:17:39,8
AMD,o1vffx8,Apple switched because of Intel node issues and now they are considering using Intel fabs for their own chips. Could have just stayed on Intel,Intel,2026-01-26 19:09:07,-9
AMD,o1wmdz8,Oled screens tend to burn battery faster.,Intel,2026-01-26 22:18:47,11
AMD,o1y37z4,"It already exists. The Lunar Lake will still have better battery life than Panther Lake, UNLESS you get the 4-core GPU which will result in worse performance for the GPU than the current lunar lake in the zenbook",Intel,2026-01-27 02:53:03,3
AMD,o1yhrze,This is all I want/wanted I caved and got a macbook air a few months ago. Damnit. Some of these machines look incredible,Intel,2026-01-27 04:18:11,2
AMD,o36okur,"Exactly! Windows 11 is horrible, Apple Silicon/ARM's Linux compatibility is not good. Linux + Intel = YES",Intel,2026-02-02 17:05:56,1
AMD,o1uvxb2,>If only Windows could catch up with Apple MacOS  I have 3 wishes.    Mac: better emulation layer for games like with steam OS using proton. A more seamless experience.    Windows: better ARM support.   Linux: easier emulation of other applications like adobe suite/teams/(insert windows program not on linux),Intel,2026-01-26 17:47:07,7
AMD,o1ul68c,Do we really have any confirmation on Nova Lake node at this point in time?,Intel,2026-01-26 17:00:01,3
AMD,o22hdnr,Windows doesn't need to catch up so much as stop ruining itself.,Intel,2026-01-27 19:01:18,1
AMD,o25ybqw,"Same if it has AVX10.2 it will likely be the best desktop CPU, and make most HEDT irrelevant",Intel,2026-01-28 05:39:35,1
AMD,o2cq8jn,TBH i like windows over Mac. only think i like about apple is their M series chips. Never interested when they had intel chips,Intel,2026-01-29 04:21:24,1
AMD,o2b8d7q,And Mac OS for those who swing that way ... mostly good on the outside (not loving the latest design tho) and Unix is just a terminal session away. I use Mac OS and Windows about 50/50 but vastly prefer Mac OS.,Intel,2026-01-28 23:23:40,1
AMD,o1uqin0,Same for me.,Intel,2026-01-26 17:23:06,1
AMD,o1us3oe,"Seems like it got deleted, don't know why.",Intel,2026-01-26 17:29:55,2
AMD,o1wpzwv,"Unless you want better battery life, reliable wake from sleep, better cpu and multi-core perf.   Apple proved you don't have to be gaming first to be successful. Intel only wins in gaming, nothing else, and only against a almost 2 year old Snapdragon SKU. Lets compare to X2 when it comes out.  This is like saying Intel just destroyed AMD, maybe in some ways, until AMD introduces their next gen, that's the way it works.",Intel,2026-01-26 22:35:43,-7
AMD,o1vulvx,You already own a laptop with a Panther Lake chip? ðŸ˜‚,Intel,2026-01-26 20:14:45,1
AMD,o1ulb4v,"Itâ€™s way ahead of midrange Ryzen, itâ€™s basically on par with high end Strix Point while delivering better battery life.",Intel,2026-01-26 17:00:36,9
AMD,o1ui763,Don't forgrt the amazing battery life.,Intel,2026-01-26 16:47:26,4
AMD,o21n3g4,What do we possibly need cpu performance for or gpu?  All software is bloat. Its debloating we need.,Intel,2026-01-27 16:51:49,1
AMD,o1vmtvy,Not really true. Apple switched so they could use ARM and control the design and vertical integration and intel wasnâ€™t a foundry.,Intel,2026-01-26 19:40:51,11
AMD,o1wvi1p,"oh definitely, it's just that after having it I can't go back. the thing is I don't even do any heavy compute on my laptop so I don't think I'm asking for anything unreasonable. most of my day is spent in an ssh session to my home server and a browser, discord, spotify. I just need like 8 hrs screen on time and I'd be happy. I get like 3-4 right now on my zenbook 14X and that's just laughably bad.",Intel,2026-01-26 23:02:15,7
AMD,o1x7x56,"If you gonna look at something all day, better make it good. Same thing with your bed, you spend so much time on that thing, might as well be good.",Intel,2026-01-27 00:05:48,10
AMD,o25y65o,"Panther lake is more efficient at all power bands, why would lunar lake have a longer battery life?",Intel,2026-01-28 05:38:28,1
AMD,o1v50a4,Proton is translation layer for windows directx api to vulcun  in Linux.  The translation layer for x86 to arm is way more difficult than what proton does,Intel,2026-01-26 18:25:28,8
AMD,o1x5fv9,> Do we really have any confirmation on Nova Lake node at this point in time?  Intel's publicly confirmed they're also using TSMC for compute. Only one thing makes sense.,Intel,2026-01-26 23:53:06,2
AMD,o1uq0o2,"18A is not in the same league of N2.   Even 18AP is not on par with N2   14A is not ready  So unless they want inferior product to AMD (Zen6) which will be on N2, NovaLake will be on TSMC N2  14A is when Intel expects to match or surpass TSMC nodes",Intel,2026-01-26 17:20:59,-8
AMD,o1wr9b8,"Tell me you haven't watched the reviews without telling me. Lol ""better battery life"" has been QCOMs only argument vs Intel/AMD and now with Panther Lake's superior battery life, QCOM has nothing more going for it. The X2's efficiency is not going to be that much of any more than the X.",Intel,2026-01-26 22:41:39,6
AMD,o2v617d,"Snapdragon has no advantages. It is not cheaper, it is not faster and it does not have better battery life at equal performance. Add the ARM performance translation tax for x86 apps and they are donezo.  Qualcomm needs to heavily discount and increase performance at the same time to have any chance to win.",Intel,2026-01-31 22:03:32,1
AMD,o1vgx5e,Keep on dreaming. Numbers speak for themselves.,Intel,2026-01-26 19:15:21,-5
AMD,o1uld9q,"I've mostly been picking up AMD as of late, but this is a great win Intel's needed for years. About the only potential downfall is pricing, but given that AMD has nearly zero design wins for Strix Halo on laptop, Intel has a pretty wide open field to work in.",Intel,2026-01-26 17:00:51,5
AMD,o1uirjk,The Ryzen will be better. Always been,Intel,2026-01-26 16:49:51,-11
AMD,o21ptj8,In theory you are correct in practice that isn't happening so...,Intel,2026-01-27 17:03:28,2
AMD,o280xud,"You can easily disable 90% of the bloat. It's especially easy if you have Windows pro. Removing bloat freed up over a GB of RAM on startup for me. But yes, the bloatware is ridiculously atrocious",Intel,2026-01-28 14:49:50,1
AMD,o1vn0ec,No thatâ€™s the spin. Apple was mad about battery life and performance on laptops.,Intel,2026-01-26 19:41:38,7
AMD,o21wkh6,Actually having a mostly back background is good for OLED as it can actually save battery. So console or terminal is good.,Intel,2026-01-27 17:32:51,1
AMD,o21wbym,"Actually for just text rendering, IPS is still better. But for colors and media and gaming, OLED wins.",Intel,2026-01-27 17:31:47,1
AMD,o263gj0,The 17W Panther Lake has a worse GPU than Lunar Lake (17W) is what I'm saying. If you want the good GPU then it will have worse battery life because that's 25W+,Intel,2026-01-28 06:18:28,1
AMD,o1vc3ax,Argh! My mistake!,Intel,2026-01-26 18:54:58,4
AMD,o1v4ocs,Does it really that matter? I mean intel 12th and 13th  gen was on par or even better than zen3 and zen4 .  Both intel 12th ans 13th gen were on intel 7 or 10nm while zen  3 was 7nm and zen 4 at 5nm.  If anything I don't get how intel dropped ball so hard with arrow lake . Going from 10nm(intel 7) in 14th gen to 3nm of tsmc in arrow lake should produce both efficiency and performance gain like how nvidia got going from samsung 8nm to tsmc 4nm in 30xx to 40xx.,Intel,2026-01-26 18:24:06,8
AMD,o1wt2oz,"X2 is already rated 45% more efficient than X1, I've watched the reviews, but more than that I've had my hands on real hardware. Try actually informing yourself before responding. I'll just leave you with that, obviously on an Intel reddit people are going to be super defensive about their ""team"".",Intel,2026-01-26 22:50:25,-3
AMD,o1vhj64,"Yes, the numbers speak for themself on NotebookCheckâ€™s review of PTL, which places it ahead of Strix Pointâ€™s multi core and single core, and way ahead in efficiency.",Intel,2026-01-26 19:18:00,5
AMD,o1ul00o,Nothing indicates that,Intel,2026-01-26 16:59:17,7
AMD,o1vcrkw,Is this a troll? It's the X7 and X9 are going to be quite a bit better than a 350 are you joking? Do you read benchmarks or reviews at all? The ultra 5 with b 370 is probably gonna be at or above a 350-360.,Intel,2026-01-26 18:57:46,3
AMD,o1wtita,AMD Glazer spotted ðŸ¤¡,Intel,2026-01-26 22:52:34,2
AMD,o25c96y,Well we have to buy new devices just cause some company is too lazy to make a native app and use a web wrapper like whatsapp,Intel,2026-01-28 03:21:16,1
AMD,o1vnubu,Apple was also mad about margins. It cheaper for them to make their own than beholden to Intelâ€™s pricing.,Intel,2026-01-26 19:45:15,7
AMD,o26idnu,"Where did you see that? Iâ€™m not aware of any regressions vs Lunar Lake, and Intel is specifically releasing a handheld sku to deal with the super low power : high gpu performance regime. Â   SKU differences aside, take a 388H and throttle it to 17W and it greatly outperforms lunar lake.",Intel,2026-01-28 08:24:03,1
AMD,o1vkgcn,"Intel 7 was competitive in peak performance, but lacked efficiency. Also note that arrowlake utilized N3b which was notorious for poor yield and over complexity and quickly replaced with N3e in Apples line of products. Arrowlake also followed the 13th/14th gen debacle and Intel reduced ring bus clocks significantly to play it safe.  Most notably, going from a monolithic design to chiplets leads to increased latency which reduces gaming performance and makes it harder to hold higher clock speeds. Monolothic designs will always be faster in a 1:1 comparison but chiplets make up for it by being modular and cheaper to scale, something intel hasnt been able to really take advantage of until Pantherlake.",Intel,2026-01-26 19:30:31,7
AMD,o1veick,"The node helps, obviously, but you still need to design a product that works. The choice of node is not a magical solution.  Arrow Lake had flaws that showed themselves in certain workloads due to design. They \*did\* still get an overall efficiency boost but thats not a high bar given what they were coming from.",Intel,2026-01-26 19:05:08,5
AMD,o1vg7we,"They were experimenting with tiles and itâ€™s an early iteration. Not to mention they have more experience with their own fab.   Arrow lake smokes 14th gen in compiler workloads. Those faster e cores help a lot!   Outside of gaming workloads, itâ€™s fine. Even with gaming workloads it does ok, just not a real jump from 14th gen. It was never going to beat x3d",Intel,2026-01-26 19:12:21,2
AMD,o1wk8a6,It hardly does. Mid range chips from seven or eight years ago can still do everything that modern chips can. All you have is a bunch of tech consumers who convinced themselves that they need the latest tech.,Intel,2026-01-26 22:08:50,1
AMD,o1x5cjb,"> Does it really that matter?  Yes, you're looking at something like 15% perf for a full node. That's very significant. A node shrink alone could constitute an entirely new gen in perf.   > If anything I don't get how intel dropped ball so hard with arrow lake  They redesigned the SoC fabric, and it's shit.",Intel,2026-01-26 23:52:38,1
AMD,o1wtbca,No way! You've had your hands on real Panther Lake hardware already?? But it hasn't even been released yet! You must be a special redditor. Give me a break.,Intel,2026-01-26 22:51:34,6
AMD,o1vizts,I'm talking about Krackan,Intel,2026-01-26 19:24:15,-2
AMD,o1umpjo,This test does: https://www.computerbase.de/artikel/prozessoren/intel-core-ultra-x9-388h-test.95776/seite-5  It looses quite strongly against the 288v (which is not anymore that efficient and far less so than the Ryzen 7 350),Intel,2026-01-26 17:06:37,-3
AMD,o2809da,Any spec sheet will show you it will have only 4 cores for the GPU. Lunar lake has 8. Panther Lake cores would have to be twice as powerful as lunar lake which we know they aren't.,Intel,2026-01-28 14:46:32,1
AMD,o1wr13m,Thanks for explaining,Intel,2026-01-26 22:40:35,1
AMD,o1vjsl5,They still lose handily in efficiency and their multi core isnâ€™t better than Strix Point,Intel,2026-01-26 19:27:42,4
AMD,o1uyy9f,Not really. It loses against the 288v in the first test and wins against in the second test.  There is no direct comparison against the Ryzen 7 350 and I don't know how you even reached the conclusion that the Ryzen 7 350 is more efficient than lunar lake?  [Notebookcheck](https://www.notebookcheck.net/Intel-Panther-Lake-Core-Ultra-X9-388H-performance-analysis-Outpaces-Arrow-Lake-and-exceeds-Zen-5-in-efficiency.1212583.0.html) conducted fairly good efficiency tests that compare the single and multicore CPU perf/W. The Ryzen 7 350 is actually included in their list and ranks dead last in the multicore comparison so there's that and in the single core comparison Lunar Lake generally performs better than both but once again the Ryzen 7 350 is beaten by panther lake.  They even include a nice graph for different PL which also happens to include the Ryzen 7 PRO 350.,Intel,2026-01-26 18:00:04,2
AMD,o28grba,"You are discussing SKUs. The reason lower stack PTL has fewer Xe cores is because they are nearly 2x efficiency.Â   However, should you take a higher SKU and configure TDP to 20W (or even 15W to account for external memory) it will be more efficient than Lunar. Lunar Lake is great, but PTL is much better.",Intel,2026-01-28 16:01:40,1
AMD,o1uz9i9,You'll see but not even in your wettest dreams the Ryzen will be beaten. Intel remains Intel after all.,Intel,2026-01-26 18:01:22,0
AMD,o28nv98,"It needs to be at least 2x as efficient to be as powerful because it has HALF as many cores. It isn't, therefore the GPU will be weaker. You can do that same TDP configuration on Lunar Lake. Not sure why that matters.  ""Nearly 2x more efficient"" translates to ""nearly as powerful"". But not quite.",Intel,2026-01-28 16:32:22,1
AMD,o299e0v,It is 70% faster. Youâ€™re comparing different SKUs. Compare top Lunar Lake to top Panther. How the TDP is configured is up to the OEM,Intel,2026-01-28 18:05:34,1
AMD,o29cjlo,"We don't know how efficient those 28W Panther Lake chips run at 17W. Just because you can run it at such low power does not mean it will work well. It will certainly be much less efficient if it were to be run that way otherwise Intel would be gloating at how powerful and efficient their Panther Lake GPU is at 17W. The fact that they don't means it is well below their sweet spot of efficiency. You are basically starving it, similar to stalling an engine.",Intel,2026-01-28 18:19:00,1
AMD,nz79v91,"The fact that it can even compare to AMDs halo product, which the avg consumer canâ€™t afford is a win for Intel. Intel has plenty on leg room to expand the GPU too.",Intel,2026-01-12 18:02:37,10
AMD,nz5c8tz,This suffers from bandwidth bottleneck. Strix halo is Quad channel while panther lake is dual. An igpu would benifit greatly with a quad channel,Intel,2026-01-12 11:58:24,14
AMD,nzgoxuf,"This thing is absolutely nuts  AMD BTFO unironically, I'm floored. I never, ever would have considered an Intel chip before 2025, now this is the most obvious laptop part ever. AMD is surely sorely regretting recycling the same 780M and 890M chips for another entire gen, betting that Intel would continue stagnating.  This thing is gonna be a monster in handhelds.  I really, really wanted a Strix Halo laptop, but the lack of SKUs, price and the inflexibility with RAM kind of make it unappealing to say the least, not to mention the power draw compared to Panther Lake is unwelcome. These laptops are gonna be probably the best x86 in mobile has eaten in a very long time.  On top of that, it's almost making the 5050 look like a stupid part in a laptop. Why bother when you have a vastly more power efficient iGPU that will handle every desktop workload on top of being viable for gaming?",Intel,2026-01-14 01:47:11,2
AMD,nz4rv7t,"â€œTakes on strict haloâ€ at about half the performance (:  Title aside, this looks pretty great.",Intel,2026-01-12 08:54:08,4
AMD,nzi9w6l,"Amd hasnâ€™t even reached 30% market share mobile yet (oscillating between 20% and 26% since 2020) and are about to be almost wiped from existence again save some low end designs using Ryzen â€œAI 7â€ 445 (6 core, 2+4, 4CU iGP).",Intel,2026-01-14 08:34:10,1
AMD,nznrlo6,"The performance looks fantastic for high end $1k handhelds.    But the ""80% faster than AMD's 890M"" claim is absolute bullshit.  They tested against the HX370 with LPDDR5 5600.  That said, against an 890M that *hasn't* been crippled, it should still be 40-50% faster which is great.",Intel,2026-01-15 02:38:40,1
AMD,nz7d64j,"Will intel make it affordable for consumers though, or price it like LNL (2000+ USD laptops and up)",Intel,2026-01-12 18:17:35,7
AMD,nz7r7or,"""compare"", it is half the performance. Still good for what it is, assuming it is priced right",Intel,2026-01-12 19:20:44,3
AMD,nz5loww,"Well it's not just memory bandwidth. It's got about as much bandwidth as it needs to feed the Xe3 cores.   Panther Lake's GPU tile size is only 54mm^2 while Strix Halo's GPU is 308mm^2. For Panther Lake to compete with Strix Halo it would need 2-4Â times as many Xe3 cores probably. That'd be expensive. There's a reason Strix Halo is so expensive and kind of low volume, bigger CPU more RAM and more expensive motherboard aside.",Intel,2026-01-12 13:04:48,15
AMD,nzbn738,DDR6 can't come too soon for igpus too. But in reality memory bandwidth will stay an issue for a long time. Of course cramping enough compute power in such a format is an issue too,Intel,2026-01-13 09:02:19,1
AMD,nzwms44,Framework desktop motherboard?  https://frame.work/products/framework-desktop-mainboard-amd-ryzen-ai-max-300-series?v=FRAFMK0004,Intel,2026-01-16 12:02:44,1
AMD,nz5dj5h,"Half the performance, half the power, (more than) half the price.",Intel,2026-01-12 12:08:11,15
AMD,nzd3th0,I preordered X7 358H laptop for 1300,Intel,2026-01-13 15:11:48,3
AMD,nzgpiya,Bro nothing's going to be affordable in computer hardware at this rate,Intel,2026-01-14 01:50:29,2
AMD,nzi9j83,There are plenty of LNL laptops around 1000 what you on about,Intel,2026-01-14 08:30:40,1
AMD,nzoqfoa,Lunar lake is in sub 800$ laptops now and 12xe cpus are available for sale 1300$ despite the ram and cpu shortage.   The comparison is good even before likely price hikes for strix halo,Intel,2026-01-15 06:39:34,1
AMD,nz9efnf,"Weâ€™re talking mobile chipsets here, strix halo is what happens when you throw efficiency out the window, with Power (TDP) range, typically from 55W up to 120W. The ultra H 300 has default TDP of 25W, with Maximum Turbo Power (MTP) going up to 65W-80W. Intel has a better design, if they threw 40 XeSS3 cores on it, it would prolly run circles around Strix.",Intel,2026-01-13 00:08:25,4
AMD,nzhjs1h,"Strix Halo is double the die size, this should be compared to Strix Point.  But price will tell everything.",Intel,2026-01-14 04:53:30,1
AMD,nz5xoxm,"> It's got about as much bandwidth as it needs to feed the Xe3 cores.  GPU's will take all the bandwidth you can feed them. It won't help EVERY benchmark, but it will help many.  I'd rather see 256-bit bus on something like this. maybe 192 since you can do that with LPDDR5X etc.",Intel,2026-01-12 14:13:42,4
AMD,nz7ptl5,">Panther Lake's GPU tile size is only 54mm^(2)  is this confirmed for the bigger tile?  edit: also, Halo has all the IO, en-/decoders, etc. in the ""GPU"" tile, so the comparison isn't quite valid",Intel,2026-01-12 19:14:20,2
AMD,nz5j09r,And about four orders of magnitude more availability.,Intel,2026-01-12 12:47:16,12
AMD,nz8m3ma,"> (more than) half the price  Have we seen pricing? Not doubting it, I just haven't seen anything personally but probably missed it.  Strix Halo does seem to be a pretty mythical chip due to its price.",Intel,2026-01-12 21:44:42,1
AMD,nzanyas,Keep in mind that each Xe3 core is about as wide as an AMD WGP. We're looking at 1536 vs 2560 shaders. The B390 is 60% as wide as the 8060S. 20 Xe3 cores would match the 8060S in width. 40x Xe3 is as wide as the 7900GRE.,Intel,2026-01-13 04:15:53,5
AMD,nzgqkco,"Bingo  Strix is also limited by being RDNA 3.5 and no FSR4, so it's rather dependent on raw throughput, and it can't possibly fit in a comfortable handheld that would last for more than an hour and a half under load.  I really, really appreciate what AMD has done historically in the APU space, but it is genuinely time for vendors to start considering Intel. The strides here are absolutely immense. They went from an iGPU being a thing that can do basic graphics and 2D gaming to something that competes against lower end NVIDIA parts at less power draw and can actually legitimately game. It's bonkers. In mobile it's a no brainer.  Of course, it's going to be interesting seeing AMD's next UDNA architecture and what they can pull off, but competition never hurt nobody, and it was sad seeing AMD stagnate in the APU space of all things, their bread and butter that gave them pretty much the entire console market plus the Steamdeck. The entire Windows and Linux handheld market has been nothing but AMD for years. This is even better than Lunar Lake.  We're getting to the point where Intel could legitimately compete in the home console space and make a really great product, but realistically they can't undermine AMD's relationship with vendors at this stage. I hope they keep it up, it would really be cool to see an AMD vs Intel APU console war generation.",Intel,2026-01-14 01:56:20,2
AMD,nzixqmc,It's about 50% bigger die with 1 CCD (which seems comparable CPU performance),Intel,2026-01-14 12:07:34,1
AMD,o21p2wl,https://youtu.be/X9eYQTkzxqU?si=Uzdz-E3QJzzVM42N  Not the only one comparing. Intel actually outperforms at lower wattage.,Intel,2026-01-27 17:00:16,1
AMD,nz610q1,">GPU's will take all the bandwidth you can feed them.Â   Didn't deny that. But 12 Xe cores is presumably considered the sweet spot, that's all I'm saying. And Strix Halo only has twice as much bandwidth to feed a GPU die 6 times the size of Panther. I'm sure it has more cache, but still. I think Intel would consider triple or quad channel memory not worth the costs. It would require new i/o, new pins, new motherboard, more RAM, and all, for what's essentially the lowest volume product.  Besides, Intel already has Nova Lake AX in the backlog, or whatever it's going to be called. Practically intel's strix halo. It'll have Xe3P cores, more powerful than Xe3, thus deemed more worthy of the halo treatment.",Intel,2026-01-12 14:31:23,2
AMD,nzbnge8,"This is a big of exaggeration, as you can see with Nvidia moving to gddr7. While bandwidth has increased substantially, performance is clearly limited by lack of compute power",Intel,2026-01-13 09:04:52,1
AMD,nza15gk,"No official confirmation yet, but JayKihn leaked the tile size for the 12Xe SKU last year. Another user somewhere else said the 4Xe GPU is 33mm2.Â Â  https://x.com/jaykihn0/status/1812898063502938260   And even without the PHYs and NPU, from what I see, Halo's GPU tile is still like almost 3 times as big. So yeah, it's on another class, that's my whole point.",Intel,2026-01-13 02:11:11,2
AMD,nz68s35,Yep,Intel,2026-01-12 15:11:22,2
AMD,nzao8ks,"I have a pre-order in for an MSI 14"" at B&H for $1300. 358H, 32GB LPDDR5X-9600, 2TB, 1200p OLED. I've seen some lower-end PTL laptops rumored around the $900-$1k starting range, but those are likely the 4Xe chips. Wildcat lake with its tiny 2Xe GPU is probably going directly into the budget sector.",Intel,2026-01-13 04:17:34,2
AMD,nzdgaj0,Good catch.,Intel,2026-01-13 16:10:17,1
AMD,nzhkchb,AMD is dormant on the APU space since it had basically the monopoly for x86 because Intel was just bad.  They are taking one of the old Intel's book by releasing rebrands and reashes,Intel,2026-01-14 04:57:28,2
AMD,nz657hj,"> But 12 Xe cores is presumably considered the sweet spot  By what? much larger Xe3 GPU's exist.  We have nothing to compare against in Intel-iGPU-land that has 256bit memory.  Strix Halo die size isn't the metric you want either. It's only 2x the fps (and who knows, panther lake could be 2x its own fps with doubled memory bus, but we'll never know, because Intel won't release a strix halo competitor)",Intel,2026-01-12 14:53:19,0
AMD,nze3jni,"Halo's die is still quite a bit bigger, but from the Intel side, you need to include IO, GPU and about half of the compute die which has the MCs, encoders / decoders, etc. to match the ""GPU"" die of Halo, so it is more like 200mmÂ² to 300mmÂ² when compared",Intel,2026-01-13 18:08:18,1
AMD,nzkoj06,AMD is paying more attention to NVIDIA for sure. ESP on the data center side.,Intel,2026-01-14 17:33:11,1
AMD,nz696vi,">much larger Xe3 GPU's exist.  The biggest one for the moment is on Panther Lake X CPUs. I wouldn't know if there's something bigger tbh.  >Strix Halo die size isn't the metric you want either.\\  Sure you can't compare two different architectures. But all I need to know is it's faaar bigger.  >panther lake could be 2x its own fps with doubled memory bus  That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.",Intel,2026-01-12 15:13:24,1
AMD,nzgcub2,"Well Panther uses mixed processes, and hybrid tiles are bound to be a bit less space efficient than putting everything on a single die. And to be fair, Halo GPU uses N4P process while Panther GPU uses N3E So, still not directly comparable.   Gotta say though, Arc's PPA has improved a lot since Alchemist and Battlemage.",Intel,2026-01-14 00:38:45,2
AMD,nz753k2,"> That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.  Yeah, it doesn't matter how much memory bandwidth you have if the GPU doesn't have the raster performance to keep up with the flow of data. Case and point, AMD's R9 Fury X. Released with 4096bit bus HBM. Had a total memory bandwidth of 512GB/s. Yet the GTX 980 Ti released with a 384bit bus and 336GB/s memory bandwidth and it out performed the Fury X in pretty much everything.   That said, I have no idea how close the iGPU is to being bandwidth bottlenecked at 1080p. But I very much doubt doubling it would also double the frame rate.",Intel,2026-01-12 17:41:14,6
AMD,nziy0qm,"Yeah, I think Intel has done a great job with the improvements, I just don't want to overhype things.",Intel,2026-01-14 12:09:36,1
AMD,nylfre3,"While I agree their naming scheme is a mess, yours is far worse.",Intel,2026-01-09 13:49:14,18
AMD,nykrwmo,"TBH, as long as the Ultra 5 338H is actually called an Ultra X5, it'd make the entire thing a lot more consistent  as in now X always means ""the one with the good GPU""",Intel,2026-01-09 11:10:20,19
AMD,nykqc9r,"You have no understanding of Intel's business and thus are not qualified to advise them what to do   Intel doesn't sell these CPUs to the end consumer, they sell them to their customers - the PC manufacturers. And that is the reason why there is so much choice, because the PC manufacturers want it.   Also, you clearly have never heard of vPro.",Intel,2026-01-09 10:57:07,39
AMD,nylcqp0,"The SKU count is roughly doubled because you have each step with/without vPro - these get a 100MHz max turbo frequency bump, but the main benefit is you can run the corporate firmware with vPro support, so you get additional security and manageability features. Exception is the Ultra 9 where they just do it with vPro support as standard.  These have a higher cost because you are getting more features.  You could make it so you just have one CPU and then the manufacturer pays a license for corporate firmware per device, but that's more work to then ensure manufacturers are licensing machines correctly, and more confusing for end-users where now if you're buying a corporate device with vPro support you know you are looking at Core Ultra 236V and 268V for vPro support whilst 226V and 258V don't have it.",Intel,2026-01-09 13:32:44,6
AMD,nykrcxa,"Because intelâ€™s customers work with thin margins and want the wide product stack with lots of performance and price steps. For them it matters if they get 4.4ghz for $300 or 4.6ghz for $350. You are not Intelâ€™s customer unless you ordered a pallet of 1000 CPUs, which I doubt.",Intel,2026-01-09 11:05:44,10
AMD,nyo7z62,Apple really isn't better. They leave out lots of the important performance information. They just don't tell you at all.,Intel,2026-01-09 21:31:03,3
AMD,nykykh6,I agree for those cpu that have no alphabet denomination at the back as that just looks like how desktop cpu is.  But for X7 and X9 is just even easier CPU differentiation,Intel,2026-01-09 12:02:34,2
AMD,nywwcvg,I only agree with the title. Your naming scheme is much worse lol,Intel,2026-01-11 03:59:55,2
AMD,nylenad,"At this point even S3, S3 Pro and S3 Pro Max would be a great improvement.",Intel,2026-01-09 13:43:16,1
AMD,nylwbke,"Totally donâ€™t get it, miss the 13900k 14600k type names.",Intel,2026-01-09 15:11:59,1
AMD,nyneqru,"Brah I don't care about the naming conventions, it is what it is.  It is petty to argue about all of this.  I need the B770 and C880 to be released.  I need more Intel Arc Pro cards to be released, there is no hope for humanity otherwise.  I need Battlemage and Battlematrix everywhere but TSMC is the bottleneck.  Hopefully there is more for 2026 where Intel IFS shines.  God help us all!",Intel,2026-01-09 19:16:03,1
AMD,nyo686v,Are you saying they are all locked???,Intel,2026-01-09 21:22:59,1
AMD,nyqfh80,I only care about the top level sku so the names don't matter.,Intel,2026-01-10 04:43:17,1
AMD,nykt7v1,Samaung Galaxy S3,Intel,2026-01-09 11:21:10,1
AMD,nyky6jo,Intel to $100 guaranteed,Intel,2026-01-09 11:59:46,1
AMD,nymfe1x,Just Josh presses them hard on this issue at about 7:30 in this videoÂ https://youtu.be/AzGFbkKZE7A?si=yq1pmpRv7exQ-7i5,Intel,2026-01-09 16:38:15,0
AMD,nyo3uhv,"Check the Just Josh interview! Dude criticizes exactly that to an intel executive... For me, they should drop the ultra naming scheme altogether... it hasn't stuck yet... they should go back to de i3/i5/i7/i9...use the X for the B390... and an S for the 16 core variants...",Intel,2026-01-09 21:11:54,0
AMD,nyps0n6,Apple always nails the small stuff,Intel,2026-01-10 02:21:54,0
AMD,nyn9p1s,"Just because it sounds similar to Apple does not make it ""bad"".  I chose ""S3"" because they literally market them as ""Series 3"" (sounds awfully similar to M-series ðŸ¤¨). Could fiddle with it but my idea stands:  Different core count = different name, Better gpu = add X",Intel,2026-01-09 18:53:22,-6
AMD,nyoegef,"Nope, its the Ultra 5 338H: [https://www.intel.com/content/www/us/en/products/sku/245531/intel-core-ultra-5-processor-338h-18m-cache-up-to-4-70-ghz/specifications.html](https://www.intel.com/content/www/us/en/products/sku/245531/intel-core-ultra-5-processor-338h-18m-cache-up-to-4-70-ghz/specifications.html)  The Ultra X7 and X9 are listed as such on Ark: [https://www.intel.com/content/www/us/en/ark/products/series/245528/intel-core-ultra-series-3-processors.html](https://www.intel.com/content/www/us/en/ark/products/series/245528/intel-core-ultra-series-3-processors.html)",Intel,2026-01-09 22:00:46,3
AMD,nz4b2zo,"the 338H doesn't have ""the"" good GPU, it has a B370, with 10 cores",Intel,2026-01-12 06:22:28,2
AMD,nylk5nn,"Btw, OEMs love the fact that the Ultra 5 336H and 338H are vastly different products with hugely different performance when it comes to graphics. Why? Cause they can market the 338H to you, and then sell you the 336H at a fraction of the cost, and if you are not very tech-savvy, well, that's too bad for you.",Intel,2026-01-09 14:12:07,16
AMD,nynb0an,"Yes because PC manufacturers want a ""choice"" to get a CPU with 100MHz higher clock speed as if that will make a difference in a mobile device at all.  If you think ""vPro"" is so important, processors with it should have entirely unique names. You and many others in the comments made an effort to point this out more than intel's own naming scheme does.",Intel,2026-01-09 18:59:08,-6
AMD,nyn93or,"I see, that does complicate things.",Intel,2026-01-09 18:50:47,-2
AMD,o1nu420,Why are you getting downvoted ðŸ˜­  For consumer products it makes complete sense to go for more simplistic naming schemes,Intel,2026-01-25 18:16:47,1
AMD,nz8mprz,"It still should have the X IMO. They're still bothering to call it a B3xx chip rather than just ""Intel Graphics"" like the <=4 Xe chips. The handheld chips running downclocked GPUs get the B360 and B380 names as well. Those should all be Core Ultra X.  As of right now only the ending 8 differentiates the 338H from the small-GPU SKUs, which IMO is not clear enough. Also, if the X became the standard for all big-GPU chips, that ending digit can be used for something else, such as noting the actual GPU performance within the stack. Perhaps just using the 6-9 from B360-390 or something like that.",Intel,2026-01-12 21:47:30,2
AMD,nynhyn2,"You are placing way more importance into the naming than any normal customer would.  The names that matter to normal customers are Core Ultra 5, Core Ultra 7, Core Ultra X7, which is what you will also find on the stickers that Intel has the PC manufacturers put on the device. The model numbers are just for the PC manufacturers and customers who want the exact SKU.  Seriously, your obsession with this is weird. Just accept that the naming is not meant for you and move on. Not everything has to be like Apple.  > If you think ""vPro"" is so important, processors with it should have entirely unique names. You and many others in the comments made an effort to point this out more than intel's own naming scheme does.  Yes, because vPro doesn't matter for normal consumers, but only for big enterprise customers.",Intel,2026-01-09 19:30:44,5
AMD,nymf8n3,Itâ€™s too hard to be a smart consumer and Google the names of the processor(s) and compare?,Intel,2026-01-09 16:37:35,4
AMD,nyo54xn,"""Obsession"" as if multiple YouTubers, some with millions of subscribers, haven't said the exact same thing I did",Intel,2026-01-09 21:17:55,-2
AMD,nynq0up,If you're a smart consumer you aren't buying windows laptops.,Intel,2026-01-09 20:07:44,-3
AMD,nyou0lx,"Right, those guys are surely the ultimate authority on anything and not just engagement driven outrage machines /s",Intel,2026-01-09 23:18:05,5
AMD,nyviuhi,That's some serious credentials you're bringing up,Intel,2026-01-10 23:32:07,5
AMD,nypm046,Ah I should instead buy Apple laptops and/or Arm laptops that don't work with my programs. Genius!,Intel,2026-01-10 01:49:37,7
AMD,nykpkf2,Probs the same reason tire companies donâ€™t make their own cars. Itâ€™s a lot of work. Plus thereâ€™s plenty of competition  What youâ€™re missing is companies like intel have initiatives such as the ultrabook initiative to help manufacturers make better laptops.    Besides itâ€™s way more profitable for them to just make the cpu and not deal directly with consumers as much as possible.,Intel,2026-01-09 10:50:30,38
AMD,nykpm6e,">Another common complaint I hear with non Apple laptops is battery life on suspend  This is still a problem, and it's because of windows. Microsoft claim to have fixed it ONLY for snapdragon laptops, but I have heard some still having the issue. Ever since apple made the M chips the blame has shifted to X86 processors being the problem when it was and still is windows the whole time. This whole thing makes x86 seem worse than it actually is.",Intel,2026-01-09 10:50:55,22
AMD,nykulsp,"They did. Intel produced NUC desktop and laptops for quite a while, then they sold the business to Asus. The desktops are absolutely excellent.",Intel,2026-01-09 11:32:25,24
AMD,nymeqg2,Because then theyre essentially *competing* against their own customers. In a market that would require a lot of effort for small margins.   Their effort is better spent horizontally expanding rather than vertically at this point.,Intel,2026-01-09 16:35:22,5
AMD,nym7ev6,"As someone who owns the Intel nuc 12 enthusiast, the engineering on them is phenomenal, Intel would do an extremely good job resulting in low margins and financially it doesnâ€™t make sense to make laptops stick and perfect the chips is probably the best financial decision seeing as neither Qualcomm, amd, nvidia make laptops themselves. Even reference designed are outsourced. I worked on intels Arc program those Intel branded cards are expensive AF as they donâ€™t have the bargaining power of getting low cost components like Asus or Lenovo would.",Intel,2026-01-09 16:02:39,3
AMD,nykpdew,"They used to, NUC was acquired by Asus.",Intel,2026-01-09 10:48:51,6
AMD,nykquc9,Making and selling CPUs is more profitable. Apple has a different business model.,Intel,2026-01-09 11:01:21,2
AMD,nymjurr,I really like Intel NUC. Too bad they sold it to Asus,Intel,2026-01-09 16:58:00,2
AMD,nykrk5a,"Why don't tire companies make their own cars? Why don't window companies build their own houses or office buildings?  Why don't the semiconductor tooling companies like ASML, LAM, Applied Materials just build their own fabs and make their own chips? Hell why can't they just build their own laptops too??   Do you really think that Apple manufactures their own M series chips? Or really any of the components in their products? Because the truth might shock you.   I recommend you look up supply chains and maybe learn a lil something :)",Intel,2026-01-09 11:07:26,5
AMD,nyq0x6r,"You are not Intelâ€™s or AMDâ€™s customer  Dell, Asus, Lenovo, MSI, etc are all customers of Intel and AMD in that they are the ones who actually buy chips  If Intel or AMD go into the laptop business then they are competing with their customers which tends to make their customers upset  The same problem exists with Microsoft and their Surface line which is one of the reasons the Surface line often seems to struggle - Microsoft has to be careful walking the line between demonstrating what they would like the hardware companies to do vs actually competing with them  As for differences in performance that can often come down to what price point the manufacturer is aiming for with the laptop. Higher price can often mean better quality components (which in a laptop can mean lower power consumption)",Intel,2026-01-10 03:12:36,1
AMD,nyszed9,A mid ground solution would be to make a reference laptop and mandate manufacturers to meet or exceed the criteria like battery life and thermals.  Intel did have an initiative called Ultrabook serving a similar purpose before.,Intel,2026-01-10 16:05:13,1
AMD,nyvag38,"They did, but got out of it  https://www.intel.com/content/www/us/en/ark/products/series/196845/intel-nuc-laptop-kits.html  They used to make mobos too, but got out of that",Intel,2026-01-10 22:47:52,1
AMD,nzmt7q4,"low margin, and competing with customers is no good",Intel,2026-01-14 23:25:58,1
AMD,nyljl73,"Please, don't.  Once, a long time ago, when dial-up was a thing and used by business, I've purchased an Intel modem. 9600 bps connection speed. It was *the* *worst modem I've ever witnessed* \[despite the fact that it was expensive\], because the firmware was expected to be run in sterile laboratory conditions. The real world with unperfect landlines drove this child into confusion and madness, it was repeatedly entering renegotiation on every disturbance, often ending in just dropped connection.  I assume that Intel entrusted the design to talented engineers who had no idea about real-world operating conditions and did not bother to study them. Even in the US at that time, analog telephone lines did not always have negligible levels of interference.  \* 9600bps was the bleeding edge at the time; the old modem at the company where I worked had a speed of 2400 bps.",Intel,2026-01-09 14:09:12,-2
AMD,nyl7kr6,"\> Itâ€™s a lot of work.  Smaller companies like Framework are able to come up with new designs from scratch, though. Slimbook too I think?  \> Besides itâ€™s way more profitable for them to just make the cpu  That pretty much sums it up & answers my question.",Intel,2026-01-09 13:02:37,-20
AMD,nyrb7mi,It's both.,Intel,2026-01-10 09:04:04,0
AMD,nzbd8ey,I have an Intel NUC 9 Extreme [LAPQC71A](https://www.intel.com/content/www/us/en/products/sku/196641/intel-nuc-9-extreme-laptop-kit-lapqc71a/specifications.html). Itâ€™s built like a tank (magnesium alloy chassis) and Iâ€™ve always been very happy with it. Itâ€™s still my main laptop.,Intel,2026-01-13 07:27:53,2
AMD,nys69j0,It's not cause SDXE also suffers from this issue Intel CPU Didn't have the issue with MacOS,Intel,2026-01-10 13:25:30,1
AMD,nyt7d08,"There are issues, but SDXE still did better on battery life than comparable Intel chips, and that's with a number of its own issues. Only with LNL/PTL has Intel meaningfully started to close that gap, the first such push since HSW-ULT.",Intel,2026-01-10 16:42:50,2
AMD,nyt7wqy,Some software on windows will break it you are one update away from Things breaking in windows be it WoA or X86_64,Intel,2026-01-10 16:45:26,3
AMD,nxtihn4,The biggest issue was that it was crippled by the ported meteor lake memory controller dies its that simple,Intel,2026-01-05 14:12:18,27
AMD,nxtjtfv,"Very good explanation, I own a 285k and I can say the stock experience is average, but the platform is great and coming from 14900k, the temps and power efficiency are impressive. Once fully tuned, 9000c38 A-die, 36 d2d and 34 ngu, gaming is on par with 14900k, but more efficient. I think nova lake will be amazing.",Intel,2026-01-05 14:19:43,33
AMD,nxvnqr5,">if you judge Arrow Lake solely by the frame rate counter in Cyberpunk 2077 at 1080p  Am I allowed to take into account that Intel went all the way from ""7"" to ""3"" lithography which is more than 2x improvement to achieve almost nothing?",Intel,2026-01-05 20:16:32,10
AMD,nxti8go,Itâ€™s not necessary for consumers to buy an inferior product from a multibillion dollar company now backed by the global superpowerâ€™s government.,Intel,2026-01-05 14:10:52,40
AMD,nxugtl3,">We need to stop looking at the Core Ultra 9 285K through the lens of a typical generational refresh  That's all what consumers care about. They don't care if ARL on paper or on theory is some great reset. Perf, power, and cost is what's important.   >he 285K is suffering from the acute growing pains of decoupling the compute complex from the uncore in a way that creates a distinct latency penalty that enthusiasts are mistaking for regression.  It's not being ""mistaken"" for a regression, it quite literally is one.   The problem is also that AMD also has disaggregated their compute from their IMC, and yet has *better* latency on *less advanced* packaging.   >The controversy here isn't that Intel failed to push frequency; it is that they deliberately chose to execute a hard pivot away from the monolithic brute force strategy of Raptor Lake to a disaggregated chiplet design that prioritizes area efficiency and performance-per-watt over raw, latency-sensitive throughput.  Nothing about ARL's current design prioritizes ppw or area efficiency over RPL's design from a chiplets vs monolithic perspective. ARL isn't enabling higher core counts from going chiplets, that seems to be left to NVL according to rumors. And chiplets carries an area penalty over monolithic designs anyway.   >The removal of Hyper-Threading from the Lion Cove P-cores is the most contentious yet logically sound decision engineers could have made given the thermal constraints of modern silicon.Â   This makes no sense   >By removing the simultaneous multithreading logic, specifically the duplication of architectural state and the complexity required in the reorder buffers and schedulers to handle two threads, Intel was able to physically widen the core and increase the L2 cache per core to 3MB without blowing up the die size  SMT costs Zen 5 less than 5% in area btw. Just throwing that out there.   >The result is a P-core with significantly higher IPC than Raptor Cove  It's not though. This has been a significantly worse ""tock"" in terms of IPC uplift compared to something like SNC or GLC.   >but this raw single-threaded throughput is being masked by the interconnect latency.  Maybe gaming or some benchmarks are, but for the large part, no.   You can see this two ways, LNC's structural gains (core width, ROB capacity, etc etc) have smaller gains, percentage wise, over their predecessor versus something you would see in GLC vs SNC, or SNC vs SKL.  And also LNL's uncore is dramatically improved over ARL, and yet you see the same unimpressive IPC gains over MTL/RWC (which is the basis for ARL's mid mem fabric).   >the architectural overhead of the Foveros packaging means that ring bus latency is higher.  No? The ring runs at a different frequency than the D2D?",Intel,2026-01-05 17:00:52,13
AMD,nxtflnr,"https://chipsandcheese.com/p/skymont-in-gaming-workloads  None of the youtubers mentioned about core-to-core latency, improvements on the schedulers and execution ports setup.  The e-cores are really great in a 4 group cluster.",Intel,2026-01-05 13:55:51,18
AMD,nxv9k2w,Designing a consumer product line around the niche of top of the line workstation is not a good decision for the average consumer.  I only once met a person with video production workstation that has more than an I7 or R7.  Previous gen I5 were amazing combo of multi core and single core . They rivaled amd r7 in preformance . Now a person wanting mid level cpu's would pay preformance tax due to it being made for the few people needing extreme amount of cores since those people would not buy dies that actually were designed for it like Xeon.,Intel,2026-01-05 19:10:59,4
AMD,nxtg5aw,"this is a lot of words, being honest this writing feels like ai (but in good sense, right to point without a bunch of bs)   i would agree this architecture is very much limited by d2d and without 200s or just pushing d2d can be kinda underwhelming in performance but for sure as first gen product is very solid and makes me personally excited for nova lake as it seems they plan to fix and improve on their current architecture  also i believe clockspeed difference was merely responsible to 13/14th gen failures which were caused by excessive idle voltage.  i would say adding DLVR was kinda smart as well as it reduces your power consumption significantly at idle especially with proper tuning.  Edit: fixed typos, autocorrect being silly with me",Intel,2026-01-05 13:58:56,9
AMD,nxtm1ov,as a person who made an upgrade from lga1700 13950hx ES laptop mutant to 265kf this cpu feels soooo smooth in win 11 despite on a huge 75 NS the ram latency and not ability to reduce latency this new E core with 800 score at 5 ghz in cpuz single core make using the pc so nice and ofc in single core game like cs2 I've loose a lot of performance with 13950hx at 5.6ghz I've had 980fps in dust 2 fps benchmark and on 265kf only 800... BUT in a multi core load game I got fps improvement but any way this upgrade is worth it for ppl who is not a pc enthusiast and don't want to tune the lga1700 CPUs,Intel,2026-01-05 14:32:00,3
AMD,ny3llfj,That's one big wall of excuses,Intel,2026-01-06 23:17:53,3
AMD,nxto7vg,"It smells like slop in here. Shit post rather than a shitpost, congratulations.  I like my 265K. It performs well for my purposes and has reduced my personal power consumption considerably vs AM4. It is behind AM5 in my testing for broad term ""gaming"" when specifically chasing framerates, but compared to a 9700X or 9800X3D it is absolutely stellar at doing stuff while doing other stuff.",Intel,2026-01-05 14:43:46,5
AMD,nxtkrwq,Yea with a z bord and the 200s boost and fast ram 15th gen is finally matching or supasing 14th gen,Intel,2026-01-05 14:25:03,4
AMD,nxwaygs,Wall of slop.  Make your points in a more concise manner.,Intel,2026-01-05 22:04:26,5
AMD,nxtzro3,"AMD had a very similar experience with their first generation of Ryzen CPUs. One of the differences here is Intel had a competitive product prior to their architectural shakeup. Had Intel totally croaked for years and not been competitive in the CPU space the narrative would be completely different and everyone would be singing their praises.  Aside from that I think the biggest problem with this new architecture is simply there's little reason to buy in. It was only recently (if memory serves, I could be wrong on this) announced that the next generation of CPUs will share this platform and later ones will be on a new socket. When AM4 was announced we knew that it would persist for multiple generations and now with AM5 - why would you buy a board that will be obsolete when its time to upgrade when you could buy into a platform that will support your next 1-2 CPUs? Especially with the old intel socket performing just as well on a more mature platform, for most end users this first core ultra series just isn't worth investing.",Intel,2026-01-05 15:41:28,2
AMD,nxua32l,"""they deliberately chose to execute a hard pivot away from the monolithic brute force strategy of Raptor Lake to a disaggregated chiplet design that prioritizes area efficiency and performance-per-watt over raw, latency-sensitive throughput.""  That good for them, but we don't want that. I would take better latency over improvements that pretty much only save money to the companies.",Intel,2026-01-05 16:29:42,3
AMD,nxu3n2t,Love my 285K rig.,Intel,2026-01-05 15:59:37,3
AMD,nxu27tg,"IMO corrupt tech sites and tech YouTubers are behind ARLâ€™s failure for two reasons.   The first: AMD MCM processors without 3D cache are bad for gaming, and itâ€™s hard to find this information in 90% of charts to warn intel about the consequences of going down this path, even though they should have done their own tests and experiments.   The second: in my own tests, the 265K was 15% faster than the 9700X on launch BIOS. Through BIOS updates, that gap extended to 20%, making it only 7%-10% slower than the 14700K. Yet on some very questionable charts, the 265K is shown as slower than the 9700X.   For experienced users, RPL CPUsâ€™ temperatures can be lowered by 20Â°C by turning Hyper-Threading off and undervolting. For inexperienced users, ARL is better, as it runs 20Â°C cooler out of the box.   I believe that in 2026 we deserve raw, unedited video benchmarks that start from the desktop, show full system specs, then enter game-by-game benchmarkingâ€”no more charts with zero evidence to back them up.   When I tried to confront HUB with my benchmarks, I got blocked to cover up their lies and corruption. If they truly cared about which CPU is faster, they would share their in-game benchmark scores and discuss them in a scientific way, but thatâ€™s not their goal. The numbers go up and down for the highest bidders.   Lastly, I believe Nova Lake, with its expected 144 MB cache, will be faster than Zen 6 X3D by 10â€“20% as 9700x in real world is 20% slower than 265k and if both got the same IPC uplift NL will end up on top.   My own testsÂ    14700k vs 9700x 30% faster https://youtu.be/1f6W6nkDS4o?si=chFUAeBWzybQopaL   265k vs 9700x 23% faster https://youtu.be/PuB0Dg-Jvyk?si=SmGJUFtYj-OjjpQh   Tech sites got same results that clearly show RPL and ARL CPUs are only slower than 9800x3d and faster than everything else from AMDÂ    https://www.pcgameshardware.de/Ryzen-9-9950X3D-CPU-281025/Tests/Benchmark-Release-Preis-vs-9800X3D-1467485/2/     https://www.purepc.pl/amd-ryzen-9-9950x3d-test-recenzja-opinia-cena-wydajnosc-gry-programy?page=0,55",Intel,2026-01-05 15:52:58,6
AMD,nxts3mv,"For me.   I will go with Intel because of reliability (I know about chip degradation) but for me I bought 13900K from first day I undervolted using offset and but Max turbo frequency to 5.5Ghz . so my chip never tried to boost 5.9Ghz with crazy voltage.  Next is the most important is Everything just works. The boot is faster. wakes from sleep. its a more mature. I have another amd pc with r5 2600, where I found some stability issue.  Another one that is important to me. is idle power consumption. My 13900K can idle at 6 watts. Imagine 24 cores idling at 6 watts. where as 6 core zen idle at 15-20watts.(and that is low side many user reported 25+watt). its all because of chiplet.  I didn't test the new Arrow Lake as this uses tile. so i cant comment on idle power draw. if anybody has test, let me know.",Intel,2026-01-05 15:03:58,5
AMD,nxwilo7,"Intel's latency problems have been around for a while now. Arrow Lake just threw gasoline on a fire that was already burning. [The 14900K had a latency of about 90ns for memory access, which is awful compared to the 10900K's 66ns and the 3950X's 73ns latency.](https://chipsandcheese.com/p/examining-intels-arrow-lake-at-the). The 285K sits at 106ns.  The 9900X sits at 82.43ns, so it seems like latency is going up across the board in general.",Intel,2026-01-05 22:41:53,2
AMD,nxmkdsp,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2026-01-04 14:07:07,1
AMD,nxxn31i,I compare Arrow Lake to Zen 1 or the first iterations of Ryzen. It is quite the techinal hurdle but it allows for future generations to bake very well. Intel was just playing catchup from having 14NM^6 nodes lmao.,Intel,2026-01-06 02:15:07,1
AMD,nxyburv,the problem for me the platform cost value over lga1700 isn't where it needs to be. namely I'm not interested in trading 32GB of DDR4 for just 8GB of DDR5.,Intel,2026-01-06 04:39:27,1
AMD,ny7t3re,"Since you are talking about latencies, best intel gaming cpu, so far, is 14700k. Excellent gaming performance and also great cpu for productivity.  Second best, is 14900k, same as 14700k, but cause of price it gets to second place.  Third place, you name it.  Period.  PS: I'm with intel since ..... Pentium 3 at 800MHz. Also have AMD cpu.",Intel,2026-01-07 15:44:56,1
AMD,nyabjt5,"End-user workloads at midsized businesses are almost entirely single-threaded. Even when the applications are multi-threaded, they often become single-threaded when they get shuffled through corporate antimalware. While AMD chases gamers and Apple chases artists, Intel is hitting a sweet spot here not perfectly served by any other single-user processor.",Intel,2026-01-07 22:22:37,1
AMD,nyduqy4,"I've been saying similar in a couple of PCMR threads.  Arrow lakes problem is chiplet to chiplet latency. Which is understandable as this was a major departure from monolithic dies this generation. This explains both why games particularly suffer, and why very high speed memory can mitigate the issue somewhat.  Does this mean they aren't bad chips? No, they are (or at least for gaming performance they are). But it's quite exciting in the sense that it's a specific issue holding them back that can be solved. The rest of the architecture has a lot of potential.  I think of these chips as in a similar place to AMD Zen 1 technology wise. A huge shift, not actually delivering much performance boost yet, but with a whole lot of potential for updates to make big improvements.",Intel,2026-01-08 12:08:37,1
AMD,nyf32oe,"Yes yes, I remember saying similar things about my FX-8350.",Intel,2026-01-08 16:03:36,1
AMD,nysqs8m,"285K seems to have fantastic workstation performance, I didn't think much before selecting Intel over AMD parts for work to be honest, considering the efficiency gains. We landed on 265K, it benches very favorably compared to AM5 parts in nearly every workload except for gaming, and even then, that's mostly the X3D parts, and even then, that's when there is virtually no GPU bottleneck.   Most initial ""gaming"" reviews were done exclusively with 5090s, which is extremely unrealistic for most gamers and gave an extremely false impression that you would see a massive uplift by buying this part compared with the alternative CPU parts in gaming.   A lot of reviewers such as HUB have done a lot of work to correct this impression with a variety of testing scenarios and explaining their reasoning behind removing the GPU limiting factor as much as possible in benchmarks. Incidentally, these reviews typically showed that anything less than a 5080 sees virtually no benefit with a 9800X3D versus, say, a 9700X, and incidentally the two parts that saw no discernible benefits were the Radeon 9070XT and the 9070.  Obviously, if you have a 5090 and an unlimited budget with price being no factor, your best bet for top tier performance in all categories is a 9950X3D, but that just isn't what most people are looking at.  I myself bought into the future proofing mentality with my CPU and bought a 9800X3D, and ironically bought another AMD Radeon 9070XT to pair it with, a GPU that seems literally no performance gain compared to a 9700X, let alone a 285K. The same 285K that takes a dump all over the 9800X3D in nearly every other productivity benchmark.  I guess the good news is that when I upgrade my GPU in 2-4 years, at least I'll take advantage of the 9800X3D in games, lol. But considering it cost me over $900 for CPU, RAM, and motherboard, that's cold comfort. Ironically I probably would have made out a lot better buying a 5700X3D or a 5800X3D and the 9070XT and keeping my AM4 board and DDR4 RAM and saving $600.",Intel,2026-01-10 15:22:09,1
AMD,o2v1ko1,Itâ€™s a stepping stone â€¦ the efficiency is super and performs the same as a 14900 drawing a lot less power. Impressive.,Intel,2026-01-31 21:41:48,1
AMD,nxtna7j,"You are correct that they needed to make a big architectural change as the 14th gen was clearly having issues and was being pushed too hard to make up for them, and ARL is the first step to that reset.  From their engineering perspective, it makes sense.  I wouldn't buy it though.  NVL... different story.",Intel,2026-01-05 14:38:45,1
AMD,nxvbwve,"No 285k is not shaving off 80-100W from 14900k, Set 100W PL1=PL2 on both and then compare. 285k is not even that much better than 9950x in MT. No amount of words can explain ARL flop, it took intel 3 gens after rocket lake fiasco to catch up, just to waste all that effort on ARL. Show me any other silicon design with advantage of 2 nodes and a new architecture just to be slower than the predcessor.",Intel,2026-01-05 19:21:42,1
AMD,nxwb6x7,"""If you are buying a 285K solely for (1080P) gaming, you are buying the wrong product for the wrong reason.Â ""  Fixed it for you. I game in 4K and I didn't buy a Ultra 9 285K to play games in 1080P to get high(er) framerate at the expense of stuttering. The ultras are outstanding CPUs.",Intel,2026-01-05 22:05:33,1
AMD,nxtfm4y,I guess.,Intel,2026-01-05 13:55:55,-1
AMD,nxte2yd,No one has time to read all that,Intel,2026-01-05 13:47:04,-11
AMD,nxvicu7,This is a very long way of saying Intel has chose to prioritize competing with Appleâ€™s SoC and ignoring gamerâ€™s and PC enthusiastâ€™s wants/needs.,Intel,2026-01-05 19:51:27,0
AMD,nxuwzmt,"Iâ€™m just waiting for Nova Lake and if it gets delayed into 2027 and if TSMC does the packaging for some chips there may be issues  with supply. TSMC does not like that Intel has IFS and they play dirty, real dirty.  Nova Lake flagship could be at or above $1000, I may go with Ultra 9 285K with the fastest pcie5 nvme and ram, by then hopefully ddr5 is accessible.",Intel,2026-01-05 18:14:53,0
AMD,nxtrgru,My same experience with both of my 265k systems. They have been extremely stable for me and very efficient. Never have to worry about temps and they perform well with a 5080 and 9070XT. Contrary to all the media rhetoric I enjoy gaming with them.,Intel,2026-01-05 15:00:40,17
AMD,nxu6pg9,Basically the same or even less FPS than a 9800X3D consuming 80 watts,Intel,2026-01-05 16:13:55,3
AMD,nxvj13w,"Hi I'm sorry to ask but what does this mean?     9000c38 A-die, 36 d2d and 34 ngu     is that an app or something?",Intel,2026-01-05 19:54:33,2
AMD,nxuzcbk,"Exactly, tuned 285k is just 2-3% away from max tuned 14900KS + DDR4 at 4300MHz CL16.  All Z890 boards are so cheap so I grabbed an APEX with a 265K. Only costed me $600",Intel,2026-01-05 18:25:21,3
AMD,o367lks,"In gaming specifically, how smooth would you say the 285k is compared to the 149k? in the sense of frametimes and shader caching.",Intel,2026-02-02 15:47:34,1
AMD,nxvtdhm,"that is exactly the point, if it was just architecture we could live with it and consider it an scurve of innovation, but the process proves that this design is going nowhere",Intel,2026-01-05 20:42:56,7
AMD,nxwwv5w,"New process nodes donâ€™t improve  performance on desktop processors when clock speeds and core counts stay the same. A 10nm monolithic ARL could have performed better and cost them less, although the newer process does improve power consumption which is essential for laptops.",Intel,2026-01-05 23:55:52,1
AMD,o1y3fmp,Stopgap design and their first major foray into chiplet. I think they learned a lot of things on the way and Intel is famous for their processes getting better with age after a shrink. Compare Alder Lake to Raptor Lake. Compare Broadwell to Comet Lake.,Intel,2026-01-27 02:54:11,1
AMD,nxum4ri,"And nobody was saying you're required to. The entire point of the post was to say regardless of your thoughts and purchasing habits, ARL was deliberate, and even smart. Those who look for single metrics by which to judge ARL are missing the point.   Yes, if that single metric is all you care about, by all means go spend your money elsewhere, but ARL is a step sideways so future generations can take leaps forward.",Intel,2026-01-05 17:25:38,7
AMD,nxtkrhd,"This is way too true. I want a healthy Intel and AMD, but I'm also not going to act like Intel being now backed by the US Government and MAGA, as well as securing a well funded partnership with nVidia, doesn't make me feel a lot less like they need consumers to pity buy things from them to encourage innovation.",Intel,2026-01-05 14:24:59,7
AMD,nxuiree,">In highly parallelized rendering workloads like Blender or Cinebench, the 24-thread Arrow Lake design is often matching or beating the 32-thread Raptor Lake parts, which proves that the removal of Hyper-Threading was not a net loss for total throughput  So matching perf with a last gen part, after you hit a double node shrink **and** a massive E-core IPC gain and a P-core tock too is fine?   >The ""rent"" paid in silicon area for HT was no longer worth the ""yield"" in multithreaded performance,  This was a mistake according to LBT himself.   >This implies that Intelâ€™s next step must be an aggressive overhaul of the interconnect topology, perhaps moving towards a mesh or a more direct active interposer solution for desktop parts if they want to reclaim the gaming crown from AMDâ€™s X3D parts  Moving to a mesh wouldn't help much, and Intel's mesh's have a reputation for being insanely slow on their Xeon parts.   How much more advanced packaging does Intel have to use to match the latency of AMD using iFOP?   >Â But if you analyze the architecture, the Lion Cove P-core is a marvel of width and prediction capability that is simply being strangled by the packaging logistics  It's not. LNC is both not that all that wide, all the ARM cores beat it in that metric, and the prediction capabilities of LNC is bad- it's a literal regression vs RWC (last gen) in accuracy. It's worse than the E-cores branch prediction accuracy. And it's much worse than Zen 5's as well.   >and the floating-point performance is stellar.  This specifically is not the case. While in previous generations Intel was very competitive with AMD in spec 2017 FP, with ARL vs Zen 5 we see an almost 15% gap.   >The 285K is the cooler, more efficient, strictly professional grown-up in the room that unfortunately forgot how to play games because itâ€™s too busy trying to figure out how to talk to its own memory controller across a microscopic bridge.  Idk why you are trying to trivialize gaming when it pushes a huge percentage of the market, and is why Intel has been repeatedly telling investors they have lost the high end DT market.",Intel,2026-01-05 17:09:57,7
AMD,nxtic59,Yes because they do core to cores transfers via their shared l2 rather than the l3/ring. It's sick. Skymont in general is so underrated for how enormous of a performance gain it was. They literally fixed all the ecore issues it's the pcores that flopped,Intel,2026-01-05 14:11:26,16
AMD,nxu08ua,"As someone who â€œwrites like AIâ€ in part because of a learning disability that made it hard for me to write, I tend to organize my thoughts very deliberately. Using lots of punctuation, dashes, etc is now often interpreted as having used AI, although I have no idea whether OP did or didnâ€™t.",Intel,2026-01-05 15:43:44,10
AMD,nxviton,">this is a lot of words, being honest this writing feels like ai (but in good sense, right to point without a bunch of bs)  It doesn't point out a bunch of BS, it introduces a bunch of new BS that is just straight up, factually wrong.   >i would agree this architecture is very much limited by d2d  Not single core performance like he is implying it is.   Just look at ARL-H vs MTL-H for example.",Intel,2026-01-05 19:53:36,3
AMD,nxv2i2x,"Soon you will not be able to tell what is AI and what was before, where you were living in the Stone Age. Fooz better buckle their seat belts and get rekt, itâ€™s about to get a Bit-Funky.",Intel,2026-01-05 18:39:29,2
AMD,nyi3r20,"\> For experienced users, RPL CPUsâ€™ temperatures can be lowered by 20Â°C by turning Hyper-Threading off     \> 265k vs 9700x 23% faster   \> 5 games    You are cute.",Intel,2026-01-09 00:12:42,1
AMD,o0ie5bv,at least you have proved that i did make the right choice on my 265k,Intel,2026-01-19 17:29:31,1
AMD,nxzan7c,AMD Unboxed are not serious and should be ignored at every opportunity. They have a long history of doing what you've said they've done to you. They get in childish arguments on twitter when they aren't blocking people who have data that disagrees with their clear bias.,Intel,2026-01-06 09:33:39,1
AMD,nxu6lhk,I have one 13900k since day one... But like all enthusiastic guys I did some benchmarks and the voltages were a concern. So I tuned the bios after just a few hours of working.  3 years have passed and I have 0 problems. It's like a rocket ðŸš€ very fast and reliable,Intel,2026-01-05 16:13:24,6
AMD,nxzllb5,"Very nice, do you also use a contact frame?",Intel,2026-01-06 11:12:32,1
AMD,nyi50z0,> so my chip never tried to boost 5.9Ghz with crazy voltage.  You have no idea what cause degradation and undervolting does not save CPU.,Intel,2026-01-09 00:19:10,-1
AMD,nxwmvqc,For sure.  I still use 10900k which benefits massively from memory frequency. When running 4500 cl16 it sits in 33-36ns territory. Lowest latency cpu around was 2020.  Doesnt hold up vs today's cpus but the user experience is great.,Intel,2026-01-05 23:03:28,1
AMD,nxxq0wj,"Really, 90ns on DDR5-3600, what else can go wrong here...",Intel,2026-01-06 02:31:08,1
AMD,nxx7vlh,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:53:14,1
AMD,nyf7a0y,"""Wait till games leverage multicore"".",Intel,2026-01-08 16:22:24,1
AMD,nxz46az,"Could also add 1440p into the mix, at max settings with ray tracing, the CPU is going to matter less and less.",Intel,2026-01-06 08:31:05,1
AMD,nydtdxm,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-08 11:58:45,1
AMD,nxuz1x1,"Are you using a B580, Iâ€™m wondering how it performs.  I have a 5070 OC and an army of Alchemist and Battlemage cards.  I may try to get a 9070 this year and OC / solder it to XT performance. There is a great deal of fake hype around the 9070 XT, Lisa and her army of gawkers really pulled out the hype train for the 9070 XT.  Many influencers (somebody who can post a video to snoozetube) received 9070 XTs for free so they could gimp primp them out to the masses. It has left a stale, rotting smell in my loins.",Intel,2026-01-05 18:24:04,2
AMD,nxwegrj,"Did you used just ""stock"" profile ie NGU and D2D at auto and Intels default profile? Or Intel 200s boost?",Intel,2026-01-05 22:21:22,1
AMD,nxuzm9f,"Both 14900K and 285K won't get frametime spike when the L3 Cache is maxed and no random stutter/issue.  If I want a good gaming experience, I would go for the 14600k instead of the 9800X3D. Way cheaper and works way better.",Intel,2026-01-05 18:26:34,1
AMD,nxvwkhz,"These are the bios overclocking settings, ram speed is 9000mhz, d2d die2die is 3600mhz, ngu chip fabric 3400mhz fully manually tuned",Intel,2026-01-05 20:57:48,7
AMD,o36dni3,"Honestly I don't feel any difference, both are max tuned and there's no stutter or dips, 285k is the one I prefer, lower power and temps for about the same performance. And probably 285k is the winner for stability also.",Intel,2026-02-02 16:15:47,1
AMD,ny1yrwf,"> New process nodes donâ€™t improve desktop performance when clock speeds and core counts stay the same.  This is correct. The node shrink itself does not give IPC gains, what it gives is energy efficiency and area. The reason why people associate node shrinks with better performance is because when they switch a processor to a newer node they typically increase cache sizes and/or improve base/turbo clocks.",Intel,2026-01-06 18:43:59,2
AMD,ny0mjxi,> New process nodes donâ€™t improve desktop performance when clock speeds and core counts stay the same.  Netburst was awesome!,Intel,2026-01-06 15:03:34,1
AMD,nxvi8ry,The problem is that MTL for all means and purposes should have been that test bed product. Or even lakefield tbh.,Intel,2026-01-05 19:50:56,4
AMD,nxxqpo4,"Nothing smart about it.. they just couldn't do any better than release a half unfinished product because the company is corrupt as hell, fully relying on US gvt injecting  tons of money that ends up in a few top manager pockets instead of being used for restarting the core architecture from scratch.",Intel,2026-01-06 02:34:57,1
AMD,nxubozd,"We were backed strongly by Biden admin too, the CHIPS act money was what Trump gave us, but demanded the stock in return instead.   Which I think is ultimately good for us American citizens. Too many times companies just got hand outs, even Bernie Sanders approved the Intel stock deal. https://www.reuters.com/world/us/us-senator-sanders-favors-trump-plan-take-stake-intel-other-chipmakers-2025-08-20/",Intel,2026-01-05 16:37:08,15
AMD,nxu3xx9,Brotha everyone is tryina get a piece of that maga pie. Or vice versa. It would probably be unlawful to go against maga on fiduciary responsibility alone,Intel,2026-01-05 16:01:00,3
AMD,nxtmb1f,This!,Intel,2026-01-05 14:33:25,5
AMD,nxv74ph,"On my 13600k I also see a shared L2 for each 4-core E-core cluster. Was there a regression between then and now that they've resolved? Or is there some hidden behavior where this shared L2 couldn't actually be used to core-to-core communication without going through the ring?  If you have a source with more info, I'd greatly appreciate it.",Intel,2026-01-05 19:00:02,2
AMD,nxvzfjz,"AI is the aggregation of all the rules and examples fed to it.  You write according to proper ""rules"" or clear organization (which is very much not ""vernacular"" level writing), then boom, you and AI aren't all that different.  It's bloody annoying to try organizing thoughts or points to be easily digested instead of a wall-of-text like you've been doing since Mavis Beacon taught you typing only to have people bitch about the style and ignore the content.",Intel,2026-01-05 21:11:12,3
AMD,nxv1yn0,There are so many of us lol,Intel,2026-01-05 18:37:05,0
AMD,nylpv7p,5 CPU bound games enough.,Intel,2026-01-09 14:41:06,1
AMD,nxzllwo,Do you use contact frame for the cpu?,Intel,2026-01-06 11:12:41,1
AMD,nxzluhl,"yes, i do",Intel,2026-01-06 11:14:44,1
AMD,nyi5ufb,"Please, High life form, what causes degradation? enlighten us, mere mortal",Intel,2026-01-09 00:23:20,2
AMD,nxv14y0,I do have a B580 but haven't paired it with the 265k cpus yet. The B580 is currently in a i5-13600 system.,Intel,2026-01-05 18:33:23,2
AMD,nya2yya,I have a 265k and b580. Had it about 6 months. First desktop I've had in about 20 years. Works flawless for everything I need. Very stable. Very fast for my needs.. I pretty much only play warcraft though.,Intel,2026-01-07 21:44:30,1
AMD,nxwoxfl,"Stock. One system uses a Z890 mb with 8200mhz cudimm and the other uses a B860 MB with 8000mhz cudimm. I tried 200S boost on the Z890 and it didn't benchmark much faster at all for the games I play, plus I had occasional lockups. It wasn't worth leaving it on for me so everything is at the Intel default profile.",Intel,2026-01-05 23:13:58,1
AMD,nxzc32p,Lmao,Intel,2026-01-06 09:47:24,7
AMD,nxv0qyc,"Lol, Userbenchmark guy making things up.",Intel,2026-01-05 18:31:40,15
AMD,nxw9ntc,thank you,Intel,2026-01-05 21:58:18,2
AMD,o36h3on,any tips/resources to tune/troubleshoot? ive been chasing stutters from 2 different 14th gen systems and no luck lol,Intel,2026-02-02 16:31:42,1
AMD,ny41irt,"Clock speeds havenâ€™t meaningfully improved since 32 nm Sandy Bridge. The 2500K and 2600K could overclock to around 5.0 GHz, and modern CPUs still run at roughly the same frequencies. Core counts are also similarâ€”Intelâ€™s 14 nm 7980XE had 18 Skylake cores. Cache increases are possible on older nodes as well, so newer process nodes mainly improve efficiency these days, which is contrary to what most expect of them.  A 10nm monolithic ARL could have performed better at least in gaming on desktop.",Intel,2026-01-07 00:40:20,1
AMD,nydvpul,Apart from straight up increasing frequency improved nodes also make improvements to CPU possible even at the same clock.,Intel,2026-01-08 12:15:24,1
AMD,nxvyiba,"I suspect they just didn't have enough time to change anything. MTL releasing in Dec 2023, internal testing I'm sure yielding some set of data, and external reviews giving other feedback, even by the release of MTL, ARL has probably been in the pipeline for years and probably locked into certain design choices regardless of the feedback and testing.  Additionally I suspect that on mobile chips/laptop gaming rigs there's less focus on each individual part because  a) few people hardcore game on laptops b) the latency can be blamed on other things since laptops are a prepackaged consortium of parts and it's harder to isolate, and  c) therefore laptops tend to be evaluated as a whole rather than the individual pieces that comprise them.   Therefore the ""latency issues"" only became a massive kerfuffle when the offending cpu could be isolated and tested alone, and reviewers needed something to complain about.   That's even if Intel was considering latency as the issue everyone made it out to be. Intel could have looked at it and considered the latency worth the cost to improve in other areas and serve as intels seminal desktop entry utilizing disaggregated silicon. Then gamers came and lost their shit that the newest Intel chip deprioritized something that impacted their precious fps- even though the impact was ""the new one is approximately the same to maybe a little worse as last gen in most games"".   Notwithstanding the fact that the 200 series chips retain healthy gen-to-gen perf uplift and a _massive efficiency improvement_ in productivity and general computing, and boosting the NGU and D2D clocks (which are _very_ low from the factory, and can be done with the app that Intel _has specifically designed for tuning their chips_ and is freely available (XTU)) brings the gaming performance to ""approximately the same to a little better in most games"". Contrary to what some people may think these chips are not solely or even majority used for gaming and there are other use cases Intel can to think about/chose to prioritize.   To be fair, gamers and tech enthusiasts are the ones who will care the most and therefore have a disproportionately large impact on the kind of publicity the chip will get. So, was this intels smartest PR decision? Maybe not. But I think it was still a sound engineering decision, regardless of whether or not they had feedback or data on the issue, even if it didn't go over very well with their loudest customer base.  And this is all overshadowed by the fact that if you stuck even the most hardcore of gamers on a blind trial and told them to identify what kind of chip they were gaming on, I have a hard time believing any of them would be able to tell with any certainty which is which.   Once you get to 60 fps, the vast majority of people stop caring. Whats five fps going to do to radically change your gaming experience so much that it's unplayable? Which brings me back to one of my original points: If you care _that much_, you can go spend your money _elsewhere_.",Intel,2026-01-05 21:06:54,3
AMD,nxun1ug,"I'm aware of CHIPS.  Like I said, I want Intel to be competitive. I hope if their new layouts mature into something that can retake marketshare that the same care will be taken to preserve AMD. AFAIK, no one came to lend them money or sign large multifaceted partnerships at that time. This ""must save Intel,"" movement is something that is borne out of how big Intel is. They're too big to fail, it would have too many implications for the economy. It's not tit for tat, but as they are both American companies and AMD has significantly more invested in my country, I don't feel particularly motivated to help bail out Intel when they have the equivalent of the iron rice bowl rolled out for them at the moment. I'll buy what works best for my needs at the time I'm buying, as always, and if Intel can make a better gaming focused, with some imagery stitching on the side, processor than AMD can at the time I'm buying, that's the direction I'll go.  Ultimately the 200 series is what the Ryzen 1000 series was, but more stable as Intel has always had better support both internally and from vendors. I have no doubt Intel may catch AMD. My worry will be, what happens when they put their boots on AMDs throat, especially now that nVidia has more control over the situation than ever before?",Intel,2026-01-05 17:29:53,1
AMD,nxxrp83,Pouring free gvt money into Intel is only making the company high managers less interested in restructuring and improving the company when instead they can just keep going at snails pace while cashing in. Gvt money is only supporting growth of corruption inside the company and stalling progress.,Intel,2026-01-06 02:40:23,1
AMD,nxv6i3j,"It's so strange to me that we've hit a spot in consumer and enterprise computing where politics is now a factor. It's a different game when companies have to worry about that side, far too many consumers make decisions based on their politics when all that does is cause other issues. Obviously I won't go farther than that in a tech focused discussion lol, but I will say again that it's a different game and I don't think anyone wins if it becomes the norm.",Intel,2026-01-05 18:57:13,1
AMD,nxty6pw,I upped the tREFI of my memory in my MSI Vector with Intel Ultra 9 275HX and got some fps gains in Fortnite/Valorant/Hogwarts Legacy at 1200p,Intel,2026-01-05 15:33:56,2
AMD,nxv7w1f,Yeah pre arrow lake it was particularly shit as it would instead go through BOTH l2 and then to l3 to communicate between cores like a shitty ISP route to a game server   That is obviously worse than normal but core to core communication being done through even shared l2 is very rare so even without that quirk it's not expected  Go to chips and cheese.com they have a ton of information about this stuff. Like their skymont article in this case it details all the huge upgrades,Intel,2026-01-05 19:03:27,3
AMD,nxx7gwg,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:51:03,1
AMD,nylunna,Every respectable benchmark is CPU-bound (including HUB whom you try to diss) because every sane reviewer uses a fat 5090 and 1080p to show CPU differences.  Even if you did not cherry-pick the games the review with fast GPU and more games is much more indicative of CPU performance than your benchmarks.,Intel,2026-01-09 15:04:11,1
AMD,ny47bmi,No I don't need it... You need a 360mm AIO that's all... After that it's all about bios settings.,Intel,2026-01-07 01:11:04,1
AMD,nxzm8r1,which brand do you use?,Intel,2026-01-06 11:18:02,1
AMD,nyi87lr,"1. tvb working incorrectly 2. unlimited current set by motherboards which gets only higher when undervolted because same power limit at lower voltage necessarily means more current   Motherboards were undervolting CPUs with wrong LLC calibration intentionally   4. ~~motherboards having wrong LLC calibration~~ EDIT: Intel CPUs themselves requesting abnormal voltage in anticipation of frequency boost resulting in abnormal idle voltage (just remembered it correctly) 5. motherboards having wrong LLC calibration resulting in abnormal load voltage  Not a single smart ass on the planet could have predicted that all of the above can happen with ""stock"" settings and foresee all of this.  No frequency limit will save CPU when it dies in idle state.",Intel,2026-01-09 00:35:27,-1
AMD,nxyej0v,"Thanks so in short:  You use only ""intel default profile"" and enabled XMP on your CUDIMM's right?   No further manual tweaks under NGU or D2D and RING values or any other critical tweaks pertaining to voltages no?",Intel,2026-01-06 04:57:20,2
AMD,o4mn0bt,"TBH, he is not wrong about L3 cache maxed out and the clock speed drop. It's many times happen in very busy scene. Most likely dipping for no reason. 9800X3D low 1% is worse than 14900k i dunno why",Intel,2026-02-10 15:28:22,1
AMD,o1y237m,"If you don't understand overclocking and don't see how a CPU with multiple millimeters physically between the IMC and cores can not possibly perform as well as the monolithic CPU I don't know what to tell you. Fully tuned 285K and 9800X3D benchmarks exist btw, check Blackbird PC Tech. 14900K is the best 1440p CPU and the 285K is the best 4K CPU.  Benchmark bar graphs of 10 games aren't the whole story anyway. There are far too many variables in various applications which will make the IMC island CPUs perform worse than the monolithic CPUs, sometimes substantially. It's enough for some of us to want to stay on monolithic just so that it always works. AMD knows this, next gen they won't put the IMC so far away and their die shots will look more like Arrow Lake.  For now, 50% more power consumption on 8 P core loads vs 9800X3D is a price I'm willing to pay. I don't want an 8+8 core CPU either so AMD doesn't really compete with the 24 core monolithic 14900K currently.",Intel,2026-01-27 02:46:57,1
AMD,o36t36x,"Sync all cores 55/57 depending on your cooler, undervolt also, ram oc is the most important for intel, frequency and tuned timings.",Intel,2026-02-02 17:26:56,1
AMD,ny5j3g0,"Bruh my 2600k couldn't hit 5ghz. I struggled to get to 4.4. same with my 5820k, which would do 4.2, both with voltage bumps and good cooling.   Modern processors definitely run faster. The 12900k in my media pc will happily sit at 5.2 on lightly threaded loads   Tho you are right about the core counts. High core numbers have been around for eons, they were just far too expensive for mainstream use",Intel,2026-01-07 05:57:02,1
AMD,nxuzom9,"It's because AMD doesn't manufacture chips, we do. That's what the funding was for, to revive manufacturing leadership in the US not to save failing architectures.  And yes as a consumer buy what works best for your needs and budget. That's the best thing about Ryzen and AMD's resurgence.  I don't get your ""boots on throat"" comment, but to think AMD hasn't done anything mischievous in the past is, well a lot has happened between the two companies in 40 years.",Intel,2026-01-05 18:26:52,5
AMD,nxv19d7,"A strong Intel IFS is a strong US. Many people get disillusioned and deceived through all the cognitive dissonance on social media, especially gamers. Unfortunately they are easy to manipulate.  Many people are buying INTC in the US to retire on, we will see this more and more as we approach 2030. INTEL IFS has to succeed otherwise the US will be doomed in this century. Even India is getting into the semiconductor industry now and Intel is working with them. We need IFS to be on top, cream of the crop, I need a taste.  You are defeating yourself by getting wrapped up with all the geopolitical propaganda, go take a walk in the woods and get away from it all.  Checks are in the mail.",Intel,2026-01-05 18:33:57,-1
AMD,nxypf2c,"What? The deal under Trump was for 10% of the stake in shares. The government can sell the shares at any time to get a return. Since the stock has doubled since, it looks like it was a great deal for tax payers.",Intel,2026-01-06 06:19:05,2
AMD,nxv1ubv,Which way is up...?,Intel,2026-01-05 18:36:33,1
AMD,nxw36zg,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.  Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2026-01-05 21:28:35,1
AMD,nymqb9s,"Here are few more games  [https://youtu.be/XZ6JJNdMW4g?si=mkuKutnT1tc7k9U\_](https://youtu.be/XZ6JJNdMW4g?si=mkuKutnT1tc7k9U_)  [https://youtu.be/Ah6izQnylsM?si=s2iqbkLFHc83jHgI](https://youtu.be/Ah6izQnylsM?si=s2iqbkLFHc83jHgI)  [https://youtu.be/mQ80rNg0k3c?si=sgXQJ\_kh0Nb22BIM](https://youtu.be/mQ80rNg0k3c?si=sgXQJ_kh0Nb22BIM)  [https://youtu.be/fDdwwx4vYrs?si=QzJ7jz5H6oFWHwuU](https://youtu.be/fDdwwx4vYrs?si=QzJ7jz5H6oFWHwuU)  Non 3ds are bad for gaming, thats a fact.  Here is my latest test for 7800x3d vs 14700k  [https://youtu.be/ZTNE0EWtA1Y?si=zoujDFCzvCSj-UE2](https://youtu.be/ZTNE0EWtA1Y?si=zoujDFCzvCSj-UE2)",Intel,2026-01-09 17:27:05,1
AMD,ny1tgj8,thermalright.   Any frame will do. get whatever is cheap,Intel,2026-01-06 18:20:04,1
AMD,nyimhf2,"Let me explain why it will not degrade my CPU.  1. **TVB (Thermal Velocity Boost) is disabled when I set the turbo limit.** 2. **Current is not the issue.**Â First, even Intel chips were failing at idle. Second, you stated that ""the same power limit at lower voltage necessarily means more current."" That is incorrect, if it drew more current, then why does lowering voltage reduce power consumption? Have you missed basic physics? 3. **CPUs follow a voltage-frequency (V-F) curve.**Â The main issue was that either the CPU or the motherboard was supplying excessive voltage, or the CPU was requesting too much. Higher voltage is required for extreme single-core boosts, such as 5.9 GHz. This is why i3 and i5 CPUs were not affected. When I limit my boost and apply an undervolt, the CPU no longer requests high voltage. It might request slightly higher voltage for 5.5 GHz, but that is nowhere near the voltage required for a 5.9 GHz boost. 4. **The same principle applies.**  Its seems you are a failure of high life form.   Maybe read here more. it was already to high voltage   [https://community.intel.com/t5/Blogs/Tech-Innovation/Client/Intel-Core-13th-and-14th-Gen-Desktop-Instability-Root-Cause/post/1633239](https://community.intel.com/t5/Blogs/Tech-Innovation/Client/Intel-Core-13th-and-14th-Gen-Desktop-Instability-Root-Cause/post/1633239)  Maybe you havent read much. if you undervolted only using AC/DC loadline and didnt limit max frequency. Then your CPU could have degraded.   While I used offset and set the max turbo limit.  I know how silicon works. I tune every CPU and GPU I buy.",Intel,2026-01-09 01:50:53,2
AMD,nyx4wv5,"Yeah, I do use XMP to get 8000/8200hz but no tweaks and everything is Intel default.",Intel,2026-01-11 04:51:00,1
AMD,nxx7x5q,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:53:28,2
AMD,ny6b0of,"Probably just bad chips. In almost every review, 4.7â€“4.8 GHz was achievable on the 2600K, and 4.6â€“5.0 GHz on Haswell-E. 5.0ghz wasn't hard to do on 8700k-9900k-10900k. Also, ARL runs at lower clock speeds than RPL, so in some cases newer process nodes actually clock lower.   this happened many times before like 32nm SNB OC better than 22nm IVB and haswell or even 14nm 6000 series skylake, so no in most cases new process nodes don't improve clock speeds or at least not on desktop.",Intel,2026-01-07 10:04:24,0
AMD,nxv4op7,"Okay, so we've gone right off the rails of a logical discussion. I fail to see how we went from talking about a failing architecture, which I don't think of Intel's product line as, to the whole bailout discussion. Maybe because I said AMD didn't get bailed out? I think maybe you forget that AMD had to offload global foundries, I don't think anyone even blinked when that happened and it's likely because that was a different era.   My post had nothing to do with the bailout versus architecture (or saving it) , apart from mentioning that I don't believe as a consumer I should feel motivated to buy Intel over AMD, or any other American firm, at the moment.   To be very clear, as this is personal for you, I do own Intel equipment including an Arc graphics setup for one of my children. I'm not anti-Intel or anti-American at all.   I know the history between AMD and Intel fairly well. Oversimplifying, AMD as it is doesn't exist without Intel. AMD only grew because they were the most successful second source producer for Intel, and the most successful at riding the thin grey line between patient infringement and unique implementation of similar IP that kept them alive while everyone else in the X86 space either died or became irrelevant to the consumer or enterprise space. After Itanium, the story levels out with both companies becoming effectively unwilling AMD64 codependents, and I'm saying that humorously.   Intel would gladly own the X86 market outright. So would AMD. At the end of the day, we need the two driving innovation through competition. Even if the CHIPS Act, the government stock acquisition, and the nVidia partnership are solely aimed at bringing more, needed, chip manufacturing to the US, it could create a situation where that amount of leverage puts Intel into a hyper dominant position again in the near future. Honestly, I hope I'm wrong.",Intel,2026-01-05 18:49:13,2
AMD,nxwdvf9,"Hey sorry to interject like that, can you ask around what ppl in the team think the safe VCCSA voltage for raptor lake is beyond standard spec? You can dm the answer if u want. I'm from Russia so it's not like I'll go run RMA'ing this stuff just because you told me that info",Intel,2026-01-05 22:18:29,1
AMD,nxv1r9g,"Did you even read my post, or are you a bot? Seriously asking. I didn't bring up geopolitics. I live in Canada my dude. AMD has their graphics office still right beside the TO airport, and that's only a side comment. AMD is American too. My concern isn't about anything you just said lol.",Intel,2026-01-05 18:36:10,0
AMD,nxv9flb,65535,Intel,2026-01-05 19:10:25,1
AMD,nyk009z,"\>then why does lowering voltage reduce power consumption?  I have no idea what is the workload you are using which is guaranteed to never hit the limits. Solitaire?   Undervolting does not guarantee lower power usage with modern boosting CPUs. It very obviously (to any sane person) depends on workload.  \>Maybe read here more.   \>Thomas\_Hannaford, Employee   â€Ž You are so cute.  \>Motherboard power delivery settings exceeding Intel power guidance.  Literally what I said in 2, 3, 4 but in stupid terms.  \>Microcode and BIOS code requesting elevated core voltages which can cause Vmin shift especially during periods of idle and/or light activity.  Literally what I said.  \>IntelÂ® reaffirms that both **IntelÂ® Coreâ„¢ 13th and 14th Gen mobile processors** and future client product families â€“ including the codename Lunar Lake and Arrow Lake families - are unaffected by the Vmin Shift Instability issue.  This is verifiably bullshit because nothing about mobile processors protects them from wrong voltage received because of wrong LLC settings.  \>CPUs follow a voltage-frequency (V-F) curve  If you do not know that Intel is requesting high voltage to avoid insufficient voltage before frequency boost you do not know jack shit. It is literally what happens under description ""**Microcode and BIOS code requesting elevated core voltages which can cause Vmin shift especially during periods of idle and/or light activity**"" and there is a video proof of that with an oscilloscope.  No settings which you mentioned are preventing irreversible damage.   Claiming that you use Intel for reliability after you did not wish to use your own Intel at stock settings is laughable.",Intel,2026-01-09 07:01:30,0
AMD,nyjz7xj,"You skipped the 8600K, it also did 5ghz",Intel,2026-01-09 06:54:55,1
AMD,nzt2k76,"Anything over 4.7ghz on Haswell-E was NOT very common. I would say 4.4-4.5 on those chips was more realistic. 5 GHz we are talking golden godly chip.  I had a 5960X for 8 years, one of the better ones manufactured in Costa Rica and not Malaysia which was better for OC, and mine did 4.7 ghz.",Intel,2026-01-15 21:49:25,1
AMD,nxv805h,"My initial comments were on, ""I'm also not going to act like Intel being now backed by the US Government and MAGA"" and what the funding was for.  Edit: and my response was about who also backed the funding.",Intel,2026-01-05 19:03:57,1
AMD,nxwhiw8,"All I can recommend and say is follow whatever guidelines you're given officially and make sure your BIOS is updated. All those teams work to make sure it's delivered to the customer. Otherwise it's all random, some parts can handle higher voltage, some can't.  The term silicon lottery is real and just a nature of small scale manufacturing, EM and quantum effects these days.  I will say, with our new CEO a lot of these customer issues are now streamlined internally. Used to be layers between engineering and customer interaction, so I expect better responses and reliability than before.",Intel,2026-01-05 22:36:31,1
AMD,nxv9vyk,"Ah, so you mean *lower* the TREFI then, from what JEDEC or XMP specified?",Intel,2026-01-05 19:12:28,1
AMD,nzv5tob,"I believe that refined process nodes (the â€œ+++â€ stages) are what really improve clock speeds, not the first generation of a new node, or at least this is the case for the last 15 years.   Here are some examples:   -Intelâ€™s 32nm Sandy Bridge came after the early 32nm CPUs (dual-core i3/i5 300/500 series and the 6-core 980X), and Sandy Bridge turned out to be an excellent overclocker.   -22nm Ivy Bridge and Haswell were mediocre in that regard, while Haswell-E (a more mature implementation) overclocked better. Broadwell-E and early Skylake also werenâ€™t great for OC.   -Once the node matured, things improved again. Coffee Lake (refined 14nm) made 5 GHz relatively easy. The same pattern repeats with 10nm: Alder Lake (12th gen) came after Tiger Lake and could already reach ~5 GHz, while Raptor Lake pushed clocks even higher, up to ~5.7 GHz.   Overall, higher clock speeds tend to come from node maturity and refinement, not from the first use of a new manufacturing process, and I believe 5.8ghz could have been possible if intel used its own 10nm process node on ARL considering its HyperThreadingless.",Intel,2026-01-16 04:43:25,1
AMD,nxv944s,"On that side, it'll depend on what the US does with the stock it has. Canada bought GM stock during the 2008 crash, and then quietly sold it off as GM recovered. If the US does that, I don't really see an issue.  The MAGA part was a half hearted comment, made to follow the weirdness of the whole situation and comment I was replying on as well. As I said in a separate post:  ""It's so strange to me that we've hit a spot in consumer and enterprise computing where politics is now a factor (speaking about how decisions beyond national security are now driven by politics and trying to be on the right side at any moment when it matters). It's a different game when companies have to worry about that side, far too many consumers make decisions based on their politics when all that does is cause other issues. Obviously I won't go farther than that in a tech focused discussion lol, but I will say again that it's a different game and I don't think anyone wins if it becomes the norm.""  I'm going to add, light heartedly, I do hope we see AMD/nVidia/Qualcomm/etc manufactured and final packaged products coming out of Intel Foundries one day. I'm not sure how or if it will work, but the situation seems dead set not to allow significant further nodes beyond 2 nm or packaging to occur outside of North America. If the US wants viable national chip production, Intel is the better option, I hope it works out in a way that maintains design level competition while meeting national security goals.",Intel,2026-01-05 19:08:59,1
AMD,nxwo9wf,"I get that. But itâ€™d be nice to have a â€œthis voltage is safe for 99% of the cpus and this voltage is the LD50 for the cpuâ€. That would probably improve customer relations but your legal team might be very, very unhappy with that lol.   Anyway cheers for the response, with the way things are looking up for intel, i might be buying some stocks soon",Intel,2026-01-05 23:10:36,1
AMD,nxvnm9c,JEDEC 5600 cl 40 kingston fury sodimm 2x 16 gb,Intel,2026-01-05 20:15:57,1
AMD,nxws02u,"I mean, you can just figure that out yourself and buy new CPUs till you get one working :P",Intel,2026-01-05 23:30:06,1
AMD,nxvpz7b,"Yeah, but what was TREFI before you lowered it?",Intel,2026-01-05 20:27:01,1
AMD,nxvvs3t,10000 ish,Intel,2026-01-05 20:54:09,1
AMD,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
AMD,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
AMD,ny85o2z,Is Tiber cloud gone forever?  https://console.cloud.intel.com/ just gives a DNS error now.,Intel,2026-01-07 16:41:57,1
AMD,o04fx6u,"I have installed new Intel Wi-Fi 6 AX210.NGWG.NV in my ASUS laptop, bcz the old one died and couldnt connect to bluetooth since, WIFI works perfectly fine tho, so i dont know if problem is with drivers or not. Also i would just instal them from Intel, but i live in russia and i dont know any trustworthy sites, so if anybody knows, i would be really gratefull",Intel,2026-01-17 15:56:24,1
AMD,o07e6d2,"How do I know the legitimacy of an Intel wifi card, model AX210? I've been searching for it in Amazon and most are manufactured in Vietnam and China, with varying prices.",Intel,2026-01-18 00:38:33,1
AMD,o0uehap,"I keep seeing mentions of TPM in our system requirements and I'm honestly a bit lost on what it actually does for our security, so who is the best person in the org to chat with to get the full rundown?",Intel,2026-01-21 12:32:18,1
AMD,o29lgdq,"i've got an i7-14700kf on an asus rog strix b760-a with a corsair h100i elite capellix xt (240mm aio) and when i run after effects my cpu temps shoot up to 90 degrees, it probably wouldve gotten higher but i closed it cause it felt too high for what i was doing, i'm looking to undervolt my cpu but i dont know anything about it. i just want something safe and simple (im not looking for an extreme undervolt, just one that would lower my temps & possibly keep the same performance)",Intel,2026-01-28 18:56:54,1
AMD,o3j0qr3,"Killer Ethernet keeps asking me to update, i already uninstall, reinstall, safe mode and reformat my laptop and keeps asking me to update.   [Killer Ethernet](https://imgur.com/a/G3YuvJN)",Intel,2026-02-04 13:40:14,1
AMD,o3v6d3v,"Hi all, if i were to buy a series 2 intel CPU laptop with accompanying NPU and iGPU (likely arc 130 or 140). I wondered if anything mentioned on CES 2026 about the Panther lake software improvement is being guaranteed to get to the older CPUs via software/driver updates. I want to buy a cheaper older laptop now while still getting any supposed/promised gains from Intel than buy a panther lake laptop amid the nonsense ram pricings",Intel,2026-02-06 07:44:05,1
AMD,o597m5l,"Hello Intel, I saw a new microcode bios update for MPG Z790 CARBON MAX WIFI II! This is a new URGENT Bios Fix? Please, let us know!",Intel,2026-02-13 23:48:49,1
AMD,o5i257f,"Hi Everyone,  I have a 13600k (running stock thermals) on an ASRock 690 Xtreme motherboard with Windows 11. I have been having some odd issues in the past 6-8 months where under certain load my computer begins to stutter. It doesn't do it all of the time and it is difficult to pinpoint it. Also coming out of standby some odd things I have been seeing is that my BD-R drive sometimes will not transfer to my HDD and my USB ports will not read external drives.  Is there a possibility that the processor is having issues? I have not seen any crashes. Any way to pinpoint this to what could be causing it?  Thanks",Intel,2026-02-15 13:06:14,1
AMD,o67rbyi,Hi guys. I just put a RMA request in for my CPU and was emailed 10 minutes later saying that my SWR case was created. I got sent a set of instructions for packaging and shipping but no attachment was sent for an AWB label or RCI. Does this come later? Or would I need to contact Intel directly to recieve it?,Intel,2026-02-19 09:36:29,1
AMD,o6cm9pw,"Hello, I purchased an Intel 265K and am looking for a motherboard for it.  I would like to know if B860 motherboards allow us to undervolt or set power limit on the CPU.  I also want to know what we can do with memory?  I'm not really into overclocking the CPU, but the first two are important to me in order to control power and temperatures since I'm putting this in a small form factor mini ITX build.  I can live without detailed memory tweaks, just as long as we can set XMP profile.  If I can save money by avoiding Z890 that would be great.",Intel,2026-02-20 02:01:10,1
AMD,o6ywtey,I can't delete my [intel.com](http://intel.com) account. The request personal information deletion thing said I should be logged out and receive a receipt for this action (I assume this means an email) but this hasn't happened and I can still log in.,Intel,2026-02-23 15:55:34,1
AMD,nxwkozf,"u/Chelostyles Thank you for your inquiry regarding the CPU and RAM upgrade for your Dell Vostro 3670. As much as I'd like to provide my technical insights on this upgrade path, I'm not in a position to provide specific suggestions since this involves hardware modifications to an OEM system.  For the best compatibility outcome and to ensure optimal system performance, I strongly recommend reaching out to your system manufacturer directly. They can provide definitive guidance on supported CPU upgrades (i5-8400 to i7-9700) and maximum RAM configurations for your specific model. We don't want to inadvertently bypass any warranty terms and conditions on your system by providing modification recommendations that might affect your coverage.  Your system manufacturer's technical support team will have access to the exact specifications, BIOS compatibility matrices, and supported hardware configurations for your Vostro 3670 model. They can confirm whether the motherboard supports the i7-9700, the maximum RAM capacity (64GB vs 128GB), and any potential limitations or requirements for these upgrades.  This approach ensures you get accurate, manufacturer-validated information while maintaining your system's warranty protection.",Intel,2026-01-05 22:52:24,1
AMD,nxwjdkt,"u/I_like_carsyay  XPS 15 9530 hardware does support Resizable BAR, which is why Intel's system detection shows it as available for both your Arc A370M and Iris Xe graphics. However, the system manufacturer has designed their BIOS interface to prioritize stability and user-friendliness, often managing advanced PCIe features like ReBAR automatically in the background rather than exposing manual configuration options. This approach ensures optimal system performance while reducing complexity for users. I recommend checking for the latest BIOS updates from your OEM's support site and contacting their technical support team, as they would have the most current information about how ReBAR is implemented on your specific model and whether any additional configuration steps are needed to fully utilize this feature.     I've posted an article below in case you haven't yet come across it:  **Helpful Resources:**  *  [What Is Resizable BAR and How Do I Enable It?](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html)",Intel,2026-01-05 22:45:46,1
AMD,ny3upu3,"u/QunatumLeader Hi, thanks for your interest!Â  You can find and apply for all of our jobs online atÂ [http://](http://jobs.intel.com/)[j](http://jobs.intel.com/)[obs.intel.com](http://jobs.intel.com/). We donâ€™t currently accept submissions via social.Â  Good luck!",Intel,2026-01-07 00:05:20,2
AMD,o0l3yzt,"Late to this, but I'm a 13900K owner. I have not had any issues with stability since applying the BIOS update and haven't noticed any performance loss, so I think this is fine. I did not thoroughly benchmark before and after though, partially because of how high peak temperatures were before the update. I am using a Noctua NH-D15 and a contact frame to reduce CPU temperatures.  Up until a few days ago I would have said that thread scheduling isn't an issue, but then I played the game Maneater and it's basically unplayable unless you use launch options to force the game to only P-cores. There's the Intel ""Application Optimizer (APO)"" utility but it seems abandoned and you can't add your own games if Intel hasn't added a profile. I was a big proponent of E-cores but honestly it seems like a half-baked technology that Intel never put the effort in to support properly. That said I guess I could just entirely disable them if I cared so much, but that's a non-trivial amount of performance to just give up.",Intel,2026-01-20 01:26:55,1
AMD,nya3rq0,Hi u/ConspiracyPhD **Post**Â a question onÂ [IntelÂ® Tiber Developer Cloud Community](https://community.intel.com/t5/Intel-Developer-Cloud/bd-p/developer-cloud)Â forum for further investigation.,Intel,2026-01-07 21:48:00,1
AMD,o0e1nqe,"u/Far-Common2207 In this case, we suggest buying the wireless module from authorized Distributors to mitigate the legit concerns. Other than that, the OEM module warranty is not covered by Intel. For more details, you need to work with the Distributor or place of purchase for support to further verify if the wireless card is legitimate.  Check this article: [Where to find the Serial Number for IntelÂ® Wireless Cards](https://www.intel.com/content/www/us/en/support/articles/000092302/wireless.html)",Intel,2026-01-19 00:35:58,1
AMD,o0ztjvw,"[**Plenty-Solution-3692**](https://www.reddit.com/user/Plenty-Solution-3692/)**, TPM (Trusted Platform Module)** is builtâ€‘in security hardware that helps protect important data on your PC using encryption**. Intel PTT** is Intelâ€™s TPM that lives in the system firmware instead of being a separate chip, but it works the same way. Most PCs from the last few years already have TPM 2.0, sometimes it just needs to be turned on in the system settings. . If youâ€™re not sure how to do that, your motherboard or PC manufacturer should be able to help.  You can check this article for more information: [What Is Trusted Platform Model (TPM) and Its Relation to IntelÂ® Platform Trust Technology (IntelÂ® Pâ€¦](https://www.intel.com/content/www/us/en/support/articles/000094205/processors/intel-core-processors.html)",Intel,2026-01-22 05:04:22,1
AMD,o2kc1kd,"Individual\_War\_129, we do not provide typical temperature operating ranges for each processor or each core, as it can vary based on the system design and workload. Processors have internal protections to prevent against excessive temperatures. Operating ranges below the protection points are highly dependent on system configuration and workload.  In case you haven't come across it yet, you may check the articles below:  [Information about Temperature for IntelÂ® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [What Is Undervolt Protection and How Does It Affect Overclocking in IntelÂ® Extreme Tuning Utility (â€¦](https://www.intel.com/content/www/us/en/support/articles/000094219/processors.html)  [Thermal Design Power (TDP) in IntelÂ® Processors](https://www.intel.com/content/www/us/en/support/articles/000055611/processors.html)",Intel,2026-01-30 07:31:28,1
AMD,o3o6m00,"nfsanton, please be advised that the product you are reporting is an OEM (Original Equipment Manufacturer) device. As such, our support may be limited, since we do not have full visibility into the specific technologies, settings, or customizations implemented by the system manufacturer on your device.  For laptop systems, we strongly recommend installing and using the drivers provided by the system manufacturer, as these drivers are customized and validated to ensure full compatibility with your hardware.  That said, you may also choose to use the Intel generic driver if needed, which is available here: [**IntelÂ® Killerâ„¢ Performance Suite**](https://www.intel.com/content/www/us/en/download/19779/intel-killer-performance-suite.html). Please note that functionality and behavior may vary when using generic drivers on OEM systems.  You may also find this public article helpful: [IntelÂ® Driver & Support Assistant (IntelÂ® DSA) Keeps Showing Available Driver Update Notificatiâ€¦](https://www.intel.com/content/www/us/en/support/articles/000090127/software/software-applications.html)",Intel,2026-02-05 05:53:34,1
AMD,o4kauft,"Phaldaz, Iâ€™ll look into this further and will update you as soon as the information becomes available.",Intel,2026-02-10 04:54:29,1
AMD,o5mzlpw,"u/Infinite-Passion6886, we highly recommend keeping your system updated with the latest BIOS version. Each new release typically includes important fixes, stability improvements, and enhancements that help ensure your hardware continues to perform reliably and efficiently.",Intel,2026-02-16 05:54:40,1
AMD,o5t7cka,"u/jsmith1300, upon reviewing the symptoms youâ€™re experiencing, it appears that they may be caused by instability with the processor. To continue with the troubleshooting, please update the BIOS to the latest version, [21.01](https://www.asrock.com/mb/Intel/Z690%20Extreme/index.asp#BIOS). If the issue persists after the update, please let me know.  May I also ask if the processor has been overclocked?",Intel,2026-02-17 04:21:36,1
AMD,o6cklv0,"u/Careful_Classic_9959, I sent you a private message.",Intel,2026-02-20 01:51:01,1
AMD,o6wll7s,"u/MinimumMarsupial6782, Motherboards are made by thirdâ€‘party manufacturers, so the features can vary depending on the brand and model. Because of this, we recommend reaching out directly to the board manufacturer to confirm what specific options like undervolting, power limit controls, or memory settings their boards support.  For the memory side, the CPU can handle DDR5 up to 6400 MT/s, so you should be able to run XMP profiles within that range without any issues.  You may also find this public article helpful with checking supported motherboards: [How to Find Compatible Motherboards for Your IntelÂ® Boxed Desktop Processor](https://www.intel.com/content/www/us/en/support/articles/000005909/processors/intel-core-processors.html)",Intel,2026-02-23 05:29:52,1
AMD,o72vo35,"Hello [PowerZox](https://www.reddit.com/user/PowerZox/) May I ask when did you request to delete your information. And also, have you tried to check your inbox inside [Intel.com](http://Intel.com) profile for any updates? Kindly send me a DM for the exact link for me to check also since Intel have different login page for different programs or products.",Intel,2026-02-24 04:12:16,1
AMD,nyarzrn,Forum doesn't exist or access denied.  I guess Tiber is just gone now.,Intel,2026-01-07 23:42:22,1
AMD,o0fgizr,Do you know any authorized distributors here in the Philippines?,Intel,2026-01-19 05:37:43,1
AMD,o0zyc8d,"I see, all good thanks for your support!",Intel,2026-01-22 05:39:13,1
AMD,o4kdh9i,"thats much appreciated, thanks",Intel,2026-02-10 05:13:43,1
AMD,o5uplgc,Thanks for replying. I am on version 21.01 for a month or more and still running into these issues. I haven't overclocked the processor and using Intel's thermal recommendations base 125W max 181W.  I actually checked this in the BIOS and it must have been changed after an update. I changed the the long setting back to 125W. At first I thought it helped but the issue is still there.,Intel,2026-02-17 12:08:18,1
AMD,o77c02w,I made the request yesterday. This is the link where I made the account deletion request: [https://www.intel.com/content/www/us/en/secure/my-intel/profile.html#edit-personal-profile](https://www.intel.com/content/www/us/en/secure/my-intel/profile.html#edit-personal-profile)  Using the Request Personal Information Deletion button. Does it take multiple days to delete an account? The popup message said I'd be logged off and sent a confirmation so I assume it would be instantaneous like with most other services.,Intel,2026-02-24 20:39:07,1
AMD,nz1jsfl,u/ConspiracyPhD I just checked the forum and it looks like itâ€™s up and running. Could you try accessing it again using your Intel account?  [IntelÂ® Tiber Developer Cloud - Intel Community](https://community.intel.com/t5/Intel-Tiber-Developer-Cloud/bd-p/developer-cloud)  [](javascript:void(0);),Intel,2026-01-11 21:15:16,1
AMD,o0jlcxj,"u/Far-Common2207 According to the directory, these are the distributors in the Philippines. [Distributor Partners](https://www.intel.com/content/www/us/en/partner/showcase/partner-directory/distributor.html#sort=relevancy&f:@sfdisticountry_en=[Philippine,Philippines,Phillippines])",Intel,2026-01-19 20:45:26,1
AMD,o4r7vk6,"u/Phaldaz, upon checking, the graphics update for the IntelÂ® Graphics for IntelÂ® Coreâ„¢ processors (Series 2) will contain the following:  \- Driver improvements for Arc iGPU (graphics performance, game compatibility)   \- NPU software stack updates (AI acceleration libraries, framework support)   \- General software optimizations that aren't hardware-dependent  However, you would need to consider that the following will not be included on the driver update:  \- Hardware-level architectural improvements (new instruction sets, hardware security features)   \- Significant performance gains that rely on new silicon design   \- Power efficiency improvements tied to manufacturing process changes  If you're primarily looking for good performance now and aren't concerned about having the absolute latest features, a Series 2 laptop could be a smart buy. You'll get most software improvements, and the money saved could offset any performance differences.  However, if you're planning to keep the laptop for 4+ years or need cutting-edge AI performance, waiting might be worth it despite the RAM pricing issues.",Intel,2026-02-11 06:16:06,1
AMD,o600ym1,"u/jsmith1300, thank you for confirming that you're already on the latest BIOS version and that the issue still persists. Iâ€™ve sent you a private message to gather more information.",Intel,2026-02-18 04:54:27,1
AMD,o79moze,"Please wait for 24-48 hours since the request needs to enter our system and it may need some checking first before getting a notification. If you are still having the same issue, please update this post. Thank you",Intel,2026-02-25 03:57:12,1
AMD,nz301xe,"Nope.  https://imgur.com/a/tYRhYoV  Access denied and a nice ""This content is no longer available.""  Guess it's a completely dead project and should be removed from Intel's website.  http://console.cloud.intel.com/ is not accessible.",Intel,2026-01-12 01:35:48,1
AMD,o4wrexm,"this was a great response, thanks very much!",Intel,2026-02-12 01:57:39,1
AMD,nz3b0gd,"u/ConspiracyPhD Please check your inbox, Iâ€™ve sent you a personal message. Iâ€™ve already coordinated your concern with the respective team, and as per their instructions, youâ€™ll need to email them directly.  [](javascript:void(0);)",Intel,2026-01-12 02:33:45,1
AMD,nw3e1uz,that is the most non-descript render of a laptop possible,Intel,2025-12-26 22:13:56,3
AMD,nw638sa,So light it visibly doesn't have any ports?,Intel,2025-12-27 09:58:34,2
AMD,nur68kw,"Does intel 10A still come out as scheduled in 2027? I googled it and found out intel said the 10A will come out in 2027, but this was old news in 2024.",Intel,2025-12-18 21:37:42,17
AMD,nuthq66,It's shameful to see LBT posing with 14A wafers when all the groundwork for this was setup by Pat Gelsinger. The entire Intel board should have been sacked instead of Pat.,Intel,2025-12-19 05:59:22,14
AMD,nuu5n9y,I wonder how intel and other companies are going to manage for next year? Prices for memory and SSDâ€™s are predicted to go even higher putting off many buyers from getting a new PC build or laptop.   This makes me concerned Nova Lake wonâ€™t sell as well because of this.,Intel,2025-12-19 09:41:00,4
AMD,nur0ojq,"GFHK also has 14a for Razor and Coral Rapids in 2H 2027, so I'm taking what they are saying with very little credibility.   Plus, we had very similar rumors during 18A, and that went nowhere. Fool me once...",Intel,2025-12-18 21:10:09,10
AMD,nutolrl,Unbelievable till official announcement,Intel,2025-12-19 06:56:47,2
AMD,nusrcmh,can't they use it to make more ram ?,Intel,2025-12-19 03:04:41,3
AMD,nur9juo,good news,Intel,2025-12-18 21:54:21,2
AMD,nvzjgd1,They can't even sell 18A to NVDA what are they doing on 14A really ?,Intel,2025-12-26 06:29:30,1
AMD,nym6t8e,"Hm, we will see what happens with the stock price soon, but so far so good",Intel,2026-01-09 15:59:57,1
AMD,nur0nvy,"Lisa So Sue Me wants a taste of the Lip? Am I living in a different dimension? I callled out So Sue Me on X, is she jumping on Big Blueâ€™s Back?  Is anyone Dollar Cost Averaging INTC? It will still be awhile before IFS is firing on all cylinders. The Lip said he would stop high end chip production for external customers (If No One Took A Byte) in order to get $$$ to build out Ohio Fab.   Letâ€™s get it done. Iâ€™m driving distance from the Ohio Fab, any chance Intel will give me a tour?",Intel,2025-12-18 21:10:03,-18
AMD,nusksfh,"If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  [https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots](https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots)  *""Intel's previously-unannounced Intel 10A (analogous to 1nm) will enter production/development in late 2027, marking the arrival of the company's first 1nm node, and its 14A (1.4nm) node will enter production in 2026.* Â ***\[Edit:Â to be clear, this means 10A is beginning development, not entering high volume manufacturing, in 2027\]*** *The company is also working to create fully autonomous AI-powered fabs in the future.""*",Intel,2025-12-19 02:26:13,13
AMD,nur6gwd,"14A probably won't be ready for 2027, much less 10A.",Intel,2025-12-18 21:38:53,17
AMD,nutent6,10A & 7A are in R&D phase,Intel,2025-12-19 05:35:17,3
AMD,nurn23y,It's gonna be 2026 soon and 18A is launching at the very start of 2026. A double node shrink in like 2 years doesn't exactly sound very possible.,Intel,2025-12-18 23:07:17,4
AMD,nuu144l,"Remember, these are just names/nicknames. 10A? The difference between 14A and 10A is probably equivalent to the difference between 14nm and 14nm+",Intel,2025-12-19 08:55:31,2
AMD,nurpt6m,And yet here you are.,Intel,2025-12-18 23:23:22,9
AMD,nutpnod,"Brother, don't hint at your place of employment when you have your full face in your profile as well as you commenting in NSFW subs.",Intel,2025-12-19 07:06:02,3
AMD,nuteqb3,There will probably still be another of layoffs next month ðŸ˜‚,Intel,2025-12-19 05:35:49,2
AMD,nutezdb,"Yes, perhaps itâ€™s better if you post it on the r/intelstock subreddit instead ðŸ¤ª",Intel,2025-12-19 05:37:45,1
AMD,nw3qq9x,"TBH I feel LBT is doing a good job. I was hesitant at first, but he's making a lot more sense than Pat's crazy descent into spending crazy amount of cash with no business in sight.  Speaking as a shareholder.",Intel,2025-12-26 23:27:31,3
AMD,nutoo6g,"The entire Intel board probably should have been sacked, but Gelsinger as well. He failed at his main mission and drove the company into a crisis. That kind of thing should have consequences.",Intel,2025-12-19 06:57:22,2
AMD,nuv6jd0,Who was it that decided to exit the SSD business.  They sold off a cash cow for pennies on the dollar.,Intel,2025-12-19 14:18:21,0
AMD,nv78q7z,"Ram should be at a more reasonable price in 2027 according to Moores Law is Dead. Maybe not $100 for 32GBs, but maybe below $200 ðŸ¤ž",Intel,2025-12-21 14:18:36,2
AMD,nvi0fpp,They have contract.,Intel,2025-12-23 05:47:11,1
AMD,nutu4bu,Nvidia is at least some what believable. AMD though?,Intel,2025-12-19 07:47:24,5
AMD,nuu5f18,"I thought that too. At least they'd have some money coming in. But apparently it takes years to rejig the plants to churn out RAM instead of CPUs. And they're heavily invested in getting the next gen CPU fabs working.   Pivoting to RAM just doesn't make sense, unless they magic'd up a new type of RAM that's cheap to make and has super low latency - which is one thing I've always thought they ought to do.   Imagine if external RAM ran with super low latencies like CL1 or CL2 or something. You wouldn't even need branch prediction and prefetch and massive caches in the CPUs.",Intel,2025-12-19 09:38:44,3
AMD,nutophj,"""news"" needs a lot of quotes around it...",Intel,2025-12-19 06:57:41,1
AMD,nuuzsz6,This isn't wallstreetbets. We don't talk like that here.,Intel,2025-12-19 13:40:01,5
AMD,nusti41,">If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  Yup, and to make it even more obvious, the same graph also has Intel 14A showing up early 2026, and 20/18A showing up at the start of 2023*,* so clearly it's not the date of when the node is going to come out (or even start HVM).",Intel,2025-12-19 03:17:42,7
AMD,nurmeyo,"Dunno why this is being downvoted, the CEO of Intel himself said that 14A is a 28-29 node in the Q2 2025 earnings call.",Intel,2025-12-18 23:03:37,13
AMD,nuy09wl,enough info about how intel names products exists to know. if it didn't increase in transistor density per mm it would not be called 10A.,Intel,2025-12-19 23:07:36,5
AMD,nutpt6n,"I think everyone knows there will be continued Q1 and possibly Q2 layoffs.   Return to office didn't lead to enough voluntary attrition. Leadership wants to hit a magic number which sounds good for financial reports, not what is actually viable to run things.",Intel,2025-12-19 07:07:25,6
AMD,nw3rzlk,That crazy amount of cash being spent by Pat is what enabled 18A and 14A. They HAD to buy multuple $250M Litho machines from ASML in order to make that possible. Pat was playing catch up after years of under-investment by Swan and Krzanich. It was necessary and LBT is getting the credit. You don't appear to understand the lead times required in the semi industry. Pat understood that. The mistakes Pat made were trying to build a fab in Ohio and not cutting headcount and getting rid of dead weight sooner.,Intel,2025-12-26 23:35:12,5
AMD,nuuu28f,The thing intel is doing rn is literally pat's groundwork isn't it?,Intel,2025-12-19 13:04:50,10
AMD,nutv81y,Still a tall order imo unless it's some defense chip for RAMP-C,Intel,2025-12-19 07:57:50,1
AMD,nv0hjyu,"If they're following industry standards I'd say it depends on how good AMD's next gen is. Intel doesn't need direct access to AMD designs to etch chips for them, and designers make way more than per wafer than foundries do.  If AMD has superior designs to intel again they could finally ship out some damn chips for laptop OEMs. It would hurt intel more than the revenue would benefit them imo since client has really been carrying intel for the last six years and demand for AMD chips has been high despite the drip feed of strix chips. honestly I'm considering an AIO/NUC/whatever the new name is with strix halo and unified LPDDR5 to upscale old footage without having to use my daily desktop. imagine if it was available at scale.",Intel,2025-12-20 10:33:05,1
AMD,nuu642g,"they don't have to make faster ram, just make it, right now, some ppl don't really care about speed",Intel,2025-12-19 09:45:35,2
AMD,nuu0o0x,"So, risk production in late 27/early 28 and HVM in 2029 I suppose?",Intel,2025-12-19 08:51:11,2
AMD,nuvsda6,YEs it is. He did make mistakes. He was hiring like crazy at the beginning of his term. And he should have started cutting sooner. But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.,Intel,2025-12-19 16:10:45,2
AMD,nuwuwrt,"Nothing they're doing *right now* is a success story. Remember that they don't actually have customers, and that is first and foremost what got Gelsinger fired. As things stand, the foundry as a whole is a failure. If things turn around, that will have to be under Lip Bu.",Intel,2025-12-19 19:20:39,1
AMD,nuu5jf7,I wasnâ€™t aware 14A is part of the RAMP-C initiative. I thought it was only Intel 16 & 18A that are currently covered by RAMP-C?,Intel,2025-12-19 09:39:58,1
AMD,nuubzhq,I think so. Maybe optimistically we see a 14A product in late 28'.,Intel,2025-12-19 10:41:56,5
AMD,nuwv4b8,"> But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.  No, that was just more wasted money. 18A doesn't even use the high-NA machines Intel bragged so much about. It seems they tried blaming their struggles in foundry on the equipment instead of the broader org culture and talent.",Intel,2025-12-19 19:21:43,-1
AMD,nv78ydu,"As much as I hate to say it, Intel arc was also a mistake.",Intel,2025-12-21 14:20:00,0
AMD,nvl276b,get out of here with your sensable comments. we only circle jerk on this sub,Intel,2025-12-23 18:22:21,1
AMD,nuu6whm,It can expand in future ? My point is how can we believe such stuff at face value without actual proof.,Intel,2025-12-19 09:53:22,1
AMD,nuzuyhs,14A does use the High-NA machines. They didn't buy them with no plan to use them That would be stupid.,Intel,2025-12-20 06:44:37,4
AMD,nvl210i,"no it wasnt. GPU's are surpassing cpu's eventually if not now.  a major part of amds success  was buying radeon all those years ago. when intel realized how utterly shortsighted they had been, they pushed arc heavy even though it wasnt going to succeed that well.  this was the right choice, as otherwise they would look like a dinosaur.",Intel,2025-12-23 18:21:32,2
AMD,nuu7dr5,"It can expand in the future but this is a trial, itâ€™s not yet a long term commitment until the outcome of the project is known (final evaluation wonâ€™t be until 2026/2027). 14A is not part of RAMP-C, itâ€™s still in phase III trial with 18A. Thereâ€™s been no additional RAMP-C design calls via NSTXL that Iâ€™m aware of",Intel,2025-12-19 09:58:04,1
AMD,nv05iea,"> 14A does use the High-NA machines. They didn't buy them with no plan to use them  They bought the very first high-NA machines, claiming it was for 18A. Now they won't be used until a node that hits volume in '28/'29, by which point TSMC will have (or rather, already has) much better machines. So what exactly was the point?  > That would be stupid.  Is that not a perfectly apt description for Intel's foundry strategy in recent years? It sounds like they really drunk the coolaid with their attempts to blame the 10nm failures on the lack of EUV.",Intel,2025-12-20 08:28:08,2
AMD,nvl37xc,Yeah. The real mistake was LBT and the other Intel board members nerfing the r&d budget.,Intel,2025-12-23 18:27:21,2
AMD,nv074kj,14A will have volume production in 2027.,Intel,2025-12-20 08:44:44,3
AMD,nv63f2d,Didn't Intel say in a presentation that 2027 is risk production for 14A? https://www.techspot.com/news/107736-intel-doubles-down-foundry-ambitions-unveils-18a-14a.html  https://www.youtube.com/watch?v=5Jbj4RQBXbo&t=818s,Intel,2025-12-21 08:18:24,1
AMD,nv088jg,"Lip Bu himself is saying '28-'29. At this point, there isn't a chance in hell it's ready for volume in '27.",Intel,2025-12-20 08:56:04,0
AMD,nvqcqv3,I just wanted arc to succeed ðŸ˜”,Intel,2025-12-24 15:43:21,2
AMD,nybmp7b,did you buy one? I have owned two. A A750 and now a B580. They are great cards for the price paid. I'd like to upgrade to a B60 PRO. 24GB VRAM sounds amazing especially for $600. but I can't find one in stock anywhere.,Intel,2026-01-08 02:20:39,1
AMD,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,58
AMD,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
AMD,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
AMD,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
AMD,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,10
AMD,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,8
AMD,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,5
AMD,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,5
AMD,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,2
AMD,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
AMD,nspzeik,If itâ€™s just â€œ16% faster than 890mâ€ itâ€™s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,5
AMD,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
AMD,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
AMD,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
AMD,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
AMD,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,6
AMD,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.Â      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
AMD,nsv64t7,"I mean no offense, but Passmark is irrelevant.Â  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.Â  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,2
AMD,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
AMD,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
AMD,nsyv727,"I guess we'll see more when we get actual info about the potential devices.Â  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
AMD,ntimkr9,Back in the day you could overclock a 2600k from 3.4Ghz to 4.5Ghz on a $25 Hyper212 cooler. The performance gains were incredible as Sandy Bridge scaled very well at higher clocks.   Now days CPUs come overclocked already.,Intel,2025-12-11 19:25:58,30
AMD,ntifd3j,"""It's crazy to think that a cpu from 2009 can be easily overclocked.. 2.9Ghz to 4.1Ghz is crazy !""  You could overclock huge amounts on earlier generations - I used to run Pentium 4 1.6GHz chips at 3.2GHz on air-cooling, more on phase-change cooling.",Intel,2025-12-11 18:50:27,28
AMD,ntikk9s,"I ran my i5 750 2.67Ghz for years at 4Ghz without any issues. I benched it some at 4.2Ghz even, but it was not fully stable.  The X58 CPU are even better tho. And even if you had insane OC potential back in the days it was not as good as it sounds, since the turboboost was higher than the stock frequency that is listed.",Intel,2025-12-11 19:15:50,8
AMD,ntjwvoj,"Lol a 15 year old computer running Windows 11, meanwhile Microsoft telling people to upgrade 5 year old laptops for win10 being EOL.",Intel,2025-12-11 23:27:29,6
AMD,ntivoqo,X5690@4.6GHz on Rampage III Extreme ðŸ˜˜,Intel,2025-12-11 20:12:02,5
AMD,ntj26xa,it is crazy that intel sold you same technology at downclocked speeds to make a nice model range with different prices.,Intel,2025-12-11 20:45:29,5
AMD,ntiq1p0,Sick stuff. I still got my i7 930 at 4.2Ghz running just fine. These types of chips overclock like crazy.,Intel,2025-12-11 19:43:28,3
AMD,ntkwagl,Be nice. Give it another stick of ram!,Intel,2025-12-12 02:58:53,3
AMD,ntoxmhs,Q6600 G0,Intel,2025-12-12 19:02:37,3
AMD,ntiqlci,"Cool. Glad it worked for you. I have dual xeon server, maybe i should try it. But its production server dont wana break my apps. Lol",Intel,2025-12-11 19:46:15,2
AMD,ntjb06t,My 2500k did ~4.8 ghz and my 6950x did 5.2 ghz. Its base clock was like 3.2ghz and this was using 128GB of quad channel DDR4.  It was â€œstableâ€,Intel,2025-12-11 21:29:41,2
AMD,ntkz9ut,45nm is crazy in 2025,Intel,2025-12-12 03:16:12,2
AMD,ntosqvz,500W power draw when,Intel,2025-12-12 18:38:31,2
AMD,nthl3mn,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-12-11 16:23:16,1
AMD,ntk2ims,"I used to run my i3-540 at 4.2GHz, air cooled on what is effectively worse than a Hyper 212 Evo. I miss the old days when I could overclock the snot out of them. These days I guess they're binned to almost their max potential out of the factory so most of the time I'm undervolting them.",Intel,2025-12-12 00:01:05,1
AMD,ntk9tcx,Well done. Still using two H55m machines with OC (x3450 and i5 661).  They also OC decently at stock voltage keeping turbo and all power savings. My X3450 does 2.6 -> 3.3Ghz(3.8 turbo). The advantage is that it idles quite low at 50-60W.   But for gaming and rendering it's better to go all in as you did. Most chips can do anywhere from 3.8 to 4.2 all cores IME.,Intel,2025-12-12 00:44:00,1
AMD,ntl2d0n,nah my 40 logical processors would smash through it all  x2 xeon e5-2680 v2,Intel,2025-12-12 03:34:33,1
AMD,o5m1tqc,"I was just given a vinyl cutter and it's powered by an old tower server with an x3450. I was thinking about junking it but for being old, it's a nice tower. Can put 8 x 3.5in drives in it. I may just clean it up and see what I can turn it into.",Intel,2026-02-16 01:57:11,1
AMD,ntk4sw2,"is that better? I dont need to dive into setting anymore, the CPU maker do it for me with warranty.",Intel,2025-12-12 00:14:36,4
AMD,ntsgvaj,"There is still more to work with, especially if one does not fossilize on static all core OC, but does 2-step TVB fueled dynamic OC, Ecores are Aldo the source of much happiness on arrow",Intel,2025-12-13 09:30:18,1
AMD,ntja1e8,I miss overclocking. Felt like you were getting a bargain. Now I donâ€™t even try.,Intel,2025-12-11 21:24:52,13
AMD,ntjnkj4,"Not as big an OC as yours, but I had a pre-built from FutureShop.  It was their home brand name.  Found a BIOS for the board that wasnâ€™t theirs.  Managed to get 3.2GHz out of a 2.4GHz Pentium 4 on pre-built from FutureShop cooling.",Intel,2025-12-11 22:34:57,3
AMD,ntihjuu,"Wow, soo cool",Intel,2025-12-11 19:00:59,1
AMD,ntnudso,The motherboard doesn't accept other stick of ram. Only my corsair ram work,Intel,2025-12-12 15:49:11,1
AMD,ntmrk4p,How did you even get a 6950x to boot at 5.2ghz? Most of them hit a wall around 4.3ghz,Intel,2025-12-12 12:03:28,3
AMD,ntkag8u,"Has its ups and downs. Now that I'm older and have less time to tweak things and mostly just want shit to be stable, I see ""pre-overclocked with maybe 5% performance left on the table"" as a pro. The con is that chipmakers just jam a ton of power through it to make it happen, and the option of buying a half-price chip and spending an entire sleepless weekend tweaking it yourself to get 95% of the more expensive chip's performance is gone.",Intel,2025-12-12 00:47:48,3
AMD,ntjl0xd,"Same - the complexity and heat rose a lot and the gains because less significant - with multi-core chips and turbo frequencies there just isn't much headroom in them.  That and I work fixing issues with computers all day, I just want my own PC to work.",Intel,2025-12-11 22:21:04,5
AMD,ntlnjst,"Thats because these older CPUs were surprisingly energy efficient. Also mostly because now modern CPUs are powerful enough where overclocking is pointless. Even my i3-12100 being overclocked would be pointless, even if its only a 80 watt CPU",Intel,2025-12-12 05:56:24,2
AMD,ntwqjex,He couldn't without LN2.,Intel,2025-12-14 01:36:27,2
AMD,ntnxj8h,"It was short lived, over ~7 years I had to pull back the multiplier from 52 to 44 to keep it stable.  I retired the system this year.  It was a full open loop from EK.  2x Pascal Titan X in SLi",Intel,2025-12-12 16:04:28,0
AMD,ntmp3kk,"I used to overclock everything, now I undervolt everything lol",Intel,2025-12-12 11:44:00,2
AMD,nonhxk4,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is â€œslowâ€ is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already â€œbeatingâ€ AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Intel,2025-11-13 16:13:13,53
AMD,nootzxi,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Intel,2025-11-13 20:08:27,11
AMD,nrd4uj2,"I've gotten one and honestly it's amazing, easily the best laptop I've ever used so far.   I was skeptical about the battery life claims but I've genuinely found that using it for about 8 hours straight for coding, only drains the battery maybe 50%.  I've set it to only charge up to 80% max for battery health conservation, and I've regularly coded for 12 hours straight on the medium performance profile and haven't needed to charge until I got back home.  (This is for the Ultra 7 258v cpu variant btw)  Also this is while running Fedora with KDE Plasma which makes the battery life even more impressive as it's one of the heavier distros running cutting edge hardware and I've heard that Linux has less battery optimization compared to windows.    Screen isn't anything to write home about but the 100% srgb one looks good enough and is bright, 60hz looks kind of bad but I know that it saves a lot on battery.   Keyboard feels very nice as far as laptop keyboards go, having it be easily swappable is lovely as I wore out the keys on my old laptop, and I want this thing to last.   Linux hardware compatibility is perfect so far, even the fingerprint sensor works out of the box on fedora.   My only real complaint is that the plastic it is made out of is a major grease magnet and if I touch it without having immediately washed my hands, even if my hands weren't dirty, it'll leave dark patches from oils. Also it would be nice to have swappable RAM but I think 32gb ought to last a very long time anyway.   Genuinely seems like arguably one of the, if not the, best laptops for actually getting work done. Maybe it's not as fancy or sleek, but it just works. It's like the 2001 Toyota of the laptop world, it's not winning prizes for looks, but it'll never die, gets good mileage (battery life), and is easily repairable. Maybe not the laptop you want, but definitely the one you need (excluding people who need something like a dedicated GPU or really need super high CPU performance).",Intel,2025-11-29 11:19:13,4
AMD,noon81e,"Intel beats AMD in software (drivers, firmware) â€¦ I got think pad 780M laptop by company I work for. Randomly display wonâ€™t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Intel,2025-11-13 19:34:25,12
AMD,non7ozt,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Intel,2025-11-13 15:23:02,8
AMD,nom9a0s,lol. Even in the cons it says weaker multicore than AMDâ€¦.?   This article seems like AI wrote it,Intel,2025-11-13 11:53:59,1
AMD,nr8651t,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Intel,2025-11-28 15:11:41,1
AMD,nopvuqn,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Intel,2025-11-13 23:25:31,1
AMD,nonh9ew,Suck at gaming.,Intel,2025-11-13 16:09:54,-13
AMD,notgml4,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Intel,2025-11-14 15:00:36,8
AMD,noy6f36,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Intel,2025-11-15 08:22:42,8
AMD,npap4we,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Intel,2025-11-17 10:42:37,3
AMD,ntz6joo,Isnâ€™t the keyboard one of the most important characteristics?,Intel,2025-12-14 13:39:49,1
AMD,nopn323,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Intel,2025-11-13 22:36:30,-1
AMD,ntz6z28,"Hey Iâ€™m looking at the exact same laptop that you have. Can you tell me about the build quality and if thereâ€™s any keyboard flex when pressing down on it? Please tell me. Iâ€™m going to use it for word, excel, reading lots of pdf files and ebooks and watch movies. Will it be enough for that?",Intel,2025-12-14 13:42:38,1
AMD,o2qkn2t,Omg a positive review for Intel Lunar Lake ðŸ˜­  https://www.reddit.com/r/laptops/s/RYInPJfnAd,Intel,2026-01-31 04:40:30,1
AMD,nop3ehp,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Intel,2025-11-13 20:56:10,5
AMD,nongtqn,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Intel,2025-11-13 16:07:47,3
AMD,nousjfs,Soldered ram is a lot faster. So no.,Intel,2025-11-14 18:58:54,3
AMD,noniq16,Yes but now RAM costs a ton of money,Intel,2025-11-13 16:17:05,0
AMD,nomcmkj,Is multicore performance the only consideration when buying a laptop?,Intel,2025-11-13 12:19:56,27
AMD,nqyoc8i,What kind of issues?,Intel,2025-11-26 23:00:59,2
AMD,ntz73fr,What kind of issues with Intel? I thought it was the AMD that had tons of issues,Intel,2025-12-14 13:43:27,1
AMD,nonhqb5,It is not a gaming laptop,Intel,2025-11-13 16:12:13,18
AMD,np9o16h,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Intel,2025-11-17 04:44:52,2
AMD,np3siex,It's an enterprise grade product you buffoon.,Intel,2025-11-16 06:03:41,4
AMD,npd9987,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Intel,2025-11-17 19:32:27,2
AMD,np8gg6z,Build quality.Â    Thinkpads are solid machines that are easy to fix.Â    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Intel,2025-11-17 00:13:54,1
AMD,ntzg92j,"So build quality will be subjective, from what I can tell, it's got very good build quality in terms of ""real"" factors such as durability. But it definitely feels less ""premium"" than similarly priced consumer grade laptops. The plastic is plastic so it will flex a little bit, but the parts all seem very well put together and it does feel ""solid"" overall.   I haven't really noticed keyboard flex, but I have noticed a slight amount of flex where my palms rest, particularly on the right side, where the smart card reader is, which makes sense as it is just a big hole in the side of the laptop. I plan on getting a dummy smart card to fill the gap and hopefully that should reduce it.   Overall whilst the internal chassis is metal, the outside is just plastic. I imagine that is good for durability, as it ought to be able to absorb shocks, but, as I said, it definitely makes it feel less ""premium"". They key press feel of the keyboard definitely does feel very nice as far as laptops go though. Obviously it's still nothing compared to a good mechanical keyboard but for a laptop it's very nice.   I bought this laptop for longevity and durability, so given that It's only just come out, I can't really say much about that, but the prestige of thinkpads of previous generations kind of speaks to their reliability. Plus it's apparent that they are still quite easy to repair and Lenovo has video guides on replacing loads of the parts.   And for your use case the battery life should be very good. It seems the Intel chip was designed to be very efficient during periods of downtime and something like viewing a PDF or editing a document has a LOT of downtime for the CPU",Intel,2025-12-14 14:40:19,2
AMD,norwnxs,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Intel,2025-11-14 07:46:09,4
AMD,noruygl,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Intel,2025-11-14 07:29:24,3
AMD,noxc6wn,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Intel,2025-11-15 03:57:11,2
AMD,nop71cl,All the more reason to make it upgradable,Intel,2025-11-13 21:14:31,-2
AMD,nomhldl,Lunar Lake isnâ€™t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Intel,2025-11-13 12:55:02,15
AMD,nomp84g,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Intel,2025-11-13 13:42:02,4
AMD,nu0g7o7,This was back when the 14th generation were having issues.,Intel,2025-12-14 17:47:28,2
AMD,nonivqb,"It's $2,000 so no excuse.",Intel,2025-11-13 16:17:52,-11
AMD,nu6fuik,Thank you so much for this valuable and comprehensive information! I really appreciate it:),Intel,2025-12-15 16:29:10,1
AMD,npd8o9g,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Intel,2025-11-17 19:29:33,3
AMD,nomwgxm,"Lunar Lake already beat AMD, nobody buys AMD laptops",Intel,2025-11-13 14:23:36,19
AMD,non5ael,i stopped at $2100 for a Thinkpad T14,Intel,2025-11-13 15:10:49,1
AMD,nu0kv9z,"Ah okay, got it thanks.",Intel,2025-12-14 18:10:16,1
AMD,o2ql7sb,Panther Lake fixed it,Intel,2026-01-31 04:44:35,1
AMD,nowos5a,wrong  Nobody Supply AMD laptop     There fixed for u,Intel,2025-11-15 01:22:32,2
AMD,nov79aa,"I do, and many of the people I know do.",Intel,2025-11-14 20:14:59,-1
AMD,nonhh1g,Nobody pays that much.,Intel,2025-11-13 16:10:57,6
AMD,np79214,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Intel,2025-11-16 20:19:32,-1
