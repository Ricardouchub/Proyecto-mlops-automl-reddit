brand,comment_id,text,subreddit,created_utc,score
Intel,n133hqd,"Same price as B580 with lower performance, 4GB less vram and 128 bit bus.  A round of applause for Nvidia",hardware,2025-07-03 06:56:28,389
Intel,n14594r,"GB207 being slower than AD107 is pathetic, what's the point of these x07 dies again? They're not thst mich smaller than recent x06 dies.  They're spaffing design cost on these barely different dues.",hardware,2025-07-03 12:22:36,30
Intel,n133i3r,"Now that we have a third-party review, it pretty much confirms what Inno3D said the other day, it's definitively slower than the RTX 4060 by about 5-7%.",hardware,2025-07-03 06:56:35,40
Intel,n13b3g4,The RTX 5050 is 2.5% slower than the Arc B580.   It's also a 50 series card that costs $250.,hardware,2025-07-03 08:09:36,34
Intel,n146i3h,> the system used a Ryzen 7 9800X3D  If the B580 only wins by 2.6% with this CPU then it's going to lose when you use something weaker because of that CPU overhead problem with Intel.,hardware,2025-07-03 12:30:30,31
Intel,n13bh59,It's actually surprising that it's that close to a 4060 considering the 5050 only has 2 GPCs as opposed to the 4060's 3,hardware,2025-07-03 08:13:18,16
Intel,n14m3pz,IDK but that performance is actually not bad. It should've just been cheaper.,hardware,2025-07-03 13:58:15,9
Intel,n13fsw1,The only positive about this is they're likely going to make a 5040 with a cut down chip that should pretty easily fit <75W.,hardware,2025-07-03 08:55:13,9
Intel,n17bpa4,"Honestly for people who don't need a lot of GPU power, not the worse.   I have two work computers, one with a 4090 and an old one with a 3090Ti.  These GPUs sit idle just taking up space, would have made more sense to get the something less performant.",hardware,2025-07-03 21:51:56,3
Intel,n13ktwe,272mm\^2 for B580 vs \~150 or less for RTX 5050 and perf within 5% lmao,hardware,2025-07-03 09:44:20,11
Intel,n150h4y,"I think to lose in sales, it would have had to lose a little more convincingly.  Yes, it's worse. But not enough to make up for features and brand-loyalty / nvidias massive reputation with gamers who are not into tech.  This thing will sell. It will sell A LOT. Because Average-Kevin who just wants to play Fortnite and some League will have a great time with it. His YT videos will get upscaling, his shitty mic-quality will get fixed (mostly) by Nvidia Broadcast... and he DGAF why we think it's the wrong move.",hardware,2025-07-03 15:08:37,2
Intel,n1t7zvj,"Does it actually fall behind the b580 or is it due to vram bottleneck in certain instances?  Either way, this should open up peopleâ€™s eyes about the b580. The b580 is just marginally above the 5050, which is a terrible product, with 4gb more vramâ€¦ even at its 250 dollar price point, itâ€™s never been a good deal. Not to mention how itâ€™s going for 400 these days.",hardware,2025-07-07 14:18:35,2
Intel,n148e4b,"People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.  Edit: r/hardware is dumb CPU overhead is a thing and budget GPU buyers need to know which of these two cards is effected the most by it. Everyone already knows that you are going to be GPU limited on top tier CPU's its not valuable information for a budget card.",hardware,2025-07-03 12:42:05,9
Intel,n13o0yi,Looks like people already forgot that testing the B580 with a top end CPU gives unrealistic results for actual budget buyers.  Every trick in the book for a nvidia sucks article.,hardware,2025-07-03 10:13:39,6
Intel,n13vnqm,Donâ€™t make Jensen sad,hardware,2025-07-03 11:16:41,2
Intel,n132z0y,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-07-03 06:51:39,1
Intel,n147qg1,Will it also fall behind them in the Steam hardware survey?,hardware,2025-07-03 12:38:08,1
Intel,n15wi0t,the 3060 is still a better value than this card bc its about same speed but 12gb vram and higher bus,hardware,2025-07-03 17:38:26,1
Intel,n16kpem,"Maybe a product for non-gamers who want/need multiple display setup, like me. Even then, if you bother to spend that amount of money for 5050, better add some more to get 5060, and if bother to get 5060 just better get 16GB one. Furthermore, you may feel why not add bit more to get 5070 series rather than 5060 16GB. That's how these things are priced.",hardware,2025-07-03 19:35:55,1
Intel,n1at07e,Value engineering to extract as much wealth as possible while unable to match performance conditions.,hardware,2025-07-04 13:00:03,1
Intel,nbicg0f,When you gimp the memory bus of course it is not as good,hardware,2025-08-30 15:52:42,1
Intel,n13okls,"No 75 watt gpu, pin connectior, yeah this is death of arrival.",hardware,2025-07-03 10:18:30,-3
Intel,n153l0f,"Trying to convince people to buy a 8gb card in 2025 should be illegal. Yet, people will line up to buy it because of the Nvidia logo...",hardware,2025-07-03 15:23:12,0
Intel,n135r6a,"It's a Nvidia Rx 7600, actually good recommendation over that card if it released for the same price in India. Rx 7600 is awful compared to rtx 5050 in pretty much everything.",hardware,2025-07-03 07:17:43,-11
Intel,n163uxd,Humiliation for the RTX 5050 is well deserved. It's good this gets curb stomped by the Intel B580.  More market share for Intel because AMD also introduced a useless 8 GB card.,hardware,2025-07-03 18:13:13,-1
Intel,n1481lj,">128 bit bus  Seems like this isn't the main issue with 5050's performance. 4060 has lowest memory bandwidth out of all 3, yet it's the fastest card. The die is just too cut down.",hardware,2025-07-03 12:39:59,46
Intel,n137c11,Unfortunately it will still sell like hotcakes in pre-builts to people who don't know much about computers.,hardware,2025-07-03 07:32:56,93
Intel,n13dk9q,"Their brand name is so strong that despite being worse in basically every way, including having poor drivers which was one of their major selling points in the past, they will still sell super well",hardware,2025-07-03 08:33:33,27
Intel,n139o7n,"I just looked at PCPartPicker and the cheapest B580 on there, in the US, is $299 (and the brand is Onix). The Intel Limited Edition is $340. If you can get the 5050 at MSRP it would have a decent price advantage.  I mean, I wouldn't get a 5050 but in the US at least, the $249 price for the B580 isn't real.",hardware,2025-07-03 07:55:44,32
Intel,n147v98,"Where I live none of the main sellers stock the B580, a round of applause for Intel.  The B580 isn't faster on the CPU's owned by the people who are in the market of a budget GPU.  People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.",hardware,2025-07-03 12:38:56,8
Intel,n13ev3s,Intel apparently hasn't shipped any GPUs last quarter. The shortage is already been seen as prices of b580 are going up.,hardware,2025-07-03 08:46:08,6
Intel,n13nzct,They're really pushing what they can get away with these days. Less VRAM and narrower bus but want the same money... bold strategy.,hardware,2025-07-03 10:13:15,1
Intel,n169a4v,"> with lower performance,  even on lower end cpu? Holy crap nvidia wtf",hardware,2025-07-03 18:39:52,1
Intel,n1521le,Because it will sell a shit load. Probably will be one of the highest selling Nvidia GPUs this generation,hardware,2025-07-03 15:15:58,19
Intel,n1sr0i1,"At these small die sizes, yields can be really high. High enough that if they wanted to have supply for a 5050, they might not have enough GB206 dies that can be cut down to 20/36 SMs.  The 5060M is probably catching the bulk of the bottom-level GB206 dies at 26/36.  There's also the memory difference, with the 5050 having gddr6 while the rest of the lineup uses gddr7. With dies this small, that cost difference in memory could actually make it worth taping out a new chip with controllers for the cheaper stuff. Gddr6 is dirt cheap at this point.  You could absolutely then argue that there's no reason for the 5050 to be so cut down that it needs a new, even smaller die, and that Nvidia can eat some margins on a budget card to give it better memory. And I'd agree. But I'm sadly not anybody who can make that decision.  Personally, I can't wait to see what becomes of the cut-down GB207s. The 5050 is a ""golden"" fully-enabled die. There will be some with only 18, or 16, or even fewer working SMs. Hell it could lose an entire GPC and be down to half-working. Those will be some absolutely sad little GPUs.",hardware,2025-07-07 12:41:55,3
Intel,n13lkxr,> It's also a 50 series card that costs $250.  I'll believe that when I see it in stock for 250. MSRP is meaningless if the product isn't available for that price.,hardware,2025-07-03 09:51:23,31
Intel,n13m4a0,And will be even worse since it's vram is 8gb. In many many games the b580 would Perform even better,hardware,2025-07-03 09:56:22,2
Intel,n14glix,Yeah it would be interesting to test with a more real-world CPU pairing.,hardware,2025-07-03 13:29:04,14
Intel,n151kyi,"Yeah, that was my question, did that ever get fixed? It seems like nobody remembers that anymore",hardware,2025-07-03 15:13:49,5
Intel,n152cbk,"TBH, they should have made THIS card a 75W SKU. It probably easily can be, without losing much performance. That would make it a very viable upgrade path for low-end systems. And kind of make the price more attractive, because it would require zero additional changes.",hardware,2025-07-03 15:17:22,7
Intel,n13qh8j,Margins go crazy,hardware,2025-07-03 10:35:09,18
Intel,n15brqh,4x the performance with MFG though.,hardware,2025-07-03 16:01:21,-1
Intel,n14yvrm,In mkst cases comparison would make sense since you testing gpu's not anything else. But when you toss in b580 complaints about using too powerfull cpu make much more sense,hardware,2025-07-03 15:01:03,1
Intel,n15bkf0,I have a system with a 9950x3d and a 1060. Iâ€™m thinking of getting a 5050 to go with it.,hardware,2025-07-03 16:00:24,-1
Intel,n13wkrr,> Every trick in the book for a nvidia sucks article.  That's just a standardised testing suite.   Someone will have low-end CPU testing once this gets any distribution.,hardware,2025-07-03 11:23:28,7
Intel,n13qrff,They're using the same cpu for the 5050 too. What a weird comment to make.,hardware,2025-07-03 10:37:36,1
Intel,n1t9745,"Donâ€™t forget how everyone is shitting on the 5050, calling it the worst nvidia card ever, but were praising intel non stop for the b580.  Even using purely stats from the article, b580 is 3% better with 4gb more VRAM than â€œworst gpu everâ€, so I donâ€™t see how this translates to a win for the b580. Being better than a terrible product by a hair doesnâ€™t exactly make it good.",hardware,2025-07-07 14:24:40,1
Intel,n13s2zk,Same can be said for the 5050 no? Both have CPU overheads unlike AMD which any mid-range AMD GPU can easily outperform Nvidia's 5090 at 1080p.   The 12GB VRAM however are better suited for people who are stuck at older Gen3/Gen4 PCIE speeds.,hardware,2025-07-03 10:48:35,-4
Intel,n18avh8,"It'll run nearly every game in existence at 1080p with playable framerates with the right settings. IDK why it would be a product for non-gamers. Calling it that means the B580, 4060 and 7600 are products for non-gamers, too.  Only game I could think of it wouldn't run properly would be oblivion remastered, which has terrible performance on anything.",hardware,2025-07-04 01:18:02,2
Intel,n14lclc,3050 released with the exact same specs and msrp. The 70w variant only came out 2 years after. The first variant was in top 10 of steam's hardware chart.,hardware,2025-07-03 13:54:24,10
Intel,n17ei31,"This is not a card for 1440p, as it would have issues with getting even 60 fps there, and in 1080p 8gb is mostly enough.  There is a difference between choking on VRAM with 80 base fps (5060ti) and 50 base fps (5050). Yes, second case is still annoying, but you might want to dial down the settings anyway, and then you would be fine.",hardware,2025-07-03 22:07:05,4
Intel,n139moa,Nvidia RX7600? That a new GPU? ðŸ˜‚,hardware,2025-07-03 07:55:19,2
Intel,n18a77s,"Funnily enough the 8gb 9060 XT is the best brand-new alternative at $300 and below, hands-down, and that includes the B580. It's x16 interface means it's less restricted by the vram compared to the 5060/Ti 8gb. The B580 still has overhead issues, VR issues, game issues and is rarely available at MSRP.  But people trash it anyway and praise the B580 without actually looking at its performance. In the benchmark linked by the OP alone it loses to the 4060 lol",hardware,2025-07-04 01:13:47,1
Intel,n14h57q,"Performance is fine I'm sure it'll perform just as good as the 3060 maybe slightly if you lower the power to 75W, call it a 5030 and sell it for $150. But these are different times and Nvidia doesn't do reasonable things like that anymore.  Bottom line, it's just the price and TDP that sucks.",hardware,2025-07-03 13:32:03,-1
Intel,n13vxvy,TBF a pre-built you're just looking at:   * What's the price?  * Does it have a graphics card or not?  * Overall package/size,hardware,2025-07-03 11:18:47,20
Intel,n15185y,"or even people who have a baseline knowledge, but think ""dlss and framegen"" will make this abomination better than its competition",hardware,2025-07-03 15:12:09,1
Intel,n14gfbw,"It's not so much the brand name but their near monopoly in the prebuilt market. There will be 1 prebuilt with a 9060XT 8 or 16GB for every 20 prebuilts with a 5050/5060. It's so bad that even the prebuilts with the 16GB 9060XT will be outsold by prebuilts with a 5050 at the same price range, as companies like to pair trash GPUs with ""high"" end CPUs like a 13700F to clear stock but the ""i7"" allows them to mark up the price.",hardware,2025-07-03 13:28:07,16
Intel,n13pika,"It was in stock at newegg monday for 3+ hours at $249.99, I picked one up after seeing a post that was 3 hours old on /r/buildapcsales   I've stopped into microcenter a couple of times and they said they get them, but I just haven't been lucky enough to be there when they were restocked.  EDIT: it is back in stock as of 11:40 EST at Newegg",hardware,2025-07-03 10:26:45,29
Intel,n13wagd,"In Germany you can find Arc B580 for â‚¬269-283 low-end, and B570 for â‚¬218-235.",hardware,2025-07-03 11:21:22,8
Intel,n13m7wm,Her in Canada they are $360 CAD. Cheap cheap. And ways to find. Might be a issue in your country perhaps,hardware,2025-07-03 09:57:18,9
Intel,n13rke5,"Went on Shopee and the lowest I could find is $280, which is not bad considering there's usually taxes which increases prices of GPUs by 10-20% from global USD MSRP.",hardware,2025-07-03 10:44:24,3
Intel,n14na3u,">the $249 price for the B580 isn't real.  Yes it is, it just gets sold out as soon as it's restocked - https://www.nowinstock.net/computers/videocards/intel/arcb580/",hardware,2025-07-03 14:04:18,2
Intel,n14edfd,Doesn't it just need a 7600?,hardware,2025-07-03 13:16:41,5
Intel,n13m638,Not in Canada. Prices are exactly as launch. Could be your countries problem,hardware,2025-07-03 09:56:50,13
Intel,n146tdx,"Eh, they recently shipped a new batch at MSRP to Newegg.  Donâ€™t know whatâ€™s going on with the partner cards. Probably the typical shenanigans",hardware,2025-07-03 12:32:28,2
Intel,n14t8qw,You forgot about the smaller dies.,hardware,2025-07-03 14:33:57,0
Intel,n151f6c,"The chip is tiny, so a single wafer -> more chips. The demands on power delivery, cooling etc. are nothing to speak of. And the GDDR6 memory is cheap and readily available.  If anything will ever be in stock, it's this card.",hardware,2025-07-03 15:13:04,8
Intel,n1krj48,5060 has been in stock at MSRP everyday since launch. This should be the same. Even the 5060ti 8gb and 5070 are at MSRP.,hardware,2025-07-06 03:12:56,5
Intel,n144vvx,MSRP has always been meaningless because it stand for suggested retail price.,hardware,2025-07-03 12:20:16,2
Intel,n151x7q,"But if the recent LTT video is halfway accurate, AMD would murder both of them in that scenario. The 9060 XT 8GB might want to have a word with those two cards.",hardware,2025-07-03 15:15:24,-3
Intel,n14n60t,negative margins for intel,hardware,2025-07-03 14:03:43,9
Intel,n15ha0f,The B580 showed we need to test in the systems the cards will end up being used in. The restricted PCIe lines on the lower tier cards i.e. only use 8 also means they need to be tested on older systems.,hardware,2025-07-03 16:27:40,7
Intel,n15gzg8,You really believe you are an average 9950x3D owner? Really?,hardware,2025-07-03 16:26:15,6
Intel,n19kdp4,"Out of curiosity, what is your use case in such a setup? I find myself cpu bottlenecked quite often on a 7800x3D but i use a 4070S, when i used 1070 GPU was bottlenecking me.",hardware,2025-07-04 06:45:36,1
Intel,n14a1ho,"But Intel Arc drivers have a really bad overhead, while nvidia does not, so in gpu bound games the 5050 will have around the same performance but the b580 will lose a lot of it, so it's not a weird comment, it's a very important point.",hardware,2025-07-03 12:51:45,26
Intel,n188xgl,>in 1080p 8gb is mostly enough.  The narrative was pushed so much that this fact became an unpopular opinion.,hardware,2025-07-04 01:05:47,4
Intel,n13aat8,nvidia Rx 7600 equivalent or Rx 7600 alternative by nvidia. Ig people can't even get a silly joke nowadays,hardware,2025-07-03 08:01:46,-8
Intel,n18u6nu,The 12 GB of RAM in the B580 will age more gracefully for a truly low end gamers. On purpose of 8 gb is retro gaming.,hardware,2025-07-04 03:21:07,0
Intel,n15lctf,"Hate to be that guy but to call this a 30-class card is insane. The 1030 was 25% the performance of the 1060, and the 1630 was around 45% the performance of the 1660. This card is 80% the performance of a 5060, which should probably be called the 5050.  To take it a step further, the GT 1030 was 13% of the 1080ti, the 5050 is 20% of the 5090. The 1050 was 25% of the 1080ti, so it would be reasonable to call this a 5040 or I could even be satisfied with 5040ti.",hardware,2025-07-03 16:47:05,36
Intel,n15k9s6,That would be an interesting card,hardware,2025-07-03 16:42:01,1
Intel,n14itr9,I think a big thing about prebuilts are the aesthetics. Does it look good? is the cables managed? Is the RGB controllable?  Slapping a PC together is easy but building something that looks beautiful through the side panel is not as easy.,hardware,2025-07-03 13:41:11,7
Intel,n1kqz69,The 4060 is 2% and DLSS did plenty of work for the even slower 3060. DLSS is going to be great for this card. Frame gen will be good in games this card can run at good base framerates.,hardware,2025-07-06 03:08:56,1
Intel,n15e4ii,"cool, it's in stock now",hardware,2025-07-03 16:12:35,1
Intel,n16ve1o,A 2-year old CPU ushering a new platform at the time of B580's launch. Far from a budget user's upgrade timeline-wise.,hardware,2025-07-03 20:28:40,5
Intel,n1881kg,"For a brand-new truly budget build, AM5 is still restrictive pricewise to people eyeing GPUs of this level. The 3050 and 6600 were well-known inclusions in $500 builds, building with a 7500F is going to cost $280-300 just for the cpu, mb and ram alone. AM4 or LGA1700 otoh can be as low as $180-200.",hardware,2025-07-04 01:00:16,2
Intel,n17vhk5,Depends on the game  The 7600 can see some serious performance deficits in some games like Spider Man  https://youtu.be/00GmwHIJuJY&t=521  and Hogwarts' minimum FPS is affected  https://youtu.be/00GmwHIJuJY&t=468,hardware,2025-07-03 23:42:58,1
Intel,n1454en,I mean tariffs in the US are a thing...,hardware,2025-07-03 12:21:46,6
Intel,n1595nj,"It wasn't always meaningless. Overall deviation from MSRP used to be smaller, and it wasn't hard to find the basic models at MSRP once upon a time.",hardware,2025-07-03 15:49:09,8
Intel,n15k42w,more like we need to test with different processors. Like with high-end to see absolute maximum and some lower grade to see how good/bad driver overhead is,hardware,2025-07-03 16:41:16,1
Intel,n16tjab,I never said I was,hardware,2025-07-03 20:19:36,2
Intel,n14c896,"I'm looking into this now, and you are correct. I am just reading this for the first time.",hardware,2025-07-03 13:04:21,10
Intel,n195igm,Lol no. For truly low-end gamers the B580 will perform worse because of its CPU overhead that still hasn't been fixed more than half a year after release.,hardware,2025-07-04 04:42:40,5
Intel,n4q5tdd,"I mean not really. The b580 is low end enough to the point where it'll become obsolete in performance long before it runs out of VRAM.  It's the equivalent of saying buying a 16gb 7600xt in 2025 is ""more futureproof"" than buying a 12gb 3060 in 2025.",hardware,2025-07-23 15:14:46,1
Intel,n53yfmn,an 8 GB card is obsolete at day one for modern games.,hardware,2025-07-25 16:12:47,1
Intel,n15s14a,"It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup. You can find those numbers on Gamer's nexus latest shrinkflation video on this topic (https://www.youtube.com/watch?v=caU0RG0mNHg). Considering those things, the 5060 is essentially a 5050 and the 5050 is a 5030.  The GT 730 had 13.3% as many cores as the full GK110 die, with a 50W TDP.  The GT 1030 had around 10.3% as many cores as the full GP102 die, with a 30W TDP though this one was impressive.  The GXT 1630 had around 11% as many cores as full TU102 die, with exactly 75W TDP.  The RTX 5050 has 10.7% as many cores as the full GB202 die (RTX PRO 6000 Blackwell), and around 11.7% vs the 5090 (which is the number you'll se on Gamer's Nexus video, he rounds it up to 12%, I think that was a mistake on his part? I'm not gonna try and correct him though), but it has a TDP of 130W somehow, just 15W less than the 5060, that's absurd.  Hence why I'm saying it SHOULD be a sub 75W TDP card, and priced way below that. My theory is that it was OC'ed past its efficiency curve to make it at least moderately better than the 3060 and still be able to call it a XX50 card.",hardware,2025-07-03 17:17:52,-1
Intel,n9995zt,Nah most people who buy prebuilts just want a good working PC with minimum effort,hardware,2025-08-17 22:53:54,1
Intel,n176oya,Yep. That's a problem for that country. Countries with normal leadership have much more viable options,hardware,2025-07-03 21:25:30,5
Intel,n17ibhv,Price is only driven by demand and supply. See how the crazy expensive GPU still sell like a hot cake during the COVID lockdown period?,hardware,2025-07-03 22:28:19,1
Intel,n14d3md,It's well known by Arc owners.,hardware,2025-07-03 13:09:23,10
Intel,n53mmej,"The 16gb 7600xtÂ  is a giant pile of crap even with 16 GB , so a 12 GB 3060 is the better choice.  If you go to techpowerup you'll see that the b580 is 17% in relative performance than the ""12 GB 3060 """,hardware,2025-07-25 15:18:15,1
Intel,n84aj5f,What if youâ€™re playing at 720p?,hardware,2025-08-11 14:46:01,1
Intel,n17rxfu,"> It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup.  That is a absolute dog shit methodology to use.   The 1080 Ti was 471 mmÂ², 5090 is 750 mmÂ².  Those two products are not the same tier. And can therefor not be used as equal reference points. When it comes to die size the top Pascal card is closer to the 5080 than 5090.",hardware,2025-07-03 23:22:30,10
Intel,n53r1fq,The point is more VRAM =/= aging better. The b580 having 12gb isn't saving it from becoming obsolete in a few years.,hardware,2025-07-25 15:38:30,1
Intel,n1glwbn,"They re not the same price tier either bro... They could have called the 5090 a 5095ti if that makes you happier.  The guy you quoted is right.   Also comparing the performance diff between the 1030 vs 1060 because guess what the 5060 should have been the 5050 looking at die size and memory bus.   Not sure why people keep defending these naming schemes, do you think the engineers use them internally lol? Its bullshit that marketing comes up with",hardware,2025-07-05 12:34:50,2
Intel,n17vpx6,"Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. ðŸ‘ Doesn't matter if die sizes get bigger, that's up to Nvidia. Just because the halo product got bigger doesn't mean they can get away with moving their product stack (below the XX90) one tier up in prices.   I guess it is a dog shit methodology to use, only Gamer's Nexus uses it and I'm sure #1 Nvidia apologist u/Alive_Worth_2032 knows more than him.   It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree.  And it's a shit methodology to use if you're an Nvidia executive.  If you can't see that Nvidia is putting all the effort into binning high end chips for AI cause it's 90% of their income, then I don't know what to tell you. Obviously it's a good move for them, and it works, but there's 0 reason for a gaming customer to defend them for it. We used to get much more out of their chips in the gaming cards for more reasonable prices, why's it wrong to want that?   You can't just draw a conclusion based on relative performance between 2 underpowered products, you compare them to the ""best"" Nvidia COULD give us (which is the full top die for each generation) and go down from there.  Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm^2. That's not even an excuse.",hardware,2025-07-03 23:44:19,0
Intel,n180tf6,"> It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree. And it's a shit methodology to use if you're an Nvidia executive.  You can chose one methodology.   You can decide on comparing what hardware you get at a certain price point.   Or you can ignore price/bom and look only at arbitrary model numbers as if they mean something.  You cannot do both at the same time.  Personally I prefer to look at die area. The 5050 today is roughly comparable to the 1050 Ti, comparing it to the top cards that are not comparable is irrelevant.  Pascal did not have a analogue for the 5090, period.  >Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. ðŸ‘  That changes nothing. The Pascal Titan are using the same die as the 1080 Ti. The whole tier of die that is used in today's consumer top SKUs and for the RTX 6000 Blackwell, DID NOT EXIST back then.  >Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm2. That's not even an excuse.  What are you even trying to say? Ampere also did not have a analogue to the 5090. The 3090 Ti, even it is sitting almost a tier below the 5090. The 2080 Ti and 5090 are unmatched in other generations.",hardware,2025-07-04 00:15:04,10
Intel,n18i4wk,"Youâ€™re still missing the point by focusing purely on die area and pretending model numbers are arbitrary. Model numbers mean something, nvidia knows that so thatâ€™s why they kept them consistent for so long (unlike AMD that's hella inconsistent). Because they rely on the perception of tier consistency from generation to generation, even if theyâ€™ve worsened the specs behind the scenes.  You say you â€œprefer to look at die area,â€ but thatâ€™s irrelevant unless youâ€™re building the chips yourself. Customers donâ€™t game on silicon real estate they game on actual performance and hardware capabilities. And the cuda core count + bandwidth vs top-die approach directly shows how much nvidia is offering relative to what they could offer if they weren't prioritizing AI margins.  So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  > Pascal did not have a 5090 analogue  Thatâ€™s just semantics. Every generation has had a full-fat top die, whether branded as â€œTitan,â€ â€œRTX 6000,â€ or whatever, which where I drew the comparison. Comparing lower-tier cards to the top die is how you reveal how much of the architecture's potential is being offered to gamers.  And yes, 3090 and 3090 Ti still preserved proportionally higher specs vs top-tier dies. What changed now is not just die sizes, itâ€™s NVIDIA reserving most of the silicon for AI and throwing scraps to gaming.  > You can choose one methodology...  Sure, and I chose one: compare the lowest-tier GPU to the top die, which accurately shows how nvidia has been shrinkflating consumer value over time. You're welcome to look at BOM and TDP too the 5050 still loses. It should be sub 75W and priced accordingly.  I think it's okay to call it an XX40 series card even if we keep the TDP at 75W but the XX30 series would need to disappear. What I don't agree with is using the die area excuse to justify shrinkflation just because the 5090 is such a halo product.",hardware,2025-07-04 02:03:25,-1
Intel,n18jqvu,"> So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  Do you even listen to your own madness?  You realize that what you are saying. Is that the lower end 5000 series would be ""better"". If Nvidia removed the 5090 entirely. And renamed the 5080 the 5080 Ti, and made the 5070 Ti into the new 5080.  Suddenly, you would praise the lower end cards for being ""better"". Purely because the high end is less powerful. Nothing else changed, no one is getting more hardware for their money.  Jesus Christ the mental gymnastics you people go trough.   The hardware Nvidia gives you for your money, is all that matters. How large the top die is and what core configuration it has. Is irrelevant for the value for cards further down the stack.",hardware,2025-07-04 02:13:15,9
Intel,mxmawxn,End of Life is not the correct term for this. End of manufacturing is,hardware,2025-06-13 19:40:01,129
Intel,mxm2d1j,Was about to shit on Intel for such a terrible product lifecycle time and how its GPU division was not going to do well if a GPU only has a ~2 years of updates until I read the article...,hardware,2025-06-13 18:57:57,108
Intel,mxmb1bk,"End of life typically means end of support, not end of manufacture, or am I wrong?  Anyhoo I blame the article for bad wording",hardware,2025-06-13 19:40:37,29
Intel,mxm07wf,"> This announcement marks the beginning of the end for a model that arrived just two and a half years ago, and it offers partners a clear timetable for winding down orders and shipments. Customers should mark June 27, 2025, as their final opportunity to submit discontinuance orders for the Arc A750.",hardware,2025-06-13 18:47:32,14
Intel,mxmcw6w,"looks like Intel didnt fire enough of its incompetent staff. EOL usually means end of support/drivers/Developmemt.  [IntelÂ® Arcâ„¢ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",hardware,2025-06-13 19:49:53,9
Intel,mxwxs3r,"Why did anyone buy these garbage Arc cards to begin with? The performance/$ was never ever ever ever ever not even for a single second better than comparable Nvidia and AMD cards.  I've never understood the point of the Arc cards. It was like a company in 2020 saying ""we're officially into console gaming and just created the PlayStation 2 for only $399!""",hardware,2025-06-15 14:35:14,2
Intel,mxsyrub,But did it really even live to begin with? ðŸ¤”,hardware,2025-06-14 21:08:51,1
Intel,mxm95g9,is this the shortest life cycle of recent GPUs? AMD was notorious for that... wasn't expecting Intel to top that....,hardware,2025-06-13 19:31:14,-6
Intel,mxpuga3,"So, for the distribution process this is also known as End-of-Sale. Since this is a B2C business, they can't really control when their resellers reach the end of their stock, but at that point Intel has stopped selling the product.  EoM and EoL are both dates that usually do not coincide with the End of Sales.",hardware,2025-06-14 10:09:38,10
Intel,mxmbvmp,"The article is poorly written, and the headline misleading.   The card is discontinued meaning no more orders will be accepted. It says nothing about software support.   Admittedly the miscommunication is Intel's fault because they specifically use EOL in their notification, but I also put this somewhat on the article writers because they didn't do a good job of clarifying that intel meant discontinued. They could have used more appropriate wording in the headline but instead chose to follow intel's lead likely knowing it would sow confusion, but lead to more clicks.",hardware,2025-06-13 19:44:48,116
Intel,mxm3esb,"Least likely reddit user behavior, actually reading the article probably puts you in the top 5 :)",hardware,2025-06-13 19:03:03,52
Intel,mxml6tk,"Don't worry, Iris Xe GPUs are still ""supported by the driver"", but their last fix was in 2023.  Intel doesn't notify when an architecture is actually dead, you're just left stranded for years until they make it finally official.",hardware,2025-06-13 20:31:06,12
Intel,mxmdlmm,"The incompetence source is actually intel themselves ...  [IntelÂ® Arcâ„¢ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",hardware,2025-06-13 19:53:22,21
Intel,mxn4bgv,> Anyhoo I blame the article for bad wording   Intel chose the wording in its own notice.,hardware,2025-06-13 22:09:46,9
Intel,nbpqm8r,"Bruh did you even read the article?  Btw it costs half of a 3060 and it outperforms it, no idea why you are saying it is a garbage product lmao shitty redditors don't know the difference between end of manufacturing and end of life/service.  The article is garbage, but it was talking about the end of manufacturing, not the end of the service/life, meaning it will still het driver support.",hardware,2025-08-31 19:30:36,1
Intel,mxm9ioh,"No because they are still supporting it, just ending manufacturing of new cards.",hardware,2025-06-13 19:33:05,22
Intel,mxn1rnm,"For a website that's predominantly text-based, a shocking amount of its users can't read for shit",hardware,2025-06-13 21:55:47,13
Intel,mxmkvoz,"The job of a journalist is to provide the translation and context for their readers, not copypasta and regurgitate headlines that laypeople will immediately misunderstand.",hardware,2025-06-13 20:29:35,28
Intel,mxmj7un,That's just for the hardware manufacturing.,hardware,2025-06-13 20:21:23,17
Intel,mxmaq6o,"ah, i thought it meant end of driver support.",hardware,2025-06-13 19:39:05,6
Intel,mxmevd8,We all did. unconventional use of EOL,hardware,2025-06-13 19:59:39,13
Intel,mxnw8ja,"Unfortunately itâ€™s been like that for decades with Intel, I can count numerous times in the past decade weâ€™ve had this same conversation about their processor lines being EOL.",hardware,2025-06-14 00:53:32,6
Intel,mxn4nhz,End of Sale,hardware,2025-06-13 22:11:38,6
Intel,mzf0gr9,"Tbf arc pro battlemage gpus are supposedly going to be fairly cheap, so its not that out of reach for consumers",hardware,2025-06-23 23:03:20,50
Intel,mzfz70f,"Imo, the lockdown of virtualization started before AI, as Nvidia and AMD didn't want people buying cheap GPUs to virtualize corporate environments.Â      At least that's the context that I'm most familiar with for this. A lot of small to midsize companies barely need the GPU for their individual VMs so being able to split a single GPU against a beefy CPU would be a good cost saver to avoid the professional GPU tax.",hardware,2025-06-24 02:23:09,30
Intel,mzf114m,"Intel has said that SR-IOV will come to consumer. Even if it doesnâ€™t, their GPUs arenâ€™t super expensive.",hardware,2025-06-23 23:06:27,28
Intel,mzg3xoe,"Splitting a GPU is so that you can run multiple VMs with each GPU. Any AI training and any inference on LLMs or similarly large models wants to run *one* application across many GPUs. There's nothing in common between those use cases. The market for GPU virtualization for remote desktop, etc. kind of stuff is still there, but it never was a big chunk of the datacenter GPU market and still isn't.",hardware,2025-06-24 02:51:39,19
Intel,mzf8eno,"Yep, I have to use Intel for my virtualization server because it's impossible to get anything to work the way I want on more powerful AMD APUs. I can easily have a VM using part of the iGPU while the host shares it.  Also kind of sad that a pretty old Intel iGPU still has more video transcoding features in QuickSync like HDR tone mapping and better stability than modern APUs.    I don't think it has anything to do with AI it's been like this  forever.  AMD is nice consumer hardware that they just don't provide the tools to use in any advanced capacity.",hardware,2025-06-23 23:46:58,8
Intel,mzezrml,"i thought nvidia grid got semi replaced by MIG. then again, MIG is only supported only on a specific subset of datacentre cards.",hardware,2025-06-23 22:59:26,3
Intel,mzg00hx,"Hyper-V supports GPU partitioning without the need for SR-IOV, which is almost perfect on nvidia cards (at least for what I need). It doesn't support VRAM partitioning though, so every VM has full access to all of the VRAM. It also disables checkpoints for the VM (or at least says it does, apparently there's a way around this), and doesn't support dynamic RAM allocation.  In my limited use of other brands of GPU, it has some infrequent but major bugs with AMD IGPUs and frequent but minor bugs with Intel IGPUs (I've never tried a dedicated card from either brand with Hyper-V).",hardware,2025-06-24 02:27:59,3
Intel,mzhzus4,"I think the future way around is to make the vm look like any other application from the gpu drivers point of view. On Linux (both host and guest for now) there exists virtio native, but for now it only works on Amd and is still in Beta and not yet really well documented how to get it working.",hardware,2025-06-24 12:15:24,3
Intel,mzf3egk,"I know AMD GPU passthrough is supported by HyperV virtualization and LXC containers without MxGPU, but that might be Linux-specific.",hardware,2025-06-23 23:19:19,4
Intel,mzfq3xm,"AMD appears to be heading towards allowing SR-IOV on consumer Radeon soon, but we'll see if it actually happens.  https://www.phoronix.com/news/AMD-GIM-Open-Source",hardware,2025-06-24 01:30:18,5
Intel,n01czo2,"I'm a bit late to the conversation, but I have more faith in Virtio-GPU Native Context landing than whatever the big three are planning  But VirtioGPU NC is, at the moment, Linux specific.",hardware,2025-06-27 09:48:18,1
Intel,mzfqyea,"I think the real question is availability and drivers. If it was like the initial Intel Arc GPUs launched in 1 laptop shop in S. Korea, good luck trying to expand market share.",hardware,2025-06-24 01:35:15,7
Intel,mzh6dpj,"Yeah, i'd reckon this mostly. Initially, more than a decade ago by now i guess, we were seeing this, with consumer grade GPU's being used or tried to power workloads on virtual desktops, RDS deployments and even research clusters.",hardware,2025-06-24 08:04:43,5
Intel,mzg1tp0,Surprised AMD just acted like Nvidia while Intel is the one actually doing this for so much GPU stuff.,hardware,2025-06-24 02:38:50,7
Intel,mzh08sa,The problem is they're not super available either,hardware,2025-06-24 07:04:06,2
Intel,mzf5byu,"PCIe Passthrough is not the same as GPU splitting. All 3 have support for PCIe Passthrough. I use it all of the time. The main goal of SR-IOV is to remove the need for multi-GPU setups in virtualization which from a market segmentation point would be detrimental to profits at the consumer level because a host may only need 1-4GB of VRAM and the rest can be allocated to the guests. This would be very impactful in ""Prosumer"" markets where someone may need more than 2 GPU outputs (typical for iGPUs) but may not want a multi-GPU setup as a multi-GPU setup has higher power draw.",hardware,2025-06-23 23:29:54,18
Intel,mzg0701,Intel Linux drivers are solid for everything except games,hardware,2025-06-24 02:29:04,3
Intel,mzgni4h,"Its an incredibly small market for this among consumers.  Most people asking for this is in a professional environment, and everyone wants to sell pro cards.",hardware,2025-06-24 05:10:46,18
Intel,mx1cb0m,TLDR:    Geomean: RX 9060 XT 16GB is 46% faster (mixed settings).     Raw performance closer to 30-40% faster.          Cheapest RX 9060 XT 16GB in EU is ~400 euros (VAT included)     Cheapest Arc B580 12GB in EU is ~294 euros (VAT included),hardware,2025-06-10 15:58:32,35
Intel,mx1kt2f,Does the B580 still suffer from the massive CPU overhead or was is ever fixed?  Considering that noone reasonable will pair something like a 9800X3D with a B580 something like a 7600 or even 5600 or 5700X3D this is something a lot of people would have to keep in mind when it comes significantly worse on the kind of CPUs you would actually see in a budget build.,hardware,2025-06-10 16:38:47,23
Intel,mx29c7r,average 58fps 45fps 1% lows with 9800x3d system will be unplayable for most people with zen3 era speeds.   Turn off rt- use fsr/xess/dlss + fg for 90fps+,hardware,2025-06-10 18:31:26,10
Intel,mx30dvt,"Since 9060 xt is $390, cheapest 5060 Ti 16gb at $430 might as well be included in this calculation",hardware,2025-06-10 20:39:15,11
Intel,mx5audi,I hope B770 delivers at a price that old-heads like me can appreciate,hardware,2025-06-11 04:36:47,3
Intel,mxrir8n,I own both. The 9060xt is better use of money.,hardware,2025-06-14 16:33:12,1
Intel,n5b0wbx,hot take   intel arc b580 is a better value then the 9060xt 16gb reason   your going entry level gpu   9060xt is a weird spot because if you can find at retail 9070 smashes it at 100 150$ more it makes it EXTREMELY HARD TO SUGGEST YES some people supposendly cant afford the extra 100 150 (although its very very likely not off the bad just a extra month of work or so) especially when your going entry you can argue fps is higher but my argument is 250$ for a 12gb gpu that competes properly against what its trying to compete against the 4060 while the 9060xt is a messy pricing and i also argue no one wants to give intel a realistic chance which is another issue if more people gave intel a chance i fondly believe it would be great for most people,hardware,2025-07-26 18:30:42,1
Intel,n9s1258,"Question is:Â  They have a deal in few days where i can get BF6 free with intel GPU. It's a game i would have bought anyway so it's like getting a 90$CA off the GPU priceÂ    Currently, i can get a Arc B580 for 360$CA which would mean an effective price of 270$CA. A 9060 XT, the cheapest comes at 490$CA (but it's on sale right now so the real cheapest would be 540$CA).   At this theorical price, i guess i can't go wrong with the intel one right? I will pair it with a Ryzen 7 5800x probably.   I'm currently running a Ryzen 7 1800x and a GTX 1070ti and playing at 1440p and i really don't care about running 4K ultra. BF6 was working incredibly well at low on my Nvidia so if graphics are higher i can only be happy but again, dont need ultra and fancy graphics that make real world look dull.Â    What's your thought people?",hardware,2025-08-20 20:44:46,1
Intel,mx2m202,"Prices differ between EU countries, in Germany the cheapest B580 is currently 270â‚¬ and 369â‚¬ for the 9060 XT.  For the US it is 310$/370$ currently.",hardware,2025-06-10 19:31:49,16
Intel,mx1d5bz,"Yes, but as he says in the video, there are some major outliers in the results due to things like Vram going past 12GB, and certain games just not liking Arc GPUs. Looks like a more ""typical result"" is 30% to 40% faster when not going over 12GB VRAM and not in a game that Arc has issues with. It's always hard to sum up the difference with a percentage, but especially hard with Arc GPUs, upscaler differences, etc.",hardware,2025-06-10 16:02:27,26
Intel,mx1ibks,"Australia pricing (AUD, GST incl.):   9060 XT 16GB: $629   B580 12GB: $449",hardware,2025-06-10 16:26:55,9
Intel,mx52fap,What's VAT?,hardware,2025-06-11 03:35:35,1
Intel,mx25eua,Never heard of any more news about this either,hardware,2025-06-10 18:13:02,9
Intel,mx1lidh,"Yes, but the conclusion was that even in this best case scenario for the B580 it offers worse value. So not looking good for the B580, especially when it's way above MSRP.",hardware,2025-06-10 16:42:08,21
Intel,mx2fuv9,I don't think it can be fixed. The card seems to have been designed to utilize extra CPU resources as much as it can.,hardware,2025-06-10 19:02:13,16
Intel,mxnt59g,"have a b580 and 9800x3d combo, and am perfectly happy with it. it even does some 4k gaming stuff just fine.",hardware,2025-06-14 00:34:48,1
Intel,mx29ssu,"As long as the zen3 era CPU can hit 58gps average and 46fps 1% lows in whatever game you are referring to, then it would make no difference vs having the 9800x3d. You only see a difference if the CPU frame times take longer to compute than the GPU frame times.",hardware,2025-06-10 18:33:38,1
Intel,mx5wf17,"Yeah, as someone trying to build a new PC for 1080p/1440p it makes no sense to go for the 9060xt since the 5060 Ti is only slightly more expensive and gets me the new DLSS, better ray tracing, etc. I guess for pure raster performance one could also include the 7800xt but it's not as ""future proof"" that's for sure.",hardware,2025-06-11 07:50:30,3
Intel,mxfi19s,"Got a 3080 ti for 550$ yesterday, man that hits hard for value against 7900 xt (750$) and 5060 ti   A 5060 ti, 4070 here costs 600 used.",hardware,2025-06-12 18:55:28,1
Intel,n2dal8f,260â‚¬ for the B580 in july...,hardware,2025-07-10 14:24:59,1
Intel,mx2qzeo,"Also, has Intel fixed the driver over head issue? People buying lower end gpus probably dont have top of the line CPUs to play with.",hardware,2025-06-10 19:55:27,12
Intel,mx5lvvg,Value Added tax. Think a better version of sales tax.,hardware,2025-06-11 06:09:55,1
Intel,mx1wp8z,"It's better then *any* 8 gig model, but that is the definition of damnation with faint praise.",hardware,2025-06-10 17:33:13,3
Intel,mx2aejk,"Hell no, the 9800x3d is doing a lot of heavy lifting with 1% lows and 0.1% lows. If you ran same tests with 3600-5700x. It wont be same experience at all especially on B580",hardware,2025-06-10 18:36:31,12
Intel,mx7xjln,dlss is only dealbreaker if they're extremely similarly priced otherwise I'm not really sold on the difference just for the features    especially where i live,hardware,2025-06-11 15:58:48,3
Intel,mx8d89v,"I don't consider a 25% price difference to be ""slightly"" more expensive.",hardware,2025-06-11 17:13:00,1
Intel,mxpjr2z,Also hits hard on the power draw,hardware,2025-06-14 08:21:05,2
Intel,mx3r68w,Is it fully a overhead issue or were tests done on CPUs that predate RE-BAR support? It looks like Arc is basically have RE-BAR and it performs close to expectation or its in the dumpster without it,hardware,2025-06-10 22:56:15,0
Intel,mx6a6s1,Oh,hardware,2025-06-11 10:07:04,1
Intel,mx25q1a,"debatable. 4060/5060, 9600xt on 3600x-5600x are faster than b580 on similar stepup",hardware,2025-06-10 18:14:29,16
Intel,mx2bmqq,"I'd rather have a 5060 ti 8GB than a B580 if you offered me them for the same price. It'd be way faster as long as I tune my settings. That being said, id never buy the 5060 ti 8GB.",hardware,2025-06-10 18:42:19,8
Intel,mx4s3ko,I would rather take the 5060 and play with lower textures than the b580 and be cpu bottlenecked with driver issues and no DLSS.,hardware,2025-06-11 02:28:26,3
Intel,mx2b549,"You still aren't saying which test you are referring to, so my answer has to say general rather than specific, but you seem to not fully understand how this works. If the CPU computes the frame faster than the GPU, then an even faster CPU will not improve the frame time. If in this particular test a lower end CPU could not compute the frame faster than the GPU, then the faster CPU is helping. But a faster CPU does not always make a difference. It only matters if the weaker one is actually limiting the performance, which is often not the case.",hardware,2025-06-10 18:40:00,-2
Intel,mx40jbk,"HUB tested very thoroughly and even did a follow up video about it because some russian tech site called HUB liars. TLDW of it is, yes rebar enabled, any game with decent CPU usage seriously hampered GPU performance, not all games but enough that it is something that anybody reviewing an Intel GPU should test for and advise the viewer about.",hardware,2025-06-10 23:47:39,15
Intel,mx86fb1,"Running a B580 with a 5700X3D, rebar enabled, tweaking a few settings, and it runs well for what I need it to do.  Basically iRacing and other sim racing titles.",hardware,2025-06-11 16:41:21,1
Intel,mx2cnqd,"Nah theres definitely a drop if you pair 9800x3d with b580 and 5600 & other gpus as well. You'll see lower average & 1% and 0.1% lows.  Setups with b580, 4060s, 9600xt could see averages of: 52fps 40fps 1% lows. Just turn off the rt, optimize and run fg for \~100fps",hardware,2025-06-10 18:47:12,2
Intel,mx2d7x5,"That is not true as a blanket statement. It is situationally true in the cases where the lower end CPU  can't keep up with the GPU frame times, but it is not true when it can.",hardware,2025-06-10 18:49:48,0
Intel,mx2fl7u,... its disingenuous to only test with 9800x3d on b580. Average person wont get those perf. Adequate step-up for me would've been a midrange cpu,hardware,2025-06-10 19:00:56,7
Intel,mx2gl9d,The conclusion was that the b580 is still worse performance and value even in this best case scenario,hardware,2025-06-10 19:05:44,1
Intel,mx2i9bj,Intel isnt serious about dgpu anyways. arc still has less than 0.15% marketshare on steam and the competitors are thriving...well only Nvidia. B580 probably 0.04%,hardware,2025-06-10 19:13:39,1
Intel,mt5nyr2,"I will say that Intel's naming is incredible. Now we have *Battlematrix*, what a badass name. Can we, as a society, *please* stop pretending that the rule of cool is ""cringe""?",hardware,2025-05-19 17:41:16,74
Intel,mt8ck76,700$ for the B60 is pretty cheap,hardware,2025-05-20 02:30:19,12
Intel,mt5olpn,"Intel badly needs a presence in enterprise or the data center, so these Pro Arc GPUs are to be expected.  Hopefully these cards can act as a lifeline for the struggling Arc division, but I hope that data center revenue doesnâ€™t compel Intel to abandon the consumer segment altogether.",hardware,2025-05-19 17:44:16,20
Intel,mt8miq9,The passively cooled one looks interesting.,hardware,2025-05-20 03:40:15,7
Intel,mt5btpw,"Original title: THIS is the Most Important GPU of 2025  Genuinely could be exciting stuff, both in terms of AI/pro use cases, and in terms of giving Intel's discrete GPU arm more sales / runway.  Also includes commitments from Intel towards the gaming side of GPUs, so all in all pretty good news, if this doesn't bomb.",hardware,2025-05-19 16:42:59,41
Intel,mta0joa,Interesting video downvoted to oblivion  GN spam upvoted endlessly  Make it make sense...,hardware,2025-05-20 11:25:20,17
Intel,mtc38v0,Itâ€™s nice that Intel will let people run the gaming drivers on these instead of artificially segmenting the consumer and professional products. Could see it being a cheap 24GB peopleâ€™s champion once games start requiring obscene amounts of VRAM and companies offload their old stuff.  I want to see how the 70w model compares to the 6GB 3050.,hardware,2025-05-20 18:04:32,2
Intel,mteku0z,The problem is it is a bit slow even compared with 5070 sigh,hardware,2025-05-21 02:05:33,1
Intel,mtvlu4c,"But think of all those nVidia cards that could have been sold to gamers instead being sold for AI...  And think about what will happen when there is a $500 24GB AI capable board the AI guys can buy instead!   Or a dual GPU 48GB!  There are many AI cases where the 48GB card is going to trounce any offering from nVidia, and every card Intel sells to AI folk is one more nVidia card available to gamers.   So whether or not you are personally interested in buying a B60, it IS going to be a crazy important card to gamers because this is going to permanently shift the supply/demand for nVidia cards as well as force nVidia to start increasing vram to compete.  Bonus, because Intel has their own foundries, the upcoming celestial line will not be competing with nVidia for foundry time, so this will also increase the total number of video cards produced!",hardware,2025-05-23 18:12:43,1
Intel,muoy0wo,"This is legit, I can see a lot of companies adopting this to use for local AI. Now if only Ollama IPEX can update their support and not lag behind for Intel ARC graphics, compared to the regular build for AMD and Nvidia.",hardware,2025-05-28 13:01:36,1
Intel,mxakky4,I will be watching out for B50 though.,hardware,2025-06-11 23:51:22,1
Intel,mt5cn8v,"Great troll clickbait title tbh, the same day that 5060 hits shelves. People will of course think this is a 5060 review and LTT knelt the knee on Nvidia if they don't watch the video.",hardware,2025-05-19 16:47:02,-33
Intel,mtbffx9,TL:DR ITS ALL DATACENTER GAMERS MOVE ALONG.,hardware,2025-05-20 16:10:32,0
Intel,mt6bkc8,"No, we gotta have AI Pro Max Ultra+   /s",hardware,2025-05-19 19:37:00,47
Intel,mt70lbr,I still feel their architectures called after D&D things is so cool.,hardware,2025-05-19 21:43:09,16
Intel,mt86tqz,I'm not a fan of *Sparkle*,hardware,2025-05-20 01:53:51,3
Intel,mt6lxvx,"What data center revenue, they have pretty much none. Their roadmap also slid, they won't have anything competitive for data center till 2027.",hardware,2025-05-19 20:28:50,5
Intel,mtn2amf,"Why is Arc 'struggling'? Every release so far, going back to the first one, have sold out very quickly. B580s haven't been in retailer stock much at all since it came out, until the new AMD GPUs that recently released. But, whether it's scalpers or consumers, Arc never stays in stock for long.",hardware,2025-05-22 11:57:12,2
Intel,mt601wk,"I mean I think that's what the dGPU for Intel is changing for Xe3P and onwards; Intel's fabs producing these GPUs which make money with AI/Workstation. I mean the original Xe produced on TSMC under Raja is certainly gone but feels like they've re-calibated with Xe3P quite a bit.  Honestly if Xe3P performs \*very\* well in labs I'd do a N44>N48 style die upscaling of a probable 256-bit part they're working on and just double the spec of it. If it still struggles in gaming Vs RTX 60 & UDNA/RDNA5/GFX14 PPA wise then they can just reuse said dies for AI easily. Especially could fill in the gap left by Falcon Shores to an extent.  An ""affordable"" 96GB Clamshell card for AI/Professional Vs Nvidia sounds pretty tempting.",hardware,2025-05-19 18:39:44,5
Intel,mtfh7k0,"> Original title: THIS is the Most Important GPU of 2025  seeing a title like that would instantly get me to click ""not interested"" in youtube.",hardware,2025-05-21 06:04:23,1
Intel,mtn1uxy,It's Reddit. How often does it really make sense? XD,hardware,2025-05-22 11:54:15,3
Intel,mur9wzh,I NEED to see this in gaming xD  I don't think gamers should be walled off from having a GPU that can do ai stuff and gaming as well. The GPU should be able to do both IMO,hardware,2025-05-28 19:50:44,1
Intel,mt7xyra,No one will actually think that 5060 is important,hardware,2025-05-20 00:58:52,17
Intel,mt7klij,No they won't?,hardware,2025-05-19 23:37:46,11
Intel,mti2kqi,"not all datacenter, but prosumers and businesses that need to leverage stuff for workstations also have a space here. plus, while nowhere near as performant as a full fledged gaming GPU, i bet the B60 with gaming drivers can be a respectable gaming card for those off work hours gaming sessions.",hardware,2025-05-21 16:50:10,2
Intel,mt5c5ii,"It was clickbait, I editorialized the best I could. Although I do kind of buy their reasoning for why it _could_ be a very significant GPU overall - if it lands well.",hardware,2025-05-19 16:44:37,12
Intel,mt7r22d,Where's my Chaotic Neutral Rogue GPU Intel?  WHERE!?,hardware,2025-05-20 00:16:20,5
Intel,muysqk8,Sparkle is an AIB who makes GPU boards. Intel didn't pick that name.,hardware,2025-05-29 22:25:09,2
Intel,mtqrh4t,Their first generation had some issues and was not great even for the low price but in the past year they have been killing it in the budget card space. Its really nice to have some options on the lower end again.,hardware,2025-05-22 23:17:55,2
Intel,mti3ac2,"VR development could be an interesting topic here, assuming one GPU die can not access the other GPU's VRAM, it could still be a decent option for development of VR games on the internal testing front. VR is very much staying, along with AI so i can see this being a nice tool for both, just not all at once.",hardware,2025-05-21 16:53:31,1
Intel,mtfh9nf,you underestimate human stupidity and overestimate their ability to read.,hardware,2025-05-21 06:04:57,1
Intel,mtfh2of,Youll have to wait for R generation for the Rogue. Alignment optional.,hardware,2025-05-21 06:03:05,1
Intel,mthkmay,Chaotic Neural Rogue.,hardware,2025-05-21 15:24:05,1
Intel,muz3jnb,"I see, ya they do have cool codenames. Also TIL Intel Arc GPUs are named after character classes from D&D. Alchemist and Battlemage for example, no idea where Battlematrix comes from though",hardware,2025-05-29 23:24:21,1
Intel,mtqzhgz,"$299 and I can do Forza high settings 120fps at 1440p? Sold. ...don't care about RT, I turn it off anyway.",hardware,2025-05-23 00:04:10,2
Intel,mtpd95g,I'm just trying to find an affordable card that will run 4k vr at 60 to 90 fps without any dlss; so many cards don't have enough vram for vr. Excited to see if the B60 will be able to do this.,hardware,2025-05-22 18:58:57,1
Intel,mt3p68x,below $1000 USD? Sounds amazing value for those that want 48GB of VRAM and BMG okay for that kind of stuff?   One way to recuperate any losses on B580s in the dGPU division.,hardware,2025-05-19 11:06:34,74
Intel,mt3qhe8,"sure, whatever. welcome back sli  edit: I KNOW ITS NOT SLI",hardware,2025-05-19 11:17:07,59
Intel,mt3z25l,i wonder how well will this perform for LLMs,hardware,2025-05-19 12:20:02,14
Intel,mt3o8fn,"Nice to finally see BMG X2 in the wild. We need more weird GPUs like this, now more than ever.",hardware,2025-05-19 10:58:49,39
Intel,mt3sgmm,"I'm in, I would buy these.  Bring back big cases with big stacks of GPUs.  Let's do this.",hardware,2025-05-19 11:32:30,17
Intel,mt3qh7t,"The shroud design oddly reminds me of 9800GX2.   But of course, the GX2 had a... 'sandwich' form factor with dual PCBs!  The late 2000s were such a great time to be a nerdy teenager. Technology was still trying to find its footing and everyone seemed to be experimenting with different ideas. We had weird smartphones, weird GPUs (with CGI mascots), and even weirder CPU coolers (Thermaltake SpinQ, Cooler Master Mars/Eclipse, anyone?).  Everything just feels *too* mainstream and 'serious' nowadays... but I digress.",hardware,2025-05-19 11:17:04,9
Intel,mt3wtq3,Am I dreaming a dual GPU in 2025.,hardware,2025-05-19 12:04:28,8
Intel,mt4bedr,he's wearing the shirt to computex lmao,hardware,2025-05-19 13:36:44,3
Intel,mt6fpfz,"From the article I read, it does the processing work of handing out the data from both gpus in the video card itself.Â  Â Kind of cool if it's fast enough.",hardware,2025-05-19 19:57:45,3
Intel,mt82s1j,Only 2 words  Fuck Nvidia!,hardware,2025-05-20 01:28:50,3
Intel,mt4e2fy,Can you buy a single one or do you have to buy it in Battlematrix form like the other B60?,hardware,2025-05-19 13:51:35,2
Intel,mt73zxl,"Finally something for modern day workstations. If you are working on a desktop these days, the most important thing is access to AI models.   With these, you can easily buy 2-4 B60s and throw them in your machine to not be reliant on external services all the time. Could be a real productivity booster.",hardware,2025-05-19 22:01:45,2
Intel,mt44olc,Is that Richard Stallman,hardware,2025-05-19 12:56:20,2
Intel,mt47xk7,This would be perfect for lossless scaling multi gpu framegen.,hardware,2025-05-19 13:16:15,2
Intel,mt3vzag,Steve trying a mandarin ad. Cute I say...,hardware,2025-05-19 11:58:24,0
Intel,mt55gmy,"This is awesome, but... are we ever going to get a B770?",hardware,2025-05-19 16:11:19,1
Intel,mt9vrqk,Dual gpu. 2012 amd...is that you?,hardware,2025-05-20 10:47:23,1
Intel,mtaoeg1,"Hey, dumb question: can a motherboard with 4 pcie slot runs 4 of this card?  That would make 48\*4=192gb, pretty doable for really large langu model.",hardware,2025-05-20 13:56:25,1
Intel,mt9d6ot,I remember when he said that Intel were scumbags  did he change his mind?,hardware,2025-05-20 07:38:14,-1
Intel,mt8m7ms,Donâ€™t get fooled by 48GB. The card actually behaves as 2 cards of 24GB with pcie x8 lanes each of gen 5.0 in bifurcation mode. So for gaming most likely you will be getting only 24GB of vram (which is still not bad). But this card really shines in AI or other professional loads because they are claiming that you can have one big shared memory across multiple cards with their drivers and without any physical connectors like crossfire or sli. This means you can run full fat deepseek completely locally on your physical system with something like 4 of B60 on a threadripper system. God damm never even imagined that intel would be shining in GPU segment.,hardware,2025-05-20 03:37:59,0
Intel,mtgsefk,this technology is from ASUS Mars II 3 GB Dual GTX 580 maybe from 14 years ago?,hardware,2025-05-21 12:59:05,0
Intel,mt4jksj,they wont sell these discretely [source](https://www.youtube.com/watch?v=F_Oq5NTR6Sk),hardware,2025-05-19 14:20:57,12
Intel,mt46q8d,no software support + half of the memory bandwidth of the 5 year old 3090,hardware,2025-05-19 13:08:59,-36
Intel,mt45axy,"Sli never left... Nvidia just hide it away from customers and sell it as nvlink (with some upgrades) for high end component for commercial side because they didn't want people stacking their cheaper cards.    Edit: got it, seems like I misunderstood what Sli does with the vram as it didn't pool.",hardware,2025-05-19 13:00:08,37
Intel,mt60i3o,Unfortunately this is just a way to get more GPUs into one system. Thereâ€™s nothing like SLI.,hardware,2025-05-19 18:41:56,3
Intel,mt4b6t6,No need for SLI to use multiple GPUs.,hardware,2025-05-19 13:35:31,6
Intel,mt6s2dz,"no this particular card does not support sli. this is just to increase the gpu density in a given system. Increased from 1 gpu per slot to 2 gpus per slot.   According to linux tech tips, the memory sharing between cards are done on the software level, which in my opinion is not a good thing. the pci-e gen5 x8 bus has a memory bandwith of 32GB/s in one direction (latest nvlink support up to 900GB in one direction). More inter-card bandwidth = better performance on large models (assuming the model support multi-gpu setup). No dedicated inter-card interface certainly will harm the maximum theoratical performance in certain workloads.",hardware,2025-05-19 20:58:50,1
Intel,mt4qknj,"Me too. Based on the TOPS, the inference speed seems to be roughly comparably to a 3060TI for the B60 and the Dual being roughly comparable to a 4080, though i think realistically more like a 3080/4060 due to the bandwiths. I've run a good bit of local inference on a 3060 12GB, and while it isn't hyper-performant, the speed is slower but ""acceptably slow"" in my opinion. You won't be waiting 10 seconds per token, but it's also not going to be spitting out a novel per minute. For me, that extra memory is really what matters because it mean less quantization at roughly the same speeds I'm used to, which means less errors in the output. I'm really more hopeful that this will finally light a fire under the NVIDIA execs who thought punishing consumers with less memory to prevent corporate customers/datacenters from using consumer cards in their servers was the right move. You can see they are reversing that somewhat with the 5090, but they can kick rocks with that 3k\* price tag that is once-again intended to get more money from data centers who decide to try to use them. It's pure market manipulation and artificial deflation of the specs to drive pro customers to more expensive hardware at consumer's burden.",hardware,2025-05-19 14:56:57,7
Intel,mt4t7jk,"Tokens/second won't be as good as a high end Nvidia card obviously, but you'll be able to fit a much larger model onto it than you could for any consumer and almost any Nvidia card. And for $1000, you could run four of these and still come out way cheaper than one A6000.",hardware,2025-05-19 15:10:11,12
Intel,mt9ngx3,"Since I've run a bunch of tests on Xe2 (and of course plenty of Nvidia and AMD chips):  * A 70B Q4 dense model is about 40GB. w/ f16 kvcache, You should expect to fit 16-20K of context (depends on tokenizer, overhead etc) w/ 48GB of VRAM. * B60 has 456GB/s of MBW. At 80% (this would be excellent) MBW efficiency, you'd expect a maximum of 9 tok/s for token generation (a little less than 7 words/s. Avg reading speed is 5 words/s, just as a point of reference most models from commercial providers output at 100 tok/s+ * For processing, based on CU count each B60 die should have about ~~30~~ 100 FP16 TFLOPS (double FP8/INT8) but it's tough to say exactly how it'd perform for inference (for layer splitting you usually don't get a benefit - you could do tensor spliting, but you might lose perf if you hit bus bottlenecks). I wouldn't bet on it processing a 70B model faster than 200 tok/s though (fine for short context, but slower as it gets longer.  Like for Strix Halo, I think it'd do best for MoE's but there's not much at the 30GB or so size (if you have 2X, I'd Llama 4 Scout Q4 (58GB) might be interesting once there are better tuned versions.",hardware,2025-05-20 09:28:26,2
Intel,mt47kk7,"Reminds me of the old days when brands would try weird stuff to see what sticks, or just for the market data/rnd.  Kudos for Intel for trying something a bit different.",hardware,2025-05-19 13:14:05,12
Intel,mt62mon,Thankfully not,hardware,2025-05-19 18:52:17,5
Intel,mvkxiec,"yes, they are even showing servers with 4 of those [https://geeksynk.com/intel-rises-all-upcoming-intel-high-vram-gpus-with-their-specs/](https://geeksynk.com/intel-rises-all-upcoming-intel-high-vram-gpus-with-their-specs/)",hardware,2025-06-02 12:58:21,1
Intel,mt9d0l7,â€œ24gbs of VRAM is not bad for gamingâ€   Itâ€™s the bare minimum these days,hardware,2025-05-20 07:36:24,0
Intel,mt48dom,If it has Vulkan it can be used for inference,hardware,2025-05-19 13:18:54,25
Intel,mt4ayzt,It has all the software stack it needs and works great.,hardware,2025-05-19 13:34:17,18
Intel,mt5cuy1,Good luck fitting 48GB of anything into a single 3090 VRAM.,hardware,2025-05-19 16:48:04,9
Intel,mt4g116,OpenVINO is fully supported on Arc discrete graphics and supports both Windows and Linux.,hardware,2025-05-19 14:02:15,8
Intel,mtdfrpa,"Man you don't know what are even talking about, these GPUs are one 192 bits per unit combined you will get 384 bit through 2 GPUs and the 3090 well it's half the amount of VRAM and it is powerful but in workstation use it is not the only thing you need, and software support, there is not support for 50 Series cards on many applications like premiere, Da Vinci and most of the work based applications and the Game ready drivers are more than capable enough to run LLMs on Windows and Linux",hardware,2025-05-20 22:06:58,1
Intel,mt56snm,"Tbh that is probably for the best. Sli was never that great for gaming, all that leaving sli open as an option would bring is greater AI demand for consumer cards.",hardware,2025-05-19 16:17:58,5
Intel,mt86nbc,"SLI, as implemented in the later years of its existence, has the same latency cost as framegen, with worse variation.  With how much rage there is about framgen... can you imagine how the public would respond if they had to buy two GPUs to get it?",hardware,2025-05-20 01:52:44,2
Intel,mt9cdul,">because they didn't want people stacking their cheaper cards.  because theyâ€™re evil and greedy. No other reason, right? ðŸ™„",hardware,2025-05-20 07:29:39,0
Intel,mt4t8b6,starting to find out why people use /s on this website,hardware,2025-05-19 15:10:17,6
Intel,mt6sywx,"According to linus tech tips, the memory sharing between cards are done on the software level. The pci-e gen5 x8 bus has a memory bandwith of 32GB/s in one direction (latest nvlink support up to 900GB in one direction). No dedicated inter-card interface certainly will harm the maximum theoratical performance in certain workloads.  Also, according to linus the purpose of this 2 gpus on 1 board is just to increase the server density, and nothing to do with bandwith. Each gpu is connected to host via pci-e 5.0 x8 interface, and the system will seperate them via pci-e bifurcation.",hardware,2025-05-19 21:03:18,3
Intel,mt6uqg3,"Pretty disappointing tbh. This card will not be a nvidia replacement by anytime soon. I think the cheapest option to run LLM locally is to buy decomissioned nvidia tesla v100 16G SXM2. Each cost around 60-70 USD and availability is pretty good atleast in China. And pair with SMX2 to pci-e conversion board.  6 of them can be linked together using nvlink, providing 96GB vram. The downside though, is this setup will drain 1.8kw of power when maxed. And the HBM2 vram inside the core is very prune to failure due to old age. The embedded vram can't be repaired, so once vram died whole gpu is pretty much gone.",hardware,2025-05-19 21:12:18,1
Intel,mt9omnj,between AMD and Intel which is more stable?,hardware,2025-05-20 09:40:35,1
Intel,mtdpdw2,"Intel's official figure is 192 INT8 TOPS. I guess this is with sparsity, so 96. Then FP16 should be 48 TFLOPS (or 4x the FP32 perf).   So essentially, a 3060 with 24GB VRAM and 25% higher bandwidth (conveniently available in a dual-gpu version for a 48GB total).",hardware,2025-05-20 23:00:51,1
Intel,mvky195,Thanks!!!,hardware,2025-06-02 13:01:29,1
Intel,mtdb0s1,Thatâ€™s true and product segment itself focuses on AI. But the fun part is bifurcated dual GPUs. And unlike Nvidia they are claiming to have support for both game-ready and professional drivers at same time. This means you can be professional video editor by day and gamer by night (or gamer by weekends or vice-versa ðŸ˜„) all with one card without tinkering drivers or monitor ports. And by the way unlike AMD they have great support for video encoding and decoding. I am really excited to see all these various combinations of things on this card.,hardware,2025-05-20 21:41:39,1
Intel,mtdbqb6,Yeah 24GB has become bare specially for 4K and above. This is good intel has hit a good sweet spot each GPU. ðŸ˜—,hardware,2025-05-20 21:45:23,1
Intel,muh28rk,source??,hardware,2025-05-27 06:28:34,1
Intel,mt4b2jt,Sure but thereâ€™s no Vulkan needed.,hardware,2025-05-19 13:34:51,3
Intel,mt5x5v4,"this isn't a single card. it's two mid cards on one PCB, you are still splitting vram across two GPU's",hardware,2025-05-19 18:25:26,0
Intel,mt58gaa,"It's an option...? Meaning that you don't have to use it if you don't like it.      And plenty of people use their commercial cards for gaming, literally some of earliest adaptors of nvidia cards were chief tech leads playing games on their workstations in afterhours, as described in many interviews and books such as the recent The Nvidia Way.   And Sli was fine once they made G-Sync, which was from the discovery of microstutters as a big problem.",hardware,2025-05-19 16:26:12,3
Intel,mt4j25h,"Yes, that's why I said:    >...sell it as nvlink (with some upgrades)...     Also, I bet that most people would gladly take just the standard sli without the nvlink improvements simply for the larger vram.    Edit: I was wrong, see below. Cunningham's Law",hardware,2025-05-19 14:18:17,12
Intel,mt9r3r7,"The question is less about stability and more about support.  AMD's ROCm support is basically on a per-chip basis. If you have gfx1100 (navi31) on Linux you're basically have good (not perfect) support and most things work (especially over the past year - bitsandbytes, AOTriton, even CK now works. I'd say for AI/ML (beyond inferencing) I'd almost certainly pick AMD over Intel w/ gfx1100 for the stuff I do. If you're using any other AMD consumer hardware, especially on the APUs then you're in for a wild ride. I am poking around with Strix Halo atm and the pain is real. Most of the work that's been done for PyTorch enablement is by two community members.  Personally I've been really impressed by Intel's IPEX-LLM team. They're super responsive and when I ran into a bug, they fixed it over the weekend and had it in their next weekly release. That being said, while their velocity is awesome, that causes a lot of bitrot/turnover in the code. The stuff I've touched that hasn't been updated in a year usually tends to be broken. Also, while there is Vulkan/SYCL backends in llama.cpp that work with Arc, you will by far get the best performance from the IPEX-LLM backend, which is forked from mainline (so therefore always behind on features/model support). IMO it'd be a big win if they could figure out how to get the IPEX backend upstreamed.  I think the real question you should ask is what price point and hardware class are you looking for and what kind of support do you need (if you just need llama.cpp to run, then either is fine, tbt).",hardware,2025-05-20 10:05:09,2
Intel,mtf85b9,"Hmm re-reading, I may have brain-farted the CU math, Arc 140V (Lunar Lake) is I believe 32 TFLOPS so obvs G21 should be higher.  B60 ([official specs](https://www.intel.com/content/www/us/en/products/sku/243916/intel-arc-pro-b60-graphics/specifications.html)) uses the full BGM-G21 which has 20 Xe2 cores, 160 XMX engines and a graphics clock of 2.4GHz (a bit lower than B580).  Each Xe2 core can support 2048 FP16 ops/clock ([Intel Xe2 PDF](https://cdrdv2-public.intel.com/824434/2024_Intel_Tech%20Tour%20TW_Xe2%20and%20Lunar%20Lakes%20GPU.pdf)).  ``` 20 CU * 2048 FP16 ops/clock/CU * 2.4e9 clock / 1e12 = 98.304 FP16 TFLOPS ```  This lines up if Intel is claiming 192 INT8 TOPS (afaik XMX doesn't do sparsity and they claim 4096 INT8 ops/clock, so double FP16/BF16).  These cards seem super cool! My main bone to pick is that the retail plans (uncertain retail release in Q4) makes it less interesting. I guess we'll see what else hits the shelves between now and then.",hardware,2025-05-21 04:45:34,1
Intel,mt6soma,But you get 48GB of VRAM for that price.,hardware,2025-05-19 21:01:53,6
Intel,mt7v4ic,"Intel has a currently working but not released software to share video memory across multiple GPUs for the purpose of AI and such with minimal performance loss. It is effectively 48GB worth of video memory for the tasks these GPUs are designed for.  These cards might be slower than other solutions, but they will be thousands or even tens of thousands of dollars cheaper. Stuff like this (and AMD's AI MAX+ 395) allow companies to have each dev be able to host their own full size(or at least near full size) AI model on their own workstation instead of fighting over time on the big server. They also allow hobbyist to work with much larger models than they usually would be able to.",hardware,2025-05-20 00:41:23,2
Intel,mt5gfjx,"The nvlink improvements are what *allow* for larger vram.  SLI's biggest problem was that GPU memory had to be duplicated across all cards, meaning if you put 2x 8gb cards in SLI, you still only had 8GB effective vram. NVLink allows for memory pooling, SLI did not.",hardware,2025-05-19 17:05:16,19
Intel,mt9ukj8,thank you,hardware,2025-05-20 10:37:11,2
Intel,mtffhpo,"If they really have 98 FP16 TFlops (i.e., 70% of a 3090), they will be pretty cool and better value than a heavily used 3090 (if we ignore the CUDA advantage)",hardware,2025-05-21 05:48:39,1
Intel,mt7fghz,with the memory bandwidth of a 5060... too slow for dense models and not enough memory for large MoE models,hardware,2025-05-19 23:08:05,0
Intel,mt9hfqt,"Unless they have some custom interconnect, it will be slow (PCIe 5.0 x8 tops out at 32GB/s)",hardware,2025-05-20 08:23:40,1
Intel,mt8bf5d,"Thx, TIL",hardware,2025-05-20 02:22:54,2
Intel,mt8tvnl,"Faster tham RAM, which is what you'd need to use with single 3090 when you go above 24GB.",hardware,2025-05-20 04:37:52,2
Intel,mtde4us,"What the hell, even a B580 has 192 Bit bus compared to 5060's 128 bit bus, and when it is combined you are getting aboout 384 bit bus accross the 2 GPUs and they are not pooled but you won't get that much VRAM even on used markets",hardware,2025-05-20 21:58:05,1
Intel,mw3qvlq,Thatâ€™s about the bandwidth of the Nvlink bridge on a 2080(/ti) (which obviously doesnâ€™t do vram sharing),hardware,2025-06-05 09:00:51,1
Intel,mt8x3pu,Then just get Strix Halo if you donâ€™t care about bandwidth and performance,hardware,2025-05-20 05:04:35,0
Intel,mt8z3xz,"Seems like I have to go through this again, but with you this time. So, again. For the price of Strix Halo, if you actually use the memory, you are going to get much higher performance than a system with better GPU but less VRAM, because you'd run out of it and had to go for RAM. Do you guys just not understand that if you are going to use the VRAM, it will have much better performance than something that would run out of it and had to use RAM for the same price?",hardware,2025-05-20 05:22:03,3
Intel,mt3mg6t,"Disappointing that we're not seeing the B770 in computex. Guess we'll have to ""stay tuned"" until intel decides to release or scrap the B770  The Arc Pro B50 is a B570 with 16gb of VRAM on a 128bit bus. It has a $299 msrp  But we DO get the Arc Pro B60 which has a single 24gb GPU model and a dual GPU B580 which has 48gb of VRAM shared between both GPU's   Intel designed it's software so that you can scale up workloads across multiple Battlemage GPU's up to 8. Intel calls this Battlematrix.  A fully equipped Battlematrix setup would have across 8 GPU's or 4 of the dual gpu B60 cards: 192gb of VRAM, 1280 XMX Engines and 160 Xe cores. This setup comes with what I assume is a Granite Rapids Xeon CPU and MSRP range from $5000-$10000. It includes a customized linux distro with software and frameworks optimized for AI workloads.  Unfortunately, Battlematrix workstations are the ONLY way to get an Arc Pro B60 as they're not selling them as individual cards yet.   EDIT: You can actually buy the Arc Pro B60 individually in Q1 2026.",hardware,2025-05-19 10:43:45,96
Intel,mt3oixr,Iâ€™m just happy Intel is still thinking about GPUs at this point.,hardware,2025-05-19 11:01:13,34
Intel,mt3nf8r,"How likely are these to support SR-IOV?  I want to buy one for GPU Virtualization. I am willing to go through my work to buy one, I don't care if they're not going to be available at retail. Hell I want the 48GB model GN just tore down.",hardware,2025-05-19 10:52:07,15
Intel,mt3md7c,Do I still need to stay tuned for a possible B770?,hardware,2025-05-19 10:43:02,30
Intel,mt3rmh2,"A dual GPU B580 with 48GB of VRAM? That's a spicy meatball, especially if the price is low. IIRC a B580 12GB is something like only $399 CAD or was $249 USD at launch.  That's the kind of cooking-with-gas product that would have the AI open source dev crowd marching to a blue tune. Especially if you could get a handful of them for the same price as a 5090 32GB card and run them together.",hardware,2025-05-19 11:26:02,18
Intel,mt45fyl,"Is doing 'AI' actually simple on this hardware though? as in wide compatibility, no headaches trying to make everything work like it just does on Nvidia etc.  I have no clue, not trying to sealion. I just know that for a long time actually doing certain tasks on non-Nvidia hardware involves a lot of extra steps just to make things work.",hardware,2025-05-19 13:00:59,7
Intel,mt3py82,MFW When Intel GPU Department is dishing out good stuff and CPU department is kina meh.,hardware,2025-05-19 11:12:53,13
Intel,mt5d7f1,"Nothing on the B770 after saying ""stay tuned"" when asked about it? Seriously?",hardware,2025-05-19 16:49:47,5
Intel,mt4cjhi,"So much for ""UNDERPROMISE OVERDELIVER"". They all but said to expect B770 and nothing...",hardware,2025-05-19 13:43:10,4
Intel,mt3pzss,Are the ARC GPUs any good or not. I've seen some saying they're crap while others say they give the most bang for your buck. So which is it?,hardware,2025-05-19 11:13:14,1
Intel,mt46a7l,"The B60 has 24 GB of VRAM for a $500 MSRP (but I imagine it will be difficult to find at that price/single GPU). The memory bandwidth (which will likely be the inference limitation) is 456 GB/s.  For inference, its closest Nvidia competitor is the $550 MSRP 5070, which has just 12 GB of VRAM but bandwidth of 672 GB/s. So if you're running an 8B model (which honestly I can only recommend for small tasks like tab code assist, not image generation) it is better to get the 5070. But if you're running a slightly larger model like Deepseek R1 14B at Q8, you will only be able to run it with B60 or something significantly more expensive, like a 3090.  https://apxml.com/tools/vram-calculator",hardware,2025-05-19 13:06:13,1
Intel,mt3q3yg,"You gotta love competition, Nvidia would never do something like this, even though they say they want to democratize AI.  If they really wanted to boost AI research, you flood the world with affordable VRAM so everyone can run big models locally.",hardware,2025-05-19 11:14:10,60
Intel,mt590wq,It wouldn't surprise if the reason they aren't selling the GPUs with Battlematrix ala carte initially is that the code hasn't been mainlined into the Linux kernel yet.,hardware,2025-05-19 16:29:04,6
Intel,mt3qevi,Depending on Price and Software Support I might actually grab myself the 24 or 48gb VRAM Variant for AI workloads. If the gaming performance is improved I might be able to bench my 6700XT.,hardware,2025-05-19 11:16:34,15
Intel,mt5wkn0,Hey I want to say good on you for updating every comment youâ€™ve made with the new information; it was a pretty upstanding thing to do.,hardware,2025-05-19 18:22:32,3
Intel,mt4ffqv,"> Disappointing that we're not seeing the B770 in computex. Guess we'll have to ""stay tuned"" until intel decides to release or scrap the B770 >  >   That was never going to happen before RTX 5060.",hardware,2025-05-19 13:59:03,3
Intel,mt3pyov,"they probably know that they can't just start making dGPUs and immediately get half the market, but that it's a long battle that they gotta continue for quite a while",hardware,2025-05-19 11:13:00,27
Intel,mt3t5pj,"From Chips and Cheese's video, it should be comming in Q4  [https://youtu.be/F\_Oq5NTR6Sk?si=A6cXIrKbRquAsNEm&t=280](https://youtu.be/F_Oq5NTR6Sk?si=A6cXIrKbRquAsNEm&t=280)",hardware,2025-05-19 11:37:47,20
Intel,mt3puql,They are guaranteed to be support SR IOV by Q4,hardware,2025-05-19 11:12:07,17
Intel,mt3vqwz,Yes https://x.com/tekwendell/status/1924417394115858735,hardware,2025-05-19 11:56:44,5
Intel,mt42xv9,"You can't buy the Arc Pro B60 by itself. Instead, Intel is bundling up to 4 of them with a xeon cpu as part of a ""Battlematrix"" workstation.   Intel must be desperate to move unsold Granite Rapids inventory  Each workstation will cost $5000-$10000 with a custom linux distro tailored to running ai workloads  Edit: ypu will be able to buy the Arc Pro B60 as an individual card in Q1 2026",hardware,2025-05-19 12:45:27,2
Intel,mt3oluv,"I guess so. Intel did throw gamers a bone, though, with the Arc Pro b50 with 16gb Vram with a 128bit bus using the cut down b570 die (16 Xe cores)",hardware,2025-05-19 11:01:53,12
Intel,mt43610,"You can't buy the Arc Pro B60 by itself. Instead, Intel is bundling up to 4 of them with a xeon workstation cpu as part of a ""Battlematrix"" workstation.  *sigh* Intel pulling an Intel again. Intel must be desperate to unload Granite Rapids inventory that's sitting in warehouses  EDIT:  intel is going to sell the Arc Pro B60 separately in Q1 2026",hardware,2025-05-19 12:46:53,5
Intel,mt4x5tf,%100,hardware,2025-05-19 15:30:06,2
Intel,mt466qx,"According to a news article: ""Intel is working to deliver a validated full-stack containerized Linux solution that includes everything needed to deploy a system, including drivers, libraries, tools, and frameworks, that's all performance optimized, allowing customers to hit the ground running with a simple install process. Intel will roll out the new containers in phases as its initiative matures. """,hardware,2025-05-19 13:05:38,12
Intel,mt4vsoz,"I own an A770, and the answer is absolutely not. I bought it with the expectation that the experience would be much worse than nvidia/amd, and they blew my expectations out of the water.  My favorite example is that, until [this release 3 weeks ago](https://github.com/intel/intel-extension-for-pytorch/releases/tag/v2.7.10%2Bxpu) ([the fix was merged ~march 3](https://github.com/intel/intel-extension-for-pytorch/issues/325#issuecomment-2694277026)), using large image generation models on Alchemist cards wasn't possible because they have a 4GB VRAM allocation limit (fixed in Battlemage). Now it mostly ""just works"", but a lot of optimizations are nvidia-only.  LLM support isn't quite as awful as image/video generation was prior to the aforementioned release, but it's still substantially worse than nvidia/amd (I also own a 6900xt). For instance, none of the inferencing tools support Flash Attention on Intel's compute backend (a [SYCL](https://en.wikipedia.org/wiki/SYCL) implementation), which _severely_ limits context length / initial generation speed because memory usage scales quadratically with context size.  Intel maintains a [repository](https://github.com/intel/ipex-llm) with forks of most of the popular tools (vLLM, llama.cpp, Ollama, etc). It vastly improves performance when using intel's backend relative to the upstreams, but it's also extremely annoying to use. It's buggier, releases lag behind upstream, there's less model support, it requires a hyper-specific environment setup, etc. I have no idea why they don't just upstream their changes.  At this rate, Vulkan will end up with better support and performance than SYCL, despite the former not being designed with compute workloads in mind at all, while the latter was explicitly built for it (I think).",hardware,2025-05-19 15:23:10,10
Intel,mt4v6dc,"Been researching what my options are outside of NVIDIA, but it honestly seems Intel cards may be a more viable solution with IPEX than AMD. There are reddit posts floating around with the B580 showing comparable inference performance to a 4060ti for text and image generation.  Getting 3-4it/s on SDXL models would be a huge improvement to my 4-5sec/it with my 1060 ti bottlenecked by RAM.",hardware,2025-05-19 15:20:03,2
Intel,mt4td07,On a new budget computer build they're really good but there's issues putting them in older systems as upgrades.,hardware,2025-05-19 15:10:56,6
Intel,mt3q9er,Decently competitive. They do have driver quirks and last I heard they also had a CPU overhead issue.,hardware,2025-05-19 11:15:22,13
Intel,mt45p8k,For gaming the B580 seems like very good value if you're prepared to encounter a few scenarios where it falls behind. In general it seems to offer a decent bit more than AMD and Nvidia at the same price point.,hardware,2025-05-19 13:02:35,3
Intel,mt4ac0l,">For inference, its closest Nvidia competitor is the $550 MSRP 5070, which has just 12 GB of VRAM but bandwidth of 672 GB/s.  No, the closest NV competitor is the $429 5060Ti 16GB with 448GB/s bandwidth.",hardware,2025-05-19 13:30:29,6
Intel,mt3r3hw,I just realized that you can actually put 4 dual GPU b60's in a large case to get 8 gpu's into 1 pc,hardware,2025-05-19 11:21:55,23
Intel,mt46qxb,"You canâ€™t, theyâ€™re not sold individually and are workstation only.",hardware,2025-05-19 13:09:06,9
Intel,mt4oa71,"Why? If they plan to release it at all, they should do it before the 5060, not after.",hardware,2025-05-19 14:45:18,1
Intel,mt3wfcg,This makes me very erect.,hardware,2025-05-19 12:01:36,13
Intel,mt43a73,They better make these available.  I have been waiting since 2018/2019 for SR-IOV to become available on GPUs ever since I tried it with enterprise NICs.,hardware,2025-05-19 12:47:38,8
Intel,mt3u9nh,"128 bit on Intel is entry level. Around A580/RX 6600 (which was $190 like 3 years ago). I don't think it makes sense for gaming.  The extra VRAM is nice, but it's too slow to be worth even 199 for gaming, let alone the MSRP of 299. Might make sense for video editing or AI or smth.",hardware,2025-05-19 11:46:02,14
Intel,mt47n7m,"Yeah which seems to translate as no, not right now.",hardware,2025-05-19 13:14:32,7
Intel,mt51h1c,"B580 is probably better, but I get closer to <2 it/s on my A770. There might also be something wrong with my setup, given how jank the whole thing is though.",hardware,2025-05-19 15:51:32,3
Intel,mt4298k,And 192 gigs of vram. Thats enough to run almost all of the open source language models out right now. I wonder how that kind of setup would stack against an Nvidia setup.,hardware,2025-05-19 12:41:06,19
Intel,mt3y0t8,Proper posture is important.,hardware,2025-05-19 12:12:54,14
Intel,mt43nkn,"If you want this setup, then you'll have to buy a ""battlematrix"" workstation with what I assume is a Granite Rapids Xeon cpu.  Guess they're really trying to get move unsold Granite Rapids inventory  as you can't buy the Arc Pro b60 on it's own.  Each workstation will cost $5000-$10000 with a custom linux distro tailored to running ai workloads  Edit: You will be able to buy the Arc Pro B60 as an individual card in Q1 2026",hardware,2025-05-19 12:49:57,10
Intel,mt6mm9z,It's definitely a posture alright.,hardware,2025-05-19 20:32:10,3
Intel,mt484h3,"Wow, what...  I need one of those.  I just wonder if they are already redy to run, or just dont have the software ready like it is for CUDA, can't be writting all the code from scratch.",hardware,2025-05-19 13:17:23,6
Intel,mt4hpko,I looked into the situation more and they're apparently going to sell the Arc Pro B60 as an individual card in Q1 2026. Before that it's exclusive to battlematrix,hardware,2025-05-19 14:11:18,7
Intel,msanmdn,"I'm convinced that Intel is launching the B770 at this point, because if they don't, this would be an *insane* marketing blunder.",hardware,2025-05-14 16:33:49,94
Intel,msah3an,"So we have gone from radio silence since the B570 to possibly seeing a B770, B580 24gb, Pro B60, and B580 sli 48gb. Wonder how many of these will actually appear at Computex.",hardware,2025-05-14 16:02:19,98
Intel,msbbj76,"everytime someone says ""stay tuned"" about their product it's like hitting a 3 month snooze button.",hardware,2025-05-14 18:26:50,49
Intel,msb4u54,"I mean this has to all but confirm it unless someone in their social media team is having a very uncomfortable meeting with management today.  Either B770/80 is going to be a thing in one configuration or another, or they've managed to move up the schedule for Celestial somehow.  I would expect that the former is much more likely than the latter.  Personally, I'm still likely to keep my money in my wallet until next gen at least, but if Intel disrupts the game a bit with a 32gb vram at prices well below 5080 prices, it would be pretty cool for people who want to run small but not tiny ML models locally to have a budget option.  Some AI workloads, and particularly for learning and developing could accept the trade off of having bigger models with slower performance.",hardware,2025-05-14 17:54:44,17
Intel,msb3ewn,Hell yeah! Might pick up a B770 if the drivers and stock aren't too bad.,hardware,2025-05-14 17:48:08,7
Intel,msb63ay,Wooo Go BMG-G31. It should be an interesting to compare the b770 to the 9070xt and the 5070,hardware,2025-05-14 18:00:35,8
Intel,msepf2x,"My guess is <10% slower than a 9070(non XT), or a 5070. For ~30% less moneys.  PS: would still be nice it was an exact equivalent to, or slightly better than, a 9060XT too. But I expect more.",hardware,2025-05-15 06:41:06,2
Intel,ms8y64x,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-05-14 10:48:37,1
Intel,mscpkbj,"Doesn't make sense to release models within six months of the next-generation which is Celestial, so if Intel was going to release more Battlemage models it had to be sooner rather than later. B580 was six months ago already.",hardware,2025-05-14 22:41:01,-1
Intel,msdj9dx,they meant tuning to the Novideo channel,hardware,2025-05-15 01:31:29,-1
Intel,msacivd,"I think for the B770, they could try adding 80 Xe engines, along with 32GB VRAM. These within a sub-$800 price tag will destroy any Nvidia GPU",hardware,2025-05-14 15:40:44,-17
Intel,msbodpt,"Wonder what their profit margins are going to be with a massive B770 32Xe2 die, which is most likely also going to be decently power hungry.   Don't get me wrong, I would love to see one, but I'm just curious how Intel is going to actually capitalize on a potential B770 launch.",hardware,2025-05-14 19:29:18,36
Intel,msetbek,"But, being Intel, if someone can toss the table and put Nvidia and AMD on their place they are the ones.  My i7 6700K still boots faster and is more stable than my r5 7600 after almost 10 years.  Intel has been doing poorly these years but their legacy and their inhouse power can absolutely turn the tides.  B770 a 192 bit and 16Gb Vram for less than 380â‚¬ and it kills the duopoly.",hardware,2025-05-15 07:19:45,1
Intel,msav8eb,"They saw how absurd graphics card prices are now and decided to release everything. Even old cards are more expensive than they used to be, like you can't buy a new rx6800 or equivalent at 360$ like you could in 2024. Even the b580 while not perfect easily sells out at <=$300 because there is nothing else in that price range with 12gb+ vram, av1 encode, dp2.1 support, and good RT/production performance.",hardware,2025-05-14 17:10:06,63
Intel,msay9ri,They definitely wanted to see what the competition had to offer. Intel knew Nvidia wouldn't offer anything that good at $250 and released them before others.,hardware,2025-05-14 17:24:15,8
Intel,msahqhn,They don't want to kill the hype so they are still quiet.,hardware,2025-05-14 16:05:26,7
Intel,msbxro6,They do experience some very unique and often unfortunate time dialation,hardware,2025-05-14 20:15:11,17
Intel,msbn1pq,"The B770 is likely a 16GB config, but even that stands a chance at being very disruptive against a 9060XT and 5060ti if priced to undercut them.",hardware,2025-05-14 19:22:44,21
Intel,msffhcx,32gb would be insane for ML workloads. Thatâ€™s ChatGPT-4o quality these days. A B770 with 32gb running Qwen-3-32b is a better programmer than 99% of computer science undergrad students.,hardware,2025-05-15 11:00:01,9
Intel,msdr5nt,They were never bad compared to Nvidia's 50 Series drivers,hardware,2025-05-15 02:17:57,5
Intel,msb9pvs,The B770 would be nowhere near the 9700XT performance wise.  I would guess it would be a 5060ti/9060XT rival.,hardware,2025-05-14 18:18:03,15
Intel,msmbtmd,Celestial dGPU is 2H 2026 on Xe3P using their internal foundries.,hardware,2025-05-16 13:12:30,1
Intel,msag9qj,I think Intel should also give you a Ferrari with every purchase.   Also the Ferrari gives you a blowjob every 10 miles.,hardware,2025-05-14 15:58:25,41
Intel,msadfgg,"hold there cowboy, do you have any idea how big that gpu would be  B580 has 20 CUs, be glad if B770 is as big as 40CUs, I would be surprised if they make anything bigger than that if celestial is coming end of the year",hardware,2025-05-14 15:45:02,14
Intel,msappf7,It won't be 80 Xe engines.  Just looking at the data width difference between a 12GB B580 vs a 16GB B770.  You are looking at 3:4 ratio for available memory bandwidth.  It won't be able to feed the bandwidth of 80 Xe you claim which is 4X of B580.  Note: 32GB is for a clamshell configuration. i.e. the data bus width is same for the base 16GB units.,hardware,2025-05-14 16:43:50,6
Intel,msay5aa,"B580 is already a decent sized die. If they quadrupled the size of the die, it would hit the tsmc 5nm reticle limit.",hardware,2025-05-14 17:23:41,7
Intel,msah1xn,"You're smoking the good stuff, huh",hardware,2025-05-14 16:02:08,9
Intel,msacnnx,Best they can do is probably $1599,hardware,2025-05-14 15:41:22,0
Intel,msdr0ew,Bro is in Delulu,hardware,2025-05-15 02:17:03,-1
Intel,msbto0v,Power hungry and  probably a huge die considering how big the B580 is.,hardware,2025-05-14 19:55:12,22
Intel,msbrp6i,"RX 6800 has been EOL for months, that's why the last few remaining are priced up.",hardware,2025-05-14 19:45:41,-3
Intel,msjw9gh,Oh shit I got called out,hardware,2025-05-16 01:29:49,5
Intel,mse5ma9,"Yeah the B580's had pretty decent drivers afaik. Nvidia really dropped the ball this gen, but they don't care because they make 10x the money elsewhere.",hardware,2025-05-15 03:54:02,2
Intel,msdqtsj,"WTF The B580 is close to 4060Ti in many scenarios when GPU is choked aka fully leveraged, and 5060Ti is a 4060Ti with GDDR7, it will be theoretically close to 7800XT, but no it wont be close to 9070 but will be better than 9060XT which is supposed to have 32 CUs",hardware,2025-05-15 02:15:56,5
Intel,msbg4kf,Performance was meant to be comparable to the rtx 3080 or 4070,hardware,2025-05-14 18:49:03,4
Intel,msdqept,"I guess <10% slower than a 9070(non XT), or a 5070. For ~30% less moneys.  PS: 9060XT is much better than the 5060Ti, so would be nice it was an exact equivalent to, or slightly better than, a 9060XT too. The B580 is already comparable to a 4060Ti in most scenarios, and 5060Ti is just a 4060Ti with GDDR7.",hardware,2025-05-15 02:13:24,0
Intel,msb6mf9,"The biggest planned battlemage die was BMG-G10 with 56-60Xe cores with 112-116mb of L4 Adamantine cache with a 256bit bus  But it got canceled much earlier on in development than BMG-G31 along with L4 Adamantine cache for Meteor Lake's igpu  BMG-G10 would've been very close to the maximum possible size for the Battlemage ip. It would've likely competed with the rtx 4090 in performance  If meteor lake released with L4 ADM cache, then we would've likely seen G10. Die size would've likely been massive from 600mm2 to 750mm2 of TSMC N5 or N4",hardware,2025-05-14 18:03:10,1
Intel,msav3ru,"If the rumors from before G31 got supposedly killed and resurrected are still relevant, it should have 32 Xe cores and a 256-bit bus (16GB).",hardware,2025-05-14 17:09:29,9
Intel,msayrcz,"They might do 32 GB since it would make it lucrative for the AI and Professional Workstation markets. I could see them not bumping the core count by that much and using the memory as a selling point. iirc, b580 is the largest that could be done on the current die, and they had one higher that was cancelled. That one might have the bus width to do 24GB non-clamshell.",hardware,2025-05-14 17:26:30,4
Intel,msbs6ij,But there is nothing that replaced it available at the price either,hardware,2025-05-14 19:48:01,31
Intel,msepm76,Exactly.,hardware,2025-05-15 06:43:04,2
Intel,msc47pn,"Was the performance of the B580 ""meant"" to be comparable to the RTX 4060 and RX 7600?",hardware,2025-05-14 20:48:44,3
Intel,msgum00,Much better how?,hardware,2025-05-15 15:52:53,3
Intel,msbnud8,"IIRC Battlemage can enumerate 64 Xe cores, so G10 would've pretty much been a maximum possible config. That die may have also been planned for an internal node if going with Adamantine. I'm not in ARC so this is pure speculation and pondering, but it would've been cool to see something like an Intel4 node big-BMG GPU.",hardware,2025-05-14 19:26:38,9
Intel,msc76d1,Yes,hardware,2025-05-14 21:03:17,1
Intel,mseh2kg,Yes and even reached 4060ti performance in certain games,hardware,2025-05-15 05:23:36,0
Intel,mscmdf4,Thatâ€™s pretty obviously not true if you look at the spec sheet. Nobody sane would bother green lighting the B580 if the **intent** was to compete with xx60 tier products.,hardware,2025-05-14 22:22:58,-6
Intel,msdoqdn,"Intel has not reached the maturity where they can match N/A performances with the same spec, it's pretty open that they're expecting it to be at least 1 or even 2 class lower performance with the same hardware spec.  They spoke about this in multiple podcasts since the beginning, even back to the alchemists days, that they're looking to close the gap but not completely catchup in the next gen even.",hardware,2025-05-15 02:03:18,5
Intel,mrx5isq,Just make a B770 please,hardware,2025-05-12 14:42:59,98
Intel,mrx5hgv,Cool. Wonder if there ever was a time when AIB makers had this flexibility with AMD or nVidia cards?,hardware,2025-05-12 14:42:48,25
Intel,mryhyif,Why can't a man get a 24GB memory in a 5070 Ti?   Too much to ask?,hardware,2025-05-12 18:39:44,5
Intel,mrxxkxt,"Interesting! I suppose they will be doing a clamshell design for the memory to double the modules, since b580 was 12 GB GDDR6, they just need to double the memory chips.  Or perhaps there is already enough PCB space, if they are using 2 GB modules, to just add 6 more.",hardware,2025-05-12 17:01:09,4
Intel,mrx4zrd,"Hello fatso486! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-05-12 14:40:18,2
Intel,ms0xsyc,"There was a BMG-G10 die planned with 56-60Xe core die with 112-116mb of L4 Adamantine cache as MALL cache with a 256bit bus.   But the die was canceled during development along with L4 Adamantine cache, which was also planned to be used in Meteor Lake's igpu.  BMG-G31 was the planned 32Xe core core. It had a 256bit memory bus and so likely memory configurations were likely 16-32gb of GDDR6  BMG-G10 and BMG-G31 would've likely been a bloated die if it targeted 2850mhz clock speeds like the B580. Less so if they targeted lower clocks.    We'll likely never see the G10 die, but we could still see BMG-G31 (32Xe core die)",hardware,2025-05-13 02:35:43,3
Intel,mrxpfbm,Which game on what resolution would require 24 GBs VRAM though?,hardware,2025-05-12 16:21:09,-5
Intel,mrx8ttf,Im sure they already made but it doesn't look worth releasing for intel. The 5070 is almost **%80 faster than the The B580 while being a smaller chip** . Intel are  probably barely breaking even with $50 above MSRP with the B580.  \--  Edit: fixed math based on TPU charts,hardware,2025-05-12 14:59:29,41
Intel,mrxxaod,"Might be one on the way, but unfortunately, an expensive Pro version.    https://www.techpowerup.com/336528/intel-teases-upcoming-unveiling-of-new-arc-pro-gpus-insiders-predict-battlemage-b60-card",hardware,2025-05-12 16:59:45,5
Intel,mrx6z98,Yes,hardware,2025-05-12 14:50:16,30
Intel,mrxpstu,Event recently as Geforce 900 series AIBs were launching their own double VRAM cards unofficially.,hardware,2025-05-12 16:23:02,7
Intel,mry0i7i,HD 7970 6GB from 2012 can still play things. Even supports Vulkan 1.3 on Linux,hardware,2025-05-12 17:15:23,6
Intel,mrxmr36,"This has even happened more recently. Towards the end of its production, Sapphire produced an 8GB version of the AMD Radeon RX 6500XT using a clamshell design, and the price was basically the same Â£150-200 (I don't know the US price, sorry).  From the past, my favourite of these (although I guess it's a different, more unique case) is the Nvidia GTX 295x2. For context, the 295 was a dual-GPU version of the GTX 280 which worked through the now-defunct SLI protocol. EVGA's engineers had the bright idea of making a dual-GPU version of this dual-GPU card, and so they essentially made their own SKU where they linked up two of these cards to form a quad-GPU card. Absolutely insane stuff, and the heat produced from it and the power usage to run it was insane, but in games that supported SLI (which was a bit shoddy) it was an absolutely insane card.",hardware,2025-05-12 16:07:53,12
Intel,mrx8m5m,"YES?  lots of examples of double memory cards in the past.  there is an 8 GB 290x for example to mention rarer ones.  and having dual memory options are simple and easy to do.  to be perfectly clear in case you aren't aware, amd and nvidia could tell aibs tomorrow to make double or 1.5x memory (for gddr7) cards and we'd see them in 2 months time maybe.  it is cheap, it is extremely low effort, there are no issues here, except amd and nvidia PREVENTING partners to produce double/higher capacity memory cards.  again this was common practice back then right from the start even.  we had polaris 10  (rx 470, 480, etc... ) in 4 and 8 GB versions  and for basically just the vram cost difference.  that is how it should be.  but again amd is preventing partners from doing this. that is why we don't see 32 GB 9070/xt cards for example.  we the customers want them,  the partners want to make them, but amd DOES NOT LET IT HAPPEN!  and again the same goes for nvidia, but harder, because they push broken amounts of vram way more.  the 5070 12 GB for example with 3 GB memory modules, without having to change the pcb even  (we can assume)  would be a 5070 18 GB  card then and acceptable at least.  and a 5060 would be a 5060 with 16 GB (clam shell), or at bare minimum 12 GB with 3 GB modules, or hell 24 GB if you go clamshell + 3 GB modules)  \_\_  the only reason we see double memory config options  extremely rare these days, is because companies are trying to scam us harder by preventing us from getting what we want, or hell preventing people from getting a working graphics card even (see all 8 GB cards)",hardware,2025-05-12 14:58:25,18
Intel,mrxkq08,I'm getting old.,hardware,2025-05-12 15:57:48,2
Intel,mryjq6g,"He can and will. Its called 5080 Super (or ti) . They already released it as Laptop ""5090"" so its just a matter of time.",hardware,2025-05-12 18:48:24,12
Intel,mrxqi8c,">Which game  This would not be sold for gaming, it's for applications or AI workloads.",hardware,2025-05-12 16:26:29,21
Intel,ms0tkfn,It's 2025 grandpa. Gaming is only one thing GPU's are used for these days.,hardware,2025-05-13 02:10:53,5
Intel,mrxu37b,GPUs aren't made to play games anymore.,hardware,2025-05-12 16:44:10,5
Intel,mryb5r5,"Lots of games will push past 12GB, and even 16GB with new games at particular RT settings and high resolutions.  Not necessarily a big problem to avoid right now by lowering settings, but you'll need to know which settings to lower. I don't feel all that comfortable getting a new card with 12GB as you'd want it to last for a good while when you pay that much to begin with.",hardware,2025-05-12 18:06:21,2
Intel,ms357mg,Path tracing and mods can go over 16GB at 4k.,hardware,2025-05-13 13:21:20,1
Intel,mrxrybm,"This shouldn't be considered for  gaming.   Considering the gaming performance level of b580, I'm sure 24GB is totally useless.",hardware,2025-05-12 16:33:38,2
Intel,mrxciam,"The b580 is only 20% as performant as a 5070?  -  E: Ah, op just phrased it oddly. Thanks for the info below. OP: percentage differences don't work in both directions, please see the info below.  E2: op fixed it, we can stop busting their chops now but I'm glad we all had the opportunity to discuss how stats work (and I'm not joking, it's good for us all to have a refresher).",hardware,2025-05-12 15:17:52,27
Intel,mryfdw3,I wonder how the RTX 5070 is so much denser than the B580.,hardware,2025-05-12 18:27:00,3
Intel,ms0y6nj,"If intel targeted 2850mhz clock speeds for BMG-G31 then you would likely be correct, however if they targeted lower clock speeds closer to 2000mhz like we saw with the dense Xe2 implementation seen on the Lunar Lake igpu then BMG-G31 could end up being more area efficient than BMG-G21",hardware,2025-05-13 02:38:04,3
Intel,mry0a8h,> The 5070 is almost %80 faster than the The B580 while being a smaller chip .  Chip size doesn't matter as much as you think.,hardware,2025-05-12 17:14:20,-11
Intel,ms2lapc,That's the last time it was allowed. Nvidia wisened up with the 10 series.,hardware,2025-05-13 11:11:20,2
Intel,ms0oclu,"Do you have more information about this EVGA card? I'm An Old and don't remember them doing sandwich style dual-GTX 295, quad die card, but would love to see more about it. There were some tongue in cheek satirical riffs on products like that though, like the obviously not real *Nvidia GTX 295X2 ATI GTFO Edition*.  The only 295X2 cards I know of were the Radeon R9 295X2 (dual-die, downclocked R9 290X on one PCB), with the earlier GTX 295 cards being similar. The older 7950GX2/9800GX2s had a dual PCB, but were also only dual die. Asus' various Mars and Ares series cards were all single PCB affairs, as were ye olden Radeon HD 5970/6970/7990/8990s and stuff like the GTX 460X2, GTX 590/690, etc.  *Edit: Ares, not Aries*",hardware,2025-05-13 01:41:12,6
Intel,mrxqfxk,"I had 4GB GTX 670s in SLI in 2012, the extra vram carried hard when I got an early 4K monitor",hardware,2025-05-12 16:26:10,9
Intel,mrxcbn8,Clamshell ain't cheap tho,hardware,2025-05-12 15:16:57,7
Intel,mry0ppl,This conspiracy theorist again...,hardware,2025-05-12 17:16:23,-2
Intel,mry8hqp,"The Polaris/Pascal launches were the last great value launches I can think of. Sure, you can argue that the 9070 series had an okay MSRP, but it's not without qualifiers.   Reddit was more Nvidia fanboyish than now, and largely wrote off Polaris. History shows the 470/570/480/580 aged better than the 1060 models.",hardware,2025-05-12 17:53:29,4
Intel,mrxsmb4,"So why is the source saying it is for ""gamers too""?",hardware,2025-05-12 16:36:55,-2
Intel,ms23bth,GPUs are still made to be generalists in their application.,hardware,2025-05-13 08:20:01,2
Intel,mrxsml2,"So why is the source saying it is for ""gamers too""?",hardware,2025-05-12 16:36:57,-2
Intel,mrxlf01,TPU lists the 5070 78% faster than the B580 on the B580 page. They also list the B580 as 44% slower than a 5070 on the 5070 page.,hardware,2025-05-12 16:01:14,6
Intel,mrxk0h5,5070 has 80% more performance than B580,hardware,2025-05-12 15:54:21,19
Intel,ms2in7f,"According to an Intel rep, density can be influenced by how you count transistors, and everybody has their own way of counting that we don't get good information on.",hardware,2025-05-13 10:50:22,3
Intel,mry0ij1,It matters for profitability,hardware,2025-05-12 17:15:25,27
Intel,mryarsw,It matters when the performance delta is so big because performance doesn't scale linearly with the hardware.  Intel would need a humongous die to reach the 5070/ti tier which would make it financially unprofitable.,hardware,2025-05-12 18:04:26,11
Intel,mryapay,Well it matters a heck a lot more than you make it sound. There is a reason why the B580 is a paper launch and doesn't really exist in the wild close to its MSRP. Its not sustainable and is  just too expensive to make.,hardware,2025-05-12 18:04:06,1
Intel,ms27rix,"I tried searching for it, and for some reason I couldn't find it even though I've seen it before (including on Wikipedia, which isn't the most trustworthy source but still). I think it might've only been an engineering sample that never hit the market and was then rumoured about on forums for years?  If not, then this is one hell of a Mandela Effect.",hardware,2025-05-13 09:06:51,2
Intel,mrxjwx7,"It's not free, but it's pretty cheap. Looking at the 5060 Ti, 50 bucks is apparently enough to make up for clamshell + 8GB GDDR7 + margins which almost certainly won't be lower than on the cheaper SKU.  Hard to explain how cards that cost 2x-3x as much don't come with clamshell configs by default other than pure product segmentation.",hardware,2025-05-12 15:53:52,20
Intel,mrxtihw,"says who????  please show me an actual reference, that clearly shows the added cost to use a clamshell design to increase memory.  not some vague statement by a gpu maker, but an actual cost breakdown from a graphics card maker like let's say sapphire.  showing how much using a clamshell design ads in cost  vs a single sided memory design.  if you don't have that data, then you yourself are randomly assuming, that it would be very expensive, or someone told you WITHOUT ANY EVIDENCE, that it ""is very expensive"".  the CORRECT assumption to make here is, that it is DIRT CHEAP to use a clamshell design, unless actually proven otherwise.  \_\_\_  and even more crucial here is, that IF it were very expensive, it would NEVER be an excuse for selling broken graphics cards.  nvidia CHOSE to put a 128 bit on the 60 tier cards and a 192 bit bus on the 5070.  they CHOSE this, same as amd CHOSE to put a 128 bit bus on the 9060 xt.  they chose this, knowing the costs of clamshell designs, knowing that 8 GB vram is already broken when they chose the memory bus (yes that includes the massive time, that dies are in development)  so bringing up a reasonably assumed FALSE idea, that clamshell designs are expensive is playing towards the bullshit from the gpu makers.  but hey we can have partners figure the cost out if they want by letting them double or 1.5 x the memory to their liking.  CHOICE, that is what the gpu makers don't want you to have.",hardware,2025-05-12 16:41:21,2
Intel,mt77tn5,"I wished amd had continued with their strategy with Polaris. Repurpose old production nodes for lower end markets.Â    The rx570 was the perfect entry level cards during 2020. Way better value than 5500, and was so readily available that during the mining boom, it remained a stable price despite and finally discontinuing production.Â    Imagine if the rx6600 still in production today for $200.",hardware,2025-05-19 22:23:34,2
Intel,mry2vqs,Because you're not limited to using it for applications or AI workloads.,hardware,2025-05-12 17:26:47,10
Intel,mrxuimd,[https://langeek.co/en/grammar/course/866/too](https://langeek.co/en/grammar/course/866/too),hardware,2025-05-12 16:46:15,10
Intel,mrxuck7,"They're saying that you can play games on your AI rig with it. It's not a ""compute"" card and is capable of running modern graphics libraries. This card's primary purpose isn't gaming, but it'll be able to handle it if you want to use your computer for both.",hardware,2025-05-12 16:45:26,10
Intel,mrxve39,[https://langeek.co/en/grammar/course/866/too](https://langeek.co/en/grammar/course/866/too),hardware,2025-05-12 16:50:31,3
Intel,mrxmcj6,"That is not how math works (not you the guy you replied to)  100% 5070 = 180% B580  1 5070 = 1.8 B580  1/1.8 5070 = 1.8/1.8 B580 = 1 B580  1/1.8 ~= 0.56 or 56% the performance  Given the 550 price tag on the 5070 and the 250 on the B580, the B580 is 45% the price. Itâ€™s better value per dollar, if that performance is acceptable.  EDIT: this reply was made to the comment above that said the B580 was 20% of the 5070",hardware,2025-05-12 16:05:53,1
Intel,mrygw1u,It's not. Bigger issue for them is 30% for retailers alongside the % for board partners.,hardware,2025-05-12 18:34:28,-4
Intel,mrybn20,> There is a reason why the B580 is a paper launch and doesn't really exist in the wild close to its MSRP.  [Lots of stock in Europe.](https://geizhals.eu/?cat=gra16_512&xf=9816_01+01+09+-+B580)  Maybe there's other reasons it's not in the US.,hardware,2025-05-12 18:08:40,4
Intel,ms5pj6y,"Definitely could have been a semi-custom or engineering sample, but if you find something send me a ping. I love obscure stuff like that! There are some wild products that end up powering oddball simulators and whatnot - remember the old Quantum3D or Intergraph simulator stuff? Aww, yiss.  [http://www.thedodgegarage.com/3dfx/q3d\_mercury\_brick.htm](http://www.thedodgegarage.com/3dfx/q3d_mercury_brick.htm)",hardware,2025-05-13 20:53:30,2
Intel,mrxu709,50 euro on a 400 euro card is a big chunk of percentage.,hardware,2025-05-12 16:44:40,4
Intel,ms34q8i,">please show me an actual reference, that clearly shows the added cost to use a clamshell design to increase memory.Â    Consumers aren't privy to this data either showing it's expensive or not.",hardware,2025-05-13 13:18:36,1
Intel,mrxpotx,How does that disagree with what he said?,hardware,2025-05-12 16:22:29,10
Intel,mrxpphv,"You make it sound that the $250 b580 is a real product that can be bought by normal people.  Personally,  even if it was available widely at that price id still opt for a used 6700xt for less than $200 instead which is exactly what i did  last year.",hardware,2025-05-12 16:22:34,0
Intel,mryniyj,"what...  AMD and Nvidia both fabs at TSMC, now that intel is doing it too with arc, that means their BoM is more or less the same  having a smaller chip means they get way more profit, and selling it at 5070 prices vs b580 prices means its a HUGE profit diff, even if we account for GDDR7 vs 6 and all that but the price gap is like 300 whole ass dollars the price of a full b580 lol.",hardware,2025-05-12 19:07:27,10
Intel,mrxxyqe,"Yes, which is why that last line is focusing on more expensive cards. It makes sense for it to be optional on midrange and below SKUs.",hardware,2025-05-12 17:03:02,3
Intel,mrxq4xp,Because it used to say B580 has 20% the performance of a 5070,hardware,2025-05-12 16:24:40,5
Intel,mryc7c2,"I mean I bought it at that price, just wait for the restocks which are pretty frequent. Not saying that itâ€™s easy, just that itâ€™s possible",hardware,2025-05-12 18:11:27,6
Intel,mrybdvp,"You're not wrong, I'm seeing it pop up on buildapcsales in the neighborhood of $300 though, a far cry from the 5070 that's usually in the neighborhood of double.",hardware,2025-05-12 18:07:26,-1
Intel,mrzhn4d,The transistor count for the RTX 4060 and b580 are same.,hardware,2025-05-12 21:39:37,-1
Intel,mrxun24,Oh I think you were supposed to respond to the comment above the one you responded to,hardware,2025-05-12 16:46:50,7
Intel,mryeohh,"Shit, mb",hardware,2025-05-12 18:23:35,3
Intel,mrt5dwj,Based. We need more vram,hardware,2025-05-11 20:57:42,228
Intel,mrt6lvz,Professional lineup is probably gonna have professional prices,hardware,2025-05-11 21:04:25,90
Intel,mrtcc3c,"The B580 version 24GB is relatively easy to do as it would need a PCB layout with double side VRAM and may be a new BIOS and driver.  Very little R&D needed.  There is no point to have both 20GB and 24GB cards as they won't worry about the tiny price saving in the Pro market for a slower card with 4GB less VRAM.  The B770  32GB on the other hand is unlikely.  All that R&D for a new B770 ASIC needs to be recouped, so it would be a waste to not also available as a 16GB card for the consumer market.  tl;dr The info is highly BS.",hardware,2025-05-11 21:36:27,38
Intel,mrt7izd,Other than Local AI enthusiasts who is this for?  And at that price cheaper non rich startups would probably be in the market for it as well.,hardware,2025-05-11 21:09:32,50
Intel,mrt6014,This will sell for 1200 and fly off the shelf at that price imo,hardware,2025-05-11 21:01:05,64
Intel,mrtnzae,Wonder how it'll do in Blender rendering workloads.,hardware,2025-05-11 22:45:45,6
Intel,mrvajbu,Whereâ€™s the 32GB Radeon cards?,hardware,2025-05-12 05:38:36,5
Intel,mrtglkd,"There are rumors of Intel exhuming the G31 chip, but no indication of it releasing so soon. Reads more like the author's wishful thinking.",hardware,2025-05-11 22:01:01,6
Intel,mrt67px,Great for workstation use  Nothing to be excited about for gamers,hardware,2025-05-11 21:02:15,9
Intel,mrter19,"""Pro"" means $2000+ I guess...",hardware,2025-05-11 21:50:23,3
Intel,mrtzvpy,"Battlemage kinda reminds me of Zen-1. Back in 2017 Zen1 wasn't as polished as Kaby Lake, wasn't as fast in single core performance, but it DID have good performance per dollar.",hardware,2025-05-12 00:00:18,5
Intel,mrtld9c,That would be brilliant.,hardware,2025-05-11 22:29:27,1
Intel,mrys4nw,32GB? It's going to be out of stock forever.,hardware,2025-05-12 19:30:35,1
Intel,ms5v1ly,NVidia shitting themselves about RAM (NVidia sell RAM as premium package - very greedy company),hardware,2025-05-13 21:21:25,1
Intel,mrtd8h2,Can you play games on the Pro GPUs?,hardware,2025-05-11 21:41:35,1
Intel,mrtoo0r,Damn was counting on the 32GB on the 5090 to hold its value for resale when 6090 comes out.,hardware,2025-05-11 22:49:58,1
Intel,mrtcgnt,Considering getting Battlemage dGPU performance for gaming seems way too much of a hurdle. Turning those G31 dies for Professional AI work seems the best bet.,hardware,2025-05-11 21:37:11,1
Intel,mrtle3f,Ideally even more memory,hardware,2025-05-11 22:29:36,1
Intel,mrtlxhs,"just triple the B580 in every way including price and i'll buy it.  60 xe cores, $749usd.",hardware,2025-05-11 22:32:57,0
Intel,mrwrnoi,Inb4 instantly sold out to AI companies,hardware,2025-05-12 13:27:38,0
Intel,mrw8ryw,We need a 69GB Vram SKU For LOLZ,hardware,2025-05-12 11:20:27,-1
Intel,mrtklga,Call me a data hoarding prepper but I have an LLM model set up locally so that if I lose complete internet connectivity for a while I have at least something I can run simple queries against. A big 32gb card at a good price makes it possible to run a bigger LLM during times of need.,hardware,2025-05-11 22:24:43,30
Intel,mrxgo2c,"They're being referred to as ""Workstation cards."" That means big $$$$ premiums over base product.",hardware,2025-05-12 15:38:11,2
Intel,mrtnvym,64GB would be great.  MORE!,hardware,2025-05-11 22:45:11,7
Intel,mrtouee,"Yeap, these are RTX Pro (Quadro) competitors meant to go in workstations, so they will be more expensive than the gaming cards. Still they should be significantly cheaper than what Nvidia charges,Â the Quadro 4060 (AD107) equivalent was $649 but came with 16GB instead of 8GB.",hardware,2025-05-11 22:51:04,1
Intel,msezeq7,"Ehhhhh, depends.  The B770 will not have enough computing power to use all that VRAM in games for example.  But for AI workloads? Yeah it will help loads.",hardware,2025-05-15 08:23:57,1
Intel,mrtqegp,and if there even remotely decent for AI they won't exist they will be vaporware like the 5090.,hardware,2025-05-11 23:00:47,-1
Intel,mrvinxr,"At this point my only hope is that games will start incorporating AI features (that barely do anything) in their game engine (NPCs, AI graphics enhancement etc). That might be the only way to pressure Nvidia to finally release affordable 32GB+ GPUs.",hardware,2025-05-12 06:58:14,-4
Intel,mrtb393,"True, but hopefully it will be in line with their pricing strategy which means it will still be   $ Intel Consumer < Intel Professional < large gap < NVIDIA anything $$$",hardware,2025-05-11 21:29:21,40
Intel,mrtq9td,The B770 parts of the article are all author conjecture. There is no solid evidence of such a card. Either way 24GB Arc card is pretty awesome and sets up the board for Celestial to improve it further.,hardware,2025-05-11 22:59:59,13
Intel,mrtzhpm,"them and anyone doing video editing, lots of vram is really good for that, and they don't typically need a whole lot of processing power like say a 5090 tier.  not sure if this is enough or with the right decode or w/e, but that is one big reason why 3090 prices were higher than normal while 4080 or 4070ti were on the market, despite those matching or exceeding 3090 performance.",hardware,2025-05-11 23:57:51,18
Intel,mrtemu0,"Many businesses would love to get that much VRAM on the cheap imo. Not even necessarily small ones, itâ€™s a huge amount of value if it can be properly utilized",hardware,2025-05-11 21:49:42,34
Intel,mrv99yo,"Computational physics needs tons of VRAM. The more VRAM, the more stuff you can simulate. It's common here to pool the VRAM of many GPUs together to go even larger - even if no NVLink/InfinityFabric are supported, with PCIe.   In computational fluid dynamics (CFD) specifically, the more VRAM the more fine details you get resolved in the turbulent flow.Â Largest I've done with FluidX3D was [2TB VRAM across 32x 64GB GPUs](https://youtu.be/clAqgNtySow) - that's where current GPU servers end. CPU systems can do even more memory capacity - here I did a simulation in [6TB RAM on 2x Xeon 6980P CPUs](https://youtu.be/K5eKxzklXDA) - but take longer as memory bandwidth is not as fast.   Science/engineering needs more VRAM!!",hardware,2025-05-12 05:26:54,20
Intel,mrtp23f,"These are workstation cards that compete against the RTX Pro (Quadro) Nvidia cards. The Nvidia cards come with ECC memory and are built for production workloads (Blender, CAD, local AI etc).",hardware,2025-05-11 22:52:24,6
Intel,mrvklot,Game artists like me. UE5 uses a shit tom of vram. I'll be able to run UE + 3dsMax + Zbrush + Painter without having to close any of them,hardware,2025-05-12 07:18:11,3
Intel,mru4cy5,Local AI enthusiasts will help build the tooling/ecosystem for you so that down the road you can more easily sell the high-margin data center products.   Just need VRAM and a decent driver.,hardware,2025-05-12 00:29:18,6
Intel,mrtjg3a,Local AI enthusiasts will quickly become working professionals whose businesses don't want them to use big tech AI,hardware,2025-05-11 22:17:48,4
Intel,mrud2ow,4K video editing for cheap.,hardware,2025-05-12 01:25:09,2
Intel,mrx604l,"How well do local AI models run on Intel GPUs, though? There don't seem to be that many benchmarks out there. Tom's Hardware has a [content creation benchmark](https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html#section-content-creation-gpu-benchmarks-rankings-2025) partially but not entirely comprising AI where the 12 GB Arc B580 sits slightly below the 8 GB RTX 4060 for a similar price. And I don't think Intel has made it a priority to optimize and catch up in that area.",hardware,2025-05-12 14:45:23,1
Intel,mrtpa7t,The 32GB B770 is just conjecture by the author. But it does look like a professional 24GB Intel card is coming based on the B580.,hardware,2025-05-11 22:53:48,34
Intel,mrxtgwf,Which means that it will MSRP for $2100.,hardware,2025-05-12 16:41:08,1
Intel,mrtfhqm,A 3090 is $1000 used so it better be less than that,hardware,2025-05-11 21:54:36,-2
Intel,mrtd32l,only if its better than a 5080,hardware,2025-05-11 21:40:44,-9
Intel,mryx2ew,2 5060 Ti' 16Gb will be way faster for AI workloads.,hardware,2025-05-12 19:55:22,0
Intel,mrtpxql,"32GB is great for local AI. It's the best a reasonably affordable card can provide atm (5090). Basically the more the better, if the 5090 has 48GB it would be an even better card, if it has 96GB like the RTX Pro 6000 then it would be better still.",hardware,2025-05-11 22:57:52,2
Intel,mrtbpkh,"With that specific card maybe not, but it could do two things to help gamers.   If it's successful, Intel's dGPU gets more cash infusion and leads to better cards down the road which might compete in the high end gaming market. Having another player is always a good thing.   It might force NVIDIA to compete by lowering prices so not to lose market share on the Ai and workstation side of things, which means the better gaming cards do get cheaper.",hardware,2025-05-11 21:32:51,12
Intel,mrwge56,The thing is that Intel dGPUs have a major architectural issue with the CPU overhead. Hopefully they'll be able to do something about it soon.,hardware,2025-05-12 12:16:40,2
Intel,mryijcr,Yes.,hardware,2025-05-12 18:42:34,2
Intel,msw6qa3,Womp womp,hardware,2025-05-18 02:31:06,1
Intel,mru0sfa,"There was a BMG-G10 die planned with 56-60Xe core die with 112-116mb of L4 Adamantine cache as MALL cache with a 256bit bus.   But the die was canceled during development along with L4 Adamantine cache, which was also planned to be used in Meteor Lake's igpu.  BMG-G10 would've likely been a bloated die if it targeted 2850mhz clock speeds like the B580. Less so if they targeted lower clocks.   We'll likely never see the G10 die, but we could still see BMG-G31 (32Xe core die)",hardware,2025-05-12 00:06:12,1
Intel,ms224gc,"with 3GB chips we may see that. It would take 416 bit bus width, which is unusual, but technically possible.",hardware,2025-05-13 08:07:16,1
Intel,mrtpu6i,If the internet is down for an extended period of time I doubt having access to an LLM will be high on the list of priorities.,hardware,2025-05-11 22:57:16,174
Intel,mruf4m4,Cheaper to just buy some acid if you want to hallucinate when the power is out.,hardware,2025-05-12 01:38:38,60
Intel,mrxgjjm,"> I have at least something I can run simple queries against.  I genuinely cannot see how that would be useful if the internet connectivity went down. Unless, what, you're using it for assists in coding?",hardware,2025-05-12 15:37:35,6
Intel,mrv0lp0,If it's a desperate scenario you'll survive with slow ram.,hardware,2025-05-12 04:11:46,7
Intel,mrtoekw,Wikipedia + local AI + voice control would be a cool doomsday support agent.,hardware,2025-05-11 22:48:21,1
Intel,ms3z1s8,"A well indexed RAID setup full of survival PDFs and guide videos, entertainment and independent power will do that job far more competently, at longer independent power up time.",hardware,2025-05-13 15:52:35,1
Intel,mrxnre3,"Â«Dear LLM, should I drink water or Coca Cola? Much appreciated !Â»",hardware,2025-05-12 16:12:55,1
Intel,mrvyzfa,LTE/5G card + failover router with recursive routing setup does not work ?,hardware,2025-05-12 09:53:11,0
Intel,mrufbhe,"Haha, when nvidia is selling $11k GPUs for workstations, even a large gap would still be thousands of dollars for the intel pro, but I mean it's all rumor anyway, conputex isn't for another week still haha.   I won't be holding my breath and pinching myself at every rumor in the next week lol.",hardware,2025-05-12 01:39:55,15
Intel,mrz8zuh,"There was a shipment of the chips (which Intel already fabbed) to one of the factories that makes the special edition Arc cards, but that's the last that has been heard. It's not much, but it is something.",hardware,2025-05-12 20:55:03,3
Intel,mrw1dum,"HA. Hahahaha, that's hilarious.",hardware,2025-05-12 10:16:54,5
Intel,mrywofw,They run models that need 32gb of VRAM way way faster than cards without 32gb of VRAM.  Though 2 5060Ti 16Gb will run them faster.,hardware,2025-05-12 19:53:27,1
Intel,mrxbu7f,It would atleast be able to run some models albeit slowly. Versus not being able to run at all on even high end GPUâ€™s like a 5080,hardware,2025-05-12 15:14:33,1
Intel,mruvpiw,Sorry I should have been more careful with my phrasing based on the leak culture for tech news.   I'm not an insider.     I predict this could easily sell for 1200 usd,hardware,2025-05-12 03:32:55,4
Intel,mrtg86n,A 3090 has less ram,hardware,2025-05-11 21:58:52,24
Intel,mrtpk8t,$700-$800 USD used.,hardware,2025-05-11 22:55:32,1
Intel,mrvj6t2,3090s at this point are all in danger of finally stopping working. Some have been in datacenters for what 5 years?,hardware,2025-05-12 07:03:31,0
Intel,mrte95e,nah that vram will be unmatched for the price,hardware,2025-05-11 21:47:28,36
Intel,mrtlb1x,"This card is for AI, not gaming.  I do want a gaming version, but *that* would have half the VRAM and can't be $1200.  Intel isn't getting into the GPU business to save gamers.",hardware,2025-05-11 22:29:04,22
Intel,mrtqj5o,not everything is about gaming if there decent for AI they will fly off the shelfs.,hardware,2025-05-11 23:01:37,1
Intel,ms2216h,Battlemage was a big improvement over Alchemist in architectural adaltation. Im hoping Celestial will also be a big improvement and reduce the overhead.,hardware,2025-05-13 08:06:20,3
Intel,mru31z2,Didn't you know LLMs is one of the basic human needs?,hardware,2025-05-12 00:20:50,88
Intel,mrwyzoy,Lol but how else can we justify having a card with 32gb of vram when ones with 12-16 crush virtually any game these days...,hardware,2025-05-12 14:09:08,3
Intel,mrzzsst,"The person you replied to might live in a rural area or somewehre with frequent outages. You're making it as if the only possibility for losing internet access is for the internet to go down globally or some shit.   I've solar/batteries/generators for my house because outages are frequent. I also have two internet providers because electrical posts get crashed frequently by trucks (maybe not thaaat frequently, but 3-4 times a year, at least) and that leaves me with no internet sometimes for days on my main provider.",hardware,2025-05-12 23:21:05,7
Intel,mruigh9,Unless it's shockful of useful survivalist tips and strategies... Which it is.,hardware,2025-05-12 02:00:19,-14
Intel,ms20olq,I think its debatable. A local database LLM could provide a lot of useful infromation that you wouldnt otherwise think of asking in a survival scenario. Do you know how to home-make water filter for drinking from a local river? You could ask LLM that.,hardware,2025-05-13 07:52:34,0
Intel,mrueoxc,", until it â€œhallucinatesâ€ some fatal advice.",hardware,2025-05-12 01:35:42,29
Intel,ms26qfr,That 11K GPU has 3X VRAM and 2X performance.  A lower end is likely not leaving too big a gap.   More importantly is how the Intel pro compares with AMD pro,hardware,2025-05-13 08:55:57,2
Intel,mrywwwv,2 5060Ti 16Gb will run them faster and probably for less money.,hardware,2025-05-12 19:54:37,1
Intel,mrtu4ms,"In addition, used prices are normally lower than for similar new items, accounting for the relatively higher risk involved and shorter (on average) remaining lifespan.  For example, you can find a used 4060 8gb for significantly cheaper used on Ebay than the same card new on Newegg.",hardware,2025-05-11 23:23:59,2
Intel,mryxirb,"2 3090's have 48Gb of VRAM, AI models don't really care how many cards they run on, the cards don't even need to be in the same machine, network is fine.",hardware,2025-05-12 19:57:39,0
Intel,mryx7o7,Why would they stop working?,hardware,2025-05-12 19:56:06,0
Intel,mrtoxpr,They killed the Flex line. Gaming is the primary market for this class of GPU.,hardware,2025-05-11 22:51:38,-8
Intel,mrts8jl,when did i say anything about gaming?,hardware,2025-05-11 23:12:10,2
Intel,mru8b0g,Why would you need to prep human needs over your internet being down?,hardware,2025-05-12 00:54:39,5
Intel,mrvz38g,"I read guns were basic human needs, according to /r/preppers  ?",hardware,2025-05-12 09:54:15,-5
Intel,ms00znq,"Cool, but what does that have to do with *whether or not you can locally host a bigger LLM*?",hardware,2025-05-12 23:27:17,5
Intel,ms3zhja,"Yeah, but for that job you're better off with a RAID made of the cheapest reliable HDDs you can find full of survival guides and entertainment. Thing will stay up longer too once it has to run on local generators.",hardware,2025-05-13 15:54:40,1
Intel,mruspyr,Just buy some survival book lmao. If you are in a survival situation you probably dont have electricity either.,hardware,2025-05-12 03:10:25,36
Intel,mrv3kt7,/r/BoneAppleTea,hardware,2025-05-12 04:36:33,7
Intel,mruiky4,If it gets them right.,hardware,2025-05-12 02:01:07,11
Intel,mrw0u1b,It's useful for having a tool that actively lies to you at complete random and will likely get you hurt or killed just as much as help.,hardware,2025-05-12 10:11:35,5
Intel,ms2b9hd,"You could also purchase survival books that wouldn't use precious electricity, and they wouldn't hallucinate and lie to you like the AI could.",hardware,2025-05-13 09:42:54,4
Intel,ms219bk,Thats why the advice is filterd by the human intelligence listening to it.,hardware,2025-05-13 07:58:25,1
Intel,ms402bh,"And given how juice-frugal modern HDDs are, a local archive of wiki and survival data on an in house NAS RAID and generator, connected to a laptop running in low power mode will stay working much longer.",hardware,2025-05-13 15:57:27,1
Intel,mruubgn,well you may want to double check the survival knowledge before attempting it XD,hardware,2025-05-12 03:22:23,-2
Intel,mryxmww,2 of these would have 64gb of VRAM.,hardware,2025-05-12 19:58:13,1
Intel,mrtq8tt,"Hey if Intel want to release this 32GB B770 in the gaming segment where it's going to be judged primarily on how well it renders frames (and has to be priced accordingly) then they can go nuts. I'll be happy to consider it as an option.Â    I just think ""Arc Pro"" and 32 GB indicates a different goal and different customer in mind.",hardware,2025-05-11 22:59:50,6
Intel,msjj3mo,"To be fair, an LLM could be a pretty nice tool if shit hits the fan.   I have a 8kw generator and solar panels that will work as backup power if things goes sideways. Of course I have books too, and a full copy of Wikipedia on a small usb drive that I refresh once a year. Oh, and I live pretty remotely.   LLMs are not a substitute for actual knowledge, but in a scenario where the internet is turned to mush, time is of essence and maybe it can allow me to do stuff I wonâ€™t have time to fully learn on my own, especially for one-off tasks. I donâ€™t expect to get an LLM to guide me through how to remove my own spleen, but it would probably be able to help me with spot knowledge. Could even load it with schematics, manuals, books on a myriad of subjects.",hardware,2025-05-16 00:07:16,2
Intel,ms20hk9,"I dont know whats in that sub, but given what scenario typical preppers think will happen guns would be useful to hunt food if nothing else.",hardware,2025-05-13 07:50:32,3
Intel,ms03tdr,You are questioning the other person's priorities based on your perceived impact of an internet outage.Â    Maybe LLMs are their hobby and they experience frequent internet outages. Why would it be so weird to want to run LLMs when the internet goes down.,hardware,2025-05-12 23:42:37,6
Intel,ms20te8,Most pre-made survivalist packs now come with a portable solar panel. Good enough to charge stuff like phone/flashlight/radio.,hardware,2025-05-13 07:53:55,1
Intel,mrv5lf0,>If you are in a survival situation you probably dont have electricity either.  You could buy a single single solar power station and couple of panels for not much more than a grand. Which would be able to run a desktop PC for a couple of hours a day.   Sure wont be enough panels and storage to run it continuously. But makeshift off grid solution like that are getting crazy affordable these days. It's starting to get to the price point where it should simply be something you should have in some form if you live somewhere where you can expect power outages every now and then. Since then you can use it for things like running a freezer and basic lighting.,hardware,2025-05-12 04:53:52,-8
Intel,ms3zqgk,Or even just a very basic RAID full of high quality survival data and a bit of entertainment if you want to go high tech.,hardware,2025-05-13 15:55:51,1
Intel,mrw190o,"And thus is the crux of problems with **ALL** LLM projects. Either yo already have the resources and experience enough to double check its work...in which case, it'll have been faster to just look it up yourself **without** the LLM. Or, you do not and are trusting in something that lies or makes up sources at literally any time and you are putting ticking time bombs into your day-to-day life, work, etcetera.",hardware,2025-05-12 10:15:36,8
Intel,mrxgrwe,"You found an issue of the ""Wasteland Survivor's Guide!"". You permanently take 5% less damage from insects.",hardware,2025-05-12 15:38:42,3
Intel,mrttdny,"> I just think ""Arc Pro"" and 32 GB indicates a different goal and different customer in mind.  Agreed, but there are other markets than LLMs. And my main point was their client dGPU line was driven primarily by gaming and productivity, not AI. As for their AI chips, well, who knows what's going on with that clusterfuck.",hardware,2025-05-11 23:19:20,-1
Intel,ms04d9k,Are you taking into account that this person is a self proclaimed data hording prepper and the context that is implied?,hardware,2025-05-12 23:45:36,0
Intel,mrwbyoe,"There's a dozen+ more important things you'd want to use your limited electricity for in such an event, like a fridge/freezer, cooking, heating water, hell even much more efficient computers like phones or laptops.",hardware,2025-05-12 11:44:57,10
Intel,mrvukxc,"Why on earth would you run a desktop PC in an outage. It's insanely inefficient and lacks portability. A laptop is smaller, portable, and uses less power. If you care about ""knowledge"" then just download a bunch of information and guides in advance, and make a copy on a separate flash drive if you want redundancy.",hardware,2025-05-12 09:06:14,11
Intel,mrwb2il,"an AI could help narrow down what you're looking for  Imagine someone is sick with a couple of different symptoms  Searching every disease wikipedia page to try and find a match could take ages  But an AI could help point you in the direction of the most probable ones  And then if you wanted, you could just double check those pages yourself",hardware,2025-05-12 11:38:12,1
Intel,ms40lcc,"Like, HDDs are a fraction of the power to work of *any* GPU that could run one of those, and frankly, in that case you need your wits about you anyway.",hardware,2025-05-13 16:00:01,1
Intel,mrx2xog,"It wold help you find relevant articles quicker, I don't need it to try and summarize or distill information into easy to read summaries.",hardware,2025-05-12 14:29:48,0
Intel,mrx2sf8,"The point is to use it as a tool to find the information you need, not to regurgitate information it's been fed, at least in this example.",hardware,2025-05-12 14:29:04,4
Intel,mt1l8n4,"An LLM is meant to construct plausible sentences that it.  â€œHallucinationsâ€ are just a way to explain to the layman that an LLM has ZERO concept of what is logical, factual, or ethical. It just builds sentences but people treat it like itâ€™s the librarian to the grand archives.",hardware,2025-05-19 00:30:28,1
Intel,n7m14u4,"If an LLM told you to jump off a bridge, would you?",hardware,2025-08-08 14:58:43,1
Intel,ms21eyi,I use LLMs to help with creating my TTRPG. The only doublechecking needed is to check against my own homebrew fantasy.,hardware,2025-05-13 08:00:00,0
Intel,ms0au0j,Why is it relevant what they are?,hardware,2025-05-13 00:21:51,4
Intel,ms20x3r,We dont know that in the case of that user those arent already accounted for.,hardware,2025-05-13 07:54:58,1
Intel,ms0mo7v,Right? I saw someone's recommendations for downloading all of Wikipedia and all maps onto a $30 phone that could be recharged from a wood burning lantern that used the heat to funnel power to a USB port.Â  All that was way less than $500.   This guy's LLM is the stupidest thing I've ever heard of. Reaks of cryptobro mindset.,hardware,2025-05-13 01:31:35,1
Intel,mrwzxzx,We already have mayo clinic and doctors hate it lol,hardware,2025-05-12 14:14:15,7
Intel,ms40wxx,a good PDF and CTRL-F does that at a fraction of every price.,hardware,2025-05-13 16:01:35,1
Intel,n7lzok7,This is the answer to all the snarky BS people want to say   https://www.reddit.com/r/hardware/comments/1kkb32p/intel_might_unveil_battlemagebased_arc_pro_b770/msjj3mo/,hardware,2025-08-08 14:51:54,1
Intel,ms213gl,that sounds needlesly complicated. You can buy portable solar panels for less that are good enough to charge phone/flashlight/radio.   That guys LLM sounds more like what person i know does. He works on ships that cross the atlantic ocean. Internet access is still rare there. So he pre-downloads enough stuff for local use. Power isnt an issue on a ship.,hardware,2025-05-13 07:56:47,1
Intel,mrxk4w0,"Which is understandable in our current situation  But this comment chain started as a discussion of a doomsday scenario, where very few or no doctors might be available   Google and mayo clinic servers would most likely be down, so i think a local AI would probably be better than nothing",hardware,2025-05-12 15:54:56,1
Intel,ms21bwc,Now imagine a situation where doctors are not available and wont be available. Would you rather have mayo clinic or not?,hardware,2025-05-13 07:59:09,0
Intel,msa1160,No risk of them pulling that off but the stock correction and long term damage to software stacks from this bubble is gonna be extremely bad in its own right.,hardware,2025-05-14 14:45:11,1
Intel,ms4kwze,"I guess you could do that but just the English wikipedia is about 90GB of xml files, could be quite time consuming.",hardware,2025-05-13 17:38:22,1
Intel,ms21vp7,"The thing I was referencing was in like, 2015 or shortly after. I dunno what the status of portable solar was back then. In any case, he was an avid long-distance hiker so it was tailored for his use case. He walked during the day, burnt some tinder into his fire lantern while camping at night to recharge his phone.   So, yeah. Add in specific use case scenarios and the arguments make more sense. Like your friend's situation.",hardware,2025-05-13 08:04:47,1
Intel,ms4l6zf,If doctors aren't available I think I'd have bigger problems to worry about than getting an opinion from a hallucination bot,hardware,2025-05-13 17:39:43,1
Intel,ms51jh6,"Then get a inexpensive but quality PCIE 3.0 SSD for it to live on, maybe a cache as part of the RAID.",hardware,2025-05-13 18:57:23,1
Intel,ms2uqdp,"so these are RDNA 5 / UDNA and Celestial, right?",hardware,2025-05-13 12:17:58,10
Intel,ms8wh16,FYI post mentions SWC for GFX13 (UDNA). [The patent for the Streaming Wave Coalescer](https://patents.justia.com/patent/20250068429) mentions it reduces thread divergence. This is AMD's implementation of DXR 1.2's SER functionality and will be used to boost thread coherency *for thread incoherent workloads like* ~~in~~ neural rendering and path tracing *for example similar to* ~~just like~~ NVIDIA's Shader execution reordering.,hardware,2025-05-14 10:34:12,2
Intel,ms9ywoq,"Posts like that make me wonder if it'd be still worthwhile for me, at end of this year (5-4+- months), to buy a 9070xt or not?   These posts are speculative, so nothing to make decisions on, I just wonder if it'd take them a year, or 3 years, to get to actual production...",hardware,2025-05-14 14:34:41,1
Intel,ms2ca98,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-05-13 09:52:52,0
Intel,ms2eyp3,I need Celestial in my PC for free now!!!!!! ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸ˜ðŸ˜,hardware,2025-05-13 10:18:14,-32
Intel,ms2zykv,"This is Druid, the successor to Celestial.",hardware,2025-05-13 12:50:45,38
Intel,ms2vtwe,Xe4 is Druid.,hardware,2025-05-13 12:25:05,30
Intel,ms356t9,The title literally says Druid.,hardware,2025-05-13 13:21:12,14
Intel,ms98mwt,Apparently does it automatically and not just ray and path tracing.  Hopefully they'll do a 6090/90 Ti competitor.,hardware,2025-05-14 12:05:47,2
Intel,ms2qdxl,Less drugs might help you,hardware,2025-05-13 11:48:47,22
Intel,ms3pv1x,Druid is no specific IP. It's just whatever next-next gen dGPUs would align with.,hardware,2025-05-13 15:08:21,5
Intel,ms3lulh,"My bad, so these are Druid and RDNA 6/UDNA 2?",hardware,2025-05-13 14:48:34,3
Intel,ms9b07a,"Edited the above comment for less confusion and indeed any workload with thread incoherency would benefit from it.  Automatic, really? So games devs don't have to implement it unlike NVIDIA's SER? If it ""just works"" then I sure hope AMD has found a way to address the reordering overhead issue or only enable it when certain criteria are met.",hardware,2025-05-14 12:21:19,2
Intel,ms5mthz,"It was a joke post. â˜ ï¸   If I don't buy a 9070XT, then I'm going for Celestial myself",hardware,2025-05-13 20:40:22,1
Intel,ms39oms,"Troll making fun of people who want cheaper gpus. Pri""cks. The only actual circle j""rk ive seen on reddit regarding pricing",hardware,2025-05-13 13:46:18,-10
Intel,msetwcn,Intel Alchemist has automatic SER. Maybe there is a reason why their GPUs have worse overalll overhead,hardware,2025-05-15 07:25:45,3
Intel,ms9mdow,Well the thread with Kepler on Twitter mentioned it. Would be damn cool and help go for the performance crown hopefully.,hardware,2025-05-14 13:29:07,2
Intel,ms9z7nc,"Your 'joke' fell pretty flat there, mate. Screaming nonsense and spamming emojis isn't much of a joke..",hardware,2025-05-14 14:36:12,3
Intel,ms3y5dk,"> So saying Druid uses Xe4 is correct for dGPU   Not even that. It's an assumption. Not an unreasonable one, but if Intel skips Xe3 for Celestial, then Druid could be Xe5, for example.Â    For that matter, Druid could not exist at all even if Xe4 lives.",hardware,2025-05-13 15:48:13,12
Intel,ms6uyan,"To make it easier to understand, it is similar to AMD: RDNA1/2/3/4 is the underlying architecture and Navi 1x,2x,3x,4x are the implementations.",hardware,2025-05-14 00:43:35,3
Intel,msg0n4b,Interesting. Someone also told me that SWC is a lot closer in design to Intel's TSU than NVIDIA's SER.  The inconsistent gaming performance of Battlemage could be related to a broken TSU implementation. Might explain why B580 in new games (as per HUB reivew) best case performs like a 4060 TI 16GB and worst case 20% slower than a 4060. Something isn't adding up.,hardware,2025-05-15 13:20:12,1
Intel,ms9orh1,Indeed another halo gen would be nice. Haven't seen anything competitive since 6950XT. At the very least hoping on UDNA being a massive architectural upgrade clean slate as rumoured last year.,hardware,2025-05-14 13:42:19,2
Intel,mr1nx4v,"It's an interesting read, clearly shows that the gap at higher resolutions with memory intensive features (upscaling ,frame gen, rt etc) is very small, however it does appear that that headline is a touch disingenuous, as the 5060ti does appear to handily beat the B580 in lower resolutions, basically all scenarios where the 8gb frame buffer isn't the bottleneck...  Can't fault the conclusion though that this card should never have been made with an 8gb option or it should have been priced as a much more direct price competitor with the b580 so customers could choose whether they wanted the better 1080p raster performance or the better 1440p and 4k upscaled performance.",hardware,2025-05-07 11:11:46,142
Intel,mr1o0x0,"I read the article, they mention they cherry picked examples - which is fine and recommend neither.  Then they say esport players shouldnâ€™t pick one up either without giving an excuse.  Why? Esport players typically play on lower res with low settings to maximize fps, cpu is more prevalent in these titles. No serious esport game (CS, Valorant, Dota, League) is reaching 8GB on 1080p low  For CS atleast even if you max it out on 4k its not gonna go beyond 4GB",hardware,2025-05-07 11:12:34,59
Intel,mr323e2,And more expensive too.,hardware,2025-05-07 15:53:43,12
Intel,mr1nbt8,This gen Nvidia is a massive joke,hardware,2025-05-07 11:07:14,42
Intel,mr207ys,Would it have been cheaper to make it 192bit 12GB GDDR6? Even 160bit 10GB GDDR6 might be just enough to be acceptable.,hardware,2025-05-07 12:36:30,6
Intel,mr2r57c,And here I am perfectly happy with my humble RTX 4060 ti 8GB,hardware,2025-05-07 15:01:01,1
Intel,mr2t8wu,"Correct me if I'm wrong, but if I'm in the market for a low to mid range card, it's VRAM volume the first consideration?",hardware,2025-05-07 15:11:13,2
Intel,mr2ccof,oh this [guy](https://www.techspot.com/community/staff/steve.96585/) wrote this?  no wonder the headline is bs & clickbait,hardware,2025-05-07 13:46:06,-9
Intel,mr31ues,"The 5060 Ti is also 180W vs. the B580's 190W. All things being equal you'd expect it to be slower, but it sounds like it's only slower because of the RAM (and you're getting the cheap version at 8GB.)",hardware,2025-05-07 15:52:31,-7
Intel,mr20uo4,Title definitely comes off as clickbaity. Of course a card that isn't VRAM bottlenecked will be faster than one that is. 5060 Ti 8 GB will get destroyed by any card with higher VRAM than it under certain workloads.,hardware,2025-05-07 12:40:25,44
Intel,mr3z07q,I feel like it is only 8GB to justify Nvidia's lack of vram across the stack.,hardware,2025-05-07 18:30:37,10
Intel,mr6lfsh,"HUB is very clickbaity, with their focus on ""elitist"" PC gamers. Basically Steve advertises himself as someone who only cares about high refresh rate gaming, to the point where I think he's said more or less that anything under 240fps at 1080p is unacceptable for him now.",hardware,2025-05-08 02:57:04,3
Intel,mr1r2ww,Yeah it's really just confusing buyers even more. The B580 literally came 10% worse against the 4060 over a 50 game sample size.,hardware,2025-05-07 11:35:16,-1
Intel,mr1th6j,Sensational article headline that nobody reads past?  Can't be...,hardware,2025-05-07 11:52:12,-4
Intel,mr23gnl,"Their excuse was that esports players may want to play a single player game occasionally. If they choose to buy the 8 GB variant of the card they will have a bad experience in said game (assuming it's a modern AAA title with a high VRAM requirement). At that point they're screwed so they will have to limit themselves to older single player games and esports games only, which is something nobody spending $400 for a GPU should have to go through. Obviously there are people who exclusively play multiplayer games so the 5060 Ti 8 GB would be totally fine for their use cases but I'd say they are of the minority.",hardware,2025-05-07 12:56:04,22
Intel,mr1pekm,"Idk if I ever met an eSports player that never played other games.   Real e-sports player buys expensive GPUs for higher refresh rate because they can afford it and they have to. Broke gamers buys these to mostly play competitive games, then also play single-player games just for fun.",hardware,2025-05-07 11:22:58,28
Intel,mr3l3ry,"2 reasons:   1. Being an esports gamer does not mean that is all you will ever be for the useful life of the card. The price difference of 60 bucks is worth it to give yourself options.   2. Resale value. When you compare both cards TCO, the 16 GB model may end up cheaper overall (as observed by the 4060 ti versions).",hardware,2025-05-07 17:25:04,3
Intel,mr201ux,> they mention they cherry picked examples - which is fine  Why is that fine?   If it made NVIDIA look good everyone would be mad that they cherrypicked.,hardware,2025-05-07 12:35:27,-5
Intel,mr3cgz0,The joke is on all of us.,hardware,2025-05-07 16:44:12,14
Intel,mtibq6t,"The MSRP prices aren't bad, it's just the actual prices suck, but that applies to literally anything rn, including amd gpus, and even Intel b580s are going for 400-500.   Dlss 4 and frame Gen is surprisingly good, but they're lacking in raster performance. Think of the 50 series as a refresh of the 40 series, which wouldn't be bad at all",hardware,2025-05-21 17:32:49,1
Intel,mr28fyv,"That would require an entire new die, or cut down the one used in the 5070 which is clearly not going to be cheaper. The alternative solution would be staying on 128 bit bus but using the illusive 3GB memory chips.",hardware,2025-05-07 13:24:35,10
Intel,mr7szxk,Personally I think the 5060Ti should just be a 16GB card with the 128 bit bus.  The 8GB variant should have never existed at all.  Below that the 5060 could have been a $330 card and NV had the option of using 3GB GDDR7 chips to make it 12GB on a 128 bit bus. If supply or cost of those chips reduced the margins such that NV did not like that option then they could have gone 96bit 12GB with the 5060. It would be a good upgrade over the 4060 and they could have commanded a higher ASP than the 8GB 5060 model they are going to end up selling. It also would have done pretty well overall with reviewers.  Still NV chose not to so it is what it is.,hardware,2025-05-08 09:28:59,1
Intel,mr3exrh,"Not for long, that's the problem.",hardware,2025-05-07 16:55:50,1
Intel,mr30s6r,"Not really, always check benchmarks. VRAM alone in isolation isn't the only number to bank on.",hardware,2025-05-07 15:47:25,11
Intel,mr78aee,"If you had asked this question four years ago, you'd be told that 8GB of VRAM was the floor for a mainstream card. 16GB of system memory was also ideal. You could get away with 6GB at the time, which is why the RTX 2060 6GB was so popular.   Today those requirements are changing quickly as UE5 and other contemporary game engines are switching their attention to current and next gen console specs.   If you want to run games with full texture quality at 1080p or 1440p, 12GB of VRAM and 32GB of system RAM is required. 16GB of VRAM is ideal if you're playing at 4K.   While you can get away with less (and games will still run), the presentation will be ass and the performance will be lower.",hardware,2025-05-08 05:55:43,3
Intel,mr8nnns,"No. How well a card performs is totally separate from vram. VRAM is one of those things where you either don't have enough and it will cripple your performance or you do and everything works properly. Performance itself is determined by the actual GPU though, which you need to look at benchmarks to figure out.   Like, adding 32GB of VRAM to a low end card would not make it perform better.",hardware,2025-05-08 13:17:39,1
Intel,mr4l56t,"That's kinda like asking whether the amount of windows is first consideration when picking a house, in a thread about a house that doesn't have any at all  No, in a typical situation VRAM is not the first consideration, especially in low-mid range. But it can very quickly become one if the card in question has a ridiculously small amount, which this one does",hardware,2025-05-07 20:17:58,0
Intel,mr2dsq5,why what's wrong with him,hardware,2025-05-07 13:53:51,13
Intel,mr2feo0,You donâ€™t actually believe this right? Nvidia doesnâ€™t care about gaming GPUs at all. 50 series is just their low effort attempt at throwing some scrap chips at their roots in gaming. This could very well be the last gaming GPUs we get from nvidia for a while.,hardware,2025-05-07 14:02:19,8
Intel,mr487io,No I wouldn't expect it to be slower considering the B580 released as a 4060 competitor lmao. We already saw with the 4060 ti the massive difference 8 vs 16 gb can make even with the same exact GPU core config.,hardware,2025-05-07 19:15:03,8
Intel,mr2agrp,>Â  Of course a card that isn't VRAM bottlenecked will be faster than one that is.Â    There seem to be no shortage of people who can't comprehend it though.,hardware,2025-05-07 13:35:52,27
Intel,mr239j1,any GPU below 10 gigs is a dinosaur as a new gaming card.,hardware,2025-05-07 12:54:53,36
Intel,mrf8dsh,"The point being, of course, that these ""certain workloads"" may easily include popular recent games at reasonable settings.",hardware,2025-05-09 14:14:40,3
Intel,mr7mwtg,HUB as in hardware unboxed? This article is [techspot.com](http://techspot.com),hardware,2025-05-08 08:24:01,0
Intel,mr1z90l,Other way around,hardware,2025-05-07 12:30:27,21
Intel,mrcgock,It be,hardware,2025-05-09 01:12:27,1
Intel,mr1pny7,"Depends on the circle I guess. When I used to play CS religiously I had friends who just had 1 game in their account - CS, theyâ€™d have mid end PCâ€™s and would only play CS. Maybe diverge to other esports games like R6 Siege, Overwatch but mainly just play CS.  Thereâ€™s a lot of people like that, maybe not in North America but definitely in Europe and Asia.",hardware,2025-05-07 11:24:54,10
Intel,mr2n5t5,"Completely missed the point.  When you say ""Esports gamers shouldn't buy this card"" the obvious implication is that it is bad FOR ESPORTS, not just in general.  That's like saying ""Olympic swimmers shouldn't eat this food"" and then finding out that the food increases the general populations chance of getting diabetes at 65.",hardware,2025-05-07 14:41:31,6
Intel,mr1tufv,Real esports players play esports games (known for very low specification requirements) on lowest settings (for maximum clarity and fps) so there is NO way in hell you would even consider buying latest GPU for esports... how much does fucking SC2 or CSGO require LOL,hardware,2025-05-07 11:54:45,8
Intel,mr209ze,The rationale they used - a $400 card should not be losing out to an older $300ish card.   Even ignoring the 4k results the 5060ti8g lost at 1080p to B580 in some games - which is inexcusable,hardware,2025-05-07 12:36:51,11
Intel,mr5603q,The point is today's worst case is a few years from now's typical case. In other words 8GB cards are going to age like milk.,hardware,2025-05-07 21:59:36,1
Intel,mraeo86,And you bet your ass the execs are all laughing their asses off at you while the billions keep raking in,hardware,2025-05-08 18:29:04,4
Intel,mr8mu55,"I mean yes it's a joke because of pricing and bad generational gains, not because they aren't still the best GPUs. It's not like 5000 series is worse than 4000 series, it just didn't make the gains we'd have hoped.",hardware,2025-05-08 13:13:01,1
Intel,mr774ha,"They're not really elusive, Nvidia is just consuming all of the 3GB GDDR7 supply for their enterprise AI products, and reserving a tiny portion for the RTX 5090 laptop GPU.",hardware,2025-05-08 05:44:54,5
Intel,mr6phl7,"?       I've been running it for a year and a half now - lots of VR games (Alyx, Star Wars Squadrons, Subnautica) and Cyberpunk, Doom Eternal, Robocop Rogue City - and have been very happy with the results.     But I've been playing since the late 70s so I'm pretty easily pleased. I don't really care about 4K or 120 fps.",hardware,2025-05-08 03:24:19,5
Intel,mr7r6ft,This. There are many gray spots like framgen vram requirement. Texture popin or failure to load because of out of memory. Bus widths cache sizes of architecture etc.,hardware,2025-05-08 09:09:43,2
Intel,mr3f5kz,"*Nothing. Stans just get mad when people objectively point out the logical flaws in their emotional reasoning. I truly don't understand why people get so defensive about their purchases or brand fandom.   Nvidia is a multi-trillion dollar company, they don't need simps.",hardware,2025-05-07 16:56:51,-1
Intel,mr40cu7,Bullshit. It's a $10B/year industry for them.,hardware,2025-05-07 18:37:09,5
Intel,mr4uu8v,"The 4060ti was a 160W card. Saying a 190W card competes with a 160W card... they're very different. 180W is closer but it's still significant.  And yes, of course, if your workload needs more than 8GB of RAM it's going to be slower, but that's obvious and it doesn't negate it if the 180W card is faster than the 190W card despite using less power (but it can only operate on 8GB of RAM.) And sometimes that's all you need so it's just better.",hardware,2025-05-07 21:02:59,-4
Intel,mr4dtmq,ok.... but should it be 50% more if it's going to lose so easily? Like I think people's issue with this card is it's MSRP $399. After you get the OEM's involved this is a $450 card verse a $250 card.,hardware,2025-05-07 19:42:27,4
Intel,mr28q6w,"I would say 12 GB, honestly.  The 1080 Ti had 11 GB. Eight years ago.",hardware,2025-05-07 13:26:07,39
Intel,mr2j43f,"Ehh even 10 GB is pushing it. I'd say it's just barely enough to use maximum texture quality settings with no RT, based on what I've heard from 3080 owners. 12 GB is the new minimum for RT + maximum textures. 16 GB is optimal for all of that plus path tracing.",hardware,2025-05-07 14:21:21,13
Intel,mr7sbg6,This is just the written version of the video that was posted a few days ago.,hardware,2025-05-08 09:21:52,9
Intel,mr1zwly,"Can confirm (my benchmarks are only 10 raster + 5 ray traced though). I have the 4060 at 79% of the b580 at 4k, 89% at 1440p and 96% at 1080p on average. B580 ranks 31 out of all gaming gpus, 4060 at 37.6",hardware,2025-05-07 12:34:32,18
Intel,mr5i6ho,"For the price it's bad for eSports, none of the features are used for competitive games and the price is garbage.  Just buy something used for as cheap as possible then, it will probably run everything anyways because you set everything to medium or lower.",hardware,2025-05-07 23:08:04,6
Intel,mr619hg,"> ""Olympic swimmers shouldn't eat this food""  A better example would be a marathon runner running on  flip-flops for the least amount of weight, or a cheap but heavy running shoes.   If you're doing things competitively and sometimes for a living, why not buy the best gear possible to give the most advantage? The best runner I've seen are running on Carbon Fiber shoes that costs as much as a 5060, they yield the most advantage.",hardware,2025-05-08 00:58:20,1
Intel,mrg9s73,You get my point or you still missed it?,hardware,2025-05-09 17:25:08,0
Intel,mr2p22m,You buy the best hardware possible to get the most FPS possible.,hardware,2025-05-07 14:50:49,13
Intel,mr608cg,"Yes you're right, maximum FPS, it's like 500fps on games in 1080p.   There's also newer gen eSports title such as Overwatch 2, new R6 Siege, Marvels Rivals, The Finals etc. Is a 8GB card enough for these?   I game at these titles on the lowest settings, there is a point where even I have problems seeing people at distance and the best settings I used are around medium for some level of details, but I lose some FPS.",hardware,2025-05-08 00:52:14,2
Intel,mr1ufqe,"In a million gamers, how many of those eSports players are exclusively playing eSports games and absolutely nothing else? Assuming costs isn't a factor for them.",hardware,2025-05-07 11:58:49,3
Intel,mr3cm99,One with a notoriously... iffy... software stack at that. edit: I am talking about the *Intel Arc* here!,hardware,2025-05-07 16:44:55,1
Intel,mr3s1oh,What I meant was comparing VRAM across architecture/generations. Say a 6gb RAM card from 2025 would likely outperform an 8gb RAM card from 2014.   So VRAM alone isn't a clear indicator whether one card is sureshot better than another. Just the way Ghz isn't the only indicator whether a CPU is better than another. It's a major component but not only component that matters.   Which is why you should check benchmarks before deciding on whether a GPU is better than another.,hardware,2025-05-07 17:57:23,3
Intel,mr58es7,"The CPU processes all the gamesâ€™ physics and logic per frame. More powerful CPUs can process more physics and logic per second.   GPUs process the graphics per frame and more powerful GPUs can process higher resolution and higher quality textures more times per second.   If you play at 1080p, the GPU can be set to higher textures since it doesnâ€™t have to work as hard as 4K so it can process graphics faster. This allows you to get higher fps but the fps may be limited to how powerful the CPU is. This is â€œCPU limitedâ€ or the CPU is the bottleneck.   The opposite would be â€œGPU limitedâ€. No e of these terms are objective, they are just a result of GPU/CPU combos and monitor resolutions. Itâ€™s all just preferences, perceptions and projection.",hardware,2025-05-07 22:12:40,0
Intel,mr4yzl5,"Intel's GPUs have needed more power to compete for a while now thats nothing new. The A750 and A770 both consume over 200 watts but are within a 4060 performance tier. The base model RX 7600 was in a 4060 performance tier while using more power than the 4060 ti. The watts don't say a lot besides the fact that nvidias 40 series cards were extremely power efficient.  Maybe for a card aimed strictly at 1080p 8 gb could still be mostly okay but I consider a 5060 ti to be a 1440p capable card and the GPU surely has good enough raw power for 1440P, it just seems the vram is holding it back",hardware,2025-05-07 21:23:19,4
Intel,mr52g2g,No it frankly just shouldn't exist. It is under-specced for today's requirements and for its own capability.,hardware,2025-05-07 21:40:59,4
Intel,mr2ejhs,"12GB is plenty of headroom for 1080p and 1440p gaming. Thereâ€™s always going to be horribly optimized games that use too much vram. I really wish people would learn how games and vram work, to cut down on some of this repeated vram fear.",hardware,2025-05-07 13:57:45,29
Intel,mr4dx14,and Nvidia learned their lesson. They'll never make that mistake again.,hardware,2025-05-07 19:42:56,1
Intel,mr2q6zv,3080 owner here and can confirm. Still not giving up on the card given that anything that provides a meaningful upgrade is stupid expensive.,hardware,2025-05-07 14:56:24,6
Intel,mr2y5c6,"10 is 6, 12 is 8, 16 is ten.",hardware,2025-05-07 15:34:45,0
Intel,mr7uuwn,"Oh yes, I had no idea Steve was a contributor for Techspot. Makes sense that the headline is a touch click baity. Still an overall valid point though.",hardware,2025-05-08 09:47:59,4
Intel,mr9f65t,"As resolution increases, the GPU takes on more of the load while the CPU becomes less of a limiting factor. The CPU-related bottlenecks that hold back the B580 gradually disappear, allowing its true performance to emerge. Thatâ€™s why, even without running into VRAM limits, the B580 can outperform the 4060 at 4Kâ€”and might even surpass the 5060. This is exactly why those who hype up X3D CPUs tend to avoid comparing gaming performance at 4K resolution.",hardware,2025-05-08 15:37:38,2
Intel,mr2qa29,"....on what settings, what resolution and which game XDDDDDDDDDDDD",hardware,2025-05-07 14:56:49,-7
Intel,mr1xxcd,A surprising amount of people play 1 game only or only dabble in games outside of that.,hardware,2025-05-07 12:22:02,11
Intel,mr24eob,Ok so then are you an esports player or everything player?...,hardware,2025-05-07 13:01:31,3
Intel,mr3eos3,"A lot of us LoL players almost literally only play LoL. Ofc we'll play something different here and there, but LoL players in general don't seem to care much for other games until they abandon LoL. Even then, they're likely to go to another esports title with low requirements because LoL players have a strong competitive itch. Look at Tyler1 moving to chess for an extreme example lmao. The most graphically demanding games tend to be single player or coop and those are generally less attractive to LoL players.",hardware,2025-05-07 16:54:40,2
Intel,mr2f745,"Basically not even Nvidia can path tracing unless few cards, not the same thing",hardware,2025-05-07 14:01:11,2
Intel,mr3t9ua,"Oh, I see.   Agreed on checking benchmarks for the ultimate answer.",hardware,2025-05-07 18:03:11,1
Intel,mr7sahn,"Agreed but think of it like this GTX 3050 is 8gb VRAM and 3070 is also 8gb VRAM, does that mean their performance will be similar?   The above answer was for someone who doesn't have a lot of idea about GPUs and was looking for a an easy way to identify the capability of a GPU.   What you're discussing is moreso about balancing a build between CPU and GPU budget allocation as per resolution. Which is perfect and valid, but not related to the question asked by the user as to how to identify a better GPU.",hardware,2025-05-08 09:21:34,2
Intel,mr2g7v8,"plenty might be a bit strong, i've noticed a lot of games using more than 12-14/24 on my card at 1080p. I've got the settings at max when at that resolution in testing but still it's still concerning that modern entry level - mid range products aren't being packaged with enough Vram for 1080p the standard display resolution since about 2008/9.",hardware,2025-05-07 14:06:35,7
Intel,mr67uoj,"You're missing the 'for now at 1440p' - don't buy a 12 gig card for 1440p; the 5070 current model and 9070 GRE are elite 1080p cards and no more.  If you're at 1440p, get the 16 gigs.",hardware,2025-05-08 01:35:22,0
Intel,mr4emju,I have a 3080 10GB and I keep following all of the new GPU news but I can't justify even an upgrade to a 9070xt my card just does everything I want at 3440x1440 so why would I need to upgrade?,hardware,2025-05-07 19:46:22,3
Intel,mr2qtl4,"You play on low settings yes, but you don't buy a card that's ""good enough"" at low settings. If they can get 1000 FPS with a 5080 in CS2 compared to 500 FPS with a 5060 Ti, they are going to buy the 5080.",hardware,2025-05-07 14:59:25,5
Intel,mr2dgef,And when they dabble it might be a crap experience for no good reason,hardware,2025-05-07 13:52:01,3
Intel,mr8gfgb,"The person I replied to didnâ€™t ask the specific vram question that you originally responded to.   I am responding to their first paragraph, and the â€œhigher frames at 1080p are more CPU dependentâ€ idea. Which is related to my comment.   For people doing cursory research, they will undoubtedly run into the hordes of commenters repeating that CPUs work harder at lower resolutions and take away the wrong impression. Which is why it gets repeated so much here to begin with.",hardware,2025-05-08 12:34:33,1
Intel,mr2gtoe,"Youâ€™re seeing allocation and texture storage. If you have 24GB available most games will store textures in vram up to about 75-80% of total vram. If you put a 12GB GPU in place of yours at the exact same settings itâ€™ll â€œuseâ€ about 7-8GB. It just scales the usage to whatâ€™s available for faster texture streaming. COD black ops for instance has the vram buffer adjustable. You can set it to how much vram you want it to keep available, but if you max the settings out at 1080p it shows you the game only needs about 4-6GB to run. I can set cod to use 16GB of 16GB available at 1080p, even though it doesnâ€™t actually need it. Thatâ€™s just an example of a game that lets you control it.",hardware,2025-05-07 14:09:45,40
Intel,mr3syak,"And it's particularly pressing in this economic climate, you want the SKU with extra VRAM to stretch the life of your card.",hardware,2025-05-07 18:01:39,1
Intel,mr2rc7l,Flawless logic bro. Are you a salesman by any chance?,hardware,2025-05-07 15:01:59,-12
Intel,mr35l6u,> no good reason  saving $1700 not buying an msrp 5090 is a good reason,hardware,2025-05-07 16:10:40,-1
Intel,mr3bkqe,"Damn people didn't like the truth I guess.  You're correct and you can often see people complain about their OS ""eating up RAM out of nowhere"" because modern OSs will similarly pre-cache commonly used binaries and assets based on how much unallocated memory you have.",hardware,2025-05-07 16:39:53,18
Intel,mr2ii79,"In general you are probably right as I have seen these points made before by reviewers, however the specific games in question (CP2077, HL, and Horizon FW were the ones I noticed this on) were using the full 11 GB buffer at 1080p on my 1080ti before I upgraded. May well be examples of poorly optimised games but it was a major reason I elected to go for a gpu with more than 16 for my personal PC, since I was upgrading my monitor at the same time and prefer not to upgrade GPU every generation. I can definitely believe CP2077 and HL in particular would use more than 80% of the available buffer and I'm sure there are more examples with the lack of optimization effort some studios put in these days.",hardware,2025-05-07 14:18:17,-7
Intel,mr4igzl,No way I've had my gpu running out of VRAM with 12gb.   Ratchet and clank at max settings 1440p ultrawide.,hardware,2025-05-07 20:04:56,-7
Intel,mr2u5hn,"They want the most FPS and best performance they can get, so they buy the best hardware they can get. This is not hard to understand.",hardware,2025-05-07 15:15:33,2
Intel,mr5xu3b,Because the only 2 options are an 8GB card or a 5090  What,hardware,2025-05-08 00:38:19,5
Intel,mr2j7u6,"I understand what you think you probably saw; but cyberpunk just does not actually use 11GB at 1080p even with RT on. That game came out 5 years ago, and does fine with 6GB GPUs. Cyberpunk was in development when the 1080ti was the top GPU available. You think they developed a game that maxed out THE top available GPU at 1080p? When the game came out the 2080 ti could max it out at 4k with 11GB.",hardware,2025-05-07 14:21:53,17
Intel,mr2wfmw,"Talk all you want, you will never convince me that pros need a 5090 to play Starcraft Brood War.",hardware,2025-05-07 15:26:30,0
Intel,mr2njf8,"you maybe right (perhaps I'm remembering the stat for the time I tried path tracing with a non rt card for science [https://www.techpowerup.com/review/cyberpunk-2077-phantom-liberty-benchmark-test-performance-analysis/5.html](https://www.techpowerup.com/review/cyberpunk-2077-phantom-liberty-benchmark-test-performance-analysis/5.html) ) guess it was probably closer to 8, Hogwarts Legacy was definitely well over 10 though.",hardware,2025-05-07 14:43:23,1
Intel,mr4mr1t,That's probably largely due to the game using low quality textures which is made worse through noise reduction.,hardware,2025-05-07 20:25:39,-1
Intel,mr30elc,Bro it's 2025 no one is playing a game from 1998 except the like 20 Koreans left in ASL. It's not an eSport anymore. It's also not even a game where FPS count matters. Quit your larp.,hardware,2025-05-07 15:45:35,2
Intel,mr31ugw,"Wait, do you think ""esports players"" just means people that play them professionally?",hardware,2025-05-07 15:52:32,-1
Intel,mqpm6s8,">Moreover, the confirmation of the Arc B770 being on its way has been done by the famous leaker OneRaichu, who claimed that we'll see the model emerging in the future, so overall, there's optimism around the Battlemage lineup. As to what we can expect from it, it was said that a variant on this die would feature around 24-32 Xe2 cores with a 256-bit memory bus and 16 GB of GDDR6 memory, however, the silicon could also be featured in AI/professional GPUs too, so nothing is certain for now.  >We expect an announcement at Computex 2025, which will start in a few weeks.  Source is WWCFTECH and OneRaichu's tweet mentions nothing about Computex",hardware,2025-05-05 14:21:01,64
Intel,mqpn778,Man the clickbait is real Imagine making leaks out of thin air,hardware,2025-05-05 14:26:13,76
Intel,mqppc5d,This is an Arc Pro Battlemage 24gb mainly cater for workstation purpose  Unsure if this you can tweak it for gaming purpose,hardware,2025-05-05 14:37:04,16
Intel,mqro09t,We really need a 3rd compeditor in the DGPU space. AMD and Nvidia are price gouging us with crap 8gb gpu offerings.,hardware,2025-05-05 20:22:26,10
Intel,mqpn860,"Help me Intel, you're my only hope.",hardware,2025-05-05 14:26:22,11
Intel,mqynvvz,Its not DOA. but it will be interesting to watch how it gonna percived at launch.,hardware,2025-05-06 21:58:20,2
Intel,mqprw38,The MLID Method.,hardware,2025-05-05 14:49:48,33
Intel,mqsismi,With a 256bit bus I don't see it being 24GB.,hardware,2025-05-05 23:04:16,4
Intel,mqpsoyu,Intel really needs to get their poop in a group in coming up with something to rival OptiX and get some software crew out on loan to help get Arc supported in more rendering engines.  AMD has left that market nearly completely.,hardware,2025-05-05 14:53:48,5
Intel,n2wginf,"This is an Arc A770 16GB ""RGB Connection""",hardware,2025-07-13 14:17:11,1
Intel,mqqqqkq,"Then they announce it's cancelled.   Like, it was never there until you brought it up....",hardware,2025-05-05 17:40:01,13
Intel,mqq5m7t,More like Red Gaming Techâ€™s â€œAlchemist+â€ and â€œat least 100% raster upliftâ€ from RDNA 2 to RDNA 3,hardware,2025-05-05 15:57:38,15
Intel,mqpxog2,Intel has embree and OIDN both of which are used in industry widely,hardware,2025-05-05 15:18:37,3
Intel,mqr2ncy,"""its gonna be 20-40% better"", no shit that happens almost every generation. if the guess is wrong tho then ""it wasnt finalized until later""",hardware,2025-05-05 18:37:35,6
Intel,mqrtnhd,That guy is the biggest fraud ever .,hardware,2025-05-05 20:49:50,8
Intel,mqq0dzd,"Open Image Denoiser doesn't hold a candle to OptiX and as for industry I mentioned rendering specifically and Intel is not an option full stop in the majority of renderers, in any that it is the performance is sorely lacking. I'm talking Redshift, Arnold, OctaneRender, etc.  If OIDN was a better option than OptiX then Pixars RenderMan would use it instead of OptiX, but it isn't, so they don't.  I love my Intel products but you're up your own behind and dickriding them a little too hard if you think that they've got a place in 3D currently so going back to exactly what I said, they need to get some software crew out on loan.  I do 3D professionally, this isn't tribalistic Green V.S. Red V.S. Blue stuff. It's reality.",hardware,2025-05-05 15:32:01,2
Intel,mqrmmeb,20-40% gains are long gone considering the last two generations of nvidia cards,hardware,2025-05-05 20:15:50,8
Intel,mqqjl15,I don't believe it's dickriding but lack of in depth experience,hardware,2025-05-05 17:06:11,4
Intel,mquz84x,i havent paid attention since i cant afford it anyway lmao,hardware,2025-05-06 10:15:11,1
Intel,mt4ywze,"Make tech fun and exciting again. Dual GPUs are fun, even if it's just AI trash.",hardware,2025-05-19 15:38:57,54
Intel,mt4t4ia,I think it's impressive that the company behind my $36 Terminator B450M AM4 motherboard can release enterprise grade Dual GPU cards.,hardware,2025-05-19 15:09:46,22
Intel,mt529p9,It would be better if they would decide to sell them as a retail product as well.,hardware,2025-05-19 15:55:25,9
Intel,mt4llbh,"Hello fatso486! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-05-19 14:31:27,2
Intel,mt515ar,HAHA .your comment is giving me mixed messages.,hardware,2025-05-19 15:49:57,21
Intel,mt5ykh1,"Yeah, I'd grab one.",hardware,2025-05-19 18:32:23,3
Intel,mt7o998,Why?  They wouldn't be able to sell enough of these to be profitable.,hardware,2025-05-19 23:59:17,0
Intel,mt545sn,I like that Intel is doing cool things again but I'm not a big fan of AI.,hardware,2025-05-19 16:04:49,10
Intel,mq0f537,Well maybe we will end up getting higher end battle mage cards. I still very much doubt it but the evidence points to a possibility of it happening.,hardware,2025-05-01 12:58:51,41
Intel,mq1ctvo,"The B580 is a damn good mid range offering, if you look at it as a dollar per frame standpoint.       I'd like to see a more powerful option, or even a new low end option to replace the meager A380.",hardware,2025-05-01 15:53:17,14
Intel,mqd5kr5,"Vietnamese are crazy fans of Intel and NVIDIA, dispite the whole world buying 9800X3D and 9070XT they still buy 12400f and 5070ti",hardware,2025-05-03 13:06:31,2
Intel,mrd7tcy,For content creation the 580 comes up short (but it's good for gaming).  Still hoping a proper A770-successor might be announced at Computex.,hardware,2025-05-09 04:14:31,1
Intel,mqaajrx,These may just be for development. They show up on a shipping manifest. They don't show the volume.   Everything still points to a cancelled product.,hardware,2025-05-02 23:27:52,4
Intel,mq3gixd,Anything short of NVIDIA dropping out completely or completely fumbling the ball for a few generations (either products worse than Blackwell or just stagnation) wonâ€™t make a different at this point.,hardware,2025-05-01 22:04:53,5
Intel,mq1tqtx,Sadly it's perf per mm of silicon sucks balls  Which is important in the high end to make it a viable product   New low end maybe but not high end this gen I think,hardware,2025-05-01 17:16:07,21
Intel,mq5uozs,"Sure, but you say that like Intel wasn't king of CPUs until like 2 years ago.  Companies completely fumble the ball.  It's a thing that happens.  A lot.",hardware,2025-05-02 08:08:27,4
Intel,mq54ce5,"> Sadly it's perf per mm of silicon sucks balls  At least the B580 is costing Intel a lot less to make thanks to being a smaller chip with a simpler memory configuration than the A750/A770/A580; while being a good bump faster than the latter. Alchemist was a good first effort compared to the Hindenburg disaster that was the last time dGPU market had a newcomer 20 years ago (XGI Volari), but Alchemist was a f'ing disaster in cost to make compared to performance.   Intel needs to keep hacking its chips to be smaller and cheaper imho while improving midrange performance, that's what really sells to the OEM market.",hardware,2025-05-02 04:06:24,10
Intel,mq3gc9c,"Feel like it matters more for low end though as the margins are more razor thin, while in the high end it might not matter as much",hardware,2025-05-01 22:03:52,1
Intel,mq3nrcg,"This is very likely the full die with certain parts disabled. The a580, a750 and a770 shared the same die. There was  big performance difference between the a580 and a750.",hardware,2025-05-01 22:45:35,-2
Intel,mq9kq1q,"And when a semiconductor company drops the ball, it's usually a problem that lasts for several years at a minimum. You can't turn that ship around quickly.",hardware,2025-05-02 21:04:41,4
Intel,mq7xoj5,">Intel needs to keep hacking its chips to be smaller and cheaper  Eh, I think you have this backwards because the natural extension is going back to integrated graphics. Obviously that will help them but I don't think it's their current main objective.  They *are* trying to raise performance, yes. But the optimisation comes afterwards. New prototype items always cost a lot.",hardware,2025-05-02 16:18:43,1
Intel,mq3q65h,"no...  the B580 is 272 mmÂ²  the 5070 is 263 mmÂ²  5080 is 378 mmÂ²  while the 5070 is almost 2x as fast as B580   if you scaled that up, and even if that isn't full die at say 400 some mm2 you likely wont get to 5080 performance because the gap is just huge in terms of perf per mm, and even if intel sells it cheap, can that be made to be ""just"" 5070 tier pricing to compete with 5070/ti and still likely not touch 5080?  for the low end, intel can and is eating margin, the 5070 is priced at 605 dollars card, while the B580 is 300 dollars, sure the GDDR6 vs 7 help but like that is already a ton of margin to be eating and a higher end card means even bigger one...",hardware,2025-05-01 22:59:16,7
Intel,mq3vi83,B580 is the full die.,hardware,2025-05-01 23:29:27,4
Intel,mq9n7to,"There's two ways of looking at it. If Intel just deleted EUs until the GPU is small enough to manufacture cheaply, they won't have any hope of long-term success. But if they work on improving their performance per mm^2 while targeting only small to medium GPU sizes and don't even try to make something that scales up enough to compete against NVIDIA's big GPUs, then Intel might be able to build upon the relative success of the B580 in the under-served *affordable* GPU market segments, while still having an architecture that's appropriate for their relatively small iGPUs.",hardware,2025-05-02 21:17:31,5
Intel,mq4sssi,4070 is 294mm. It is more appropriate to compare it with cards of same process. 4070 is bigger and 45% faster. Intel is not that far off. 1.5 generation behind Nvidia at time of 580 release.,hardware,2025-05-02 02:47:00,6
Intel,mq44zot,"Which makes me wonder if they couldnâ€™t use part of the die for a lower tier b380 or b310, if only to make use of the not so good dies.",hardware,2025-05-02 00:24:16,1
Intel,mq7u37k,> It is more appropriate to compare it with cards of same process.  Do you think the 5070 is made on a newer process than the 4070?,hardware,2025-05-02 16:01:34,2
Intel,mq45bc3,"I mean, it's on N5. That node should be really mature right now. They'd probably be better off merely cutting the price on the 580/570 than cutting good silicon. Though maybe they'll *eventually* accumulate enough bad dies to justify a new SKU.",hardware,2025-05-02 00:26:11,8
Intel,mq8wv6f,It is on N4 no? From N5.,hardware,2025-05-02 19:06:40,1
Intel,mq99ump,"I think they're both on ""4N"", which is NVIDIA's customized TSMC 5nm. I'm not sure what that customization entails other than adding an extra layer of metal wiring.",hardware,2025-05-02 20:11:08,2
Intel,nb8jouv,I'd personally suggest the 9060xt.  The 9000 series are spectacular tbh...,buildapc,2025-08-29 01:08:31,12
Intel,nb8khvg,If it's the 16gb 9060 xt I'd say the upgrade is worth it,buildapc,2025-08-29 01:13:15,1
Intel,nb8tz26,If its the 16gb 9060xt then absolutely that. Thats the one i just got for my first pc,buildapc,2025-08-29 02:11:04,1
Intel,nb8z9g2,"It honestly depends on what your needs are... Fps, solo campaign, rpg, streaming, vid rec., etc ... Do u have an interest in A.I. or M.L.? RX is amazing for raw frames n streaming. ARC is amazing for AI n ML so it depends on what your use cases are",buildapc,2025-08-29 02:43:36,1
Intel,nb9it9e,9060 XT and get the 16GB version.,buildapc,2025-08-29 05:05:01,1
Intel,nb8jzrh,is the current price still good with it?,buildapc,2025-08-29 01:10:18,1
Intel,nb8lgjv,the pricing would be my real question as it seems to be too much for both,buildapc,2025-08-29 01:19:03,1
Intel,nb8kctz,"they're both overpriced, is this USD?",buildapc,2025-08-29 01:12:27,2
Intel,nb8kk4l,Nah. MSRP is 350. +/-50 is what I'd accept. 100+ more is crazy. That's 30% upcharge.,buildapc,2025-08-29 01:13:38,1
Intel,nb8ohbu,What country are you in? It would depend on local pricing and what alternatives are priced at. Sometimes countries are just expensive unfortunately,buildapc,2025-08-29 01:37:21,2
Intel,nb8kjao,usd equivalent of php,buildapc,2025-08-29 01:13:30,1
Intel,nb8lbsr,where can we get msrp ones im from the philippines.,buildapc,2025-08-29 01:18:15,1
Intel,nb8l2bd,"if I had to overpay for something, it'd be amd over Intel.  Especially because their new CEO has been threatening to discontinue their GPUs",buildapc,2025-08-29 01:16:41,4
Intel,nb8lyby,I see. Are the 9060 xt around those price?,buildapc,2025-08-29 01:22:02,1
Intel,nb8lddy,thanks for that info ill keep that one in mind,buildapc,2025-08-29 01:18:31,1
Intel,nb8m7z8,on stores here yeah around those price,buildapc,2025-08-29 01:23:36,1
Intel,nb8nu3n,I see. Then I guess it's fine to buy it. It's a much better card that the b580 by 25%.,buildapc,2025-08-29 01:33:25,2
Intel,n92pgwe,"Everybody was talking about it when it came out. At MSRP, itâ€™s a good value   It was just a bitch to get because it sold out instantly everywhere. Iâ€™m sure thatâ€™s changed now. Itâ€™s a good purchase   It also requires a decent CPU, thatâ€™s the only issue",buildapc,2025-08-16 21:29:37,83
Intel,n92s3u3,They make up something like 2% of the total GPU market.  You don't see a lot of talk about them because so few have them.  They are ok for what they are.,buildapc,2025-08-16 21:44:51,16
Intel,n92oj13,Theyâ€™re really good for what they are and are pretty universally well received.   Theyâ€™re budget tier cards with a limited lineup that simply donâ€™t have the same recognition and acclaim as Nvidia or AMD.,buildapc,2025-08-16 21:24:13,13
Intel,n92qk9a,"The Intel arc GPUs used to be avoided because of the drivers. Some games straight up don't work with them, some had way worst performance than the competitions. Intel's driver has improved leaps and bounds, it's a great one to get unless you have a really dated CPU.",buildapc,2025-08-16 21:35:56,33
Intel,n93o943,"I just got one and itâ€™s awesome, $395 AU and I got a nice increase at 1080p. Made it very cheap to upgrade!Â   I think my end game is to try get the next line of graphics card at early retail price, or just hop on the next Intelâ€¦ tbh done me great. The only problem I have had so far was tabbing out of helldivers caused a few glitches on my mates end for the 15 seconds I was gone.",buildapc,2025-08-17 01:03:11,4
Intel,n935ovh,"Their A series gpus had some major issues alongside drivers still being pretty raw. The a310/380/580 have some niche uses in video encoding or for driving 1080p displays.   The B series cards have turned out to be really good budget gpus for 1080/1440p. The drivers have come a long way since launch, theyre still not as refined as Nvidia but theyre mostly stable now. They both require more modern platforms to perform well but not the top end hardware alot of reviews claim they need. The b580 has had alot of issues staying in stock since it offers 12gb of vram at a price point that almost exclusively offers 8gb cards.  Ive had my b580 since around launch. I got it on launch day for MSRP and aside from an unstable driver update i had to roll back its been a great upgrade from the 1070 i had previously. If you play on 1080/1440p displays at high settings theyre good cards",buildapc,2025-08-16 23:07:06,7
Intel,n947yxo,"Intel arc a310 is one of the best cards for transcoding videos, can handle multiple 4k streams. I love mine, using it for plex server and immich",buildapc,2025-08-17 03:09:07,3
Intel,n92ogae,It's good,buildapc,2025-08-16 21:23:47,5
Intel,n92oiup,No they arenâ€™t. The b580 in particular is praised for its great value at its price range,buildapc,2025-08-16 21:24:11,2
Intel,n930cbr,As long as your CPU and motherboard supports RE-Bar it's a great option for 1080p/1440p for the price,buildapc,2025-08-16 22:34:04,2
Intel,n93bvyj,Idk but I take it as a good sign because people will only post if there is something wrong with the product. This also applies to other subreddits.,buildapc,2025-08-16 23:45:13,1
Intel,n93txb5,"They're good and I have one that I use for a rig that only plays games like esport titles and multiplayer steam games. They aren't the best for any type of AAA game.  The bigger deal is that intel has announced some pretty heavy changes and cuts coming and there is good speculation that intel's gpu sector might be cut. If it doesn't happen, it's no big deal, but if it does get cut or restricted, any type of updates or driver updates that are needed for future releases will probably be unlikely to come out. That's probably the only reason I wouldn't recommend it to anyone, but for the price its great for someone who only enjoys current multiplayer games or indie games.",buildapc,2025-08-17 01:38:44,1
Intel,n953vyb,"Intel Arc has physical hardware that works ok, but it's not high performance yet.  But MOST importantly, this is a new company in terms of graphics drivers and industry cooperation in software development. So what? NVIDIA and AMD partner with game developers to optimize graphics, sample and super-sample textures in AI for optimization for best performance. This is an ongoing development of specialized software and tools to improve performance to the maximum with the given hardware. This software is NOT available to Intel and they will have to spend years catching up to their competition.  In short, Intel graphics cards will work fine for low demand popular programs. They may eventually get there for high end performance, but it will take years for them to catch up.",buildapc,2025-08-17 07:29:05,1
Intel,n93yzaj,"Have you considered 9060 xt at all? There has been a price decrease recently, you might be able to get one for the same price as a b580 (8gb model 9060 xt).  Except for the vram department, the 9060 xt is pretty much better in every way and performs better too.  Was on the same boat with you. The b580 was $360 CAD, and the 9060 xt 8gb was $380. After going through countless benchmarks (especially at 1440p), I eventually decided to pick the 9060 xt sapphire.   In only like 1 or 2 games the b580 scored higher, and everything else was in 9060 xtâ€™s favour. Idk much about the driver issues on the b580, but at least the 9060 xt seemed like the safer bet. I wish it was at least 10gb thoughâ€¦. Amd really missed an opportunity. Otherwise this gpu is a beast! Iâ€™ve never seen an 8gb gpu do so well at 1440p. Amd did score with that.",buildapc,2025-08-17 02:09:47,1
Intel,n93udpj,They are good for 1080p gaming also if you have a really low budget and want to game. For slightly more money you can pick up a 9060xt 16gb which is good for 1440p and has better long term returns.,buildapc,2025-08-17 01:41:28,1
Intel,n93qv6o,"They're okay, if you hate yourself get an Intel Arc GPU otherwise get an AMD Radeon GPU or if you have the money to burn literally and figuratively speaking an Nvidia GPU.",buildapc,2025-08-17 01:19:51,0
Intel,n92t8e9,I think better price to performance can be found on a used card.,buildapc,2025-08-16 21:51:23,-1
Intel,n93bgu5,"Has driver issues with more dated and/or less popular games, but they've made it a point that major AAA titles and esports games usually have good drivers. It also basically requires a fairly beefy CPU and it will at least slightly negatively impact the CPU performance too. But when I say fairly beefy I basically just mean AM4 5000 series or basically any AM5 CPU even the 7500f, I personally don't know the Intel comparison but I imagine any Core Ultra or 12th gen would absolutely crush it and be fine.  But the B580 is basically competing in the 60 series tier and is alright, but definitely if you can get it for MSRP or within I'd say even $30 over MSRP it's a steal. Something of note too is at least from my experience and seeing benchmarks, the Arc GPUs normally do this split the middle kind of thing, where they tend to outraster/render Nvidia GPUs of their tier, but get outrastered/rendered by AMD GPUs of their tier, but on the other end XeSS upscaling/frame gen performance and raytracing usually outperforms the comparable AMD GPUs, but loses to the comparable Nvidia GPUs. Although FSR4 and the better raytracing on the 9000 series might've changed that, a lot of those comparisons were from when we were still comparing to the RTX4000 and RX7000 cards.",buildapc,2025-08-16 23:42:39,0
Intel,n93dl7l,B series is getting a bit old now. Eagerly awaiting what the C series will bring.,buildapc,2025-08-16 23:55:45,0
Intel,n94a4jq,No idea about the drivers but B570 and B580 which released some months ago are very good value and also very efficient.,buildapc,2025-08-17 03:24:16,0
Intel,n94duvw,"The B580 is actually very good for a $250 card as it performs similarly to the 4060 and more importantly has 12GB memory instead of 8GB. Even today it's still better value compared to the 5050 at the same price.  The main issue with the card is availability in certain regions, which impacts it's price. Where I'm from (Bangladesh) availability is actually pretty decent and so most B580s can be had for at or near the $250 msrp. The other issue with the card is it's driver overhead, which mostly shows up on the AM4 platform where the performance delta can sometimes be so massive that the card ends up slower than the 3060 12GB.  The issue is far less rampant on AM5. It performs around the 4060/5050 on a cheap R5 7500F, and is already beating the Nvidia cards in terms of value. It can still go faster if you pair it with the fastest CPU on the planet, but not by much and I personally don't think it's that big of a deal.   Alternatively you can play at 1440p and eliminate the driver overhead; the card has the performance and memory for it anyway, so why not. You can even go all the way upto 4k Balanced with XeSS 2.1 which is actually pretty good and supports frame gen. Kinda insane for a $250 card if you ask me.  Driver support and game compatibility is nowhere near as bad as Arc Alchemist, and people have even played the Battlefield 6 Beta on day 1. Older games such as those on DX9 or DX10 may still struggle compared to a 4060/5050, but the card is so fast you should still get highly playable framerates. We're talking 150 fps instead of 300fps, big deal.",buildapc,2025-08-17 03:50:52,0
Intel,n92yskg,No one buys them because people truth internet myths even from years ago but the reality is that those are very good gpus and near to MSRP are a great value.,buildapc,2025-08-16 22:24:43,-1
Intel,n939hns,"They're decent GPUs but for gaming yo'ull want something a lot more powerful, like an RTX 5070 or RTX 5070 Ti.  [https://www.youtube.com/watch?v=VcKPJvGNr0c](https://www.youtube.com/watch?v=VcKPJvGNr0c)",buildapc,2025-08-16 23:30:30,-2
Intel,n93cwvi,Nobody talk about the A series anymore. Is it still worth to buy?,buildapc,2025-08-16 23:51:32,3
Intel,n9ay9g8,"You must think all of us can afford Nvidia. In Europe you guys probably can, but in America all of us have millions in debt from medical bills and we make minimum wage!",buildapc,2025-08-18 05:56:13,0
Intel,n92ra8x,So for exemple an 7500f is ok ?,buildapc,2025-08-16 21:40:06,7
Intel,n93y470,"> unless you have a really dated CPU.  Read: more than a generation or two old. So, not actually dated at all, which is why the overhead issues with Battlemage were so criticized.",buildapc,2025-08-17 02:04:19,5
Intel,n944jl6,"I ran an a750 in Linux until Monday. It worked well.  Rarely had problems.  I just wanted more vram so I got a 9060xt. If Intel had an upgrade path for the a750, I would have bought one.",buildapc,2025-08-17 02:46:03,1
Intel,n94qd93,Idk if I'd call Nvidia refined either.   I was still using the 566.36 driver until the Battlefield 6 beta made me update and since then I've encountered the flickering display and sudden black screen issues a few times that have been going on with pretty much every driver version since the 50 series came out.,buildapc,2025-08-17 05:25:48,1
Intel,n99smmg,"Rebar isn't all it needs. It needs a good CPU. I bought one to replace a 7600, paired with an i5-10400. After extensive testing it didn't really offer any improvement, and even performed worse in CPU-heavy/multiplayer titles. I thought the overhead issue was overblown, it isn't.",buildapc,2025-08-18 00:52:07,1
Intel,n935f0h,It really cant,buildapc,2025-08-16 23:05:24,4
Intel,n92v3ab,Like which one ?,buildapc,2025-08-16 22:02:20,1
Intel,n93y7iv,B series launched literally under a year ago.,buildapc,2025-08-17 02:04:56,2
Intel,n93dshg,Depends. For gaming? Not really. Better bargains. But for a home server the a310 is the champ of media encoding,buildapc,2025-08-16 23:57:00,26
Intel,n9xi0z3,"In gaming, the B series will generally be better in pretty much every way. It just runs faster, cooler, and cheaper. Outside gaming it gets a bit weird, but actually favors A series. The 310s are *very* good at media encoding for the price. The A770 can also outperform the B series in some non gaming tasks (generally its inferior, but it can get 4gb more vram and has a wider bus, which can make it better, or sometimes close enough that it being older and thus cheaper can make up the difference.) For gaming, B series is strictly better",buildapc,2025-08-21 17:40:26,1
Intel,n9449nv,The b580 is close to the a750 in performance. Not worth buying that. The a770 is better but I wouldnâ€™t recommend them now with the amd 9060xt out.,buildapc,2025-08-17 02:44:14,-1
Intel,n9azast,"The median net worth in the US is 200k and the US accounts for more of Nvidiaâ€™s revenue than the rest of the world. Speak for yourself, big bro.   Regardless, what even prompted you to say this? Iâ€™m just answering OPâ€™s question about Intelâ€™s popularity lmao",buildapc,2025-08-18 06:05:56,1
Intel,n92rprb,Totally fine.,buildapc,2025-08-16 21:42:35,16
Intel,n94srky,"Given you dont need to run DDU or (in some cases) do a fresh install of Windows to get the cards to run decently id argue theyre pretty well refined.   Even compared to AMDs drivers Nvidias just tend to work without much effort, from what ive been seeing it sounds like bf6 has had a ton of issues regardless of which gpu youve got. Having a set of rough updates off the back of 20+ years of development doesnt discount the amount of time theyve had to work on getting things working well. Intels 1st dgpus came out in 2022 IIRC",buildapc,2025-08-17 05:46:16,0
Intel,n937vuc,Sure it can. You can get a 3070 at the cost or a 3070 Ti for a little more and it beats the B580.  I just sold my 3070 Ti for $290 a month ago because that's all people were willing to buy at. Some sold the card for even less.,buildapc,2025-08-16 23:20:40,-2
Intel,n96o38v,"A used b580 is better price to performance, theyâ€™re just kinda speaking arbitrarily",buildapc,2025-08-17 14:48:12,1
Intel,n9381sa,3070 or 3070 Ti. One may cost more. I sold my 3070 Ti for $290 and others are selling for less.,buildapc,2025-08-16 23:21:41,0
Intel,n93t9zb,Youâ€™re comparing the used market to the new market. You can get a used B580 for $170 on ebay as the first listing I see.,buildapc,2025-08-17 01:34:47,3
Intel,n981kxq,"Not arbitrary with my assumption they want to spend $250 on a new B580. I don't know for sure they wanted to go new, but in case it was their plan, I'm saying instead of spending $250 on a new one if that's what their plan is, just spend $250 on a used card which will have better performance. It's really common sensical as to what I am saying. Don't understand what is arbitrary about that.",buildapc,2025-08-17 18:56:23,1
Intel,n944bb3,"Indeed, that was my intent and my assumption that OP was going to go for new. $250 spent on a used card is spent better than on a new B580.  I don't think this point is invalid to the point where it should be downvoted.",buildapc,2025-08-17 02:44:33,1
Intel,n949c44,"Okâ€¦ but if OP is willing to buy used cards, $170 is better spent on a used B580 than $250 on a 3070 as you can spend the extra $80 to get other components that are better as well",buildapc,2025-08-17 03:18:40,2
Intel,n94a2m5,"Sure this is true, but I was only talking about going for a brand new B580 versus a used GPU. I naturally assume at such a low price point that people will go brand new on that GPU rather than used.  You can argue different scenarios with a different good answer, but what you stated wasn't exactly relevant to my assumption.",buildapc,2025-08-17 03:23:52,1
Intel,n94b4w6,"Yes, but your assumption is inherently flawed is all Iâ€™m pointing out.",buildapc,2025-08-17 03:31:29,1
Intel,n94g29o,"No, my assumption isn't flawed when I had no evidence to suggest that OP was going used on that card. How can I possibly know what OP intends on doing without it having been explicitly stated? I would personally expect them to go for a new card and not used as the card is already cheap enough to get it new (without considering budget constraints, but they also never stated any details on a budget and other essential info).   Hindsight and what comes after the comment was made is different, but my comment was based entirely on a very specific scenario. I think it's clear from my comment that I was specifically discussing differences between a new B580 and a different used card. I wasn't talking about anything else.",buildapc,2025-08-17 04:06:31,1
Intel,n94iyut,"Your comment here already dismantles the scenario you thought of. You claimed you assumed they would be going for a new card. This assumption automatically means that going for a used card is out of the question, so comparing the new card to the used card doesnâ€™t make sense. If OP were to be considering going used on cards, why would they not consider going used on the B580? The problem is that you didnâ€™t think to expand the logic you presented beyond one specific card, which is inherently flawed.",buildapc,2025-08-17 04:28:42,1
Intel,n7vint8,"The first release might be sold with a new PC. If you live in US, you might as well go to China and buy one there (or one of their online stores). Tarif and scalpers might jack up the price by a lot. This is not Intel, but he went to Hong Kong to buy the rare 5050 and found a few of them easily. https://youtu.be/-jXMaS8e8oo?si=6M1XiK54SrVDI3h9",buildapc,2025-08-10 02:11:28,2
Intel,n7xtw3n,"For the 48GB Maxsun dual variant of the Pro B60, I was quoted $3,300 New Zealand Dollars (NZD) per GPU. Not available in retailers, only to business customers through limited suppliers. The supplier who services my area doesn't even have a website.   For comparison, the RTX PRO 5000 with 48GB is $8,800 NZD, and the cheapest RTX 5090 is around $4,650 NZD at the moment.   I was interested initially because of the RAM in the dual Maxsun card, but was hoping for a price closer to $2,000 NZD.",buildapc,2025-08-10 13:44:43,2
Intel,n7f7mhv,Why specifically the Intel when plenty of GPUs aren't going to interfere with AI.,buildapc,2025-08-07 14:16:23,1
Intel,n7xqndu,That's an interesting idea :) I'm not in the US but going to China to buy some hardware sounds very cool! Maybe I'll do it some day. Thank you.,buildapc,2025-08-10 13:24:52,1
Intel,n7xxnpt,"My idea was to buy a singular one now and another one later when I have the money for it. But dual variants look cool too if you can afford it. $2000 USD sounds too much though. I feel like the supplier in your area is jacking up the prices since it is difficult to find them.  I asked a friend of mine who does business hardware deals. He couldn't even get a quote. I guess it is not even available on many regions now.  Anyway, thanks for the info :)",buildapc,2025-08-10 14:06:54,1
Intel,n7fl1xe,24GB VRAM for 500$ sounds super nice and not offered by any other GPU manufacturer. Plus they advertised a card with dual B60s (48GB VRAM total) sharing one PCI-E x16 slot (x8 each).,buildapc,2025-08-07 15:20:46,1
Intel,n7fs5xm,"Then price the GPUs that have Tensor cores. I'm just wondering because Intel GPUs aren't traditionally for gaming, though that is slowly changing",buildapc,2025-08-07 15:54:41,0
Intel,n7hhge2,"Tensor cores is not the only thing that matters when it comes to performance. B60 performance was looking pretty good in the demos but now there is nothing on the internet, no benchmarks, no prices, no people with crazy connections showing off a hardware others cannot buy, nothing.",buildapc,2025-08-07 20:46:19,1
Intel,n7ismbe,You have a very narrow vision of speed. What you are saying is like your truck is faster than my audi because it has more horsepower. Nvidia is the top dog right now but that doesn't mean you get more from nvidia for 200$. VRAM size and speed is something you cannot ignore. If you have to load half of the model to system memory it is gonna decrease your speed significantly. And guess what? If you can't fit the model into your memory you won't be running it even if you have a billion cuda cores. This is why intel is offering 24GB VRAM for 500$ while you need to pay 2500$ for the same VRAM size in nvidia (rtx a5000 for example).,buildapc,2025-08-08 01:06:31,0
Intel,n7ifcir,"But compared head to head with an nVidia or AMD/ATI of the same price and the Intel is not the top performer, last I checked.   I looked into this recently because I was thinking about upgrading.  $200 for that versus $200 for an nVidia/AMD and the non-Intel GPU was a slightly better performer.",buildapc,2025-08-07 23:48:23,0
Intel,n7ifjl8,"But compared head to head with an nVidia or AMD/ATI of the same price and the Intel is not the top performer, last I checked.   I looked into this recently because I was thinking about upgrading.  $200 for that versus $200 for an nVidia/AMD and the non-Intel GPU was a slightly better performer.",buildapc,2025-08-07 23:49:35,0
Intel,n7j6hep,"No i don't have a narrow vision of speed. Maybe ask questions first. Like i said, i read reviews, Intel didn't break through. Maybe I'm incorrect, but i could care less. Reviews change daily. And i could find a dozen that support my opinion. Not like i care. Intel said it wasn't primarily intended for gaming.  What's worse is your rudeness.",buildapc,2025-08-08 02:30:55,1
Intel,n7jo11a,"Yeah, sure mate. You do you.",buildapc,2025-08-08 04:31:45,0
Intel,n9i90uf,9060xt 16gb,buildapc,2025-08-19 10:46:32,1
Intel,n9idiq7,Thats about 100$ more expensive that the two I mentioned. Is the improvement worth it for the price? If not I would rather go for the lower price range.,buildapc,2025-08-19 11:21:07,2
Intel,n9ifhfi,yes you can run games at 1440p fine,buildapc,2025-08-19 11:35:32,1
Intel,n9ikti3,"The P3 Plus uses QLC flash, which has low durability and becomes noticeably slower after a few years. Get a better drive with TLC flash like the Kioxia Exceria Plus G3, TeamGroup G50, Intenso MI500, WD Blue SN5000, or Verbatim Vi5000.",buildapc,2025-08-19 12:11:26,1
Intel,n9irtld,ASrock b850 motherboards have killed x3d cpus recently. While your 9600x should be fine a future x3d chip that you may want to upgrade too years down the line may not be. Iâ€™d look at a different board for your â€œfuture proofingâ€ spec.,buildapc,2025-08-19 12:53:41,1
Intel,n9j2uun,"pcpartpicker is not great with german pricing imo, use geizhals instead.  [https://geizhals.de/wishlists/4616811](https://geizhals.de/wishlists/4616811)  here i have assembled a very similar setup with a slightly better cpu and bigger ssd for \~1050â‚¬ with shipping.",buildapc,2025-08-19 13:54:16,1
Intel,n9ionob,"Thanks m8, I didn't know this. Already looked up your suggestions and they have a decent price, I will adapt my build accordingly.",buildapc,2025-08-19 12:35:06,1
Intel,n9iuw34,"Oh my, thats an important information ðŸ˜…, thanks for the info, I will look for an alternative",buildapc,2025-08-19 13:11:06,1
Intel,n9jv0u5,"Oh sweet, thanks m8ðŸ‘ŒI really like the cpu upgrade",buildapc,2025-08-19 16:09:34,1
Intel,na8zxam,"yes just get a new gpu. The cpu is still alright. Also make sure the psu is good, check the model.",buildapc,2025-08-23 14:08:45,1
Intel,na9bnzu,Do you have a suggestion on a good gpu should I get one of the newer ones I mentioned in my post or something older I searched around and found some decent deals on gtx 1080 gpus.,buildapc,2025-08-23 15:13:12,1
Intel,na9cei3,"the 1080 is outdated. The one you listed are good, see what deals you can find",buildapc,2025-08-23 15:17:10,1
Intel,na9dzzc,"Okay, need to do some price checking thanks for the help.",buildapc,2025-08-23 15:25:39,1
Intel,n4j4ewg,"Driver support in games is very good these days, I have had considerably more issues on my NVIDIA PC than on my arc PC, over the last six months or so (I've also used that one more but still felt like a worse overall experience in terms of gaming drivers). Productivity software is a different story and a bit hit or miss, in my experience especially Adobe Photoshop and to a lesser extent After Effects don't play nicely with Arc.  XESS 2 is also really good, I think right now it's generally the worst upscaler between itself, FSR4 and DLSS 4 but it's by a very small margin to a point where it depends on a game by game basis. Biggest problem is the lack of widespread adoption. XESS framegen works great.   Ray tracing compares pretty well to competing AMD and NVIDIA cards, in terms of raster performance to RT ratio it seems to land somewhere in the middle between RDNA 4 and Blackwell. That being said it's still a low end card, so you're not gonna be running heavy RT workloads in it.  I think the biggest issues are the higher CPU overhead, requirement for rebar and only supporting an x8 PCIe connection, somewhat limiting it as a viable budget upgrade for older systems.  It's more of a 1080p card for modern AAA games, that can work with a 1440p monitor in a pinch. For $260 I think it's decent and NVIDIA and AMD don't really offer anything better in the sub $300 range.",buildapc,2025-07-22 14:26:29,6
Intel,n4iwdf8,"Using my 3-fan B580 OC for 1440 gaming and it does a good job. I love its non existent noise level. Back in march I bought it for 330â‚¬. Updated driver is released like every 1-3 weeks. I appreciate intel's clean and intuitive driver GUI. I love the way the driver handles crashes, it's exemplary. The driver literally invites to try out some OC. Don't regret my B580. If I'd buy a new card right now, I'd be torn apart between B580 or 9060 XT.   But my main goal was/is a decent gaming experience with a  reasonable power consumption. I don't have a problem to fine tune settings to get stable 60 FPS and a flatline frametime. And it was a decision against NVIDIA, against their dominant position, which they exploit and hurt the market.",buildapc,2025-07-22 13:46:37,4
Intel,n4k7xid,It has bad driver overhead and isn't great for now end systems.  It also doesn't have that great price to performance now that both AMD and Nvidia have released newer gpus.  I would take the 5060 over it at 1080p.  Far less driver overhead for Nvidia.  Dlss 4 is far better than xess,buildapc,2025-07-22 17:28:36,1
Intel,n4iqq5q,"While Iâ€™m sure you want to take advice from someone with that username all day, yes itâ€™s a good card. Itâ€™s certainly better for 1080 but especially with a string CPU is going to do most things very well. I have no issue at all with mine and it often stays so cool the fans donâ€™t even turn on.",buildapc,2025-07-22 13:16:14,1
Intel,n4iq9s0,"I'd say find a used card if you can as you'll usually get better value out of a used card. B580 is not a 1440p card, it's a 1080p card.",buildapc,2025-07-22 13:13:43,1
Intel,n4iql95,its a 1440p card wdym,buildapc,2025-07-22 13:15:29,3
Intel,n4is3qg,"Looking at benchmarks, it is really quite underwhelming compared to my old 3070 Ti (which I just sold for $295), which was enough to do 1440p at 144FPS at medium settings. It's definitely not a 1440p card that's going to give you high fps on modern titles. imo a proper 1440p card would be more like a card like a 5070 Ti which can actually do 1440p 144+ fps at highest settings, but 1440p and above with highest settings aren't really budget territory. You'll find that a majority of PC gamers are still playing at 1080p because 1440p isn't offering a great experience on cards lower than a 3070. And I would do the same myself if I had such a low end card.",buildapc,2025-07-22 13:23:48,-3
Intel,n6orlng,"Most tasks in 3D/CG/VFX are predominantly single threaded, until you simulate (not always), render or export something when more multithreaded performance is nice to have available. A 7600 has solid single core performance and in day to day performance will be great.  2x8GB is a mistake though, at least get 2x16GB - preferably 6000cl30 (cl32 and cl36 are fine if the price difference is big). You can save on the cooler for now, use the stock one (comes with a 7600) until she has enough saved for an upgrade.  The Intel Arc A750 is certainly a contentious choice. The only reason I'd go arc at that price point is if she'll be doing a lot of high res video editing, otherwise I'd look at alternatives, possibly second hand. A 3060 12GB would be a nice to have for 3D as NVidia is completely dominant in that space, but you'd probably need some second hand luck; I find them for around 200 EUR in the Netherlands. It'll perform a bit better than the A750 in gaming too, and has way better software support.  You can always save on the case and may be able to shave off a bit going for an mATX board, though less than 140 EUR for a wifi one is unlikely.",buildapc,2025-08-03 12:58:30,4
Intel,n6osl9e,"All parts look good but something to consider is some software just runs better on nvidia than others. I know this is true with video editing so maybe look at any applications sheâ€™ll need to run and see how the performance is before going with the intel card.   This is as good as I could manage being 100 over budget assuming the applications for vfx, cgi etc would play nicer with an nvidia card but donâ€™t know for certain.   [PCPartPicker Part List](https://fr.pcpartpicker.com/list/G3VgWc)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7500F 3.7 GHz 6-Core OEM/Tray Processor](https://fr.pcpartpicker.com/product/ms4Zxr/amd-ryzen-5-7500f-37-ghz-6-core-oemtray-processor-100-000000597) | â‚¬160.97 @ Amazon France  **CPU Cooler** | [Thermalright Peerless Assassin 120 SE 66.17 CFM CPU Cooler](https://fr.pcpartpicker.com/product/hYxRsY/thermalright-peerless-assassin-120-se-6617-cfm-cpu-cooler-pa120-se-d3) | â‚¬45.90 @ Amazon France  **Motherboard** | [MSI PRO B650-S WIFI ATX AM5 Motherboard](https://fr.pcpartpicker.com/product/mP88TW/msi-pro-b650-s-wifi-atx-am5-motherboard-pro-b650-s-wifi) | â‚¬135.83 @ Amazon France  **Memory** | [Patriot Viper Venom 32 GB (2 x 16 GB) DDR5-5600 CL36 Memory](https://fr.pcpartpicker.com/product/mbJgXL/patriot-viper-venom-32-gb-2-x-16-gb-ddr5-5600-cl36-memory-pvv532g560c36k) | â‚¬92.99 @ Amazon France  **Storage** | [Timetec 35TTFP6PCIE 1 TB M.2-2280 PCIe 3.0 X4 NVME Solid State Drive](https://fr.pcpartpicker.com/product/GPPQzy/timetec-35ttfp6pcie-1-tb-m2-2280-nvme-solid-state-drive-35ttfp6pcie-1tb) | â‚¬54.99 @ Amazon France  **Video Card** | [Gigabyte WINDFORCE OC GeForce RTX 3060 12GB 12 GB Video Card](https://fr.pcpartpicker.com/product/8d4Ycf/gigabyte-windforce-oc-geforce-rtx-3060-12gb-12-gb-video-card-gv-n3060wf2oc-12gd) | â‚¬301.27 @ Amazon France  **Case** | [KOLINK KLA-003 ATX Mid Tower Case](https://fr.pcpartpicker.com/product/QKQKHx/kolink-kla-003-atx-full-tower-case-kla-003) | â‚¬25.00 @ Amazon France  **Power Supply** | [Gigabyte P750GM 750 W 80+ Gold Certified Fully Modular ATX Power Supply](https://fr.pcpartpicker.com/product/RhH8TW/gigabyte-750-w-80-gold-certified-fully-modular-atx-power-supply-gp-p750gm) | â‚¬82.95 @ Amazon France   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **â‚¬899.90**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-08-03 15:03 CEST+0200 |",buildapc,2025-08-03 13:04:45,2
Intel,n6orkrx,well better go for 3060 that gpu is fine but it has some issues for a new users that can be challenging,buildapc,2025-08-03 12:58:21,1
Intel,n6oss38,"Most if the 3D / vfx software are optimized (written) for nVidia cards. I would avoid intel and AMD because of that. I've even seen some used RTX 3070 cards under 200 â‚¬. To avoid frustrating moments, i would rather choose something like that.",buildapc,2025-08-03 13:05:55,1
Intel,n6oto3p,Which country in the EU is your friend currently in?,buildapc,2025-08-03 13:11:24,1
Intel,n6p19qn,"Hot take here.   That first year, I can guarantee she's not going to do anything heavy duty, if at all. That's what the school computers are for.  I would say save and purchase a quarter of the way or half way through her first year, when the needs actually come to head. It'll be less money spent overall for something nicer.",buildapc,2025-08-03 13:55:47,1
Intel,n6qe8tc,* intel gpu is likely bad idea for someone who will use it for CGI etc - usually people recommend nvidia gpus for that with more then 8gigs of ram   ram - highly recommended getting 2x16gb as it will effect performance of everything and not wait for future upgrade,buildapc,2025-08-03 18:05:21,1
Intel,n743o2o,"You can get a ryzen 7 7700 from aliexpress for â‚¬150(i did have to pay 11 euros customs charge though) but it'll take 3 weeks to get to you.  As for the gpu, take the advice here and just get the 3060 12gb model. It's a great card for the money.  She will almost certainly need 32gb of ram. I have a laptop with a 4060 in it so nothing too intensive with 16gb of ram and was getting out of memory errors just playing marvel rivals so had to upgrade.  Get a cheaper case, you can get decent cases for 50 ish euros and money saved there will give you the extra for 32gb of 6000/30 ddr5 ram(think patriot viper 32gb(2x16)is like 90 euros on amazon.",buildapc,2025-08-05 20:40:05,1
Intel,n6q0yw0,"that motherboard can take 4 RAM sticks so I thought that she will just add another 2 in the future.    As for the GPU - I forgot to check if the software has some preferences or not. I was considering either A750 or RX6600XT, but I guess she will have to pay a bit more for gpu.   Thank you!",buildapc,2025-08-03 16:59:32,1
Intel,n7tpvxc,"Faster, but not white.  [PCPartPicker Part List](https://pcpartpicker.com/list/RDQQv4)  Type|Item|Price :----|:----|:---- **CPU** | [\*AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $179.99 @ Amazon  **CPU Cooler** | [Thermalright Assassin King SE ARGB 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/9Gstt6/thermalright-assassin-king-se-argb-6617-cfm-cpu-cooler-ak120-se-argb-d6) | $18.59 @ Amazon  **Motherboard** | [\*MSI PRO B650M-P Micro ATX AM5 Motherboard](https://pcpartpicker.com/product/LdHqqs/msi-pro-b650m-p-micro-atx-am5-motherboard-pro-b650m-p) | $109.99 @ MSI  **Memory** | [\*Patriot Viper Venom 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/4cCCmG/patriot-viper-venom-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-pvv532g600c30k) | $84.99 @ Newegg  **Storage** | [TEAMGROUP MP44L 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/2x4Ycf/teamgroup-mp44l-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-tm8fpk001t0c101) | $61.99 @ Amazon  **Video Card** | [\*ASRock Challenger OC Radeon RX 9060 XT 16 GB Video Card](https://pcpartpicker.com/product/BxRnTW/asrock-challenger-oc-radeon-rx-9060-xt-16-gb-video-card-rx9060xt-cl-16go) | $369.99 @ Newegg  **Case** | [Cooler Master MasterBox Q300L MicroATX Mini Tower Case](https://pcpartpicker.com/product/rnGxFT/cooler-master-masterbox-q300l-microatx-mini-tower-case-mcb-q300l-kann-s00) | $39.99 @ Amazon  **Power Supply** | [\*Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $89.90 @ Newegg   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$955.43**  | \*Lowest price parts chosen from parametric criteria |  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-08-09 15:39 EDT-0400 |",buildapc,2025-08-09 19:39:22,3
Intel,n7trskv,The motherboard already has a heatsink for the M.2 drive (and it's larger and will work better).,buildapc,2025-08-09 19:50:21,2
Intel,n7tn7ii,"can you post the full link of the build from pcpartpicker.com?   I see some potential here to improve on the build and will address your questions, however I'd need to know how many VMs you'd be running to determine if more CPU-cores would be useful in your case. Also, are your programs dependent on CUDA?  edit: also, do you have a microcenter nearby? They have some very nice bundles which can save you a good bit of money",buildapc,2025-08-09 19:23:58,1
Intel,n7ttt9h,"It looks like you may have posted an incorrect PCPartPicker link. Consider changing it to one of the following:  * [Use the Permalink](https://i.imgur.com/IW0iaOm.png). note: to generate an anonymous permalink, first click [Edit this Part List](https://i.imgur.com/uqDIcdt.png).  Or, make a table :    * [new.reddit table guide](https://imgur.com/a/1vo0GHH)   * [old.reddit table guide](https://imgur.com/C86vdxB)         *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",buildapc,2025-08-09 20:01:50,1
Intel,n7tnja5,"I have the same processor and theyâ€™re known to run pretty hot, as a result this makes my bedroom pretty warm. Itâ€™s a good cpu at its price but knowing what I know now Iâ€™d check for alternatives that are as powerful or better for the same cost or slightly more. Just something to consider",buildapc,2025-08-09 19:25:51,1
Intel,n7tuifw,Thank you!,buildapc,2025-08-09 20:05:48,1
Intel,n7ttpvt,"Here is the link to my [pcpartspicker build](https://pcpartpicker.com/list/zNm8gn). In my studies, I've never needed more than one VM at any given time, and the programs I use are somewhat reliant on CUDA, but it's not essential",buildapc,2025-08-09 20:01:17,1
Intel,n7tuer1,"wrong assumption, just because it runs hot does not mean it dumps a ton of heat into your room. The used electricity is what determines how much it heats up the room, and GPUs use 3-6 times the wattage of a modern AMD CPU.",buildapc,2025-08-09 20:05:13,2
Intel,n7ttz28,"Thanks, I appreciate that insight! Tells me I probably would need more fans too",buildapc,2025-08-09 20:02:45,1
Intel,n7u0b0g,"That Montech PSU is likely only a tier C, doesn't deal well with sagging voltage on mains.  I'd go with something better and likely a bit more expensive such as one from Corsair.",buildapc,2025-08-09 20:38:26,1
Intel,n7twa74,"ok now it worked - there you go  [https://pcpartpicker.com/list/js2RPJ](https://pcpartpicker.com/list/js2RPJ)  cheaper parts, same performance. Unless you want the AIO for looks I wouldn't keep it or just use a cheaper Thermalright model with a 240mm radiator. So if you just need one VM a 6 core CPU is more than enough, if it isn't essential the extra VRAM of the card should also boost productivity.",buildapc,2025-08-09 20:15:41,2
Intel,n7tu8gu,your list is private,buildapc,2025-08-09 20:04:14,1
Intel,n7uggmb,"Thank you, I appreciate your help",buildapc,2025-08-09 22:11:31,1
Intel,n7twieu,whoops try now [https://pcpartpicker.com/list/zNm8gn](https://pcpartpicker.com/list/zNm8gn),buildapc,2025-08-09 20:16:58,1
Intel,n3e2jm3,9060 XT is just like 10% better. Stay with the B580 until 2027,buildapc,2025-07-16 04:15:52,10
Intel,n3eahsp,9060xt is prolly faster but for the price/p b580 prolly is the best rn  if u have the b580 then just stick with it until the next intel card like celestials or something unless ur running into issues with ur games i guess,buildapc,2025-07-16 05:22:13,4
Intel,n9s2z5x,"I saw you bought the b580. Considering it, can you tell me how it's doing so far? Any issues with any games?",buildapc,2025-08-20 20:54:11,3
Intel,n3ey6eh,I wouldnâ€™t fidget if I already had B580 coming in. Itâ€™s got 12GB of VRAM and quite a bit cheaper than 9060XT 16GB and not that much worse of a performance. Especially if youâ€™re planning big upgrade in 2027,buildapc,2025-07-16 09:00:22,2
Intel,n4aiete,I have the Steel Legend B580 and Ive had corruption and cold crashes with Hitman 3 and the latest drivers removed my optimizations as the features were gone!  So it has been frustrating if not annoying however it makes me sad to get rid of it like the others say when it works nice thats nice.  I'm calling the B580 a hassle but its not junk its simply not getting top tier support for layered driver support.   Bought the Sapphire Pulse 9060XT OC 16Gb and it matches is little siblin' the 6600XT OC i still use on my second PC.  Hitman 3 for me went from 127 FPS down to at least 60 fps with the former Tessellation off.  The Radeon is everything the B580 promises actually.  The AI cores on the B580 were interesting. I lost about $30 in the mix but Ive had the B580 all Summer.  Sad,buildapc,2025-07-21 05:21:25,2
Intel,n3e2kr3,"If you are not having any issues, then why change? The B580 is a pretty good card (I recommend watching LinusTechTips video on it, 2 people changed their 4090s or high end cards and used ARC for some time). Plus, its performance is just getting better wich each driver update, and more games are adapting XeSS faster than FSR (lol).",buildapc,2025-07-16 04:16:05,1
Intel,n3e2x30,"okey thanks for the suggestion, good day mate!",buildapc,2025-07-16 04:18:36,4
Intel,n3ezuhg,"Daniel Owen did a comparison and it's like 30% or more difference between the two, not 10%, so imho if he can, just get 9060xt 16gb, he get 30% better performance with more vram for 40% more cost if we use MSRP pricing for both, but yeah if he choose to get B580 now then there's no point in upgrading to 9060xt, 30% is hardly a worthwhile upgrade, and B580 already has enough vram at least until next gen console arrived",buildapc,2025-07-16 09:16:31,2
Intel,n9tjmh8,"well I had no problems so far, I played quite a few games so far already, never had an issue",buildapc,2025-08-21 01:47:52,3
Intel,n4aiw7d,..and if you haven't made the buy yet the extra $70 is well worth it. Likely $85 difference.  Count the extras. This thing trades hit with a 5060 Ti and the B580 somewhere well under and can perform nice (not missing much)  for you there for the $ but the hassle!,buildapc,2025-07-21 05:25:29,1
Intel,n3e2v3y,"Sounds like I made a good deal. Actually, I am yet to experiment with the Arc card, as I am getting the card today. Can you tell me what should be the ideal way to test the new card and what should be the ideal temps while gaming? (ASRock B580 Steel Legend 3 fan) I got",buildapc,2025-07-16 04:18:12,2
Intel,n3f09mg,"Don't listen to him, raw perf the 9060 xt is 20 to 30 percent better. If you account for vram it can go to 3-4x times better because of the b580 bottleneck. Also b580 has cpu bottleneck issues. Just spend 100 bucks more for a 9060 given you won't upgrade till 2027. If you're on budget you could just get a i5 14400f with ddr4, this being said ddr4 price somehow skyrocketed, i had my 32gb for 45 euros like 2 month ago but now it's much more i believe.",buildapc,2025-07-16 09:20:37,2
Intel,n4aji43,Actually purchased b580,buildapc,2025-07-21 05:30:44,1
Intel,n4akdx2,..B580 had it all on paper.  B580 CAN play ball but will it today ....  Doesn't deliver well enough for your money.  First game that is annoying you will be doing what I did.  Its pretty nice at $250 MSRP only as others declare simply because of the cheap feature set. Should have performed around the $350 mark and we all expected it to but a long time ago.  This is where the Enthusiast market was excited.  $369 Sapphire Pulse 9060Xt OC 16Gb is about as good as it gets right now.  Then again I drive a 2018 Dodge Challenger V6 AWD not the V8 HEMI.  lol  Two different franchises!,buildapc,2025-07-21 05:38:19,1
Intel,n3e3ous,"I should not worry for temps, the B series improved efficiency... Just play your games, and actually, the B580 has better performance on 1440p. Not literally, rather than it performs better in higher resolutions.  And look into OC the safe way. Which is basically increasing power limits and adding boost, which makes the card to clock higher with same voltage, isn't dangerous.  And INSTALL ONLY WHQL DRIVERS. Basically this one's are like the Polished version of the first release. Like, they send v1, people download and test it (it's not a beta driver, is supposed to be stable), complain about issues and later they send v1 WHQL, which is like the 100% stable version.  I never had issues with any WHQL drivers. People that installed the newest (not WHQL) always have problems.. I suggest joining the Arc sub and ask any question you have.",buildapc,2025-07-16 04:24:15,2
Intel,n3f49k8,12gb is more than enough for 1080p,buildapc,2025-07-16 09:58:25,2
Intel,n774roc,"you arent the sharpest tool in the shed are you mate, the B580 doesnt have cpu bottleneck issues, it just has compatibility issues with AM4, I run it with a 7600x so this guy with a 7700 wouldnt have the slightest of issues.",buildapc,2025-08-06 08:37:49,2
Intel,n4aknkb,What did you pay?  I will miss mine a pinch as it was weird. I am currently marketing my 3d Voodoo 5 6000 for auction. Good luck you may have exactly what you need.,buildapc,2025-07-21 05:40:42,1
Intel,n3e40cb,"you're the champion, thanks for explaining it much detailed appreciate it, and [https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html)  is this one you talking about?",buildapc,2025-07-16 04:26:36,1
Intel,n3vtzrq,but not for 1440p or 4k.,buildapc,2025-07-18 20:48:29,1
Intel,n4amijn,"I paid more than 300 USD because the market is just like that and for 9060xt it's over 500 here   So I had no options, I didn't played hitman 3 yet, btw did you tried the cracked one or from steam?  Game crashes for many reasons, so far I played 5 games in my PC, valorant, vanguard, carxstreet and the finals  I got no issue of crashes yet, temps are at 50 at idle.",buildapc,2025-07-21 05:56:45,1
Intel,n3fnj5s,"Yes, sorry for the late response, reddit was having problems and didn't send my message. As long as you use that one, everything should go smoothly. Feel free to dm me or ask on the arc subreddit for any questions/help.",buildapc,2025-07-16 12:23:06,2
Intel,n3vu889,"Yeah, something which op clearly doesnt care about",buildapc,2025-07-18 20:49:38,1
Intel,n4nwbrm,You wont have temp problems unless you live in the desert with no AC,buildapc,2025-07-23 05:35:07,2
Intel,n4nw7ye,Mine was exactly 300. I like its AI cores.  It produces way better images than my 6600XT and noone but noone simply comments on overall image quality ..anyway I have hundreds of legal Steam games no cracked eh I also own the Epic games version.  See this damn card had a driver update while I had it that improves the performance and that's good its what we want but once the features drop and performance drops (for me anyway) after the next driver update I had to call it.  Its the poor driver software stack.  The hardware has serious potential!  A770 is maxed.  I see this B580 ownership  as I leased the card for the summer +$60.      Can you buy from the [Amazon.com](http://Amazon.com) USA website for our cost?? Shop our stores in the USA.   I should remind you the Steam cold crashes suck.  I have an Asus 850w Platinum for it and I know there's no power fail issue sooo its a goner.  Im old enough to say ive seen this before.  Dont hang out until its done right so long after its release.  Not a hot buy now imo.  There's the AsRock Challenger or the Sapphire Pule within $10 of each other. $359-369.  Newegg and Amazon,buildapc,2025-07-23 05:34:13,1
Intel,n3frwt6,Thanks man,buildapc,2025-07-16 12:49:25,1
Intel,n3vw4lw,"You're right my bad, I've quickly re-read the text before replying, without re-reading the obvious title.",buildapc,2025-07-18 20:59:08,2
Intel,n6ses8s,"Honestly? If I were you Iâ€™d get a 9060xt 16gb for 350-370 and then enjoy your $400 saved.  Reason being that with your setup, once you start moving up in GPU, youâ€™re not gonna see as big of a performance uplift because of your 5700x, which will start to slightly but noticeably bottleneck a 9070/5070 or above  Which if you did go with a 9070/5070, say you get one for $600, youâ€™d need a new PSU, which is around $70-$100. And at that point, itâ€™s not worth paying $300 extra for a 25-30% performance uplift at 1440p because thats a 50-60% price increase.  You could upgrade your CPU sure, but then at that point, with a new CPU/RAM/MOBO/GPU and Power Supply, thatâ€™s basically just building a whole new PC, and youâ€™re not gonna be able to do that for under $800, nor should you because again, the price increase isnâ€™t worth the performance increase.  If I were in your shoes, Iâ€™d get the 9060xt 16gb and ride out the rest of my PC for 4-5 more years playing at 1440p. After that, if Im unsatisfied with the performance and frames im getting in games, Iâ€™d sell it/keep it as a spare and start from scratch a brand new build ground up. And with the $400 youre saving today, why not get some games, a peripheral upgrade, a second monitor, or shit you could always just pocket and save it. More money on hand is always a good thing.",buildapc,2025-08-04 00:37:22,3
Intel,n6qmv32,"9070 non xt and a psu, nvidia prices for the same perf and vram amount is nuts. I say that running nvidia.",buildapc,2025-08-03 18:49:38,5
Intel,n40tox6,"While drivers have gotten a lot better Arc especially Alchemist still has the occasional issue, but it is very rare in gaming these days. I've noticed though that Adobe's software really doesn't play nicely with Arc Alchemist, so if you're using applications like photoshop or after effects that's something to keep in mind. You're also looking at more CPU overhead, so if you have a weaker CPU it might not be the best idea.   I also don't know how much I'd trust Intel with long term driver support for the a750. Their GPU division is on shaky legs and given that arc alchemist was never that popular I wouldn't wager a lot on them spending a lot of resources on long term support    On the flip side you get much better ray tracing performance with the a750 (although not really relevant for either card in this performance tier imo), a vastly superior upscaler (xess runs on AMD cards but looks and performs considerably worse than natively on Intel) and a bit more rasterized performance.  With a 5600x I'd probably pick the a750 if you're willing to put up with the occasional smaller problem.",buildapc,2025-07-19 16:54:43,1
Intel,n41jd31,whats your total budget? if its within 400 then i'd just pull the trigger on the rx 9060xt,buildapc,2025-07-19 19:08:18,1
Intel,n40wdnv,Hey there thx for the info. I was also considering an rtx 3060 12 gb which would be the better pick in terms of future proofing however I am a little hesitant about it as I think the 5600x might be too weak for the 3060. What do you think? Will the 5600x and 3060 pair well?,buildapc,2025-07-19 17:08:26,1
Intel,n440ja7,"Hey there my budget is 350 dollars for gpu, in this price range i found a new rtx 3060 should i grab it?",buildapc,2025-07-20 04:04:20,1
Intel,n40x67o,The 5600X is fine for the 3060. They're both relatively lower end parts from the same timeframe.,buildapc,2025-07-19 17:12:22,3
Intel,n44c6ud,Rx 9060xt 16gb in your region not available for 350? If it is at that price then get it instead of the 3060,buildapc,2025-07-20 05:35:55,1
Intel,n40zv5j,"Oh, thats great! In that case i will go for the 3060, thx for clearing my confusion!",buildapc,2025-07-19 17:25:55,1
Intel,n5v0wco,9060 xt 8gb is the fastest card under $300. theres no such thing as a futureproof card under $300 it will be outdated in a matter of a couple years.,buildapc,2025-07-29 20:46:38,24
Intel,n5v3njr,"The most ""future proof"" one is the fastest card you can afford with the most amount of VRAM  Buying new, your options are basically either a 9060 XT 8gb or 5060 8gb both at $299 MSRP. Lots of people are proclaiming ""8gb isn't enough in 2025"" but it all depends on your setup, what games you play, and how you play them.  Planning to play only AAA games at ultra/max settings at? With full raytracing/path tracing enabled? Not even gonna use DLSS/FSR to upscale from a lower resolution? Yeah 8gb is already not enough.  Planning play at 1080p? Just some AAA games? Willing to lower settings when needed to something like medium? Planning to use DLSS/FSR to upscale from a lower resolution? Don't really care about Ray Tracing? You'll be fine still.  You could consider buying used too.",buildapc,2025-07-29 20:59:20,10
Intel,n5vbdea,Easily the best option is the b580. It's better and easier than second hand options. And should be comfortably under your budget. Btw it outperforms a 5060 above 1080p and is more future proof. Please buy that not any over option.,buildapc,2025-07-29 21:36:58,7
Intel,n5uzrl2,RTX 3060 (12 GB) or B580 are both great options.,buildapc,2025-07-29 20:41:24,18
Intel,n5veqqu,"Assuming your actual max budget is 300 flat the b580 is best bet. Currently being for ann Asrok one from New Egg around 240-270. Do with tax itâ€™ll be 300. If you can go a bit higher than 100% 9060xt. Best bang for buck and better performance. Just saw one for $308. I checked eBay and didnâ€™t like the price to performance I was seeing. Â The 16gb model will obviously be the most future proof, and AMD isnâ€™t going anywhere in the next few years. Canâ€™t say the same about intel. They are likely to stay but not guaranteed. If you just so happen to see a reasonably priced 5060ti 16GB that is a great option too and you can use Cuda cores if you wanna use blender.Â   I use hardware unboxed for benchmarking: https://youtu.be/-LAH5vh-Cpg?si=zPDp-_QMPEJZbfoK",buildapc,2025-07-29 21:53:52,2
Intel,n5vium3,amd rx 6800 (used),buildapc,2025-07-29 22:15:04,2
Intel,n5vre93,look used? i just sold my 4060 today for $200,buildapc,2025-07-29 23:00:34,2
Intel,n5w0ped,Stay away from anything 8gb VRAM,buildapc,2025-07-29 23:50:57,2
Intel,n5xvzdx,"I bought a used rx 6800xt 16gb for $320, imo for the 300ish price range a used gpu is ur best bet u can find mid ranged gpuâ€™s in the $280-$320ish price point",buildapc,2025-07-30 07:45:16,2
Intel,n5vg1ja,8gb is not futureproof. It's not even enough right now.  A 9060xt 16gb or a b580 would be a better choice,buildapc,2025-07-29 22:00:31,2
Intel,n5vhf5w,Just got an ASUS TUF RTX 3080 for $280.,buildapc,2025-07-29 22:07:40,1
Intel,n5vhqzv,"If looking second hand, consider Rx 6800, 6750, 6700xt, rtx 3070/ti, 3080.",buildapc,2025-07-29 22:09:22,1
Intel,n5vjk36,B580,buildapc,2025-07-29 22:18:47,1
Intel,n5vmpmy,Used 1080ti,buildapc,2025-07-29 22:35:28,1
Intel,n5w2q6y,"Used market is your best bet right now. Other 2 new options are a little underwhelming right now, intelâ€™s b580 and AMDâ€™s 8 gb 9060xt. Both decent cards in their own right, but both with some glaring flaws. (Work in progress drivers and only 8 GBs of ram, respectively.)  On eBay/fb marketplace/jawa.gg, if youâ€™re patient, you can probably pull off a RTX 3080/Radeon 6800 xt for $300 or very close to it. Both of those absolutely manhandle the 9060xt and b580 in raster.",buildapc,2025-07-30 00:02:17,1
Intel,n5w6sio,For that piece I'd try to get a faster used card rather than a slower new one.,buildapc,2025-07-30 00:25:29,1
Intel,n5w86ys,The rtx5050,buildapc,2025-07-30 00:33:39,1
Intel,n5wdv9l,"If you're not adamant about getting brand new and are not in a rush, I'd suggest buying used. You can get lucky to snatch one for a really good deal. I was lucky enough to snatch a 2080 super for $100 (upgraded from 1070 gtx).",buildapc,2025-07-30 01:06:11,1
Intel,n5wmlhg,I've have the 16gb 9060....save the extra 50 or 60 bucks would be my advice. L. 9060 is great I can play much more at much higher settings than I thought.  Like I just started everspace 2 and it default to4k epic settings,buildapc,2025-07-30 01:56:50,1
Intel,n5wyz79,"At under $300 USD, you get what you get. At that price point, you're not getting future proof, you're getting whatever shit GPU manufacturers think you'll pay $300 USD for.",buildapc,2025-07-30 03:13:04,1
Intel,n5xn6on,"if you want something with the most performance, the 9060xt 8gb is the best option also because its readily available below its msrp quite often, ive seen it as low as the msrp of the 5050. but if u can snag a b580 within this price range i'd take it that instead",buildapc,2025-07-30 06:22:00,1
Intel,n5xuwqh,if you can save a lil money and up it to 600 you can usually snag a 9070 non XT off newegg thats what i did in my opnion 8gig cards arent worth it new in box uness you find one super cheap 12gig or better is the way to go however if 300 is your limit buy used,buildapc,2025-07-30 07:34:39,1
Intel,n5yhdzn,"ASRock Challenger OC Arc B580 (12GB), on newegg and amazon for $270 at this point in time  Sapphire PULSE Radeon 9060 XT (8GB), on newegg and amazon for exactly $300.  Zotac GAMING Twin EDGE RTX 3060 (12GB), on newegg for $260 and on Amazon for $283.   All of these listings were grabbed from PCPartPicker, they may not be accurate in a later point in time.",buildapc,2025-07-30 11:02:43,1
Intel,n5ylklg,"Could you push 50 more bucks for that sweet price to performance Rx 9060 XT 16 GB GPU ?? That card is so much better and not that expensive , it's great for 1440 and godlike for 1080.",buildapc,2025-07-30 11:33:17,1
Intel,n629slk,"Get a 5060 , it's not about the hardware software side too nvidia is unbeatable even though they are greedy AF",buildapc,2025-07-30 22:28:13,1
Intel,n5uynhv,"at that price you wont get anything decent new , you might have to go 2nd hand , really depends how demanding your needs are",buildapc,2025-07-29 20:36:13,0
Intel,n5v8vru,"Definitely not in today's money or product offerings, but a 1060 6gb lasted until basically a year or two ago.",buildapc,2025-07-29 21:24:29,2
Intel,n5vam34,Disagree at that price you can go used and get a faster card,buildapc,2025-07-29 21:33:05,1
Intel,n5vntxj,"If you're planning on sticking with 1080p (and for the love of god if you're on a very strict budget, do that), 8gb will likely be at least adequate for quite a while.  IMO the vram wars in these comment sections have incredibly little bearing on the actual benchmarks.",buildapc,2025-07-29 22:41:26,7
Intel,n5xf2ny,"Yeah it just depends on what you play. If OP wants future proof gpu and focuses on single player games (mostly AAA games) under $300, an 8 gb wonâ€™t just cut it. If you watch this q&a vid from HUB, their response and reasoning makes so much sense.   https://youtu.be/CN6SlMCZvtA?si=yrb8dQ4y7rNBb1rm (starts at 17:04)",buildapc,2025-07-30 05:10:40,1
Intel,n5v5v8f,"Yeah. In 2025, but in 5 years later with 8gb you're doomed.",buildapc,2025-07-29 21:09:51,-4
Intel,n5vpbe3,"A 3060 in the year of our lord 2025 is not a good call.    This current generation has somewhat disappointing performance gains relative to cost and the previous gens, sure.  But not *that* disappointing.    Also, remember that all else held equal, a newer card is better for driver support and game support if nothing else.  In 4 more years, devs are not going to be thinking very hard about the 3060 when optimizing, and they will be thinking about the 9060 and 5050.  Will they be thinking about intel?  Will intel still be developing its drivers?  Will intel even be in the GPU industry at all?  Who knows.",buildapc,2025-07-29 22:49:22,38
Intel,n5vmopa,I would think that way six months ago. Truth is there is not a single game the 3060 beats the 9060 8gb or the 5050. Welcome to 2025.,buildapc,2025-07-29 22:35:20,5
Intel,n5vzmxq,Iâ€™ve also seen used Rtx 3080 10gb cards for as low as $300,buildapc,2025-07-29 23:45:03,5
Intel,n5vvu7p,"damn, lucky find lol",buildapc,2025-07-29 23:24:28,1
Intel,n5w4ccr,"just found a 3080 10gb for $340, is that a better option",buildapc,2025-07-30 00:11:27,0
Intel,n5uyv4p,"What cards would you recommend 2nd hand? I want to be able to play games at around 1080p medium, plus maybe some AAA games like Spiderman",buildapc,2025-07-29 20:37:12,1
Intel,n5wnjxn,im speaking strictly new cards. used always has better options.,buildapc,2025-07-30 02:02:25,1
Intel,n5var3o,Well hopefully youâ€™d be able to upgrade by then,buildapc,2025-07-29 21:33:48,5
Intel,n5v8pqx,"Truthfully, thatâ€™s an opinion because we donâ€™t know the future of gaming requirements and we donâ€™t know OPs preferred gaming style nor setup  Im using a 3080 10gb for 1440p for the last 4 years and Iâ€™ve never come close to the VRAM limit. Iâ€™m sure it could last me another 4 years of usage but by then Iâ€™ll probably have an upgrade with more VRAM. People are shitting on the 5070 for only having 12gb for its intended 1440p usage, but in my case it would be more plenty.",buildapc,2025-07-29 21:23:40,2
Intel,n5vzpvs,"If I could find those cards under $300, I'd recommend them. I would have recommended the RX 7600 XT but it rarely goes below $329 or $349 in my region. These newer cards are hitting above $400 and I won't recommend 8 GB cards.",buildapc,2025-07-29 23:45:30,2
Intel,n685y4h,"Me, as someone who got a 3060 for free and has been thrilled with the games I play: ðŸ¥¹ðŸ‘‰ðŸ‘ˆ",buildapc,2025-07-31 20:05:01,1
Intel,n6ngcpj,"The sad part is 5060 gpu die itself is actually decent, same with 9060xt with hub conclude that they never seen such huge jump for a while, if only they're equipped with at least 10gig they would've been the definitive choice, sadly here we are...",buildapc,2025-08-03 05:59:09,1
Intel,n67qq28,You can find them pretty regularly in the 300 to 350.,buildapc,2025-07-31 18:51:28,1
Intel,n5wcp9n,"depends how desperate you are. $340 isnâ€™t a bad price for it per se, but I would either hold out for closer to $300 3080 or would bump up to 350-360 for the 16 gb 9060.",buildapc,2025-07-30 00:59:32,1
Intel,n5we1nn,Can your PSU handle it?,buildapc,2025-07-30 01:07:14,1
Intel,n5xbeye,"I bought a 3080 10gb for about 355 usd last February. Got 1.5 year warranty since it was a zotac card, and 1 month return from the reseller side (ive known him personally for 4-5 years) i think u can find better offers for a 3080. Considering 300$ budget, i'd assume you are gonna be using 1440p or 1080p high fps, both of which are somewhat safe. But if u prefer aaa games then consider getting something like 6800xt for slightly more vram, native support for frame generation as compared to 30 series nvidia and similar performance. If u dont like amd for any reason, then 4070 is gonna be the best card in the second hand market, but u might have to wait a few weeks to months to find one in your price range. Ps. I dont have any experiance with intel cards so i cannot suggest that personally.",buildapc,2025-07-30 04:41:29,1
Intel,n5uzpiu,Maybe a 3070? You can find them for about 250USD on eBay,buildapc,2025-07-29 20:41:09,3
Intel,n5uz59o,"Strongly recommend stretching for an RX 9060 XT 16GB. But if you absolutely cannot go above 300, then an RTX 5060.",buildapc,2025-07-29 20:38:32,4
Intel,n5v48sz,"What's your target frame rate for1080p medium? For 60fps, pretty much anything xx60 / X600 class and up from the last 3 generations will do. Get at least 12GB vram, preferably 16.",buildapc,2025-07-29 21:02:08,2
Intel,n5vonb5,"IMO if your budget is *that* strict, buying new is always the way to go.  Slightly underpowered gaming is a lot better than no gaming, which means you want a factory fresh card with a warranty.  Is the increased fps per dollar really worth the increased risk of no fps at all?  To me it wouldn't be.    Secondhand cards are always going to be at least a little bit of a gamble.  I mean hell, new cards are a gamble too, even! No product has a 0% failure rate.   But a gamble under warranty is a lot different than a gamble off of ebay.",buildapc,2025-07-29 22:45:47,1
Intel,n5vliu2,Wow I play at 1440p  on a 5070 and almost always reach to the limit on vram or close to it.,buildapc,2025-07-29 22:29:09,3
Intel,n5wohbp,> I won't recommend 8 GB cards.  IMO by far the best cards in this price point are 8gb cards and I think the comment section vram wars on the subject have very little to do with any benchmarks.,buildapc,2025-07-30 02:07:47,2
Intel,n5vvt4f,"Buying off eBay using PayPal gives you amazing buyer protection. As long as you read the listing & make sure itâ€™s not listed as broken or unknown condition PayPal will make sure you at least get a working card or your money back. It wonâ€™t help you if the card dies 3 months after purchase, but electronics typically fail very early into their expected lifespan or past their expected relevancy. That typically means if you get a working card from the last generation or two you will likely enjoy it until itâ€™s obsolete.",buildapc,2025-07-29 23:24:19,1
Intel,n5vonlj,"For me its probably because I hardly play any AAA games, donâ€™t care for turning on Ray tracing, and Iâ€™ll set everything to low/medium on basically every game except the easy to run ones (cs2, valorant, ow2, etc). Iâ€™ll use DLSS if itâ€™s available and If I feel I need it for more fps. Running a 1440p 240hz monitor so I prefer FPS over everything else. Because of my setup that doesnâ€™t tend to be GPU heavy, I felt the 7800x3d was appropriate even for my old 3080 10gb. Big uplift over the 10700k which otherwise wouldâ€™ve been fine if I played different games/used higher settings or had a 1440p 144hz monitor.   I feel like the 5070ti is enough of an uplift over the 3080 10gb to one day get it, but at this point I can hold off for another year until the rumored Supers. My build is still good for my needs.",buildapc,2025-07-29 22:45:50,1
Intel,n629910,That's the thing these newer cards seems to be using a bit more Vram than usual,buildapc,2025-07-30 22:25:19,1
Intel,n5vq5o6,A 3080 is still a beast for a few more years imo,buildapc,2025-07-29 22:53:54,2
Intel,n5wigvk,I think you could hold of a few years based on your usage. Maybe wait until the next console generation to get a new gpu,buildapc,2025-07-30 01:32:55,1
Intel,n5wwkv3,"I do wish I had just a smidge more performance on some games, which is why Iâ€™ve been researching the market. Was hoping I could pay (or pay less) what I paid for my 3080 ($699 right after launch) now and get a nice noticeable uplift, but itâ€™s a shame itâ€™s come down to â€œfake framesâ€ in order to get that uplift at a lower price like the 5070. Iâ€™d rather not rely on frame generation. I do like my FPS but not at the cost of added latency or artifacts",buildapc,2025-07-30 02:57:25,1
Intel,n4lznbo,When I was researching my 5 build people tended to say just go with the 7600x and if you want something better get a x3d cpu.  My 7600x is paired with a 7800xt and its doing great,buildapc,2025-07-22 22:32:05,12
Intel,n4m0mqm,240aio is super overkill   A 35$ air cooler can do the job,buildapc,2025-07-22 22:37:26,6
Intel,n4m0mcb,9600x is more power efficient compared to 7600x. It's up to you if the price difference is worth it.  A cheap air cooler from thermalright would be enough to cool either chip you'll get. No need for an aio unless you want aesthetics.,buildapc,2025-07-22 22:37:22,12
Intel,n4lztzq,"No AIO needed....    For gaming 7600x is the better deal. 50$ price gap isnt worth it for the 9600x, maybe maybe if you do workstation stuff but even then not.",buildapc,2025-07-22 22:33:05,15
Intel,n4mf8i7,I went for the 9600x due to to lower TDP. But the price difference was much smaller in my region.   Other can be air cooled easily and their performance is near enough to identical.,buildapc,2025-07-22 23:55:59,4
Intel,n4njpt9,Gap in gaming is almost no existing so buy the 9600x only if it is almost thr same price or you need single core performance for some reason.,buildapc,2025-07-23 03:58:56,2
Intel,n4m140u,"About AIO, please DO NOT put an AIO on a Ryzen 5, heck not even a Ryzen 7 either. It's extremely overkill and it's really not the most reliable option. An AIO is best kept for a Ryzen 9 or Intel Core i9/Ultra 9 where you want to maximize the performance. Ryzen 5s are relatively low wattage chips and are really easy to cool. Go for a Thermalright Air Cooler like the Phantom Spirit, it's just $35, and even that is far far more than you need for cooling.  AIOs have pump failures, in rare cases can leak, have lower useful lives, are often significantly more complicated to fit in a case, and have annoying software supposedly. An air cooler is simple, lasts way longer, and failure is not catastrophic and is very obvious unlike an AIO where you have to investigate if it is a fan, pump or whatever and they don't leak.  I personally run a gen 1 Noctua NH-D15 on my Core i9-12900k and it does what I need. Some of the best air coolers are rated for up to 250+ watts (a Ryzen 5 is not getting anywhere close to that at all, probably up to 105W as far as I can see at maximum, but usually you are never touching that). It's a huge misconception that AIOs are better in all scenarios.",buildapc,2025-07-22 22:40:01,7
Intel,n62v6pg,Aios and high end air coolers are pretty similar performance and price-wise nowadays. Air is much more reliable. Aios look much better.,buildapc,2025-07-31 00:25:35,1
Intel,n62uotu,cough\* cough\* phantom spirit 120 se,buildapc,2025-07-31 00:22:41,1
Intel,n4m26yv,"Yeah,only reason im thinking about aio is for asthetics,i love the look... And for $50 i can get a 240 or a 360,same brand aswell Id cooling fx240/fx360",buildapc,2025-07-22 22:45:52,3
Intel,n4nqbh5,9600x being more power efficient is purely a misconception because AMD decided to run it at lower base tdp. Enabling eco mode on 7600x is gonna net about the same power efficiency.,buildapc,2025-07-23 04:46:57,2
Intel,n4o1e59,Why are you being downvoted,buildapc,2025-07-23 06:18:44,3
Intel,n62vfom,"Yep, that's what I have, with a phase change pad on my 9800x3d",buildapc,2025-07-31 00:27:04,1
Intel,n4mzsmp,Thermalright sells 360mm AIOs for about $50-60 USD too. Check them out.,buildapc,2025-07-23 01:53:33,3
Intel,n4pfto0,"Theres basically no performance difference, and the 50$ you save on the 7600x you might as well get that aio you want",buildapc,2025-07-23 13:05:55,1
Intel,n4o6vjp,"because of elitists who cant take the truth they wasted 100-120â‚¬ on one, they could have just spend 30â‚¬ and be fine.",buildapc,2025-07-23 07:08:21,4
Intel,n96ogv5,Do you have a (amazon or other) link for the phase change pad? I've never heard of that before and it sounds like it's a reusable thing instead of having to deal with thermal paste?,buildapc,2025-08-17 14:50:09,1
Intel,n62uw6j,"the only aios that are 120$ are overpriced. Artic is cheaper and usually performs much better. No ones buying aios just for performance, it's for aesthetics.",buildapc,2025-07-31 00:23:52,1
Intel,n96zeoj,"Its one time use, but it doesn't dry out like thermal paste does. Has a much longer service life, which is basically the lifetime of the pc.   https://www.amazon.com/Gelid-Solutions-HeatPhase_Parent/dp/B0DCN7DVWS Price went up 4$ since I used it.   Its very close to as good as liquid metal.   Its adhesive. You sticky it to your heat sink then put the heat sink down.   Then when it gets hot, it turns into a liquid, and then when it cools down, it turns back into a solid",buildapc,2025-08-17 15:46:10,1
Intel,n66neir,"""performs much better""  I have a phantom spirit se with a gelid phase change pad, and my 9800x3d cinebench r23 is 24422, which is one of the highest scores I've ever seen on the internet, with a stable overclock.",buildapc,2025-07-31 15:49:26,1
Intel,n973hyf,"Ok that sounds awesome based on the science alone lol.   I'll check it out. Finally pulling the trigger and getting a new cpu, mobo, cooler, and such to go with my 1 year old 3080. Mr. i7 8700k isn't really cutting it nowadays on the new AAA titles  edit: or The Finals, which I've recently gotten into",buildapc,2025-08-17 16:06:43,1
Intel,n66szje,But the lf3 360s still 4C cooler. And the lf3 420s 7C cooler than the phantom spirit,buildapc,2025-07-31 16:15:34,1
Intel,n974j6l,"Its a log easier and cleaner than thermal paste, atleast   And it outperforms it by a degree or 2   And lasts longer   I love the stuff",buildapc,2025-08-17 16:11:54,1
Intel,n66ss53,"It's worse than that   [https://cdn.mos.cms.futurecdn.net/U2LNciPo8F4uXUgbYZo2id-1200-80.png](https://cdn.mos.cms.futurecdn.net/U2LNciPo8F4uXUgbYZo2id-1200-80.png)  The phantom spirit evo here is out performing many AIO coolers  Which admittedly is not the se, but it's very close",buildapc,2025-07-31 16:14:35,1
Intel,n66xj7b,"What matters are the results  I'm already getting god tier cinebench scores with my phantom spirit se, with a 35$ cooler, and having spent 6$ on the phase change pad  I'm doing +200mhz OC, and am getting that on all cores during a cinebench workload... and gaming temps are in the 60C range  What is a liquid cooler supposed to do when I'm already max overclocked using +200mhz pbo boost?  I'm sitting on a 120w cpu which with curve optimizer isn't even using 90w, and the thermalright cooler can keep 234w@38DB@95C",buildapc,2025-07-31 16:37:06,1
Intel,n9791wa,Sounds like an absolute deal,buildapc,2025-08-17 16:34:39,1
Intel,n258qe4,"For what it's worth, I got a used 3070ti that was used for mining from a trusted friend for 350â‚¬ back in 2023 and I have been loving it.  I've played games such as Cyberpunk, BG3, GoW 1&2, Hogwarts Legacy, Expedition 33 and some more.  I game on a 1440p 165hz monitor and my goal is at least 80fps with dlss quality on. If I have to lower the graphics to achieve that it's going to be the shadows turned down to medium and everything else set to max (RT OFF).  I haven't ran into any VRAM issues at all, and I see myself upgrading with the 6000 series only if I don't hit that 80fps.   Currently playing KCD2 at around 110fps but I have not been to a big city yet. Looks amazing.   I'd say if you can find it a bit cheaper go for it!",buildapc,2025-07-09 09:33:55,3
Intel,n258rgc,Youâ€™re right budget wise the 3070 Ti is around 20% faster than the B580. However the B580 offers 4GB more VRAM than the 3070 Ti. The B580 drivers have improved alot but still not NVIDIA level yet.  If you want maximum performance and compatibility I recommend you get the 3070 Ti. Just ask the seller to show the temps of it and its performance if it matches online.,buildapc,2025-07-09 09:34:12,2
Intel,n26iq9y,3070ti for sure.  Yeah the B580 has 4gb more Vram but it will still runing slower in game (and that's if the game is able to run properly),buildapc,2025-07-09 14:24:13,2
Intel,n25r78u,3070 ti hands down. It's just better.,buildapc,2025-07-09 11:54:27,2
Intel,n2598eb,"What's your CPU? B580 has performance issues with weaker CPUs.  Nevertheless, I'd go for the 3070 Ti anyway.",buildapc,2025-07-09 09:38:29,1
Intel,n259hwp,"CPU: AMD Ryzen 5 7600  Motherboard: MSI B650 Tomahawk WiFi with original packaging  PSU : EVGA SuperNOVA 750 GT, 80 Plus Gold 750W, Fully Modular  RAM: Corsair Vengeance DDR5 2x16GB 6000MT/s CL36 EXPO enabled  CPU Cooler : Arctic Liquid Freezer II 240 ARGB",buildapc,2025-07-09 09:40:52,2
Intel,n259yjs,"Yeah. I'd avoid the B580 then. Google for something like ""arc b580 overhead issue"".",buildapc,2025-07-09 09:45:02,-2
Intel,n25a9qv,Thanks brother. Do you have anything else in mind for that budget or should I choose the 3070ti?,buildapc,2025-07-09 09:47:47,2
Intel,n25m54d,"Nah, I think it's solid!",buildapc,2025-07-09 11:20:38,1
Intel,nbffxjy,you're probably CPU bottlenecked though the slow RAM isn't helping you at all either. Get a 2x16 3600 speed RAM kit and/or grab a 5000 series AMD CPU,pcmasterrace,2025-08-30 02:44:07,3
Intel,nbfgumc,"Yeah, you're probably being hit by the driver overhead issues. You want at least a 5000-series Ryzen to mitigate that, as the other commenter suggested.",pcmasterrace,2025-08-30 02:50:23,1
Intel,nbfpfgt,Intel cards have a terrible CPU overhead issue with lower end processors.,pcmasterrace,2025-08-30 03:51:44,1
Intel,nau5nn4,My buddy has one and loves it,pcmasterrace,2025-08-26 21:21:38,2
Intel,naubvf8,I hope you enjoy the new card. Are you gaming at 1080 or 1440?  I would highly suggest you update your [BIOS](https://www.gigabyte.com/Motherboard/H610M-H-DDR4-rev-10/support#dl) says version F4 but the release date has the date of F29 current version is F31 from June?,pcmasterrace,2025-08-26 21:52:53,1
Intel,navbitn,B580 is only a risk if your CPU is relatively old. Anything LGA 1700 or AM5 will be fine.,pcmasterrace,2025-08-27 01:13:38,1
Intel,nauog8r,1080p @ 100 hz,pcmasterrace,2025-08-26 23:01:46,1
Intel,naw7wtt,I have an i5-12400. I seen someone say you need Intel 10th Gen on up.,pcmasterrace,2025-08-27 04:46:30,1
Intel,naux6yk,awesome it will be completely capable of handing that and more so you can upgrade your monitor at a later date.,pcmasterrace,2025-08-26 23:50:25,2
Intel,naw7zst,"Or just keep the same monitor. I had 1440p @ 180 hz at some point, but I'm quite happy with 1080p now because 1080p will last a lot longer.",pcmasterrace,2025-08-27 04:47:09,1
Intel,n9ijfu3,just get the cheapest,pcmasterrace,2025-08-19 12:02:31,2
Intel,n9ik20p,"It just depends on the game tbh. Its a good GPU, alot of improvements over the last generation. A gamble, but when it works it works well.",pcmasterrace,2025-08-19 12:06:31,1
Intel,n9iv19g,"If you have a budget for a computer on AM5, you might consider building a specification with a better graphics card on AM4.",pcmasterrace,2025-08-19 13:11:55,1
Intel,n9imy90,"i'm mostly going to play the following: warthunder, teardown, people playground (not that gpu dependent but still) cs2, and roblox, but i will also probably buy new games such as ready or not, and some offroad explorer game i saw my friends play but i have no idea what its called (i think its snowrunner? or over the hill, not sure). would it run most of these games fine?",pcmasterrace,2025-08-19 12:24:45,1
Intel,nb7iaw7,You don't really need to apologise if you pick Intel.,pcmasterrace,2025-08-28 21:37:27,6
Intel,nb7boad,Reason for intel cpu: I know I picked Intel. But this is because my sister doesn't really play demanding games and she is a student who needs to edit videos for school and more. Any future builds i will use an AMD cpu obv. But this build will get her things done. The total price is about Â£550 ($743 for Americans).,pcmasterrace,2025-08-28 21:04:24,2
Intel,nb7cg76,Probably makes it more simple too for her cpu and gpu to work together in a menu for overlocking or something. Idk if they have anything like amd with their cpu and gpu.,pcmasterrace,2025-08-28 21:08:02,2
Intel,nb7cpks,yeah she isnt really good at pc's so thats why I chose this combo,pcmasterrace,2025-08-28 21:09:15,1
Intel,nbdcuc1,"if you don't have a 3rd gen ryzen or 10gen intel or newer CPU you're kind of SOL on intel GPUs, they need reBAR pretty much to perfrom well",pcmasterrace,2025-08-29 19:30:36,7
Intel,nbddvto,If you already have Rebar on your system.  Then BUY the B580! If I remember the B580 is around 36K! Potaka IT & SkylandBD got the card within the price.,pcmasterrace,2025-08-29 19:35:57,2
Intel,nbdefn1,"Not much reason to go for the a770 when the b580 improves performance by a decent margin and consumes less power. Yeah, less vram, but I doubt youâ€™ll be running out of vram before youâ€™ll be running into a gpu performance wall.  Also, as the other commenter said, make sure you have a relatively newer cpu, not just because it requires rebar but because the drivers can have an impact on cpu performance.",pcmasterrace,2025-08-29 19:38:45,1
Intel,nbdxkqu,"check for any of the rtx 3060 / 4060 / 5060   i can see some in similar price, you will save yourself a lot of trouble",pcmasterrace,2025-08-29 21:15:42,1
Intel,nbee6ah,Some 9th-gen Intel chipsets got the ability to use reBAR in later BIOS updates. Iâ€™m building a 9600kf media PC with it enabled right now. Z390 chipset.,pcmasterrace,2025-08-29 22:48:42,1
Intel,nbgwj59,"Eh, B580 was a real improvement in terms of software and hardware so I'd say it's ok  But then again, seeing Intel currently bleeding money left and right I wouldn't be surprised if they were forced to drop the GPU market again which would be a real shame but that's what happens when shareholders run a tech company into the ground by not pushing innovation but profit above everything else I guess",pcmasterrace,2025-08-30 10:26:22,1
Intel,nbqr8j2,"Radeon RX 9060 XT 16GB  Yes, buying new is expensive, but at least you get warranty.",pcmasterrace,2025-08-31 22:47:54,2
Intel,nbqu3kz,"3070 TI. That is, if the 9060 XT 16gb is out of budget. It's better than the 5060 and on par or better than the 9060 XT 8gb depending on if you're running PCIE 5 or not.",pcmasterrace,2025-08-31 23:05:54,2
Intel,nbuf9db,Have you considered making it without the clear plastic so it restricts the airflow less?,pcmasterrace,2025-09-01 14:41:47,3
Intel,nblsiva,Yup. Should be good,pcmasterrace,2025-08-31 03:33:47,1
Intel,nbhd1so,"Swapping RAM is a bad idea. DDR4 is overpriced since we aren't getting any more stock. Also, you can probably just clock up your RAM manually with some trial and error (e.g. I got a 3000/15 kit and clocked it to 3600/16).  As for the GPU: An RX 9070 should be around 30% faster than a 3080 on average. For Cyberpunk specifically, the RX 9070 should even be close to 50% faster. And you'd save some money due to lower consumption on top of that (usually \~2-3ct/h gaming, which won't make a big difference at that cost, but it is something to keep in mind).",pcmasterrace,2025-08-30 12:36:16,3
Intel,nbkicu3,">My idea is to upgrade the GPU to a RX 9070XT (most important factor being 16Gb VRAM)  a RX 9070XT would be about a 40% improvement in 1440p it's in the range where you really fell the boost.  is it worth is another question, up to you to know if 650-700â‚¬ for a 40% lift is enough.  in your own exemple of recommended spec you can play at 1440p High with a 3060 Ti and DLSS balance so the 3080 a still lots of life for that resolution and I would keep it. The only reason I changed mine is because I brought a 4K screen in the mean time.  >and to swap my RAM for a DDR4-3600 CL18 (32Gb) kit.  I would not invest in DDR4 ram at that point, better to keep the money aside for the next CPU/RAM/MB update.",pcmasterrace,2025-08-30 22:40:47,2
Intel,nbn5us9,I wouldn't. Save more money for a completely new platform (AM5) and new generation of GPUs. This PC is good enough for at least 2 years still. Games you listed run well enough on your PC.,pcmasterrace,2025-08-31 11:00:53,2
Intel,nbgothd,Extra info: this would cost me â‚¬900 at todays prices,pcmasterrace,2025-08-30 09:10:30,1
Intel,nbosxmn,"Personally I probably wouldn't move from 3080 to 9070XT - while it is an upgrade, I don't think it's good enough upgrade to justify current pricing.  I agree what someone else here said, that saving up for a new platform (AM5 - to also upgrade RAM from 16 to at least 32GB) would be wiser for now as I have RTX 3080 12GB and it works fairly well for 4K (obviously not ultra on everything, but generally even medium or high looks great these days + you can always use DLSS) and then maybe wait for Nvidia Super cards or one more generation if the GPU is working for you. (it's not like when I had to move from GTX1070 because new games were dropping support for it completely and drivers as well later this year)",pcmasterrace,2025-08-31 16:43:24,1
Intel,nbhlfy5,"Thank you for the response.  It's about â‚¬140 for the RAM kit (Kingston Fury). Wouldn't it be a substantial upgrade going from 16Gb to 32Gb? I see a lot of games switching to 16Gb minimum and 32Gb recommended.  30% to 50% increase in some titles seems like a decent improvement, better power efficiency is a very nice bonus! Despite still having good framerate on my 3080, I think the 10Gb is not gonna be enough sooner than later at 1440p.  The 5800X3D is a beast and doesn't fall far behind the newer flagship AMD CPU's, so I'm very comfortable staying on AM4.",pcmasterrace,2025-08-30 13:28:36,1
Intel,nbn4ho9,">Â overpriced since we aren't getting any more stock.  Not true, samsung restarted production as the demand is still high.",pcmasterrace,2025-08-31 10:48:51,1
Intel,nbmqg4e,"Thanks for the response!  It's nice to have an idea what the idea % wise would be. From what I can find in benchmarks and what people are saying it falls somewhere between 30-50% depending on the game.  I'm aware that the 3080 still is a good card nowadays, the only 'annoyance' is the 10Gb of Vram (Nvidia kinda fucked up there and made the 3080 12Gb later on).  I'm playing the new update in PoE2 and put my metrics on so I can see the amount of load and Vram usage. If I run into no issues and have some headroom I will keep it. Will do the same thing once BF6 launches or when playing other demanding games.  Cheers",pcmasterrace,2025-08-31 08:31:39,1
Intel,nbn9xhr,Yes they do run well even on high/ultra still. Maybe I'll even make it to AM6 and UDNA could be amazing if AMD starts competing again in the high end GPU segment. I'm keeping an eye on my Vram usage.,pcmasterrace,2025-08-31 11:34:44,1
Intel,nbp1hxc,"Aye that's what I figured by now. So far highest Vram usage I've seen in PoE2 act 1 is 5,9Gb so all good still.  Might even last me even longer past AM5. We have a very similar setup, only difference is the 2Gb of Vram.",pcmasterrace,2025-08-31 17:24:50,1
Intel,nbhovzd,">It's about â‚¬140 for the RAM kit (Kingston Fury). Wouldn't it be a substantial upgrade going from 16Gb to 32Gb? I see a lot of games switching to 16Gb minimum and 32Gb recommended.  You can always free up your Windows install and remove bloat. Minimum for Win 11 is like 2-4 GB base RAM usage, so you should have enough left for gaming. Really only becomes an issue when you do stuff like open Chrome with dozens of active tabs and have like 12GB base RAM usage before even opening the game.  140â‚¬ is just a horrible price for outdated RAM. I got my 32GB kit (which is running at 3600/16 right now) for \~110â‚¬ 6 years ago.  >The 5800X3D is a beast and doesn't fall far behind the newer flagship AMD CPU's, so I'm very comfortable staying on AM4.  Well, 5800X3D is around 7600X performance in most games. Maybe slightly better whenever the 3D-cache matters. Usually, you get \~10% per gen, but just the one gen step to the 7800X3D is already 20% faster.  There certainly is no big reason to upgrade right now, but it also makes no sense to get new DDR4 RAM. Realistically, you could just sell RAM+MoBo+CPU and get like 300-400â‚¬ for it. Which you can then use to get an AM5 system with a 9600X or so. You might even be able to get a 7800X3D system if we include those 140â‚¬ that you would be willing to spend on new RAM. So imo IF you want an upgrade for the RAM, just upgrade the whole thing.",pcmasterrace,2025-08-30 13:48:54,1
Intel,nbiunzi,The X3D chips don't really care about memory speeds as much as other AMD/recent Intel chips.  You could just drop in another 16GB kit or go for a cheaper 2x16GB kit that's 3200MHz.  Another Redditor in r/AMD tested it and saw only a 1.8% increase in **synthetic** benchmarks with overclocked and tuned memory. I guarantee real world results will be even less.,pcmasterrace,2025-08-30 17:23:02,1
Intel,nbn7zqr,They just extended the deadline. Others producers like Micron have already stopped.,pcmasterrace,2025-08-31 11:18:57,1
Intel,nbmucpg,">I'm aware that the 3080 still is a good card nowadays, the only 'annoyance' is the 10Gb of Vram (Nvidia kinda fucked up there and made the 3080 12Gb later on).  people talk about that way more than games actually need it doubly so in 1440p not ultra.",pcmasterrace,2025-08-31 09:10:13,1
Intel,nbfjgdy,tiktok language god weâ€™re so screwed,pcmasterrace,2025-08-30 03:08:32,1
Intel,nb6wepb,If you hear that sound while playing something intense like cities skylines 2 I would suspect one of the fans on your video card has developed a wobble. If it uses these type of fans expect to hear more and it to get louder as the card ages. They even occasionally just fall out.  https://preview.redd.it/45nsw7mvetlf1.jpeg?width=1946&format=pjpg&auto=webp&s=ef1f1934fafa47900737a1ddbfed096196132a63  Iâ€™ve also had a machine make clicking type noises while under load and it was the PSU about to kick it.,pcmasterrace,2025-08-28 19:51:43,3
Intel,nb73uub,"GPU fans and noise do kick in up to 800-1100rpm during playback on editing software but not playing games, just ran a quick match of warzone with settings cranked up and everything seems smooth, checking hwinfo fans hover around 1100 and 1300 rpm which is normal during gameplay....but no noise at all",pcmasterrace,2025-08-28 20:27:35,1
Intel,nb77uui,Run cinebench r23 and see if the noise shows up when just the cpu is pinned at 100% if it does might be the PSU making the noise. Possibility a popped solder joint on a cap or something.,pcmasterrace,2025-08-28 20:46:28,2
Intel,nb7ewwx,"Cinebench running as I type this, no noise, only hear fans at 1100-1300 rpm, everything here is barely 8 months old , PSU is Asus Loki SFX - L 850W platinum",pcmasterrace,2025-08-28 21:19:54,1
Intel,nb7jyct,"I dunno, that was all I had for such a strange noise. :( Take the side of the case off and try to listen for where exactly its coming from.",pcmasterrace,2025-08-28 21:46:08,1
Intel,naoxzr3,congrats enjoy  bios update motherboard before swapping cpu,pcmasterrace,2025-08-26 02:03:50,2
Intel,nap283p,A anti sag bracket holy shit there so rare thank you for helping this fat gpu  https://preview.redd.it/wlbggs1ny9lf1.png?width=277&format=png&auto=webp&s=62eb3976b56ccbcd4a43220dfc8e76a13e1f119c     Your cpu power cables need to be loosen abit there very tight and if i was you i would run 2 pcie power cables to that gpu the whole one cable pgtail is so 8 year's ago these new cards like 2 cables separate for more power leg room.,pcmasterrace,2025-08-26 02:29:01,2
Intel,naudfl1,Happy birthday!,pcmasterrace,2025-08-26 22:01:01,2
Intel,naozsil,"I'm on a B550-A Pro and have kept my BIOS up-to-date, so there was no need to update the BIOS.",pcmasterrace,2025-08-26 02:14:22,1
Intel,nap608t,"I had pretty tightly cable-managed my cables, though I did try to loosen the GPU cable a bit, it's still pretty tight. I will consider using two separate power cables in the future too.  also not sure if I needed the anti-sag bracket (lego post!) but I'm keeping it there for assurance",pcmasterrace,2025-08-26 02:52:30,1
Intel,naphugz,You do those pcie connectors on the pcb are tiny and the new fat cards overtime sag/break those connectors its very common on the 40/50 series high end cards.      A anti sag bracket helps this alot my card i went vertical sense according to those that repair them that is the best position.,pcmasterrace,2025-08-26 04:13:45,1
Intel,n7ujlpv,recognise boat mountainous crowd unique reply familiar lunchroom consist hard-to-find   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,pcmasterrace,2025-08-09 22:30:20,2
Intel,n7umpuz,"i5-12400. MBD (Gigabyte H610M S2 V2) supports Resizeable Bar according to BIOS. And FWIW, RAM is 32GB DDR5 6000 but only runs at 4800 due to the CPU.",pcmasterrace,2025-08-09 22:49:26,1
Intel,n7uod8p,paltry hunt offbeat strong wide versed engine safe marble screw   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,pcmasterrace,2025-08-09 22:59:33,2
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,157
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,85
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,47
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,15
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,6
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,4
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,4
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,3
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,3
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,Youâ€™re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asfðŸ˜­,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,46
Intel,mz2vfon,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",AMD,2025-06-22 01:09:53,19
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,15
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,16
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,4
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here:Â https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,13
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D:Â https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md ðŸ––,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (Iâ€™m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,7
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,5
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,4
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,4
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,8
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,18
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,8
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,4
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,4
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,4
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  â€‹â€‹â€‹  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"ðŸ‘   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,20
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,15
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,6
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,36
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-6
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,7
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,6
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didnâ€™t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,77
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,11
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,21
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,16
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-16
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-15
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,6
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,4
Intel,m84dadg,"Iâ€™m pretty sure HUB doesnâ€™t like Nvidia *or* AMD. Theyâ€™re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,9
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,4
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,58
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,21
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-15
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,46
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-2
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,23
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,1
Intel,lgze3vw,"It depends on what your goals are for a laptop.Â  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!Â Â  I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.Â  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-7
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-10
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,12
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,18
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,6
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,4
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,4
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-3
Intel,lfkw8g2,throw it in the next steamdeck and Iâ€™ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,4
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if youâ€™re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,11
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,11
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, itâ€™s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,9
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,4
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,9
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.Â  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.Â  The system should have an actual TRUE quadcore memory setup.Â  Many of these systems have currently (and will absolutely continue to have)Â dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.Â  This matches my system that runs a 780m with 7500mhz lpddr5x.Â  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.Â  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"â€œAbsolute monsterâ€? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.Â   Â AMD has a long way to go before claiming â€œMonsterâ€ class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet arenâ€™t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Maxâ€¦ let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. Thatâ€™s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,3
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming â€œMonsterâ€ class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMDâ€™s biggest markets. You are kidding if you think they donâ€™t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they donâ€™t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Appleâ€™s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games arenâ€™t the only thing APUs are used for so PS5 isnâ€™t wholey in the conversation. PS5 also costs monthly to play online and their games arenâ€™t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Appleâ€™s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP â€” PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,32
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,20
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,7
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,4
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,8
Intel,lf385p0,"This was a graphics card, not a â€˜GPUâ€™ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,5
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-2
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,9
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,3
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, thereâ€™s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,21
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,13
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,4
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,10
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,1
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,2
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I donâ€™t mind free markets, Iâ€™m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,10
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,222
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,24
Intel,kxiush3,"""The ability to â€œturn it off and on againâ€ should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,111
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,16
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,30
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,120
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,65
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,7
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,38
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,33
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,59
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,13
Intel,kxiah6c,"Iâ€™ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, itâ€™s usually a day late and a dollar short. Blender on Linux still canâ€™t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that thereâ€™s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but weâ€™ll have to wait and see.",AMD,2024-04-01 05:59:50,26
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*â„¢ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,22
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,5
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,4
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,25
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,5
Intel,kxp7mvs,Iâ€™ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,4
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,5
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,6
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,12
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100Âºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,15
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still canâ€™t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,4
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,7
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,14
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,4
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,4
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,3
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,4
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,4
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,1
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-8
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-23
Intel,kxk9iir,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-4
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-3
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-6
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,46
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  ðŸ˜¤ðŸ˜­,AMD,2024-04-04 17:12:00,3
Intel,kxjwsde,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",AMD,2024-04-01 15:08:22,29
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,17
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,5
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,34
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,13
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,13
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-3
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,19
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,8
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,4
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,8
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,5
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:Â Â  Â    ""Speaking of VRAM, The drivers use VRAM less efficiently. Look atÂ any side-by-side comparison between games on YouTubeÂ between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.Â    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?Â    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.Â    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?Â    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,6
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,27
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,7
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,8
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-4
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,12
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,3
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-5
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,25
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocksÂ EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,8
Intel,kxjbu8k,"I dunno man. Iâ€™ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I havenâ€™t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,4
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems ðŸ˜‚ but I guess some do idk ðŸ¤·. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,3
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me Â£540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent Â£200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,5
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,13
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,6
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,5
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,4
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-15
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,9
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,14
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,1
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-1
Intel,kyy38w2,"Hey OP â€” Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,30
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,3
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,6
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,8
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,45
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,25
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,4
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,13
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,4
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,9
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,10
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,6
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,11
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,2
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,0
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,1
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,13
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,3
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,10
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,21
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,21
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,4
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,6
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,6
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,6
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-14
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,0
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,32
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,32
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,-1
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,5
Intel,kyhsjnw,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-3
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,9
Intel,kxj49ms,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,5
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,6
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,15
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,12
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,-2
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,14
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,12
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,4
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting ðŸ˜‚ðŸ¤£,AMD,2024-04-01 14:23:58,2
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,9
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,8
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,5
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,8
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-2
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,3
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,7
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,-2
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,2
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,2
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,1
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-7
Intel,kxih401,Oh then just ignore my comment ðŸ˜…,AMD,2024-04-01 07:20:10,-1
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,9
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-13
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-13
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,4
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,2
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,7
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,13
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,4
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,27
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pcâ€™s are better and donâ€™t really complain about upscaling and 30fps.. youâ€™re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldnâ€™t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,6
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned offÂ   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,5
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,36
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,13
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,20
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,17
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,16
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,6
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,14
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,5
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,16
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete.Â And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,6
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers werenâ€™t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,3
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,6
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,-2
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,16
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?Â    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,3
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,6
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-7
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-6
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-2
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-2
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-5
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,6
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,8
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,20
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,7
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,7
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,8
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,8
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,0
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,8
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,3
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-1
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,-1
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,5
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,6
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,kpp4kwl,Really love how the 6000 series radeons look.,AMD,2024-02-09 21:57:31,12
Intel,kpqv9od,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",AMD,2024-02-10 05:25:10,5
Intel,kpougfk,That's a good looking line up,AMD,2024-02-09 20:58:04,2
Intel,kps7pkq,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",AMD,2024-02-10 14:18:43,2
Intel,kpr86tx,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",AMD,2024-02-10 07:45:28,5
Intel,kpq3r57,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",AMD,2024-02-10 01:49:13,3
Intel,kptibdx,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,AMD,2024-02-10 19:15:04,-1
Intel,kptwmeu,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",AMD,2024-02-10 20:44:28,3
Intel,kpv2g8f,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,AMD,2024-02-11 01:23:45,1
Intel,kpv5euk,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",AMD,2024-02-11 01:44:32,3
Intel,kpvwyyr,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",AMD,2024-02-11 05:16:13,2
Intel,kcvx2pq,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,AMD,2023-12-11 10:20:41,10
Intel,kcvsq1w,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",AMD,2023-12-11 09:20:24,15
Intel,kcvzwca,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,AMD,2023-12-11 10:58:26,2
Intel,kcyc7u2,That 7900xtx sale number is insane,AMD,2023-12-11 21:59:22,2
Intel,kcytq9l,That just shows that most people that buy GPU's don't know a thing about them.,AMD,2023-12-11 23:54:41,1
Intel,kcwedyi,"4060 Ti is not discounted well at all here.   390â‚¬ and 460â‚¬ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",AMD,2023-12-11 13:30:14,16
Intel,kcvzjgq,best discounts were 6750xt 6800 and 7800xt,AMD,2023-12-11 10:53:41,1
Intel,kdazjvv,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,AMD,2023-12-14 10:36:15,1
Intel,kcvv71l,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",AMD,2023-12-11 09:54:52,6
Intel,kcwe3k6,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",AMD,2023-12-11 13:27:46,5
Intel,kcw3vwl,"They're not out of stock there, duh",AMD,2023-12-11 11:47:20,7
Intel,kcyhmsr,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,AMD,2023-12-11 22:33:46,1
Intel,kd0h0lm,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-12-12 07:54:01,1
Intel,kcxlwiu,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and donâ€™t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didnâ€™t trust my word or AMD  Itâ€™s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",AMD,2023-12-11 19:18:18,4
Intel,kcxu0yw,">390â‚¬ and 460â‚¬ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",AMD,2023-12-11 20:09:16,1
Intel,kcx65jb,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",AMD,2023-12-11 16:46:01,2
Intel,kcw55l4,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",AMD,2023-12-11 12:01:31,1
Intel,kcy5bwz,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299â‚¬ for a while, think they are back at 330â‚¬ tho)",AMD,2023-12-11 21:17:17,1
Intel,keln136,the card is pretty bad if you missed that somehow,AMD,2023-12-23 12:50:02,1
Intel,kcw5qf2,AMD probably ships leftover to countries in which they know it will sell,AMD,2023-12-11 12:07:48,3
Intel,kdhrs0l,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",AMD,2023-12-15 17:44:32,1
Intel,kemomla,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",AMD,2023-12-23 17:01:05,1
Intel,kb07k5w,"As you notice the photoshop version differs, so you can't compare them really",AMD,2023-11-27 18:28:41,4
Intel,kao517l,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),AMD,2023-11-25 07:14:52,2
Intel,k74d0ev,So basically PC games are never going to tell us what the specs are to run the game native ever again.,AMD,2023-10-30 18:21:26,539
Intel,k751wfu,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",AMD,2023-10-30 20:52:50,49
Intel,k748hf1,The true crime here is needing FSR to reach these requirements.,AMD,2023-10-30 17:53:41,193
Intel,k74fig7,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",AMD,2023-10-30 18:36:48,36
Intel,k74fyp7,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),AMD,2023-10-30 18:39:33,17
Intel,k74c3qr,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,AMD,2023-10-30 18:15:55,66
Intel,k7407mp,"well, at least the chart is easy to read, not a complete mess",AMD,2023-10-30 17:03:14,16
Intel,k7466bs,This gives me hope that Star Wars outlaws will also have FSR 3. ðŸ¤Œ,AMD,2023-10-30 17:39:45,11
Intel,k741zrd,60fps with balanced upscaling on a 4080/7900xtx â€¦ Thatâ€™s not a good sign,AMD,2023-10-30 17:14:11,56
Intel,k748t1r,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,AMD,2023-10-30 17:55:39,52
Intel,k76jevo,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",AMD,2023-10-31 02:51:26,5
Intel,k76we4z,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",AMD,2023-10-31 04:46:34,8
Intel,k75qe4v,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,AMD,2023-10-30 23:33:24,3
Intel,k74d5ad,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",AMD,2023-10-30 18:22:16,25
Intel,k748nsg,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",AMD,2023-10-30 17:54:45,12
Intel,k74kjmk,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",AMD,2023-10-30 19:07:25,18
Intel,k7533wp,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",AMD,2023-10-30 21:00:16,3
Intel,k77mpgx,So i bought an 1200â‚¬ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,AMD,2023-10-31 10:35:56,3
Intel,k74hphk,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,AMD,2023-10-30 18:50:08,12
Intel,k773lbl,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",AMD,2023-10-31 06:11:46,6
Intel,k74eavi,I hope they will bundle this game with CPUs/GPUs,AMD,2023-10-30 18:29:20,2
Intel,k74fxtg,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,AMD,2023-10-30 18:39:23,2
Intel,k77022h,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,AMD,2023-10-31 05:27:42,2
Intel,k78gl5o,This game better look like real life with those specs. I does looks beautiful!,AMD,2023-10-31 14:43:10,2
Intel,k75as90,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",AMD,2023-10-30 21:48:53,4
Intel,k748ctr,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,AMD,2023-10-30 17:52:56,3
Intel,k74mjvt,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,AMD,2023-10-30 19:19:43,2
Intel,k74oqng,I love it how absurd these things are these days.,AMD,2023-10-30 19:33:13,2
Intel,k74awed,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,AMD,2023-10-30 18:08:29,2
Intel,l06s6dc,Does anyone know if it supports SLI or crossfire?,AMD,2024-04-18 19:16:23,1
Intel,k73yt0i,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,AMD,2023-10-30 16:54:36,-1
Intel,k74kcre,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I donâ€™t have super high hopes that thatâ€™ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying youâ€™re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",AMD,2023-10-30 19:06:17,1
Intel,k74mb9y,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,AMD,2023-10-30 19:18:13,1
Intel,k74oabx,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,AMD,2023-10-30 19:30:24,1
Intel,k74sd5w,Why in the f*ck is upscaling included on a specs page?,AMD,2023-10-30 19:55:26,1
Intel,k77tobm,no more software optimization and full upscaling  bleah,AMD,2023-10-31 11:50:04,1
Intel,k749m34,Now THIS is what I call a reasonable spec sheet for a next-gen game ðŸ‘ðŸ‘ðŸ‘,AMD,2023-10-30 18:00:34,-2
Intel,k74d7ll,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",AMD,2023-10-30 18:22:41,0
Intel,k74phj0,"I've never even heard of this game, nor care about it, but these system requirements offend me.",AMD,2023-10-30 19:37:49,1
Intel,k79vqgg,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,AMD,2023-10-31 20:00:03,0
Intel,k744khv,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",AMD,2023-10-30 17:29:53,-1
Intel,k79zrsa,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",AMD,2023-10-31 20:25:07,0
Intel,k7c9frd,How about support them with games that people actually PLAYS ? Like Alan wake 2 ðŸ¤£,AMD,2023-11-01 08:20:56,0
Intel,k75roib,Fuck FSR and DLSS. This isnâ€™t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,AMD,2023-10-30 23:42:07,-1
Intel,k741y61,Nice,AMD,2023-10-30 17:13:54,-1
Intel,k765iq2,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",AMD,2023-10-31 01:14:49,0
Intel,k77fyxm,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",AMD,2023-10-31 09:08:01,0
Intel,k77zvvn,Native gaming died or what ? Wtf they turning pc gaming into console gaming,AMD,2023-10-31 12:44:58,0
Intel,k76r1ve,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",AMD,2023-10-31 03:53:47,-1
Intel,k74pme9,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,AMD,2023-10-30 19:38:39,1
Intel,k74qrim,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,AMD,2023-10-30 19:45:38,1
Intel,k74t7ty,"Omg, it would be a graphic master piece or  bad optimized thing.",AMD,2023-10-30 20:00:39,1
Intel,k74ybxr,First time I see matches recommendations for nv and amd GPUs...,AMD,2023-10-30 20:31:22,1
Intel,k756ha4,Rip laptop rtx 3060 6gb,AMD,2023-10-30 21:21:24,1
Intel,k759rkx,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",AMD,2023-10-30 21:42:14,1
Intel,k75e3m3,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,AMD,2023-10-30 22:10:21,1
Intel,k75ej2l,*NATIVE* resolution gang ftw!,AMD,2023-10-30 22:13:11,1
Intel,k75k1fl,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ainâ€™t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen canâ€™t push enough pixels for these games.",AMD,2023-10-30 22:50:05,1
Intel,k75n4rl,Looks capped at 60fps?,AMD,2023-10-30 23:11:06,1
Intel,k75ops0,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",AMD,2023-10-30 23:21:57,1
Intel,k75qeie,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,AMD,2023-10-30 23:33:28,1
Intel,k760or5,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",AMD,2023-10-31 00:42:35,1
Intel,k77465m,Is it using UE5?,AMD,2023-10-31 06:19:31,1
Intel,k77bz45,"Ubisoft, rip on launch.",AMD,2023-10-31 08:09:13,1
Intel,k77htgp,guessing no DLSS3 then?,AMD,2023-10-31 09:33:36,1
Intel,k77l01d,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,AMD,2023-10-31 10:15:17,1
Intel,k77twmu,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",AMD,2023-10-31 11:52:12,1
Intel,k77x5ay,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",AMD,2023-10-31 12:21:45,1
Intel,k77z2e6,Farewell 1660tiâ€¦ looks like itâ€™s time for an upgrade,AMD,2023-10-31 12:38:12,1
Intel,k78p32u,well my 3300x is now obsolete for these new AAA games...,AMD,2023-10-31 15:37:57,1
Intel,k798boq,4k ultra right up my alley ðŸ˜,AMD,2023-10-31 17:36:35,1
Intel,k799x2x,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,AMD,2023-10-31 17:46:17,1
Intel,k7ec5fz,7900xtx will do 4k 120fps with FSR 3 then I guess?,AMD,2023-11-01 18:22:39,1
Intel,k7fkb1o,What must one do to achieve a higher rank than an enthusiast? A demi-god?,AMD,2023-11-01 22:56:10,1
Intel,k7gz4pf,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",AMD,2023-11-02 05:23:56,1
Intel,k7k2rqe,Is vrr fixed with frame gen then?,AMD,2023-11-02 20:32:54,1
Intel,k859q58,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",AMD,2023-11-07 00:16:09,1
Intel,k8kv1d8,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",AMD,2023-11-10 00:27:13,1
Intel,kaeixym,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,AMD,2023-11-23 05:18:15,1
Intel,k74h55l,They recommend upscaling even at 1080p.. disgusting ew,AMD,2023-10-30 18:46:41,223
Intel,k75hnu4,pretty much this we all knew they would start using upscaling as a crutch.,AMD,2023-10-30 22:34:04,14
Intel,k76ze7a,Welcome to modern times and how you canâ€™t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,AMD,2023-10-31 05:19:49,5
Intel,k74f7tr,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",AMD,2023-10-30 18:35:00,26
Intel,k74e6u4,My thoughts tooâ€¦,AMD,2023-10-30 18:28:39,6
Intel,k778snb,Thanks to all of you that were screaming dlss looks better than native lmao.,AMD,2023-10-31 07:23:19,6
Intel,k79niu3,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",AMD,2023-10-31 19:09:38,1
Intel,k77adqn,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,AMD,2023-10-31 07:46:18,1
Intel,k74x8ab,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",AMD,2023-10-30 20:24:48,-1
Intel,k74zbiw,Nope we as a community abused a nice thing,AMD,2023-10-30 20:37:19,1
Intel,k755j7t,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",AMD,2023-10-30 21:15:25,-7
Intel,k75o59y,Didn't take them long to make upscaling worthless.,AMD,2023-10-30 23:18:03,-1
Intel,k78qsdu,i smell a burgeoning cottage industry of game spec reviewers!,AMD,2023-10-31 15:48:28,0
Intel,k7hhgqi,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",AMD,2023-11-02 09:36:54,0
Intel,k7k4fzn,"Itâ€™s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. Theyâ€™ve given you all the pieces you need to calculate it out yourself, but thatâ€™s not how the game is supposed to be run so they arenâ€™t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",AMD,2023-11-02 20:43:01,0
Intel,k74js59,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",AMD,2023-10-30 19:02:44,-3
Intel,k78m5tc,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,AMD,2023-10-31 15:19:10,-1
Intel,k74pa1g,yeah this is the new standard,AMD,2023-10-30 19:36:32,1
Intel,k77dlbv,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",AMD,2023-10-31 08:33:14,24
Intel,k78n190,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,AMD,2023-10-31 15:24:48,1
Intel,k77nqtn,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",AMD,2023-10-31 10:47:59,-7
Intel,k74z5qq,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",AMD,2023-10-30 20:36:21,14
Intel,k76jxv9,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",AMD,2023-10-31 02:55:18,2
Intel,k76rqab,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,AMD,2023-10-31 04:00:06,2
Intel,k75qnpb,The actual true crime here is even having fsr to begin with. It should just have dlss,AMD,2023-10-30 23:35:12,-5
Intel,k76ju0a,Seems like they tried to cover every basis with these.,AMD,2023-10-31 02:54:33,7
Intel,k75ao0c,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",AMD,2023-10-30 21:48:08,9
Intel,k74htwr,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",AMD,2023-10-30 18:50:52,15
Intel,k74jsww,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,AMD,2023-10-30 19:02:51,3
Intel,k760d0a,"GCN support is over, RDNA1 is the lowest currently supported arch.",AMD,2023-10-31 00:40:29,6
Intel,k74k4hi,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",AMD,2023-10-30 19:04:51,2
Intel,k77v39i,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",AMD,2023-10-31 12:03:13,0
Intel,k74efc2,What do you mean forced raytracing?,AMD,2023-10-30 18:30:05,11
Intel,k77dids,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,AMD,2023-10-31 08:31:59,6
Intel,k74fapr,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",AMD,2023-10-30 18:35:29,15
Intel,k7gd959,It's actually 960p :(,AMD,2023-11-02 02:12:51,2
Intel,k78nhhp,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",AMD,2023-10-31 15:27:45,0
Intel,k74go33,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,AMD,2023-10-30 18:43:49,6
Intel,k78s16k,We'll see in a year.,AMD,2023-10-31 15:56:02,-1
Intel,k744q6u,AFAIK  &#x200B;  It is with RT,AMD,2023-10-30 17:30:51,21
Intel,k745n5v,Seems pretty good to me given it is at 4K with RT,AMD,2023-10-30 17:36:31,19
Intel,k743fzm,"Likely includes the RT features , its also 4k",AMD,2023-10-30 17:23:02,7
Intel,k743sfv,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",AMD,2023-10-30 17:25:06,-14
Intel,k74aacw,"I wouldnâ€™t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",AMD,2023-10-30 18:04:43,35
Intel,k74c9q7,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",AMD,2023-10-30 18:16:57,25
Intel,k78pyma,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,AMD,2023-10-31 15:43:22,1
Intel,k771jw5,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",AMD,2023-10-31 05:46:01,1
Intel,k7aj8ac,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",AMD,2023-10-31 22:34:23,2
Intel,k77bs4b,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,AMD,2023-10-31 08:06:28,1
Intel,k75qm0a,Exactly! It even got xess so Intel users also can use xess,AMD,2023-10-30 23:34:53,2
Intel,k75k4vk,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",AMD,2023-10-30 22:50:44,3
Intel,k74wzhj,"I agree with this, thatâ€™s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",AMD,2023-10-30 20:23:22,5
Intel,k7708p1,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,AMD,2023-10-31 05:29:56,4
Intel,k77n6u7,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,AMD,2023-10-31 10:41:33,3
Intel,k75ya7o,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",AMD,2023-10-31 00:26:50,3
Intel,k7a5u7g,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",AMD,2023-10-31 21:03:27,0
Intel,k77o6pg,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",AMD,2023-10-31 10:53:04,1
Intel,k755rf9,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",AMD,2023-10-30 21:16:52,4
Intel,k75jput,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",AMD,2023-10-30 22:47:54,3
Intel,k761p0l,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",AMD,2023-10-31 00:49:12,0
Intel,k75i4pq,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,AMD,2023-10-30 22:37:13,2
Intel,k77cvge,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",AMD,2023-10-31 08:22:22,-1
Intel,k755zaf,"Mirage is PS4 game, Avatar is PS5",AMD,2023-10-30 21:18:15,7
Intel,k74a84y,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine ðŸ˜³  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that ðŸ˜‰",AMD,2023-10-30 18:04:20,4
Intel,k75qyxk,Timed epic exclusivity? Aww man.,AMD,2023-10-30 23:37:17,3
Intel,k74vid8,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,AMD,2023-10-30 20:14:32,0
Intel,k74dkaf,You... are.. joking... right..?,AMD,2023-10-30 18:24:49,2
Intel,k770p8h,Because itâ€™s not coming to steam. Itâ€™ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,AMD,2023-10-31 05:35:28,1
Intel,k7ftdl4,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,AMD,2023-11-01 23:57:41,3
Intel,k74azia,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,AMD,2023-10-30 18:09:00,0
Intel,k749l36,Try ubisoft achievements :),AMD,2023-10-30 18:00:24,-1
Intel,k7a8c78,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",AMD,2023-10-31 21:19:36,1
Intel,k767klo,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",AMD,2023-10-31 01:28:47,1
Intel,k783twl,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,AMD,2023-10-31 13:15:34,0
Intel,k76sj05,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),AMD,2023-10-31 04:07:38,3
Intel,k76mhsf,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",AMD,2023-10-31 03:14:45,3
Intel,k747o6w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2023-10-30 17:48:49,1
Intel,k74w3s3,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",AMD,2023-10-30 20:18:07,0
Intel,k75if4b,yup that is why I will play 1440 UW native with a 7900XTX.,AMD,2023-10-30 22:39:08,1
Intel,k74vwla,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,AMD,2023-10-30 20:16:56,1
Intel,k75qeu1,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",AMD,2023-10-30 23:33:32,2
Intel,k75p425,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",AMD,2023-10-30 23:24:38,1
Intel,k75qigc,Very similar performance I guess,AMD,2023-10-30 23:34:13,1
Intel,k77bjls,Ye,AMD,2023-10-31 08:03:05,0
Intel,k7813wv,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,AMD,2023-10-31 12:54:43,0
Intel,k77obao,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",AMD,2023-10-31 10:54:29,1
Intel,k77o9a4,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",AMD,2023-10-31 10:53:51,1
Intel,k77udwp,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2023-10-31 11:56:37,1
Intel,k78piiq,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,AMD,2023-10-31 15:40:37,1
Intel,k7e7bz2,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,AMD,2023-11-01 17:53:25,1
Intel,k7eem4z,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,AMD,2023-11-01 18:37:35,0
Intel,k7fs4ss,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",AMD,2023-11-01 23:49:18,0
Intel,k7k3mjx,"with AFMF it works , didnt test with FSR3 now.",AMD,2023-11-02 20:38:03,1
Intel,k8l2h6h,Reading before raging :),AMD,2023-11-10 01:18:48,1
Intel,k74j51i,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",AMD,2023-10-30 18:58:47,67
Intel,k774q01,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,AMD,2023-10-31 06:26:48,3
Intel,k74rg7l,For 30fps even lol,AMD,2023-10-30 19:49:50,2
Intel,k74tk9h,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",AMD,2023-10-30 20:02:46,1
Intel,k779kpw,Or be happy your shitty video card is still supported.,AMD,2023-10-31 07:34:39,0
Intel,k78mpu0,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,AMD,2023-10-31 15:22:45,0
Intel,k77deta,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,AMD,2023-10-31 08:30:28,6
Intel,k77kqzi,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,AMD,2023-10-31 10:12:09,1
Intel,k74w3jk,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,AMD,2023-10-30 20:18:05,13
Intel,k74frkt,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,AMD,2023-10-30 18:38:21,9
Intel,k74k70d,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",AMD,2023-10-30 19:05:17,0
Intel,k7a00mh,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",AMD,2023-10-31 20:26:38,1
Intel,k74w35a,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPUâ€™s and thereâ€™s nothing AMD can do to win them, only join them.",AMD,2023-10-30 20:18:01,0
Intel,k77ddfz,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,AMD,2023-10-31 08:29:53,6
Intel,k79hiyj,But it does in many ways.,AMD,2023-10-31 18:33:00,6
Intel,k77ktee,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",AMD,2023-10-31 10:13:00,1
Intel,k78mx3z,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",AMD,2023-10-31 15:24:03,-2
Intel,k792qlt,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,AMD,2023-10-31 17:02:03,5
Intel,k77jb2i,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",AMD,2023-10-31 09:53:38,5
Intel,k776y63,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",AMD,2023-10-31 06:57:06,12
Intel,k75f8fi,speaking facts my guy,AMD,2023-10-30 22:17:50,1
Intel,k77ezjy,You shouldn't have downvote dood wtf redditors ?,AMD,2023-10-31 08:54:03,0
Intel,k79hyhx,People with AMD cards dislike upscaling more because FSR sucks ass lol.,AMD,2023-10-31 18:35:41,6
Intel,k77ebzi,"Uhm, me btw...",AMD,2023-10-31 08:44:25,5
Intel,k77kv35,"> Who actually plays games at native these days, if it has upscaling?  I do.",AMD,2023-10-31 10:13:35,3
Intel,k75rfzg,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",AMD,2023-10-30 23:40:30,1
Intel,k7kxjkt,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",AMD,2023-11-02 23:49:47,3
Intel,k7xpols,onto absolutely nothing.,AMD,2023-11-05 15:20:55,1
Intel,k785u5t,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",AMD,2023-10-31 13:30:13,11
Intel,k75w2l1,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,AMD,2023-10-31 00:12:04,5
Intel,k76yitl,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,AMD,2023-10-31 05:09:41,1
Intel,k77agw9,The game is raytracing only with a ridiculous ammount of foooliage.,AMD,2023-10-31 07:47:35,6
Intel,k77h1xr,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",AMD,2023-10-31 09:23:11,6
Intel,k78n7c0,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,AMD,2023-10-31 15:25:54,3
Intel,k74jbjo,Honestly my 1070 still runs great for what it is. Itâ€™s in my sons rig now because it couldnâ€™t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,AMD,2023-10-30 18:59:54,3
Intel,k77uxf6,False.  guy blocked me lmao,AMD,2023-10-31 12:01:42,1
Intel,k770n5h,"I thought that was on Linux, though I might be wrong",AMD,2023-10-31 05:34:47,0
Intel,k75wi30,1070 doesn't have hardware RT though.,AMD,2023-10-31 00:14:59,4
Intel,k74g227,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),AMD,2023-10-30 18:40:06,25
Intel,k78amg9,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,AMD,2023-10-31 14:03:39,0
Intel,k74qlaz,Completely agree.,AMD,2023-10-30 19:44:35,3
Intel,k7hr3n6,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,AMD,2023-11-02 11:28:02,1
Intel,k7476pu,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),AMD,2023-10-30 17:45:53,2
Intel,k75aybf,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",AMD,2023-10-30 21:50:00,9
Intel,k74gjdn,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",AMD,2023-10-30 18:43:00,12
Intel,k74gjiu,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,AMD,2023-10-30 18:43:01,1
Intel,k74d1es,"Derp, derp.",AMD,2023-10-30 18:21:37,0
Intel,k7gqdq3,Sounds like John on Direct Foundry Direct every week.,AMD,2023-11-02 03:55:28,1
Intel,k7721zf,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",AMD,2023-10-31 05:52:19,3
Intel,k77id8y,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",AMD,2023-10-31 09:41:01,5
Intel,k75quo4,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,AMD,2023-10-30 23:36:30,3
Intel,k75wl3m,Avatar is RT only. There is no non RT mode.,AMD,2023-10-31 00:15:31,9
Intel,k77ar4b,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,AMD,2023-10-31 07:51:38,5
Intel,k76ly3i,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",AMD,2023-10-31 03:10:28,2
Intel,k78qssi,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",AMD,2023-10-31 15:48:32,3
Intel,k75m1nd,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,AMD,2023-10-30 23:03:44,7
Intel,k75t1dz,I thought the PS5â€™s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,AMD,2023-10-30 23:51:23,1
Intel,k74c5po,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",AMD,2023-10-30 18:16:15,13
Intel,k74ejc7,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",AMD,2023-10-30 18:30:46,4
Intel,k74f5gw,"Next gen seems promising so far, low native rez hidden by upscaling, 60 FPS target on +500$ GPUs, traversal stuttering thanks to the glorious UE5. But hey, we have great reflections on puddles so at least we can take good looking screenshots.",AMD,2023-10-30 18:34:36,6
Intel,k762gt6,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,AMD,2023-10-31 00:54:19,0
Intel,k7795r7,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,AMD,2023-10-31 07:28:35,5
Intel,k8xdnpw,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,AMD,2023-11-12 13:53:39,1
Intel,k77azgf,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",AMD,2023-10-31 07:54:57,0
Intel,k7ad42q,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",AMD,2023-10-31 21:51:09,-1
Intel,k771qsz,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",AMD,2023-10-31 05:48:28,1
Intel,k7baowx,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",AMD,2023-11-01 01:58:28,1
Intel,k76iowi,This is what I am thinking too.,AMD,2023-10-31 02:46:08,1
Intel,k7al3b9,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",AMD,2023-10-31 22:48:01,1
Intel,k78r7ik,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",AMD,2023-10-31 15:51:02,0
Intel,k7eeuc1,Let's hope so. 30fps is far from recommended for FSR 3,AMD,2023-11-01 18:38:59,1
Intel,k7kbe2u,Yeah I heard it works with that hoping it works with fsr3 now,AMD,2023-11-02 21:25:15,1
Intel,k74jv5m,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",AMD,2023-10-30 19:03:14,40
Intel,k770h31,"Just lazy optimisation, thatâ€™s pretty much a PS5 equivalent GPU and you can be damn sure that the console version wonâ€™t be 1080P 30fps with FSR",AMD,2023-10-31 05:32:44,0
Intel,k74va9v,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",AMD,2023-10-30 20:13:10,-8
Intel,k78mlfi,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,AMD,2023-10-31 15:21:57,5
Intel,k776zix,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,AMD,2023-10-31 06:57:37,4
Intel,k77ebsi,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",AMD,2023-10-31 08:44:20,-1
Intel,k82kpsv,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",AMD,2023-11-06 14:19:02,1
Intel,k792d5j,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",AMD,2023-10-31 16:59:46,2
Intel,k77dtn2,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",AMD,2023-10-31 08:36:45,3
Intel,k77togb,Assassins creed origins and odyssey side quests/collectibles oh my god,AMD,2023-10-31 11:50:06,2
Intel,k74jzj8,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,AMD,2023-10-30 19:04:00,0
Intel,k74m5ig,I appreciate your insights and opinions. Thank you.,AMD,2023-10-30 19:17:14,-1
Intel,k775lx7,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",AMD,2023-10-31 06:38:51,1
Intel,k77sn6f,console games started upscaling way before PCs .,AMD,2023-10-31 11:40:07,4
Intel,k79hu3v,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,AMD,2023-10-31 18:34:55,3
Intel,k79vmwl,you forgot who owns one of the most popular engines out there?,AMD,2023-10-31 19:59:27,-1
Intel,k7abo7k,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",AMD,2023-10-31 21:41:33,-3
Intel,k77b8ol,downvoted by devs lol,AMD,2023-10-31 07:58:40,5
Intel,k79ib8i,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,AMD,2023-10-31 18:37:52,0
Intel,k78h0ux,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",AMD,2023-10-31 14:46:05,-1
Intel,k792yre,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,AMD,2023-10-31 17:03:29,0
Intel,k79ll9w,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,AMD,2023-10-31 18:57:46,2
Intel,k78xgtm,Why is there a dialog message about unsupported hardware when you try and run a 390X?,AMD,2023-10-31 16:29:37,3
Intel,k77blcl,It can do software based RT just like every other modern GPU out there.,AMD,2023-10-31 08:03:49,2
Intel,k74gdb3,Well then no wonder rx 5700 can't manage 30 fps lol,AMD,2023-10-30 18:41:59,28
Intel,k74m0rw,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",AMD,2023-10-30 19:16:25,10
Intel,k74ihd8,Avatars uses a Lumen like software RT solution.,AMD,2023-10-30 18:54:49,20
Intel,k78qd8n,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",AMD,2023-10-31 15:45:52,2
Intel,k74lfw8,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",AMD,2023-10-30 19:12:53,12
Intel,k75797r,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),AMD,2023-10-30 21:26:16,1
Intel,k74khy7,"Yeah, thatsl happened.",AMD,2023-10-30 19:07:09,0
Intel,k7a6im2,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",AMD,2023-10-31 21:07:50,1
Intel,k75wfsk,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",AMD,2023-10-31 00:14:32,1
Intel,k77chzy,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",AMD,2023-10-31 08:16:52,2
Intel,k77pinh,You didn't understand. It's another Swiss knife engine,AMD,2023-10-31 11:07:56,0
Intel,k76pny2,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",AMD,2023-10-31 03:41:22,1
Intel,k75nvkv,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,AMD,2023-10-30 23:16:13,-1
Intel,k77ap82,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,AMD,2023-10-31 07:50:56,-2
Intel,k78n2dv,The PS5 has a 6700.,AMD,2023-10-31 15:25:00,1
Intel,k7i9v2e,">I thought the PS5â€™s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",AMD,2023-11-02 13:58:02,1
Intel,k74jbph,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",AMD,2023-10-30 18:59:55,1
Intel,k8xdwme,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,AMD,2023-11-12 13:55:46,1
Intel,k77fln1,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. Iâ€™ve been waiting for kingdom hearts 3 to come to steam for so long that Iâ€™m basically going to skip it at this point. Itâ€™s still stuck on the epic game store.,AMD,2023-10-31 09:02:53,0
Intel,k7ahuzb,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,AMD,2023-10-31 22:24:31,1
Intel,k78sexp,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,AMD,2023-10-31 15:58:21,1
Intel,k74o0rt,TAA,AMD,2023-10-30 19:28:44,55
Intel,k74rhkm,Temporal anti aliasing.,AMD,2023-10-30 19:50:04,7
Intel,k78mepd,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",AMD,2023-10-31 15:20:46,3
Intel,k78zoea,"Ahh, welcome to r/FuckTAA",AMD,2023-10-31 16:43:15,1
Intel,k7fo2qt,Even 4K looks blurry with some implementations of TAA,AMD,2023-11-01 23:21:50,1
Intel,k775ulz,they're giving you the bare minimum until your upgrade!,AMD,2023-10-31 06:42:13,0
Intel,k74lgqm,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,AMD,2023-10-30 19:13:01,-6
Intel,k7551yk,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",AMD,2023-10-30 21:12:24,11
Intel,k74xjh2,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",AMD,2023-10-30 20:26:40,12
Intel,k7549zz,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",AMD,2023-10-30 21:07:34,3
Intel,k78jvij,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",AMD,2023-10-31 15:04:19,5
Intel,k79h6tb,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,AMD,2023-10-31 18:30:53,3
Intel,k77gqlw,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",AMD,2023-10-31 09:18:51,2
Intel,k78mn3v,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",AMD,2023-10-31 15:22:16,4
Intel,k7kwbcl,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,AMD,2023-11-02 23:41:35,1
Intel,k783kme,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720pâ€¦",AMD,2023-10-31 13:13:39,-1
Intel,k7adm22,No I havent. AMD is bigger than Epic.,AMD,2023-10-31 21:54:30,2
Intel,k7aqfmz,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",AMD,2023-10-31 23:27:21,5
Intel,k77exua,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",AMD,2023-10-31 08:53:22,0
Intel,k79j5la,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",AMD,2023-10-31 18:43:01,5
Intel,k78wlj1,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,AMD,2023-10-31 16:24:13,6
Intel,k79ipwc,It's software ray tracing which isn't accelerated by hardware.,AMD,2023-10-31 18:40:20,2
Intel,k79sb21,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,AMD,2023-10-31 19:39:06,1
Intel,k79a73q,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,AMD,2023-10-31 17:47:59,2
Intel,k74o26m,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,AMD,2023-10-30 19:28:58,20
Intel,k778s32,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",AMD,2023-10-31 07:23:06,1
Intel,k75bea7,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,AMD,2023-10-30 21:52:53,4
Intel,k75j5le,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,AMD,2023-10-30 22:44:05,7
Intel,k74nnyo,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",AMD,2023-10-30 19:26:34,2
Intel,k7a88v5,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",AMD,2023-10-31 21:19:00,1
Intel,k77pvj3,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,AMD,2023-10-31 11:11:45,4
Intel,k78lq5t,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",AMD,2023-10-31 15:16:17,3
Intel,k76zwtm,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,AMD,2023-10-31 05:25:55,2
Intel,k77ddap,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100â‚¬ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",AMD,2023-10-31 08:29:49,7
Intel,k77dhd7,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",AMD,2023-10-31 08:31:33,5
Intel,k77h10q,Both excellent ðŸ‘Œ Down the Rabbit Hole was another solid one.,AMD,2023-10-31 09:22:50,1
Intel,k7b776f,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",AMD,2023-11-01 01:31:29,1
Intel,k78ut1x,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",AMD,2023-10-31 16:13:03,0
Intel,k74xlfo,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,AMD,2023-10-30 20:27:00,26
Intel,k74uq1m,I have to turn it off in borderlands 3. Ugh.,AMD,2023-10-30 20:09:47,2
Intel,k78zmsb,r/FuckTAA,AMD,2023-10-31 16:42:58,0
Intel,k7n69qz,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesnâ€™t he says that he sees the image about the same as without glasses",AMD,2023-11-03 12:46:56,1
Intel,k7gyunv,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",AMD,2023-11-02 05:20:45,1
Intel,k7bw0wl,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,AMD,2023-11-01 05:13:29,-1
Intel,k78p5rj,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",AMD,2023-10-31 15:38:25,2
Intel,k7kx549,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 canâ€™t max out the game at native 4K resolution.,AMD,2023-11-02 23:47:05,1
Intel,k7c5sa3,epic is tencent...,AMD,2023-11-01 07:26:14,-1
Intel,k79evb5,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",AMD,2023-10-31 18:16:40,0
Intel,k79vqds,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",AMD,2023-10-31 20:00:02,1
Intel,k79dbl9,You don't need RT hardware to do software RT. That's what I'm saying.,AMD,2023-10-31 18:07:06,3
Intel,k74wirt,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",AMD,2023-10-30 20:20:36,20
Intel,k755cwh,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",AMD,2023-10-30 21:14:19,19
Intel,k74ogkv,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),AMD,2023-10-30 19:31:29,15
Intel,k751p3z,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,AMD,2023-10-30 20:51:38,10
Intel,k75za9c,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",AMD,2023-10-31 00:33:28,9
Intel,k76k5hm,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,AMD,2023-10-31 02:56:53,3
Intel,k77u7zx,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,AMD,2023-10-31 11:55:06,2
Intel,k77anl2,Yes hah,AMD,2023-10-31 07:50:17,1
Intel,k78kxsq,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",AMD,2023-10-31 15:11:02,1
Intel,k75bm3k,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I donâ€™t know. Perhaps it signal not so great optimization for Intel GPUs...",AMD,2023-10-30 21:54:17,1
Intel,k75jy4k,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,AMD,2023-10-30 22:49:28,6
Intel,k75cf9l,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",AMD,2023-10-30 21:59:30,10
Intel,k74s4b6,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",AMD,2023-10-30 19:53:57,3
Intel,k78qh5r,Oh man the hair... its so crazy how much upscaling kills the hair..,AMD,2023-10-31 15:46:32,0
Intel,k74vpzd,i mean they already arent the smartest bulbs considering they went team green.,AMD,2023-10-30 20:15:50,-9
Intel,k7bwwni,Reading comrehension dude.,AMD,2023-11-01 05:24:02,1
Intel,k77rq8u,"I do, but thanks for your interest.",AMD,2023-10-31 11:31:01,0
Intel,k781oqh,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,AMD,2023-10-31 12:59:12,1
Intel,k77gylo,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",AMD,2023-10-31 09:21:55,0
Intel,k77prws,Nice. Itâ€™s on the list. Thanks man,AMD,2023-10-31 11:10:40,0
Intel,k79111l,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,AMD,2023-10-31 16:51:38,1
Intel,k74y62b,Probably because TAA is (unfortunately) more prevalent than it ever was.,AMD,2023-10-30 20:30:23,12
Intel,k77crxd,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",AMD,2023-10-31 08:20:57,5
Intel,k7ky7yi,Oh my bad if I read that wrong,AMD,2023-11-02 23:54:13,1
Intel,k79nw3z,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,AMD,2023-10-31 19:11:53,1
Intel,k74xr8d,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,AMD,2023-10-30 20:27:57,-11
Intel,k78nlj0,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",AMD,2023-10-31 15:28:29,1
Intel,k78nx4p,imagine being mad that 4 year old cards arent high end,AMD,2023-10-31 15:30:32,2
Intel,k78qsw0,I'm on a 6700XT. ðŸ™‡. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,AMD,2023-10-31 15:48:33,0
Intel,k75i9u4,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",AMD,2023-10-30 22:38:10,1
Intel,k77gx0m,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",AMD,2023-10-31 09:21:18,1
Intel,k74smha,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",AMD,2023-10-30 19:57:03,0
Intel,k78qwba,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",AMD,2023-10-31 15:49:08,0
Intel,k74wqog,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",AMD,2023-10-30 20:21:55,2
Intel,k7bxbe9,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",AMD,2023-11-01 05:29:04,1
Intel,k77y4x2,"It's for textures, not object edges from FSR use, lol. Two completely different things",AMD,2023-10-31 12:30:16,5
Intel,k793lf6,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",AMD,2023-10-31 17:07:23,-1
Intel,k77g0i6,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",AMD,2023-10-31 09:08:37,12
Intel,k77ubrs,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",AMD,2023-10-31 11:56:04,-1
Intel,k8f5yw4,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",AMD,2023-11-08 22:27:09,1
Intel,k79oe6d,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,AMD,2023-10-31 19:14:56,1
Intel,k753gel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",AMD,2023-10-30 21:02:23,12
Intel,k78lli0,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,AMD,2023-10-31 15:15:24,5
Intel,k7au2y4,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",AMD,2023-10-31 23:54:23,1
Intel,k75wc3v,5700 was probably the lowest AMD card they had to test with.,AMD,2023-10-31 00:13:51,1
Intel,k74tcrl,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",AMD,2023-10-30 20:01:29,6
Intel,k7byrit,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,AMD,2023-11-01 05:47:35,1
Intel,k7a8jw6,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,AMD,2023-10-31 21:20:59,1
Intel,k7kn3dl,AMD CAS is aimed to restore detail,AMD,2023-11-02 22:40:21,1
Intel,k7k5dha,"They arenâ€™t chief proponents of anything, TAAU has been popular for well over a decade at this point and itâ€™s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",AMD,2023-11-02 20:48:40,1
Intel,k7cl7iw,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,AMD,2023-11-01 10:59:12,2
Intel,k74ugbm,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",AMD,2023-10-30 20:08:12,0
Intel,k7bzcr6,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",AMD,2023-11-01 05:55:17,1
Intel,k7cntlm,Yeah but a 3060ti didn't,AMD,2023-11-01 11:27:22,1
Intel,k78mcrt,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",AMD,2023-10-31 15:20:25,2
Intel,k74zgnn,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",AMD,2023-10-30 20:38:09,3
Intel,k1rzmke,Confirmed Can it run CP2077 4k is the new Can it run Crysis,AMD,2023-09-22 22:22:20,587
Intel,k1s9f6d,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,AMD,2023-09-22 23:31:10,588
Intel,k1rtgmz,In 1440p with 7900 xtx with fsr 2.1 quality it doesnâ€™t even get 30fps,AMD,2023-09-22 21:41:53,307
Intel,k1tezss,"Makes sense. Is almost full path tracing, itâ€™s insane itâ€™s even running.",AMD,2023-09-23 05:11:42,25
Intel,k1spmgk,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,AMD,2023-09-23 01:30:07,20
Intel,k1ryede,3080 falling behind a 3060? what is this data?,AMD,2023-09-22 22:14:11,127
Intel,k1say8p,wake me up when we have a card that can run this at 40 without needing its own psu.,AMD,2023-09-22 23:42:17,47
Intel,k1seqym,5090 here I come!,AMD,2023-09-23 00:09:51,28
Intel,k1sf8id,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",AMD,2023-09-23 00:13:23,37
Intel,k1samn8,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",AMD,2023-09-22 23:39:57,28
Intel,k1se7w7,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,AMD,2023-09-23 00:06:00,28
Intel,k1tcwd4,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",AMD,2023-09-23 04:49:38,6
Intel,k1swxw6,It's a good thing nobody has to actually play it native.,AMD,2023-09-23 02:26:41,19
Intel,k1sd0w5,Well good thing literally nobody is doing thatâ€¦,AMD,2023-09-22 23:57:14,11
Intel,k1tv5m1,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidiaâ€™s technology it runs so damn smooth as well.,AMD,2023-09-23 08:24:44,10
Intel,k1svrss,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,AMD,2023-09-23 02:17:32,15
Intel,k1u5n8e,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,AMD,2023-09-23 10:40:09,5
Intel,k1t06iw,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",AMD,2023-09-23 02:52:43,10
Intel,k1sta67,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,AMD,2023-09-23 01:58:23,14
Intel,k1ru0dy,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",AMD,2023-09-22 21:45:23,27
Intel,k1sh29m,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",AMD,2023-09-23 00:26:40,21
Intel,k1rwvmw,Running this way means you lose RRâ€¦why in the world would you run at native 4k?  Itâ€™s completely pointless now with RR.,AMD,2023-09-22 22:04:05,24
Intel,k1s5ads,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,AMD,2023-09-22 23:01:20,8
Intel,k1t5pl6,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",AMD,2023-09-23 03:40:25,7
Intel,k1sde74,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",AMD,2023-09-22 23:59:57,16
Intel,k1sbtem,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",AMD,2023-09-22 23:48:27,6
Intel,k1sr9s2,This doesnâ€™t seem rightâ€¦. 3080 performing worse than a 2080TI?,AMD,2023-09-23 01:42:55,2
Intel,k1t2kii,How tf is a 2080 ti getting more fps than a 3080?!?,AMD,2023-09-23 03:12:42,2
Intel,k1t66da,I'm not seeing the RTX A6000 on here...,AMD,2023-09-23 03:44:41,2
Intel,k1tglq7,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,AMD,2023-09-23 05:29:29,2
Intel,k1u9v0r,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",AMD,2023-09-23 11:28:08,2
Intel,k1ufe40,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",AMD,2023-09-23 12:22:22,2
Intel,k1witvs,Does anyone actually play this game? Its more of a meme game imho,AMD,2023-09-23 20:46:05,2
Intel,k1xd78i,4.3 fps ðŸ˜‚ðŸ˜‚ðŸ˜‚,AMD,2023-09-24 00:13:50,2
Intel,k1sczh9,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astoundingâ€¦. 19.5FPS ðŸ˜«ðŸ˜‚",AMD,2023-09-22 23:56:57,10
Intel,k1rumnu,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",AMD,2023-09-22 21:49:26,9
Intel,k1shlno,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,AMD,2023-09-23 00:30:36,2
Intel,k1t2byj,Lol the 3060ti is better than a 7900xtx...crazy,AMD,2023-09-23 03:10:40,4
Intel,k1vuadd,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,AMD,2023-09-23 18:12:06,3
Intel,k1s8b2u,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,AMD,2023-09-22 23:23:07,10
Intel,k1t61f2,ThE gAMe iS PoOrLY OpTImiSed!!!,AMD,2023-09-23 03:43:26,3
Intel,k1ttlda,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,AMD,2023-09-23 08:04:31,2
Intel,k1sgjyj,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",AMD,2023-09-23 00:22:53,4
Intel,k1sxb06,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   Thatâ€™s a big difference from this chart what Iâ€™m getting vs what theyâ€™re getting. For record, Iâ€™m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",AMD,2023-09-23 02:29:34,2
Intel,k1sydqu,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",AMD,2023-09-23 02:38:06,2
Intel,k1tbdbm,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,AMD,2023-09-23 04:34:09,2
Intel,k1twz2u,"""4090 is 4 times faster than 7900XTX""",AMD,2023-09-23 08:48:39,2
Intel,k1uouoc,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",AMD,2023-09-23 13:40:46,2
Intel,k1wadup,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,AMD,2023-09-23 19:52:54,2
Intel,k1tl3qo,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),AMD,2023-09-23 06:20:29,2
Intel,k1s8g0f,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",AMD,2023-09-22 23:24:08,3
Intel,k1s8o3g,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",AMD,2023-09-22 23:25:45,2
Intel,k1sg5kw,Sounds like Ray tracing isnâ€™t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. Iâ€™ll worry about Ray tracing when itâ€™s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,AMD,2023-09-23 00:20:00,3
Intel,k1skakr,Native resolution is a thing of the past. DLSS looks better than native anyway.,AMD,2023-09-23 00:50:07,1
Intel,k1txff8,Is this with DLSS + FG?,AMD,2023-09-23 08:54:33,1
Intel,k1smbmr,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",AMD,2023-09-23 01:05:06,-2
Intel,k1s3u4i,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",AMD,2023-09-22 22:51:01,1
Intel,k1t2qkw,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,AMD,2023-09-23 03:14:12,0
Intel,k1sgzlr,No one is using these settings.,AMD,2023-09-23 00:26:07,1
Intel,k1spqtc,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",AMD,2023-09-23 01:31:01,1
Intel,k1shcs4,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",AMD,2023-09-23 00:28:49,1
Intel,k1sj5mo,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",AMD,2023-09-23 00:41:56,1
Intel,k1ulwx6,cyberpunk sucks don't worry about it,AMD,2023-09-23 13:17:47,1
Intel,k1rvukw,3080ti?,AMD,2023-09-22 21:57:22,1
Intel,k1sljwz,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",AMD,2023-09-23 00:59:20,1
Intel,k1sqr9g,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,AMD,2023-09-23 01:38:55,1
Intel,k1tt1b6,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",AMD,2023-09-23 07:57:19,1
Intel,k1tuc3v,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",AMD,2023-09-23 08:14:11,1
Intel,k1uq4ce,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,AMD,2023-09-23 13:50:25,1
Intel,k1s9ert,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),AMD,2023-09-22 23:31:05,-1
Intel,k1rzvbb,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesnâ€™t always look better. If the one true God card canâ€™t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most arenâ€™t even spending $1600 on their whole build!",AMD,2023-09-22 22:23:58,-18
Intel,k1sfn3w,What body part do you think will get me a 5090?,AMD,2023-09-23 00:16:18,0
Intel,k1txo0m,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",AMD,2023-09-23 08:57:42,0
Intel,k1u7ngf,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",AMD,2023-09-23 11:04:00,0
Intel,k1uxont,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",AMD,2023-09-23 14:44:13,0
Intel,k1s5mej,That not even proper path tracing.,AMD,2023-09-22 23:03:45,-2
Intel,k1s6qt1,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,AMD,2023-09-22 23:11:53,-2
Intel,k1sb5wk,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT ðŸ¤‘ðŸ¤‘ðŸ¤‘",AMD,2023-09-22 23:43:51,-10
Intel,k1svwfa,Native is a thing of the past when dlss produces better looking image,AMD,2023-09-23 02:18:33,-3
Intel,k1sdfih,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,AMD,2023-09-23 00:00:13,-4
Intel,k1s7xzc,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,AMD,2023-09-22 23:20:30,-9
Intel,k1slr2s,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,AMD,2023-09-23 01:00:47,-1
Intel,k1sfvki,"Rather than having 2-3fps, I would go and see a video of path tracing.",AMD,2023-09-23 00:17:59,0
Intel,k1sihxn,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",AMD,2023-09-23 00:37:09,0
Intel,k1su9sh,cool idc i wont play that shit at such shit settings,AMD,2023-09-23 02:05:55,0
Intel,k1syk9g,Do we need this BS? Seriously? Gpuâ€™s will end-up costing over 2 grand soon,AMD,2023-09-23 02:39:33,0
Intel,k1thwl5,"And everyone loose their minds about the last ports that came out! Starfield, TLOU, etc. ItÂ´s not that the gpuÂ´s are not good enough. Ray Tracing and Path tracing are just not optimized.",AMD,2023-09-23 05:43:58,0
Intel,k1tpxos,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,AMD,2023-09-23 07:18:17,0
Intel,k1tvoqm,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",AMD,2023-09-23 08:31:41,0
Intel,k1tytwk,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,AMD,2023-09-23 09:12:53,0
Intel,k1tzugl,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,AMD,2023-09-23 09:26:15,0
Intel,k1uhusa,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",AMD,2023-09-23 12:44:21,0
Intel,k1v2ghf,RTX 4090 1600â‚¬ 19FPS ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼ðŸ‘ðŸ¼,AMD,2023-09-23 15:16:08,0
Intel,k1v8o7m,"Yay i will get 13,8 FPS with my 4080 ðŸ‘ðŸ˜‚ Someone want to borrow some frames? I don't need them all!",AMD,2023-09-23 15:55:59,0
Intel,k1vfxae,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,AMD,2023-09-23 16:41:57,0
Intel,k1vjs6i,"Looks like 4K is mainly just for videos, not gaming for the time being.",AMD,2023-09-23 17:06:09,0
Intel,k1vnxlu,How can you bring out such technology and no hardware can process it properly.  an impudence,AMD,2023-09-23 17:32:09,0
Intel,k1wm3gk,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,AMD,2023-09-23 21:06:13,0
Intel,k1ysyyz,You could just you know... disable path tracing and get 70 fps or go below 4k,AMD,2023-09-24 08:12:13,0
Intel,k1z173o,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",AMD,2023-09-24 09:58:57,0
Intel,k25idkz,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",AMD,2023-09-25 16:06:18,0
Intel,k1rv0mr,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,AMD,2023-09-22 21:51:56,-34
Intel,k1sbmt5,"so it is settled, Devs have zero optimization in games.",AMD,2023-09-22 23:47:08,-12
Intel,k1t5ilx,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",AMD,2023-09-23 03:38:40,-2
Intel,k1t8qzc,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",AMD,2023-09-23 04:08:39,-2
Intel,k1syqs1,Once again proving RT is useless,AMD,2023-09-23 02:41:00,-7
Intel,k1sfuie,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,AMD,2023-09-23 00:17:45,322
Intel,k1sw4a8,"""only gamers get this joke""",AMD,2023-09-23 02:20:15,1
Intel,k1tqj3o,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",AMD,2023-09-23 07:25:29,-3
Intel,k1uey5o,Starfield is peaking around the corner.,AMD,2023-09-23 12:18:19,0
Intel,k1ume3r,starfield as well,AMD,2023-09-23 13:21:33,0
Intel,k1smpf3,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,AMD,2023-09-23 01:07:57,411
Intel,k1sm1vi,The new Crysis,AMD,2023-09-23 01:03:02,18
Intel,k1t68ad,"I wouldnâ€™t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",AMD,2023-09-23 03:45:09,40
Intel,k1tduwg,How long until a $200 card can do that?,AMD,2023-09-23 04:59:33,11
Intel,k1tl6bz,Most improvement is probably going to AI software more than hardware in the next few years.,AMD,2023-09-23 06:21:18,12
Intel,k1sui5w,"8800GT giving advice to 4090:  â€œI used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",AMD,2023-09-23 02:07:40,32
Intel,k1tggmb,GPU need to have its own garage by then,AMD,2023-09-23 05:27:53,9
Intel,k1sjd2a,Yep! Insane how fast things change.,AMD,2023-09-23 00:43:26,16
Intel,k1th7kz,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. Itâ€™s increasingly more challenging to make more dense chips.  Thatâ€™s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",AMD,2023-09-23 05:36:13,26
Intel,k1tksc7,Yeah rtx 12060 with 9.5 gb Vram will be a monster,AMD,2023-09-23 06:16:50,14
Intel,k1toc97,I'm willing to bet hardware improvement will come to a halt before that.,AMD,2023-09-23 06:59:10,4
Intel,k1tp7w0,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",AMD,2023-09-23 07:09:42,9
Intel,k1tk7k9,10 is too much. Give it 5.,AMD,2023-09-23 06:10:19,2
Intel,k1u5t5c,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,AMD,2023-09-23 10:42:10,2
Intel,k1u9trt,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,AMD,2023-09-23 11:27:46,2
Intel,k1umihs,But then the current gen games of that era will run like this. The cycle continues,AMD,2023-09-23 13:22:31,2
Intel,k1vcazz,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,AMD,2023-09-23 16:19:04,2
Intel,k1t8b6k,Who knows what new tech will be out in even 4 years lol,AMD,2023-09-23 04:04:25,2
Intel,k1tl2z8,"And thatâ€™s when Iâ€™ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, itâ€™s more like a proof of concept at this point.",AMD,2023-09-23 06:20:14,1
Intel,k1u0yj3,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",AMD,2023-09-23 09:40:52,1
Intel,k1u6uez,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",AMD,2023-09-23 10:54:31,1
Intel,k1sby3m,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,AMD,2023-09-22 23:49:24,310
Intel,k1rw4fg,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",AMD,2023-09-22 21:59:09,106
Intel,k1sbwfh,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",AMD,2023-09-22 23:49:03,10
Intel,k1t5qx0,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isnâ€™t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",AMD,2023-09-23 03:40:46,-2
Intel,k1tf1z9,I think that most of the progress will go together with software tricks and upscalers.,AMD,2023-09-23 05:12:21,4
Intel,k1s07rb,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",AMD,2023-09-22 22:26:17,125
Intel,k1rz345,That's VRAM for you,AMD,2023-09-22 22:18:45,131
Intel,k1tc7vb,we are counting decimals of fps its just all margin of error.,AMD,2023-09-23 04:42:40,12
Intel,k1s03of,If you buy a card with low ram that card is for right now only lol,AMD,2023-09-22 22:25:32,5
Intel,k1v5pv8,Wake me up when a $250 GPU can run this at 1080p.,AMD,2023-09-23 15:37:12,8
Intel,k1uiun2,Well DLSS isn't best. DLAA is,AMD,2023-09-23 12:52:43,6
Intel,k1svcks,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,AMD,2023-09-23 02:14:15,26
Intel,k1si3ng,Canâ€™t you just disable TAA? Or do you just have to live with the ghosting?,AMD,2023-09-23 00:34:15,5
Intel,k1v2dce,TAA is garbage in everything. TAA and FSR can both get fucked,AMD,2023-09-23 15:15:33,3
Intel,k1svesl,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",AMD,2023-09-23 02:14:43,7
Intel,k1v301u,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",AMD,2023-09-23 15:19:41,2
Intel,k1v2lmw,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",AMD,2023-09-23 15:17:04,2
Intel,k1sv2p6,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",AMD,2023-09-23 02:12:07,7
Intel,k1vdbm6,Imagine buying a 4090 and then using upscaling.,AMD,2023-09-23 16:25:29,-5
Intel,k1scblp,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,AMD,2023-09-22 23:52:08,17
Intel,k1sfzkt,PC gaming is in Crysis.,AMD,2023-09-23 00:18:47,4
Intel,k1y7rss,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,AMD,2023-09-24 04:15:05,-1
Intel,k1t8lmy,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,AMD,2023-09-23 04:07:13,2
Intel,k1rxtnn,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",AMD,2023-09-22 22:10:20,8
Intel,k1u60lu,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features ðŸ˜",AMD,2023-09-23 10:44:39,14
Intel,k1sizpa,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",AMD,2023-09-23 00:40:43,-6
Intel,k1u5vqx,Yeh because native 4k looks worse than dlss + RR 4k,AMD,2023-09-23 10:43:02,8
Intel,k1xi8dd,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",AMD,2023-09-24 00:50:15,2
Intel,k1u5mhq,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,AMD,2023-09-23 10:39:55,11
Intel,k1sshhs,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",AMD,2023-09-23 01:52:13,8
Intel,k1sr5lk,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",AMD,2023-09-23 01:42:00,1
Intel,k1t41gi,VRAM,AMD,2023-09-23 03:25:30,3
Intel,k1uiuuv,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",AMD,2023-09-23 12:52:46,1
Intel,k1zcvhv,still better than 90% of games in 2023,AMD,2023-09-24 12:08:19,0
Intel,k1rvtrp,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,AMD,2023-09-22 21:57:13,20
Intel,k1s0kpr,"â€¦no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",AMD,2023-09-22 22:28:43,-1
Intel,k1sbnjt,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",AMD,2023-09-22 23:47:17,-8
Intel,k1u6o55,All graphics you see on your computer screen is fake,AMD,2023-09-23 10:52:26,2
Intel,k1t62rv,Path tracing is more demanding than Ray tracing,AMD,2023-09-23 03:43:46,5
Intel,k1u17zi,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",AMD,2023-09-23 09:44:22,4
Intel,k1s9p4d,I guess between the 3090 and the 4070,AMD,2023-09-22 23:33:11,0
Intel,k1sjydp,Yep hahaha,AMD,2023-09-23 00:47:40,2
Intel,k1v42p3,RemindMe! 7 years,AMD,2023-09-23 15:26:41,2
Intel,k1s3vi6,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",AMD,2023-09-22 22:51:16,19
Intel,k1s45n5,"This is an absurd, fanboyish thought lmao",AMD,2023-09-22 22:53:15,14
Intel,k1u3kdb,dont let novidia marketing see this youll get down voted into oblivion,AMD,2023-09-23 10:14:19,2
Intel,k1s2le1,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,AMD,2023-09-22 22:42:21,-8
Intel,k1sylo6,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,AMD,2023-09-23 02:39:51,3
Intel,k1sltal,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",AMD,2023-09-23 01:01:15,2
Intel,k1u6rsu,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,AMD,2023-09-23 10:53:38,2
Intel,k1t37ky,Fanboyism of both kinds is bad,AMD,2023-09-23 03:18:18,0
Intel,k1sbir3,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",AMD,2023-09-22 23:46:20,-1
Intel,k1u85m7,Since it needs more than 10gb vram,AMD,2023-09-23 11:09:41,3
Intel,k1tq0gr,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,AMD,2023-09-23 07:19:12,4
Intel,k1rw9dl,Itâ€™s not unoptimized at all. Game runs great for the quality of the graphics. You guys just donâ€™t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw â€œunoptimizedâ€ at it because they donâ€™t understand the technical marvel happening in front of their eyes.,AMD,2023-09-22 22:00:02,30
Intel,k1sl1fh,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",AMD,2023-09-23 00:55:34,8
Intel,k1rx0ka,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",AMD,2023-09-22 22:04:58,-15
Intel,k1u2f5k,It's settled that you have no idea what you talking about,AMD,2023-09-23 09:59:44,3
Intel,k1sczqa,Its full path tracing u cannot really optimize this much.,AMD,2023-09-22 23:57:00,11
Intel,k1tpj68,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",AMD,2023-09-23 07:13:34,2
Intel,k1s9h3d,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",AMD,2023-09-22 23:31:33,2
Intel,k1sprwd,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,AMD,2023-09-23 01:31:15,143
Intel,k1tds1v,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",AMD,2023-09-23 04:58:42,27
Intel,k1u7ebf,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,AMD,2023-09-23 11:01:03,18
Intel,k1vo73s,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",AMD,2023-09-23 17:33:46,5
Intel,k1u7ms4,Amd had a tessellation unit. It went unused but it was present,AMD,2023-09-23 11:03:47,2
Intel,k1srvji,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",AMD,2023-09-23 01:47:34,162
Intel,k1sqn2i,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",AMD,2023-09-23 01:38:00,33
Intel,k1tzlza,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",AMD,2023-09-23 09:23:04,5
Intel,k1uatm9,Yeah. People with Windows 7 were using AMD to play the game with old Nvidia card for Physx. Nvidia didn't like that and blocked it via driver,AMD,2023-09-23 11:38:16,5
Intel,k1u8wkg,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",AMD,2023-09-23 11:17:47,17
Intel,k1tdixd,"Oh damn, I forgot about those cards.  I wanted one so badly.",AMD,2023-09-23 04:56:03,3
Intel,k1u3mme,Remember when companies tried to sell physics cards lol,AMD,2023-09-23 10:15:05,3
Intel,k1thwg4,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,AMD,2023-09-23 05:43:55,14
Intel,k1tjl5e,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",AMD,2023-09-23 06:03:08,2
Intel,k1umriw,I remember when people had a dedicated PhysX card.,AMD,2023-09-23 13:24:29,2
Intel,k1un2sg,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,AMD,2023-09-23 13:26:56,16
Intel,k1tob83,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",AMD,2023-09-23 06:58:48,4
Intel,k1txgpk,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,AMD,2023-09-23 08:55:01,6
Intel,k1ugyqs,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",AMD,2023-09-23 12:36:34,3
Intel,k1ubuqx,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, thereâ€™s not much incentive to do the latter.",AMD,2023-09-23 11:48:42,2
Intel,k1uc6ox,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, weâ€™re just gonna spend it on space crack.",AMD,2023-09-23 11:51:55,7
Intel,k1uln5g,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",AMD,2023-09-23 13:15:37,4
Intel,k1ull2c,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,AMD,2023-09-23 13:15:10,2
Intel,k1tocdn,25 years,AMD,2023-09-23 06:59:12,6
Intel,k1u1mrk,"The software needs to run on hardware, right now it eats through GPU compute and memory.",AMD,2023-09-23 09:49:38,6
Intel,k1tqm89,And sadly ended up with 450 Watt TDP to achieve that performance.,AMD,2023-09-23 07:26:36,11
Intel,k1tvo5h,This. We'd be lucky to see more than 3 generations in the upcoming decade.,AMD,2023-09-23 08:31:28,4
Intel,k1u1jnl,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",AMD,2023-09-23 09:48:32,3
Intel,k1ucdje,Not with that attitude,AMD,2023-09-23 11:53:45,2
Intel,k1ugj3a,Love how it's still gimped on memory size ðŸ˜‚,AMD,2023-09-23 12:32:43,8
Intel,k1u3901,"They just need to render the minimum information needed , and let the ai do the rest",AMD,2023-09-23 10:10:17,2
Intel,k1u9l5i,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",AMD,2023-09-23 11:25:11,-4
Intel,k1u27rs,"Feels pretty good with 120 frames. Playing with a controller, i donâ€˜t mind or even feel the input lag on a projector screen.",AMD,2023-09-23 09:57:10,4
Intel,k1ufc12,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",AMD,2023-09-23 12:21:51,2
Intel,k1t3hpt,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",AMD,2023-09-23 03:20:42,158
Intel,k1tek93,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",AMD,2023-09-23 05:07:01,12
Intel,k1uoawi,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",AMD,2023-09-23 13:36:30,3
Intel,k1su9mw,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,AMD,2023-09-23 02:05:53,-20
Intel,k1sbp7z,"Thatâ€™d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",AMD,2023-09-22 23:47:36,83
Intel,k1sxtj8,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I wonâ€™t upgrade for another year or 2 (just bought a phone so Iâ€™m broke right now)",AMD,2023-09-23 02:33:36,6
Intel,k1svx2t,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area itâ€™ll drop to mid 40s. But itâ€™s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Havenâ€™t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,AMD,2023-09-23 02:18:41,3
Intel,k1sbvoe,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",AMD,2023-09-22 23:48:54,17
Intel,k1slgjk,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,AMD,2023-09-23 00:58:39,10
Intel,k1tf7yk,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",AMD,2023-09-23 05:14:13,3
Intel,k1sad2c,So rendering at 960p? Oof...,AMD,2023-09-22 23:38:00,7
Intel,k1sd3lh,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",AMD,2023-09-22 23:57:47,-5
Intel,k1rxn27,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,AMD,2023-09-22 22:09:06,-18
Intel,k1s6fmd,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,AMD,2023-09-22 23:09:38,-25
Intel,k1tzl0f,Dont use useless raytracing and you wont have any problems lol,AMD,2023-09-23 09:22:43,-2
Intel,k1vpidg,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",AMD,2023-09-23 17:42:04,6
Intel,k23ed9f,AW2 looks insane. Can't wait to play soon,AMD,2023-09-25 04:14:36,2
Intel,k1si0t0,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,AMD,2023-09-23 00:33:40,23
Intel,k1s5qaj,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",AMD,2023-09-22 23:04:33,35
Intel,k1sor0g,My 3080 in shambles,AMD,2023-09-23 01:23:27,4
Intel,k1u5gbh,"It looks better than native even with fsr quality, the Taa in this game is shit",AMD,2023-09-23 10:37:48,-1
Intel,k1t69ca,You dont need to increase raw performance. You need to increase RT performance.,AMD,2023-09-23 03:45:25,31
Intel,k1vko89,Imagine!,AMD,2023-09-23 17:11:45,6
Intel,k1vz5q2,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,AMD,2023-09-23 18:42:28,6
Intel,k22igme,Only people who don't give a shit about high quality graphics don't care about ray tracing.,AMD,2023-09-25 00:09:28,3
Intel,k1tamw3,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,AMD,2023-09-23 04:26:51,4
Intel,k1s8m60,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",AMD,2023-09-22 23:25:22,3
Intel,k1v3dh2,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",AMD,2023-09-23 15:22:07,6
Intel,k1u1pck,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now itâ€™s just absurd,AMD,2023-09-23 09:50:33,8
Intel,k1sl137,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",AMD,2023-09-23 00:55:30,3
Intel,k1u5xqe,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,AMD,2023-09-23 10:43:42,-3
Intel,k22yxwq,The Evangelical Church of Native,AMD,2023-09-25 02:08:44,1
Intel,k1vk8iw,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",AMD,2023-09-23 17:09:00,2
Intel,k22yk42,Nice but ive been playing video games since the 70s and its Shit end off..,AMD,2023-09-25 02:05:53,2
Intel,k1s3cm8,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,AMD,2023-09-22 22:47:37,-13
Intel,k1ssze4,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",AMD,2023-09-23 01:56:05,-4
Intel,k1tpcbe,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,AMD,2023-09-23 07:11:12,10
Intel,k1srkr7,"Yeah, sure, now go back to play starfield.",AMD,2023-09-23 01:45:16,7
Intel,k1siya9,"There literally is a /s, what else do you need to detect sarcasm?",AMD,2023-09-23 00:40:26,3
Intel,k1ycket,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",AMD,2023-09-24 05:03:07,1
Intel,k1wclz4,"I know, which makes path tracing even worse off imo.",AMD,2023-09-23 20:06:58,1
Intel,k1v47c6,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,AMD,2023-09-23 15:27:32,2
Intel,k1vh7xa,haha this is gold ðŸ¥‡,AMD,2023-09-23 16:50:10,2
Intel,k1sakbd,"At the moment, itâ€™s definitely what it seems like.  Especially seeing how thereâ€™s only one card on the planet that can run this. Do you have a 4090?",AMD,2023-09-22 23:39:29,-7
Intel,k1s5318,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",AMD,2023-09-22 22:59:52,7
Intel,k1t4yk0,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",AMD,2023-09-23 03:33:41,-3
Intel,k1sakwq,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",AMD,2023-09-22 23:39:36,-9
Intel,k1ryjta,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,AMD,2023-09-22 22:15:11,1
Intel,k1stpfv,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,AMD,2023-09-23 02:01:36,41
Intel,k1w9mwj,Funnily Crysis still have better destructible environments than Cyberpunk has tho,AMD,2023-09-23 19:48:19,1
Intel,k1vonge,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",AMD,2023-09-23 17:36:37,8
Intel,k1txp0j,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,AMD,2023-09-23 08:58:05,17
Intel,k1tg0xs,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",AMD,2023-09-23 05:23:00,6
Intel,k1syqzz,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",AMD,2023-09-23 02:41:04,6
Intel,k1u23r0,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,AMD,2023-09-23 09:55:42,3
Intel,k1u67ug,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,AMD,2023-09-23 10:47:01,13
Intel,k1t2x0x,LOL I remember this exact scene also.,AMD,2023-09-23 03:15:45,37
Intel,k1ua0tu,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",AMD,2023-09-23 11:29:52,10
Intel,k1t6cm4,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",AMD,2023-09-23 03:46:15,20
Intel,k1tlbna,"I'd like to see ray tracing addon cards, seems logical to me.",AMD,2023-09-23 06:23:01,10
Intel,k1tyf7o,"TBH, I could probably run some of the old games I have on CPU without the GPU.",AMD,2023-09-23 09:07:31,3
Intel,k1ub8fu,Had the exact same experience. One scene in the whole game that actually used PhysX,AMD,2023-09-23 11:42:32,3
Intel,k1uhy6a,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,AMD,2023-09-23 12:45:10,3
Intel,k1tcex2,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,AMD,2023-09-23 04:44:40,6
Intel,k1tsmml,What gpu you have,AMD,2023-09-23 07:51:58,2
Intel,k1tyt5q,Now it even runs fine on a Ryzen 2400G.,AMD,2023-09-23 09:12:37,2
Intel,k1t9hds,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",AMD,2023-09-23 04:15:43,40
Intel,k1vaqdn,"Yeah, itâ€™s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and itâ€™s not particularly close.",AMD,2023-09-23 16:09:02,11
Intel,k1tf4z4,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,AMD,2023-09-23 05:13:17,3
Intel,k1tt5y9,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",AMD,2023-09-23 07:59:01,18
Intel,k1w7xof,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",AMD,2023-09-23 19:37:48,4
Intel,k1tyzar,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,AMD,2023-09-23 09:14:51,2
Intel,k1v3dv1,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",AMD,2023-09-23 15:22:12,2
Intel,k1uaqi2,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,AMD,2023-09-23 11:37:22,6
Intel,k1uke2m,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,AMD,2023-09-23 13:05:30,2
Intel,k1udkaf,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:05:15,4
Intel,k1tl1au,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",AMD,2023-09-23 06:19:42,26
Intel,k1t9pys,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",AMD,2023-09-23 04:17:58,17
Intel,k1u3wuq,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",AMD,2023-09-23 10:18:38,1
Intel,k1sx1p7,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",AMD,2023-09-23 02:27:32,40
Intel,k1t81d2,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,AMD,2023-09-23 04:01:49,3
Intel,k1szqei,"RT looks significantly better than raster when itâ€™s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. Thatâ€™s why devs love RT and want it to grow, as RT/hardware does the work for them.",AMD,2023-09-23 02:49:05,13
Intel,k1syew7,It's significantly better than raster. It kills fps but the quality is great,AMD,2023-09-23 02:38:21,8
Intel,k1ug9ty,Better graphics needing more expensive hardware is hardly a hot take.,AMD,2023-09-23 12:30:24,17
Intel,k1u52xg,"Yes, better graphics costs performance. SHOCKING",AMD,2023-09-23 10:33:12,14
Intel,k1t1ej8,People do it!,AMD,2023-09-23 03:02:50,7
Intel,k1tqio6,Itâ€™s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly itâ€™s insane.,AMD,2023-09-23 07:25:19,12
Intel,k1t7tj4,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",AMD,2023-09-23 03:59:48,4
Intel,k1sx4o4,Sounds like most nvidia fanboys,AMD,2023-09-23 02:28:11,14
Intel,k1u2qx4,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",AMD,2023-09-23 10:03:52,1
Intel,k1u3no0,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,AMD,2023-09-23 10:15:26,-1
Intel,k1y7ifv,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,AMD,2023-09-24 04:12:33,0
Intel,k1ss6lq,What? I thought amd was always more tuned to raster as opposed to reflections,AMD,2023-09-23 01:49:57,2
Intel,k1staby,Which is why nvidia is rabidly chasing AI hacks,AMD,2023-09-23 01:58:25,18
Intel,k1sv3ph,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,AMD,2023-09-23 02:12:20,3
Intel,k1sm72w,Say that to the people playing upscaled games at 4k (540p) on PS5.,AMD,2023-09-23 01:04:07,-2
Intel,k1s6hg3,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",AMD,2023-09-22 23:10:00,31
Intel,k1s7ntd,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,AMD,2023-09-22 23:18:28,10
Intel,k1slxqu,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",AMD,2023-09-23 01:02:10,13
Intel,k1s73qo,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",AMD,2023-09-22 23:14:27,-14
Intel,k1ttfgf,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,AMD,2023-09-23 08:02:21,11
Intel,k1tby96,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,AMD,2023-09-23 04:39:58,2
Intel,k1sb10i,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",AMD,2023-09-22 23:42:52,14
Intel,k1sf5sr,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",AMD,2023-09-23 00:12:51,2
Intel,k1u66ws,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),AMD,2023-09-23 10:46:42,5
Intel,k1t46jg,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",AMD,2023-09-23 03:26:44,-1
Intel,k1u6zio,You can already have 120fps on $1000 PC,AMD,2023-09-23 10:56:12,9
Intel,k1vnjmf,It improves both looks and fps so it is a win win,AMD,2023-09-23 17:29:44,0
Intel,k1scklw,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",AMD,2023-09-22 23:53:55,7
Intel,k1s9sl2,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,AMD,2023-09-22 23:33:53,4
Intel,k1s6x1t,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,AMD,2023-09-22 23:13:07,5
Intel,k1s97ev,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",AMD,2023-09-22 23:29:38,1
Intel,k1s30i9,It absolutely is. Iâ€™ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS Iâ€™ve ever seen. Itâ€™s basically unusable.,AMD,2023-09-22 22:45:15,-8
Intel,k1s8eym,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,AMD,2023-09-22 23:23:56,-2
Intel,k1wd23h,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",AMD,2023-09-23 20:09:52,1
Intel,k1save0,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD cardâ€™s currently do not match Nvidiaâ€™s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",AMD,2023-09-22 23:41:42,5
Intel,k1sama1,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,AMD,2023-09-22 23:39:52,0
Intel,k1tastr,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",AMD,2023-09-23 04:28:29,20
Intel,k1tf175,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",AMD,2023-09-23 05:12:08,10
Intel,k1v57gq,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,AMD,2023-09-23 15:33:56,4
Intel,k1uehqr,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,AMD,2023-09-23 12:14:02,3
Intel,k1uvdfc,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,AMD,2023-09-23 14:28:22,3
Intel,k1th9tj,"> q9300  Yes, it was not a good bin. I was at 4.0GHz with my Q9550, with just a copious amount of voltage.",AMD,2023-09-23 05:36:54,2
Intel,k1uvhzl,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",AMD,2023-09-23 14:29:15,2
Intel,k1vxcz9,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",AMD,2023-09-23 18:31:17,2
Intel,k1u94ng,moving data between the two is the issue,AMD,2023-09-23 11:20:15,11
Intel,k1ud2dh,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:00:31,9
Intel,k1tzket,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",AMD,2023-09-23 09:22:31,2
Intel,k1tcnbk,Game physics doesn't seem to be a focus anymore though,AMD,2023-09-23 04:47:04,17
Intel,k1vx974,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,AMD,2023-09-23 18:30:38,1
Intel,k1y74dq,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,AMD,2023-09-24 04:08:48,0
Intel,k1zf2ze,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",AMD,2023-09-24 12:27:53,0
Intel,k1u0kty,"Yes, but not after Nvidia bought Ageia.",AMD,2023-09-23 09:35:56,3
Intel,k1y5xsz,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",AMD,2023-09-24 03:57:42,1
Intel,k1usgsa,I like how you said: static image  because in motion upscaling is crappier,AMD,2023-09-23 14:07:47,0
Intel,k1t97dw,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",AMD,2023-09-23 04:13:02,-3
Intel,k1t7en2,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",AMD,2023-09-23 03:55:55,4
Intel,k1t22gi,">Thatâ€™s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",AMD,2023-09-23 03:08:23,2
Intel,k1u3zfu,"It *is* way slower, you're just compensating it by reducing render resolution a ton",AMD,2023-09-23 10:19:33,-9
Intel,k1ubnzq,If it was bloodborne i am guilty of that myself,AMD,2023-09-23 11:46:49,6
Intel,k1uerlc,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2023-09-23 12:16:37,6
Intel,k1y7wcn,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you canâ€™t even hold 10. Guess Iâ€™ll shill some more to at least not be spoon fed shit from amd,AMD,2023-09-24 04:16:20,1
Intel,k1tarlv,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",AMD,2023-09-23 04:28:09,5
Intel,k1tfawo,Rasterisation is a â€œhackâ€ too,AMD,2023-09-23 05:15:07,39
Intel,k1sxetf,If it works it works....computer graphics has always been about approximation,AMD,2023-09-23 02:30:22,36
Intel,k1t8lwj,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",AMD,2023-09-23 04:07:17,19
Intel,k1tqzxo,"I guess on the internet, you can just lie. Thereâ€™s no game on ps5 that upscale from 540p to 4k.",AMD,2023-09-23 07:31:22,4
Intel,k1vlzcm,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",AMD,2023-09-23 17:20:00,0
Intel,k1s7up5,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,AMD,2023-09-22 23:19:50,-11
Intel,k1sony7,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",AMD,2023-09-23 01:22:48,-7
Intel,k1sot43,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,AMD,2023-09-23 01:23:54,-5
Intel,k1s8jx2,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,AMD,2023-09-22 23:24:55,4
Intel,k1ubk56,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",AMD,2023-09-23 11:45:47,2
Intel,k1snovf,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",AMD,2023-09-23 01:15:25,3
Intel,k1sd914,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,AMD,2023-09-22 23:58:55,8
Intel,k1ubcdm,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",AMD,2023-09-23 11:43:38,-3
Intel,k1tno35,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",AMD,2023-09-23 06:50:58,10
Intel,k1ucpbc,Get that fps with a 5120*1440 240Hz monitor,AMD,2023-09-23 11:56:59,1
Intel,k1u928y,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",AMD,2023-09-23 11:19:32,-4
Intel,k1vo11q,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",AMD,2023-09-23 17:32:45,1
Intel,k1s3igv,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,AMD,2023-09-22 22:48:46,5
Intel,k1ugb6v,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",AMD,2023-09-23 12:30:45,1
Intel,k1wecws,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",AMD,2023-09-23 20:18:14,1
Intel,k1sbp25,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then itâ€™s a gimmick. IMO,AMD,2023-09-22 23:47:34,-6
Intel,k1sj47l,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",AMD,2023-09-23 00:41:39,-6
Intel,k1tbdg8,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,AMD,2023-09-23 04:34:12,7
Intel,k1tgkh1,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",AMD,2023-09-23 05:29:06,3
Intel,k1ueofm,8600gt in SLI man you are Savage!ðŸ”¥,AMD,2023-09-23 12:15:48,2
Intel,k1v8q15,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",AMD,2023-09-23 15:56:19,4
Intel,k20zqc9,there thereâ€™s no way there will ever be another 8800 GT. you got so much for your money.,AMD,2023-09-24 18:32:27,3
Intel,k1v3f8y,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",AMD,2023-09-23 15:22:27,2
Intel,k1vqgag,Seemed like a perfect use case for the sli bridge they got rid of.,AMD,2023-09-23 17:48:01,3
Intel,k1y719d,Why would it need to send the data to the other card? They both feed into the same game.,AMD,2023-09-24 04:07:58,2
Intel,k1uqeyb,Big up the Vega gang,AMD,2023-09-23 13:52:34,3
Intel,k1tgpce,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",AMD,2023-09-23 05:30:35,38
Intel,k1u8rwh,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",AMD,2023-09-23 11:16:24,9
Intel,k1tffr8,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,AMD,2023-09-23 05:16:37,16
Intel,k1toby8,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,AMD,2023-09-23 06:59:04,3
Intel,k1w7uac,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that Iâ€™d struggle to recommend their products even if they *were* competitively priced.",AMD,2023-09-23 19:37:13,2
Intel,k1thrmi,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",AMD,2023-09-23 05:42:24,4
Intel,k1t66vz,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",AMD,2023-09-23 03:44:48,3
Intel,k1ueiu5,We are guilty of the exact same sin.,AMD,2023-09-23 12:14:20,2
Intel,k1tzi21,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-09-23 09:21:41,0
Intel,k1u5aaz,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",AMD,2023-09-23 10:35:43,3
Intel,k1u0whe,would you care to explain ? Kinda interested to here this,AMD,2023-09-23 09:40:08,1
Intel,k1scoi2,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,AMD,2023-09-22 23:54:43,8
Intel,k1tcqbd,"It isnâ€™t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someoneâ€™s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",AMD,2023-09-23 04:47:55,6
Intel,k256x6l,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,AMD,2023-09-25 14:57:47,2
Intel,k1s8ne8,Turn on path tracing. Embrace the PowerPoint.,AMD,2023-09-22 23:25:36,3
Intel,k1ssjdy,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,AMD,2023-09-23 01:52:37,9
Intel,k1vpcr3,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",AMD,2023-09-23 17:41:03,0
Intel,k1s8f3n,"I tried 1080p DLSS Quality and it was just as bad, but Iâ€™m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didnâ€™t want to use it.",AMD,2023-09-22 23:23:57,-3
Intel,k1wloml,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",AMD,2023-09-23 21:03:39,1
Intel,k1tpscr,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",AMD,2023-09-23 07:16:33,6
Intel,k1sen3q,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldnâ€™t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it canâ€™t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. Youâ€™re in denial if you think otherwise.  Iâ€™m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You donâ€™t care about those things? Thatâ€™s fine, but it is solely a personal opinion.",AMD,2023-09-23 00:09:03,4
Intel,k1sdc8m,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",AMD,2023-09-22 23:59:34,1
Intel,k1tcqfl,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",AMD,2023-09-23 04:47:57,1
Intel,k1ttjgv,Shame you don't. They did it for a reason.,AMD,2023-09-23 08:03:50,2
Intel,k1v42qp,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,AMD,2023-09-23 15:26:42,3
Intel,k1wmal2,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",AMD,2023-09-23 21:07:26,2
Intel,k1w2bdg,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,AMD,2023-09-23 19:02:32,4
Intel,k1wmeoj,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",AMD,2023-09-23 21:08:09,0
Intel,k1zjknf,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",AMD,2023-09-24 13:05:06,0
Intel,k1ue2iz,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,AMD,2023-09-23 12:10:02,3
Intel,k1tkit9,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",AMD,2023-09-23 06:13:48,5
Intel,k1t75sa,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",AMD,2023-09-23 03:53:39,4
Intel,k1v732r,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",AMD,2023-09-23 15:45:58,7
Intel,k1ub5y8,All lights in a rasterised scene are â€œfakeâ€.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,AMD,2023-09-23 11:41:49,12
Intel,k1t4gtg,Or a bag of fake tricks.,AMD,2023-09-23 03:29:16,13
Intel,k1u716i,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",AMD,2023-09-23 10:56:45,0
Intel,k1tnk4a,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",AMD,2023-09-23 06:49:39,0
Intel,k1szwwn,Have you tried Metro Exodus Enhanced?,AMD,2023-09-23 02:50:34,6
Intel,k1ttno1,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,AMD,2023-09-23 08:05:18,3
Intel,k1wlywq,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",AMD,2023-09-23 21:05:25,1
Intel,k1u602a,What is this? A reasonable comment in this dumpster fire of a sub?,AMD,2023-09-23 10:44:29,3
Intel,k1sg0o3,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,AMD,2023-09-23 00:19:01,2
Intel,k1tgd2j,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",AMD,2023-09-23 05:26:46,3
Intel,k1v2w6l,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",AMD,2023-09-23 15:18:58,3
Intel,k1wpc9k,Except it was just a rebadged 8800GTX/Ultra,AMD,2023-09-23 21:27:07,2
Intel,k1wso05,"I mean, the Nintendo Switch is using Nvidia graphics, albeit itâ€™s a tiny 28nm iGPU from 2014â€¦ but I get your point. Theyâ€™ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and Iâ€™m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",AMD,2023-09-23 21:49:06,1
Intel,k1tlekn,Pixel art go brr,AMD,2023-09-23 06:23:58,2
Intel,k1uefox,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,AMD,2023-09-23 12:13:29,3
Intel,k1tws4k,"Yeah, I agree. Itâ€™s just a shame because itâ€™s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",AMD,2023-09-23 08:46:03,0
Intel,k1shejh,"It just doesnâ€™t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and itâ€™s weird to think the game shouldnâ€™t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",AMD,2023-09-23 00:29:11,2
Intel,k1tykau,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",AMD,2023-09-23 09:09:21,2
Intel,jws0ze9,"Speaking of the AMD overlay, am I the only one that gets near zero CPU utilization every time they pull it up? GPU utilization fluctuates up and down depending on resolution and graphical settings, so it seems to be fine, but my CPU utilization readings never go above more than a few percent tops. I have a 5800X3D.",AMD,2023-08-18 21:33:20,72
Intel,jwrrxaq,GN has already made a [video about it.](https://www.youtube.com/watch?v=5hAy5V91Hr4),AMD,2023-08-18 20:34:56,53
Intel,jwtujwf,Honestly this is everything that I wished Adrenalin overlay had. It's pleasantly lightweight too.   Hats off to Intel's development team. They didn't need to publish this at all.,AMD,2023-08-19 06:46:11,12
Intel,jws0ogv,I wonder if this will get added to Mangohud and Gamescope.,AMD,2023-08-18 21:31:17,10
Intel,jws656f,This is quality. Great work.,AMD,2023-08-18 22:08:17,7
Intel,jwtl7yn,Doesnâ€™t capframeX uses presentmon as its monitoring tool?,AMD,2023-08-19 04:58:08,4
Intel,jwse5d4,I can finally see if it really is the ENB taking down my Skyrim gamesaves,AMD,2023-08-18 23:05:28,3
Intel,jwt3rjk,"FYI, the open-source version has been available for years, was last updated 9 months ago, and does not contain an overlay or GPU busy stats. (It also has version number 1.8 while this new thing is v0.5)  Maybe they're planning to release source code later, but right now the source is closed.",AMD,2023-08-19 02:18:52,8
Intel,jwv7vh1,This is pretty damn awesome and will be incredibly useful when they develop the future features Tom mentioned. (insightful metrics),AMD,2023-08-19 15:06:20,2
Intel,jwx068e,"This is not bad at all. But it's not replacing Radeon overlay for me... yet.  1. Needs an option to minimize to system tray.  2. Many important Ryzen CPU metrics (temperature, power consumption etc.) aren't available without an ability to access Ryzen Master SDK. 3. Radeon Overlay can continue to monitor on the desktop, which I use constantly. I tried manually hooking DWM.exe and it won't. I might just be dumb, but I couldn't get it the overlay working without 3D running. 4. Will give credit where credit is due, unlike Radeon overlay, it works right with OpenGL. 5. And the credit is immediately removed, because unlike Radeon Overlay, it doesn't work right with DXVK.  6 out of 10.",AMD,2023-08-19 21:46:34,2
Intel,jx6nzqo,This is so good. The only thing I dislike is that there isn't a way to scale the whole thing up an down.,AMD,2023-08-21 21:00:38,2
Intel,jwt26ko,"At this rate Intel will have a more polished driver and software stack than AMD in 2 or 3 years, if not before.  Seriously, I suck for being able to buy an AMD GPU like the 6990 I owned, but I can't get near any hardware they made while using my system for work.  I need a CUDA alternative that works on more than 6 GPUs on specific linux distributions when the red moon shines above my building.",AMD,2023-08-19 02:06:15,6
Intel,jwvd88w,it's sort of misleading to call it opensource    when whatever intel beta version released is just MSI installer and binaries inside ...    i see no source of what is available for download ...   what's on gihub is something old w/o overlay,AMD,2023-08-19 15:41:34,4
Intel,jwsaw5e,Thanks Intel! I will try this out at least since I hate MSI afterburner.,AMD,2023-08-18 22:42:00,3
Intel,jwss1oh,The download link gives a warning in Windows:  >This site has been reported as unsafe   >   >Hosted by intelpresentmon.s3.amazonaws.com   >   >Microsoft recommends you don't continue to this site. It has been reported to Microsoft for containing harmful programs that may try to steal personal or financial information.  ???,AMD,2023-08-19 00:48:31,2
Intel,jwsaaac,"I really wanted to test with it why I was getting stuttering in Apex. Unfortunately, due to some work, I would be possible on Sunday.",AMD,2023-08-18 22:37:37,-1
Intel,jwteu8t,Doesnâ€™t work for me. It crashed at the start with an error message and made Dolphin run way worse.,AMD,2023-08-19 03:54:41,0
Intel,jwscc3e,And AmD gives far more than Nvidia.,AMD,2023-08-18 22:52:16,-11
Intel,jwwf6wi,I can't get this to work. Is anyone having the same issue?  CPU Temperature (Avg) NA C  CPU: 5800X3D,AMD,2023-08-19 19:37:09,1
Intel,k25hh4a,I have a 5800x3D and PresentMon does not recognize it. Says UNKNOWN\_CPU.  Anyone?,AMD,2023-09-25 16:00:53,1
Intel,jwsaffm,People have reported that cpu usage in Radeon software is not very accurate.,AMD,2023-08-18 22:38:38,35
Intel,jwsejpm,I saved this because it is posted so frequently lol  https://www.reddit.com/r/AMDHelp/comments/14n55gd/cpu_usage_percentages_differ_between_task_manager/jq66gop?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button,AMD,2023-08-18 23:08:23,6
Intel,jwsu2it,"Probably overlay wasn't updated for W11 22H2.  Microsoft updated something and CPU reading was broken for MSI Afterburner also, after working the same for a decade or more before. They fixed it in the meantime.",AMD,2023-08-19 01:03:33,3
Intel,jwubtsm,"I had the same problem, googled around and found the problem to be c state in the bios. Just turn that off and it should be accurate.",AMD,2023-08-19 10:34:05,2
Intel,jws5mkd,Just use afterburner as OSD.,AMD,2023-08-18 22:04:42,2
Intel,jwtvx7w,I had that issue with my 5600X both with Nvidia and AMD overlay. More recently it randomly fixed itself.,AMD,2023-08-19 07:03:38,1
Intel,k3rohbn,"Yeah adrenalin was useless for it. It was displaying 1-2% usage for me, but msi afterburner, hwinfo, and rtss were showing much more, like 30-50, which made more sense.",AMD,2023-10-06 20:51:17,1
Intel,jwrzqfw,You beat me to it :),AMD,2023-08-18 21:25:03,5
Intel,jx08btc,"Yup, ""new Intel"" definitely seems to be getting nicer and with a ""younger"" company culture. Also, it's a smart move. They're building a nice little ecosystem around Arc cards, which is eventually what is going to drive sales when performance and stability matches NVidia and AMD.",AMD,2023-08-20 15:17:16,8
Intel,jwu03xe,And it's open source!  I'm liking Intel more and more since they're getting their ass kicked.  Apparently it was one guy's pet project.,AMD,2023-08-19 07:58:51,6
Intel,jx8l1n5,Technically it should be possible to add in MSI afterburner because it's open source,AMD,2023-08-22 06:01:19,1
Intel,jwtnlcl,"The new PresentMon shows more information.  Since it's open source, capframeX can implement this as well.",AMD,2023-08-19 05:23:42,3
Intel,jwyec42,It was a pet project of one of the Intel engineers.   6/10 is not bad!,AMD,2023-08-20 03:57:36,4
Intel,jx6uru1,It's still beta. I'm sure they'll fix it.  Intel has amazing engineers.,AMD,2023-08-21 21:43:19,1
Intel,jwtopeu,Intel is working on their 2nd generation GPU and it's said to compete with Nvidia highest tier.  And now AMD isn't planning to release a big RDNA4 chip.,AMD,2023-08-19 05:36:25,5
Intel,jwus2bx,"didn't nvn get merged with mesa recently edit: sorry, I mean NVK not NVN",AMD,2023-08-19 13:12:58,2
Intel,jwwr3eh,I hate afterburner and RTSS. This is way better,AMD,2023-08-19 20:49:19,1
Intel,jwst65i,"people probably spammed microsoft because this PresentMon also gives you the option of enabling amd, nvidia, or intel telemetry.",AMD,2023-08-19 00:56:51,8
Intel,jwtohd3,"Thank you for contributing exactly NOTHING to this discussion.  Truly, the world is a better place because thanks to you.",AMD,2023-08-19 05:33:51,-9
Intel,jwt37au,"Tell that to anyone that needs CUDA. Or GPU accelerated workloads in general.  Id suck for being able to get something like the 6990 I owned back then, but between lack of support for almost all workloads and the driver being shit since like forever... Nope.  Yes, yes, nvidia's control panel is old and looks like a windows 98 app. But it at least works and actually saves youre settings, unlike AMD's one that half the time simply forget about everything.  I currently own 5 systems. 2 nvidia ones, 3 amd ones. And all of the amd ones had driver related issues.  You can't simply work with that. Is not viable. And as a gamer you should not need to see if the drivers are working or not.  Fuck, amernime drivers exists just because how hard amd sucks at it.",AMD,2023-08-19 02:14:23,2
Intel,jwsd0ai,"Well, everyone uses RTSS anyway and it gives you basically everything.",AMD,2023-08-18 22:57:07,1
Intel,jx7hc39,Similar issue here with an i5 12600kf. Can't get temperature readings from it. I'll need to look at it more.,AMD,2023-08-22 00:20:25,1
Intel,jwscka1,"Really? Good to know it's not just me, then. Nonetheless, this seems like a pretty egregious oversight on AMD's part. If I'm not mistaken, CPU metrics are on by default in the overlay and every time someone switches it on, it's an extremely visible reminder of work that AMD still needs to put into the drivers.",AMD,2023-08-18 22:53:52,9
Intel,jwxcu6w,Speaking of said software why does it keep setting gpu max frequency to 1200MHZ?,AMD,2023-08-19 23:14:33,1
Intel,jwsf4i1,"Thanks for sharing your findings. It's pretty damning if even the Windows Task Manager does a markedly better job than Adrenalin. That's probably an understatement since, as I found out via other helpful Redditors here, the Adrenalin overlay may as well be broken. I'm just glad it's not something wrong with my system.",AMD,2023-08-18 23:12:39,3
Intel,jwsnfu5,Afterburner fucks with my settings in adrenaline,AMD,2023-08-19 00:13:59,5
Intel,jwu3toe,"NVIDIA's highest tier? Nah, but around RTX 4080 is the target, which might be bad news for AMD.",AMD,2023-08-19 08:48:58,5
Intel,jwv9n3n,"By NVN do you refer to Nintendo's API for nvidia? Or you wanted to refer to NVK that got into mesa?  By CUDA alternative I mean stuff like ROCm. CUDA have so many years of development on top of it and a stupidly widely support from software vendors.  While yes, NVK could help to alleviate the issue, is not even close to what native CUDA can do regarding GPU usage for general computing.  The issue with ROCm is mainly a lack of support on a lot of software and lack of support for a lot of hardware.  On the nvidia end you can get the cheapest trashiest GPU and it will have the same level of CUDA support as the most expensive enterprise product.  Yes, performance is not the same, but I can use a 3090 Ti for loads that needs more than 12 GB vram without needing to buy a dedicated AI GPU, and if a given task needs serious shit, then again, I can buy a dedicated solution that will run the exact same software with the exact same code I ran on the ""weak"" 3090 Ti.  That is not possible with AMD, if you buy an enterprise grade GPU from them, you need to build all your software stack for it instead of just moving it and keep going.  And while yes, developers from tools like PyTorch COULD improve their ROCm support, I totally get why they are not doing that. ROCm works on like what? 7 GPUs? On specific linux distros?  Is a matter of scalability and costs. ROCm needs to be supported on ALL AMD GPUs in order for it to be something worth developing for, otherwise there is not really a use case where the end user wont be writing their own stack.  And again, if you can start small, then scale up, nvidia is the only option, so AMD on this space is either ""I got a stupidly great offer where AMD provides the same hardware power and performance per watt at like half the prize so I justify building all my software from scratches"" or ""Ok, I'll go for nvidia and just use all the software already built for it and call it a day""  I really, really want to being able to use AMD GPUs aside of trivial tasks, but this is a very, VERY gigante issue that appears a lot, especially on the profesional space.",AMD,2023-08-19 15:18:05,3
Intel,jxi33zm,Except PresentMon seems to not be compatible with Ryzen CPUs at all... :/,AMD,2023-08-24 02:22:09,1
Intel,jwsus0t,I get downvoted for asking a valid question?,AMD,2023-08-19 01:08:51,3
Intel,jwtt73f,Thank you for continuing to contribute Nothing to this conversation.,AMD,2023-08-19 06:29:18,9
Intel,jwslkce,Clearly not more than this beta of presentmon,AMD,2023-08-18 23:59:56,1
Intel,jwsd4sf,I use amds for overclocking and that's about it really. But it's still far more tools than nivida gives,AMD,2023-08-18 22:58:01,-2
Intel,jwsfkyz,"To defend AMD here as loathe as I am lately. CPU utilization isn't a very useful metric to begin with, so it misreporting would be super low priority.  The stat tells you nothing useful about the hardware, just the thread scheduling. CPU could be stuck in wait and reporting a high number while doing no work or it could be bottlenecking somewhere else while reporting a low %.",AMD,2023-08-18 23:16:00,11
Intel,jwxd4ky,Where are you seeing this?,AMD,2023-08-19 23:16:32,1
Intel,jwsfr15,"The Adrenalin overlay is good for really everything except usage. But yeah, this has seemed to have been a problem for a good while now. Ive been using AMD for a little over a year now at least Adrenalin and the CPU usage was always wrong on two different CPUs (AM4 and AM5) as well. Me and that other person tested it on my system and he had 3 systems running different AMD CPUs and all were the same. Ill find the thread one day but its been a while so it is way down the list of my comments.",AMD,2023-08-18 23:17:13,1
Intel,jwsno08,"False setup.  Disable the button in the ui ( not settings)  For ""load on startup""  You literarily told afterburner to load it's settings on start up.  Huge mistake many people do because most people think it's ""boot on startup"" but in reality it's loading it's oc settings and stuff effectively overwriting adrenaline if you leave that enabled.",AMD,2023-08-19 00:15:40,14
Intel,jwuero7,3070 Also was their target for their first generation. I don't expect Intel to suddenly be great just because that's what they aim for.,AMD,2023-08-19 11:08:44,4
Intel,jwu8j4q,And XeSS 1.1 looks better than FSR.  XeSS 1.2 was just released and it has even more improvements.,AMD,2023-08-19 09:51:11,3
Intel,jwv8abu,Great news to gamers though,AMD,2023-08-19 15:09:05,2
Intel,jwvbwid,ah sorry I meant NVK,AMD,2023-08-19 15:32:34,1
Intel,jwsuuv1,"I don't know, I didn't downvote you.",AMD,2023-08-19 01:09:27,6
Intel,jwsec7c,My point is if you've got nvidia card you simply use 3rd party soft and got everything and more. So comparing that AMD's software to Nvidia's is kind of meaningless.,AMD,2023-08-18 23:06:52,5
Intel,jwt2g8b,"I actually agree with you, but I don't need the metric to be some scientifically accurate reading of performance or whatnot, I just think it'd be a useful tool for adjusting game settings. For example, if I'm trying to optimize performance in a game, does this setting make CPU utilization go up or down? How much almost doesn't matter. Is there a bottleneck, where is it, and am I relieving pressure on it with these settings or not?",AMD,2023-08-19 02:08:23,3
Intel,jwxvkn7,Itâ€™s where you oc in adrenaline,AMD,2023-08-20 01:27:06,1
Intel,jwuly2q,"There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Not because upscaling, but also because how important is to have drivers that work in tandem with specialized hardware units in the GPU.  Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  AMD fails hard on 2 of those things.  Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.",AMD,2023-08-19 12:21:17,5
Intel,jwv200x,"Yep, XeSS is just great, I happily use it in MWII on my Arc A770. But I still use DLSS on NVIDIA. But both are better than FSR.",AMD,2023-08-19 14:26:34,3
Intel,jwtzdkr,"Then you should be looking at ***GPU*** usage, because CPU usage will not help you answer any of those questions",AMD,2023-08-19 07:49:04,6
Intel,jwuvpd6,"Problem is, by itself it still tells you little to nothing. If GPU utilization drops like a brick you know something is bottlenecking. If CPU utilization changes in the absence of other data you know absolutely nothing of value.   The real kicker is the CPU utilization may not even be ""wrong"", it may just be polling at a very low rate because that's the other thing with these OSDs if you poll too frequently for up-to-date data that can kill performance from the overhead.",AMD,2023-08-19 13:41:23,2
Intel,jwv1po6,"> There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Agreed, not to mention that when they made Alchemist, yes their target was 3070 and they underperformed that, but they will eventually hit their targets as they learn to improve their architectures. For a first shot, getting to around 85-90% of their target is actually quite good. It's not like they undershot by 50% of the target. They got close.   But like you said, software is super important and these days having good software is half of a driver. I have an A770, the driver actually pretty good, like the software menu. It used to suck when it first came out because it was some crap overlay, but over time it's become a separate program and it works really good considering they're not even a year into Arc's release. They also have a good set of monitoring tools built in and the options are all simplified. They don't have super unnecessary things like a browser integrated into the driver suite, or a bunch of other stuff. I'm sure by the time Celestial is around if Intel does stick with Arc, that they will be ahead of AMD on the software game, assuming AMD continues down the same path they have now.  > Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  Yep, tools like [this one from NVIDIA](https://youtu.be/-IK3gVeFP4s?t=51) have made it easier than ever as a dev to optimise for NVIDIA hardware.  > AMD fails hard on 2 of those things. Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.  Yep because Intel recognises that to get close to NVIDIA they not only have to make great hardware, but also great software to go along with it. You can only do so much with hardware to match NVIDIA. You have to make great software along with in-game solutions like XeSS to match NVIDIA. FSR is behind not only DLSS but now also XeSS. AMD is just not competitive with NVIDIA on all fronts. Arc is just getting started, give Intel 5-10 years of time in GPU and they will outpace AMD for sure.",AMD,2023-08-19 14:24:33,6
Intel,jwuapdo,"There are many settings that directly impact CPU usage, so of course CPU utilization reading can be useful.  Not everything needs to be precisely accurate, this is a lousy defense. Getting 0% reads all the time is entirely worthless but some accurate reads is still useful information.",AMD,2023-08-19 10:19:39,2
Intel,jx3nm3i,"You are going out of your way to defend a broken feature.    7800X3D shows up as 1% CPU utilization during games such as Watch Dogs, Stellaris, Age of Empires. Clearly broken. Undefendable.",AMD,2023-08-21 06:56:43,1
Intel,jwv7qel,"And with a bit of luck, they will also hit AMD in the console market.  Right now AMD stays afloat GPU speaking because consoles, they were during a lot of years the only vendor offering both CPU and GPU in a single package, reducing costs A LOT.  Nvidia was not able to compete against that with the lack of CPUs and Intel with the lack of GPUs.  Now that Intel is working on dedicated GPUs they are going to eventually create APU like products, its just the natural outcome of producing both CPUs and GPUs.  And then AMD will NEED to compete against them instead of being the only option.  And if that happens, unless FSR gets turned into a hardware specific tech like XeSS that falls into different techs depending on the GPU its running on, AMD will be murdered.  Better software stack, better general hardware to accelerate and improve upscalling keeping costs down AND being a second player pushing prices down towards Sony and Microsoft?  AMD will be screwed up if they don't up their game. And hard.",AMD,2023-08-19 15:05:22,2
Intel,jwucdfz,"Right, so what kind of useful information can you tell me if I raise graphics settings and CPU utilization goes from 12-16% to 16-17% while framerate goes down from 120 fps to 110 fps?",AMD,2023-08-19 10:40:52,2
Intel,jwuwjmm,"All CPU utilization tells anyone is thread scheduling, and nothing about what the threads are or aren't doing. Like the only situation that it marginally tells someone anything is how many threads a program uses. But that usually doesn't fluctuate with games, games usually scale up to <x> number of threads and that is that. GPU utilization is the one that is far more useful for spotting bottlenecks, general performance observations are far more useful for tweaking.  Even Intel and co. will tell you that with modern processors and all the elements they contain as well as hyperthreading/SMT CPU utilization isn't a very useful stat anymore. Meant a whole lot more when CPUs were one core unit and didn't also include the FPU and memory controller and everything else.",AMD,2023-08-19 13:47:45,1
Intel,jx4di9f,"Cause unfortunately even if it wasn't broken, it's not useful in any meaningful capacity. Should it be broken? No. Would anything notable come of it if it were fixed? Also no. Given the bugs they got and room for improvement elsewhere it's one of those things that should be super low on the priority list or just removed wholesale.",AMD,2023-08-21 12:07:55,1
Intel,jwuithf,"That whatever setting you are changing does not have much of an impact on CPU utilization, and if your GPU allows, you can use it.  What is the purpose of this question, are you really not aware of any settings that impact CPU usage? Some RT settings have big impacts on CPU usage too.",AMD,2023-08-19 11:51:44,4
Intel,jwuq603,"The problem you're missing here is that settings that are CPU demanding generally don't have an easily observable impact on CPU usage, if they have any impact at all.  The reliable way to see if a setting is CPU demanding is to turn it up, see the frame rate go down, and then look at how **GPU** usage changed.",AMD,2023-08-19 12:57:40,3
Intel,jwuozk8,"I'm aware that some settingd impact CPU usage, but I think looking at the CPU usage is about as useful as your daily horoscope, or AIDA64 memory benchmark",AMD,2023-08-19 12:47:47,2
Intel,jwvi4ei,Congratulations on the new gaming laptop. Donâ€™t let people get you down and not everyone has to have the very best to be happy so if it makes you happy then Iâ€™m happy for you.,AMD,2023-08-19 16:13:46,24
Intel,jwwunye,"Congrats the 30 series isn't half bad. Laptops aren't bad but gaming desktops add more umph to their performance. My first gaming rig was a gaming desktop with 64gbs ddr4 3200mhz, rtx 3060 12gb, 850w platinum psu, b660m gigabyte motherboard and a intel i5 13600kf. Couldn't have had a better first rig.my current build now is a hellhound raedeon rx 7900xt,gigabyte  z790x ax gaming motherboard,  850w platinum psu, 64gbs ddr5 6000mhz,Samsung 980 1gb ssd, intel i5 13600kf(watercooled).best upgrade I made for 1440p gaming.",AMD,2023-08-19 21:10:15,2
Intel,jx1t2ia,"Congratz!  Also, what's that wallpaper? Looks awesome",AMD,2023-08-20 21:29:18,2
Intel,jwvupca,I don't understand the stickers. Ryzen CPU Check. Both Radeon and GTX graphics? Is one discrete and the other dedicated? Just needs an Intel sticker to make it more confusing.,AMD,2023-08-19 17:32:54,1
Intel,jwv4u8q,"That is not a ""gaming rig"" not even close my friend. ;)",AMD,2023-08-19 14:46:02,-10
Intel,jwvgjd6,3050ti?  Gaming rip?  No,AMD,2023-08-19 16:03:09,-5
Intel,jwwhi65,"5 or so years ago i built my first real gaming rig. Few years later i bought new laptop, because i already had a gaming pc i wanted something capable to game but didnt want to spend too much money for high end. So i bought Lenovo legion with same gpu - 3050ti.   What i soon realized was that i love the comfort of being able to browse the net and game from the couch, being able to put my legs up, put my back against a big pillow etc... Despite having much more capable pc, i was lazy to play on it and was using gaming laptop instead 90% of the time.   Problem is, 3050ti in the last year or so really starting to show its limitations, most of new games are straight up unplayable anymore. Even on lowest settings it doesnt run newest games at playable standards, probably because of only 4gb of vram. Hogwartz, Rachet and Clank, Remnant 2 all pretty much unplayable unless you drop resolution to 720... I really regret not putting few hundreds more for something like 3070. (unrelated, but why stepping up laptop gpu tiers are so expensive, for example 3060 to 3070 is like 300e here for essentially same system otherwise, 3070 to 3080 is like 4-500e extra more, its even more expensive overall than desktop gpu tiers for way lesser performance improvements...).   So my advice would be, if you can get at least 3060, its worth it to pay up a bit more, ideally 6600m or 4060, those few hundreds increase in price will be worth in a few years.  ALSO: not sure if OP knows, but these laptops come with shittiest ram, upgrading it to a dual channel dual rank is pretty much a must as it nets 10-20% more performance.",AMD,2023-08-19 19:51:52,1
Intel,jwxgjhs,There is nothing wrong with laptops I'd build my own if there was a market readily available for it like a desktop.,AMD,2023-08-19 23:39:57,1
Intel,jwz7u2u,No!,AMD,2023-08-20 09:40:51,1
Intel,jx2ecvu,"This isn't exactly what I'd imagine hearing the words ""gaming rig"" but I'm glad you like it. Happy gaming!",AMD,2023-08-21 00:03:02,1
Intel,jz3n7rj,Ignore the haters dude. Congratulations on your new gaming laptop. Gaming is something that is supposed to make us happy not cater to snobs. Not everyone needs 4k 60fps. If it satisfies your requirements it's the best rig for you. I have a Ryzen 5 2500u and I game on that so I am super happy for you.,AMD,2023-09-04 14:46:53,1
Intel,jwvkdlj,"People take things too seriously, and tend to forget that every enthusiast was once a novice.  This is OP's first gaming setup; they deserve to feel that excitement.",AMD,2023-08-19 16:28:13,9
Intel,jwvnufp,who even buys the 'very best' laptop anyway?,AMD,2023-08-19 16:50:14,2
Intel,jwvc7sf,Lenovo Ideapad Gaming 3,AMD,2023-08-19 15:34:40,3
Intel,jx1ui6e,"It's just the default one that came with the laptop, theres a red and a blue version probs from AMD and Intel lol.  You should be able find them online.",AMD,2023-08-20 21:39:12,2
Intel,jwvwcww,Its a ryzen cpu with integrated radeon graphics and an nvidia dedicated gpu. Its completely normal and not really confusing.,AMD,2023-08-19 17:43:04,9
Intel,jwwk8it,APU + dGPU,AMD,2023-08-19 20:08:00,1
Intel,jwvby00,"You know what I mean.   It's my very first PC/Laptop set up so I'm definitely calling it a rig, it's way more powerful than anything I've owed before it.",AMD,2023-08-19 15:32:51,14
Intel,jwv71nz,"It games, it's a gaming rig. :)",AMD,2023-08-19 15:00:53,11
Intel,jwvgp6w,you do realise not every wants or needs a 4070 to play games they like,AMD,2023-08-19 16:04:16,12
Intel,jwyz4ai,I upgraded the ram. Has 16gb ddr 5 dual channel now,AMD,2023-08-20 07:45:41,2
Intel,jwvm2xc,"Thanks dude! I think people forget that not everone needs the same kinda thing from a computer which is why they sell multiple models.  This is great for what I need it for, and I feel like a kid at Christmas, happy to be able to dip my toe into PC gaming, finally!",AMD,2023-08-19 16:39:11,10
Intel,jwvpsjs,You get my point at least you should,AMD,2023-08-19 17:02:04,4
Intel,jwvh0km,"Just to let you know , these laptops have different versions , one I have is something like Ar15mhqp or some shit like that   It is less powerful than yours but decent enough",AMD,2023-08-19 16:06:23,4
Intel,jwvc5tx,That is okay if you see that way and congrats.,AMD,2023-08-19 15:34:19,-5
Intel,jwv9ikl,"There's nothing ""rig"" about a laptop. Laptops aren't built to be rigged.",AMD,2023-08-19 15:17:14,-6
Intel,jwvboff,"No, it's a notebook.  Also have a low end gpu dude.  Not a game rig. It's nice but is not that.",AMD,2023-08-19 15:31:06,-7
Intel,jwvib92,"I know, but the same laptop or any other with 6800m or other AMD gpu would have been a much better choice lol",AMD,2023-08-19 16:15:00,-8
Intel,jwxg0r9,All that matters is that your happy with it,AMD,2023-08-19 23:36:26,3
Intel,jwvhehi,Glad youâ€™re enjoying it. People something think that unless you get something with everything maxed out then you couldnâ€™t be happy lol,AMD,2023-08-19 16:08:55,2
Intel,jwvcmin,"Yeah I'm just happy to be able to start playing around on some games on PC now. I've wanted to play flight simulator on pc instead of my xbox for ages now becuase of all the addons,and now i'll have access to those :D",AMD,2023-08-19 15:37:25,5
Intel,jwvc36g,"3050ti isn't really low end. And it will do for everything I need it for. I have an Xbox too for the most modern stuff.  People think that everyone has to have 4k 60fps in every game these days and not everyone needs or wants that, especially in a first machine.",AMD,2023-08-19 15:33:49,6
Intel,jwvlvjc,"for you maybe, this fits my needs perfectly.",AMD,2023-08-19 16:37:52,5
Intel,jwzgnvu,"Yup I am more than happy!  I've only ever had and played with integrated graphics on any other laptop I've had so this is a huge step for me.  I will play most brand new stuff on my Xbox for the best ecperience but stuff like KF2 I can now play a 1080p ultra and get 100s frames, could only play that before at 900p low.",AMD,2023-08-20 11:29:53,3
Intel,jwvho43,Well you say that right now but one day a game you really wanna play will come and your pc will be shitting itself trying to run it. If I had the money I would buy the best rig.   But if you donâ€™t have a lot of money like me itâ€™s all about price/performance,AMD,2023-08-19 16:10:41,0
Intel,jwvcxpz,That is the important that you are happy but still the reality is one.  Just enjoy the notebook.,AMD,2023-08-19 15:39:35,-6
Intel,jwwilm3,"not too similar but I can recommend grabbing dirt rally 2.0 on a steam sale with all DLC for 8â‚¬ / $10 or whatever it is, generally games that simulate some things are pretty much at home on PC",AMD,2023-08-19 19:58:19,1
Intel,jwvcs7g,"Yes, the 3050 ti is a low end card.",AMD,2023-08-19 15:38:30,3
Intel,jwvljzf,I have an Xbox too so I'll be able to play it on there anything that is out of spec.,AMD,2023-08-19 16:35:47,1
Intel,jww7sx0,I have a friend playing Baldurs Gate 3 on a 980/ You don't know what low end is apparently lmfao,AMD,2023-08-19 18:51:09,5
Intel,jwvcxzc,It's better than the 1650 and the 2050 though. And they were the other choices.,AMD,2023-08-19 15:39:38,3
Intel,jwwudqt,And that is normal the game don't requiere a big gpu to run the game is a mid / low graphics.  Besides the 980 was the top of that generation. The 3050 is the low end of that particular generation.,AMD,2023-08-19 21:08:34,1
Intel,jwvw154,"Dont let the haters bring you down dude, if you are happy, then enjoy your new laptop!",AMD,2023-08-19 17:41:04,5
Intel,jwwuzrc,"Both cards are the lowest version of his generation.  Again the notebook is nice but is a low end ""gaming"" notebook and again the point is that you ENJOY IT and play many games.",AMD,2023-08-19 21:12:17,2
Intel,jwwv6vy,He needs to know what he have but also I congrats OP for your purchase. The point is that you enjoy it.,AMD,2023-08-19 21:13:29,1
Intel,nbhfmxf,Not surprising  It has 16Xe Cores (equivalent to 32CU/SM)   14Gbps GDDR6 memory   ~2.3Ghz core clocks   B580 has:  20Xe cores   19Gbps memory  2850mhz core clocks,Intel,2025-08-30 12:53:00,33
Intel,nbibnhv,Itâ€™s a workstation so,Intel,2025-08-30 15:48:42,9
Intel,nbifzsy,This is a workstation card.  Its drivers won't be optimized for games -- they will be tuned for things like Autodesk Maya and AI.  This rumor is totally immaterial.,Intel,2025-08-30 16:10:25,16
Intel,nbhvq3x,That colorway is actually... Cool.,Intel,2025-08-30 14:27:05,4
Intel,nbivcsx,$90-$100 MSRP? ðŸ¤”,Intel,2025-08-30 17:26:25,1
Intel,nbmp556,hope this helps drive down the prices of LP cards on the used market,Intel,2025-08-31 08:18:54,1
Intel,nbhfu7f,Arc Pro? Who's the target audience? Doesn't look like a gaming oriented card to me.,Intel,2025-08-30 12:54:14,-8
Intel,nbhv3bn,"They're comparing it against the B570, not B580.",Intel,2025-08-30 14:23:37,18
Intel,nbj9s7i,And just 70W  https://www.intel.com/content/www/us/en/products/sku/242615/intel-arc-pro-b50-graphics/specifications.html,Intel,2025-08-30 18:38:55,15
Intel,nbnjk4g,Why do people who visit this sub assume everyone is a gamer?,Intel,2025-08-31 12:43:45,3
Intel,nbtglxd,"This used to be true decades ago (back then even the hardware was significantly different in some cases), but nowadays pro cards are quite similar. The ""special drivers"" usually just exist for certification purposes and are made from the same code as the gaming drivers. It's mostly product segmentation these days. So the pro cards tend to have more memory, sometimes have ECC memory, different form factors and different power/performance optimization. That's it.",Intel,2025-09-01 11:08:47,1
Intel,nbtjnsj,Doesnt really matter. This is just another cut down version of Battlemage. It will scale however they want to chop it up to. Like the Radeon Vega Frontier workstation  card came out and had mediocre performance so everyone outcried saying just wait for the gaming â€œoptimizedâ€ Radeon Vega 64. It was the same card and performed identical.,Intel,2025-09-01 11:32:51,1
Intel,nbn02nc,Which part of geekbench said it is a gaming benchmark to you?,Intel,2025-08-31 10:06:55,0
Intel,nbjak4a,MSRP for the Arc Pro B50 is $299.,Intel,2025-08-30 18:42:53,6
Intel,nbhgoj1,Professionals / AI freaks,Intel,2025-08-30 12:59:27,27
Intel,nbhgtgd,Low-end workstations and lab equipment. I had a similar low-end professional GPU installed on an [X-ray diffractometer.](https://www.malvernpanalytical.com/en/products/product-range/empyrean-range/empyrean),Intel,2025-08-30 13:00:18,12
Intel,nbiqh1e,The term â€œproâ€ is short for â€œprofessionalâ€,Intel,2025-08-30 17:02:31,9
Intel,nbrl67u,"Also depends on the price, if its under $300 then it's a pretty good deal as far as workstation cards go.",Intel,2025-09-01 01:52:35,2
Intel,nboqwvj,Because the vast majority of redditors aren't very smart people to put it mildly.,Intel,2025-08-31 16:33:07,3
Intel,nbp46we,"That's its typical usage.  But you're right, Geekbench is a shoddy cross platform bench, despite its typical use for gaming by bad reviewers.  That said, it is very unlikely that this card's drivers are tuned to Geekbench, unlike the mature drivers in the B570.",Intel,2025-08-31 17:37:36,1
Intel,n9is5y3,I bought my B580 2 weeks ago :(,Intel,2025-08-19 12:55:40,20
Intel,n9ign1k,"Good enough a reason to get my buddy a b570 for his birthday, might just pocket the game!",Intel,2025-08-19 11:43:46,44
Intel,n9iuewn,You gotta be joking. Literally got a 14600K for $150 a few days ago. :(,Intel,2025-08-19 13:08:28,10
Intel,n9mwntx,Hopefully this means BF6 will get an APO profile.  u/Aaron_McG_Official can you confirm?,Intel,2025-08-20 01:25:39,8
Intel,n9isjsk,I got Borderlands 4 with my 5090. Would rather trade it for this.,Intel,2025-08-19 12:57:54,12
Intel,n9ksl0i,Maybe its a good time for them to drop big battlemage on the market??,Intel,2025-08-19 18:47:47,3
Intel,n9nk7b7,I'm surprised hardware companies are still doing this. People can easily abuse this by getting the game for free and returning the product they bought. Well it works in places that have strong consumer protection laws.,Intel,2025-08-20 03:53:03,2
Intel,n9kclxi,I got assassin's creed shadows with my $140 Intel CPU.  Good deal indeed.  I like the AC games.,Intel,2025-08-19 17:31:46,1
Intel,n9pkqq4,"I bought my intel ultra 7 less than two weeks ago, damn",Intel,2025-08-20 13:36:32,1
Intel,n9zejrm,Deep thought stimulation.,Intel,2025-08-21 23:27:21,1
Intel,na0vyfw,"I wonder what retailers, will amazon be one of them?",Intel,2025-08-22 05:16:50,1
Intel,na35vt4,"That's a pretty sweet deal, especially for pc builders on a budget.",Intel,2025-08-22 15:15:53,1
Intel,na56wcq,Are latest Achemist/Battlemage GPUs faster than my 3080 at 4k?,Intel,2025-08-22 21:20:10,1
Intel,nam49mt,Is this live now?,Intel,2025-08-25 17:08:03,1
Intel,napztur,I bought 13700K on Jan 2023 can I get a serial as a compensate ?,Intel,2025-08-26 06:49:04,1
Intel,n9kg3eh,I bought it at the height of the voltage controversy when the fix wasn't even known. Nothing for me then? ðŸ˜,Intel,2025-08-19 17:47:56,1
Intel,n9k1uw4,Even if it's free it's not worth it.,Intel,2025-08-19 16:42:03,-4
Intel,n9hnrzl,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-08-19 07:21:54,0
Intel,n9hxgxo,Price scheme or they are that desperate.,Intel,2025-08-19 09:00:00,-31
Intel,n9ifdfr,Iâ€™ll do this deal when my stock sells ha,Intel,2025-08-19 11:34:44,-4
Intel,n9itlw5,Mine just got delivered on Friday tooâ€¦.,Intel,2025-08-19 13:03:56,10
Intel,nap9vbe,Return it and than rebuy it,Intel,2025-08-26 03:17:45,1
Intel,n9ljaij,"That's still a great deal, even without BF6!",Intel,2025-08-19 20:55:16,6
Intel,n9u7unw,Same here lol. I got one to upgrade my sisters build.,Intel,2025-08-21 04:28:01,3
Intel,na6f1ek,Canâ€™t return it?,Intel,2025-08-23 01:42:36,2
Intel,nabo7gy,Check you probably got the previous deal,Intel,2025-08-23 22:53:44,2
Intel,n9sex8a,"Per Intel policy, I can't comment on unreleased products. Historically, APO has tested the top superlative and popular titles; if the team gets them to show improvement then they are added.",Intel,2025-08-20 21:55:58,7
Intel,n9iyx6o,"That's fair. I can trade you a B580 with Battlefield 6 for a 5090, you can even keep the copy of Borderlands 4.",Intel,2025-08-19 13:33:12,34
Intel,n9m1uw7,"That would be amazing, but I doubt it.",Intel,2025-08-19 22:30:13,3
Intel,n9o9p5i,It will likely come in Q4 2025 or Q1 2026,Intel,2025-08-20 07:34:59,1
Intel,nbdcq2g,I have a buddy at bestbuy and asked if this would work and he told me that when you go and return the product the price of the game will be withheld from your refund if the code has been redeemed. He did also say that would happen when nvidia included a code for star wars outlaws and I had no problems getting a full refund with the game redeemed,Intel,2025-08-29 19:30:00,1
Intel,n9q85wa,Same man ðŸ˜…,Intel,2025-08-20 15:32:52,1
Intel,na9scx4,Nope. 3080 is faster by a long shot.,Intel,2025-08-23 16:41:37,1
Intel,namiblo,Yes,Intel,2025-08-25 18:15:05,1
Intel,n9i7nod,A megacorp that has almost 70% of CPU market share is so desperate that they're giving away a game with a purchase of one their CPU's. You're a genius.   I got Dying Light 2 with my old 12th gen.,Intel,2025-08-19 10:35:03,27
Intel,n9iqaui,"I remember getting FarCry 6 with my Ryzen purchase a few years ago. Right now looking at a local pc shop website, you can get Borderlands 4 with a 50 series gpu purchase.  I guess by your logic, both AMD are desperate for CPU sales and Nvidia is desperate for GPU sales ?",Intel,2025-08-19 12:44:55,5
Intel,n9lx4s5,If only there was something called a return period....,Intel,2025-08-19 22:04:36,7
Intel,n9iz1ed,How generous. But no thanks. ðŸ˜…,Intel,2025-08-19 13:33:51,6
Intel,n9juy35,"I'll up it to a 1070, or my current 3070.",Intel,2025-08-19 16:09:12,1
Intel,n9qirpi,So unlucky lol,Intel,2025-08-20 16:23:13,2
Intel,n9ibsnz,"These idiots also act like AMD never bundles games, I got Star Wars Jedi Survivor with my Ryzen 7 7700X.",Intel,2025-08-19 11:08:11,22
Intel,n9i9kig,"I got ""Dying Light: The Beast"" and ""Civilization VII"" as a gift from Intel with my Core Ultra 7 CPU.",Intel,2025-08-19 10:50:59,5
Intel,n9m4i3c,Let's not pretend Intel isn't in desperation mode right now. The company is in the worst shape they've been in 30 years.,Intel,2025-08-19 22:44:58,5
Intel,n9kgfth,"Intel: here kid, have a balloon   Kid: what's in it for *me?*",Intel,2025-08-19 17:49:33,3
Intel,n9jpkiv,"> A megacorp that has almost 70% of CPU market share  Since you mentioned your 12th gen, on steam it's under 60% intel now, an 11% reduction in intel cpus in just 1 year. On amazon, there's [0 intel cpus in the top 10 best sellers.](https://i.imgur.com/LGwqOZs.png) Anecdotally the only high end cpus I see recommended in gaming discords is 7800x3d/9800x3d.  Obviously intel still has the lion's share in oem/laptop/server but that's been falling rapidly too, despite intel illegally bribing oems to not use amd products. Apple ending their partnership+their own laptop chips have been chipping away at intel's share too.  One company's been rolling losses year after year with falling market share, one is the complete opposite, the writing was on the wall a long time ago.  It took amd the better part of a decade to claw back from bankruptcy, intel needs to stop their attitude of looking for short term gains if they want to even start a comeback.",Intel,2025-08-19 15:43:53,2
Intel,n9irl7k,"TBF, you shouldn't be looking at marketshare but percentage of new CPUs sold/ profit.  Marketshare just tells us intel was number 1 for a long time. Change in marketshare tells us they're losing ground fast.",Intel,2025-08-19 12:52:20,0
Intel,nalvlmx,Yep thatâ€™s exactly what I did since they wouldnâ€™t honor the promotion even though itâ€™s during the return period,Intel,2025-08-25 16:25:38,2
Intel,n9j6dj3,Well you said you would trade soâ€¦,Intel,2025-08-19 14:12:27,9
Intel,n9ijalk,AMD literally set up a store at one point to give away games and GPUs with the purchase of a bulldozer.,Intel,2025-08-19 12:01:34,4
Intel,n9jqz06,"What are you talking about. AMD has been on a brink of going extinct since the 90's with the exception of AMD64 (Clawhammer etc) days. 2005-2008 or thereabouts.  Second, every heard of anti-monopoly laws? It would be cheaper for AMD to pay from their own pocket to keep Intel afloat rather than let them go bankrupt. Why do you think AMD is still around after all these years. Intel will never fail.",Intel,2025-08-19 15:50:22,-3
Intel,n9jwxfq,"AMD's zen architecture was the company's final hail mary which is their last decade, ryzen being a literal play on words for risen (from the dead). Their own engineers admitted if it failed the company was going to go under.  >Second, every heard of anti-monopoly laws?  Same laws that intel has been consistently losing for decades?  >cheaper for AMD to pay from their own pocket to keep Intel afloat rather than let them go bankrupt.  That's not how anti trust/monopoly laws works. Company A going bankrupt because they kept shooting themselves in the foot doesn't mean that Company B automatically become subject to anti trust lawsuits. Nvidia isn't exactly paying AMD even when they hold 90-95% of the market. Legislative scrutiny tends to come from manipulative practices, not from just selling a better product.  Since you might bring it up, Microsoft propped up Apple because they wore forcing their own software through Windows, not because they were just simply selling Windows. In a twisted turn of fate, now Apple is going through anti trust lawsuits/legislation around the world despite holding a minority share of operating systems worldwide.",Intel,2025-08-19 16:18:36,4
Intel,n85cm1l,That is surprisingly affordable for how much vram it has. I expected close to 3.5K.,Intel,2025-08-11 17:51:17,15
Intel,n8567ex,"Yep, I totally need this for plex",Intel,2025-08-11 17:20:56,10
Intel,n86ka6v,Perfect for AI inference market Lip Bu Tan is targeting,Intel,2025-08-11 21:29:21,4
Intel,n890k9x,Would this be a competitor to the Framework Desktop?,Intel,2025-08-12 07:19:08,2
Intel,n8at2ae,You can now buy CORE 1 series cpus on desktop motherboards from ali.,Intel,2025-08-12 15:05:20,1
Intel,n8ikpoc,How did I not see this? Has anyone tested the B60? There seems to be extremely limited supply.  Maxsun does not have good support in the US?,Intel,2025-08-13 18:49:15,1
Intel,n886cmq,48GB Vram,Intel,2025-08-12 03:10:43,3
Intel,n8q7z45,It's just two B60 Pros. They're supposedly priced at $500 each if they'll ever actually be available for purchase.,Intel,2025-08-14 21:52:20,1
Intel,n4rtwgd,"Biggest thing in this update for me is finally they allowed iGPU memory allocation for Intel Core Ultra Series 1 & 2. This is huge!Â      I haven't tested this drivers on my MSI Claw yet, really excited to use this features because it can solve a lot of texture issue and low memory warning in the game where it only detect Arc integrated graphics with 128MB vram.",Intel,2025-07-23 19:51:12,17
Intel,n4uaxy8,"I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  (Will make an actual post for more visibility)",Intel,2025-07-24 04:03:55,3
Intel,n4u3ud5,Are the iGPU memory allocation settings in the BIOS or the Intel ARC control panel?,Intel,2025-07-24 03:14:42,2
Intel,n572exj,"intel dynamically scale IGP usage and has a hard cap that is 57% of system memory. As per reporting by CapFrameX, Vulkan) maybe DX12 too, not sure) will allocate dynamically as high as the aforementioned hard cap but DX11 (legacy API) will stuck at 128MB VRAM reporting   I shall test this update on my MTL-H laptop",Intel,2025-07-26 01:54:56,2
Intel,n57ks35,Where is this settings at? Doesn't appear on my U125H,Intel,2025-07-26 04:00:00,2
Intel,n4snj7k,Just used this by accident and i think i accidentallx bottlrnecked CPU on the 258v lmao,Intel,2025-07-23 22:14:07,-1
Intel,n4v71jx,"It's on Intel Graphics Software. On BIOS usually it will be greyed out and written as 128MB. Last time i remember Intel allowed iGPU memory allocation on BIOS is at skylake era, nowadays it's locked and it's done automatically until the recent update.",Intel,2025-07-24 08:41:39,3
Intel,n4v74zp,"What do you mean bottlenecked CPU? iGPU memory allocation isn't affecting how CPU works because it only changed the amount of ram reserved to iGPU, not the speed.",Intel,2025-07-24 08:42:33,3
Intel,n4v8s60,Because the CPU had too little memory to work with!,Intel,2025-07-24 08:58:01,2
Intel,n4yc9ay,"CPU has its own memory called as cache (L0, L1, L2, L3 and L4), it's nothing to do with RAM. The only thing RAM affecting overall performance is latency and speed.  The amount of RAM doesn't matter much, as long it has enough RAM for OS and application then it wouldn't make the system runs slow, let alone to make the CPU runs slower which is not possible.",Intel,2025-07-24 19:12:01,2
Intel,n2eity1,I am also questioning why is Intel still focusing so much on battlemage when celestial is supposed to be just around the corner.   It feels more and more like Lip bu tan axed Celestial.,Intel,2025-07-10 17:50:16,17
Intel,n2ekd7d,The weird thing is when he did a keynote a slide noted dGPUs was part of Intel.,Intel,2025-07-10 17:57:07,10
Intel,n2esx4q,"Huh?  They launched one die, and haven't said anything about launching more.  How is that being focused on Battle mage?",Intel,2025-07-10 18:37:07,7
Intel,n2ovsok,Celestial IGP is pretty much done with `force-probe` option has on its way to be removed (driver promoted from experimental to mainline)   Celestial dGPU is pretty much unknown. If Tan wants to bet dGPU because Gaudi accelerators didn't sell (or badly marketed? too expensive? dunno) then he better not kill it. Arc Pro B50 and B60 received good AIB reception that they can go crazy and make whatever from it,Intel,2025-07-12 07:12:47,2
Intel,n2etvjo,There is big battlemage coming in Christmas apparently.   Which worrying since celestial supposed to be 2026.   I guess we are lucky if we get celestial early 2027,Intel,2025-07-10 18:41:41,2
Intel,n2fe3us,"This is some good news after the layoffs  Hopefully, the Arc DGPU Division didn't get cut too deeply and that there's still gaming Xe3 Arc cards and Arc Pro cards in the pipeline  Insane that Lip Bu Tan would gut so much of the workforce   The board must've been itching for layoffs after firing pat or Lip Bu Tan might think this is how you save Intel.  Either scenario is bad.  Deep across the board cuts like this is how you destroy a company as talented people and lazy underperformed are caught in the dragnet.  Irreplaceable talent will go to companies with better stock options, 401ks and compensation like Nvidia, AMD Qualcomm, ARM, Apple.  The board and Lip Bu Tan are fools for thinking this is a good idea.",Intel,2025-07-10 20:18:48,9
Intel,n2eqaqt,I wish that they just added way more memory instead of messing around with dual GPU,Intel,2025-07-10 18:24:39,2
Intel,n2h61tf,Seems like a decent option for a low power PC,Intel,2025-07-11 02:03:40,2
Intel,n2hsnlo,Gorgeous.. love these builds.   How hot did she get?,Intel,2025-07-11 04:31:41,1
Intel,n2j7jkd,I would buy HX cpus adapted to lga1851 with an interposer like the chinese 12900hx and 13950hx es chips.,Intel,2025-07-11 11:48:40,1
Intel,n9iopqd,I. . .WANT. . .THIS!,Intel,2025-08-19 12:35:26,1
Intel,n2eths0,The 192-bit bus and density of gddr6 limits them to 24GB per GPU chip max. They have to go multi-gpu to have more than that.,Intel,2025-07-10 18:39:52,11
Intel,n2f2m1i,Yes. You are right about the memory bus .  I completely forgot about that,Intel,2025-07-10 19:23:17,3
Intel,n2f7rym,When compared to this  AMD intel and Nvidia with thier supposed Ai GPU's are just creating E-Waste . https://support.huawei.com/enterprise/en/doc/EDOC1100285916/181ae99a/specifications,Intel,2025-07-10 19:48:21,1
Intel,n2ukq9e,"Just like motherboards have 4 RAM slots for 128 bit bus and only two channels, GDDR6 can also use clamshell mode to double the VRAM without going multi GPU.",Intel,2025-07-13 04:58:01,1
Intel,n2f2svk,"Kind of a shame honestly. If bigger Battlemage does materialize, that 256-bit bus gets you a 32GB max capacity instead. 64GB per dual-GPU card.",Intel,2025-07-10 19:24:12,1
Intel,n2vn1vb,"Yes, and that tops out at 24GB for a 192-bit bus.",Intel,2025-07-13 10:55:55,1
Intel,n2f8hwv,Yeah..Shave really because there's a company which has developed an ai GPU which will have an ability to add more memory just like we do with system ram..  It's still in works and very long time to go until an actual release .   But they have demoed their FPGA card which very soon will be turned in to an ASIC,Intel,2025-07-10 19:51:51,1
Intel,n3bt4j7,AI inferencing would save intels ass if they'd pump out high bandwidth high VRAM cards. They don't even need massive chips to do it. Maximum profit.,Intel,2025-07-15 20:33:21,1
Intel,n1nccv6,We need Intel to be a viable competitor to AMD and nvidia. That means better prices all across the board and more options for us consumers.   Iâ€™m rooting for Intel in the gpu sector. If only they would get their shit together for CPU.,Intel,2025-07-06 15:26:45,10
Intel,n1sfyur,Amd is competitor with intel? XD,Intel,2025-07-07 11:24:12,-8
Intel,n1v1te3,â€¦ yes.,Intel,2025-07-07 19:50:33,2
Intel,n1w7xtw,"What the hell are you even talking about? CPUs? GPUs? Both? The lack of specificity is really irking me, and it shows you don't care enough about the topic to even be right about it.",Intel,2025-07-07 23:40:06,2
Intel,n1x6o0y,"I work with AI, comfyui and other heavy things, the AMD gpu are ðŸ’©ðŸ’© there, good luck if you amd fanatic",Intel,2025-07-08 02:59:48,-1
Intel,n1yg9dq,The unintelligible ramblings of a madman.,Intel,2025-07-08 09:23:54,1
Intel,n1ypylu,"Sure, a madman not amd fanatic ðŸ¤¡",Intel,2025-07-08 10:52:03,2
Intel,n0uunzj,"Intel needs to fix their Arc supply issue. They really need to start pumping out Arc cards on US soil.  However this is good news to publicly announce Arc BattleMage will expand into AI markets.  So now Intel Arc, a discrete graphics card has a consumer line, a pro line and venturing into edge/AI.  Nvidia across the board has become too expensive and AMD continues to limp along in typical mediocre fashion. Intel needs to execute persistently.   I canâ€™t give financial advice, but Intel is at a good price. if Intel can get the foundries up and running sooner, its stock will be a buy for most.  Intel is getting ignored by the White House, dominated by TSMC with the Taiwanese government, backing them all the way, and seen as a corrupt entity by AMD famboyz.   There are people to this day, who keeps complaining that Intel didnâ€™t innovate back in the day and charged so much for their CPUs. They live in this alternate reality where AMD can do no wrong,  as Amd 9800 X 3-D chips melt along with other AMD CPUs and Radeon cards are only mediocre at best.  We are seeing many 9000 series Ryzen chips start to overheat, especially the high end chips like the 9950 X 3-D and the 9800 X 3-D. There could be more issues on the horizon. Keep your eyes open.",Intel,2025-07-02 00:14:23,19
Intel,n0t7uvi,LBT will fix this.,Intel,2025-07-01 19:12:46,5
Intel,n0whrmr,"Interesting strategy compared with Nvidia which absolutely refuses to let consumer GPUs be used for enterprise (spoilers, everyone still do tho) That's why even brands that make local AI training desktops, which are in effect entry level workstations, like Gigabyte and its AI TOP www.gigabyte.com/Consumer/AI-TOP/?lan=en need to pretend these are just gaming rigs that happen to be really good at AI to keep Nvidia from getting on their case.",Intel,2025-07-02 07:04:40,2
Intel,n15yff6,"Good, good, keep them coming, flood the market with them cards.",Intel,2025-07-03 17:47:27,2
Intel,n0s6tnb,"I was actually hoping for this, but looks like the software support for intel died for a bit. They cut development for their Npu libraries and theres limited support for their gpus if you dont want to use openvino",Intel,2025-07-01 16:21:48,4
Intel,n7i9nn4,"you are finished intel, i'm going to start a class action lawsuit regarding having to FINISH OUR PAID FOR INTEL PRODUCTS OURSELVES, THERE IS NO INSTALLATION SUPPORT THAT ACTUALLY WORKS, USERS HAVE TO MANUALLY IMPLEMENT IT? that's A CRAZY LAWSUIT I AM SURPRISED NOBODY NOTICED. FALSE ADVERTISING, REASONABLE ENJOYMENT, AND OF COURSE, THEY STOLE OUR MONEY!",Intel,2025-08-07 23:15:59,1
Intel,n7ifgc2,"go ahead and ban me, i'll add that fact to the lawsuit, instead of help customers you decided to ban them.",Intel,2025-08-07 23:49:03,1
Intel,n7ifote,"the entire intel corporation is owned by some asian guy.   he's obviously SABOTAGING IT, as they take all the tech back to china.   LIKE THEY DID WITH NORTEL in the 90's. WAKE UP INTEL, you are being DESTROYED INSIDE OUT.",Intel,2025-08-07 23:50:25,1
Intel,n7ifsqc,"china loves the new ai from intel, nice how it works for china, BUT NOT FOR NORTH AMERICANS.   LAWSUIT TIME.",Intel,2025-08-07 23:51:03,1
Intel,n7iggdq,"u/intel WILL BE INTERESTING WHAT US AND CANADIAN SUPREME COURTS BOTH THINK ABOUT CHINA RUNNING YOUR AI, BUT ALL OF YOUR OTHER INTERNATIONAL CUSTOMERS CAN'T SEEM TO GET IT RUNNING AT ALL.",Intel,2025-08-07 23:54:49,1
Intel,n7iia1k,"Intel #TRASH.   hundreds of gigabytes later,   still no working #AI",Intel,2025-08-08 00:05:35,1
Intel,n7iiod9,"u/reddit i'll sue you guys too. literally you are letting u/intel lie through their teeth about unsupported products they knowingly sell without support and then expect the end user to build said support-THEMSELVES. THAT'S A LAWSUIT GUYS, i'm sure you don't want to be known as a COLLABORATOR for intel's illegal activity, ya? STEP OFF.",Intel,2025-08-08 00:07:57,1
Intel,n7ijctd,"if i can't use what you advertised, that's false advertisement, you made it almost impossible for normal average people to use your hardware and software, and expect not to be sued? GET REAL :D u/intel",Intel,2025-08-08 00:11:59,1
Intel,n7ijm4v,i can't even stand anything intel now. 3 years of this disappontment. i'm finished with u/intel besides to sue them.,Intel,2025-08-08 00:13:30,1
Intel,n7ikjfh,"they are firing employees around the world, intel is finished. DROP your intel stuff. it's about to lose support forever. LOL.",Intel,2025-08-08 00:18:58,1
Intel,n7iktaa,"proof intel is dying. forget intel, forget their ai. turn your backs on them, like they did when it was time to support their gpu u/intel-ModTeam u/intel you deserve what comes your way, INTEL.   [https://www.youtube.com/watch?v=pWT5eyto\_P0](https://www.youtube.com/watch?v=pWT5eyto_P0)",Intel,2025-08-08 00:20:36,1
Intel,n7im5qw,"they can't even make their ai work properly. which is why people went intel in the first place, all of those false promises. 3 years later. my intel gpu is pretty much useless, all those advertised features, still waiting on those. there is no solid way to install their ai suites, it's a joke. since when did we have to script and code our own support for SOMETHING SOLD TO US>NEVER. sue intel, move on. u/intel your own fault guys. YOUR FAULT. [https://www.youtube.com/watch?v=pWT5eyto\_P0](https://www.youtube.com/watch?v=pWT5eyto_P0)",Intel,2025-08-08 00:28:32,1
Intel,n7imn0z,"the 12700k is nice. but my A770 16gb was a bad buy. having to run scripts and code myself just to utilize features advertised, that's a big mistake on their part. they should have made a no-brain solution for installing their garbage. it certainly will always be garbage in my mind, none of it has worked after hundreds of gb of git/python downloads.",Intel,2025-08-08 00:31:19,1
Intel,n7ioabt,"intel deserves what comes their way. their AI support for their GPUS is literally NON EXISTENT. a whole lot of links to ""do it yourself"" who sells a product like that: HERE'S THE HARDWARE, now go BUILD the SOFTWARE support, YOURSELF. how STUPID can a COMPANY GET. u/intel",Intel,2025-08-08 00:41:06,1
Intel,n7ipr1b,"Intel #intelai is trash: HUNDREDS OF GIGABYTES OF GIT/PYTHON and INTEL SOFTWARE,   and none of it worked, NONE OF IT. u/intel",Intel,2025-08-08 00:49:47,1
Intel,n7iqed5,"Intel #intelai is trash: HUNDREDS OF GIGABYTES OF GIT/PYTHON and INTEL SOFTWARE, and none of it worked, NONE OF IT.    FALSELY ADVERTISED HARDWARE CAPABILITIES. that's a lawsuit. #IntelGraphics #Intelai <- ARE WORTHLESS PURCHASES. u/intel-ModTeam u/intel",Intel,2025-08-08 00:53:30,1
Intel,n7iqnx9,i want my money back u/intel    and you can pay for my wasted 3 years of time.   (that's a huge lump sum of money) works for me.   u/intel <= SUE THESE CROOKS WHILE YOU CAN.,Intel,2025-08-08 00:55:03,1
Intel,n7is2uo,"HARD TO FOLLOW REDDITS POLICY, WHEN INTEL WON'T EVEN RELEASE PROPER INSTALLATION SOFTWARE, AND IS NOW RUNNING AN ILLEGAL RACKET THAT SCAMS CUSTOMERS. u/intel",Intel,2025-08-08 01:03:17,1
Intel,n7isqcf,"PEOPLE: are dropping billions into online ai generators(myself included)   that could have been ALL u/intel-ModTeam \#intel u/intel revenue. but nah,   intel literally is self sabotaging itself. how peculiar indeed.",Intel,2025-08-08 01:07:11,1
Intel,n7it7ti,DO NOT BUY u/INTEL there is no SOLID WAY to DEPLOY the AI SOFTWARE. aka none of it works.,Intel,2025-08-08 01:10:05,1
Intel,n7j1gpq,"just glad i found out the real truth, intel still has no solid ai solution for their gpus. the real truth saved me from wasting money on max sun 48gb gpus. honestly what a relief really, to go back to nvidia. this intel a770 GPU sucked the whole f-ing time. it honestly can't do any of the stuff they claim. it's one of intel's biggest lies going. no wonder they are going out of business. YA YOU GUYS: u/intel-ModTeam  u/intel",Intel,2025-08-08 01:59:51,1
Intel,n7j2zux,"the longer your stuff doesn't work, the longer i encourage everyone to sue you u/intel-ModTeam  u/intel",Intel,2025-08-08 02:09:13,1
Intel,n7j3ifw,nobody likes their job at intel u/intelÂ [u/intel-ModTeam](https://www.reddit.com/user/intel-ModTeam/)Â or else you'd see them jumping miles high to make this stuff work flawlessly. they're obviously tired of their jobs or are too wealthy to be concerned with their futures. the ceo included. #INTEL is synonymous to other words like TRASH and NON-FUNCTIONAL. their stuff just DOES NOT WORK as they CLAIM it does. period.,Intel,2025-08-08 02:12:24,1
Intel,n7j3xu0,"good try intel, but i'm not buying any max sun 48gb anymore u/intel u/intel-ModTeam i was going to get two of them. BUT YOU CAN'T EVEN USE THE AI FUNCTIONS on INTEL GPU, every ui wants CUDA. really sad intel has NO CHANCE in the ai market, their stuff DOES NOT EVEN FUNCTION. whereas nvidia's is a cinch to setup, and works flawlessly.",Intel,2025-08-08 02:15:04,1
Intel,n0wq7k2,"> There are people to this day, who keeps complaining that Intel didnâ€™t innovate back in the day and charged so much for their CPUs.   When AMD launched Zen 3 and later Zen 3 3D, they were fast but also very expensive compared to what Intel offered. Intel, AMD, Nvidia, same shit, different maker. The moment they will make a product better than the competition it will be more expensive. It's called a monopoly.Â    Back then when Intel charged a lot for their chips they did it because they had no competition. Don't understand why Intel was in the wrong and somehow AMD doing the same was justifiable. Every company does these things.",Intel,2025-07-02 08:27:29,10
Intel,n0uy3p5,">Intel needs to fix their Arc supply issue. They really need to start pumping out Arc cards on US soil.  Margins have to be way too low for Intel to do this, especially since Intel also said they would be cutting low margin product development.   They might still end up biting the bullet, but I don't think they should at all. There are tons of better areas where that money can be spent.   >I canâ€™t give financial advice, but Intel is at a good price. if Intel can get the foundries up and running sooner, its stock will be a buy for most.  Honestly, I still think the risk is too high. No external 18A customers is a terrible, terrible sign. Intel can bear the economics of 18A internally, but they outright said that doesn't apply anymore for 14A.   >They live in this alternate reality where AMD can do no wrong, as Amd 9800 X 3-D chips meltÂ   Tbf a pretty small scale issue, which hasn't stopped the popularity of those chips at all... unlike the RPL fiasco.",Intel,2025-07-02 00:35:01,4
Intel,n0yywsv,"Different strategies. X3D has its place in time. X3D chips have to have another separate line to build and now test. No mobile X3D chips so far.   Silicon cost is greatly lowered if the design is the same and desktop/mobile chips can be binned to create separate product lines.Â    AMD I will argue absolutely needed to produce stellar products because of their past history before Zen 1. No one will defend AMD products before Zen 1.   I will definitely use a newer better AMD chip today but back then he'll no. There GPU line has driver issues and its why all of us just use NVIDIA gpus. There is a good reason.Â    Intel's new chip design strategy and leading edge fab is solid. The problem is foundry capital expenditure and foundry capacity. They have to build excess and train excess personal. The Ohio Silicon Heart Land. A whole new campus, town, new families, and Silicon ecosystem.Â    That takes time and a ton of money. And investment (aka the money levers) just dont want to pay for this. They rather get there money's worth of AI and self driving vehicles today. They know Chinese EV investment is bad and Chinese AI is bad. So rather than invest in Chinese companies and not make money, they want to use there leverage to pump up American companies.Â    This way American companies like Tesla and AI will succeed and win this AI race.   In other words, China ain't in the leading edge foundry game. None at all. No chance. So why pump money into that? They instead are attacking the EV and AI race. So our American companies will need capital to defend that space.   Intel design products are absolutely solid. 1000% they maintain better MT performance without SMT. SMT has security vulnerability.Â    Intel products are wafer Silicon efficient with p and E cores. Intel design is solid. Just foundry is extremely expensive and there isnt a China competitor. Taiwan is a very safe island.   If Taiwan was like a Hawaii, easily conquered, then for sure TSMC is at risk. But Taiwan today is like an island carrier/fortress. Too difficult to go against. At least not yet.   Its only value is in its tropical fruits and strategic island location.",Intel,2025-07-02 16:46:14,2
Intel,n0vvhnx,"It's such a small market, and it honestly doesn't really matter EXCEPT as practice for their data center GPUs. Intel has bigger fish to fry (like the fact that they're ceding significant server and laptop market share, and are in the process of building several 10 billion dollar+ facilities on the foundry side).",Intel,2025-07-02 03:59:19,1
Intel,n10my48,"I hadnâ€™t seen that there was a problem with 9000 series chips apart from with some Asrock boards, has there been another issue identified?",Intel,2025-07-02 21:38:16,1
Intel,n7ilj0i,"intel is dying, and i doubt things are going to get better.   [https://www.youtube.com/watch?v=pWT5eyto\_P0](https://www.youtube.com/watch?v=pWT5eyto_P0)   really too bad they made their ai stuff almost impossible to install and use.   they literally KILLED their company, themselves, with CRAP SUPPORT.",Intel,2025-08-08 00:24:50,1
Intel,n0usdim,What is LBT?,Intel,2025-07-02 00:00:53,5
Intel,n7ibwwv,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-08-07 23:28:47,1
Intel,n7mv4ae,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-08-08 17:20:35,1
Intel,n7mvvu5,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-08-08 17:24:07,1
Intel,n93rk79,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-08-17 01:24:09,1
Intel,n1ey5jp,">Don't understand why Intel was in the wrong and somehow AMD doing the same was justifiable.   It's a cultists thing. Amd for some stupid reason has a lot of braindead people in their community who gonna support Amd even though they blatantly doing shaddy business practice, even though those people got robbed so bad, same as Nintendo and Linux.",Intel,2025-07-05 03:26:38,5
Intel,n7j8tks,"i can't get any video generator to work in comfyui, meanwhile xpu generation has been advertised, but i see no one using it (seems like no one can, hahaha)  i hear all kinds of positives about NVIDIA tensor   even their GTX can run it.   i pity intel and their slow fall down the drain, but it's their own fault.   nobody wants stuff that doesn't work as advertised.",Intel,2025-08-08 02:45:49,1
Intel,n7j8zg7,"was straight up working towards 2x max sun 48gb, now i'm just gonna get a RTX 6000   i'm not playing games with u/intel (and that's all intel seems to want to do, lie to customers)",Intel,2025-08-08 02:46:52,1
Intel,n7j97w4,"intel AI doesn't EVEN EXIST.  PROVE ME WRONG.    PROVE COMFYUI WRONG.  u/intel your stuff doesn't work, at all.",Intel,2025-08-08 02:48:22,1
Intel,n7j9hp2,gave u/intel 3 years to do a consumer grade ai theme like nvidia's.   still looking around wondering where it is (nothing works on huggingface or comfyui)   intel gpus are a lie.,Intel,2025-08-08 02:50:06,1
Intel,n7jajv8,"the moral of the story...(as intel goes out of business)   don't lie to customers, and make stuff you can't properly support, out of box.   nobody wants to get dirty deep in script and code or batches, for something that still won't work anyway. u/intel 100% team ""ULTIMATE-fail"", 2022-2025. u/intel-ModTeam",Intel,2025-08-08 02:56:56,1
Intel,n7jb2m4,"if intel could have delivered something like what nvidia has going now,   they'd be topping the charts, in everyones ""to buy"" lists.   the proof in their failure, well just look around.   you can see many more people are nvidia fans, than amd or intel fans.   the reason: NVIDIA'S STUFF ACTUALLY WORKS.    u/intel u/intel-ModTeam it's no wonder it's over for all of you. enjoy the streets :D",Intel,2025-08-08 03:00:23,1
Intel,n7jbfef,"and until i can generate a video via xpu over in my comfyui, i'll continue to tell the truth about intel products, and how dysfunctional and unacceptable they are, by today's industry standard, set by NVIDIA CORPORATION.    u/intel u/intel-ModTeam  there is no intel, at least not in my mind, not anymore.",Intel,2025-08-08 03:02:45,1
Intel,n7jbkj6,"i'm getting a RTX 6000 96gb and that new CPU nvidia is making, and their motherboard. hopefully ddr6 is out by then. RIP, INTEL and AMD. (in fact i hear both the gpu and motherboard will both use HBM2 ram or something similar) more stuff intel and amd can't bring to the table. like i said, there are no such things as INTEL or AMD in my mind, anymore. times sure have changed.",Intel,2025-08-08 03:03:42,1
Intel,n0uw58u,Lithography But Tinier,Intel,2025-07-02 00:23:14,5
Intel,n1hlhpp,"New Ceo Lip Bu Tan.  Though I think Celestial might come to US soil, there is no way battlemage ever will.",Intel,2025-07-05 16:03:01,2
Intel,n7ibxdj,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-08-07 23:28:51,1
Intel,n0p8ab5,unfortunately the 48GB card won't be priced for mere mortals https://www.reddit.com/r/LocalLLaMA/comments/1lokp88/intel_arc_pro_b60_dual_48g_turbo_maxsun_gpu/,Intel,2025-07-01 03:46:30,7
Intel,n0zxr42,Has anyone purchased any of the Maxsun Arc Pro GPUs? It is a Chinese company with little support or none at all in the US?  Someone in the US has to be a guinea pig and buy these Maxsun Arc Pro GPUâ€™s and let us know how it goes .,Intel,2025-07-02 19:34:36,1
Intel,n1c0nmk,Has anyone opened up a business account and is in the process of buying one of these Pro Arc GPUs.,Intel,2025-07-04 16:49:53,1
Intel,n0mmrqp,"""passive"" is a huge technicality here as it still requires loud case flow-through fans to push enough case air to make it matter.",Intel,2025-06-30 19:10:21,0
Intel,n0pmhya,"I'd take that post with a large grain of salt. He also said:  >Nope, that quote was just for the GPUs! I managed to talk them down to 3800 per unit for 4 cards, and for 5+ units 3000 per unit. A little better, but still not a good value compared to what's already out there right now. We'll see how prices change in the future I guess...  The fact he called them and they dropped the price by $1200 per card, suggests that's not a reputable seller with pricing that will reflect real world MSRP.  Most of all, that pricing doesn't make any sense. No one is going to buy them when you could just buy an Nvidia card.",Intel,2025-07-01 05:36:14,10
Intel,n0n0bkt,It's passive server card for as blower cards can create turbulence.  It's not a consumer passive card,Intel,2025-06-30 20:16:55,18
Intel,n0rhmsc,Yeah the only way to get enough forced air through it is in a narrow rack mount case with 9000rpm fans going max.  It's a technicality because the fans the passive card needs aren't attached to the GPU.,Intel,2025-07-01 14:23:07,0
Intel,n0rzbmt,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-07-01 15:46:52,1
Intel,n1s9740,Is it safe to install an Arrow Lake CPU without a third-party contact frame over the long run?,Intel,2025-07-07 10:28:03,1
Intel,n212egt,"Edit: solved. Windows is capable of disabling e-cores on boot. The setting to turn them back on in MSCONFIG, boot tab, cores dropdown. Whatever feature causes this, I do not know but would love to find out, and furthermore their is no legitimate purpose for this feature, and Intel should prevent Microsoft from doing this, in my opinion, if there is any legal means available. Unless it interferes with Intelâ€™s revenue.  Poor performance, incredibly low benchmarks. Beginning to suspect bad CPU.  I have tried bone stock with cleared CMOS settings. I have loaded multiple overclocking profiles, clearing CMOS in between. I have tried undervolting, and adjusting Load Line. I have changed Lite Load settings. I have tried with and without XMP enabled. Changed windows power plans. Nothing gets my cinebench r23 score above 11k. CPU-Z benchmarks it as 43% as fast as the previous generation CPU. I never overclocked this cpu, and I changed to the updated bios with new microcode the day it was available. CPU Purchased from Intel, retail box.  Windows 11 Build 26100.4484  CPU: Intel I9 14900K, stock clock, 360 AIO liquid cooling  RAM: Patriot Viper Venom DDR5 32GB (2x16) 7200MT/S CL34  GPU: MSI Ventus NVIDIA RTX 4080S  Motherboard: MSI Z790-S Pro Wifi with most recent bios  Storage: WD Black 2TB NVME on CPU; Toshiba 20GB X300 Pro  PSU: NZXT C1000 PSU (2022)  Display: Samsung Odyssey G93SC  I am at my wits end with this thing. I first noticed games crashing to desktop a few months ago. I thought it was just poorly coded (Helldivers 2). I checked my FPS in the game, and while previously I had to frame limit it 10 144FPS, I was now getting close to 60-80FPS.  Is my CPU a toaster or is there something I'm missing?  CPU-Z output, and HWiNFO64 sensor readings:  [https://imgur.com/a/ROuZOKS](https://imgur.com/a/ROuZOKS)",Intel,2025-07-08 18:18:07,1
Intel,n2cfmmi,"I bought a 265kf CPU from Amazon. On the checkout page it clearly stated that my purchase qualified for the Intel Spring Gaming Bundle and that I would receive an email with a Master Key. Well, I didn't receive anything and I spent all day being transferred from one Amazon support staff to another to no avail.  The promotion is literally still active and if I try to buy the CPU again it shows the same offer.  But somehow no one on Amazon or Intel support can tell me why I didn't get the email.",Intel,2025-07-10 11:27:48,1
Intel,n2h44v7,Does anyone know if the upcoming Bartlett Lake-S 12 p-core no e-core CPU will suffer from the same stability issues as Intel 13th gen and 14th gen CPUs?,Intel,2025-07-11 01:52:33,1
Intel,n4at6sf,Any news to share regarding this link? https://www.reddit.com/r/intel/s/Bg4QnVzIdD,Intel,2025-07-21 06:57:04,1
Intel,n4vm1wc,"Bug report: Latest ARC driver 32.0.101.6972 causes crashing using Speed Sync  I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  Monitor is non VRR compatible (older gsync monitor ASUS PG348Q)  Hope this gets resolved as it sounds like a great feature.",Intel,2025-07-24 10:55:22,1
Intel,n5c8s6s,"Hello,  I own a i9-14900K, mobo MSI MAG Z790 Tomahawk. I am wondering if itâ€™s possible a specific program can cause my cores to be power limit exceeded, and is there any fix to that? Currently it drops the cpu speed to 0.8 GHz. This one program LeagueClient.exe for a game, League of Legends, has recently started to cause this problem, the only fix is to restart the computer. I have updated my bios, changed power limits within the bios, disabled c state, disabled EIST, disabled e-cores, tried to go into safe boot but the program wonâ€™t launch, reinstalled multiple times.",Intel,2025-07-26 22:34:12,1
Intel,n663btu,Has the degradation of the 13th and 14th gen CPUâ€™s been fixed yet?,Intel,2025-07-31 14:14:55,1
Intel,n6pmegm,"Been getting the following error A LOT while playing Expedition 33, and now after 18 hours I can't start the game without it crashing:  `LowLevelFatalError [File:C:\Hw5\Engine\Source\Runtime\RenderCore\Private\ShaderCodeArchive.cpp] [Line: 413] DecompressShaderWithOodleAndExtraLogging(): Could not decompress shader group with Oodle. Group Index: 760 Group IoStoreHash:52bcbf8ac813e7ee35697309 Group NumShaders: 29 Shader Index: 9301 Shader In-Group Index: 760 Shader Hash: 3BF30C4C9852D0D23B2DF59B4396FCC76BC3A80. The CPU (13th Gen Intel(R) Core(TM) i7-13700KF) may be unstable; for details see` [`http://www.radgametools.com/oodleintel.htm`](http://www.radgametools.com/oodleintel.htm)     Am I just screwed because I bought the wrong generation of Intel CPUs 2 years ago?",Intel,2025-08-03 15:45:17,1
Intel,n7b4kpy,"I'm not sure if this matters, but when runningÂ `garuda-inxi`Â on my laptop, it tells me that my i3-8130U is Coffee Lake Gen 8, not Kaby Lake Gen 9.5. I've seen similar problems reported with users using CPU-Z, is this just a known bug or is there something else going on?  I'm going to be tweaking my CPU performance soon, so I'd like to make sure of the CPU capabilities/options first.",Intel,2025-08-06 21:44:11,1
Intel,n85zrk7,"I've been planing to build a PC from scratch for video editing and Ultra 7 265k and Arc B580 are good in my price bracket. Now, I'm worried with all the talk of Intel potentially going bankrupt or having layoffs, is it a good idea to still buy their products? Will we lose support with drivers and stuff like that?",Intel,2025-08-11 19:48:10,1
Intel,n8auefg,My gaming laptop Intel core i7-12700h runs at a constant temperature 95 . Doesn't matter if I'm playing a high end game like cyberpunk or some indie game like hollow knight. Is this thing suppose to always run at this temp?,Intel,2025-08-12 15:12:02,1
Intel,n8i5ccq,"con el nuevo microcodigo de intel , los juegos de ubisoft (ASSASINS CRREED ODYSSE) tienen tiempos  de carga excecivamente altos, diria que al menos 10 veces mas de lo normal, Al devolverr la bios al microcodigo anterior todo funciona bien, cuando lo van a parchar?",Intel,2025-08-13 17:35:56,1
Intel,n8k6lv9,is it worth the upgrade for ai preformance cus my 5070 ti wont work for some reason so now my cpu is my main accelertor and user so should i upgrade to ultra 9? and i  will remove 5070 ti from my flair soon so yeah .,Intel,2025-08-13 23:38:35,1
Intel,n9pv76c,"ok now this is something. my Intel HD Graphics Control Panel is no longer there? no idea how long its been gone but i clearly remember it being there at a point.  using a Lenovo G510(i7-4700MQ, HD Graphics 4600)   Windows 10 Home 22H2 (Build 19045.6216)  windows apps in settings shows the intel driver but not the control panel   drivers from [lenovo's website](https://pcsupport.lenovo.com/in/en/products/laptops-and-netbooks/lenovo-g-series-laptops/lenovo-g510-notebook/20238/downloads/ds103802-intel-vga-driver-for-windows-10-64-bit-lenovo-g410-g510?category=Display%20and%20Video%20Graphics) are dated 16jul 2015. version seems to be 10.18.15.4240   drivers from [intel's website](https://www.intel.com/content/www/us/en/download/18388/intel-graphics-driver-for-windows-10-15-40-4th-gen.html?wapkw=intel%20hd%20graphics%204600) are dated 9jan 2015. version is 15.40.7.64.4279  now whats funny is that my device manager shows that my driver is dated 8mar 2017 which is version 20.19.15.4624   i also have another driver that i can see in the update drivers menu(drivers already present on my device) along with this one which is dated 29sep 2016 version 20.19.15.4531  i have tried reinstalling the driver from the update driver menu using the 8mar 2017 version, no change at all.  my drivers dont seem to be DCH drivers.   intel also says that they [discontinued the ms store version of the control panel](https://www.intel.com/content/www/us/en/support/articles/000058733/graphics.html) anyway.  this is all that i could think of writing here. any other details required just ask.   any help would be good lol",Intel,2025-08-20 14:29:49,1
Intel,na637rq,"my i5-14600KF is being throttled at low temps and refuses to go past 0.8ghz of clock speed. Nothing I do seems to get it to stop throttling. According to throttlestop i have a red EDP OTHER ongoing throttle under CORE and RING. My average CPU temp is 31 degrees C across all cores and im getting 0.69 Voltage to my CPU  CPU: Intel i5-14600KF stock settings no overclock   GPU: Intel Arc B580 ONIX Odyssey BAR resizing enabled   Motherboard Gigabyte Ultra Durable Z790 S WIFI DDR4   RAM: Corsair Vengeance DDR4 16 GB x 2 (32GB)   Storage: 1TB Corsair MP600 CORE XT SSD + 2 TB WD Black SN770   PSU: Cooler Master MWE Gold 850 V2  EDIT: NEW INFO ACQUIRED   When running in safe mode and when booting into BIOS settings my CPU acts normally and receives typical voltage. Something running on my computer is throttling my CPU as i boot into windows. If i open Task Manager quickly after booting, I see system interrupts consume a mild amount of CPU before quickly going away, rather than sticking around when their is an ongoing hardware issue. I have reason to believe a program, either maliciously or due to error, is fucking with my CPU. Also worth noting is that Intel Graphics Software reports my CPU utilization as far higher than task manager, anywhere between 2-50% higher.",Intel,2025-08-23 00:28:09,1
Intel,nand9o5,"Currently in the planning/purchasing phase of a small NVR/Steam Cache server. Information on VROC on X299 is pretty limited. so far I've seen mixed information on the drives supported. Before I purchase x4 Intel P4510 drives, I was hoping someone on here has a similar configuration that works.  The mobo manual states that only Intel based drives are supported but doesn't clarify which intel drives. Also saw on the intel forum that X299 CPU raid is further limited to only Optane based NVME drives. This drive will not be booted to, and I dont want to do a windows based raid.  My planned specs are:  CPU: 10900X  Mobo: X299 Taichi CLX - One of the few that seems to support bifurcation & VROC  Drive: Intel P4510 1TB  VROC Key: VRoc Standard  Are the Intel P4510 supported for Vroc on the x299 platform?",Intel,2025-08-25 20:44:18,1
Intel,natm1ml,"My Intel I210 ethernet device has device id 1531 meaning unprogrammed. The freebsd ethernet driver does not work with 1531. It needs device id 1533 meaning programmed. (Can I use a different driver? No, it's an embedded system that only supports this driver.)  I was linked this:  https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html  but 1) have no idea how to do it 2) cannot access the .bin file and tool it requires  Does anyone have ELI5 steps for getting the device to show devid 1533? Where can I get the .bin?",Intel,2025-08-26 19:48:39,1
Intel,nb5mwcf,"The RMA process for Intel is absolutely atrocious. I don't know if  anyone can give advice on this but here is what is happening:  Based in Germany, for geolocation info. I was one of the early adopters of the 13th gen processors, but as I don't follow tech news too strongly I didn't find out about the issues with these chips until autumn 2024 when the issues I was having with my PC escalated to a point I couldn't ignore any further. Identified the CPU as the likely culprit and started the RMA process.  Firstly, Intel would not offer any solution where I could continue to use my PC whilst they analysed my CPU. As I use my PC for work, having it out of action for weeks/months was simply not an option, so I was forced to pause the RMA ticket whilst I saved up for a new cpu way ahead of my expected timeline.  With that aside, I reopened my ticket and the requirements they lay out are near impossible to meet - they wanted a clear photo of the matrix on the front of the chip and the matrix on the pcb itself.  The front of the chip was simple enough but on this series Intel printed the matrix in dark grey on a dark green pcb. It's barely visible just with the naked eye and I've tried so many ways to take a picture of the matrix with my smartphone and nothing I do is getting a clear picture.  I don't understand how your average consumer can possibly meet this requirement - solutions online apparently suggest purchasing a special type of expensive scanner or a macro lens for your smart phone? Which is ridiculous to me.  There is no way they cannot verify my chip with the rest of the information I have been able to give them, as well as far as I am aware, the matrix on the side of the chip on the 13th gen is literally the same as the one on the front of the chip.  The whole experience is proving to be awful, time consuming, feels like it should be illegal and has completely put me off ever using Intel again, or recommending it to anyone I help spec builds for.",Intel,2025-08-28 16:16:20,1
Intel,n22cmoi,"u/Progenitor3  yes, it is generally safe to install an Arrow Lake CPU without a third-party contact frame over the long run. CPUs are designed to function properly with the standard mounting mechanisms provided by the manufacturer. Third-party contact frames are optional and may offer additional stability or cooling benefits, but they are not necessary for the safe operation of the CPU. Always ensure proper installation according to the manufacturer's guidelines to maintain optimal performance and safety.",Intel,2025-07-08 21:50:47,1
Intel,n22gizs,"u/TerminalCancerMan  Intel cannot comment or interpret results from third party benchmark tools. RunÂ [IntelÂ® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html)Â to confirm if there are any issues with the CPU. You may try this, If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.Â  If the instability ceases with Turbo disabled, it is likely that the processor need a replacement.",Intel,2025-07-08 22:10:13,1
Intel,n2h78m4,"u/Progenitor3  Just fill in your info, and itâ€™ll automatically create a ticket for you. Our team that handles those items will get in touch within 3â€“5 business days.Â   [Software Advantage Program](https://softwareoffer.intel.com/Support)",Intel,2025-07-11 02:10:36,1
Intel,n2jp4dz,Gotta wait for real-world tests to know for sure tho.,Intel,2025-07-11 13:32:10,1
Intel,n4nvw3m,"u/Special_Ad_7146 To stay on top of Intel news,Â **visit**Â ourÂ [Newsroom](https://newsroom.intel.com/).",Intel,2025-07-23 05:31:29,1
Intel,n523x8x,"u/Hippieman100  just checking can you confirm which software you're using? From what Iâ€™ve seen in the latest [ReleaseNotes\_101.6972.pdf](https://downloadmirror.intel.com/861295/ReleaseNotes_101.6972.pdf), there doesnâ€™t seem to be a â€œSpeed Syncâ€ feature in the current version that comes with this driver. It does support V-Sync and Adaptive Sync though. Just wanted to clarify are you referring to the older Intel Arc Control software or the Intel Graphics Command Center? Just making sure weâ€™re on the same page!",Intel,2025-07-25 09:46:51,1
Intel,n5hzwbu,"u/TheCupaCupa to better understand and isolate the issue, I kindly ask for some additional information. Please find the details requested below:Â   1. When the CPU drops to 0.8 GHz, do you notice anyÂ **error messages or warnings**Â in Windows or BIOS? 2. Does the issue happenÂ **only with LeagueClient.exe**, or have you seen it with other programs too? 3. Have you checkedÂ **CPU temperatures and power draw**Â when the issue occurs? 4. Can you checkÂ **Event Viewer**Â for any critical errors or warnings around the time of the slowdown? 5. Are you using anyÂ **custom power plans**Â in Windows, or is it set to Balanced/High Performance? 6. IsÂ **Intel Turbo Boost**Â enabled in BIOS? 7. **When did this issue first start happening?**Â Has it occurred before? 8. Have you made any software or hardware changes to the system recently?Â   Once I receive this information, I will be able to properly assess the situation and provide further assistance.",Intel,2025-07-27 21:08:02,1
Intel,n6sn19c,"u/Alloyd11 Not all 13th and 14th generation processors show instability issues. Just to better assist you are you planning to buy or use one of these processors, or do you need help with your current system?  Let me know how I can support you!",Intel,2025-08-04 01:26:43,1
Intel,n6sobcw,"u/amitsly â€œcrashesâ€ is a pretty broad term, and not every system issue points directly to the CPU. There are quite a few steps we go through to fully isolate the problem before concluding itâ€™s a processor fault.  To help us assist you better, could you please share a bit more info about the crashes?  * When did the issue first start happening? * Have you made any recent changes to the system either hardware or software? * Is there any visible physical damage to the system? * What troubleshooting steps have you already tried? * Have you noticed any signs of overheating? * Have you tested the processor in another working system, or tried swapping it out to see if the issue follows the CPU?  The more details you can provide, the quicker we can get to the bottom of it!",Intel,2025-08-04 01:34:29,1
Intel,n7crjzp,"u/Jay_377 its  likely comes from Intelâ€™s naming convention. The i3-8130U is part of the 8th gen, but it falls under â€œKaby Lake Refreshâ€ (for mobile chips), not â€œCoffee Lakeâ€ (which is for desktops). Tools likeÂ `garuda-inxi`Â orÂ `CPU-Z`Â might label it differently based on how they categorize architectures, not a bug, just naming differences.  [IntelÂ® Coreâ„¢ i3-8130U Processor](https://www.intel.com/content/www/us/en/products/sku/137977/intel-core-i38130u-processor-4m-cache-up-to-3-40-ghz/specifications.html)",Intel,2025-08-07 03:21:15,1
Intel,n7j7cu2,It is because Kaby Lake CPU's are 7th gen processors for laptops and 8th gen to 9th are Coffee Lake check rhis link for better understanding:https://www.intel.com/content/www/us/en/ark/products/codename/97787/products-formerly-coffee-lake.html,Intel,2025-08-08 02:36:25,1
Intel,n88eg87,"u/Reality_Bends33 Intel has been around for a long time and is known for making solid, reliable products, so you can feel confident about choosing them for your PC build. The Ultra 7 265k and Arc B580 are great picks for video editing, offering the performance you need without breaking the bank. While thereâ€™s been some discussion about Intel facing challenges, remember that big companies like Intel usually keep up with driver updates and support, even if theyâ€™re going through changes. The tech world is always evolving, and Intel is investing in new technologies to stay ahead. So, youâ€™re likely to get the support you need for your products.",Intel,2025-08-12 04:07:16,1
Intel,n8jfndw,"u/unknownboy101 Itâ€™s normal for your processor to heat up during heavy tasks like gaming. Intel CPUs are built to manage heat by adjusting power and speed, so they stay safe and avoid damage. However, running at a constantÂ **95Â°C**Â on your Intel Core i7-12700H even during light gaming isÂ **not ideal**Â and could indicate a cooling issue. While Intel CPUs are designed to handle high temperatures and will throttle performance to avoid damage, consistently running near the thermal limit can shorten the lifespan of your components and affect performance. **Feel free to check out this article for more info or steps to try. Just a heads-up this is specifically meant for boxed-type processors. You can still take a look, but I strongly recommend reaching out to your laptopâ€™s manufacturer to get help with the overall system configuration.**  [Overheating Symptoms and Troubleshooting for IntelÂ® Boxed Processors](https://www.intel.com/content/www/us/en/support/articles/000005791/processors/intel-core-processors.html)  [Is It Bad If My IntelÂ® Processor Frequently Approaches or Reaches Its...](https://www.intel.com/content/www/us/en/support/articles/000058679/processors.html)",Intel,2025-08-13 21:16:44,1
Intel,n8lchtt,"u/Frost-sama96 Tenga en cuenta que solo puedo apoyarlo en inglÃ©s. He utilizado una herramienta de traducciÃ³n web para traducir esta respuesta, por lo tanto, puede haber alguna traducciÃ³n inexacta     **To help me dig a little deeper into the issue, could you share a few details?**  * Whatâ€™s theÂ **make and model**Â of your system? Is it aÂ **laptop or desktop**? * Do you rememberÂ **when the issue first started**Â happening? * WhichÂ **BIOS version**Â are you referring to, the one that works fine? If you can share the exact version , thatâ€™d be super helpful.",Intel,2025-08-14 03:51:06,1
Intel,n8lcvrc,"u/Fluid-Analysis-2354 If your 5070 Ti isn't functioning and your CPU is now your main accelerator, upgrading to the Ultra 9 could be beneficial. The Ultra 9 offers enhanced AI performance, which can significantly improve your computing tasks. If AI capabilities are a priority for you, the upgrade is worth considering.",Intel,2025-08-14 03:53:51,1
Intel,n9tzi8c,"u/BestSpaceBot , As with all good things, your product has reached the end of its interactive technical support life. However, you can find [IntelÂ® Coreâ„¢ i7-4700MQ Processor](https://www.intel.com/content/www/us/en/products/sku/75117/intel-core-i74700mq-processor-6m-cache-up-to-3-40-ghz/specifications.html) recommendations at [Intel Community forums](https://community.intel.com/) and additional information at the [Discontinued Products](https://www.intel.com/content/www/us/en/support/articles/000005733/graphics.html) other community members may still offer helpful insights or suggestions.. It is our pleasure to continue to serve you with the next generation of Intel innovation atÂ [Intel.com](http://www.intel.com/). You may also visit this article for more details [Changes in Customer Support and Servicing Updates for Select IntelÂ®...](https://www.intel.com/content/www/us/en/support/articles/000022396/processors.html)",Intel,2025-08-21 03:26:48,1
Intel,nagz2n7,"u/ken10wil  Â To help figure out whatâ€™s going on with your CPU throttling issue, Iâ€™d like to ask a few quick questions thatâ€™ll help me dig deeper:  * When did you first start noticing the problem? * Have you made any changes to your system recently like installing new software, updating drivers, or swapping hardware? * Is there any visible damage to your PC or loose connections? * Have you updated your BIOS to the latest version for your Gigabyte Z790 board? * Did you try resetting the BIOS to default settings to see if that helps? * Have you tried reapplying thermal paste to the CPU? Just to rule out any cooling contact issues. * Are there any startup programs or services that might be messing with your CPU? * What background processes pop up right after boot? You can check Task Manager or use Reliability Monitor to trace anything unusual. * Can you run theÂ [IntelÂ® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) and let me know if it passes or fails?  Let me know what you find happy to help you troubleshoot further once we have a bit more info!",Intel,2025-08-24 20:24:35,1
Intel,nap6uz8,u/PM_pics_of_your_roof   Interesting thanks for pointing that out! Let me check this on my end and Iâ€™ll post an update here once I have accurate information.,Intel,2025-08-26 02:57:55,1
Intel,navntia,u/UC20175 Let me check this on my end and Iâ€™ll post an update here once I have accurate information.,Intel,2025-08-27 02:26:11,1
Intel,nb7ks0h,"I don't think that there it is awful, I mean in the past where the issue blew out and many consumers are reporting. People at Intel can barely accommodate them but they still give the replacement needed for the consumer.   If i am right, that image is necessary for them to get your serial number and other stuff. Unless you have the box of the processor, or you did not purchase it as a tray then I think you'll be fine.",Intel,2025-08-28 21:50:30,1
Intel,n4u7q4r,I fixed the problem. It was windows msconfig disabling e-cores on boot. You should forbid them from doing this.,Intel,2025-07-24 03:41:03,1
Intel,n527mwk,"I can't remember which (not at my pc), I think it's intel graphics command center. I get a list of v-sync options, off, on, smooth, smart and speed (from memory). Speed was not available in previous versions of the software/driver. Underneath there is an FPS limiter and a control for latency improvement (I can't remember the name) with off, on, and on + boost options available.",Intel,2025-07-25 10:19:53,1
Intel,n53dndg,"Just got back to my PC, it's actually neither Arc Control or Intel Graphics Command Center. I'm using IntelÂ® Graphics Software (25.26.1602.2), should I be using something else?",Intel,2025-07-25 14:36:18,1
Intel,n5i96k3,"Hello, thank you and I'll try my best.  1. No error messages or warnings in Windows, unsure how to check BIOS. 2. Yes, currently I have only found this happens when LeagueClient.exe is started. 3. Using HWInfo64, right when I open LeagueClient.exe, all P and E cores have ""Yes"" in the Current column for ""Power Limit Exceeded"". Core temperatures are: Current- 33C Minimum- 31C Max- 72C Average- 42C. CPU Package Power, minimum is 65.699 W, maximum is 129.230 W. Upon starting the program, the maximum value doesn't change. 4. For Event Viewer, no warnings come up when LeagueClient.exe is started. 5. I have tried setting it to Balanced, High Performance, but I'm mainly on Ultimate Performance. 6. Yes, Intel Turbo Boost is enabled. 7. The issue first started happening 2 days ago, 07/25/25. No it has not occurred before. 8. I have not made any new hardware changes to the system. I did have a windows update, 2025-07 Cumulative Update Preview for Windows 11 Version 24H2 for x64 based Systems (KB5062660) (261.000.4770) installed on 07/25/25, however I installed this update later on during the day after the problem had already started.",Intel,2025-07-27 21:57:08,1
Intel,n6tt5m6,"Well, I didn't immediately blame the CPU. The crash message specifically mentions the issue that point the finger to the CPU, plus the provided link a saying that's the root cause.  Anyhow, here are more details:  1. The issue first started happening about 2 hours into my Expedition 33 playthrough and has happened at least 20 times since (in about 18 hours of gameplay) 2. I have not made any changes whatsoever to software or hardware before it started happening. Last night, after the 20th crash, I did run an update for various drivers & the BIOS (including the 0x12F update) using Gigabyte's CC software. 3. No physical damage that I could observe through the PC's glass window 4. I have tried everything the internet had to offer about this specific issue regarding Expedition 33. This includes verifying game files, changing config and settings, reinstalling, lowering the CPU clock speed and more.  5. I didn't notice the temperature when the crashes happened but I'll be on the look out. 6. I don't have a way to swap out the CPU nor do I have a replacement CPU",Intel,2025-08-04 06:32:53,1
Intel,n7jewjr,"Weird, mine isn't in that list.",Intel,2025-08-08 03:26:09,1
Intel,n8urw7c,"I5 14600k desktop  When i try Odysse ACC, \*game\* I notice the start up and loading in to the game was so slow, like 8 or 10 minutes to Load.    \--VersiÃ³n 182011.06 MB2025/05/21SHA-256 ï¼š493D40A2351EED41FCF60E51346B9065880E87F986C1FD1FB1A3008E8C68DA26  ""Update the Intel microcode to version 0x12F to further improve system conditions that may contribute to Vmin Shift instability in Intel 13th and 14th Gen desktop-powered systems.   Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later. ""-- With this BIOS frrom ASUS page, The game do that   But, when i rollback to>   VersiÃ³n 181211.04 MB2025/03/28SHA-256 ï¼š98528167115E0B51B83304212FB0C7F7DD2DBB86F1C21833454E856D885C7EA0  ""Improve system performance and stability      Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later."" Â Intel microcode 0x12B t  The game Load A lot faster and run well.  I dont know what this happens, but is a error caused by New microcode",Intel,2025-08-15 16:11:42,1
Intel,nai5re4,"\- I started noticing the problem between august the 19th and august the 20th.  \- I updated the system around the time I \*noticed\* the problem, but I am not sure if that is when the issue started. I reverted the machine to a state prior to the update and the problem still occurs. Any other driver updates or software installs happened after I had already noticed the problem, and were initiated trying to solve the problem  \- There is no visible damage to my pc or wires  \- my Bios is up to date, and aside from turning on resizable BAR months ago for my GPU, my bios settings are default. I reset to default and turned resizable BAR on again just to be sure, and the issue still occurred.  \- I have not, mostly because it would be a hassle and because the CPU has remained very cool. I have manually overclocked it as a temporary solution to the problem and even under these conditions it is averaging 35-40 degrees Celsius when idle and hovers around 50 degrees when under stress  \- While the issue did not occur in Safe Mode, I am not sure which program is causing the throttling. I have disabled all installed startup programs and still get throttled.  \- No unusual processes pop up right after boot, though the sum of all processes hits 15% CPU utilization, stays there for a while and then the CPU throttling begins (all happens in less than 30 seconds after boot). I could see what happens after disabling even the security related startup programs and see if there is any difference  \- I pass the PDT tests but I have abysmal performance on various benchmark tests, and the PDT takes a long time to complete. Overclocking leads to more expected results.",Intel,2025-08-25 00:28:15,1
Intel,napdcv6,"Thank you. I spent some more time looking and it appears VROC on x299 was discontinued sometime in the past two years? Seems intel pushed people towards RST. I guess the question still stands since VROC on X299 has moved into sustaining mode, so drives that maybe came out during that time should still work.      [https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html](https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html)",Intel,2025-08-26 03:41:24,1
Intel,naxwb58,What I've gathered from support so far is documents needed by section 2.14/2.15 of https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html are behind a premier account registration. I'll try registering an account.,Intel,2025-08-27 13:04:17,1
Intel,nb7ugs2,"I'm mostly referring to the fact they've designed the matrix to be printed near invisibly with it being absolutely miniscule and basically the same colour as the pcb, then made it a mandatory requirement for the RMA process.   It reeks of a company trying to dodge responsibility for faulty products by making the hoops so difficult to jump through theres no reasonable expectation the average consumer will be capable of complying.   Why do I need to go out and purchase a macro lens for my smart phone? Its putting me out of even more money than I already am.",Intel,2025-08-28 22:43:46,1
Intel,n5hx86x,"u/Hippieman100 **Intel Graphics Command Center**Â andÂ **Intel Arc Control**Â have been the go-to software for many users, but theyâ€™re being phased out soon, and support will be limited moving forward.  Since system (not your PC) includes anÂ **Intel Arc B580**, I highly recommend switching toÂ **Intel Graphics Software** instead. This newer software is bundled with the graphics driver package, which you can find at the link Iâ€™ll provide-[IntelÂ® Arcâ„¢ & IrisÂ® Xe Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html). Before making the switch, please take a moment to read through all the details and driver descriptions on that page.  Also, based on my checks, it looks like you're currently usingÂ **driver version 25.26.1602.2 on your PC**, which is outdated.  Let me know if you have any questions.",Intel,2025-07-27 20:54:17,1
Intel,n5q60xv,"u/TheCupaCupa If the motherboard BIOS allows, disable Turbo and run the system to see if the issues continues.Â  If the instability ceases with Turbo disabled, please let me know.",Intel,2025-07-29 02:44:56,1
Intel,n6y9ah3,"u/amitsly For further analysis, please provide the crash dump or log files generated by the game. You can follow the guide and scroll down to the section titledÂ **""Crashing/Freezing Issues/BSOD""-**[Need help? Reporting a bug or issue? - PLEASE READ THIS FIRST! - Intel Community](https://community.intel.com/t5/Intel-ARC-Graphics/Need-help-Reporting-a-bug-or-issue-with-Arc-GPU-PLEASE-READ-THIS/m-p/1494429#M5057)Â for instructions.  Once youâ€™ve obtained the files, kindly notify me so I can send you a private message to collect the logs.  For isolation purposes, please try the following step and let me know the outcome:   **If your motherboard BIOS allows it, disable Turbo Boost and observe whether the system crashes continues.**",Intel,2025-08-04 22:17:06,1
Intel,n7jf4yf,Yeah that's weird try checking qith laptop manufacturer about it,Intel,2025-08-08 03:27:44,1
Intel,nanj5h3,"u/ken10wil  Thanks for the detailed info - this is really helpful for narrowing things down. What I'm seeing here points to a software issue rather than hardware failure. The sudden onset timeline, passing PDT tests, and the fact that safe mode works fine all suggest you're dealing with a software or driver problem.   Your CPU temps are totally normal too, so I can rule out thermal throttling. That 15% CPU usage spike right before throttling kicks in is actually a big clue - something's definitely triggering this behavior.     [Information about Temperature for IntelÂ® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [How to Know the Idle Temperature of IntelÂ® Processor](https://www.intel.com/content/www/us/en/support/articles/000090343/processors.html)  [What Is Throttling and How Can It Be Resolved?](https://www.intel.com/content/www/us/en/support/articles/000088048/processors.html)  For overclocking: Note that the motherboard does have an impact on the ability to overclock.Â Some are better quality and more capable than others. Furthermore, the way the motherboard is set up also impacts the overclockingÂ ability of a particular system (such as liquid cooler vs fan). Please note that if the system was overclocked, including voltage/frequency beyond the processor supported specifications, your processor voids warranty.Â   **Next steps to try:**Â â€¢Â **Check Windows power settings**Â \- sometimes updates mess with power profiles. Set to ""High Performance"" and see if that helps â€¢Â   **Look at that 15% CPU spike**Â \- open Task Manager right after boot and sort by CPU usage to catch what's eating those cycles â€¢Â   **Try disabling Windows security temporarily**Â \- sometimes antivirus can cause weird throttling behavior â€¢Â   **Check Event Viewer**Â \- look for any error messages around the time throttling starts â€¢Â **Check Reliability Monitor**Â \- go to Control Panel > Security and Maintenance > Reliability Monitor to spot any anomaly issues or critical events around August 19-20thThe fact that overclocking fixes it temporarily suggests your CPU is being artificially limited by software, not hardware.   Since you've already tried the obvious stuff like BIOS reset and driver rollbacks, you're probably looking at a Windows service or background process gone rogue. Keep me posted on what you find with that CPU usage spike and reliability monitor - those are likely our smoking guns!",Intel,2025-08-25 21:12:03,1
Intel,navgo8f,"u/PM_pics_of_your_roof  You're absolutely correct about VROC on X299 being discontinued. Intel moved VROC for X299 platforms into sustaining mode within the past two years and has shifted focus toward Intel RST (Rapid Storage Technology) for consumer applications. While VROC is no longer actively developed, it remains supported in sustaining mode for existing users, which means NVMe drives that were released during VROC's active development period (roughly 2017-2022) should still function properly. However, newer drives may work but won't receive official validation or certification. For anyone building new systems, Intel recommends using RST instead of VROC, but existing X299 users can continue using VROC with supported drives from the qualified vendor list. ***Support is now limited to sustaining mode with no new features or drive certifications planned, as confirmed in the support article you referenced.***",Intel,2025-08-27 01:43:49,1
Intel,nb2m74y,"u/UC20175  Per your inquiry:  The necessary tools and firmware files, but they are only accessible through Intelâ€™s Resource and Design Center (RDC), which requires a Premier account.  1. Register for an Intel RDC Premier Account 2. Visit: Intel RDC Registration GuideÂ [https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html](https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html) 3. Use a corporate email address for faster approval. 4. From the RDC, search following Content IDs:  EEPROM Access Tool (EAT) â€“ Content ID: 572162  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162)  Production NVM Images for I210/I211 â€“ Content ID: 513655  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655)",Intel,2025-08-28 03:42:30,1
Intel,nb7yt2f,I guess you are the only one who is having trouble on that part. I mean I haven't seen anyone get mad about it. So I don't really seem to find any fault I mean it is their preventive measure for fake processors that are out there. Just ask for some help to take a picture it's not that hard,Intel,2025-08-28 23:07:57,1
Intel,n5hzs51,"You have misunderstood, I am using intel graphics software already as I said, 25.26.1602.2 is the version number of that software. My driver's are up to date according to intel graphics software.",Intel,2025-07-27 21:07:25,1
Intel,n5u22od,"Hello,    great news, it solved itself. I opened the program this morning and everything was fine. If this happens again, I will try disabling Turbo and see what happens. Thank you for the help!",Intel,2025-07-29 18:03:17,1
Intel,navgz13,So what is the QVL list for x299 VROC? I canâ€™t find that information. I can find QVL for C621,Intel,2025-08-27 01:45:34,1
Intel,nb80g5j,"[This is literally what I am dealing with.](https://imgur.com/a/WBowyjO)  I'm not even worried about posting this image because please tell me how to get a better photo than this with a regular smart phone, and you can barely see there's anything there.",Intel,2025-08-28 23:17:04,1
Intel,n5i13kx,"u/Hippieman100 Ah, I see I overlooked that part! Looks like this is the installer version, my apologies for the confusion since we were discussing three different software options earlier.  Alrighty, since you're using the latest version now, how can I help? Are you running into any issues with the new application, or is there a specific feature that's not working as expected?",Intel,2025-07-27 21:14:17,1
Intel,n5why78,"u/TheCupaCupa Great to hear it fixed itself! If it happens again, trying Turbo off sounds like a good plan.",Intel,2025-07-30 01:29:50,1
Intel,navj4sy,"u/PM_pics_of_your_roof  You're encountering a common issue, Intel doesn't maintain a centralized QVL (Qualified Vendor List) specifically for X299 VROC at the platform level. Since X299 is a consumer chipset, the VROC compatibility lists were typically maintained by individual motherboard manufacturers rather than Intel directly. The C621 QVL you found is for Intel's server/workstation chipset, which has more formal validation processes. For X299 VROC compatibility, you'll need to check with your specific motherboard manufacturer (ASUS, MSI, Gigabyte, etc.) as they would have maintained their own compatibility lists during VROC's active period. However, since X299 VROC is now in sustaining mode, many manufacturers may no longer actively update these lists. Your best bet is to search for your specific motherboard model's support page or contact the manufacturer directly, though given the discontinued status, this information may be limited or archived.",Intel,2025-08-27 01:58:06,1
Intel,nbi5a7l,Relax ðŸ˜… and follow this:https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html,Intel,2025-08-30 15:16:56,1
Intel,navjsxu,"Thank you for the reply. Sadly asrock doesnâ€™t have a QVL for nvme drives in U.2 form factor or enterprise grade drives, just m.2. Let alone a QVL for VROC support. Looks like Iâ€™ll be the test subject for this.",Intel,2025-08-27 02:02:02,1
Intel,nbknxhv,"That is what I'm following, I provided support everything on that list, including decoding the matrix on the front of the chip but they don't proceed with the ticket until they get a picture of the code on the pcb which is nigh on impossible.  I wound up taking it into the office to use a high resolution scanner thats used to digitise paintings and just about got something readable. I remain firm in my stance that whilst wanting to verify a serial number isn't outrageous, making the process in which you verify said information virtually impossible is.   I simply don't understand how regular people can take such a detailed photo using smartphone technology. My phone is 4 years old and was absolutely not capable (Samsung S20+)",Intel,2025-08-30 23:14:21,1
Intel,navmgeg,u/PM_pics_of_your_roof  **Best of luck with your configuration!**Â Your pioneering work might just pave the way for others looking to implement similar setups.,Intel,2025-08-27 02:17:54,1
Intel,nbpwuw1,"u/Danderlyon  **just sent you a message, check your inbox when you get a chance!**",Intel,2025-08-31 20:03:06,1
Intel,mxdyomr,"I hope this rumor is true and if it is, that it will be as interesting a card and the B580 was.  Then i might finally be able to replace my RTX2080.",Intel,2025-06-12 14:34:07,27
Intel,mxeeplx,"Sounds cool, but not sure how this will stack up against the competition by then. Must offer at least 4070S level performance well below $400.",Intel,2025-06-12 15:50:25,6
Intel,mxe6yrp,"I do hope it won't be too long. We need Intel to shake more in GPU market to destroy Nvidia and Amd duopoli. They already did good thing with Arc B580, they need to do more for competition and i hope more people will jump into Arc!",Intel,2025-06-12 15:13:55,15
Intel,mxe7s3f,Stay tuned!! ðŸ‘€ðŸ«¡,Intel,2025-06-12 15:17:47,19
Intel,mxdyu1q,"If they release also 32GB PRO version for good price, this will sell like a hot cake :) For gaming the 9060XT will be strong competitor.",Intel,2025-06-12 14:34:52,7
Intel,mxkx3r4,"They are simply waiting too long to release dgpus. Both Nvidia and AMD are working on the successors to their currently released and successful products, meanwhile Intel still needs many more months to release a flagship follow-up to a product released in 2022 that was already a mid-range, at best, entry.     We need a more confident release schedule for these Intel gpus. The new leadership at the company is slashing things that aren't profitable, so they can't really afford to take these long shot ill-timed risks.",Intel,2025-06-13 15:42:15,4
Intel,mxnia5a,Right before the Super series and the AMD response. Way too late.,Intel,2025-06-13 23:29:30,4
Intel,mxf1ikk,"Iâ€™d like an upgrade for my a750, especially more ram.  Iâ€™m hoping this is true   I was quite disappointed with the b580 launch that we didnâ€™t get an a750 upgrade.",Intel,2025-06-12 17:38:38,2
Intel,mxjufpg,Lets hope the cards are available for purchase because finding a B580 is hard AF,Intel,2025-06-13 12:20:23,1
Intel,mxlx9as,Hope Intel can work on their drivers a bit more and these A770 are successful at a good price range.  Getting tired of crap Nvidia is pulling of with their pricing.  AMD had their chance but like a bunch of lemmings they follow a similar pricing strategy.  No wonder Nvidia has a 90 percent market share.,Intel,2025-06-13 18:33:06,1
Intel,mxr01zh,I am still waiting for it but if the 9060xt gets near 300â‚¬ Intel will be late (again) for me.,Intel,2025-06-14 14:55:38,1
Intel,mxeq1ns,Make something as powerful as the 5090. Then maybe I'll be interested.,Intel,2025-06-12 16:44:50,-7
Intel,mxez33w,"I switched from 3090 to a B580 because the 3090 was an overkill for me anyway. I only play Valorant and CS2, CS2 isnâ€™t running as well as Iâ€™d like because you cannot play with vulkan on FaceIt but itâ€™s serviceable enough on 360hz screen. I really love this card for how little I paid for it - I just walked into store and walked out 2min later with this unsuspcious cardboard box",Intel,2025-06-12 17:26:54,4
Intel,mxylqgp,"You better have a beefy CPU, the overhead will be worse than the b550, unless they fixed the problem.",Intel,2025-06-15 19:45:03,1
Intel,mxe9x63,Arc B580 only got rave reviews because Intel priced it so low that it is unprofitable.,Intel,2025-06-12 15:27:58,2
Intel,mxe6k1m,"Not even 9060XT competitor but Arc B7XX 32Xe cores with PCI-E gen 5 actually could competes with Amd 9070. That's because the B580 could beat the Nvidia 5060Ti 8GB in 1440P, but it's only losing to 5060Ti 16GB and 8GB version when Vram isn't exhausted, even when B580 lose it only lose by 20-30 fps.Â    Also keep in mind the Arc B580 only has 20Xe cores and it can competes with RTX 5060Ti which is faster than RX9060 XT 16GB/8GB so the B780 could competes with RX 9070.",Intel,2025-06-12 15:11:58,2
Intel,n2a3vse,Absolutely insane how bad Intel's timing has been on this. Two whole years to release a card that barely beats their previous flagship but has less vram.,Intel,2025-07-10 00:46:08,1
Intel,mxftd5g,You are not the target audience of these products..,Intel,2025-06-12 19:49:32,8
Intel,mxf0m2m,"A downgrade would suck for me, because i play a lot of different games that goes from AAA to random indie games.  But i do have luxury of time given that my 2080 still serves me fine, but some more vram would be very nice.",Intel,2025-06-12 17:34:19,6
Intel,mxn3dit,"Iâ€™m curious on your reasoning (not judging). If you already had the 3090, why downgrade?",Intel,2025-06-13 22:04:32,2
Intel,mxebzpy,"Nope. The biggest reason it got rave reviews was because the architectural leap from Alchemist to Battlemage was huge. That, and 10GB VRAM on the lowest SKU. People are just itching for something to challenge the duopoly in GPUs. Although yeah, the price was also good.Â    And there's no telling it wasn't unprofitable. Way lower profit margins than its competitors for sure, but Nvidia and AMD do aim for some high margins.",Intel,2025-06-12 15:37:47,24
Intel,mxhtwpq,"I think they had to. There was no way they couldn't acknowledge how dismal the A series cards performed in General relative to the specs they had. Making the b series so cheap was likely the only way they would get them moving.  We have seen the massive leap they got with the B series, now they have people watching.  I have my fingers crossed that what comes next will be worthy of attention, but I am also prepared to be disappointed.  Let's just see what happens next.",Intel,2025-06-13 02:26:13,2
Intel,mxgidcr,"Unprofitable or not but their main focus is to get market share first, once they build that and got many people buying their products then they could make more profit by either increasing the price a bit, or they could use their own node. Knowing the rumors so far then it would be totally makes sense for Intel to use 18A-P. At the end Intel will get so much profits once they got bigger market share.",Intel,2025-06-12 21:55:59,0
Intel,mxecyu1,"I'm not confident the B770 can achieve 9070 or 5070 tier performance. B580 is way less performant than 5060. But 5060 Ti is barely any more powerful than 5060, and so is 9060 XT 16GB. I think it's going to be easy for B770 to outperform both, comfortably sitting in the $400-ish price range.Â    B780 is never happening. Maybe we'll see a B750 variant, but higher tier than B770 is definitely not. We'll be jumping straight to Celestial in the distant future.",Intel,2025-06-12 15:42:19,5
Intel,mxe7yj0,LMAO,Intel,2025-06-12 15:18:39,8
Intel,mxeeucn,"> Not even 9060XT competitor but Arc B7XX 32Xe cores with PCI-E gen 5 actually could competes with Amd 9070.  No, B580 is slower than both the 9060XT 16GB/5060Ti 16GB and 9060XT is half the size of a 9070XT and the 9070 is ~10% slower than the 9070XT.  32/20=60% more cores, at best a 32Xe core GPU is going to perform like a 4070 / 7800XT going by TPU's relative performance, which would put it roughly in between the B580 and 9070, and this is assuming there's no issues with driver overhead (Which even the B580 has).  I'd expect this new GPU to be around 15-25% faster than the 9060XT 16GB/5060Ti 16GB.",Intel,2025-06-12 15:51:02,5
Intel,mxedswo,"What I wanted to say is that they announced B580 24 GB There are a lot of use cases in AI for home users so B770 16GB is late, if this would be released sooner the impact would be bigger. Same for B770 32GB if they release it for a good price there is no cheap competition on the market.",Intel,2025-06-12 15:46:12,1
Intel,mxn4ao3,"I bought it for extra cheap off a work friend because the fans died. Replaced the entire shroud and used it for like 2y or whatever. In that time I played Valorant and CS2 more and more and I just dedicated myself to these a lot.   The next AAAAAA+ PC game Iâ€™ll play is GTA 6 so Iâ€™ll probably upgrade to something nicer then but for now Iâ€™m good and happy.      There wasnâ€™t much logic behind my decision, I just really wanted to have an Intel GPU and I lost nothing by downgrading.",Intel,2025-06-13 22:09:38,3
Intel,mxgy0jq,"It was a big leap, but that would have meant little if it weren't for the pricing strategy that they used. They're not going to challenge the duopoly without the right pricing and once they have the market/mind share to do so, their prices will rise.",Intel,2025-06-12 23:21:07,3
Intel,mxis3xw,"> The biggest reason it got rave reviews was because the architectural leap from Alchemist to Battlemage was huge.  The architecture was solidly behind *then*-current AMD and Nvidia gens, never mind current ones. And as we see with each new release from those two, the architecture doesn't get you good reviews. Everything positive was 100% tied to pricing.   > That, and 10GB VRAM on the lowest SKU  Also a pricing thing. AD104 (e.g. RTX 4070) has a comparable die size with BMG-G10 (B580), and ships with similar amounts of VRAM.    > And there's no telling it wasn't unprofitable  Intel's been very clear that dGPUs as a whole are still losing them money.",Intel,2025-06-13 06:45:01,2
Intel,mxfnxqp,Intel's Tom Petersen said that it is unprofitable.,Intel,2025-06-12 19:23:32,1
Intel,mxicj9w,idk man the new ceo seems to be cutting down ops that arent making moneh,Intel,2025-06-13 04:31:52,4
Intel,mxh9hp3,"That's not how it works.  R&D is constantly needed to not fall behind, and that has to be funded.  If Intel falls behind, any market share that Intel gained will quickly evaporate into nothing.",Intel,2025-06-13 00:27:18,0
Intel,mxha1kf,"He said it's not making money yet and that could mean many things. AXG was bleeding a lot of money from its establishment to right before its dissolution, reorganization, and Alchemist's launch. Around $2-3.5 billion, and the thin margins of the GPUs aren't going to have that covered. I think Alchemist A580 and A750 were legit sold at a loss, back when you could see them for around $150 years ago. But Battlemage's situation isn't that bad.",Intel,2025-06-13 00:30:35,5
Intel,mxjrt2j,"Exactly, it's funny how he keep spreading misinformation when Tom's didn't even say ""they aren't making money"", that's totally different meaning from ""they aren't making money yet"" but hey, knowing his post and comments history it seems like he is on mission to spreading every false news about Intel to pump up Amd stock.",Intel,2025-06-13 12:03:08,1
Intel,mxep5pr,Why tough? It's not even needed? Intel hasn't even managed to saturate PCI4 yet...,Intel,2025-06-12 16:40:33,2
Intel,mxev51k,Makes sense to invest RnD into PCI5 now since itâ€™s the latest standard for when they do have cards which can utilize it fully.,Intel,2025-06-12 17:08:49,21
Intel,mxhrtvp,It's probably for the pro version of the B770 which is likely called the Arc Pro B70,Intel,2025-06-13 02:13:51,5
Intel,mxghnwx,Work started already on PCIE8. So it makes sense to get into the latest standard. https://videocardz.com/newz/pci-express-7-0-official-specifications-released,Intel,2025-06-12 21:52:15,3
Intel,mxiaaud,"It helps when lane count or PCIe version is limited, by supporting 5.0x16 youâ€™re minimizing the chance of a PCIe bottleneck which I believe was a concern for Alchemist cards when tested on lower end CPUs.",Intel,2025-06-13 04:14:59,2
Intel,mxsq6p0,"This is for data center, not consumers",Intel,2025-06-14 20:22:56,1
Intel,mxij2b1,Thereâ€™s no RND intro Gen 5 - itâ€™s a fully last gen solution at this point with off the shelf IP available for years now.,Intel,2025-06-13 05:24:16,1
Intel,muuh7yl,Would this pretty much mean there's certainly a b770 around the corner?? I hope it does.,Intel,2025-05-29 07:50:53,8
Intel,muvm5c9,Alteady late!!!,Intel,2025-05-29 13:12:28,4
Intel,mzwqlno,"This is promising, considering the B580 BMG-G21 was added to the Mesa drivers about 3 months prior to release, in August of 2024.    Going off the same timetable, it might actually be possible for the B770 to get a release around September... which probably means late November announcement like the B580.",Intel,2025-06-26 16:33:12,1
Intel,muyr9sj,It is...  ...until it isn't,Intel,2025-05-29 22:17:17,2
Intel,mv0zeab,"With how greedy Amd and Nvidia selling utterly trash overpriced 8GB GPU more than $300, i would say never too late for Intel. If they sell 24GB B770 arround $500 then they already winning.",Intel,2025-05-30 07:02:48,4
Intel,mv078ir,Arguably part of Intel's problem is they've had two vastly different teams/designs for AI chips vs client dGPUs. And it sounds like they're on essentially the 3rd *team* on the AI side.,Intel,2025-05-30 03:14:52,7
Intel,mv0idty,"AI/ML is already dominated by Nvidia and Google's TPUs. AMD and Nvidia dropped the ball for gaming so Intel has some chance there. B580 sold well, it speaks for itself.",Intel,2025-05-30 04:34:24,2
Intel,mv1agne,NVIDIA wouldn't exist without consumer GPUs,Intel,2025-05-30 08:51:33,2
Intel,mv227sl,"not really how it works. Nvidia grew from fixed blocks to programmable compute to fix function tensor all delivered through both consumer and enterprise. source code and IP blocks can and should be reused if you want to ship something within the next 10 years. there is plenty of overlap between consumer and enterprise IP.  you can effectively bulk purchase gpus from intel if you're a large partner/SI, though I have no idea how well this would perform at scale. They also need money, which is a big problem when nvidia is sinking tens of billions into multiple reticle limit dies. TSMC is suggesting people offload work to intel, that's how overbooked they are, so they clearly do not have room for another MCM competitor.",Intel,2025-05-30 12:31:20,1
Intel,mv6i7qk,> If they sell 24GB B770 arround $500  Wouldn't that completely invalidate the B580 they're *already* planning on selling for 500$?,Intel,2025-05-31 02:25:31,0
Intel,mwd3utx,MSRP for the B580 is $250,Intel,2025-06-06 18:48:22,1
Intel,mwdbhvy,"I was referring to the 24GB version. Why would they sell a pro B580 for the same price as a gaming B700, both with the same memory capacity?",Intel,2025-06-06 19:26:46,2
AMD,nbfgjnj,I'm waiting for the AMD 3100x3d,hardware,2025-08-30 02:48:16,125
AMD,nbfrgj8,Waiting for a special edition die-shrink of the 5800x3d as a last hurrah of AM4.,hardware,2025-08-30 04:06:58,47
AMD,nbfuaux,it's not available worldwide :(,hardware,2025-08-30 04:29:21,14
AMD,nbi98bu,"I don't really get what AMDs strategy is here. They have gone 5800X3D, 5700X3D and down the stack to 5500X3D while the those earlier higher performing models are largely gone and not made anymore we get lower performing ones introduced.  We get these while X3D is not coming to lower end CPUs in the 7000 and 9000 series. It can not be the case that this is a higher margin way to sell the cache silicon add-on of X3D.  Are AMD just trying to shift old poor quality dies they have laying around?",hardware,2025-08-30 15:36:30,13
AMD,nbfpr3d,I'm lazy why doesn't gamers nexus just have an average data slide,hardware,2025-08-30 03:54:06,35
AMD,nbfktrf,"AM4 : ""I didn't hear no bell"" ðŸ’ª",hardware,2025-08-30 03:18:07,23
AMD,nbsunyi,give me 5950x3d to replace my 5950x so it can catch up with my rtx4080 somehow,hardware,2025-09-01 07:43:39,3
AMD,nbgu9op,"With those crazy fps we currently get, reviewers should focus more on 0.1% lows instead of 1%. 1% were good enough when average framerates were <60.",hardware,2025-08-30 10:04:57,7
AMD,nbh6cyf,"Charts are missing the 5800XT which is currently around the same price as this ($205 USD). Would've been nice to include it to see if the 8-core non-X3D part is a better value. The 5950X is in the charts, but you never know whether games will act funny due to the 5950X having 2 CCDs.  The 5500X3D has a very poor showing in productivity so perhaps the 5800XT would just be a better all-around pick especially if playing in 4K.  All-core sustained clocks:  * [8C] 5800XT: 4500MHz (via HUB) * [8C] 5800X3D: 4300MHz * [8C] 5700X3D: 4000MHz * [6C] 5600X3D: 4350MHz (This is why the 5600X3D is better than the 5700X3D in some game benchmarks despite the core deficit.) * [6C] 5500X3D: 3950MHz  It seems that the 5500X3D is just poor quality silicon if it can't even hit 4GHz. That's a huge clock speed deficit compared to the 5800XT and 5800X3D. When the 5800X3D first came out, it spanked the 5800X even though it was clocked 200MHz lower all-core (4500MHz vs 4300MHz).  But 5500X3D vs. 5800XT? I'm not convinced the X3D CPU is better. The 5800XT is top-binned silicon and could be OC'd to 4.9-5.0GHz all-core. The 5500X3D is stuck with 2 fewer cores and 3950MHz (you can't OC X3D CPUs). That's a ~1GHz clock difference and I don't believe the +64MB 3D V-cache makes up for this. The 5800XT still has 32MB L3 cache; it's not a Celeron. For those on a budget, the 5800XT looks even more attractive because it has gone on sale for $125-150 and comes with a decent cooler.",hardware,2025-08-30 11:49:49,8
AMD,nbfev7n,"Its awesome to see AM4 still alive, while Intel requires a new mobo for every generation...",hardware,2025-08-30 02:36:56,15
AMD,nbfqcrc,"Remember the thread from a couple days ago when everyone said that Hardware Unboxed cherry picked the benchmarks to show that 6 cores CPUs keep up with 8 core CPUs in gaming and even when there's an improvement, it usually wouldn't be large enough to be noticeable with the naked eye? Funny how nearly all the benchmarks here show the same thing.",hardware,2025-08-30 03:58:35,7
AMD,nbgl2cb,Looks like AMD's marketing department figured out how to make product focused videos instead of drama again.,hardware,2025-08-30 08:33:23,-4
AMD,nbi6szz,how has there been no 7600x3d or something yet thats not a best buy exclusive  i think a 199 USD  x3d chip on the 7 series would be an AMAZING seller,hardware,2025-08-30 15:24:24,0
AMD,nbfmj6f,Alternative title:   YouTuber who never used AM4 long-term declares AM4 is the GOAT platform because AMD keeps releasing new defective CPUs to maximize profits.,hardware,2025-08-30 03:30:15,-44
AMD,nbfhhcu,"Factory reject CPU that should not exist, on a stone age platform.",hardware,2025-08-30 02:54:44,-42
AMD,nbfvdkz,Backport 3D cache to Bulldozer.,hardware,2025-08-30 04:38:08,95
AMD,nbfj7pv,I'm waiting for the budget Ryzen 3 2200G3D,hardware,2025-08-30 03:06:50,12
AMD,nbfgztc,Single core,hardware,2025-08-30 02:51:23,6
AMD,nbih6nd,I'm waiting for the AMD 3150c3d. Let's see how zen1 holds up,hardware,2025-08-30 16:16:17,1
AMD,nbg8kr0,"nah, zen4 or zen5 on am4 package. 100% it is doable.",hardware,2025-08-30 06:34:06,17
AMD,nbfxjo0,Yet.   AMD has done this in the past with regional or retailer specific launches before quietly releasing to the wider market after a few months.,hardware,2025-08-30 04:56:15,-1
AMD,nbibixl,"> Are AMD just trying to shift old poor quality dies they have laying around?  Pretty much. All of the newer AM4 CPU releases are low-binned CPUs. The exception is the 5800XT which is a slightly faster 5800X. (The 5900XT is a slightly worse 16-core 5950X, not a better 12-core 5900X, adding to the confusion.)  The 5600X3D has higher clock speed than the 5700X3D, but 2 fewer cores. So the silicon is better on the enabled cores, and it wins on some games where clock speed matters more. The 5500X3D has lower clock speed _and_ 2 fewer cores, making it the lowest quality silicon of the bunch.",hardware,2025-08-30 15:48:03,16
AMD,nbpyrw6,>Are AMD just trying to shift old poor quality dies they have laying around?  Pretty much. Stockpile dies not good enough for 5700X3D and sell them late.,hardware,2025-08-31 20:13:05,2
AMD,nbugavn,"Given that I thought the x3D packaging has a non zero failure rate.   Like, does breaking the CPU dies by putting the vCache on them allow them to write the broken dies off on taxes? While letting them sit in inventory unsold doesn't?   Like, obviously being able to sell them as working products is better, but. I mean, if the issue is they have unsold 5500 level CPUs, I would be more worried about. Breaking them with the die stacking, which I thought was something that could happen.",hardware,2025-09-01 14:46:58,2
AMD,nbfudqm,"My guess is because their business relies on YouTube metrics and if people just skipped to the slide, it could be financially bad for them.   Also, there's nuance that overall average performance can miss. Maybe a part excels/sucks in one genre of game specifically and it skews the average",hardware,2025-08-30 04:30:00,49
AMD,nbjt2tl,Yea 0.1% is better for sure. Frame time health would be best but only DF does that.,hardware,2025-08-30 20:20:47,3
AMD,nbheq8o,The 0.1% lows are still crazy fps. At this point fps is a solved problem in non RT games.,hardware,2025-08-30 12:47:20,-6
AMD,nbhltrj,"Better value for gaming - i don't think so  If productivity is a real concern, a high core count dual CCX cpu is probably needed anyway (I also don't think many who used am4 for productivity need/want to change cpu at this point)",hardware,2025-08-30 13:30:53,5
AMD,nbjbv3e,"Why would you spend a 200 on a slightly binned non-x3d zen3 8-core? Like only ppl who think about am4 are ppl wth zen1/+/2 cpu:s and at that price point, why would you make that upgrade. The whole meme about the XT cpu:s is that it's $50 for letter and nothing else, can just manually OC a 5700X to be near the same or more/less cores for more/less money if you care/don't acre about productivity.  Just don't get a 5700 it's a 5700G without the IGPU, great naming amd!",hardware,2025-08-30 18:49:32,0
AMD,nbfj34s,I mean this is no different than if intel released a 14599K with a 5% performance reduction to the 14600kâ€¦ simply repackaging worse dies on a bad yield CPU isnâ€™t anything special,hardware,2025-08-30 03:05:57,64
AMD,nbg950v,"proper ddr4 ram is very expensive though if u want that, or if u need a new am4 mobo well there are no good left to be bought. only a couple of meh models left in retail. At least when I sourced the retail channels so to speak.",hardware,2025-08-30 06:39:26,2
AMD,nbh378f,"LGA 1700 supported 2 gens, not bad. Raptor Lake was a really strong upgrade except for the RMA rate. Yeah it's not like AM4 which supported 4 gens, but i5 Raptor Lake beats i9 Alder Lake in gaming. Very rarely we see such leap within a single gen.Â    LGA 1851 was supposed to support 3 gens but became a single gen platform because Meteor Lake-S and Panther Lake-S were cancelled, perhaps for a good reason, which is the chips were very mobile focused and scaled poorly on desktop as shown by ARL-S only achieving low 5GHz and poor ring clock speed and latency.   Considering intel's current financial difficulties, I'm sure it was considered a waste of resource to design and validate desktop chips that aren't amazing on anything but power efficiency. LGA 1851 still supports 3 gens on mobile, where power efficiency matters the most.   LGA 1954 will likely support 3+ gens again, like LGA 1851, but with all the products coming to desktop. The first desktop CPUs for it, Nova Lake-S, are definitely coming. The future gens reportedly will feature unified cores, which is good news for desktop. No more p-core/e-core gap.",hardware,2025-08-30 11:24:59,3
AMD,nbfl2j8,This is the main issue Intel needs to fix for me. Core2 duo was my last Intel they'll need to do what AMD does,hardware,2025-08-30 03:19:49,5
AMD,nbg778m,There is Bartlett Lake. Same deal as this. Double interesting too that no one calls AM4 a dead end platform when talking about buying it's low end CPUs,hardware,2025-08-30 06:21:15,4
AMD,nbgeobh,"HUB has consistently claimed that new gen 6 cores are always faster than previous gen 8 cores, dont try to twist facts by changing it to ""keeping up"". And in this video we can clearly see that's not the case in BG 3 where a 5700x is slightly faster than a 7600 even though the latter has higher frequency and uses way faster memory with higher bandwidth.",hardware,2025-08-30 07:31:31,18
AMD,nbfsktg,"The people claiming otherwise literally provided 0 evidence from their own hardware or any benchmarks from any reviewer.  ""You're suppose to have 30 tabs open, 4k utube video, rendering a vid, discord, spotify, netflix and benchmark at 720p""  But 14600k is better than 7600/7500 if you're going to keep ur gpu under 4090 for the next 5 years because you're going to be gpu bound.",hardware,2025-08-30 04:15:37,5
AMD,nbfuo9v,"I thought that was known for a while now, in even some CPU intensive games only utilize up to 4 cores. That's why it was notable when BF6 beta showed capabilities of  actually using all cores when playing.   I'd understand it if people are more afraid of the future when games suddenly release that has that capability, but in the meantime any 6 core CPU with similar spec should be similar in games with higher core CPUs.",hardware,2025-08-30 04:32:22,-3
AMD,nbjdguc,"There is 7600X3D available, maybe not in 'murica, but in Europe it's very available. Obviously not $199 cause why would it be the normal 7600 seems to be $185 USD and the launch price was $299 USD a year ago and it's currently ~$307 converted when removing tax here.",hardware,2025-08-30 18:57:53,3
AMD,nbgc7k2,"well it is not that far off, but I would call it more like. Manufacturers gimps the model u want so that they can release a cheaper version of it instead of offering us the older model for less several years later because it would cut into the sales of the current gen models.     The binning costs money so I guess it is cheaper to just not trying to bin the old am4 cpus and just release them as the new sku with the new lesser binning.",hardware,2025-08-30 07:08:06,7
AMD,nbfou7v,I really hope that title shows up on dearrow because it's the only accurate title,hardware,2025-08-30 03:47:20,9
AMD,nbfvz9o,"That is possibly the most unfavorable, disingenuous way to frame this.   If I ran a successful YouTube channel, and my career was to benchmark hardware and then edit the content - i wouldn't use AM4 either. I'd run a 5090 and at minimum a 9950X3D - possibly even Threadripper. But my career requires Office Suite, a web browser, and RDP, so I use a ThinkPad issued by the company. I'm also sure that GN/HUB probably don't have tons of free time to play games because they have to work more than 40 hours a week to hit their current release schedules.   As for ""releasing new defective CPUs"" - that describes pretty much everything that *isn't* a 9950(X3D) or 285K.  A more reasonable take like ""the release of a 5500X3D doesn't mean AM4 isn't dead. It's just a limited release bin of an existing CPU to clear out the remaining inventory that didn't meet spec"" - I'd 100% agree with you.",hardware,2025-08-30 04:43:10,7
AMD,nbfm0k7,and still matching Intel latest in games,hardware,2025-08-30 03:26:33,26
AMD,nbihjfh,Release a `Socket 7` Ryzen CPU with massive 3d cache.,hardware,2025-08-30 16:18:03,8
AMD,nbg755y,lol. ive forgotten about bulldozer. I dont think ive ever seen one in person.,hardware,2025-08-30 06:20:43,7
AMD,nbjryi2,Would have saved bulldozer tbh.,hardware,2025-08-30 20:14:51,2
AMD,nbhb0ts,opteron 144,hardware,2025-08-30 12:22:42,1
AMD,nbgzcz9,On ddr4 speeds??? You cant balance that even with l3 cache,hardware,2025-08-30 10:52:20,9
AMD,nbg6i45,"Ehhh, Ryzen 5 5600X3D was Microcenter-exclusive and never became worldwide. If they had the stock to do a worldwide release I imagine they would have.",hardware,2025-08-30 06:14:50,43
AMD,nbhddy8,Maybe Steve needs to be more entertaining so people want to watch the video for more than just the slide at the end.,hardware,2025-08-30 12:38:32,27
AMD,nbfxh8e,"It's impressive how Google is acting more and more like a stereotypical monopolistic mega corporation of a cyberpunk dystopian world as of late.  Google search is absolute sheet, YouTube is getting sheetier, side loading (as we know it) is being killed off on Android next year, and Chrome is... well, Chrome.  That's one reason I've migrated to FireFox, switched to DuckDuckGo, and already looking into installing a custom ROM on my Android with zero Google crap.   And YouTube isn't half bad with UBlock + YT Control Panel but... I've digressed enough!",hardware,2025-08-30 04:55:42,-5
AMD,nbjwnbl,"> The whole meme about the XT cpu:s is that it's $50 for letter and nothing else  The 5800XT has gone [on sale for $159](https://www.reddit.com/r/buildapcsales/comments/1lty6e5/cpu_5800xt_included_1tb_mp44l_m2_ssd_159/) as late as last month. That's why. We're not talking about MSRP prices. The 5800XT hasn't been at MSRP price for a long while now.  The last 5700X sale on r/buildapcsales was 9 months ago. So no, the 5700X isn't easily available for cheap anymore, just like the 5700X3D and 5800X3D. Obviously everyone wants the 5800X3D as the endgame AM4 CPU but now they're $450+ even on Aliexpress.",hardware,2025-08-30 20:39:39,2
AMD,nbfk5kv,"Ye, for all intents and purposes AM4 died with the 5800X3D launch. That was the last actual meaningful launch.   Intel will still be selling LGA1700 CPUs for years to come. Doesn't mean it isn't a dead platform, unless Bartlett Lake is actually launched for desktop.",hardware,2025-08-30 03:13:25,40
AMD,nbg9cdz,"yep, remember that amd was not willing to enable support for the zen3 cpus on older mobos until the very end of the am4 platforms life. That was something that many including I thought was such a f-you to us that bought their platform.",hardware,2025-08-30 06:41:20,3
AMD,nbgy0ay,"Intel even did that a while ago, with their non e Core CPUs which were announced at the beginning of this year. So, basically the same, just a slightly worse version of the existing product. But in there no one cared",hardware,2025-08-30 10:40:10,1
AMD,nbgcua8,But they don't,hardware,2025-08-30 07:14:04,-4
AMD,nbidlnr,That is the most negative thing I have heard about a platform having a 10 year longevity. You are good at finding small details in a sea of positive news. lol,hardware,2025-08-30 15:58:30,-1
AMD,nbslvry,"> Raptor Lake was a really strong upgrade except for the RMA rate.  Aside from that, Mrs Lincoln, how was the play?   > but i5 Raptor Lake beats i9 Alder Lake in gaming. Very rarely we see such leap within a single gen.   That's pretty normal, if not underwhelming, for a generational upgrade though? The gap between i5 and i9 in gaming is pretty small to begin with. Hell, depending *which* i5 you get, it's literally just the same ADL silicon.  > LGA 1851 still supports 3 gens on mobile, where power efficiency matters the most.  LGA 1851 is a desktop socket. Mobile uses something entirely different. And what 3 gens are you talking about there? It's just MTL and ARL.   > The future gens reportedly will feature unified cores, which is good news for desktop.  That's probably well after this socket's lifespan.",hardware,2025-09-01 06:23:08,1
AMD,nbh7jj7,"> Very rarely we see such leap within a single gen.   Not really. For Intel that happened with 11->12->13 gens, for AMD non X3D chips Zen+->Zen2->Zen3->Zen4.",hardware,2025-08-30 11:58:10,1
AMD,nbgfgil,"The 9700X is only 0.5% faster than the 9600X on average at gaming  https://www.techpowerup.com/review/amd-ryzen-5-9600x/18.html  >Compared to the Ryzen 7 9700X the 9600X loses by a small 2% at 720p, but it keeps on gaining as resolution increases and beats it at 1440p and 4K by a wafer thin 0.3 and 0.4%. While that's not exactly conclusive, it's strong evidence that gaming performance between those two processors will be virtually identical.   https://www.techpowerup.com/review/amd-ryzen-5-9600x/29.html",hardware,2025-08-30 07:39:16,-3
AMD,nbfxvn9,"Yeah, I looked into this very closely many years ago when choosing PC parts and found that the 5600X performed nearly the same as the 5700X and even the 5800X in most games. So too did the 5600X3D, 5700X3D and 5800X3D. And while games may have gotten more CPU intensive, the 9600X is still very good against 9700X.",hardware,2025-08-30 04:59:03,3
AMD,nbga2x0,"i play with 2500tabs open with a video playing in the background and playing at the same time, even on the hexa core systems and the perf penalty is not at all that bad, pretty much the same perf as an octa core or if I enable the e cores, heck even worse with the e cores on.  But if u render a video or something with the cpu then I guess it gets a tiny bit better on the higher core count cpu, even though it would still make the game unplayable on a higher core count system but for less time than on a hexa core.  People dont understand how it works.",hardware,2025-08-30 06:48:13,-3
AMD,nbhgwvj,"Even if the main event loop in those few games you play do not take advantage more than a handful of cores, there are lots of games now doing shader compiles.    Some new games (e.g. Sony, Unreal) do that at first run or after new GPU driver install,  while better ones run it at the background (can cause stutters) and some older games do that between levels.  Shader compile is using most of my threads in my 5800X and cut back of the wait time and possible stutters.  There are other things people do that requires CPU cores. e.g. emulation, virtual machines, video encoding.  Silly to buy/build a PC for single use case and restrict yourself.",hardware,2025-08-30 13:00:54,1
AMD,nbntg8j,The 7600X3D launched in America as well.,hardware,2025-08-31 13:43:18,1
AMD,nbfpull,>Look guys I know I only do reviews where I test CPUs for a few days with like 1-3 motherboards that are rarely updated with their BIOS revisions and are rarely on but let me tell YOU how great AM4 is.  Absolute cinema. Any sane person with a brain would ignore these tech reviewers.,hardware,2025-08-30 03:54:49,-24
AMD,nbg6kh5,It's also matching the AMD latest 9600x according to the graph.,hardware,2025-08-30 06:15:26,9
AMD,nbgckg7,"does it? 5800x3d lost pretty badly in hubs own testing when they tested against an 12900k and faster ddr5 sticks compared to ddr4 sticks it used before.  I loved my 5800x3d and had no issues what so ever with what same on the net would call amd-dip, but my lga 1700 cpus including my 12700k tuned outperformed my 5800x3d which had tuned b-dies so even higher perf that we see here.",hardware,2025-08-30 07:11:30,2
AMD,nbgcmyh,My first cpu was an amd Athlon 1.53ghz single core.   Then a phenom X 4 9850.   Then a fx 6100. (Bulldozer),hardware,2025-08-30 07:12:09,4
AMD,nbuehp0,"IIRC, there were Excavator CPUs for AM4. The A12-9800 I think.",hardware,2025-09-01 14:37:57,1
AMD,nbueozz,"Having any L3 at all could probably have helped Bulldozer, yeah.   At a certain point though, the question becomes, if you make a big enough change to the architecture, when does it stop being that architecture and start being a new one?",hardware,2025-09-01 14:38:58,1
AMD,nbh6l9j,"Because Infinity Fabric is bottlenecking hard, there is limited benefit from DDR5 for Zen4 and 5. Keeping old Zen3 era I/O die would cost ~10% gaming performance for non X3D chips, even less for X3D.",hardware,2025-08-30 11:51:33,9
AMD,nbm8m1l,"I've been wondering for a while if Zen4 or higher could be back ported to AM4 by adding a HBM module as an L4 cache, but I'm sure the engineers at AMD already ran the numbers and it wouldn't be cost effective.",hardware,2025-08-31 05:43:18,2
AMD,nbgyjcu,"same as 7600x3d, sad times",hardware,2025-08-30 10:45:00,10
AMD,nblu1cr,"I feel like he missed an opportunity to poach Emily Young out of LTT or hire them after quitting. They're a great presenter who has a similar tone but feels more engaging and speaks at a more measured pace. Look at the videos where Emily was host or the foil to Linus' energy.  Of course, I'm not sure Steve's ever gonna pull from LTT at any point in his career, but it's quite surprising how, despite his and his team's improvements in writing, the hosting stagnated.",hardware,2025-08-31 03:44:50,-4
AMD,nbg83ka,You're allowed to say shit.,hardware,2025-08-30 06:29:35,31
AMD,nbg18j9,"4.3 petabytes get uploaded to YT *every day*. I don't think Google's actions surrounding ads or YT Premium have been unreasonable considering that metric.   I pay for YouTube premium because its YT is my most used service. Don't mind the price vs the value.   I also dont think YT's stance on minimum watch time to count as a view is unreasonable. YT's algorithm is based around clicks - and as much as people say we hate click bate titles, they work. Thats more to blame on human nature than Google specifically imo.   But I do also think Google as a company sucks. They lack a unified, creative vision. They extract value from the few hits they had years ago while being unable to replicate past success. They dont care for a tight UX and overall attention to detail, and care only about having a ""good enough"" product to extract user data....  But realistically, only a massive company like Google that has other revenue streams based primarily on user data can even make a service like YT viable at all.   Also I played around with custom ROMS a lot back in the day. If I ever reach the point of taking the next step in de-Googling by life, I'd just switch to iPhone.",hardware,2025-08-30 05:27:45,36
AMD,nbhv0a8,"I mean there is a reason Google lost three different anti-trust cases in like the last year or so (one over web search, one over their ad network, one suit filed by Epic games over treatment of third party stores); trials right now are going on to determine the penality Google will face, hopefully the DoJ follows through on some of their more hefty proposed remedies like forcing Google to sell off Chrome.",hardware,2025-08-30 14:23:09,7
AMD,nbhdsy9,Your just being contrarian hopefully that's just youth as irrational contrarianism isn't a good look for an adult.,hardware,2025-08-30 12:41:15,0
AMD,nbfzwfy,breh your neck beard needs a trim,hardware,2025-08-30 05:16:12,-3
AMD,nbjskpe,"They've always been like this, the moment they made Chrome people called this shit from a mile away. They make money from ads they need as many people to see ads and all the things you listed can get in the way of that. Fuck, Google that's why I never used chrome.",hardware,2025-08-30 20:18:09,0
AMD,nbgjd2b,Nah the 5700x3d because it was actually affordable.,hardware,2025-08-30 08:16:45,14
AMD,nbh3slw,At least you can still find AM4 boards at retail at reasonable prices now. You cannot say the same about Intel platforms from 2017.,hardware,2025-08-30 11:29:46,0
AMD,nbhdyi5,You are forgetting all the 1600X owners that still see any 5XXX series CPU as a great upgrade.,hardware,2025-08-30 12:42:16,1
AMD,nbgy18b,"they did, Q1 this year.",hardware,2025-08-30 10:40:24,3
AMD,nbievkx,"there was a lot of negativity before am4 got obsolete. The biggest thing is that AMD did not want us to run zen3 cpus on older boards which they let us when the platform was end of line.  That was much more negative than the fact that fast ddr4 sticks and the vast amount of am4 boards that once were avaible are not anymore. How many owners of earlier am4 boards were forced to upgrade to a new board when they went with zen3 cpus when they did not really needed that at all.  That was negative, not that some components are rare now.",hardware,2025-08-30 16:04:53,2
AMD,nbghokm,Why are you bringing up averages when HUB literally brought one game?,hardware,2025-08-30 08:00:41,8
AMD,nbgbrmu,omg people were down voting you...wth...,hardware,2025-08-30 07:03:54,-8
AMD,nbisl4q,"yea, pretty good huh",hardware,2025-08-30 17:12:56,2
AMD,nbge344,"never owned a bulldozer/piledriver as at that time a phenom was just as good at the start of those fx cpu life and at the end intel was the only way.     But, my first proper pc was an amd k6 200mhz cpu, which was barley faster than my friends 120mhz pentium. But then athlon came out and omg... those were the times.",hardware,2025-08-30 07:25:49,3
AMD,nbufb63,Isn't Infinity Fabric speed tied to memory speed though? At the same rate or half the rate or something? Or has there been some big change when I wasn't paying as much attention over the past few years?,hardware,2025-09-01 14:42:01,1
AMD,nbh181i,7600x3d can be bought in Europe,hardware,2025-08-30 11:08:41,0
AMD,nbj4uh2,"YouTube was self-sufficient the last time I checked.   Google isn't doing any of this out of the goodness of their hearts.   There's no such thing as free lunch, a simple fact most people seemingly lack the capacity to appreciate.",hardware,2025-08-30 18:14:04,3
AMD,nbh38z2,"> But I do also think Google as a company sucks. They lack a unified, creative vision. They extract value from the few hits they had years ago while being unable to replicate past success. They dont care for a tight UX and overall attention to detail, and care only about having a ""good enough"" product to extract user data....  That description made me think of Microsoft. Strikeout Google and replace with Microsoft and it's just as accurate.",hardware,2025-08-30 11:25:23,1
AMD,nbgrls2,"Don't forget constantly killing good products and somehow having 7 overlapping calendar or task services.  But yeah, I don't think YouTube could exist as an independent company. Not unless they dropped above 1080p resolutions and compressed everything to hell. It's insane that a free service provides 8K60 HDR upload and playback.",hardware,2025-08-30 09:38:35,0
AMD,nbgyobv,it was?,hardware,2025-08-30 10:46:14,1
AMD,nbm6ysb,"what no.... they are crap boards and ddr4 are getting more expensive and they are crap sticks as well. no good am4 stuff left, at least when I looked.",hardware,2025-08-31 05:28:47,2
AMD,nbh6x7l,"So what, so that the 10 people who has a motherboard that breaks long enough after purchases that had they gone Intel it would be EOL can get one?  That is a extremely niche positive. While motherboards do break more often than CPUs. They as all electronics follow the bathtub curve. If it didn't break in the first year of you owning it, it is not likely to break within the reasonable life time of the product either. Baring some glaring issue like the capacitor plague or specific design flaws with that model.  And if you are using that 2017 platform and upgrading it along the way. You will pay a price for it. A 5800X3D will be held back by PCIe 3.0 before it becomes obsolete as a CPU. Do you then get a new board for a dead platform to gain 4.0? Nullifying the main advantage vs going with a newer contemporary Alder Lake platforms or just getting a at the time new 4.0 AM4 board?",hardware,2025-08-30 11:53:55,-2
AMD,nbjalyz,"And why is that? Because AMD started at a very low point. The 1000 > 5000 series jump is not normal in that time period. AMD started at similar performance level of Haswell, which was 4 years old at the time.   Reddit is heavily biased towards entusiasts when it comes to PC hardware discussions. People upgrading every other generation is not the norm.    Had you instead bought a 8700K back in 2017, you would still be using a chip that is perfectly passable today. Sure the fastest AM4 has to offer is considerably faster for things like gaming. But it is not nearly the same leap as someone who jumped on Ryzen 1 and is now comparing vs a 5800X3D. The jump from a 8700K to 5800 series is closer to ""normal"" progress in the 5 year period from 2017 until the last major AM4 launch.  Normally, most consumers would see no reason to upgrade in that time frame. If you bought a 7800X3D at launch, by the time that thing is obsolete for a normal consumer and they would consider a upgrade. You would be looking at a whole new platform. Because the last X3D chip released for AM5 will itself be old news by then.",hardware,2025-08-30 18:43:09,6
AMD,nbslywu,Which CPU are you referring to?,hardware,2025-09-01 06:23:54,1
AMD,nbgi7lw,"Most of the criticisms were that the benchmarks were cherry  picked because the video in question showed one game. Therefore, Gamer's Nexus above as well as the 9600X review from TechPowerUp show an average of games which corroborate HUB's claims. This addresses the criticism of ""cherry picking"" data.  In addition, HUB has made countless of those videos over the years and they had an average of the games. There is a clear trend in the data.  https://youtu.be/7L9rPNSuPCA?si=4zAgnP8t2z1OKEJN",hardware,2025-08-30 08:05:38,1
AMD,nbhefx8,They all bought 9800X3D's and play at 4K and can't admit it was a waste of money.  X3D's only make sense if you are playing on a 5090 at 1080p and only play esports titles. For sane gaming it makes zero difference over a 9600X. We are all GPU limited at the resolutions and settings people actually use.,hardware,2025-08-30 12:45:27,1
AMD,nbhvxbo,"I went with my dad to buy a Compaq in 98 or 99.. it had an amd k5 533 mhz cpu. Lol. He spent 2500 on that pc lmao. It had 128MB SD ram. We had computers before that, like ibms and stuff, but I wasn't into computers then. But yeah, the thuban amd x6 six core cpu i remember was a true 6 core and people liked it more than bulldozer",hardware,2025-08-30 14:28:10,1
AMD,nbunxgl,You can change the mem clock to IF bus ratio around however you'd need to.  It'd be fine.,hardware,2025-09-01 15:24:35,1
AMD,nbh1ay2,not worldwide,hardware,2025-08-30 11:09:22,8
AMD,nbhq2kp,TIL the world is US and Europe,hardware,2025-08-30 13:55:42,2
AMD,nbj544p,">But yeah, I don't think YouTube could exist as an independent company.   Yet their business model is somehow self-sufficient...",hardware,2025-08-30 18:15:26,-2
AMD,nbhii62,"I remember it hitting 170 euros in the netherlands, the 5800x3d was never below 300.",hardware,2025-08-30 13:10:45,6
AMD,nbhe17y,It was like $130 for Ali express at one point.,hardware,2025-08-30 12:42:45,5
AMD,nbk5bqj,it was selling for 200CAD at one point (~160USD?),hardware,2025-08-30 21:26:20,1
AMD,nbtvko4,"Yeah the dual rank, Samsung b-dies are long gone. Also, there are not much AM4 mobo options at this point except for budget B550 mobos out there.",hardware,2025-09-01 12:53:48,2
AMD,nbsrzhe,"Intel released the Raptor lake CPUs, this time without e Cores. Just look at their website",hardware,2025-09-01 07:18:22,0
AMD,nbgiivp,Show me that trend on the 9600x HUB review vs 7700 where the former is significantly faster than the latter on average.,hardware,2025-08-30 08:08:37,4
AMD,nbhjfo8,"yep, I only play at 1080p/1440p lowish with an high end gpu/cpu. Actually had two 4090 but now I swapped over both of mine desktop system both intel and amd x3d to 9070xt because of how well they perform at lower res in bf6 beta and warzone.",hardware,2025-08-30 13:16:28,0
AMD,nbm6ge2,"It was common here in Sweden as well, to get a pc via the job, here the biggest employer was Volvo/Saab and people from Volvo got to buy/lease whatever it was IBM Aptiva machines. A freind got a pention 75 I think and omg what a machine. But then the development started to move in a blistering fast pace. from 486 to pentium at max 120mhz to 1ghz and accelerator cards and then proper 2d/3d video cards. Now it kinda is at a standstill and only the Halo gpus are actually worth the upgrade but they are locked by the mega extreme pay-wall.  yep, those fx cpus were very meh, and even my friend that hates everything that is intel or nvidia used his phenom until am4 came out.",hardware,2025-08-31 05:24:18,3
AMD,nbhwm19,"Not worldwide yes, but not microcenter exclusive",hardware,2025-08-30 14:31:46,9
AMD,nbhwxn0,The point was that its not microcenter exclusive unlike 5600x3d,hardware,2025-08-30 14:33:30,0
AMD,nbhvtwq,"For whatever reason mine was going to be 140 but they had it even cheaper with klarna, So I got mine out the door at 123, then I sold my 5600X for 80 bucks   All in all, a $40 upgrade, which is insane",hardware,2025-08-30 14:27:40,3
AMD,nbsslbz,"Are you talking about Bartlett Lake? So far, at least, those are just lesser versions of the 8+16 RPL die. Maybe some ADL mixed in there as well. Also not available at retail.",hardware,2025-09-01 07:24:03,1
AMD,nbginh1,You can see it in the TechPowerUp benchmarks.  https://www.techpowerup.com/review/amd-ryzen-5-9600x/18.html  A 5600X also consistently outperforms the 3700X.,hardware,2025-08-30 08:09:51,3
AMD,nbie7za,"You missed the crucial ""never became worldwide"" part of that comment.",hardware,2025-08-30 16:01:34,-1
AMD,nbsuydw,"Yes, thatâ€™s what Iâ€™m talking about.  And no, they are available in retail",hardware,2025-09-01 07:46:25,0
AMD,nbh0kt7,Zen 2 is 4 cores per CCX while Zen 3 is 8 cores per CCX. That means the benefits of more cores were nulled by die-to-die latency. All the 6 cores on 5600X exists on one CCX while 3700X is 2x4 cores.,hardware,2025-08-30 11:03:06,4
AMD,nbgirhr,"Techpowerup is a garbage source for cpu benchmark results because they benchmark on low settings instead of max that are more cpu demanding and also hiding % lows metrics per game. Even taking into account those results, 3% difference can barely be called a difference in the first place, way off the mark of significantly faster and more like leaning into flop gen territory.",hardware,2025-08-30 08:10:54,0
AMD,nbky12m,"i didnt miss anything.    I never stated that its worldwide available. I only pointed out that its not microcenter exclusive, so i dont understand why are you trying to correct me so much...",hardware,2025-08-31 00:18:02,4
AMD,nbsxhvb,"> And no, they are available in retail  Where?",hardware,2025-09-01 08:11:05,1
AMD,nbgp9ay,"nah man, it is not that simple. I have not found that many games that actually impact the cpu the higher settings u chose. not in say like bf and wz and the like.   Some games like say gta5 and cp2077 have settings that affect the cpu by increasing the crowd/ai/object population but that issue is not here in mp fps games.",hardware,2025-08-30 09:14:54,2
AMD,nbsxo12,Pretty much everywhere. Just search for them.,hardware,2025-09-01 08:12:46,0
AMD,nbgpwv6,RT/PT require more cpu power aside other settings like crowd density.,hardware,2025-08-30 09:21:26,0
AMD,nb1fff1,Looks like the discoloration is under the IOD.   Also they are using some really cheap and crappy ASUS boards(like the VRM on those is probably hitting well over 100C with a 9950X at 100% load). Though if the motherboards still work I wouldn't necessarily blame them.  EDIT: Just for clarity. The VRM running well over 100C isn't going to hurt the CPU until one of the mosfets dies which will kill the motherboard and might kill the CPU.  EDIT2: if the motherboards are broken I would go and check if CPU 8pin power connector isn't shorted out. If there is a short chances are the motherboard sent 12V to the CPU.,hardware,2025-08-27 23:24:38,55
AMD,nb1734z,Interesting. Would be awesome if AMD could review this.,hardware,2025-08-27 22:38:46,13
AMD,nb38snz,">These are supposedly top-quality motherboard   These are trash tier boards with discrete mosfets for power delivery. Wholly incapable of sustained power for a 9950X. The top 2 sets of MOSFETs have NO cooling. I don't have a diagram for this board, but if any of those are VSOC and they have a highside FET fail short, you will shoot 12v directly into SOC and explode whatever it is. Using these boards for sustained 230W PPT (AKA AMD's 170W TDP) is kinda insane.",hardware,2025-08-28 06:51:42,27
AMD,nb1ke67,"1. The problem of burning AM5 CPUs is not limited to ASRock and the community at large is failing by either supporting that myth or actively not dispelling it. 2. Those motherboards will not sufficiently power that CPU. The CPU will simply not boost as high (HUB has demonstrated this in several videos, 1 of which is very recent and a part 2 is expected) or the motherboard VRMs should burn out. This is a bit of ignorance on \*gmplib part. Asus has their loyalist. 3. [The 9950X can suck down 270 watts](https://hwbusters.com/cpu/amd-ryzen-9-9950x3d-cpu-review-the-top-multi-core-gaming-processor/attachment/power_consumption_peak-40/), well past the abilities of the the Noctua NH-U9S to cool, which they admitted to though they went with advertised numbers instead of tested numbers. The CPU should have throttle. That's a double whammy of failures.",hardware,2025-08-27 23:52:38,45
AMD,nb1hvt7,> 9950X  > Asus Prime B650M-K  > Asus Prime B650M-A WIFI II    https://imgur.com/a/As6wT53  and when that blew up they bought this very different one  https://imgur.com/a/B61teYf  hmm...  It's a mystery  Steve what do you think?  https://youtu.be/ZtHOOyWYiic?t=274,hardware,2025-08-27 23:38:29,20
AMD,nb5zh86,"That reminds me of the burn out we saw earlier, from high SoC voltage, but I thought that was fixed with efi updates",hardware,2025-08-28 17:14:35,3
AMD,nb18qwz,Where's GN and HUB pristine journalism over this hot report ? Steve & Steve must become specialists of Multiple Precision Arithmetic ASAP ! Melting Zen 5 doing math is no joke.,hardware,2025-08-27 22:47:56,12
AMD,nb23287,"was PBO enabled in this system? It is known that higher PBO settings can cause relatively fast degradation, even back to Zen3.",hardware,2025-08-28 01:41:09,4
AMD,nb3c0pw,this needs fast clarification,hardware,2025-08-28 07:21:48,1
AMD,nb3c7rw,this needs fast clarification,hardware,2025-08-28 07:23:39,-1
AMD,nb9cos3,This is why you shouldn't buy AMD for serious work.,hardware,2025-08-29 04:16:47,-2
AMD,nb1vpvt,The picture quality isn't giving me confidence in their reporting tbh.,hardware,2025-08-28 00:58:34,-7
AMD,nb5k033,"Asus and high voltages? Name a better duo.  The X670E Crosshair Hero that I had before, had a lot higher voltages than the MSI X670E Carbon wifi, to the point that with the same settings, the MSI system was 10-12ÂºC cooler with the same CPU cooler and CPU. (CPU core temperature)  This was a overpriced 400â‚¬ motherboard, now imagine on a crappy sub 100â‚¬ motherboard like the ones they used without VRM heatsinks, that's like dumping gasoline into the fire. Surprised pikachu meme.",hardware,2025-08-28 16:02:36,0
AMD,nb2p6un,"We can only hope that AMD has a new IOD in the works, hopefully one that allows for a higher FCLK or more bandwidth per cycle to each CCD, and preferably with an IMC capable of utilizing per-bank refresh",hardware,2025-08-28 04:04:09,9
AMD,nb1mb7k,"The discoloration seems to be off-center. Isn't the IOD more symmetrically placed with respect to the axis that is perpendicular to the line separating the LGA pads?  That would mean the pads below one of CCD is discolored, right?  Looks like they died due to excess heat. They were using a NH-U9s after all, and described it as a gradual death.",hardware,2025-08-28 00:03:35,9
AMD,nb22u9b,"They list the specs, and yeah the Asus Prime B650M-A WIFI II isn't some paragon of quality. I'd be looking more at the Noctua NH-U9S being inadeqaute for the task though.",hardware,2025-08-28 01:39:50,7
AMD,nb3kxxj,[https://imgflip.com/i/a4drtt](https://imgflip.com/i/a4drtt),hardware,2025-08-28 08:48:41,2
AMD,nb3gzu2,"> The top 2 sets of MOSFETs have NO cooling. I don't have a diagram for this board, but if any of those are VSOC and they have a highside FET fail short, you will shoot 12v directly into SOC and explode whatever it is.  The VSOC rail shouldn't have been overwhelmed though, especially on the first setup with DDR5-4800, and especially with Zen5 CPUs which seem to have a quite neat improvement over Zen4 as far as VSOC requirements go for driving memory.  I find this part amusing though: ""We don't overclock or overvolt or play other teen games with our hardware.""  They may not play ""teen games"", but the manufacturers surely do. Even without getting into silly XMP/EXPO matters, a lot of motherboards just can't have safe settings out of the box.",hardware,2025-08-28 08:09:24,18
AMD,nbaemtn,inadequate power shoul dnot result in cpu having problems related to overdelivery of power. I suspoect their noctua based cooling is far insufficent and teh real problem. they also don't seem to hgave any kind of temerature monitoring in place.,hardware,2025-08-29 09:57:49,3
AMD,nb1my3l,"I hope there is an understanding that if a weak MB kills a CPU, it is because the MB is broken and not because it is weak.  I also hope there is an understanding that weak coolers shouldn't kill CPUs either. If the CPU dies because it let itself get too hot, then the CPU is buggy. Modern CPUs *should* survive even without a cooler. And these appeared to be repeated or long-running workloads, so I would hope 100Â°C temps would be noticed.",hardware,2025-08-28 00:07:16,63
AMD,nb3cezf,My CPU died on Gigabyte mobo last week.  My RMA got accepted and I'll receive cash,hardware,2025-08-28 07:25:34,17
AMD,nb2omy1,Youâ€™re right it isnâ€™t Asrock.  Itâ€™s just a total coincidence that they have multiple dead CPUs reported in their subreddit daily while other brands have once monthly.,hardware,2025-08-28 04:00:04,11
AMD,nb3ae68,"when 97% of the cases is Asrock, you can assume Asrock is at fault.",hardware,2025-08-28 07:06:35,-4
AMD,nb3jcy2,"But it can't be these ""top-quality motherboard[s]"", and the setup was safe because ""We don't overclock or overvolt or play other teen games"", so the motherboards weren't just the best, they also surely had a safe default configuration!  Shortly after starting to use heavy AVX512 workloads, I started smelling something burning on a motherboard with a better VRM setup than that new motherboard, with a better CPU cooler, and with a CPU frequency limit (that may have not been the limiter though during heavy AVX512 usage).  Using that first motherboard was simply a suicide run.  I don't know though why the hell can't we have safe defaults, configuration of VRM throttling, and let's be fancy, even VRM temperature monitoring. It's great that the VRM may survive 100-110 Â°C suicide runs for at least a couple of months, but I neither want that, nor expect other components soaking in the heat in the neighborhood to be rated for these temperatures.  Of course the alternative would be good VRM designs with properly sized heatsinks, but that costs more money, so I'm staying realistic. Just give me configuration (and monitoring).",hardware,2025-08-28 08:32:51,9
AMD,nb19io1,> This seems to suggest a gradual but predictable degradation.   Hope they can actually reproduce something.  > running very tight handcrafted asm loops sustaining one MULX per cycle,hardware,2025-08-27 22:52:16,17
AMD,nb50tsv,"so what's the issue with GN exactly? last time AMD had cpu's burning he did launch an investigation, but i haven't watched his channel ever since",hardware,2025-08-28 14:32:05,3
AMD,nb2xw96,Why would they? Doesn't AMD get a free pass on whatever they do?,hardware,2025-08-28 05:12:33,2
AMD,nb1wwq1,"Because they aren't a tech news reporting website, but maintainers of a software library.",hardware,2025-08-28 01:05:26,28
AMD,nb80305,I've had MSI motherboards that overvolted too. Asus ain't alone.,hardware,2025-08-28 23:15:00,1
AMD,nb2k0yy,"CPU's since the Pentium 3 or Athlon XP have thermal protection built into their CPUs. Under no circumstance should a modern CPU burn itself up with under any circumstance. No matter the load applied, as long as the power provided by the board isn't grossly out of spec. IE, huge voltage spikes out of spec.   Excessive heat can still damage other motherboard components, but a modern CPU should never be able to damage itself. The only exception I've seen in modern times is a CPU completely naked being able to damage itself in a micro-second at time of POST. IE, no heatsink touching the IHS.",hardware,2025-08-28 03:27:15,14
AMD,nb2ek21,"> on a part that is rated for  \* on a part that has been measured at 270W  But if the chip is overheating, it should be throttling itself.",hardware,2025-08-28 02:50:49,8
AMD,nb7xs9v,That has been the current rumor for a while: Zen 6 will have a new IOD. It's one of several reasons I'm waiting for Zen 6 X3D to arrive before upgrading.,hardware,2025-08-28 23:02:16,4
AMD,nbc3o8z,"Interestingly, the Strix Halo processors (AI 385/395) use infinity link vs infinity fabric.    Infinity link is smaller, more efficient, and allows much higher transfer rates.    It's also what AMD used in the 7900 xt and xtx.   It's more expensive to produce than the interconnects used in Infinity fabric.  Edit:  https://chipsandcheese.com/p/amds-strix-halo-under-the-hood  >We use fan out, we're for level fan out in order to connect the two dies. So you get the lower latency, the lower power, it's stateless. So we're able to just connect the data fabric through that connect interface into the CCD. So the first big change between a Granite [Ridge] or a 9950X3D and this Strix Halo is the die-to-die interconnect. Low-power, same high bandwidth, 32 bytes per cycle in both directions, lower latency. So everything - and almost instant on-and-off stateless - because it's just a sea of wires going across.",hardware,2025-08-29 15:50:56,3
AMD,nbdvbn4,"Should be very doable, after all Turin got GMI ""Wide"" which is an upgrade over the standard GMI by doubling the CCD/IOD bandwidth, it got left out of consumer Zen 5 for using the same IOD as Zen 4, but a newer consumer IOD would allow for its use.",hardware,2025-08-29 21:03:55,1
AMD,nb2paa1,It's bulging in the exact same spot Ryzen 7000 chips started to bulge due to excessive SOC voltage.,hardware,2025-08-28 04:04:53,16
AMD,nb34qee,"The CPU will just throttle if the cooler can't keep up, it won't be a sudden or catastrophic event like that, it'll just sit at 95 constantly. A VRM redlining for hours/days seems a more likely reason for failure.",hardware,2025-08-28 06:13:30,18
AMD,nbchusv,"The issue is that the VRM will try to deliver the requested power. With woefully inadequate VRM components coupled with nonexistant or extremely weak cooling, those components will get HOT (110C+ tCase). When (not if) those components fail, depending on which ones fail since they are discrete parts, you can totally nuke the load (CPU)",hardware,2025-08-29 16:59:34,2
AMD,nb3ak7m,modern cpus without a cooler can even boot. they just keep throttling themselves like crazy and takes half an hour to load windows. modern CPUs can also work fine in 100C. The throttling level for many are set at 105/110C.,hardware,2025-08-28 07:08:08,15
AMD,nb44iyj,"Asrock admitted fault, but other vendors having the same issue is a deeper problem though and perhaps there is some issue with Zen 5 CPUs.",hardware,2025-08-28 11:33:47,17
AMD,nb4zy3y,companies dont really give a shit about consumer hardware evidently. its all about cutting corners on cost at every possible opportunity and taking 0 responsibility for whatever happens afterwards. the devs of GMPlib should know better than to run a heavy compute workload on consumer hardware continuously for months on end.,hardware,2025-08-28 14:27:51,10
AMD,nb4dndz,"It really is crazy to start a GN video where Steve starts with a almost 10 minute history lesson on some recent examples of AMD's incompetence only for Steve to give three thumbs up in the conclusion!  So long as Nvidia is Nvidia, it seems AMD can for real burn your house down, as the hyperbole loves to reach, but somehow it's still Nvidia's fault even though they have zero to do with CPUs.  Nvidia is ruining gaming! Thanks, Steve!  HUB, I don't even think they know how to get into the BIOS.",hardware,2025-08-28 12:30:38,4
AMD,nb4es4x,"If they are reporting damage to the LGA they need to include a non-blurry image of the LGA. Any phone from the last 5 years can do that.  Also curious what the failure is. Can the units still POST or are they crashing during workloads? What does this mean:   ""Neither of the 9950X CPUs died immediately, instead they died the exact same way after a couple of months at high load. This seems to suggest a gradual but predictable degradation.""  If they think they found an issue and are broadcasting it publicly they should have receipts / data to support the degradation claim (esp. if they have 10+ of these systems running the same workloads).   Not saying they shouldn't broadcast this, but basically all they said is ""CPU dead"". Hopefully they follow up with more details.",hardware,2025-08-28 12:37:10,-7
AMD,nb4e97z,I'm still amazed that a place like r/hardware has posts that just disregard the thermal protections that have existed for years.  You got literally buildzoid throwing the MB under the bus even though he acknowledges it is likely not the cause! Why the misdirect then?,hardware,2025-08-28 12:34:09,2
AMD,nb3t7h3,A friend and i booted up an i7 2600k without cooler and it worked for a few minutes until it shut down. Worked like a charm on next boot (with cooler this time),hardware,2025-08-28 10:06:32,7
AMD,nb9ljna,but other vendors arent having the same issue. they are having many many times lower amount of issues.,hardware,2025-08-29 05:27:59,-1
AMD,nb5d5cn,">Any phone from the last 5 years can do that.  Very minor counterargument - that's actually not true. Some of the newer phones with 'top' sensors have terrible close-focus capabilities as one of the tradeoffs. To the point where an iPhone cannot take a clear photo of an ID with the wide (1x) lens and ~15cm min. focus, relying on the ultrawide+software to do the stitching (fake macro mode). That's the effect that's visible on the image.",hardware,2025-08-28 15:30:02,8
AMD,nb8c7uc,"When it is a known fact that the failed set ups are using garbage motherboards with overvolted stock settings and an insufficient cooler, it seems stupid to immediately assume the CPU is the problem. Yes the CPU shouldn't damage itself, but also maybe it's not fair to basically set the CPU up for failure in every possible way then blame it when it dies. Imo all 3 components involved here likely share some of the blame",hardware,2025-08-29 00:24:25,0
AMD,nb9zv7d,Two ASUS motherboards failing in succession in similar ways suggests otherwise,hardware,2025-08-29 07:38:03,5
AMD,nboke2a,"ah, two ASUS motherboards vs 2000 AsRock ones.",hardware,2025-08-31 16:00:45,1
AMD,nb41do2,First gen ryzen mainly excelled in more heavily multithreaded things lile video editing.   And even on that front it wasnt THAT impressive. The big shock with ryzen 1000 was that it was anywhere near intel performance. That had not happend for like 8 years at that point. So this isnt that much of a suprise.,hardware,2025-08-28 11:12:03,186
AMD,nb4tfu3,"Back when it was ok to be 30% slower in gaming,Â  as long as it was cheap.",hardware,2025-08-28 13:56:05,17
AMD,nb3ea95,A casual video that put into perspective that core count is not the only metric for gaming performances.  Ryzen 7 1800X (OC) vs Core i3 12300 in a few older and more modern titles.  Zen1 was never a great performer to begin with but even with 8 cores it's still way behind a modern 4 cores CPU.,hardware,2025-08-28 07:43:16,65
AMD,nb49u58,I use a 2700x in my workstation. It really works great. For comparison i also use a 5900x in my home server and 9800x3d in my gaming/home office PC.,hardware,2025-08-28 12:07:49,21
AMD,nb4pdhx,"Shoot i used a 1700 until maybe 1.5 years ago or whatever. IT was a solid CPU, yes it was not the best for gaming, but considering i came from a FX-6300, it might as well have been a Ferrari to my shitty commute car.  I just know that, just like with Nvidia now, i refused to buy Intel because they were so far up their own ass with ego. So yes i took a performance hit, but it was not that bad and i paid a fair bit less than for an i7. NEver regrated it and i built an entirely AMD system 1.5 years ago and its a beast.",hardware,2025-08-28 13:34:59,14
AMD,nb4586n,Still very usable but then I still use my 5820k. It's only games that are very poorly optimised do these older CPU's show actual problems.,hardware,2025-08-28 11:38:29,6
AMD,nb4n0q8,"Oh, I remember when people claimed that this would age better than a 7700K or a 8700k for gaming because of the cores and game only used like 30% of the total CPU!",hardware,2025-08-28 13:22:36,15
AMD,nb94k5t,"Yea, first gen Ryzen was maybe about on par with CPUs in the Sandy Bridge / Ivy Bridge / Haswell era. Way, way better than the bulldozer stuff they were putting out before, but still a little bit slower than the Skylake / Kaby-Lake / Coffee-Lake stuff Intel was putting out at the time.   AMD made up for it by having really generous core counts. The single threaded performance was close enough, and the multithreaded performance was impressive.",hardware,2025-08-29 03:18:31,4
AMD,nbowa2j,I used a 4th gen i7 for modern games until last year. I'm sure this thing is fine.,hardware,2025-08-31 16:59:51,1
AMD,nb9mo0v,"Still use my 1700 overclocked to 3.7Ghz daily. It helps that in the few competitive games I play, it's generally enough for me to get over 100 avg frames (with Marvel Rivals being my first regular game where I've really felt how slow my CPU is), and in the story games I do play, usually my 2060 is the limiting factor before my CPU in reaching a steady 60.",hardware,2025-08-29 05:37:49,1
AMD,nb41wlj,and also the price for it... was a huge seller...,hardware,2025-08-28 11:15:48,113
AMD,nb47b0g,Yeah. People don't remember how bad the bulldozer family was.  Zen 1 could keep up in multithreading and was behind in single core. Zen 2 was ahead in multithreading and about even in single core. Zen 3 is when they were convincingly in the lead.,hardware,2025-08-28 11:51:50,34
AMD,nb65sn3,"The R7 line more or less matched the 6800x while running on $80 motherboards instead of $300 boards and also simultaneously being priced as low as $300 for the CPU (vs $1000)  The R7 line did NOT beat kabylake (e.g. i5 i7 7700k) in gaming overall but was still ""fine"" for people who didn't buy the fastest card on the market for 1080p gaming. It was often practically tied with an OCed 7700k at 4K.  The R5 line swept the floor vs the Kably Lake i5s (e.g. i5 7600k) in productivity. It also beat them in newer/more demanding games (lower frame rate scenarios). It did NOT win in older/high-frame rate esports titles.   My expectation is that in the real world (mid range CPU + mid range CPU), the R5 was generally the better gaming chip.  The R3 (4C/8T) vs i3 (2C/4T) comparison was VERY one sided and the clock speed gap was smaller than with the i7s and most i3 models couldn't be OCed if I recall correctly.   \------  This is at/near launch. AMD was generally (not always) ahead on everything that wasn't 1080p gaming with high performance video cards when comparing the top end parts.     About 6 months later intel upped the core count of their desktop line by 50%. This are up most of the reason to go for Zen 1 and in my view Zen + wasn't a big enough jump.",hardware,2025-08-28 17:43:34,6
AMD,nb4yzp8,Also the the R5 1600 was a better buy than 7600k with 4 threads.,hardware,2025-08-28 14:23:17,9
AMD,nb7gowu,"Man, I remember when AMD was selling the 8350 over and over and it was just so bad compared to Intel offerings. Huge fan of how they turned it around, I just upgraded my Ryzen 1600 to a 5700x last month on a b350 board.",hardware,2025-08-28 21:28:53,5
AMD,nbifypl,> And even on that front it wasnt THAT impressive.   You could get a $399 1700x (but often cheaper) which basically performed as well or better than the 6800k which was like $1100.  Basically more than twice the perf/$$$. Not THAT impressive smh.,hardware,2025-08-30 16:10:16,1
AMD,nb8khb8,Yeah people were so shocked with what AMD had achieved with the Ryzen 1000 series because the FX line was so dogshit haha,hardware,2025-08-29 01:13:10,1
AMD,nb4frnu,"I just retired my r7 1800x back in April.   that thing was a monster. Loved it for productivity. Obviously it's showing its age now, but it did 6 years.   First Gen ryzen had snowflakey stuff like faster ram timing very affecting performance.",hardware,2025-08-28 12:42:47,0
AMD,nb93lk7,"I mean it's still okay to be slower at gaming if the price is right. Budget CPUs are budget CPUs. Zen 1 never competed with the 7700k well, but eh, you could make an argument for the 1600 and below given how anemic intel's offerings really were.",hardware,2025-08-29 03:12:02,9
AMD,nb3m7hl,"early zen is in a bit weirder spot with cores compared to others as you have them in multiple CCXs with horrible latency inbetween, combined with the cores themselves underperforming and a bad imc it's not much of a surprise that it falls behind so much",hardware,2025-08-28 09:00:50,58
AMD,nb3ve3o,It was way behind by my 4670k that was released like 5 years before it... Well at least for things that only scaled up to 4 cores...,hardware,2025-08-28 10:25:18,21
AMD,nb527s8,Oh first Gen Ryzen sucks now according to this subreddit? lmao,hardware,2025-08-28 14:38:44,8
AMD,nb4iea5,you hould be comparing it to 7700 though. which also had 4 cores.,hardware,2025-08-28 12:57:30,4
AMD,nb3equw,>core count is not the only metric for gaming performance   You're unfortunately going to get nowhere with this argument on Reddit    Reddit is obsessed with 8-core CPUs despite evidence that it makes almost no difference vs a 6-core of the same CPU generation for gaming  Overall CPU-performance > Core count,hardware,2025-08-28 07:47:36,-44
AMD,nb4o9a4,"Was riding one of those puppies until end Q1 this year, absolute champ at anything I asked it to do.",hardware,2025-08-28 13:29:06,6
AMD,nb53hhd,"Are you me? I went from an FX-6300 to an r7 1700! Still rocking the 1700, tho. Hopefully not for too long",hardware,2025-08-28 14:44:39,6
AMD,nb66y8n,"Yea I remember ryzen 1 and 2 got undeservedly high praise on reddit despite both having pretty terrible single core perf. (worse than even cpus from 2013 / 2014). A lot of cores, but slow ones, good for niche tasks or certain multicore optimized ganes but bad for everything else including most games, web browsing, most apps, etc. Reddit just hates intel that much it gaslit people into thinking those early ryzens were much better than they actually were.",hardware,2025-08-28 17:48:56,14
AMD,nb58aj6,8700k is a whole different generation when they started actually using more cores,hardware,2025-08-28 15:06:57,4
AMD,nbbsnk0,"Its a fools game to plan for longevity except the extremely obvious. I have a slight preference for IPC over cores based on history, but its hard to know what moment of history you're in. In the early days of Intel 4c era everyone said you should get the 4c/4t part for gaming. And that proved correct for a long time. But if you still had that CPU by the time covid and inflation came around and everything was expensive you were pretty miserable compared to the corresponding 4c/8t part which definitely would last you longer.",hardware,2025-08-29 14:59:00,0
AMD,nb43wl9,R7 1700 for 330 was a steal,hardware,2025-08-28 11:29:32,60
AMD,nb9idwr,Then Intel sold skylake for like 9 generations! Iâ€™ve been rocking a x470 through 3 different processor generations over 7 years and itâ€™s still just fine for gaming. Intel would change sockets every few years just out of spite before they had real competition. I hope Intel can come back with a strong one soon to keep the rivalry going cause everyone benefits when they are trading blows,hardware,2025-08-29 05:01:29,3
AMD,nb4kqxj,"Yeah, there were lots of little adjustments to make on the RAM side to get it to run to it's full potential. That said, those 1st gen x370 motherboards have proven themselves. I went from a launch day 1800X to a 5800X. I kinda wish I had sprung for the X3D but I upgraded before the price cut and it was hard to justify at the time.",hardware,2025-08-28 13:10:32,5
AMD,nb4nrj3,Zen got good (for gaming) with Zen 2 onwards.,hardware,2025-08-28 13:26:32,14
AMD,nbh3ikh,And 4 threads. Hyperthreading in 4770K really helps.,hardware,2025-08-30 11:27:32,1
AMD,nb53ouz,"Objectively it was still worse than Intel, but the reason Zen 1 was and is beloved is because of what it represented",hardware,2025-08-28 14:45:36,24
AMD,nb9n4id,it always did. It wasnt until third gen - Zen 2 - that AMD CPUs got good.,hardware,2025-08-29 05:41:51,4
AMD,nb41uoz,"Well even if it does not now, when I bought my 8cores in nov 2024, it was to keep it for the next 5-7y  So I'm pretty sure at that point that 8 cores may perform better than 6  Just like 6 now is the norm over 4",hardware,2025-08-28 11:15:27,7
AMD,nb3s94n,Is this the part where everyone tells you how smart you are and congratulates you on your brilliance?,hardware,2025-08-28 09:57:58,14
AMD,nb3qevb,"And until consoles get more than 8 cores, developers won't target more either.",hardware,2025-08-28 09:41:29,1
AMD,nbfukx8,"You're right, for most games even say a 3300X will match a 3600, but it was bloody nice for people with real tasks to have options that didn't cost the earth.",hardware,2025-08-30 04:31:37,1
AMD,nb63rx3,"That depends, what was your GPU for your FX and then what did you use with the 1700?",hardware,2025-08-28 17:34:19,4
AMD,nb6mjq5,"One of the most common thing I started hearing back then was ""What about Chrome, Spotify and Discord running in the background!?"" or ""What about if you want to stream?"" as if suddenly everyone was streaming.",hardware,2025-08-28 19:03:26,14
AMD,nb959dc,YEP....,hardware,2025-08-29 03:23:19,1
AMD,nb6lwld,"7700K, 1700X and 8700K all released in 2017",hardware,2025-08-28 19:00:22,9
AMD,nb49q02,"R5 1600 for i5 prices was also another steal from zen 1, people may be used to 6 core being the main stream budget option nowadays, but before zen anything over 4 cores forces you to the hedt platform which cost even more on the mb side, not to mention intel 6 cores like the 5820k and 6800k cost around the 400 mark",hardware,2025-08-28 12:07:06,73
AMD,nb4yif4,I remember everyone buying the 1800x which was $500 when all the benchmarks showed the 1700x (and often the 1700 on many workloads) performing like 95% of the 1800x. Never made any sense to me.,hardware,2025-08-28 14:20:58,13
AMD,nbl2823,It was only a steal if you valued the extra cores. Even those only really mattered if you were hosting VMs or rendering really long videos. They didn't really translate in gaming or everyday multitasking. Pretty sure it launched right before the 8700k too and a ton of people were holding out for that CPU. For good reason.,hardware,2025-08-31 00:44:43,1
AMD,nb5psfi,I just swapped to a 5800XT.   I am very much trying to ride til it dies on the am4 platform.,hardware,2025-08-28 16:30:04,3
AMD,nb9331c,"Zen 1 was pretty bad. They were like 30-40% behind intel there.  Zen+ reduced this to around 20-25%  Zen 2 reduced this to 10-15%  Zen 3 they were ahead  Then alder lake came out  THen they added X3D to Zen 3 and were on par with the 12900k  Then Zen 4 and raptor lake were about on par outside of X3D, which thrashes everything.  And zen 5 barely improved on zen 4, and arrow lake ended up being like intel's version of zen 1, regressing to alder lake performance in gaming. And Zen 5 X3D once again thrashes everything.",hardware,2025-08-29 03:08:33,9
AMD,nb56w4w,"This subreddit and others went from ""first gen Ryzen good"" to ""it was worse than Intel but we like what it represented"" lmao.",hardware,2025-08-28 15:00:23,-12
AMD,nb9neoa,unlikely. Unless you use it for paralelelized workloads or play strategy sims that actually utilzie the cores its mostly useless because in next 5 years we wont be over cros-gen for next console gen so all games will be aimed at current slow consoles.,hardware,2025-08-29 05:44:21,3
AMD,nb45g57,"People made the same argument in 2019 with the 3600/3700x   The 3700x hasn't aged any better    The consoles have used 8-core CPUs for over a decade, we still aren't seeing apprciable gains from 8-cores on PC",hardware,2025-08-28 11:39:56,2
AMD,nb3xo0p,They really arenâ€™t wrong about the general notion of this Reddit particularly around the Zen2/3 days though.,hardware,2025-08-28 10:43:47,10
AMD,nb9nheh,no its the part where you failed to take in any context to what you read.,hardware,2025-08-29 05:45:01,1
AMD,nb484vp,But developers aren't targeting a core count in the consoles   They're targeting the consoles level of CPU-performance   The PS4 used an 8-core CPU. And got bodied by quad cores of the day that were far faster,hardware,2025-08-28 11:57:03,8
AMD,nb65j2s,"Same gpu, the shite, but still rocking, r9 380. At the time I was a kid that wanted to make and edit videos, so I only upgraded my cpu to the best I could afford",hardware,2025-08-28 17:42:20,4
AMD,nb6s141,"Yea lol. Also don't forget how suddenly everyone was doing 3D rendering, video encoding, physics sims etc, when in reality 99% of people who bought those's CPU's would rarely, if ever do such tasks. Furthermore, even back then, many of those could be done much quicker using GPU acceleration (i remember using nvidia's hardware video encoder nvenc more than 10y ago already). Nowadays the CPU is becoming even less relevant for those tasks as more and more programs implement GPU acceleration.  And finally the price, the 1800x was around the 500 usd mark wasn't it? A 7700k was 300 or so i think. Yes sure slower in those niche computation tasks, maybe a bit less suited for heavy multitasking, BUT cheaper, faster in almost all games, faster in web browsing and day to day usage. The 1800x was a terrible value, except for those rare cases where you were actually doing physics sims or 3d rendering on the daily.",hardware,2025-08-28 19:30:05,7
AMD,nb95d16,"I had a friend with a ryzen 1700 who was like ""i can play a game while playing another game!"", I mean that's nice but not particularly helpful.",hardware,2025-08-29 03:24:00,2
AMD,nb9jzq2,Just like people talk about AI stuff/local LLMs for GPUs these days,hardware,2025-08-29 05:14:47,-3
AMD,nb8w6nk,Still a 40% multithread improvement gen over gen at the time,hardware,2025-08-29 02:24:30,4
AMD,nb4hq7n,"Kaby lake (and for a long while beofre that) i5 was 4 core, i7 was 4 core 8 threads. Coffeee lake i5 was 6 cores no ht, i7 was 6 cores 12 threads.      but really, we are seeing the same stagnation in ryzen now. ryzen 7 ought to be coming with more memory channels and cores by now.",hardware,2025-08-28 12:53:44,9
AMD,nb52wfi,yeah I bought the 1700... then later slapped the 3600 on,hardware,2025-08-28 14:41:57,1
AMD,nbacpfw,Because the by far biggest channel at that time Linus Tech Tips pushed Ryzen a LOT. He especially loved the Cinebench performance,hardware,2025-08-29 09:40:33,1
AMD,nb6mjvj,Yeah my plan was to skip AM5 and hop back in on AM6.,hardware,2025-08-28 19:03:27,1
AMD,nb63009,Those two statements are not mutually exclusive. You realize that right?,hardware,2025-08-28 17:30:42,15
AMD,nb58ugx,"It can be good, and still worse than what Intel was offering at the time (in a lot of facets). The price to performance in multi threaded applications was way better though.  Also people were absolutely fed up with Intel by 2017 and absolutely liked what Ryzen represented in terms of price to performance.",hardware,2025-08-28 15:09:33,8
AMD,nbh3tg5,"It was like intel's current Arrow Lake. Not great performer, but represents a great leap in its architecture. Namely, the usage of chiplets.",hardware,2025-08-30 11:29:57,1
AMD,nb9n6a4,it helped that it was much cheaper than Intel offerings at the time.,hardware,2025-08-29 05:42:17,1
AMD,nbdda9c,You mean vs 3600X or vs 9600X as I have Sen people compare?,hardware,2025-08-29 19:32:53,1
AMD,nb460ir,It's like I said. You can't get anywhere with this discussion on reddit,hardware,2025-08-28 11:43:39,1
AMD,nb73t97,6 cores being the sweet spot for gaming isn't a revelation.It's well known knowledge and has been for a while.,hardware,2025-08-28 20:27:22,-1
AMD,nb49wxy,Bulldozer 8 cores is more similar to 4 cores with smt rather than 8 independent core,hardware,2025-08-28 12:08:18,2
AMD,nbc1cl3,"I was rocking a 960 with my FX-6300. Then i wanted a whole new PC, so bought a 1070 first and the bottleneck with the FX-6300 was insane. Of course once i got the rest of the PC with the Ryzen 1700 it ran great. USed that from the launch of Ryzen until the 7000 series came out. So a good long life once i replaced my 1700 with a 7900X and i replaced my GTX 1070 with a AMD 7900XT.",hardware,2025-08-29 15:40:01,2
AMD,nb6yo1y,"1800x sucked, R7 1700 was the best value 8 core.",hardware,2025-08-28 20:02:43,6
AMD,nb95gm7,"Yeah but most people werent buying 1800x, they were buying 1700s, at the same price as the 7700k, and then OCing them to 1800x performance.",hardware,2025-08-29 03:24:41,4
AMD,nb5540o,> ryzen 7 ought to be coming with more memory channels  This is segmentation to keep Threadripper relevant for more use cases. If you really need memory bandwidth you need threadripper.,hardware,2025-08-28 14:52:12,17
AMD,nb4jh0q,">with more memory channels  This is like arguing that cars have been stagnate for a century because we still have 4, 6, and 8 cylinders.  128b memory is fine for a desktop CPU. Improvements come from new generations of RAM.",hardware,2025-08-28 13:03:30,17
AMD,nb51dha,Yeah Ryzen stagnation is real. Same core count on mainstream for about 8 years now  Just as long as Intel stayed on 4 cores.,hardware,2025-08-28 14:34:44,10
AMD,nb5hyy4,"More memory channels for consumer hardware are just a no go unless you are soldered in. Mainboard layout, cost factor of needing to populate them all for nominal performance, etc.  On the professional side you can easily get a dozen channels.",hardware,2025-08-28 15:53:00,1
AMD,nb4sjlj,"> ryzen 7 ought to be coming with more memory channels and cores by now  That's happening next gen but the point stands that more mt is kinda useless for majority of the consumers. 9950x ain't providing meaningful experience difference over a 9700x for the average desktop user, you'd probably not even know that it has double the cores without checking. The highest core count ryzens are some of the worst selling skus even if comparing sales revenue and not unit sales  If anything you should hope for larger cores, more bifurcation, more cache, accelerator blocks for offload, or other features such as lp cores over just more and more classic cores. Any one of those would create a more meaningful experience change than just giving ya another 2 or 4 cores  >""but but value, more cores means more value!""  Yeah but you ain't doing anything useful with those cores, so their existence becomes a ""feels good"" thing with little actual use.",hardware,2025-08-28 13:51:33,0
AMD,nbgkw15,"The 6 & 8-core of the same generation   E.g. a 3600 vs 3700x. 5600 vs 5700x, 7600x vs 7700x etc",hardware,2025-08-30 08:31:40,1
AMD,nb476xe,"I wish I had some of the gems from back then saved. But hereâ€™s a tangentially related and hilariously similar one, about gpus instead:   https://www.reddit.com/r/Amd/comments/lxhvm9/any_news_on_when_ray_tracing_will_work_on_radeon/gpozhnb/?context=3",hardware,2025-08-28 11:51:08,7
AMD,nb5egzv,"Jaguar in the Xbox One and PS4 was not related to Bulldozer, FYI. It was an Intel Atom competitor.",hardware,2025-08-28 15:36:21,3
AMD,nb4ains,Which goes back to what I said   The overall performance of the CPU is more important than the core count,hardware,2025-08-28 12:12:01,5
AMD,nb6knby,"sure, but I want more segmentation, not less. so not just either 2 or 8 memory channels, but also a segment with 3/4 in the middle between HEDT and vanilla.",hardware,2025-08-28 18:54:23,4
AMD,nb56wom,We are getting 12c mainsteam cpus with zen 6 at least.,hardware,2025-08-28 15:00:27,9
AMD,nb76271,"> Yeah Ryzen stagnation is real. Same core count on mainstream for about 8 years now >  > Just as long as Intel stayed on 4 cores.  LOL, what?  From Zen 1, Ryzen has doubled the core count of their parts (mainstream is 8 cores, higher end is 12 and 16), massively increased the cache (and then increased it massively _again_ with X3D), massively increased clock speed, drastically improved performance of AVX2 (and implemented AVX-512), and has just all-around improved various aspects of performance.  The performance leap from Bulldozer to Zen 1 was absolutely massive. The difference between a Ryzen 7 1800X and a Ryzen 7 9800X3D is even larger.",hardware,2025-08-28 20:38:03,11
AMD,nbftncf,"They have something to offer there. If you need more than sixteen, they have threadripper. Intels complete obliteration in HEDT is real.",hardware,2025-08-30 04:24:05,1
AMD,nb6jpig,"this is not a credible issue given the price of X870E motherboards. X870  might just as well been a ""we mandate 3 / 4 memory channels"" and these would still be expensive motherboards but not by much.",hardware,2025-08-28 18:49:56,4
AMD,nb5fk6n,You should look into what AMD is doing with strix point and the ryzen AI max processors they are doing almost everything you just said lol,hardware,2025-08-28 15:41:35,3
AMD,nb47okd,"That's a banger, made me laugh   Weirdly enough I didn't even knock 8-core CPUs, just said that you don't see appreciable gains over a 6-core of the same generation",hardware,2025-08-28 11:54:12,0
AMD,nb6pwa0,We already have that Ryzen is 2 channel threadripper is 4 channel and threadripper pro is 8 channel.,hardware,2025-08-28 19:19:39,16
AMD,nb5du4c,Now only if we also get scheduler updates on windows,hardware,2025-08-28 15:33:19,6
AMD,nb7t2gt,"It's also worth remembering that to a vast majority of people, more than eight cores is largely not particularly useful and how improved IPC, frequency, and memory latency are the actually useful parts. Like, back in the day, I bought a 5900X specifically because my boss gets weird with what he wants and sometimes I need to encrypt and zip a .7z archive at maximum compression preset that's dozens of gigabytes at the pace of yesterday (I couldn't afford the 5950X and the X3D parts didn't exist yet). If it wasn't for that, I could have gone for a 5600X and that would have been more than enough for gaming and VR.",hardware,2025-08-28 22:35:38,5
AMD,nb8y31u,"Ryzen 7 1700X 8 cores  Ryzen 5 1600X 6 cores  Ryzen 7 9700X 8 cores  Ryzen 5 9600X 6 cores  Really not that complicated. How do you not understand?   Yeah they did increase performance a lot, obviously. Only the core count is where they stagnated. Even the street price per core stayed largely the same.",hardware,2025-08-29 02:36:13,2
AMD,nbapawp,"if we get 12 cores on a single ccd than the schduler is not an issue, windows usually just put workloads on a core that is avaible. If they are on the same silicon on the same ""ccx"" then there is no issue.  If we get p/e core u-arch on the same ccd then we have issues.",hardware,2025-08-29 11:22:26,6
AMD,nb5i2px,"Why would that matter? Windows has no problem with 8 cores either. If we now get 12 cores on one CCX, it should not be different",hardware,2025-08-28 15:53:29,1
AMD,nbacjes,Its not like the 4 core was limiting back then. The hate by this argument was unkustified,hardware,2025-08-29 09:38:58,2
AMD,nb966b5,"> Ryzen 7 1700X 8 cores  > Ryzen 5 1600X 6 cores  > Ryzen 7 9700X 8 cores  > Ryzen 5 9600X 6 cores  > Really not that complicated. How do you not understand?  You listed two arbitrary SKUs in the respective families, while failing to mention that the Ryzen 1000 series SKUs you listed are the mid-range and high-end, while the 9000 series SKUs you listed were the low end.  The 12- and 16-core Ryzen 9 parts are now Ryzen's high end, while the X3D parts offer 600+% more L3 cache than even the best that Ryzen 1000 offered.  Your comparison was against Intel's numerous generations with the same quad-core part with barely any performance increase between successive generations, and that's not the case with Ryzen at all.",hardware,2025-08-29 03:29:37,-3
AMD,nbapc06,agree,hardware,2025-08-29 11:22:39,2
AMD,nb7n8do,"This isn't a simple matter of it being about the number of cores a CPU has, it comes down to the architecture, and how Windows sees/communicates with them and designates those tasks. Ryzen 1000 suffered from performance issues, because Windows didn't know how to correctly work with Ryzen's core setup. While its gotten better over the years, Microsoft is still releasing updates to make sure Windows utilizes AMD correctly.",hardware,2025-08-28 22:03:44,3
AMD,nb9a2kc,The price at each level has not changed much. The 12 and 16 core parts are substantially more expensive than the 8 cores were back then.,hardware,2025-08-29 03:57:42,7
AMD,nb96oeb,Arbitrary? lol okay,hardware,2025-08-29 03:33:12,1
AMD,nasn76k,"Ryan Smith.  Now thatâ€™s a name I havenâ€™t 'seen' in a long time!  Anyhow,  >As mentioned previously, RDNA 4 has new memory compression/decompression features. This is entirely transparent to software; it is all handled in hardware. AMD has seen a \~25% reduction in fabric bandwidth usage.  That should explain why AMD chose to stick with GDDR6, or perhaps how the 9070XT manages to compete with the 5070 Ti, despite the latter having a massive 40% raw bandwidth advantage.  Of course, the 9070XT also has 33% more SRAM (64MB vs. 48MB), so itâ€™s not exactly a 1:1 comparison, but stillâ€¦ quite impressive.  In any case, I have a feeling things will really heat up on the Radeon side next generation thanks to GDDR7, N3, and the shift to a brand-new architecture (hopefully). The stars certainly seem to be aligned.  Thatâ€™s not to say Nvidia will be sitting on its hands, of course, but regardless, fingers crossed.",hardware,2025-08-26 17:04:05,120
AMD,natwzmj,A dedicated hardware transfermer. Now that's something even Nvidia don't have!,hardware,2025-08-26 20:40:23,21
AMD,navc6l9,Does Ryan write for STH now? Very cool write up.,hardware,2025-08-27 01:17:30,8
AMD,nasvbhw,Great. Does that mean that Zen5/RDNA4 desktop APUs are coming soon ? Maybe even beefier dGPUs ?  It seems that it wouldn't cost them much to plop GDDR6W on the same 9070XT boards along with 2 chips per channel and have a 9070XT with 64GiB RAM.  Is that in pipeline ? And/or maybe bigger cousin with 80-96 CUs ? ðŸ™„,hardware,2025-08-26 17:40:55,-8
AMD,nasqx7d,If AMD also has hands on the 3GB GDDR7 vram modules that would be a dream come true,hardware,2025-08-26 17:21:11,26
AMD,naspba1,New architecture is a question mark. Dont know its from the ground up or compartmentalization of RDNA4 modular design steps. But Im expecting driver woes at launch. Thats my bar set. Nvidia will be busy with neural server processing rathern than normal graphical improvements.,hardware,2025-08-26 17:13:56,44
AMD,nateg80,"Nvidia is also supposedly doing a new uarch for the 60 series, so it might be a bigger gap than 40->50",hardware,2025-08-26 19:12:04,15
AMD,nauredf,">compete with the 5070 Ti, despite the latter having a massive 40% raw bandwidth advantage  That's because blackwell gaming skus have excessive mem bandwidth due to how aggressively nvidia decided to cut the core counts across the stack (except 5090). The 5080's 12% faster than 4080 but has 34% increase in bandwidth, that's an unintended effect of moving to gddr7 when the decision was to cheap out on all sku core count  The difference would have been larger if nvidia didn't choose to squeeze gamers and give them 10+% generational gains.",hardware,2025-08-26 23:17:59,10
AMD,natm838,"This type of compression is nothing new. It's been done for decades. Nvidia has historically been much better than AMD when it comes to compression so I would be very surprised if AMD has actually surpassed Nvidia in that regard.  I think it's a mistake to think ""AMD does compression so their GDDR6 actually performs like Nvidia's GDDR7"".  Here is a link to an Anandtech article about the improvements to memory compression Nvidia did in 2018: https://web.archive.org/web/20240229212853/https://www.anandtech.com/show/13282/nvidia-turing-architecture-deep-dive/8",hardware,2025-08-26 19:49:31,12
AMD,nav940m,This is good news for future consoles. Squeezing more out of less will show benefits.,hardware,2025-08-27 00:59:37,4
AMD,nb1jk4f,"Yes but the 5070ti is a disabled chip like the base 9070 is, fully enabled AD103 is the 5080 and the 9070xt is certainly behind it in several aspects.",hardware,2025-08-27 23:47:58,2
AMD,nat1mij,The 9070XT would have been even better if AMD decided to shove 24gbps GDDR6.  A 20% bandwidth bump would have been very helpful.,hardware,2025-08-26 18:09:57,5
AMD,naurbyd,"Right now the claim is that lower end RDNA5/UDNA will use LPDDR5X on discrete GPUs to get around supply constraints on GDDR memory for GPUs that are in like the 60 tier class.   Now that claim makes no sense to me, because I can't imagine GDDR6 has supply issues with mostly only AMD needing it. But maybe LPDDR5X is cheaper, and with AMD's memory bandwidth, and cache changes, that benefits them somehow. Plus they can use the same silicon on their massive laptop APUs with 128gb to 256gb of RAM next generation. But their architecture and the way they are moving does signal this.",hardware,2025-08-26 23:17:37,4
AMD,nay36xi,"I donâ€™t think amd would use N3 for gaming gpus in a while, i think theyâ€™re gonna use it for their gold goose which is data center first",hardware,2025-08-27 13:41:10,1
AMD,nb2x8t0,going to new architecture was never a good outcome for AMD. it always too another 2-3 iterations to actually get it right.,hardware,2025-08-28 05:07:02,1
AMD,natzdab,Nobody has any transfermer as far as I know.,hardware,2025-08-26 20:51:31,20
AMD,nax4eih,An asic within an asic,hardware,2025-08-27 09:49:16,3
AMD,natu1tn,There's no RDNA 4 APUs,hardware,2025-08-26 20:26:44,18
AMD,naunq6k,"Given the newest RDNA3.5 APUs came out after the 9070 XT, I wouldnâ€™t expect it for a while.",hardware,2025-08-26 22:57:43,11
AMD,naux7uh,"Rdna 4 is a holdover generation, it's only used for a couple products 9070/9060, and won't be used anywhere else.   RDNA5(or whatever they call it)is the next full generation, it will be in next gen APU's.",hardware,2025-08-26 23:50:33,7
AMD,nat3pxt,"RDNA4 was itself a new uArch (RDNA3 is GFX11, RDNA4 id GFX12) with relatively few software bugs. Don't think I would expect driver woes at launch, at least with RDNA4 AMD actually showed that driver stability was a key focus and they held back the launch a bit to ensure it.   RDNA5 also looks to be a new uArch, with GFX1250 looking to be CDNA4 instead. So next should be GFX13-based GPUs for consumer grade GPUs.",hardware,2025-08-26 18:19:54,37
AMD,nau09sg,"New Arch is UDNA based on the MI400 series AI GPU.  It should be quite interesting.  My guess is AMD becomes a strong contender from here on out.  There's no longer a benefit from new manufacturing technique, which means chiplet can actually spread its wings.",hardware,2025-08-26 20:55:46,10
AMD,nax32xq,"GFX13 is a clean slate Âµarch. Been confirmed so many times now that's it's a given.  It's nothing like RDNA4. I've read some of the patents, that was shared by Kepler\_L2 3 weeks ago and RDNA 5 looks like it cold be the biggest redesign since GCN. Scheduling is completely overhauled, CU gets a RDNA like rework, massive changes to RT pipeline etc...",hardware,2025-08-27 09:37:10,4
AMD,nax3vqb,They better do. They've been coasting since Ampere on a fundamental architectural level. NVIDIA needs a proper redesign and a massive RT increase nextgen if they want to distinguish from RDNA5 and its massive rumoured PT gains.,hardware,2025-08-27 09:44:38,5
AMD,nb1ke10,It'll be a bigger leap regardless because Nvidia is getting a die shrink with the 60 series unlike with Blackwell.,hardware,2025-08-27 23:52:37,2
AMD,natr02z,"No one said this type of compression is new, just that AMD used a newer method for RDNA 4.",hardware,2025-08-26 20:12:34,25
AMD,natp3bv,I remember the Maxwell (900 series) reviews and the hype was all about compression to improve memory bandwidth as well.,hardware,2025-08-26 20:03:27,13
AMD,nawj5lq,"No, it's definitely not new!  Delta color compression was how the ""Tonga/Antigua"" (GCN 3) with a 256-bit wide bus was able to compete with ""Tahiti"" with a 384-bit bus on the original GCN 1 architecture.  And like someone else pointed out, memory compression was how the Maxwell 2.0 got as good as it did, and it was further improved with Pascal (GCN 4 on AMD's side).",hardware,2025-08-27 06:25:09,4
AMD,nat3zgr,"Do those even exist? Iâ€™m not believing Samsung until the actually product come out, just like GDDR6W.",hardware,2025-08-26 18:21:09,24
AMD,nau0upj,Not true.   OCing the VRAM has close to zero impact on performance.  SRAM adds a lot of flexibility.  I'd rather them use GDDR6 again and stack 32-64GB I stead of jumping to 7.,hardware,2025-08-26 20:58:31,20
AMD,nawz5ui,"It's not a supply constraint or a cost problem, the rumors simply say the smaller RDNA5 dies will be shared with mobile APUs (Medusa Halo and Medusa ""small"", if that will be a thing). By using lpddr they can also add a lot more memory, if needed.",hardware,2025-08-27 08:59:18,10
AMD,naw3e5s,I can only see GDDR6 supply being a issue if AMD was going to go big on production for their cards.    I somehow doubt that will happen.  They seem fine with playing a very distant 2nd to NV.  The more likely reason to go with LPDDR5x for low end cards is cost.  Its a good enough cheap option which make sense for low end cards.,hardware,2025-08-27 04:11:12,1
AMD,nb51aa4,They have to move on from N4 at some point. That node is becoming the next 28nm with how slow everyone is moving off it (AMD and NVIDIA).  RDNA5 isn't releasing until prob +1.5 years from now. By then the CDNA chiplets are probably already on N2 or even A16.,hardware,2025-08-28 14:34:18,1
AMD,nb51rpp,Really hope that's changed. They do have unlimited funds now relative to where things were in pre zen era.,hardware,2025-08-28 14:36:37,1
AMD,nauhzqh,Lersa Su is a genius.,hardware,2025-08-26 22:25:24,15
AMD,nax4voc,"xD  IIRC Imagination technologies has had Ray instance transform in HW for a while, but they're pioneers in HW and has been so for decades. Introduced Tiled base rendering 18 years before Maxwell.",hardware,2025-08-27 09:53:24,4
AMD,nat6qgj,"Also AMD did postpone there 9070XT launch by 3 months, leading AIBs to show off coolers and cards on tables, but unable to show any demos. That probably gave AMD some extra time to polish the Driver stack vs Nvidia who's launch was Middling with some driver black screen issues on top.",hardware,2025-08-26 18:34:22,32
AMD,nat3yp7,Neat thx for the info!,hardware,2025-08-26 18:21:03,7
AMD,nb2xdp1,RDNA4 was a safe iteration in a known direction from RDNA3. dont think its comparable.,hardware,2025-08-28 05:08:12,2
AMD,nauetgf,New arch also gets a lot of Sony and Microsoft money assuming itâ€™s used for the new consoles,hardware,2025-08-26 22:08:25,10
AMD,nawbkp9,>New Arch is UDNA based on the MI400 series AI GPU.  What are the hints that point to MI400 using the same arch as gaming GPUs?,hardware,2025-08-27 05:16:54,2
AMD,nb2xhwu,yet in comparable architecture chiplets still result in worse performance as evident by AMDs attempts to use them.,hardware,2025-08-28 05:09:13,1
AMD,nb1k5ia,"People have been saying this for well over a decade. I still think Intel being a better contender with Druid is more likely. AMD just manages to disappoint in so many ways time and time again but I do think RDNA5 will be modern AMDs best uarch yet, better than gcn1? Time will tell.",hardware,2025-08-27 23:51:16,1
AMD,naxidlt,"It seems like RDNA5/UDNA has a dramatically different cache hireachy.  The conventional 2/4/6mb L2 cache +32/64/96mb of L3 infinity cache is replaced by a bigger pool of L2  that's smaller than infinity cache and much larger than the old L2  but likely has lower latency than inf cache.  If I had to guess why they did this, it's probably to save on die area allocated to on-die SRAM since scaling for it has collapsed with 5nm.  Having less levels of cache also likely reduces validation time since it's less complex.  AMD is instead likely going to rely on a faster GDDR7 memory and a wider memory bus to make up for the smaller capacity",hardware,2025-08-27 11:39:02,4
AMD,nb2y2zf,"the rumoured massive RT gains for RDNA5 would put it on part with 50 series though, so its not like they are eclipsing them. and AMD rumours never turn out to be as good as promised.",hardware,2025-08-28 05:14:07,2
AMD,nb50pck,Yep node and architectural change. They can't do Ada lovelace ++ or Ampere+++ . They have to make fundamental changes to the GPU if they want to keep up with AMDs scalable AT0 monster.,hardware,2025-08-28 14:31:28,1
AMD,nau22kp,"I do think the wording implies this is some new AMD trick that makes AMD's GDDR6 equal to Nvidia's GDDR7. That only makes sense if we pretend Nvidia doesn't already have aggressive compression, which theyâ€™ve had for years. Historically, Nvidia's compression has been better than AMD's as well.  A ""\~25% fabric traffic reduction"" in gaming tests isn't a free +25% to external bandwidth, and it doesn't erase a \~40% raw GB/s gap. Even if we assume AMD just matched or let's even go as far as to say \~10% better than Nvidia's compression, they'd still be well behind on effective bandwidth.  The simpler explanation for those results is that the tested scenarios aren't that bandwidth-bound. Once bandwidth clears a threshold, other limits (shader/geometry/queues, cache behavior, drivers) dominate. So ""RDNA4 compression makes GDDR6 keep up with GDDR7"" overstates the feature by a lot.",hardware,2025-08-26 21:04:21,20
AMD,nax4ar7,"No G6 24Gbps it's still sampling, not in mass production yet.   Doubt that'll happen before GDDR7 3GB becomes widely available.",hardware,2025-08-27 09:48:22,6
AMD,nb1tfh9,They're not spending that precious die space on more cache.,hardware,2025-08-28 00:45:19,3
AMD,nawzduy,"It's reuse as APU GCDs. You save cost on the actual memory, but increase die size because of the bigger memory controller, so cost wise it should be a wash.    It's all rumors anyway though.",hardware,2025-08-27 09:01:30,6
AMD,nb9m4la,"true, the funding situation is a lot better, and they did increase their software team significantly. so hopefully better launches from now on. yet still felt a need to lie about zen 5 launch so....",hardware,2025-08-29 05:33:00,2
AMD,nauo993,That he is.,hardware,2025-08-26 23:00:41,8
AMD,nazu06q,I remember reading about the kyro evil king when it came out.,hardware,2025-08-27 18:40:25,2
AMD,nb3lrru,"A safe iteration how exactly? Pretty much everything across the WGP and outside of it was tweaked in some way. Out of order memory handling, oriented bounding boxes for RT, dynamic register reallocation, the hugely beefed up WMMA capabilities.  You don't get a ~50% performance bump per WGP - about 35% of which being per-clock - with just small iterations.  No, reality is like an other commenter suggested, AMD just held back RDNA4 a little longer whilst they focused on nailing down drivers for a few months. They left a larger gap between mass production and release intentionally compared to RDNA3.",hardware,2025-08-28 08:56:38,3
AMD,naugp6y,"Sony has a 30B contract.  I bet Microsoft is similar.  Like you said, the word is that they are using UDNA with a focus on Path Tracing performance.  Hoping for 48-64GB VRAM.",hardware,2025-08-26 22:18:28,7
AMD,nb1juc1,"It is it's being used on everything including handhelds, it's basically the new RDNA2.",hardware,2025-08-27 23:49:32,2
AMD,nawec6e,https://www.tomshardware.com/pc-components/cpus/amd-announces-unified-udna-gpu-architecture-bringing-rdna-and-cdna-together-to-take-on-nvidias-cuda-ecosystem  Isn't the whole point of UDNA to unify RDNA and CDNA into one arch?,hardware,2025-08-27 05:41:11,5
AMD,nb50b5u,"Due to the shitty MCM implementation in RDNA3. They're laying the groundwork for something better but it's reserved for a post RDNA5 as confirmed by the leaked ATx lineup.  It'll probably be a Accelerated interface die acting as a base die (with PHYs, L2 and CP) connecting directly to mem PHYs. This is the workload preparation stage (work items), then these are distributed and load balanced across autonomous shader engines that handle their own scheduling and dispatch. We can call these shader engine dies (SED). This will probably be clusters as one SE is very small, half the size of Zen chiplet rn. On the side AMD has a Media interface die with encode/decode, display, and IO.  Speculation but it's not far fetched, based on CDNA GPU packaging and AMD patents.",hardware,2025-08-28 14:29:35,2
AMD,nb3vnkm,"With how far behind Intel are on PPA even with Battlemage, it's probably going to take those 1/2 generations to get to Druid for them to catch up with a product that doesn't actively cause them to bleed cash whilst selling it.   I wouldn't count of a technically competitive Intel GPU product before 2029 at the earliest. Sure it can be competitive on market value if they're willing to continue to sell at a loss, but that doesn't help Intel in the long run that is already very cash strapped.",hardware,2025-08-28 10:27:29,3
AMD,naxro34,"Kepler\_L2 said L0 and L1 is merged in CDNA4. Maybe RDNA 5 goes even further and merges all CU level caches (including VRF) into one big shared flexible cache like Apple did with M3 and A17 Pro.   Like you said it's about SRAM scaling being bad and minimizing SRAM investment. N3P = no scaling, N5 poor scaling vs N7.      NVIDIA definitely has the advantage here while AMD's cache system is overly complicated and inflexible. The next logical step is to make caches into one big shared block that can be dynamically allocated for different purposes.   L1 will be much more important with the new WGS and ADC scheduling and dispatch within each SE. No more global scheduling only work item preparation and load balancing. This and other methods could explain why L2 will be less important and less used allowing AMD to use 24MB L2 on AT2 (According to MLID and Kepler\_L2 rumours).   Actually the rumour for AT2 is +25% CUs and -25% PHY width vs 9070XT, so perhaps AMD has some novel and forward looking memory saving technology in UDNA. AT2 on paper could easily be stronger than 4090 in raster and that's wild with 24MB L2 and such a weak memory subsystem.  Not expecting anything at AMD FID 2025 but maybe there's a slim chance that they'll share a glimpse into RDNA5. 2027 can't come soon enough.",hardware,2025-08-27 12:37:37,1
AMD,nb40da7,"Should've worded that differently. All I can say is look at the patents, including the ones Kepler\_L2 shared 3 weeks ago. AMD is matching and exceeding 50 series functionality + tons of scheduling overhaul. If we assume AMD just caught up to 50 series then AMD still has OBB + Dynamic VGPR allocation and Ray instance node transform in HW, NVIDIA doesn't have these rn. But like I said there's so much more in the patent filings going beyond NVIDIA 50 series implementation.  Don't care about the BS MLID PS6 perf PT >5080 or Kepler's RT 2X perf gain per CU.  What will likely happen is that AMD goes 20-30% ahead of Blackwell at iso-raster but NVIDIA cranks PT HW to eleven and more than doubles PT performance at each tier.",hardware,2025-08-28 11:04:44,1
AMD,nav2vkc,"Compression aside, AMD relies on large caches.",hardware,2025-08-27 00:23:14,1
AMD,naxj4e6,I didn't think of that!  And yeah its rumors but its still interesting to think of what they'll do.,hardware,2025-08-27 11:44:10,3
AMD,nb9ojw1,"Now that NVIDIA has lowered the bar with 50 series drivers, then perhaps AMD can get away with more xD  We'll see.",hardware,2025-08-29 05:54:35,1
AMD,nb4xvl0,"Agree with u/Strazdas1 RDNA4 was about adressing a ton of issues in RDNA3 + AMD GCN baggage and obvious issues that needed to be fixed.  Meanwhile RDNA5 will literally change everything. Complete clean slate moment. You're comparing a Terascale -> GCN moment with a major architectural tweak, albeit still no more than a tweak.  NVIDIA has had OoO memory since Turing. WGPR is really cool but AMD wasn't the first to introduce it. Apple has had it since 2023. OBB is nice and actually an AMD first for once but I suspect this is really Cerny's doing more than anything. WMMA is just catching up to NVIDIA 40 series ML HW 2.5 years later.  RDNA4 had a small relatively IPC increase. I did the math months ago. Think it was around 6-8% but not sure. Only exception is 7600 -> 9060XT but that card used RDNA 2 WGP data stores and not the beefed up RDNA 3 stores.  AMD held it back because they wanted to see what NVIDIA priced the cards at, then they didn't invest enough in the driver team + had absurdly low stock at launch so they had to delay the launch.  AMD has not been serious about competiting against NVIDIA since IDK when. where's the prebuilt SIs and OEM contracts? 8% total market share what a joke, if you look at prebuilt vs DIY then it's not good for AMD. They need to increase market share in ALL markets not just some.",hardware,2025-08-28 14:17:54,5
AMD,nb9k3w9,"yes, a tweak everywhere to improve on RDNA3 rather than an attempt to make any real improvements. Going the same paths already taken by others and known to work.",hardware,2025-08-29 05:15:44,1
AMD,nb4y24j,...Except better. Forward looking instead of regressive and reactionary. UDNA or whatever AMD ends up calling it is another GCN moment for AMD.,hardware,2025-08-28 14:18:47,3
AMD,nawual1,AMD never said MI400 is going to use UDNA architecture. I have been following the scene very closely and there's no proof MI400 and RDNA5 will use the same arch.  I believe the unification won't happen in the next generation and it needs more time. Unifying architectures doesn't happen at the snap of fingers. It takes a long time especially when architectures are very different.,hardware,2025-08-27 08:11:42,6
AMD,naxu0ts,How do you think RDNA5/UDNA will compare with Xe3? (Design work finished for Xe3 core ip in dec 2024)   More importantly if Intel wants to keep pace with RDNA5/UDNA assuming the WGS and ADC scheduling from the patents makes it into the final uarch then Intel needs to finish the Xe4 core IP in late 2025 or mid 2026 and release products using Xe4 in 2028 or 2029   **Intel vs AMD big APU's**  Nova Lake-AX will use 512 XVE's or 64 Xe3 cores and a mini version will have 256 XVE's or 32Xe3 cores according to rumors. Both of them will compete with RDNA5 48CU Medusa Halo and Medusa Halo Mini  Nova Lake will use Xe3 graphics and the Xe4 media engine (guess the core IP wasn't finished yet),hardware,2025-08-27 12:51:24,2
AMD,nb9lcrv,"if the patents you mean are ones you laid out in a few posts you did about it, i think the conclusion we came to was that it would put it on part with Nvidia, but Nvidia isnt going to sleep either so AMD unlikely to eclipse.  Also patent filing and actual product isnt the same thing. A lot of patents never get fully implemented. with AMD we have a history of overpromising rumous and underdelivering releases.",hardware,2025-08-29 05:26:21,1
AMD,nax4493,Yeah but UDNA seems to completely change that. No MALL in ATx lineup and only 24MB L2 in rumoured AT2 SKU despite a possible +25% CUs vs 9070XT.,hardware,2025-08-27 09:46:45,2
AMD,nbrbwmh,"For sure, AMD better pray Nvidia doesn't have a Kepler to keep with it like GCN1 did.",hardware,2025-09-01 00:54:59,1
AMD,nawxgir,"MI400 is the generation after next - CDNA5. CDNA4 is MI350 (gfx1250), CDNA3 is what's currently available on the market.",hardware,2025-08-27 08:42:53,4
AMD,nax3r8r,There won't be a unification. The last thing AMD needs is another HPC Vega disaster situation. DC blackwell is entirely different from consumer Blackwell.  It's about merging the fundamental design (ISA + basic blocks) and how the chip cache hierarchy is configured. The rest will be wildly different.,hardware,2025-08-27 09:43:30,4
AMD,nayin12,"Kepler\_L2 confirmed it's all in RDNA 5/UDNA. SWC, WGS, ADC, GMD, MID, Local launchers and likely many other yet to be disclosed changes. For RT LSS, DXR 1.2 compliance, DGF and prefiltering nodes, low precision ray/tri intersection, overlap trees and many more HW level changes to RT core in UDNA.   No idea but expect RDNA 5 to leapfrog pretty much everything else. This is another GCN moment for sure. 60 series might hold PT advantage and introduce novel NeRF ASIC block in 3D FF.  But Intel needs to expedite their Xe gen roadmap if they want to stay relevant.     Sounds very interesting. Didn't know Intel will have a mobile GPU targeting high end using LPDDRx as well. Interesting. First Apple, then Qualcomm, AMD, then NVIDIA and now also Intel.",hardware,2025-08-27 14:57:28,3
AMD,nb9ocih,"The new patents changes things quite significantly especially the scheduling changes that align with GPU workgraphs, but that's reserved for games many years into the future (fine wine). Apparently also major cache level changes in RDNA5 so that's another source of potential RT gains.  Conclusion on Spring post was a conservative on par with NVIDIA Blackwell, new patents suggest significantly ahead. Yes indeed and I mentioned that at the bottom of my comment. They can't allow AMD to take the PT crown. Also maybe something insane nextgen like a NeRF ASIC within the geometry pipeline. IIRC NVIDIA had a paper on this a while ago.     That's true but the interesting thing is that if you go even further back IIRC I couldn't find an RT patent that didn't get implemented in RDNA4. So at least for RT patents most of them can reasonably be expected to be implemented in RDNA5. Skeptical about the DMM patents, but everything related to DGF and prefiltering nodes is pretty much a given.  Perhaps this time it's different, we'll see but not fully convinced either. And there is still the possibility that a lot of the patents might be reserved for AMD's nextnextgen.",hardware,2025-08-29 05:52:44,1
AMD,nbssx5f,"Don't think that kind of thing will ever happen again. Designs are already extremely optimized on both ends. Doubt NVIDIA can do another Fermi -> Kepler or Maxwell (not happening) moment, but perhaps they'll surprise us. I could see them pulling a Maxwell moment with PT though. The fundamental core is still Ampere which at is core is still Turing/Volta so a clean slate might happen and that could have massive ramifications similar to AMD's RDNA5/UDNA.  On AMD side let's just hope the architectural changes are massive + AMD's patent derived (unconfirmed) scheduling overhaul is forward looking like GCN Async compute. Prob not relevant for launch and first couple of years. but the other changes can make a massive impact as well.",hardware,2025-09-01 07:27:06,1
AMD,nax4959,>MI400 is the generation after next - CDNA5  Indeed and that was known. MI400/CDNA5 is coming late 2026. RDNA5 in H12027?  >CDNA3 is what's currently available on the market.  AMD announced that they started shipping MI355X/CDNA4 to hyperscalers 1-2 months ago.,hardware,2025-08-27 09:47:58,4
AMD,nazjbsu,"I second this, there is no reason to waste MI400+ series die area on things like RT engines.",hardware,2025-08-27 17:50:50,3
AMD,nb1k6f3,Intel needs to pour money and manpower into finishing the Xe4 graphics IP and pulling forward it's release date  Nova Lake-A and AX will use the Xe3 graphics IP (both are expected to release sometime in 2027]  Intel needs to have Xe4 versions of both big APU products ready in 2027 or 2028   **What I think happened**  I think Exist50 was right. Xe3P based Celestial products were likely canceled after the disastrous midyear earning call in 2024  After the unexpectedly huge success of the B580/B670 Intel must've then restarted their DGPU/Large iGPU plans  That's because We're not seeing any DGPU Celestial leaks but we are seeing Nova Lake-A and AX that are supposed to ve ready in 2027  Considering Xe3 will be used in Panther Lake (Q4 2025 and Q1 2026 release) it supports my hypothesis.  **Intel's DGPU future:**  If we don't see DGPU Celestial products in 2026 then either we don't see them or Celestial will be like Battlemage and Intel will have to sell bigger dies that compete with smaller AMD and Nvidia counterparts in 2027 (likely sharing GPU silicon dies with Nova Lake-A and AX)   If they do release a Celestial DGPU then it will likely only be 1 or 2 SKU's that compete on price and are ready in 2027  It will likely be very quickly replaced with Xe4 Druid DGPU's in 2028 which should look more like a proper    AMD/Nvidia  GPU lineup.,hardware,2025-08-27 23:51:25,2
AMD,nay8q4m,"MI400 is early 2026, AMD seems to be pulling it up",hardware,2025-08-27 14:09:17,2
AMD,nazsd69,"Indeed. There will be a unification but not how most people think.. CDNA is GCN++++. RDNA is well RDNA. Now it's time to unify the foundation for each architecture into UDNA.   It's probably more so about unifying how the cache hierarchy, share core ISA and CU layout (partitions, etc...). Everything else will be wildly different between the two, just like NVIDIA Blackwell DC and Consumer.",hardware,2025-08-27 18:32:28,3
AMD,nb40wpk,"Your previous two comments were deleted. Something about cache hierarchy overhaul and another comment about Xe4.  I hope you're right. If Intel wants to compete then it can't be a Vega 64 vs 1080 situation, as was the case with 4060 vs B580. Hope Xe4 is a miracle GPU architecture. PC market needs some competition rn.",hardware,2025-08-28 11:08:40,3
AMD,naycwja,"No, they're not! LOL! AMD can't launch 3 new INSTINCT series in just 1.5 years! LOL! AMD pulled MI355X/CDNA3 by half a year because it was a minor redesign of MI300X/CDNA2, but MI400 is still on track for late 2026., because it brings major changes to the architecture, NICs and cabinets designs. When you make such a major design changes you don't pull a launch by a year. That's crazy.  LE: MI400 is not coming in early 2026 because it comes bundled up with Venice, which launches in late 2026, so your speculation got KOed.",hardware,2025-08-27 14:29:45,2
AMD,nav1xbv,"So it sounds like quantum compute is going to have conventional compute fallback, specifically for error correction and/or fault tolerance.  So, basically, quantum-powered compute output, and conventional compute checking the faults in quantum output, and potentially correcting them, so quantum output can continue.   Sounds to me like they're trying to throw conventional AI at quantum computing to see what happens?",hardware,2025-08-27 00:17:41,8
AMD,nal6r0e,"I heard Asrock boards are having issues with 9000 series processors. Isn't it just the X3D chips that are having problems? Even the normal ones are getting fried too? I'm using a B650 Asrock board and am now concerned about my chip dying, should I update my bios?",hardware,2025-08-25 14:22:18,24
AMD,naktu1w,Asus Prime are terrible as always.,hardware,2025-08-25 13:11:17,69
AMD,nalfp4g,"Good testing.  But it should be remembered this is **testing with Ryzen 9 9950X** and DDR5-8000.    Your 7800X3D/9800X3D on DDR5-6000 will be fine on pretty much any board, gamers.",hardware,2025-08-25 15:07:27,44
AMD,nakwtqf,like the look of the Gigabyte+ B850M Gaming X.,hardware,2025-08-25 13:28:30,8
AMD,nal77br,"One of my two new builds this spring is an Asrock B850I Lightening ITX board paired with an Ryzen 8700G APU. Intended for office work, browsing, Netflix, VPN and a little 3D graphics it works very well for its intended purpose. It's quiet and small while drawing little power. This will do for many years.",hardware,2025-08-25 14:24:37,7
AMD,nalfiv8,"Ah, the B850 where AMD mandated PCIe5 but forgot they cheaped out on PCIe lanes so we get cursed lane distributions and M.2 slots sharing lanes with anything like it's B450/550 all over again. Like, the B650 boards are actually better unless you *reeeaaaaly* need that PCIe5 (and no, you don't).  Man, Intel boards are so much better featured it's not even funny...",hardware,2025-08-25 15:06:36,18
AMD,nal2ff5,"As'ss brands being ass as usual.   Reddit conjecture doesnt undo the fact that asrock can't have a well used usb port last 3 years or asus can't deliver a decent vrm or memory support for under $300. And I don't blame malice on their part for that, whomever is sourcing their capacitors is the biggest culprit imho.",hardware,2025-08-25 13:59:44,12
AMD,namfboa,I wish they had the Gigabyte B850M Force available to test  It is a motherboard with a lot to like and a lot to hate,hardware,2025-08-25 18:00:20,2
AMD,nalbisv,"Oh look, Asus Prime still being a dogshit board, who would have guessed.",hardware,2025-08-25 14:46:37,4
AMD,namiyqt,30 minute spec sheet readout. Very informative.,hardware,2025-08-25 18:18:13,4
AMD,nalddkq,Not sure whether I'm more afraid of ASUS firmware or ASUS hardware these days.,hardware,2025-08-25 14:55:53,3
AMD,nap9dp1,The Micro ATX Gigabyte B850M GAMING X WIFI6E looks to be pretty good.?,hardware,2025-08-26 03:14:27,1
AMD,nau6n2y,even tho Asus primes boards are garbage their TUF boards are banger .,hardware,2025-08-26 21:26:28,1
AMD,naw2qli,"Asus as always has the worst cheap boards. They rely on their high to medium priced board quality to get a good reputation, so they can then sell bad and cheap boards at good profits at the low end.",hardware,2025-08-27 04:06:19,1
AMD,nazwyfu,"Props to the companies putting a double digit number of USB ports on their boards.  You can easily have too few USB ports, but you can't have too many.",hardware,2025-08-27 18:55:04,1
AMD,nbgm3vp,Best b 850 msi mag tomahawk max wifi,hardware,2025-08-30 08:43:34,1
AMD,nalme9z,"Most of the reports seem to be 9000 series X3D, but thereâ€™s a lot of buzz, so some reports of other 9000 and 7000 line up are getting reported. Could be normal failure rate, could be not. Asrock subreddit mods promised summary of all reports theyâ€™ve collected so far to be available this week",hardware,2025-08-25 15:40:34,16
AMD,namftxq,My 7700X died in an ASRock Pro RS X670E,hardware,2025-08-25 18:02:49,11
AMD,nal7zs2,">I heard Asrock boards are having issues with 9000 series processors. Isn't it just the X3D chips that are having problems? Even the normal ones are getting fried too? I'm using a B650 Asrock board and am now concerned about my chip dying, should I update my bios?  It's mainly the X3D CPUs that is reported in the Asrock subreddit, but AMD do (going by posts) honor the RMA with no hassle.  Edit: And using 800 series chipset.",hardware,2025-08-25 14:28:38,8
AMD,namxpsf,"There are a handful of reports of 7000 series CPUs as well, and a slightly higher number of 9000 series. A vast majority of the reports are 9800X3D though.  We also don't know if it's still a problem. ASRock released a BIOS to supposedly fix it, but we've also seen failures after that. Was that because the CPUs were on the previous BIOS for some time? Dunno.",hardware,2025-08-25 19:30:21,2
AMD,nalu88o,"It seems that even other brands have issues, there was a news about it today with AMD giving a comment that ODMs should follow the base settings.",hardware,2025-08-25 16:18:52,2
AMD,nalmktw,The issue has been addressed via BIOS update. There was a problem with AsRock allowing too high of PBO currents by default which was frying some CPUs (in combination with a Ryzen Master bug which was enabling PBO without user interaction).,hardware,2025-08-25 15:41:26,5
AMD,nanbps0,"I mean, you probably should update BIOS either way. It's free performance and fixes. But it's a little vague right now. So... No way to know for sure.",hardware,2025-08-25 20:36:52,1
AMD,nalbire,">I heard Asrock boards are having issues with 9000 series processors.  We are talking maybe 100-200 out of tens of thousands of boards, and other brand also had some problems with the 9000 series.",hardware,2025-08-25 14:46:37,-5
AMD,nalgavp,Prime used to be decent. I'm still using my Prime X370-Pro from 2017 with 5700X3D. Then the series got enshittified by Asus really hard.,hardware,2025-08-25 15:10:30,41
AMD,nalvu6f,">as always.  Prime was good, guessing before you were into PCs.",hardware,2025-08-25 16:26:47,14
AMD,namyo2o,ASUS boards should just start at $250. Their low-end stuff is total dogshit and has been for a long time.,hardware,2025-08-25 19:35:00,0
AMD,namg2b4,The gigabyte b850/m gaming x wifi 6e is well priced enough that it should be the default recommendation.,hardware,2025-08-25 18:03:57,11
AMD,namn63j,"If I recall correctly, the Techpowerup Chart said in their gaming suite the 9800X3D used on average 65W with slightly more in PBO Max mode.",hardware,2025-08-25 18:38:59,3
AMD,nams42t,The only thing I miss with these boards is clear information about recommended and not recommended cpus.   I donâ€™t really think anyone pairs lower end boards with high end cpus but it should be in writing what CPUs they are intended for.,hardware,2025-08-25 19:03:08,2
AMD,naqovnk,"It's a torture test across platform life IMO. If it can take that stress test, it will handle the final gen on the socket with aplomb",hardware,2025-08-26 10:45:48,1
AMD,nar5bf5,it's a good way to actually stress the components tho,hardware,2025-08-26 12:35:39,1
AMD,namfxvs,I've used Gigabyte boards for years and they've been a bit of a letdown.  Sure they work but their feature set kinda seem like a bit nickel and diming.  I've been running Asrock for the last few generations and have been really enjoying their bang for the buck boards and love the stability.  Trying to get an old B550 mobo with 2.5GBe was hard to find on any Gigabyte boards but was easy to find on Asrock.,hardware,2025-08-25 18:03:21,1
AMD,nambrv2,"The PCIE 5 main slot doesn't affect lane sharing in any way, If any lane sharing exists it's just the manufacturer giving more options like they did with 600-series boards. b650, b650e, b850 are all the same essentially just with PCIE 5 being needed on some, even X870 confusingly is only single chipset rather than the daisy chained one of X670, X670E and X870E, and the difference to B-boards is just mandatory usb4 maybe better wifi or something as well.  >Man, Intel boards are so much better featured it's not even funny...  Are they? The low end stuff quickly glancing looks similar and ofc no OC on intel B-boards(aside from 12th gen BCLK on some). Yea Z-boards have x8 chipset connection rather than x4 and the cheaper Z boards are seemingly chepear than amd dual chipset ones are(of any gen I think X670(E) wasn't exactly cheap at launch either vs Z690/790) so i guess that way sure.",hardware,2025-08-25 17:43:34,19
AMD,nalo5mb,"> Like, the B650 boards are actually better unless you reeeaaaaly need that PCIe5 (and no, you don't).    OR be like me and luck out with a b650m-e that ASUS accidentally enabled 5.0 for the PCIE slot",hardware,2025-08-25 15:49:04,2
AMD,napny8m,"learned this recently myself but most b650 users can enable pcie5  while not officially supported, works just fine",hardware,2025-08-26 05:01:56,1
AMD,nal6msi,They do it deliberately.  They built up a good brand with their ROG Strix line before and then enshittification hit and they drastically lowered quality while keeping prices the same to increase margins.  Why it's important to read reviews and not depend on brand name.,hardware,2025-08-25 14:21:42,16
AMD,naluhcz,"And the crazy part, Asrock was a spin off Asus group, now Asrock is having better motherboards than Asus.",hardware,2025-08-25 16:20:07,6
AMD,nap8wx9,"Yeah, they put in zero effort on their low end. There's a couple exceptions though, like the x570 prime is fine.",hardware,2025-08-26 03:11:22,1
AMD,nalfl1o,">Not sure whether I'm more afraid of ASUS firmware or ASUS hardware these days.  Their Windows software is the big security problem, along with loosing private keys for signing their drivers.",hardware,2025-08-25 15:06:54,5
AMD,nawlo6s,Its almost 10% failure rate,hardware,2025-08-27 06:48:40,1
AMD,nalbdx0,">but AMD do (going by posts) honor the RMA with no hassle.  most people will prefer avoiding RMA by not buying Asrock - troubleshooting, then waiting for days-weeks for RMA is not something that you want to do - it was the exact reason why i purchased Gigabyte motherboard for my 9800X3D and not Asrock, i don't want to go through RMA because Asrock is faulty, i would rather choose a different brand.",hardware,2025-08-25 14:45:56,18
AMD,nan80bn,Heard rumours of a refresh release with  better memory controller.,hardware,2025-08-25 20:19:15,1
AMD,nan5dub,And what looks an awful lot like a few bad batches of Zen 3ds.,hardware,2025-08-25 20:06:55,-1
AMD,nalww6n,If you go to r/asrock itâ€™s a couple posts per day.,hardware,2025-08-25 16:31:59,9
AMD,nalqn13,"Yep, Prime used to be what TUF is now.  Mostly no bullshit, solid components and a decent price point with almost all reasonable features vs the Crosshair/Maximus lineup.  Classic ASUS, really.",hardware,2025-08-25 16:01:07,29
AMD,nammo6f,"Hell, Asus used to have the equivalent of the Noctua tax, and people didn't mind much.",hardware,2025-08-25 18:36:32,7
AMD,nase7j7,"They can only afford to be bad because they used to be good. Built the name on affordable quality, slowly scaled back both affordability and quality in favour of profit. Here we are.",hardware,2025-08-26 16:20:41,1
AMD,naq998j,Price doesn't mean the product will be good.,hardware,2025-08-26 08:21:23,1
AMD,namw6v6,"For gaming workloads, that's probably about right. 50-75W on average, maybe maxing at around 85-90W in heavier games. For all core workloads, it's probably closer to 115-125W depending on your settings. Still low enough to be handled by even mediocre low-end boards, but not nothing.",hardware,2025-08-25 19:23:03,8
AMD,naqur4b,"> I donâ€™t really think anyone pairs lower end boards with high end cpus  Why not? Consider how ridiculous would it be to say that you shouldn't pair cheap USB devices with your expensive computer.  Limitations should be clear, but we are way beyond the old times of hardware safety depending on the user. Power delivery systems of consumer products destroying themselves without protections kicking in are simply faulty.  > it should be in writing what CPUs they are intended for.  A user friendly list could be nice, but it would be more generic to properly disclose power requirements and capabilities instead of CPUs advertising TDPs not matching power consumption, and bad motherboards attempting to deliver power beyond their safe capabilities.  An intended CPU list wouldn't tell me a whole lot, while I could make a better decision seeing a 90W CPU, and potentially settling for a motherboard delivering only 80W continuously, knowing that I ended up with a budget build limited by the motherboard. This can already happen, it's just not disclosed properly, and some motherboards don't have properly set power delivery limits.",hardware,2025-08-26 11:28:47,3
AMD,narl809,">I donâ€™t really think anyone pairs lower end boards with high end cpus  I mean there are low end boards with good vrm that'll have no problem powering higher end cpu:s, so if you just want ""a board"" and don't care about any extra stuff, something like the asrock B650M hdv/m.2 is better than a lot of these boards due to it's very low price at jsut shy over $100 USD. Sure no pcie 5.0 main slot, but that's a non issue unless you run out of vram on an x8 wired card, even a 5090 doesn't really lose anything going pcie 4.  Also shockingly it's in stock currently cause last year it spent most of it's time out of stock or incresed price, though there is the asrock cpu issuebeing talked about and idk if that affects 600-boards as well.",hardware,2025-08-26 14:01:56,2
AMD,nao52o8,"I think that given there is no Ryzen 3 on AM5, the only possible difference is if they support OCing on 16 C cpus properly with enough power fed to them. And because AM5 is positioned as the premium option over AM4 where the value buyers were pushed for the longest of time (and even now unless you have ali you can't get sub 100 dollar CPUs for AM5), all boards at least support fully loaded 16 C AM5 offerings and most likely is fine with OCing said chips too unless you are going for something like LN2.   there isn't so much really cheap, really low quality boards that only supports ryzen 3 4 core processors and maybe stretch to 6 and 8 cores and without OCing like it was with AM4 (where you can for example pair up cheap and old A320 boards with 5800X3D no problem because you can't OC the thing).  And as a result, not a lot of places do them now, as most people who games just grabs X3D and rarely OCs them since its the cache that mostly make them good and with all the issues with them cant be OCed or possibly burn them out by all that jazz, people stick with stock with them now unlike before when if you want to game on intel you pushed a 5ghz all core OC at the minimum. which meant you had to have good motherboards.",hardware,2025-08-25 23:13:53,1
AMD,napnvc8,pretty sure most folks are reporting being able to run pcie 5 on most/all b650 boards,hardware,2025-08-26 05:01:16,1
AMD,naqv907,"> Why it's important to read reviews and not depend on brand name.  Going with a mix works too, just not like how some people do it.  I avoid ASUS because they've shown they are willing to associate their name with garbage quality. They may make some okay products, but with good competition, it's just easier to skip the riskier companies not caring about their reputation.",hardware,2025-08-26 11:32:15,1
AMD,nal87h3,"My fuckin b450 Strix can't even handle 64GB of ram at 2666mhz. 2400 is the best it can do. Don't even bother asking it to let the CPU run to TDP either. 80w, best it's got. (1000w PSU)",hardware,2025-08-25 14:29:43,0
AMD,nasfuuj,That's been the case for a long time,hardware,2025-08-26 16:28:35,1
AMD,nan5yz6,In a way that seriously calls their whole damn software stack into question.,hardware,2025-08-25 20:09:35,0
AMD,naws6ol,"Didnâ€™t word it well enough, I meant non-X3D cpus that are getting reported could be within normal fail rate, but are getting reported as well, while in normal circumstances itâ€™d be just quietly RMAâ€™d",hardware,2025-08-27 07:51:05,1
AMD,nali6ld,"Yeah, I have an asrock for my 5800x3d cpu, but if I upgrade, no brand loyalty here.",hardware,2025-08-25 15:19:53,5
AMD,nampkex,"Same boat here, it's just not worth the headache. It's a shame because as proven even in this review Asrock boards offer excellent hardware for the price.",hardware,2025-08-25 18:50:49,4
AMD,naldcj5,">most people will prefer avoiding RMA by not buying Asrock - troubleshooting, then waiting for days-weeks for RMA is not something that you want to do - it was the exact reason why i purchased Gigabyte motherboard for my 9800X3D and not Asrock, i don't want to go through RMA because Asrock is faulty, i would rather choose a different brand.  I absolutely sympathize with that! But all motherboard manufactures have their various issues occurring to some of their motherboards.  Missing PCIe lanes for the GPU slot is a bummer for MSI and Gigabyte, for instance. Various longstanding serious security issues in software and UEFI: Hello Gigabyte and ASUS!  So I went with Asrock. None of my builds use an X3D version CPU, though.",hardware,2025-08-25 14:55:44,3
AMD,namcfv7,">If you go to [r/asrock](https://www.reddit.com/r/asrock/) itâ€™s a couple posts per day.  In that forum any boot problem is by default attributed to a dead CPU, so there is that. Also a number of trolling posts.",hardware,2025-08-25 17:46:41,10
AMD,namvttd,if it's on r/asrock it must be true,hardware,2025-08-25 19:21:17,-1
AMD,napdaew,"And TUF motherboards used to be relatively high end, no-nonsense boards with a focus on durability (hence the name). At some point they turned it into a ""budget"" gaming brand of sorts.",hardware,2025-08-26 03:40:57,7
AMD,narmm8k,"Well no, I more meant they should drop their <$200 product lines, because they're complete crap.",hardware,2025-08-26 14:08:56,2
AMD,nalrkvo,"Had X570 ASRock for 3900X, now went with MSI X870E Carbon for 9950X3D after reading about ASRock current issues",hardware,2025-08-25 16:05:47,2
AMD,namyd47,"Yeah, people don't seem to realize that there is a natural failure rate for *all* hardware. For CPUs it's pretty low at ~0.25-0.5% from what I can tell of past hardware data. That still means hundreds or thousands of CPUs failing in their lifespan.  At /r/asrock they seem dead-set on blaming ASRock for any failure at all. Not saying they don't deserve *some* blame, but failures happen regardless of brand.",hardware,2025-08-25 19:33:31,6
AMD,nasehre,Why would they its free money? The real quality products sell the asus brand and they can sell their leftovers to the poors who watch tech tubers glazing their top end products.,hardware,2025-08-26 16:22:03,1
AMD,nan5lib,"Especially when some of it's gonna be the longstanding ASROCK problem with being fussy about RAM makes... which honestly, why the fuck if you can get it for a not inane price, are you not rocking GSKILLs with that warranty and compat anyway?",hardware,2025-08-25 20:07:53,-1
AMD,nasqr5r,"I really wish I could justify the price tag for one of these things, they are super cool, and upgrade-ability is so unique in the laptop space",hardware,2025-08-26 17:20:25,40
AMD,nauw5fh,Yup. I love the idea   Not the price,hardware,2025-08-26 23:44:34,10
AMD,navfdue,"Hopefully once more people start buying them, they'll be able to scale up production and get better volume discounts.",hardware,2025-08-27 01:36:10,7
AMD,naxffht,"I've been following them for a couple of years, but they still don't ship to Norway. ðŸ˜",hardware,2025-08-27 11:18:11,2
AMD,nbrju0n,At this price shouldn't it be more compared to snapdragon plus / i5 tier processors?,hardware,2025-09-01 01:44:16,35
AMD,nbs6g7e,">MediaTek Kompanio Ultra 910, a high-end System-on-a-Chip (SoC) ~~designed specifically for Chromebooks.~~ which is basically a rebranded Dimensity 9400    FIFY  Not saying it's a bad SoC, but come on the thing was designed for phones, not *specifically for Chrombooks*. Mediatek ain't planning on going bankrupt just yet.",hardware,2025-09-01 04:15:56,33
AMD,nbr6unx,Is there a teardown vid,hardware,2025-09-01 00:23:42,5
AMD,nbr670i,If only Chromebooks were made with less exotic hardware. These things are going to end up in a landfill somewhere faster than a normal laptop would.,hardware,2025-09-01 00:19:40,20
AMD,nbqume7,More on the Kompanio Ultra 910: https://www.mediatek.com/products/chromebooks/mediatek-kompanio-ultra,hardware,2025-08-31 23:09:11,5
AMD,nbr4501,How well do Crostini and VMs function on an ARM Chromebook?,hardware,2025-09-01 00:07:01,2
AMD,nbqubz6,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-08-31 23:07:22,1
AMD,nbtonfv,People actually review these piece of shit chromebooks?,hardware,2025-09-01 12:08:55,1
AMD,nbt88io,"It's ARM.  It'll never take off, now that RISC-V exists. In 1-2 years we will even forget these ARM laptops were a thing.",hardware,2025-09-01 09:54:46,-6
AMD,nbsyohn,"I think you need to realize that phones today are generally more demanding than a Chromebook is.  Most android phones sport a screen that's higher resolution than a 1080p screen which is what's on the Chromebook, so graphics wise, it's easier on the chip.  As for programs, Chromebooks aren't known for their processing chops but they're really glorified webcrawlers which again, the processors will be great for.  IMO this sounds like a great little machine.",hardware,2025-09-01 08:22:41,10
AMD,nbr72px,"Google's doing ten years Chrome OS support these days, and various right-to-repair laws compel parts availability for 5 years from last sale, so it's certainly improving.  Although I do think 10 years is not really ""that old"" for a computer anymore in terms of functional usefulness.",hardware,2025-09-01 00:25:06,26
AMD,nbsxmiq,> These things are going to end up in a landfill somewhere faster than a normal laptop would  Because people have trash mindset these days. They don't even try fixing things.,hardware,2025-09-01 08:12:21,6
AMD,nbulz42,The X925 does seem like a decent core.,hardware,2025-09-01 15:15:00,2
AMD,nbsxrxo,"No issues, Debian has amazing aarch64 support for quite a few years.  I would not expected Steam for ChromeOS to work, though, since that is abandoned now and will disappear in January.",hardware,2025-09-01 08:13:48,1
AMD,nbuivbt,How many years have we been hearing this now?,hardware,2025-09-01 14:59:39,3
AMD,nbv4i6r,What's it like still living in 2019?,hardware,2025-09-01 16:45:12,1
AMD,nbtacqg,"Apple is doing Macbooks with iphone chips now, it makes total sense. Specially in the near future when Mediatek and Qualcomm start using chiplets for mobile which TSMC says it's happening Then they can divide the mobile part (modem,Crazy good ISPs,etc) from the compute part (CPU+GPU+IO).",hardware,2025-09-01 10:14:22,7
AMD,nbt38x9,"You do not have way to fix these things, thats the pain   My uncle used to fix washing machines, most of the time that could be fixed with few electronic components swap or barrelling, now manufacturers block the way to change $5 barrelling part because is all sealed up as a one module with rest of tub. Same with programmers/control panels, instead of re-soldering few cheap components you need to buy whole module as it all sealed with ""hot glue"" or epoxy or they use ICs that are not available outside of their factory.  But from computers perspective is same, phones with locked bootloaders preventing you to install other OS even when phone is still in good shape and fully working, bunch of SoC Arm boards - like from ""fruit""Pi named - where manufacturer do not provide any drivers or way to install other OS and device is just DoA with custom debian 9 and android 10 image hosted on random MEGA server.   This is the problem  Ok, google give you 10 years of update, but what they will force that you are not allowed anymore to sideload apps - see current android thing - and let only use ""verified"" apps by Google to install for another reason labeled as ""child protection"". Now if you are locked from installing anything else on this machine you will ditch this device as well, who cares now its still 7 years of updates left, these are shitty updates that you dont want.",hardware,2025-09-01 09:07:05,14
AMD,nbt2apv,"They donâ€™t try to fix things because companies babe tried to make fixing more expensive than buying, from expensive quotes to parts pairing to soldered on memory and NAND.  Blaming consumers for this is the wrong root to start from.",hardware,2025-09-01 08:57:49,9
AMD,nbtovkp,How would you even fix this thing? It's a PCB in a plastic shell.,hardware,2025-09-01 12:10:27,1
AMD,nbtg63n,"One of my neighbors was fixing their broken washing machine and had to buy a proprietary part that was only available through the manufacturer. And that part was priced higher than a new washing machine, defeating the whole purpose of ""right to repair"".  His workaround was to buy another broken machine from Craigslist and pull the needed part from it.",hardware,2025-09-01 11:05:11,8
AMD,nbtfpsg,"Yes I agree with everything you say here. We do not have a way to fix things in most aspect, but some things are repairable. On topic about chromebooks, people think of them as cheap consumables they'd rather throw these away and buy a new one when they encounbter a problem because they're cheap. I was pointing this out, more than half of the things are purposefully made unrepairable by the oems but we can't ignore a consumer's responsibility to also give a damn about trying to repair every piece of tech.",hardware,2025-09-01 11:01:29,2
AMD,nbtf5wd,"I'm not talking about board level repairs or schematics. I'm aware of the fact that manufacturers are more to blame than consumers. But people do have this mindset of throwing ""cheap"" things away no matter the cost. And Chromebooks fit in this aspect really well, people don't even bother with basic troubleshooting because they think ""it's cheap so I'll just buy a new one"".",hardware,2025-09-01 10:56:58,3
AMD,nbtsaud,"It's not just a PCB, it has a screen, Keyboard, track pad, usb ports, power button and a speaker. Those things can be replaced with proper tools and a little bit experience.",hardware,2025-09-01 12:33:12,2
AMD,nbthvqe,Maybe if chromebooks were actually worth repairing people would repair them. Unfortunately spending $200 on a new display for a device worth $150 makes no sense,hardware,2025-09-01 11:19:00,7
AMD,nbtk6b3,"I agree, but you don't have to replace the whole LCD anytime there's a problem with it. It depends on the problem, if the glass is broken then yes it's pretty much trash although you can extract the driver board and the backlight plus it's the diffusers.  I've seen people here on reddit trash a whole laptop because they thought the display was broken, it was not, it just had a loose LVDS connection. Same thing with spots on the diffusers, they are technically replaceable but do require some effort.",hardware,2025-09-01 11:36:46,3
AMD,nbtpb4w,"The problem is extracting the separate layers of the LCD is incredibly difficult for a home user to do, on top of that you can't get separate components. Yes you can order specific panels and do panel swaps, but thats not an option if you have a bonded digitiser or bonded glass, the only option is to swap the entire display assembly.  I would love to repair my X1YG3 but Lenovo sells the display assembly for $800 AUD, when the entire thing is worth $350 at most.",hardware,2025-09-01 12:13:23,3
AMD,naatr6u,"If possible, AMD & Intel should force motherboard manufacturers to operate CPUâ€™s with default settings by default, unless the customers chooses to do otherwise.",hardware,2025-08-23 19:59:02,349
AMD,naawt1k,"Asrock must have a very poor clickthrough rate in the YT analytics of the usual hardware channels because you know if this was Asus, we'd have them screaming about this issue from every rooftop.",hardware,2025-08-23 20:15:40,91
AMD,nae3li9,"I have an ASUS MB, how do I check if the board is not over volted and which correct settings to apply?",hardware,2025-08-24 10:27:34,7
AMD,naasim5,I mean... It happens exclusively to Asrock. So unlike previous intel situation it seems believable.,hardware,2025-08-23 19:52:20,137
AMD,nb0c0tw,Fresh from the oven involving ASUS boards: https://gmplib.org/gmp-zen5,hardware,2025-08-27 20:06:29,3
AMD,nab09o5,Does it only happend to lga or also to pga?,hardware,2025-08-23 20:34:25,4
AMD,naar8uv,"Hello imaginary_num6er! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-08-23 19:45:27,2
AMD,nackfkw,">While this is a complex issue, we are working closely with our partners to resolve issues and further evolve the platform.  So it is not just an issue with AsRock messing up their BIOS.  It is also the only thing new that has been stated, and the rest of it is a repetition of what AMD said before.  It's funny that they're trying to somehow obfuscate this issue as harder to pin down because they say that they support more CPUs across generations on a platform vs the competition.",hardware,2025-08-24 02:20:27,2
AMD,nacdy3q,I have asrock and amd 9800x3d. How can I avoid this issue?,hardware,2025-08-24 01:37:10,1
AMD,nb0yc3w,"It is very likely, but I also blame the design of the socket as any form of warping can result in arching.  They should make ball socket pins, little dimples on the substrate integrates and rests on the socket. It would avoid this issue.",hardware,2025-08-27 21:53:22,1
AMD,nalac7g,amd can get away by producing defective cpus by putting asrock under the bus same thing happened with asus and gigabyte lot less lets hope someone can replicate the dying cpus and investigate it further,hardware,2025-08-25 14:40:43,1
AMD,naave4b,Considering it's only happening to ass rock yeah I can believe this. They always do something fucky,hardware,2025-08-23 20:08:02,-14
AMD,naats9g,"Considering it's mostly Asrock, it is plausible. But the fault still lies on AMD for not enforcing said guidelines or at least performing QA checks before permitting manufacturers to release the product into the market.",hardware,2025-08-23 19:59:12,-14
AMD,naex9xb,"Huge shocker, it wasn't AMD but the only vendor, Asrock, where chips have been dying",hardware,2025-08-24 14:02:32,-2
AMD,nab1e1g,"If we all want vanilla, the same flavor, the same performance, then forcing board makers to adhere to AMD's recommendations is fine.  Many enthusiasts are building pcs to tinker with them, to make adjustments to make them a little better a little different.  This is why we have options in bios to make changes.The ""p"" in pc stands for personal, we want to make it ours.  Manufacturers understand this and create products to sell into the enthusiast market to meet this need.  The manufacturers need to be mindful of the risks they take and not go to the extreme of torching cpus and mobos (a la Intel).  The manufacturers are trying to balance performance vs longevity.  In general they do a good job.  The problems with Asrock boards effected a small percentage of boards.  A significant number of boards were impacted to be sure, but still a small percentage.  The estimates I've seen are perhaps 1 to 2%.  Had it been as widespread as some think, 50%+, Asrock would have had to pull their boards immediately because that high a failure rate is unsustainable financially AND it will destroy the brand's credibility.    It is helpful to keep perspective on the magnitude of the problem and why the manufacturers are enticed to take risks.  The board makers are appealing to builders who are looking for competitive advantages.  Sometimes, the engineers fly a little too close to the sun.  This is similar to the problem Nvidia has with the 12vhpwr connectors.   There is no doubt there is a problem, yet the vast majority of 5090 cards run fine with no melting cables.  Most people with Asrock or other boards suffered no failures.  If you were someone whose board/cpu failed, it was a major problem and unacceptable.  I get that.  It is important to keep perspective.  I encourage the designers, engineers, manufacturers to push the envelope to wrest better and better performance from their designs and products.  When they do this we all benefit.  Amd occasionally, mistakes are made.  The test is how people and companies respond to the mistakes.  Will we own our mistakes, make amends, learn from the mistakes and move forward?  Or do we want to prosecute and persecute those that made the mistake?  If companies like Asrock were more forthcoming in publicly owning the mistake, then it might diffuse things.  We are all in this together and need to work together.  Thus endeth today's sermon!",hardware,2025-08-23 20:40:31,-12
AMD,nadbaao,"I dont care if it burns, as long as there are no external damages u just go to the store and return it as I have not damaged it and all the settings in the bios/software are there given to us to play with, ie deemed safe.",hardware,2025-08-24 05:51:38,-6
AMD,nabcqwl,"Definitely agree. Yet it seems complicated when it comes to XMP settings to run RAM above default jedec speeds. Different minimum voltages needed for different memory vendors and specs of the kit, not just the memory voltage but the VSOC voltage for the memory controller on the CPU. A lot of the AMD cpu failures were from mobo makers juicing the VSOC too high to guarantee the memory is stable.  Itâ€™s possible to enforce it but a lot of effort needs to go into testing and verifying minimum voltages needed needed on every single board and every single memory kit. And it leads to more product RMAs when just a tiny bit more voltage is needed to make a certain kit stable but the board isnâ€™t giving it. IMO they should be making the effort though. Could cause increased costs but thatâ€™s better than ruining reputation frying chips and getting all that bad press.  Not saying the current situation with the mobo vendors is OK, they seem pretty lazy about setting too high voltages, especially for VSOC, when XMP is on, and just calling it fine and shipping it. They could do better. Not to mention straight up bugs and bad code in the UEFI and interface that cause overvoltages even when it *should* have worked fine.",hardware,2025-08-23 21:44:22,71
AMD,nabvxk9,I would be okay with a first party motherboard too.,hardware,2025-08-23 23:41:29,15
AMD,nac5848,"The problem is most do, until you enable XMP or Expo, at which point the board makers go nuts and start cranking power and voltages to make their board seem artificially better.  They need to be more forceful around the XMP and Expo settings otherwise the only way to guarantee the safety of the CPU from an end user perspective, is to cripple the performance with JEDEC ram speeds and timings.",hardware,2025-08-24 00:39:43,10
AMD,nabpjmz,amd and intel profit from those companies to take the risk otherwise they perform noticeably worse in benchmarks.   AMD is in it themselves. XMP is considered overclocking by them and supposedly voids warranty.,hardware,2025-08-23 23:02:12,16
AMD,nagyrij,"Except it's in their interest to do the exact opposite.   If the vendors are pushing past the cpu limits to claim higher performance the cpu manufacturers get all of the kudos for the higher performance and better benchmark scores but then get to put the blame and liability for defects in the motherboard vendors.  It's a win/win for them, they aren't going to stop them.",hardware,2025-08-24 20:23:03,2
AMD,nadv9ec,"Where do settings like EXPO and such should stand, then? Because, EXPO has become the defacto ""default"" if your RAM is compatible.",hardware,2025-08-24 09:04:10,1
AMD,naq0k0z,The mobo makers wouldn't like that. It would make differentiation in their product stack more difficult. They want to be able to offer the XsuperXgamingXXX model that costs $800 for marginally improved default performance.,hardware,2025-08-26 06:55:52,1
AMD,nadtbtx,"How do you â€œforceâ€ them?  Theyâ€™ll send a black and white document, follow this shit or elseâ€¦!, manufacturers would just say ok bro chill, we will.   Who goes and check after that?",hardware,2025-08-24 08:44:35,-1
AMD,nab43h4,"Then AMD dollars and free x3d chips might stop flowing if they talk about chips burning up no matter who's to blame.  Now if they were all buying their own hardware, they'd be eating up them views for sure.",hardware,2025-08-23 20:55:13,-31
AMD,naemaa2,"Contact ASUS support with the question or ask in their forums. Each MB makers have their own names for settings.   In the past ASUS had an oopsie where if you used XMP/EXPO that raised VRAM voltage it also raised SOC/CPU voltage, resulting in some fried 7800X3D CPUs which was the most voltage sensitive model. Asus fixed that after Gamers Nexus did a video about it as some customers were getting denied warranty.Â    If you are on default settings or only have expo enabled you should be fine. Just update BIOS to be sure as AMD can make some updates/fixes on their part.",hardware,2025-08-24 12:55:45,5
AMD,nb42v6t,Disable PBO and Expo.,hardware,2025-08-28 11:22:26,1
AMD,naazu8b,"It wasnâ€™t exclusively, but it was massively weighted towards ASRock boards.",hardware,2025-08-23 20:32:06,117
AMD,naba6g4,"I had 2x asus boards that were way over volting the 2x 9700x i went through with them.  2 sets in a row did the exact same thing after bi9s update, pushed the baseline voltage above even the safe range for temporary boosts.  Third cpu went with the msi edge ti, no issues, voltage in the safe ranges.",hardware,2025-08-23 21:29:37,27
AMD,nabd69e,"ASRock was just the latest. ASUS and GB were both frying chips back at AM5's launch, for multiple reasons no less. It's nuts to me that every single board maker can't seem to adhere to basic specifications anymore, on both Intel and AMD platforms.",hardware,2025-08-23 21:46:52,45
AMD,naata0e,"Initially, it affected all boards as far as I understand, but everyone except ASRock has fixed it with BIOS updates.",hardware,2025-08-23 19:56:25,56
AMD,nabbs79,"Only AM5, and only started happening after 9000s cpu came out, but appear to affect 7000s too if you look through the asrock sub",hardware,2025-08-23 21:38:49,21
AMD,nachkq8,Update to the latest BIOS or turn PBO off.,hardware,2025-08-24 02:01:09,7
AMD,naci3nb,Ensure you're running latest bios.,hardware,2025-08-24 02:04:39,3
AMD,nb436il,Disable PBO and EXPO.  But that will make you lose 10-30% performance,hardware,2025-08-28 11:24:35,1
AMD,naban5j,"Nope, had 2x asus rog strix b850e boards that killed 2x 9700x by way over volting.   Hwmonitor showed voltage a fair bit above the safe range.",hardware,2025-08-23 21:32:18,19
AMD,nab5hql,"I want to love ASRock, but they keep making it so hard.   They had/have the cheapest 9070XT in most markets, and it's not a bare bones, loud ass 2 fan solution. It's competent.   They supported ECC on all of their desktop AM4 motherboards (at least the 500 series boards). All of them, so they make a handy NAS boards when users retire them. (Almost all non-APU-based Ryzen AM4 chips support ECC. And any APU with Pro In the name). Fantastic use of retired parts, it's how I built my NAS.   They one of the last companies to make and sell AM4 boards.   Like ... You folks do some neat stuff ... But y'all screw up a lot.",hardware,2025-08-23 21:02:59,14
AMD,nab44kf,Ass rock is awesome lmao,hardware,2025-08-23 20:55:23,-6
AMD,naavu54,Not sure how this would even be realistic. How is AMD supposed to perform QA testing for other companies products. But I agree that they should probably be more strict on said guidelines,hardware,2025-08-23 20:10:28,28
AMD,nab3x4e,"It's the difference between you choosing too high a voltage yourself and the board doing it hidden inside bios code without any indication it's doing anything different or dangerous. Motherboards lie to the CPU about what voltage the soc is receiving or how much power the cores are receiving or lies about load line behavior so it will boost higher than is safe, that makes that board faster in benchmarks but kills the CPU, but they don't care about killing CPUsÂ  that's the customer and AMD's problem not theirs.",hardware,2025-08-23 20:54:14,12
AMD,naw6ajx,Thatâ€™s not how that works for tuning. Just because itâ€™s there to mess with doesnâ€™t mean itâ€™s safe..,hardware,2025-08-27 04:33:34,1
AMD,nabd7el,"For XMP, I personally think on first boot your BIOS should notify you that you have profiles which you can load to potentially improve performance, then let the customer choose whether or not to apply.  I feel bad for all the thousands of people out there with fancy memory running at default speeds :(",hardware,2025-08-23 21:47:04,64
AMD,nacos61,">Yet it seems complicated when it comes to XMP settings to run RAM above default jedec speeds. Different minimum voltages needed for different memory vendors and specs of the kit, not just the memory voltage but the VSOC voltage for the memory controller on the CPU. A lot of the AMD cpu failures were from mobo makers juicing the VSOC too high to guarantee the memory is stable.  Personally I'd like to see the whole memory ecosystem overhauled, especially now that JEDEC is being more forthcoming in defining frequency specifications with DDR5.   We shouldn't need cruft like XMP/Expo, and the likes of AMD/Intel should be more explicit about what their platforms are capable of, rather than coy statements about ""sweet spots"".",hardware,2025-08-24 02:50:30,10
AMD,nac636z,"I think what we have come to accept over time is madness, you should not sell a product that is advertised at an overclocked speed. Just advertise X frequency and let them run like that like it used to be before xmp became a thing.",hardware,2025-08-24 00:45:20,15
AMD,nabdhpk,"> Yet it seems complicated when it comes to XMP settings to run RAM above default jedec speeds  XMP should never be enabled by default, and even when enabled, rated voltages should be used unless specifically opted in.",hardware,2025-08-23 21:48:43,11
AMD,nad68uy,"Easier to just set the SOC voltage to the maximum safe/stable votlage for all RAM tested per CPU family. For example, set the default SOC to 1.3V for Ryzen 9000 CPUs when enabling XMP. 1.3V was the guideline AMD set for the chips. Then overclockers can play with it and undervolt as they see fit. Solves your testing issue and stops boards frying too.",hardware,2025-08-24 05:06:45,2
AMD,nahx0kp,"There's no good solution for all cases, the ""silicon lottery losers"" simply won't work in all cases. It's especially tricky lately as even the old overvoltage solution doesn't cover all cases, as too much voltage can also cause instability, likely with overdriving increasing noise.  However there are incredibly helpful tools and solutions for power users which are either just not presented to users, or usage of them are even presented.  A lot of chip(let) to chip(let) communication is already covered by ECC or at least EDC, and error counters are commonly available, they are just not properly (especially not uniformly) exposed.  For example on modern AMD CPUs, increasing FCLK too high can result in experiencing stuttering, making it highly likely that the IFOP is protected by some EDC. EDC error counters being exposed to users could be helpful with treating errors before they turn into crashing, and they could be also useful for faster and more reliable stability tests.  Then there's that whole ECC memory issue of reliable memory not considered being important for regular users. XMP/EXPO problems would be significantly better if users could look at error counters (even better, getting error notifications) instead of just running long memtest sessions and then hoping for the best.",hardware,2025-08-24 23:34:37,2
AMD,naqc6h9,XMP is not a default expectation though. Its custom overclocking and should be left to users choice/risk.,hardware,2025-08-26 08:50:14,1
AMD,nac12j4,"Intel did actually make desktop motherboards up until about a decade ago - they got into the business to reduce the ""white box junk"" that was being sold around the 386 in order to avoid that having a knock-on effect of tarnishing Intel's brand, and to a lesser extent, box out AMD an Cyrix who were also making competing CPUs at the time.  There's not a lot of money to be made in making desktop motherboards when the vast majority of machines sold nowadays are laptops (and servers), and the vast majority of desktops are OEM desktop models using custom designs.  I think the time of first-party motherboards being a profitable business or even a valuable marketing loss-leader has passed, and also important is that nowadays there are open-source and code-available models for system firmware that would be more beneficial to everyone than having CPU manufacturers making mainboards again.",hardware,2025-08-24 00:13:07,18
AMD,nacowlx,It would be nice if AMD actually made their own reference motherboard. Just for people that want a motherboard that just does motherboard things.,hardware,2025-08-24 02:51:22,3
AMD,nac3q2a,"Yep, right now they get the benefit of higher benchmark performance while having a convenient scapegoat for whenever things go wrong. Why would they give that up?",hardware,2025-08-24 00:30:06,7
AMD,nb42lmy,"So ist PBO too which is enabled by default and PBO is the reason why AMD says itâ€™s not on them.  Doesnâ€™t stop any reviews from using PBO and doesnâ€™t stop amd from using it in their advertisements when they make performance comparisons.  PBO is on by default, because reviews and amd marketing use it. Everyone not using it is at huge disadvantage.",hardware,2025-08-28 11:20:36,1
AMD,naqfkcf,I agree with you.,hardware,2025-08-26 09:23:02,1
AMD,nafisje,"Give them the boot. Stop giving them R&D help, documentation access (NDA territory), and bios cryptographic signing access; stop selling them necessary chips (southbriges, BGA CPUs, GPUs).  You may be able to reverse engineer your competitor's work and buy your competitor's products to take the chips, but you're now you're many days late and many dollars short.",hardware,2025-08-24 15:56:41,1
AMD,nb42r8h,You can only build a mainbord at scale when you have access to the chipsets. AMD could stop supplying chipsets when they donâ€™t like what Asrock is doing,hardware,2025-08-28 11:21:41,0
AMD,nabhkqe,"lol, amd bucks. UBM, is that you?",hardware,2025-08-23 22:12:54,30
AMD,nabnuca,ASRock is not a subsidiary of Asus. The co-founder of Asus spun off to create his own company which was ASRock.,hardware,2025-08-23 22:51:25,40
AMD,naqdd34,when 97% of failures is one manufacturer it can be accepted that other 3% may be user error.,hardware,2025-08-26 09:01:42,7
AMD,naaxsj9,A guy just posted a dead 9800x3d on Asus mobo on the Asus forums just yesterday,hardware,2025-08-23 20:21:04,47
AMD,nabc743,I am so happy about my pga cpu and standard pcie powered gpu :-),hardware,2025-08-23 21:41:10,-13
AMD,nacrjlj,"They also offered the widest selection and cheapest entry into full PCIe 5.0 support with X670E/B650E.  Which was a nice response to the rest rolling along with AMDs crappy segmentation, with very limited and needlessly expensive choices.  Compared to how they've been mostly good and fair to customers for years, their handling of the X3Ds burning up was especially disappointing.",hardware,2025-08-24 03:10:05,5
AMD,nabtgpl,"Asrock had a really good thing going. They had the best value AM4 boards toward the end of the platform's life, as well as launch AM5 boards. But then they just had to screw it up badly.",hardware,2025-08-23 23:26:17,2
AMD,nad7rtu,"I really like ASRock's offerings, they used to have post code displays even on mid and low end boards for most of AM4 which was a big reason I put them on parts lists for new builders.Â    They also have really neat AM4/AM5 server offerings, and were basically the first to do so.   Just sucks they're burning up CPUs now and won't take responsibility.",hardware,2025-08-24 05:20:03,2
AMD,nb43bhx,"Like Intel did it? Intel had a similar ish problem, just not as critical.",hardware,2025-08-28 11:25:31,0
AMD,nab9ktv,"I haven't read of Asrock denying an rma for a failed board.  Asrock should have gotten in front of this sooner.  When it started to happen there was a dearth of data so a root cause analysis could pinpoint the cause.  I would like to see manufacturers step up more quickly.  It does take time to identify the real cause of the problem.  As I said, if a large percentage of boards had been affected, then I believe the response would have been different.  If you were affected by this problem, then it was catastrophic.  I don't dispute that.  There were thousands of other Asrock board owners with 9800x3d chips who were interested but not impacted.  My point is we need to have a balanced perspective.  We can take a zero tolerance attitude and as a consequence we will see prices skyrocket and innovation stifle.",hardware,2025-08-23 21:26:13,-5
AMD,nabed3a,"100%.  The mobo makers also need to do a better job with setting voltages on XMP profiles safely, and preventing over voltage bugs.",hardware,2025-08-23 21:53:47,27
AMD,nacpusa,the 128gb of ddr5-6000 in my server can't have EXPO enabled or it becomes unstable as hell..  ....its still an improvement over refusing to post with all four sticks and EXPO enabled when I first built it in January.  (All four sticks worked with EXPO and no instability on another board so its not the ram),hardware,2025-08-24 02:58:01,7
AMD,nabvjwc,"Why are people buying fancy expensive ram if they don't know how to use it?  That's an insane waste of money.  It's exactly like those douche bags that buy supercars to drive to the super market.  I mean, it's *their* money at the end of the day, but why buy something if you don't know how to use it?",hardware,2025-08-23 23:39:09,4
AMD,nadged3,"Truly. This world just keeps getting dumber. Don't care what the spec of RAM is and what it's rated for under ideal conditions, it should be advertised at the lowest speed it's stable on that generation CPUs (with some headroom) and people can decide to apply OC profiles if they want. This is how we did it back in the day and it worked great, now it's all bullshit marketing and problems like this happen. Worse solution. How did we get here?",hardware,2025-08-24 06:38:46,7
AMD,nahzfo6,"That's not so simple though, because it's hard to control indirect advertisement like reviews which often go for a setup considered ""realistic"", not one guaranteed to be working for everyone.",hardware,2025-08-24 23:48:56,2
AMD,nachhuj,"Yeah. Alternatively, at least for AMD, they should accept DDR5-6000 CL30 as an officially supported configuration with 2 sticks of RAM up to 64 GB. Itâ€™s very easy to run and doesnâ€™t need dangerously high memory or SOC voltage. It shouldnâ€™t be a â€œthis will technically void your warrantyâ€ type overclock to run that.",hardware,2025-08-24 02:00:38,4
AMD,nabe5a2,"It shouldnâ€™t be enabled by default, yes. But the product performance is advertised with XMP enabled, reviewers test it with XMP enabled, and consumers expect to be able enable XMP safely.  The problem seems to be with mobo makers juicing voltage ranges for XMP profiles to try to guarantee stability for a wide range of memory lots and configurations, and that can cause problems. They are just saving time and money taking the easy way out instead of widely testing individual kits and setting reasonable voltage ranges.   And sometimes they have a semi reasonable voltage and the bios is just straight up bugged and over volts it anyways.  They have definitely been dropping the ball.",hardware,2025-08-23 21:52:31,15
AMD,nac8zpo,"> There's not a lot of money to be made in making desktop motherboards when the vast majority of machines sold nowadays are laptops (and servers),  Yeah I was starting to see the effects of that with my recent build earlier this month. All the Gaming PC motherboards I could find just had one dealbreaker or another, or just bad reviews on quality etcetera.  Ended up going with a ""Workstation"" W680 chipset board to end up meeting my needs, which was just dual NIC, POST Code LED with temp display after POST, and multiple PCI-e x16 slots, which like in 2016 would've been practically every gaming motherboard out there. Now it was basically a unicorn.",hardware,2025-08-24 01:04:14,9
AMD,nae9lnf,"Hey edparadox, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-08-24 11:21:35,1
AMD,nab83k8,"If its just one example that could be anything, faulty board, or cpu, etc not necessarily this issue.",hardware,2025-08-23 21:17:48,33
AMD,nab2e9z,Old bios or just overriten power limits for Manual OC?,hardware,2025-08-23 20:45:59,7
AMD,naaz2zk,"yeah and a couple of days ago seven different people posted on the asrock sub about their dead cpus, in just one day.",hardware,2025-08-23 20:28:01,4
AMD,nachsp1,They still do have by far the best value AM5 boards IMO  Hopefully they've got the frying CPU issue fixed because it would be a massive shame if they lose their market position because of one very stupid mistake.,hardware,2025-08-24 02:02:38,6
AMD,nadqex8,"Did you check if your exact 128gb kit was listed on the QVL of your motherboard with EXPO checked, and that it isn't 2x 64gb kits which will also be marked on the QVL as not supported?",hardware,2025-08-24 08:14:57,3
AMD,najys5x,"> 128gb of ddr5-6000 in my server can't have EXPO enabled or it becomes unstable as hell  Basically no Zen 4 chip can do 2DPC2R (ie 4 sticks of dual rank memory) at 6000 MT/s, generally the max recommended speed is around 5600",hardware,2025-08-25 09:14:43,1
AMD,nacnqy8,"> Why are people buying fancy expensive ram if they don't know how to use it?  Have you seen the volume of posts on computer/gaming related subreddits of people discovering their 144Hz monitor was running at 60Hz for years, or they had plugged their monitor into the IGP instead of their RTX 3090?",hardware,2025-08-24 02:43:16,18
AMD,nabzsu0,"Some SKUs are or have been only slightly more expensive than JEDEC RAM, they look pretty, heatsinks look confidence inspiring to newbies, and theyâ€™re inundated with advertising.  Donâ€™t underestimate how many customers will pay a few bucks to have a good feeling buying a popular product from a known brand, and assume itâ€™ll just work.",hardware,2025-08-24 00:05:05,13
AMD,nac94eu,"People get butthurt when you say this but it's true. Too many people buying things they have no business buying but get it anyway because ""they can"".",hardware,2025-08-24 01:05:05,4
AMD,naqcbb0,because advertisement works.,hardware,2025-08-26 08:51:33,1
AMD,nagwah6,"> it should be advertised at the lowest speed it's stable on that generation CPUs  Who/what defines a generation for set of compatible ram? Like the official supported intel 12th gen ddr5 speed is 4800MT/s for ONE stick, you want 2 sticks? 4400MT/s is the official rated spec(never mind that basically 12th gen cpu:s can hit 6000+ on 2 sticks and would love to see one that can't), so will all 2 stick DDR5 ram boxes just have to say 4400 and nothing else?   Gonna make the packaging plain white with a photo of burning cpu to the box as well saying overclocking ram causes cancer or something next?",hardware,2025-08-24 20:10:35,4
AMD,nade3n4,"> It shouldnâ€™t be enabled by default, yes. But the product performance is advertised with XMP enabled, reviewers test it with XMP enabled, and consumers expect to be able enable XMP safely.  Frankly i was completely put off when i finally bought a new computer a few years after chilling with my old 6 core intel for many years that XMP is overclocking ""in the books"". Like motherboards, CPUs and memory is sold with those speeds as advertised, but suddenly if you want to actually use it its ""oh its best effort, no gurantee""?",hardware,2025-08-24 06:17:21,9
AMD,nabg0qg,"> But the product performance is advertised with XMP enabled, reviewers test it with XMP enabled, and consumers expect to be able enable XMP safely.  Then motherboard and memory manufacturers shouldn't make promises they can't keep.  Cranking up the voltage is a band aid solution.",hardware,2025-08-23 22:03:33,12
AMD,naqd64p,>But the product performance is advertised with XMP enabled  which should be considered false advertisement and punished accordingly.  > reviewers test it with XMP enabled  good ones test ate JEDEC.  >consumers expect to be able enable XMP safely  You should always expect risk with overclocking.,hardware,2025-08-26 08:59:49,0
AMD,nabv4ql,Ya so that is otherwise known as being entirely wrong.,hardware,2025-08-23 23:36:34,31
AMD,nabz5tl,"It could even be a faulty AIO. I had an ASUS AIO with a USB motherboard connection which was electrically defective, it caused damage to both the MSI motherboard and killed the Ryzen 9 9950X3D CPU I was using.",hardware,2025-08-24 00:01:08,15
AMD,nac1c6v,"Yeah, ones and twos isnâ€™t *necessarily* indicative of a problem.    ASUS is still in my doghouse after two of their X570 boards failed on me in a row.  They were also IIRC the brand that was blowing up the most chips when the 7800X3D launched because they were shoving too much voltage.",hardware,2025-08-24 00:14:50,7
AMD,nab3crw,Idk I just know it affects all the mb vendors but seems to be mainly asrock,hardware,2025-08-23 20:51:09,1
AMD,nab3ojc,How many total so far?,hardware,2025-08-23 20:52:56,2
AMD,nadr3tn,"One 64gb kit is supported but not two on the servers b850 gigabyte board, but is on my desktops ASRock x870 board.   Lesson learned. This was my first build with four sticks so I assumed it would work the same (and yes, I know what happens when you assume).  Maybe one of these days I'll switch boards out, the server could use another nvme slot, and my desktop could use the 9950x.",hardware,2025-08-24 08:21:52,1
AMD,naeyosz,"â€œThink of how stupid the average person is, and realize half of them are stupider than that.â€     I am no genius, but good lord... I switched to the medical field, the amount of people who can't even operate a computer in 2025 is staggering and these are not boomers either.    Plus, the complete lack of common sense for the most banal everyday task. Some people are alive because of God's mercy, because they should be dead by consequence of their own actions or inactions.",hardware,2025-08-24 14:10:25,7
AMD,nac5uv2,"Yeah I mean honestly the marketing and specs on ram is overwhelming for many new or relatively inexperienced builders. Even after many builds for myself, family, and friends, itâ€™s one of the parts I hate  having to choose. Throw in model names, color, and RGB types in SKU and itâ€™s also a PITA to source sometimes.",hardware,2025-08-24 00:43:51,3
AMD,naqd1bh,>Who/what defines a generation for set of compatible ram?  JEDEC?,hardware,2025-08-26 08:58:33,1
AMD,nabh2vf,Yeah they take the easy route and crank the VSOC way higher than needed for the vast majority of configurations. Like waaayyy high. Kinda seems like you didnâ€™t read past the first paragraph of my comment there lol.,hardware,2025-08-23 22:09:53,6
AMD,nacs4ab,Curious to know if you got Asus to pay for the damage.,hardware,2025-08-24 03:14:14,4
AMD,nab6no0,"who knows? not everyone posts on reddit, imagine how many get sold around the world, and then taken back to shops, because the pc doesn't work.  [megathread #1](https://www.reddit.com/r/ASRock/comments/1iui7lx/9800x3d_failuresdeaths_megathread/), [summary](https://www.reddit.com/r/ASRock/comments/1i5iy9a/update_and_summary_on_the_dead_9800x3ds/) (before the megathread opened)  [megathread #2](https://www.reddit.com/r/ASRock/comments/1mvgndh/9000series_cpu_failuresdeaths_megathread_2/) opened just a few days ago, feel free to read through.",hardware,2025-08-23 21:09:35,7
AMD,nai978q,"I wouldn't put all the blame on just users though, as learning is still becoming harder since at one point it was decided that catering to the lowest common denominator means that anything confusing stupid people like silly technical details must be hidden.  Even without going into the details of hidden tech info, just consider the much simpler localization problem, users robbed of the opportunity to learn English. The degraded language skills as a result were already visible online several years ago, but I've started seeing more and more non-English posts/comments in otherwise English environments, seemingly with the expectation of others using translators (or does ""new"" Reddit automatically do it?).  Of course there are people who don't even try, but a lot of people learned not by actively seeking out knowledge, but just solving challenges on their way towards a goal. When an Apple phone user (think of all the tablet kids) isn't even allowed to know that files exist, or a gaming console user isn't allowed to run anything not related to gaming on what's essentially a locked down x86 PC, then how are they supposed to learn without even knowing what they are missing out on?  Hell, I'll go as far as claiming that curiosity about technology is even punished, and not even culturally like kids calling you a nerd. Can't even do a lot of tasks on a PC anymore because a lot of services are phone-only, and then it turns out that the phone crapp isn't willing to run on a non-stock OS. Why learn and experiment when anything else than just going with the flow gets you in trouble, and culturally we seem to be in a phase of the majority of the people being okay with everything getting dumbed down, users being expected to just consume without thinking too much?  We had a golden age of computing where people could (and had to) learn quite a bit about technology to get to their goals, and many people could do so with having to learn English first. Nowadays people are funneled towards entertainment/consuming with just a few clicks/taps, and even if anything goes wrong, the technical details are often hidden, only presenting a ""Something went wrong"" or ""Oopsie woopsie"" page. It's almost like knowledge is considered so dangerous, there's a lot of effort put into hiding it, and unfortunately I can actually see it scaring some kinds of people, like Apple users who seem to be proud of not even being allowed to utilize technical knowledge.",hardware,2025-08-25 00:49:57,6
AMD,naqch8q,Want to be more depressed? the computer litaracy is getting worse. the new generation is growing up on iphones and androids and have even less knowledge how to use computers. I had to teach university educated colleagues of mine what a folder is because they never saw anything more complicated than ipad. I feel like old man yelling at clouds now.,hardware,2025-08-26 08:53:09,1
AMD,naqcw4m,yep. even for experience builders timing can look like black magic with how complicated it can get nowdays. Does not help that many retailers flat out does not display it unless you go digging at manufacturers website.  Also love that half the manufacturers do not give physical dimensions on specification page. Have to guess if it fits under CPU cooler or not. So i just go to manufacturers that do give the detail.,hardware,2025-08-26 08:57:08,1
AMD,nargw9c,"JDEC is just a timing spec(which i think xmp/expo profiles still follow outside of the primariy timings? not sure, but there sure isn't an xmp/expo that actually changes the important timings like tREFI, some fancy motherboards do have some pre-set timings though) and it goes up to 8800MT/s for ddr5. Obviously 8800MT/s ram will not work for vast majority of  cpu:s/motherboards currently, so the JDEC spec doesn't have really anything to do with what intel/amd deem ""offcial spec"" ram speed for their cpu:s, which tops out at 5600(6400 for cudimms on arrow lake) even for the latest ones still with 1dpc(aka 2 sticks) and goes down with more sticks/ranks.",hardware,2025-08-26 13:39:45,1
AMD,nacvm66,"Oh, I'm pretty sure I'm now blacklisted by ASUS. I had some very choice words for them that made people uncomfortable.",hardware,2025-08-24 03:40:13,4
AMD,nalbohw,"I think new Reddit might actually do it. I did a search for a post just a little bit ago, and clicking the link unexpectedly brought me to new Reddit. It was also unexpectedly in French, though the URL was in English, but had something like `?tl=fr` at the end (not sure if that was it exactly, and can't check right now)",hardware,2025-08-25 14:47:24,2
AMD,naqcopm,"yeah, the random different language comments are weird. At least on something like youtube theres a translate button next to it, but on reddit i always wonder if i should report it in this sub. I know some subs have english-only policy.  English is not my first language. I learned english originally by needing to communicate with others inside MMOs when i was a kid.",hardware,2025-08-26 08:55:09,2
AMD,nabnns1,>They are just saving time and money taking the easy way out  >sometimes they have a semi reasonable voltage and the bios is just straight up bugged and over volts it anyways  >They have definitely been dropping the ball.  This sounds like defending them to you? Wat,hardware,2025-08-23 22:50:15,4
AMD,nacx553,"Oh boy, it was that bad eh? I was hoping that Tech Jesus' admonishment had set them right but I guess they just PR'ed their way through that storm and went back to their old ways.",hardware,2025-08-24 03:51:58,3
AMD,natej9f,"Since I am in Brazil atm, for the past 6 months now when I do a google search, there are Reddit links showing up as results and they are being translated by Reddit to Portuguese. There is a link at the OP titled ""See the original post"" which undo the translation    The users have no idea english is being used and make a post in their own language...    the translation is good, but most of the time, internet slangs and such are translated poorly and things get pretty weird",hardware,2025-08-26 19:12:28,1
AMD,naqzf99,"Haven't considered reporting, especially as it seems like just a couple of reports may not even show up anywhere anymore (that gets human eyes), I just downvote and move on, but I did wonder if there's a different interface which auto-translates.  There's no need for English-only policies, it's more of a common sense matter that communication should be done in a matter that's valuable for the other parties, avoiding not just languages foreign in a specific environment, but also low effort messages like ""this"", or dumping a stream of thoughts with no punctuation, and no basic checks for at least gross writing errors.  Your story isn't unique, I know a lot of people who learned English a similar way. Unfortunately some of them regressed by embracing localizations, and whenever I see that they are playing cooperative games with translations, I cheekily ask them how do they intend to communicate with others without even knowing the item/spell/ability names others know and see. I occasionally get to see how that works out between friends, and short term it's somewhat hilarious as the observer, but unfortunately long term I've seen even people not wanting to use localization just giving in, and more isolated groups forming mostly based on language (and nationality).",hardware,2025-08-26 11:59:41,1
AMD,nad15wy,"> Oh boy, it was that bad eh?  I barely managed to not get myself fired from TH for the words I used - I was *extremely* angry.   I wasn't quite emotionally stable at the moment, I had just dumped my girlfriend and I was still recovering from the evil I faced while I was in Arkansas if y'all remember how I was rambling back around the end of last year and beginning of this year.",hardware,2025-08-24 04:23:54,5
AMD,narmgat,"I hate the forced localization. I always choose english for software but microsoft is trying to force revert to english language every chance it gets. The language groups in MMOs were always a thing. A server i played on a random person would ask you ""br?"" and then you will have to play the lottery. If you say yes, one of two things will happen. They start speaking to you in portugese or its someone who does not like brazlilians and will kill you. But if you answer no, but the person asking was portugese, he will kill you instead.",hardware,2025-08-26 14:08:07,2
AMD,nad2kva,"Oh wow, sorry to hear that. Hope things are better now.  I was more referring to Asus' RMA service.",hardware,2025-08-24 04:35:38,5
AMD,nadx3nf,"Hi, random dude from southern Germany. I wish you all the best.",hardware,2025-08-24 09:22:45,2
AMD,nadgecl,simply means that they have super duper mega margins on their stuff from the getgo.  Remember that every sale u see they still make a profit. Very seldom any company sells a product below their profit margin. If they do it is an old product that sat there on the shelves for ages they they want to replace with something that actually sells.,hardware,2025-08-24 06:38:46,378
AMD,nadxb4b,It's always  Low sales.,hardware,2025-08-24 09:24:55,42
AMD,nadeazd,"This is normal and happens very frequently, maybe just not by 50%, then again, it's    ""up to 50%"" so that may explain it.",hardware,2025-08-24 06:19:11,88
AMD,nadga5a,Canary in the Coal Mine for a recession?,hardware,2025-08-24 06:37:38,70
AMD,nadty01,Data center demand dropping?,hardware,2025-08-24 08:50:45,13
AMD,naelxfk,"Retailers are setting up for Q4 sales, and they need to rotate out old stock to make way for new stock.  Retail shelf space is limited. But seasonal buying is a behavior that goes back to millennia of farmerâ€™s markets. While nowadays itâ€™s much less about retail shelf space (because of e-commerce) and these new prices are likely more about corporate buying than individuals, two things remain true.  - The global supply chain of making to shipping to warehousing to offering for sale is still based in Q4. Thatâ€™s because of above. - Likely every companiesâ€™ budget for outside spend will be cut last year like it was this year. But this yearâ€™s budget isnâ€™t all being given back, while investment in future things is down due to uncertainty. So thereâ€™s *some* budget remaining, but not enough for paying at sticker-price   What makes me think this is the case is the final paragraph of the article:   > both AMD and Intel are producing CPUs aggressively to secure market share in data centers. If supply briefly overshot immediate OEM demand, distributors may be offloading excess stock into retail channels, and to move volume, they could be discounting well below list prices. While nowadays is less about limited retail store shelf space,",hardware,2025-08-24 12:53:26,12
AMD,nagde49,"I genuinely would love to know how many $5k, 8k, 10k, and 10k+ CPUs consumer oriented retailers like Newegg move per year strictly through their consumer facing store.   Like.... Just how many people or companies are getting their 5k+ CPUs through consumer channels versus through large enterprise contracts, commercial wholesalers, large SI's like Supermicro, or smaller SI's like Falcon Northwest.  So far I can only imagine it's a fraction of 1% of the units moved. Anyone have any insight?",hardware,2025-08-24 18:30:39,6
AMD,nadtevs,Do manufacturers normally explain why they are cutting prices? The world must be so confusing to this headline writer if they really think this is inexplicable lol.  > inexplicable  > unable to be explained or accounted for,hardware,2025-08-24 08:45:28,15
AMD,nagllgi,"RAm for 12 channel memory costs more than the massive CPUs now. I do wish I had a 12 channel epyc gen 4 or 5, or a granite rapids 12 channel with AMX for local inference.",hardware,2025-08-24 19:13:40,3
AMD,nae98yj,Probably hecause high inventory because everyone's budget is going to AI...,hardware,2025-08-24 11:18:41,5
AMD,naf99q3,Clearing off the shelves for new models.,hardware,2025-08-24 15:07:13,5
AMD,naj16x0,"Hopefully Threadripper will be next, it would be really nice to be able to afford one of those.",hardware,2025-08-25 03:58:08,2
AMD,naeofd3,"more supply than demand. too many other arm options, and more of a push toward gpu solutions. its not really something that should be that hard to understand",hardware,2025-08-24 13:09:30,3
AMD,naetyx2,"The Chinese are not buying, thatâ€™s all.",hardware,2025-08-24 13:43:30,5
AMD,nadr37j,I can explain it:  the big boys know that the AI bubble is due to pop (or at least start deflating) and they're looking to reduce stock in hand.,hardware,2025-08-24 08:21:42,4
AMD,naegpf8,"Data processing is one of the easier things to outsource, once you strip the PII (or more likely, have the appropriate licensing agreements). This sale is probably an indicator that companies are building out surplus processing elsewhere instead of in the US.",hardware,2025-08-24 12:17:14,2
AMD,nafz2ic,"* Seasonality  * Intel is actively ramping Granite Rapids  > DAVID ZINSNER: ...in the process of ramping Granite Rapids, which will drive more volume of Intel 4.  * Price war for market share  > INTC FORM 10-Q: Server ASPs decreased 8% from Q2 2024, primarily due to pricing actions taken in a competitive environment.  > Server ASPs decreased by 9% from YTD 2024, primarily due to pricing actions taken in a competitive environment.   * Intel channel stuffing to keep the Intel 4/3 fabs hot  * Muted server CPU demand due to AI compute spend",hardware,2025-08-24 17:19:04,2
AMD,nahezay,I want this to mean Hetzner will now offer Turin and Granite Ridge in their dedicated line...,hardware,2025-08-24 21:48:44,1
AMD,nahxg4e,I wonder if there is about to be a 3rd competitor in this area and the price decreases are meant to damage the new comer.,hardware,2025-08-24 23:37:12,1
AMD,nahxv08,Now do the xeon 2500 and 3500 cpus!,hardware,2025-08-24 23:39:38,1
AMD,naiwr2q,"There is a possibility that those chips have malware in them and need to be moved fast, or the ones coming after have malware in them and the rest of the stock has to be cleared.  Just considering the 10% buyin of the us government into intel  Edit: mallard changed to malware",hardware,2025-08-25 03:24:36,1
AMD,nadd9j2,Theyâ€™re probably releasing new chips in a few months.,hardware,2025-08-24 06:09:42,0
AMD,nahu7fw,"> ShopBLT and Newegg  Those aren't real prices.  Real prices are what you get from AMD / Intel directly, or what you get when buying through Dell / HP / etc.  If you're buying these chips at ""retail"", then you're paying over double the actual market rate to begin with.",hardware,2025-08-24 23:17:56,1
AMD,naetzzx,"Maybe they realized that reached planned profit level, so now can sell without profits",hardware,2025-08-24 13:43:40,1
AMD,nafaapq,AI bubble collapse incoming?,hardware,2025-08-24 15:12:32,-1
AMD,nadn02v,"Yeah super margins, think that 5090 is value....",hardware,2025-08-24 07:41:39,0
AMD,nadvuoi,>Remember that every sale u see they still make a profit. Very seldom any company sells a product below their profit margin.   [Loss leaders](https://en.wikipedia.org/wiki/Loss_leader) are pretty common in commoditized products like cpus. Retailers can make their margins back on perifierals and/or other components.,hardware,2025-08-24 09:10:07,-2
AMD,nadh1rj,"The actual sale prices of server hardware has typically been closer to these discounted prices instead of the list prices. Server CPUs are sold to system integrators, who then build servers and sell them to the final customers. They like having official list prices that are much higher than the actual contract prices that the CPU makers sell them chips at.",hardware,2025-08-24 06:44:54,164
AMD,naee776,"Like people said the list prices ain't real, major customers don't buy at anywhere close to those prices. Talkin about the ""super duper mega margins"" also ain't right because most of the cost of developing a processor comes from the r&d and not the material cost. The margins don't reflect the hundreds of millions in development costs  It's the same for drugs, people love to talk about how each dosage costs a dollar to manufacture but don't mention all the years of research needed to finally result in the products.",hardware,2025-08-24 11:58:48,19
AMD,nae20vf,"That or they need to unload it now or they'll have difficulty later.  We have no idea what their internal data shows.  And not every sale still has profit, remember that things like gaming consoles for example usually dont make a profit on the hardware until its been out a few years, yet they have sales and bundles. The consoles make their money on the games and extras.  Pc hardware though, whats probably happening is slow sales in the west, decrease in china sales especially potentially in the near future, plus new hardware coming out that might make offloading this hardware more difficult the longer they wait especially if they have oversupply.  But yeah most sales they still make a profit, just not every time, sometimes its strategic or desperate offloading.",hardware,2025-08-24 10:12:11,15
AMD,nafpsgf,"And yet you'll see people in the comments lining up to defend those poor, poor dears with their meager billions of quarterly profit.",hardware,2025-08-24 16:32:21,11
AMD,nadjjsv,They take a big loss at the start. Yield is at its lowest point and the cost of node development is huge. It all depends on the wafer agreement.,hardware,2025-08-24 07:08:14,19
AMD,naduo89,Seems weird when prices will go up fairly significantly with tariffs.,hardware,2025-08-24 08:58:12,5
AMD,nadnuvx,"Ehh, there are a bunch of times when items are sold at a loss. There are times when they expect to make their money back in other ways. Like Playstations and Xboxes where they expected to make the money back selling games. Or super cheap TVs where they expect to make the money back in ads.  There are also loss leaders where you sell one item at a loss to get more customers in the door and hope they buy other items as well.",hardware,2025-08-24 07:50:04,12
AMD,nadwjd3,"No it's not normal.  EPYC 9565, a 10 month old chip, is at $5.7k PER UNIT at retail, when MSRP is $10.4k PER 1000 UNIT.  Intel's FLAGSHIP 128-core Xeon 6980P is available for $5,166, 53% lower than recommended 1k Unit price of $12k.   I'm guessing AI spending has been curtailed massively by one or more companies recently.",hardware,2025-08-24 09:16:58,80
AMD,naf396h,"Gotta love ""up to 50% off"" sales. It's always like 1 item no one wants that only has 1 left.",hardware,2025-08-24 14:35:21,1
AMD,naep9zr,Wallmart missed their earning for the first time in last 3 years.,hardware,2025-08-24 13:14:53,32
AMD,nafnpg1,"The canary has been dead for a few months now, we're just counting how many miners are missing after each shift and chalking it up to nature",hardware,2025-08-24 16:21:38,50
AMD,naeb5fu,"Maybe. Though my bet would mostly be on business as usual (large discounts on high margin products can and *do* happen).  Or also decently likely - it might be one of first signs that the AI bubble is popping soon. A bunch of companies in that space are playing chicken between themselves - under assumption that whoever ""wins"" the AI race will easily recoup their infrastructure investment. Meta is already reported to be seeking external funding for AI, presumably because they ran out of billions of dollars they could spend by themselves.",hardware,2025-08-24 11:34:24,13
AMD,nadm5c3,Maybe. There's been a lot of signs that there is a tech bubble however which would have popped a few years ago if it wasn't for the AI goldrush. We might now be looking at the AI bubble popping as well.,hardware,2025-08-24 07:33:14,39
AMD,naf1x4o,"2023 was when the server CPU market saw it's biggest contraction. The market is still growing, but hasn't yet gone back to 2022 levels.",hardware,2025-08-24 14:28:06,6
AMD,naeuwlu,Why would *this* be the canary?,hardware,2025-08-24 13:48:58,7
AMD,nak1af5,"No. Intel and AMD overestimated the growth in demand and have undoubtedly manufactured too many CPUs. Demand did increase, however, just not by as much as they had projected. It is better to discount product to sell it than to sit on the inventory for a year.  Much to Reddit's dismay, the sky is, in fact, not falling.",hardware,2025-08-25 09:39:44,2
AMD,nakiys7,Spending all their money on GPUs for AI most likely.,hardware,2025-08-25 12:03:11,4
AMD,nak6gtw,"It is not about shelf space, but leading edge hardware loses value by just sitting in a warehouse, due to progress of technology.",hardware,2025-08-25 10:27:12,6
AMD,naj2qen,"I think it's not manufacturers who are expected to explain it, but analysts / journalists / pundits / etc.",hardware,2025-08-25 04:10:05,4
AMD,nadspl4,"lol, smart add",hardware,2025-08-24 08:38:20,2
AMD,nae2qg8,"Ai bubble? If it was gpu prices sure, but this not that. Nvidia even sells their units with their own cpus these days",hardware,2025-08-24 10:19:07,12
AMD,nadsri3,Source?,hardware,2025-08-24 08:38:51,5
AMD,nahm873,Server CPU demand has been negatively correlated with AI demand.  So it's not really a good leading indicator.,hardware,2025-08-24 22:30:13,3
AMD,nadxeeu,"Every greedy corpo knows it's a bubble but wants to stay in until last second to maximize earning.  This seems more like it's already popped, and we will hear more about it in coming months from public announcements and earnings releases.",hardware,2025-08-24 09:25:50,8
AMD,naddp81,"Nothing new is coming until H2 2026. Even then, retail availability doesn't mean it has to coincide with an upgrade cycle of an organisation that might be interested in buying them.",hardware,2025-08-24 06:13:39,27
AMD,nadjbdq,Neither Venice nor Diamond Rapids are due for a year or more.,hardware,2025-08-24 07:06:03,9
AMD,naqgvka,"what you get from AMD/Intel directly is wholesale, not retail. Newegg and the like is retail.",hardware,2025-08-26 09:35:39,2
AMD,nahvq4f,Would CPUs drop _first_? Maybe I missed a recent drop in GPU prices for datacenters ?,hardware,2025-08-24 23:26:58,4
AMD,naqgxdb,loss leaders in server CPUS (the most profitable kind)?,hardware,2025-08-26 09:36:08,1
AMD,naeydyv,And also kickbacks from the manufacturers,hardware,2025-08-24 14:08:44,1
AMD,nadldcr,Can't wait to get a Xeon from one of the more recent generations to screw around with.,hardware,2025-08-24 07:25:43,34
AMD,nafeyaa,"Exactly this. No server customers actually purchase these CPUs at anywhere close to list price. 50% off is quite normal starting point before the ""sharpening the pencil"" negotiations.",hardware,2025-08-24 15:36:53,22
AMD,nair9a4,Better than the pinkos who've been marinating in unexploded commie propaganda so long that they think profit margin is a measure of how immoral a business is.,hardware,2025-08-25 02:45:54,1
AMD,nadt5r3,"The high tech stuff that's in demand can easily sell at 10x margin just for the chip, something like Nvidia's bundled together servers are probably close to 20x margin.   Even those $0.0001 resistors sold on reels of a 10,000 are still making significant margin.  One day look up the prices of the chips on something like a USB hub, even the best won't be more that $4 of components sold in a device retailing at $80, the ports and connectors are the most expensive parts.",hardware,2025-08-24 08:42:54,26
AMD,nadpzqi,"That's not how their wafer pricing works. Also, N4 is an extremely mature node on the AMD side, and Intel 3 *should* be pretty mature for Intel.",hardware,2025-08-24 08:10:44,9
AMD,nae60nc,"> Like Playstations and Xboxes where they expected to make the money back selling games.  Apparently this practice is very over now and has been for a couple of generations.  No clue if that is true but people have said it to me, it does feel that way given how much both companies charge for console hardware.",hardware,2025-08-24 10:50:27,25
AMD,nadp5le,"nah, they are actually lying to us be it willingly or not. no way consoles that often are discussed are sold at a loss, because Sony, MS and Nintendo dont get the prices as we get offered from the ""supply companies"". When u see some media outlet trying to dig into the cost they dont get to see the deals that say sony, ms or Nintendo lock in with their ""suppliers"" because they are so large and sell in such huge volumes the prices are totally different. That is why Sw2 is so much cheaper than other pc handhelds, even though the hw is old an obsolete as it should have come out years ago as a sw pro :P  I have worked in product development, and every cost was hiked up and then we could say,  this and that was such and such when in fact the parts we bought for our product and the end result was much cheaper than what was officially displayed. Not only that but the price for a product that we bought in differed so much depending on the company we represented.  But sure, officially some components are sold at a loss because u get the return in game-sells.  Usually u sell something at a loss when u just could not get rid of it, or u give it away in some kind of pr campaign and then it turns to the thing u mentioned like consoles get the return via games.",hardware,2025-08-24 08:02:31,-10
AMD,nahlrn4,Every so oftenÂ Jensen HuangÂ probably wakes up in a cold sweat from a bad dream where AMD didnâ€™t tank ATI Radeon to fund Ryzen.  Then he looks at the stocks app he leaves open on his bedside table and collapses back into peaceful sleep.,hardware,2025-08-24 22:27:29,16
AMD,naeq3e3,You provide no argument to why it's not normal though,hardware,2025-08-24 13:20:00,32
AMD,naj8hep,Perhaps they finally realised people might actually buy them in small amounts if they didn't have artificially jacked up prices that are discounted by slimy salesmen.,hardware,2025-08-25 04:57:38,3
AMD,naiepa6,Maybe theyâ€™re trying to cash in on smaller users building AI servers. It sounds like historically no one paid anywhere near full price.,hardware,2025-08-25 01:24:19,2
AMD,namijjc,Tbf Walmart was also priced and analyzed as a growth stock because they have started to sell online ads and have tons of online grocery deliveries.,hardware,2025-08-25 18:16:09,6
AMD,nasesvd,The real winner is who rents centers to others without too much spend on the useless AI software crap; you can repurpose that big iron but that AI crap is a white elephant made of legal corium.,hardware,2025-08-26 16:23:33,2
AMD,naf0xcm,"The server CPU market has been heavily depressed *because of* AI, as datecenter spend has heavily shifted towards Nvidia.   A lot of Nvidia's recent growth has come, partially, at the expense of Xeon and Epyc.  You can see how the [TAM shrinks](https://www.notebookcheck.net/fileadmin/_processed_/f/e/csm_Intel-manages-to-shrink-server-CPU-share-to-62-AMD-still-trails-but-gap-narrows-02_ee3129e37a.jpg) in 2023 and it still hasn't fully recovered.   Epyc and Xeon are fighting over a smaller pie",hardware,2025-08-24 14:22:41,37
AMD,nas4g0n,"Eh, AI is liable to drag down tech and data center spend with it.",hardware,2025-08-26 15:34:10,2
AMD,naed47q,"For now it's got to all be either insider knowledge or inference from news. There is a bunch of things that *could* imply AI bubble popping somewhere around now or pretty soon, depending how you look at them and how much credence you are willing to give:  * Recent chat GPT5 release was rather underwhelming. Maybe it's a fluke, *or* it's a sign that LLMs have hit a soft ceiling of capability. Mere possibility of the latter is going to chill the investors currently pouring untold billions of dollars into AI infrastructure. * It obviously depends on what you look for, but you do certainly see a lot of voices, increasingly substantiated, that haphazardly forcing AI into every nook and cranny doesn't actually increase productivity in most cases. Again - to what exact degree this is true is up in the air, but merely shifting the sentiment is also going to chill the pace of AI investment. * There are also many voices coming in with various extrapolations of current AI development cost, scaling difficulty and results. Those coming form the mayor players in the race obviously are very positive, but from the outside there is a *lot* of real challenges - like models needing exponentially more good data even after they already used basically everything there is. * In more substantial news, there is Meta seeking external funding for further AI development. This would seem to indicate that at least one of the major players in this space is showing first signs of running out of cash to burn on this whole huge game of chicken.",hardware,2025-08-24 11:50:24,10
AMD,nadx9rr,Me reading the news and getting an impression of how things are shaping up.  v0v maybe I'm just wishcasting.  Please consult a qualified advisor for financial advice.,hardware,2025-08-24 09:24:31,0
AMD,nasejva,"And it will persist after the bubble I suspect; with all that excess GPU big iron lying about, they're gonna find ways to use it for tasks we'd use other silicon for normally I'd suspect.",hardware,2025-08-26 16:22:21,1
AMD,nadxyb2,"A plausible scenario. All I see are the ripples on the surface, not exactly what the big fish are doing down there.",hardware,2025-08-24 09:31:31,7
AMD,nah1yd3,">  stay in until last second to maximize earning.  If it's generating massive earnings it's not a bubble. So far it's looking nothing like a bubble. There's real usage, which only seems  to accelerate.",hardware,2025-08-24 20:39:02,1
AMD,nafib0q,> Nothing new is coming until H2 2026.   AMD's new server (Venice) CPU is coming out on 2nm. It was taped out back in April: https://www.hpcwire.com/off-the-wire/amd-tapes-out-1st-hpc-product-on-tsmc-2nm-process-with-venice-epyc-cpu/  So that release could happen in H1 2026.,hardware,2025-08-24 15:54:09,4
AMD,nadfvzt,Kinda nice though. You usually have to go through official channels to get these kinds of numbers (though Iâ€™m sure itâ€™s even cheaper for them now).,hardware,2025-08-24 06:33:53,2
AMD,nadosbp,Supposed to come out sometime in 2026.,hardware,2025-08-24 07:58:55,3
AMD,nai544b,Are GPU data center prices public? I think all we have is speculation on what Nvidia charges for their most in demand stuff.,hardware,2025-08-25 00:24:13,1
AMD,nadq9n1,I want a cheap Ponte Vecchio to play with,hardware,2025-08-24 08:13:29,27
AMD,nah5cxv,"ES/QS for Sapphire and Emerald Rapids processors fit that bill well.  Drop at least 128gb in them and watch them sing.   If you have a wallet burning desire, Xeon Max processors with onboard HBM2e are doable as full systems for 2000-3000ish",hardware,2025-08-24 20:56:29,10
AMD,nag3w09,Not uncommon to see DOL (discount off list) agreements in the 60-70% range.,hardware,2025-08-24 17:42:34,10
AMD,naj4kmi,What an asinine take.,hardware,2025-08-25 04:24:47,1
AMD,naj1dp6,"Grandpa, youâ€™re not supposed to be on the internet after 8  PM, you know this.  Youâ€™re gonna get caught and miss out on bingo and jello night again.  Just go to bed, ok?",hardware,2025-08-25 03:59:32,0
AMD,nafm6c5,"Looking at margin as cost of bill of materials vs retail price is misleading, even if an accurate use of the word.   It costs millions to develop the technology to create those things, millions to employ the people who get it out the door. None of these products could afford to be developed if the company charged what it cost to print the chips after all the research that goes into them.  AMD and other manufacturers are able to drop the pricing because they finally recoup the cost of developing those chips, or at least are getting closer to it. And it's usually a good sign that the next generation of products are significantly better and they have confidence they will sell. For parallel computing, it's very risky to sell old stock at a deep discount, since you can just use more of the older, cheaper chip. Like a gamer just getting an RTX 4080 instead of a 5070.",hardware,2025-08-24 16:13:53,11
AMD,nadrktk,But the ~600mm2 Intel 3 die Hogs Wafers,hardware,2025-08-24 08:26:43,2
AMD,naflpop,"Before Intel internally split fab and design, they had no wafer agreement with itself. You could hide the cost of things like additional steppings, which resulted in a lot of waste.",hardware,2025-08-24 16:11:35,2
AMD,nae6b88,I linked an article in an answer below where an Xbox VP said under oath that Xboxes have never sold for a profit in 2021. Can't imagine that changed since then.,hardware,2025-08-24 10:53:04,6
AMD,nael2y6,"If you compare the prices to what it costs for the PC equivalent, it most definitely is either a loss or barely break even. Thereâ€™s a reason why the Air Force built a supercomputer cluster out of PS3s.",hardware,2025-08-24 12:47:46,2
AMD,nadtszw,"Xbox's VP said *under oath* that Xboxes were always sold at a loss: https://www.pcmag.com/news/microsoft-says-xbox-consoles-have-always-been-sold-at-a-loss there is not much more proof you can get than that.  Also loss leaders are very real and very prevalent. I worked at a big cash and carry store as a teenager and saw how much profit we were making on things. We had the most volume in paper towels in my segment and we always had a huge loss on them. So much so that if we somehow weren't making a loss my boss got concerned and told me to look at the numbers again, because that can't be right. I never understood why we *wanted* to make a loss that just didn't make sense to me. But it was certainly a thing.",hardware,2025-08-24 08:49:20,20
AMD,naey33s,Yes he does. Per unit vs bulk unit buying makes a big difference and itâ€™s not normal for discounts to be this great on SINGLE units of this class of chip when MSRP was so much higher on specifically bulk orders.,hardware,2025-08-24 14:07:02,45
AMD,nafe1o8,"AI is a part of it, but it's also the post pandemic glut. People going back to office/school and less need for Zoom etc. During the pandemic we saw a huge demand for CPUs where companies were double and triple ordering.  You can see the same thing happened to PC shipments, which have nothing to do with AI crowding datacenter orders. https://i.imgur.com/M1Rsjcf.jpeg",hardware,2025-08-24 15:32:11,21
AMD,nas5kc9,"Maybe, maybe not. Nvidida earnings tomorrrow might paint a clearer picture for us.",hardware,2025-08-26 15:39:34,1
AMD,naqggxi,GPT5 was underwhelming while all its competitors have been accelerating faster. I think its more indication of OpenAI loosing the leadership more than any soft ceiling. Look at what google is doing with Gemini/Genie for example.,hardware,2025-08-26 09:31:46,0
AMD,nagpqc1,I'm the source,hardware,2025-08-24 19:35:43,0
AMD,nasfgql,I mean the business press has been rife with folks smelling a rather rank fish for around a year now...,hardware,2025-08-26 16:26:42,2
AMD,nahywf7,"Huh a bubble can generate massive earnings and still be a bubble. That's like, a major way to get people into a bubble.   What makes something a bubble or not is sustainability. Right now there is a major issue in monetizing AI while retaining users in face of the very expensive CapEx (it's a whole discussion on it's own).",hardware,2025-08-24 23:45:44,7
AMD,nafjokv,"Theoretically, yes. But I don't recollect AMD launching EPYC in the first half of any year, other than maybe Milan-X IIRC, so it is unlikely. But I have no doubt that Venice will be the first product of any kind - HPC or mobile - on N2.",hardware,2025-08-24 16:01:20,3
AMD,nadpvvc,"Think Intel's publicly said H2 for DMR (insofar as their roadmap can be trusted), and it's very unlikely (though not impossible) AMD meets H1 for Venice.",hardware,2025-08-24 08:09:42,4
AMD,naqgqw8,H2 of 2026 so a year or more.,hardware,2025-08-26 09:34:23,1
AMD,naqgth0,"the accelerators themselves have public prices like the server CPU does, and neither case reflect actual install cost for the big players.",hardware,2025-08-26 09:35:04,1
AMD,naiq2g1,"Do you know if the hbm has ecc? I assume so, but just wondering.  Temptingâ€¦",hardware,2025-08-25 02:37:46,4
AMD,nag45mp,"Yep, and some OEMs (*cough cough* Cisco *cough cough*) are worse than others in artificially inflate the list price to show a bigger ""discount"".",hardware,2025-08-24 17:43:53,10
AMD,najjjgk,The asinine take is hating those who have more than you.,hardware,2025-08-25 06:41:18,7
AMD,najiofa,Please don't insult the young by assuming only old people can recognize your failed envy-based ideology for what it is.,hardware,2025-08-25 06:32:48,9
AMD,nadupog,It's not like they have a shortage of capacity.,hardware,2025-08-24 08:58:36,6
AMD,nagfojg,What does that have to do with my statement?,hardware,2025-08-24 18:42:41,2
AMD,nae9hyb,"> Analyst Daniel Ahmad confirmed that the PS4 eventually became profitable for Sony and that Nintendo developed the Switch to be profitable quickly, **so Microsoft is the odd one out.**  You are using the exception to prove the rule. Most companies don't sell at a loss a product, unless for market penetration reasons. (also, this is probably just recency bias speaking, MS had a very popular console in the Halo generation)",hardware,2025-08-24 11:20:45,9
AMD,nagft9f,> Thereâ€™s a reason why the Air Force built a supercomputer cluster out of PS3s.  That was like 20 years ago.,hardware,2025-08-24 18:43:24,18
AMD,nagukm7,I don't know. What's the current market price of a zen2 cpu on 12 nm process node that's running in the ps5? Kind of obsolete tech of you ask me.   Or the Switch 2 for a more recent release. Handheld battery powered device with 8 nm SOC?,hardware,2025-08-24 20:01:42,1
AMD,nais5gf,"The PC equivalent is a consumer laptop minus the screen and battery, pretty much.  Remember the slots and sockets and ATX you get in a DIY PC don't come for free. Neither do the 4-5 extra fans you don't need if the whole thing is an integrated unit.",hardware,2025-08-25 02:52:04,-2
AMD,nadukpk,"Microsoft sells them at a loss to retailers, not retailers to consumers. That's simply illegal in a lot of places unless you're straight up clearing obsolete inventory or going out of business. Your anecdotal paper towels might make sense if they can point to some other store in the area selling a comparable product for that price. As for non-clearance sales, they're either within margin or paid for by the company providing the inventory.   It would also make no sense for retailers to take a bath on a console when they can't reliably expect associated sales to be with them since you can buy that stuff anywhere.    e: actual F&A reqs getting downvoted in favor of the guy who stocked shelves one summer, never change reddit",hardware,2025-08-24 08:57:12,0
AMD,naqg77n,yet after going back to office they still do online meetings even when everyone is present at the location because they learned its just simpler.,hardware,2025-08-26 09:29:11,2
AMD,nas5vif,"Eh, I'm not sure we'd get that even then - the trains will be in motion, no matter earnings tomorrow or not.",hardware,2025-08-26 15:41:04,1
AMD,najryaa,> Right now there is a major issue in monetizing AI  the monetization is already happening. The adoption speed and willingness to pay for e.g. AI coding tools is mind boggling. That alone is an enormous market.  The 'dot com' bubble pop due to slow adoption not the case here. Granted the spending pace probably dwarfs that of the 2000's  Then there's the whole national security concern. The US simply can not have US based AI research slow down and risk falling behind China. The GPU export restrictions show how serious they are about it.,hardware,2025-08-25 08:05:23,2
AMD,nadqf7z,Venice Dense on N2. At the very least the Zen 6C versions will not come until late 2026.,hardware,2025-08-24 08:15:02,3
AMD,nadqfia,Havenâ€™t seen a date for DMR,hardware,2025-08-24 08:15:06,1
AMD,nakx7z3,"Not likely to be ECC, but very much like the HBM you would encounter on a graphics/compute card.",hardware,2025-08-25 13:30:44,1
AMD,naiy7g9,"As a Optical Network Architect designing L1-L3 for core networks, I HATE Cisco and their damn pricing. Juniper and Nokia are so much more easier to work with, along with Ciena,",hardware,2025-08-25 03:35:16,8
AMD,najmrsa,Who is doing that?,hardware,2025-08-25 07:13:16,1
AMD,nadvs53,But it takes time to ramp it,hardware,2025-08-24 09:09:25,2
AMD,nah6y7m,"I thought it'd be obvious. In some instances, there is no wafer agreement. Its not possible to know margins on a single product if your business units aren't separated out. Hence, wafer pricing doesn't always work like TSMC does it.",hardware,2025-08-24 21:04:53,2
AMD,naear6s,"All three consoles were sold at a loss at one point. The PS4 **eventually** became profitable, the Switch became profitable **quickly.** Xbox is only the odd one out inso far as it never became profitable. Also Xbox is not really an exception if it's 20-30% of the market.",hardware,2025-08-24 11:31:08,11
AMD,naieb00,"The PS5 APU is 6nm, 7 for older ones. The APU is also much more than some Zen 2 cores.",hardware,2025-08-25 01:21:48,1
AMD,nai028g,"> Microsoft sells them at a loss to retailers, not retailers to consumers.  /u/Pillokun was saying MS and Sony were lying about selling at a loss, not retailers. He wasn't talking about retailers.  > e: actual F&A reqs getting downvoted in favor of the guy who stocked shelves one summer, never change reddit   Misrepresent the topic, blames on Reddit. Never change reddit.",hardware,2025-08-24 23:52:45,3
AMD,nadw79j,"it is always a play with numbers, and where the loss occurs. For instance one can develop a product and u have healthy margins, but then the parasites up top will suck up so much of the money given to the project that it will look like the project is at a loss. That is how it works, why do u think so many games cost so much to develop? there are managers/ceos and other people that just suck the project dry before it is even launched.",hardware,2025-08-24 09:13:37,4
AMD,nakeqsg,"The monetization is happening, yes, but sustainability is not clear.   [I recommend this read.](https://ethanding.substack.com/p/ai-subscriptions-get-short-squeezed)  > The 'dot com' bubble pop due to slow adoption not the case here.   lmao no. It's much more complex than that.  > The adoption speed and willingness to pay for e.g. AI coding tools is mind boggling. That alone is an enormous market.  Again, this is not a sign that it isn't a bubble.  The tulip craze is textbook example for this.   Just so we see eye to eye, I'm not saying it is one, I am saying we do not know.  People always find new ways to delude themselves into a bubble, and outreason themselves  of very clear signs  of  one. Don't think you are above the many many investors, some good,  that lost a lot  by not recognizing a bubble.",hardware,2025-08-25 11:33:17,3
AMD,nadudzt,"N2 is, at least on paper, ready end of this year. Don't think that's necessarily the limiting factor.",hardware,2025-08-24 08:55:17,4
AMD,nbmvyly,"Curious, I thought Eriksson still did business within communications-networks? Especially in the wireless area. Are they entirely irrelevant in the wired networks-sector these days? \*looks at eriksson web-page\* Huh... I think that might be the case. I see nothing for wired broadband, only wireless.",hardware,2025-08-31 09:26:16,1
AMD,naodh6w,"Everyone who uses the word ""billionaire"" as a slur for enemies. (Which is most of the uses I see.) People who write totally sober, hinged, and admiring phrases like, ""poor, poor dears with their meager billions of quarterly profit"". Nope, no hatred there whatsoever!  The correct understanding of 'billions of quarterly profit' is, ""producing more desirable things and helping more people than you or I will in a hundred lifetimes"".  If you find yourself saying 'billions of quarterly profits' with a sneer on your lip, *that's weird*.",hardware,2025-08-26 00:04:45,3
AMD,nah1m2r,They're not limited by that either. It's a demand limitation.,hardware,2025-08-24 20:37:19,5
AMD,nah7jci,> Its not possible to know margins on a single product if your business units aren't separated out  But they are now.,hardware,2025-08-24 21:08:02,1
AMD,nafhu07,"> the Switch became profitable quickly  With the notable exception of the Wii, Nintendo doesn't sell any console at a loss. All their consoles have been sold above costs.",hardware,2025-08-24 15:51:43,8
AMD,nadwlg4,"It can make sense for Microsoft because they make the real money on games being sold (platform royalties + dev tools etc.), accessories (again royalties or just profitable OEM stuff), XBL subscriptions and so on. Sony famously burned a whole lot of money on the first generations of PS3s.   It would make zero sense for retailers because they have no reasonable expectation of seeing those sales reliably come back to them. At most they're selling them at BEP.",hardware,2025-08-24 09:17:33,2
AMD,nakqay7,"> lmao no. It's much more complex than that.  things usually are, but that's what it boils down too.   > Just so we see eye to eye, I'm not saying it is one,   nor am i saying it doesn't have the chance of becoming one. Bubbles events usually are described with 'but this ones different', but in this case I do believe it due to the political implications, it's self fueling nature etc. If you are MS, google, facebook, usa, china... you simply can not afford to pull out.   Google learned it first hand how quickly their search was rendered near useless.",hardware,2025-08-25 12:50:27,1
AMD,naduy8j,but do we know about volume,hardware,2025-08-24 09:01:02,1
AMD,naon34g,"You're odd, dude.",hardware,2025-08-26 01:00:48,2
AMD,nahbgjq,"yes, it'll be the one year anniversary next month. But the previous 56 years, they just ran wafers and sold the chips for what they could. I guess I need to adjust my thinking to the new way Intel does it. Optoelectronics fabs/industry typically don't separate out their business units.     The better point that I should have made was that as yield improves, the cost per die decreases, even though wafer prices are steady.",hardware,2025-08-24 21:29:14,2
AMD,naq064z,"> Wii  Actually the Wii U: https://www.gamesindustry.biz/nintendo-still-selling-wii-u-at-a-loss  I mean, considering the Wii uses 50% higher-clocked die-shrink GameCube processorsï¼Š, I'd be *very* surprised at the Wii being sold at a loss.  ã€€  ï¼Šwhile architecturally very different, it's the exact same idea as between an earlier 2.0GHz 90nm Athlon 64 X2 3800+ from 2005 and a later 3.0GHz 65nm Athlon 64 X2 5800+ from 2008 (the Athlon 64 being the last consumer desktop CPU I know of that had a true die-shrink *only* with no architectural and/or IPC changes like Intel did with their tick-tock)",hardware,2025-08-26 06:52:16,2
AMD,nakzvja,"> things usually are, but that's what it boils down too.   It's too empty of meaning to me but ok   > you simply can not afford to pull out.   That's a given, but that doesn't make it sustainable. Subprime crisis was a recent one that FI willingly invested on  to everyone's detriment. There are signs already that AI is a race to the bottom profit-wise, it's a given the current rate of capex is not sustainable, that the market is probably oversaturated and that there are a chunk of   hastily made AI startups burning VC cash. To be seen what market is left when investors want a return for their money.",hardware,2025-08-25 13:45:33,2
AMD,nadv1z1,"The dates TSMC gives are for volume. Historically, that's even meant *iPhone* levels of volume.",hardware,2025-08-24 09:02:06,3
AMD,nawh325,"That's some crazy specs for the price. I hope for an17 inch version of this, xd.",hardware,2025-08-27 06:06:00,6
AMD,nawql2j,"How would one go ordering this laptop to countries outside of China, just out of curiosity? Also, who gets subsidies on laptops? lol",hardware,2025-08-27 07:35:31,3
AMD,naxwuhs,"Mechrevo is mostly reselling machines made by ODMs like TongFang and Clevo. So there is a good chance this will pop up at similar resellers like XMG/Tuxedo (EU), PC Specialist (US), and others.",hardware,2025-08-27 13:07:18,2
AMD,nawubc8,"Ask your chinese friend to help, known as å›½è¡¥.",hardware,2025-08-27 08:11:56,1
AMD,naxyrjo,No way it'd cost anywhere under 1200 EUR at XMG/Tuxedo though lol,hardware,2025-08-27 13:17:45,2
AMD,nayu3rn,This is not made by Tongfang or Clevo thouggh.,hardware,2025-08-27 15:52:18,2
AMD,naxyq2u,Can't really do that as an LLC,hardware,2025-08-27 13:17:31,1
AMD,nay2htd,"Haha, for sure! There are some cheaper resellers like [this one](https://laptopparts4less.frl), but there is always going to be a significant markup compared to importing yourself.",hardware,2025-08-27 13:37:35,1
AMD,nazse3c,Mechrevo is literally Tongfang's own brand though?  Edit: OK no idk why I thought they were (probably because at one point they were selling reference the lightweight reference designs from tongfang when no one else were) but they're not.,hardware,2025-08-27 18:32:35,1
AMD,naztsv2,Interesting! Is this going to be a Mechrevo exclusive then?,hardware,2025-08-27 18:39:25,1
AMD,nayu05i,"As an LLC, you technically arenâ€™t allowed to use å›½è¡¥ since itâ€™s meant to reimburse individual consumers, not organizations. However, I did manage to ship to an office in China under my name for personal use.",hardware,2025-08-27 15:51:50,2
AMD,nb042rq,It's manufactured by Huawei's contractor appereantly.,hardware,2025-08-27 19:29:09,2
AMD,na5085v,FYI:   Upstage was chosen by the Korean government's top 5 list of AI LLM startups that will receive astronomical financial backing to produce frontier AI models.  That list will be reduced to top 2 and Upstage and their Solar model is a strong contender alongside LG AI (Exaone)  Samsung wasn't even shortlisted in the top 15 btw.,hardware,2025-08-22 20:45:35,11
AMD,na6ni36,"amd provides the gpus, what does amazon contribute?",hardware,2025-08-23 02:37:26,2
AMD,na6q5nu,Money.,hardware,2025-08-23 02:55:35,6
AMD,na8bpa3,Packages,hardware,2025-08-23 11:30:52,2
AMD,nadhy4d,Prime delivery.,hardware,2025-08-24 06:53:13,2
AMD,nada72c,Instance!,hardware,2025-08-24 05:41:38,1
AMD,n9znmbe,Figured it was only a matter of time.,hardware,2025-08-22 00:22:07,216
AMD,na033ng,AFAIK only 3 cards in that AMD generation use the connector.  The AsRock 9070 XT Taichi they're talking about and both the Sapphire 9070 + 9070 XT Nitro. In case you're thinking about getting those.,hardware,2025-08-22 01:54:57,152
AMD,n9zslnc,"Since its not a nvidia card, can I assume the lack of load balancing is part of the spec for this connector?",hardware,2025-08-22 00:51:13,78
AMD,n9zuovu,this spec is cursed atp,hardware,2025-08-22 01:03:35,72
AMD,n9zzkdo,"When they used the same board side power delivery design as Nvidia's 40 and 50 series for the 12vhpwr cards I knew this was only a matter of time. It sure would be great for Sapphire, Powercolor, Asrock, and whoever else uses the connector to revise the board designs and make them more like the 3090ti.",hardware,2025-08-22 01:33:23,17
AMD,na0iq7d,"Ok good, so this rules out the Sapphire Nitro and Asrock cards.",hardware,2025-08-22 03:36:03,16
AMD,na08krj,its unfortunate that these connectors and the spec isnt validated by the likes of UL/CSA/CE/etc like every other appliance that plugs into 120v/240v outlets. most likely because 12v devices are all exempt from testing.,hardware,2025-08-22 02:28:58,17
AMD,na1ghqb,Can you believe that a simple xt60 connector would be enough and problem free?,hardware,2025-08-22 08:26:38,7
AMD,n9zmbij,Some of the new Intel workstation cards have 12V HPWR. Lets see if they can get it right,hardware,2025-08-22 00:14:16,42
AMD,na07aqx,This connector should be banned by international treaty.,hardware,2025-08-22 02:20:58,25
AMD,na0ibvq,"When I saw there were models with the 12V connector, I said this was going to happen... it was a hot take at the time and pissed quite a few people off. Sometimes I hate being right.",hardware,2025-08-22 03:33:16,21
AMD,n9zo1vh,Weird because for cards of the 9070XTs power draw you can barely find any cases of burned 12V-2x6 cables. 99% are 4090s or 5090s with a few extremely rare 4080 and 5080s. Never seen an affected 300W TDP nvidia card.,hardware,2025-08-22 00:24:44,19
AMD,n9zo1w5,"> There have been no widespread issues with AMD GPUs and melting connectors, seemingly due to the lower power draw, but it clearly anything is possible, depending on the circumstances.  Did AMD (or their board partners) also remove all load balancing components like Nvidia did? If they did, then it's the same ""No shit sherlock, of course thermal runaway results in failure"" that Nvidia is dealing with.",hardware,2025-08-22 00:24:44,24
AMD,na03oet,Oof yeah this connector is definitely cursed no matter who uses it,hardware,2025-08-22 01:58:29,4
AMD,na0uqyu,how is this possible its like half the wattage of the spec of the connector,hardware,2025-08-22 05:07:02,4
AMD,na0i4h2,"gosh, you mean the bad connector is still bad even on AMD cards? who could have possible predicted this.",hardware,2025-08-22 03:31:50,6
AMD,na0cnh6,"why buy 9070xt with 12VHPWR, 12VHPWR is a shit",hardware,2025-08-22 02:55:04,8
AMD,na429vn,"If I were in the market for a new GPU, having a 12v would be a deal breaker. Not worth the risk to the card, the computer, or my home.",hardware,2025-08-22 17:56:57,4
AMD,na0wvvq,"I wonder if 50 series super refreshes will include load balancing this time around (hotspot sensor would be nice as well to return). I know Nvidia won't market those two anyway, but reviewers will do if they are there.",hardware,2025-08-22 05:24:27,2
AMD,na1qj6k,"I donâ€™t remember if my XFX even has the connector but my PSU specifically recommends multiple PCIE cables (3) over the 12V, which I found interesting.",hardware,2025-08-22 10:02:37,1
AMD,na1t98f,Surprising with a 300W TDP? you would need a lot of the pins to go high resistance to result in overloading the remainder.  When it happens to 5090s the margins are much slimmer due to much higher power so even one or two pins going high resistance can cause overloading.  They really need to start implementing fault protection at the PSU end.,hardware,2025-08-22 10:26:21,1
AMD,na7ou51,"my gigabyte can draw up to 520w according to hwmonitor so, yeah.",hardware,2025-08-23 07:51:06,1
AMD,naeh8lu,"As much as how nVidia uses this shit badly on the 5080 and above wattage cards *and that this standard needs either massive overhaul or replacement*, this is probably a nothingburger; just the usual rate of infantiles at this wattage.",hardware,2025-08-24 12:21:02,1
AMD,naimv05,hardly surprising. its not the gpu causing the issue.,hardware,2025-08-25 02:16:18,1
AMD,nb3ag0t,Those crappy connectors are not designed to handle 300 watts.,hardware,2025-08-28 07:07:05,1
AMD,nbkjjv8,# a Xfx Placa grÃ¡fica RX 9070 XT tambÃ©m usa esse conector?,hardware,2025-08-30 22:48:02,1
AMD,n9zzufg,"Now I am even more genuinely scared by this connector, because it managed to melt a GPU like a 9070 XT which probably is only around 350W on full power, I thought this issue only existed on ludicrously power-hungry GPUs like a 4090 - 5090 at nearly 600W",hardware,2025-08-22 01:35:05,1
AMD,na14is8,That guy used a questionable Kolink PSU,hardware,2025-08-22 06:31:06,-1
AMD,na8i4ws,"Iâ€™m here for the pitchforks and outrageâ€¦ oh nm, itâ€™s amd.",hardware,2025-08-23 12:18:37,1
AMD,n9zse91,Letâ€™s wait for tech YouTubers to really hold AMD accountable here.  Lol,hardware,2025-08-22 00:50:00,-12
AMD,na00cg4,I saw the Reddit post for this earlier. I think the guy was using a super budget 700w PSU so it makes me a little less worried. Hopefully this doesn't become a common problem as I love my Nitro lol,hardware,2025-08-22 01:38:08,-5
AMD,na2ko7j,To be perfectly blunt this has to be a user error or a QC failure because plenty of 5070 ti and 4080 super exist that have NEVER failed on this connector as a result of anything BUT user error and QC failure.,hardware,2025-08-22 13:26:42,-1
AMD,na2pi4b,"How is it possible to know this is the first? Lol the first one posted on reddit is not the first one it happened to.  Edit: You guys need to get outside more often, reddit isn't the real world what happens here is unique.",hardware,2025-08-22 13:52:24,0
AMD,na0znkl,"pair a card that constantly hovers \~360-400W power usage with connector that has problems and OLD PSU that doesn't have native support, rather use adapters and you have your receipt for disaster.   Also connectors rating falls down with temps. So with the lack of load balancing as well and potential lower Amps per pin due to heat plays a role here - we saw some crazy temps with 5090s connectors (albeit that was 600W+)",hardware,2025-08-22 05:47:33,-3
AMD,na2cift,It's Ngreedia's fault.,hardware,2025-08-22 12:40:40,-5
AMD,na281jb,Yeah that's how statistics work. A single failure of thousands doesn't mean anything.,hardware,2025-08-22 12:13:16,22
AMD,na1f7b1,"These are also the cards I recommend avoiding, due to the connector.  It is a failure of a connector. Do not support it, and it will eventually go away.",hardware,2025-08-22 08:13:52,83
AMD,na2llf3,I thought about buying the Taichi and the Nitro but didn't because of this connector.  I got the Red Devil instead.,hardware,2025-08-22 13:31:41,5
AMD,n9zuyqi,"Yes, its the spec.",hardware,2025-08-22 01:05:13,93
AMD,na3uq3d,"The connector is designed in a way that the octopus cables are probably load-bearing for the pin resistance tolerances. If you try and do crimp connectors like PSU vendors typically do, it isolates the thermals and the resistance to individual wires instead of being able to share heat across wires. So one wire gets hot, the pin fails, the other wires get hotter, their pins fail.",hardware,2025-08-22 17:19:30,2
AMD,na1n7fc,"NO, the correct move for those companies is to recall the 12 pin nvidia fire hazard devices and replace the units with 8 pin pci-e connector ones.  there is no ""safe 12 pin nvidia fire hazard"" connector. it doesn't exist.  the only safe nvidia 12 pin fire hazard connector is a recalled one.",hardware,2025-08-22 09:31:49,8
AMD,n9zqih5,Nobody is getting this right. The 12V HPWR is a flawed design.,hardware,2025-08-22 00:39:15,173
AMD,n9zzbxj,Fundamentally bad due to not enough copper canâ€™t be fixed,hardware,2025-08-22 01:31:56,34
AMD,n9zmrum,"The workstation cards tend to have some extra protections and care taken with them, so I don't expect a failure rate beyond what other brands have seen in that setting.",hardware,2025-08-22 00:16:59,7
AMD,na1dg1v,"I warned about this too, I was seriously disappointed the Saphhire Nitro model used it and then was later confirmed to have NO load balancing which is the root problem here! The design cannot just handle going without it yet the spec is to have no load balancing, it's stupid.   Now I wasn't looking to buy this gen, I want next gen Radeon if they're good value but dammit I have a Saphhire and Saphhire is commonly called the EGVA of Radeon, they make good cards and stuff. But jeez I really do not want to touch this connector EVER if I had another choice for a GPU unless it is CONFIRMED to have load balancing which then I could trust it because some 30 series had this connector but Nvidia was handling the spec at the time and yeah all of them had load balancing and nobody had the melting issues we now have with 40 series since the official specification without load balancing came out, then 50 series and the odd 9070 XT with them again melting. Just took the 9070 XT this long because only a few specific models use it and then they still don't draw as much as a 4090 even for the OCs, biggest out of box OCs are about 5080 level which is less risky but still CAN melt which we saw here.",hardware,2025-08-22 07:56:16,10
AMD,n9zp1ms,the 9070 xt can draw nearly as much as a 5080 despite having the same 300w tdp  [https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-nitro/41.html](https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-nitro/41.html)  yeah the nitro for example draws 50 more watts than the 5080 in gaming  [https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-pulse/41.html](https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-pulse/41.html)   even the pulse draws a little more than the 5080 in gaming  I imagine an oced 9070 xt could draw even more than a 5080 at max load,hardware,2025-08-22 00:30:32,22
AMD,na1cqhd,The 9070XT is probably far less common than the 4090 or 5090.,hardware,2025-08-22 07:49:14,7
AMD,n9zzz2s,"In these cases it's most likely due to extenuating circumstances like the cable not being seated fully. Lower TBP shouldn't lead to runaway heating, even if all the load passes through one pin.",hardware,2025-08-22 01:35:53,5
AMD,na0it75,"If you discount shunt mods, that is.",hardware,2025-08-22 03:36:37,0
AMD,n9zr7ar,AMD doesn't really police what power connectors their board partners use or how they're implemented. The use and implementation of the connector on the card here was entirely on the company who produced this card.,hardware,2025-08-22 00:43:19,79
AMD,n9zuu50,"Having load balancing would be out of spec, as the spec requieres merging all input limes when they reach the board.",hardware,2025-08-22 01:04:27,43
AMD,na06shi,"> Did AMD  As far as anyone has found, AMD's only reference design had 2x8 Pin connectors. You can find that PCB in cards like the ASRock Steel Legend or Sapphire Pulse.   Anything that deviates from that design is likely designed by the card maker themselves.   Media comments by AMD in the past were that, paraphrasing, ""they'd allow card makers to make their own decision around 12v-Poor-Power vs classic 8 pin connectors.""",hardware,2025-08-22 02:17:49,39
AMD,n9zxoeu,"In this case you are talking about Sapphire, since they are the ones responsible for this card. AMD isnt as strict as Nvidia when it comes to board design. If they were, this connector would never been used in this card.  And no, there's no load balancing on the 9070XT Sapphire Nitro+",hardware,2025-08-22 01:21:43,11
AMD,n9zzn0m,"Yes, they did.",hardware,2025-08-22 01:33:49,-1
AMD,na25e1n,"The spec is broken. 12VHFRPWR is simply a bad design, and everyone should stop using it.",hardware,2025-08-22 11:56:06,15
AMD,na330ku,Because it's a garbage standard.,hardware,2025-08-22 15:01:26,3
AMD,na2qmga,"Yeah, that is the question.",hardware,2025-08-22 13:58:13,3
AMD,nal6erb,this overclocked card draws more wattage than a 5080.,hardware,2025-08-25 14:20:34,1
AMD,na0vksw,"Probably because the taichi and the nitro are the best reputation radeon cards. People shouldn't buy them because of the absurd premium, but yeah, the connector is stupid as well.",hardware,2025-08-22 05:13:42,10
AMD,na1o8rt,Just buy a Powercolor then. I have a Powercolor Hellhound 9070 and it's so quiet and runs like a beast. And it has two 8pin.,hardware,2025-08-22 09:41:39,3
AMD,naiciyu,"Even if it isn't for a 12vhpwr connector, you should always use multiple cables if possible for current balancing",hardware,2025-08-25 01:10:42,1
AMD,na0j81a,"Its not a reference AMD card though, AMD uses 8 pin. This is 100% on the board partner and the shitty connector.",hardware,2025-08-22 03:39:31,20
AMD,na14z45,"It wasn't an AMD decision to use it. Amd's reference designs do not use it, but they allow their board partners the freedom to choose what connector and implementation they wise to use. In comparison, Nvidia actively forces their board partners to comply and use thr standard.",hardware,2025-08-22 06:35:19,9
AMD,na04o3o,Why? Nvidia is the one who forced everyone else to adopt it.,hardware,2025-08-22 02:04:36,-2
AMD,na03b3j,They will find excuses to not do it // why itâ€™s probably Nvidiaâ€™s fault this happened ðŸ¤£,hardware,2025-08-22 01:56:13,-13
AMD,na1j8m2,For this particular failure it does not matter what PSU you have. None of them have load balancing on the supply side,hardware,2025-08-22 08:53:27,7
AMD,na0rjfo,"Itâ€™s a low-quality PSU, but I donâ€™t think the cable failed because of it. It has low 12V rated power, and its protections arenâ€™t tuned well. Iâ€™m not an expert, but the only way the PSU could cause this would be through a crazy transient spike - and this PSU seems to be within spec. Back in the day, I personally built more than a dozen PCs with Chieftec PSUs (also 230V) together with Radeon GPUs (which were power-hungry but offered good bang for the buck), and I never saw an 8-pin connector fail.  Edit: I should also point out that there werenâ€™t 80+ certifications for 230V PSUs back then, since the rating was done at 110V (or whatever the U.S. standard is). Some tier lists rate them as F-tier just because of that.",hardware,2025-08-22 04:42:01,9
AMD,na4fjgo,It means the whole thing is badly designed and should be corrected.,hardware,2025-08-22 19:02:47,10
AMD,naegr71,"Yeah, until we'd see more this is well within the baseline rate of infantile failure that should not cause alarm. The connector ain't great, but if it's run like the old school 8 pin standard, it's fine.",hardware,2025-08-24 12:17:36,2
AMD,na1g3wa,"My personal recommendation is to treat 12VHPWR as if it was rated for 300W.  I'd be okay with that 9070 (245W TDP), but those two OCd 9070 XTs can both reach ~350W, as per TPU reviews.  Those are all cards from just half a year ago, concerning how they'd fare after 3-5 years.",hardware,2025-08-22 08:22:50,36
AMD,na2x0gn,"It's a PCI-SIG standard. It won't go away.  And no, this is most likely not due to the connector.",hardware,2025-08-22 14:31:01,-8
AMD,na05bol,You are probably not going to believe this but the spec actually says the other way around. It states that all lines should be bridged on the device side.   The AMD cards don't have load balancing either. Both AMD and Nvidia followed the spec. It's the damn specification itself that's the problem. The culprit is working for the PCI-SIG. (Of course Nvidia making it mandatory on all their cards didn't help either),hardware,2025-08-22 02:08:40,95
AMD,na4h6v2,The connector itself is ok if there is load balancing. There is no need to throw out the baby with the bath water.,hardware,2025-08-22 19:10:55,1
AMD,na17s3y,With multiple load balanced rails it would be safe. But for some reason the official spec just does not require it.,hardware,2025-08-22 07:01:33,7
AMD,n9zzuw4,12V-2x6 seems to work well,hardware,2025-08-22 01:35:10,-32
AMD,n9zpnuu,"Skeptical if there's any practical difference in the construction, but if nothing else, they have much lower power draw than something like a 5090.",hardware,2025-08-22 00:34:13,16
AMD,na04key,We're also going to be far less likely to hear about a failure in professional/enterprise systems unless it's by chance an enthusiast that it happens to.,hardware,2025-08-22 02:03:58,6
AMD,na2qixx,"I wanted to buy my first ever Sapphire card, I loved the look of the Nitro. I saw the connector and said nope.",hardware,2025-08-22 13:57:42,2
AMD,n9zqnzi,"If you look on slide below, wirh RT on they have the same power draw.  If you're looking at a less conservatively tuned card than the FE the 5080s RT power draw is slightly higher than the 9070XTs.  Didn't expect the 9070XT to draw 350W in raster though. That's really mad.  Still, 350W power draw is a normal use scenario for a 5080 as many people probably buy the card for its RT performance.   We should see the same very rare failure rate.",hardware,2025-08-22 00:40:10,1
AMD,n9zx1ed,"Well then fuck the spec, do it anyway. Then work to get the spec changed.",hardware,2025-08-22 01:17:50,37
AMD,na0kb3r,"To add to that, it's mostly 4090s and 5090s that have been chewing up 12VHPWR connectors.   A card pulling 300ish watts saturates less than half of the connectorâ€™s peak threshold, after all.  That's not exactly a fire hazard, load balancing or not.   Personally, I won't be terribly surprised if the card was shunt modded. It's surprisingly easy to mod 9070s and a must have to push it up to ~3.5GHz.   Next to no reason to point fingers at AMD.",hardware,2025-08-22 03:47:17,0
AMD,na03p2u,"The GPU in question is an AsRock 9070 XT Taichi OC, as per the article/reddit post.  Still good to know about the Sapphire implementation.",hardware,2025-08-22 01:58:36,15
AMD,naehch1,It doesn't really run hard enough at stock to need it.,hardware,2025-08-24 12:21:48,1
AMD,na0hi04,Well most 9070s are not using it so...,hardware,2025-08-22 03:27:32,7
AMD,nal3sj9,"while the design is bad, the failure rates are bellow usual hardware failure rates here so its not something extraordinary.",hardware,2025-08-25 14:07:07,2
AMD,naef7cj,"Yes, no hardware ever failed before. This is a totally new phenomena",hardware,2025-08-24 12:06:15,1
AMD,nal45yd,"Back when this was big news in 4090 days someone dug up some retailer data. The basic failure rate for consumer GPUs were 2% based on returns/RMAs. The failure estiimate from the connector issues was bellow 0,5%, well within the regular failure rate.  The old school 8pins burned too btw. It was rare and it never set anything on fire. It usually was manufacturing defect that caused it.",hardware,2025-08-25 14:09:04,3
AMD,na1q0w0,There was even a report of a 5070 non-Ti with a melting 12VHPWR cable. No one is safe.,hardware,2025-08-22 09:58:07,38
AMD,na1u6zo,"Iâ€™ve got the Sapphire 9070xt. Iâ€™m Pretty concerned now. I do check that the connector is firmly seated and have a PSU with a connector on it, not using adaptors, and my PSU spec is rated for 600w through the connector.   But itâ€™s still worrying. This is my first big-boy GPU and I donâ€™t want it to cause a fire.   That being said Iâ€™m undervolted for 96% of the performance at 240w.",hardware,2025-08-22 10:34:12,7
AMD,naeh0zv,"Up to 375 watts seems to be the safe zone for the standard, no matter the on paper specs. This and that 5070 are just the standard rate of infantiles you'd have seen on the old standard IMHO.",hardware,2025-08-24 12:19:33,2
AMD,na38rgd,"If the connector is melting on multiple models for hundreds of different users even with gpus with moderate power draw... the connector is the problem. I don't care if its user error, your power connector should not allow for user error to cause a dangerous failure state. Clearly this connector is more prone to failure than the old style ones, which makes it a bad design.",hardware,2025-08-22 15:30:09,12
AMD,na16xdn,> You are probably not going to believe this but the spec actually says the other way around.  But they said the lack of load balancing is in the spec. Everyone here agrees the spec is at fault.,hardware,2025-08-22 06:53:30,25
AMD,na08icm,"You're right, and that's the weird part for those who don't know:  The first implementation of the 12HPWR cable, the GeForce 30 series, was out of spec. In fact, it was way above spec. Load balancing seemed to have been forced on the AIBs by NV. GF 40 series rolls out, and all signs of load balancing are completely gone. Which is paradoxically ""in spec.""   You're right, PCI-SIG, and it's development sponsors Dell and Nvidia, screwed the pooch on this one. They screwed it, and have done jack shit to resolve the issue. Seeing all the issues with GF 40 series, NV acted like GPU's self immolating is intended behavior and repeated the design failures in 50 series.   Seeing AIB's, especially ones who don't even make NV cards, doing the same shit is infuriating.   Makes me wish AMD had contractually mandated load balancing if AIBs wanted to deploy 12v-Poor-Power.",hardware,2025-08-22 02:28:33,59
AMD,na1rw0o,"Thats what I said, that not having load balancing is the spec.   However I would not put all the blame in pci-sig. As far as I know, Nvidia pushed them to go It this way.",hardware,2025-08-22 10:14:41,7
AMD,na4yp92,"If they really wanted the GPU to not do any load balancing, the spec should have mandated overcurrent protection at the PSU level. Allowing the possibility of a single pin to theoretically draw 50A is ridiculous.",hardware,2025-08-22 20:37:52,6
AMD,na3bc5p,Nvidia paid PCI-SIG off to go with this design. The only thing that'll solve this problem is a class action suit.,hardware,2025-08-22 15:42:59,6
AMD,na5o06i,"it is literally a 0 safety margin connector with tiny extremely fragile connections.  there is no saving this fire hazard. it is fundamentally broken.  and NO load balancing the individual pins by splitting the vrm can not be used at all ever, because even if it would prevent all melting, which absolutely no one should expect, then that would still be inherently unsafe, because the connector would now have insane pcb requirements on the target, which would also be impossible for certain uses, that can't just split a vrm into half.  just think this through.  it would be the equivalent of saying: ""this one wallplug with 12, instead of 2 power connectors is perfectly safe and not the reason it melted when connected to my mini oven, because it is the mini ovens fault, which used the connector wrong.""  can you see how insane that idea is. the connector by itself needs to be safe and it is not and it CAN NOT be ever.  you want a small safe connector? alright sure, let's use xt90 or xt120 connectors. both have 2 connections for power (one of which ground and one 12 volt then) and the xt120 connector does 60 amps sustained, or 720 watts at 12 volt.  so again YES you throw the nvidia fire hazard connector out with nvidia.  remember all the people claiming, that eps 8 pins (cpu power connectors 235 watt per connector) need to have their load balanced through splitting it in half? oh yeah NO we don't, because that is insane and doesn't matter and it needs to NOT matter to safe connectors, which the eps 8 pin connector is.  please stop trying to defend this nvidia fire hazard, as if it would suddenly be perfectly fine, if only x would happen.  how many x did we have now?  >is x: user education after the wrongful claim, that it is just user error.  >is x: the sense pins being too long, but now they are shorter with 12v2x6, so now it is safe  >is x: rtx 50 series release, which nvidia claimed will be completely free from any melting  >is x: using the adapter, that comes with the card, instead of anything else, because some random people lied about it and claimed, that certain nvidia 12 pin fire hazards were safe, while others aren't, yet ALL ALWAYS end up melting?  can you give up on looking for x and accept, that there is no saving an inherent nvidia fire hazard?  is that possible?",hardware,2025-08-22 22:57:26,4
AMD,na1aajd,"no, it should have been overspeced like every other power connector.  instead they decided to go with zero headroom at every point so it cannot handle any load imbalance, which is insane.  pcie connectors speced for 12.5a per pin, whereas 12vhpwr only  speced for 9.2a per pin, despite the much higher loads per pin.  pcie can handle 150w per pin giving it 100% headroom, 12vhpwr can handle 110w per pin giving it 10% headroom.  Both use 16awg which can load 12a, which again is nearly 100% headroom on pcie, but only 44% headroom on 12vhpwr.  The fact that the only cards not having issues are those under 300W suggesting we do need as much as 100% headroom, and 12vhpwr should have moved to at least 14awg.",hardware,2025-08-22 07:25:26,53
AMD,na0h7xy,Shouldn't this be the 2x6 version at this point.,hardware,2025-08-22 03:25:38,19
AMD,na07jq4,"It does not, they are all failures of design and should be scrapped.",hardware,2025-08-22 02:22:33,31
AMD,n9zqbny,"True on the construction. I'm thinking more about installation and monitoring care. Workstation and server builders won't take ""eh looks alright"" as assurance very often, and especially not on GPUs that get close to 5 figures at the top end.  The only chance I see for danger with ARC cards is the B60 Dual. If a version of that tries to power the whole board on a single 12-pin, that could get a bit risky. Given we've now seen a <400W card melt, and 4090s were quite melty at 450W, that card could be at risk.",hardware,2025-08-22 00:38:08,11
AMD,na4z5fl,6000 Pro is 600W,hardware,2025-08-22 20:40:08,1
AMD,na05xvf,"True, though I'd expect even a slightly higher than normal failure rate to have made some noise in reports by now. Maybe it has and I'm unaware.",hardware,2025-08-22 02:12:28,1
AMD,n9zr4uj,[https://www.techpowerup.com/review/asrock-radeon-rx-9070-xt-taichi-oc/41.html](https://www.techpowerup.com/review/asrock-radeon-rx-9070-xt-taichi-oc/41.html)  yeah and the taichi goes to 365 lol. The AIBs on amd really push the cards to the limit lol. I think the nitro xtx pulls almost 100w over the reference xtx stock  I think nvidia is more strict with the AIBs bc the stock power draws are all pretty similar stock but you can increase them manually,hardware,2025-08-22 00:42:56,13
AMD,na15dlt,"There's something I don't think most people understand about the 12vhpwr connector. While it does have thicker gauge wire, 16AWG vs common 8pin PCIe 18AWG, the 12vhpwr uses a smaller contact point.   And 12vhpwr actually has the same number of voltage supplying cables as 2x8Pin, 6 total.   So technically speaking, the odds of a 12vhpwr having connection issues and melting at 300w are higher than 2x8 Pin doing so.   It's a bad design at 300w, and it's a *God Awful* one above that. All they had to do was make the connector a bit bigger and this whole thing could have been avoided.   If you watched der8auer's videos on the subject, you'll note that all it takes is for one point of poor, or very poor contact to start a slow snowball affect that could take out the whole connector. In some instances, like while using Asus's Astral cards and their software, you can get absolutely horrible contact off of any cable at random. Perhaps this 9070 XT just had a shitty cable, like so many, and the snowball affect occurred.",hardware,2025-08-22 06:39:03,9
AMD,na03uy2,My bad. But none of them has load balancing.,hardware,2025-08-22 01:59:35,7
AMD,na1w4z0,"Yup, for some numbers:  * 1 model out out of ~15 different 9070 SKUs  * 2 models out of ~20 different 9070 XT SKUs  That's with all the OC/non-OC and color subvariants excluded from the lineups.",hardware,2025-08-22 10:49:50,7
AMD,nalfvyq,"The connector needs revision hard, but the reason the 4090 and co were dying was abandonment of very basic design standards.",hardware,2025-08-25 15:08:25,1
AMD,na4x8g5,IIRC the non Ti only has 2 connectors.Â    The 5070 Ti was like 5 W over the limit for 2 connectors so it has 3 and is theoretically much safer because of that with a lot of overhead.   Its also been a while since I looked at this so I could be misremembering.,hardware,2025-08-22 20:30:28,1
AMD,na21rqj,"Hear me out, if you do insert it properly there is literally no problem with it. The problem with it that people apply a normal level of force installing other components, itâ€™s just that 1% which fail to do so when it comes to that connector. If you do insert it with a normal level of force and double check it there shouldnâ€™t be any of burning/melting issues.",hardware,2025-08-22 11:31:34,-24
AMD,na1wshq,"That last part alone already makes it a lot safer. 1/3 of the wattage cut is pretty solid.  Checking that everything is fully seated and no cables near the connections are overly flexed or otherwise stressed, you should be good.",hardware,2025-08-22 10:54:52,7
AMD,na4giki,The cable being fully seated means nothing. What matters is the contact of the pins and you can't do anything about that.,hardware,2025-08-22 19:07:34,2
AMD,naerwn1,">Clearly this connector is more prone to failure than the old style ones, which makes it a bad design.   What is the failure rate on this connector vs old ones?",hardware,2025-08-24 13:31:13,2
AMD,na39m91,"""Hundreds of different users"" is about 0.2% of consumer GPUs sold, not even including workstation and datacenter GPUs that use the exact same connector, exact same power draw, but 0 issues.  It isn't the power connector issue.",hardware,2025-08-22 15:34:26,-8
AMD,na17rnw,"Read over the post too fast before I responded. Oops. But yeah, load balancing is not part of the spec.",hardware,2025-08-22 07:01:26,13
AMD,na13kqk,"Smells like a lot of conflict of interest going on here, and nothing at all being done to stop it",hardware,2025-08-22 06:22:30,14
AMD,na2cw6u,"AMD (and IBM, Intel, Qualcomm, more) are board members. It's not just Dell and Nvidia. *Everyone* was on board with this spec, they're all equally to blame.",hardware,2025-08-22 12:42:55,4
AMD,na4zjsd,"True, that's the second part I keep thinking, why don't we have any PSU yet with per-pin overcurrent protection? With all the drama going on, you'd think a marketing department would jump on this and rush such a feature to the market.",hardware,2025-08-22 20:42:09,2
AMD,na1hhme,To be safe without load balancing 12vhpwr would need to be 500% overspeced. The real problem is that 5/6 wires can fail and no safety mechanism will trigger. The card will just keep pulling power like always.,hardware,2025-08-22 08:36:29,17
AMD,na2q3f7,"It can take 800w but most cards using it do not draw anywhere near that, its used in 5070's etc which use less than half the available power.",hardware,2025-08-22 13:55:28,0
AMD,n9zto2n,This is a valid point. Not many of these are going to be bought for DIY.,hardware,2025-08-22 00:57:27,2
AMD,n9zvvuk,">I think nvidia is more strict with the AIBs  Yeah it is, Nvidia is very strict with what its AIBs are allowed to do. AMD is more lenient and lets the board partners decide more on their own and Intel is the most lenient, more or less allowing anything so long as the AIBs promise to handle potential warranties/returns",hardware,2025-08-22 01:10:49,4
AMD,na22x3w,"Even if you insert it properly, if there's e.g. dust on the connector, or after some insertion/removal cycles some pins have lost their coating, it'll affect the resistance of individual pins unevenly.  On RTX 5000 this will cause uneven load distribution, which can lead to fires.",hardware,2025-08-22 11:39:35,21
AMD,na1xa98,"Mine does have the hidden plug though, which takes a pretty hard 90 degree turn out of the connection.   Iâ€™ll take my chances I guess.",hardware,2025-08-22 10:58:37,2
AMD,nb03tzo,"Reddit doesn't understand statistics, don't bother.",hardware,2025-08-27 19:28:00,1
AMD,na15ia9,"Probably saving more money by using the cheaper connector solution than they're losing on RMAs. Assuming they even honor the RMA. And after warranty expires, they actually benefit as cards won't live as long.",hardware,2025-08-22 06:40:16,18
AMD,na8eyls,"They didn't have to be exactly on board. If you are just on dissenting voice and the big guys like Nvidia convince others, what can you do?",hardware,2025-08-23 11:56:06,2
AMD,na1kcqi,"I think most people would be pro load balancing required on future connectors, but overspeccing the replacement cable and connector will be as important.",hardware,2025-08-22 09:04:18,11
AMD,nal5ied,and yet even with 150% overspecing 99% of the reported issues would not have happened.,hardware,2025-08-25 14:15:58,1
AMD,na2u797,"Under spec 662W is all the connector can take. If you have an over engineered connector, then maybe you can push it to 800W, but anything just to spec will begin to heat up at the 662W mark.   And that's assuming everything is completely load balanced. Which won't happen naturally.   Similarly, the older 300W 8pin PCIE can actually take 576W under spec. The connector can do 600W, but the use of 16awg wire becomes the limiting factor holding it back at 576w.",hardware,2025-08-22 14:16:37,6
AMD,n9zxmun,"You'd be surprised, if they are useable for consumer llm I can see a lot of people buying them instead of 3090/4090.  edit: apparently 1200$, which is insane considering 3090 still sell for 800$ with half the ram.",hardware,2025-08-22 01:21:28,3
AMD,na2xmqy,> some insertion/removal cycles some pins have lost their coating  After 30 cycles.   How many times do you plug / unplug your GPU during its life time? Pretty sure you can count that with your hand.,hardware,2025-08-22 14:34:10,-10
AMD,na23da2,Fires? Did we have a single case where a system with a 5000 card which went up in flames? I donâ€™t think so,hardware,2025-08-22 11:42:41,-14
AMD,na3ybfi,Keep an eye on it.,hardware,2025-08-22 17:37:23,2
AMD,na9ewjt,"Do you genuinely think Intel, AMD, IBM, Qualcomm, and ARM are cowed by Nvidia and... Dell... to just go along with whatever they say?   (Spoiler: no they aren't.)",hardware,2025-08-23 15:30:27,0
AMD,nawg6qk,"A 400w card was able to burn a 660w spec power connector. In this case it was already 165% overspecced. Without seperate power rails, load balancing or shutoff mechanisms, even if it was 200% overspecced it will only delay the failures, as long as a single wire cant carry all 600w safely there will be a risk of burning.",hardware,2025-08-27 05:57:46,2
AMD,na3dyhk,OneAPI blows compared to CUDA,hardware,2025-08-22 15:55:57,1
AMD,na3jwz8,"In reality, much fewer than 30 cycles have been found to cause this issue.  And while this isn't recommended, some people remove the GPU to properly clean and de-dust their computer every quarter or so. That increases the risk significantly, as the freshly disturbed dust can settle inside of the connector.  That's a perfect recipe for disaster in just one or two years.",hardware,2025-08-22 16:25:43,11
AMD,na2akya,"[1](https://www.pcgamesn.com/nvidia/geforce-rtx-5090-fire-bf6-beta)  [2](https://www.reddit.com/r/nvidia/comments/1iv7277/my_5090_astral_caught_on_fire/)  I don't really understand why you've got a problem with what /u/justjanne said. Uneven loads on a circuit melts components of the circuit. Melted components lead to shorts. Shorts lead to fires. Are you expecting aluminum cases to burst into flames, or some other exaggerated situation?",hardware,2025-08-22 12:29:02,14
AMD,nawjn1a,"in the case of OP, it was a card overclocked to use more than 600W.",hardware,2025-08-27 06:29:43,1
AMD,na7kz4y,For inference you can just run with Vulkan. CUDA is only a necessary if you train,hardware,2025-08-23 07:13:21,1
AMD,na4xhob,All the more reason to not use a native PSU. If you use a dongle then you never need to unplug the connector from the GPU.,hardware,2025-08-22 20:31:46,2
AMD,na3l2kb,">In reality, much fewer than 30 cycles have been found to cause this issue.  Source needed. [PCI-SIG's own spec is 40 cycles](https://www.lowyat.net/2022/285407/new-pcie-gen5-power-cables-supposedly-only-last-for-40-cycles/).  >And while this isn't recommended, some people remove the GPU to properly clean and de-dust their computer every quarter or so... That's a perfect recipe for disaster in just one or two years.  Then they don't really know what they're doing, isn't it? We shit on people who use water to clean their motherboard. Why are we giving a pass to people who treat their hardware in a way that is way out of spec and out of norm?",hardware,2025-08-22 16:31:26,-6
AMD,na31zj3,"> I don't really understand why you've got a problem  They just bought a 5080 3-4 days ago and are worried about their puchase, thats why.",hardware,2025-08-22 14:56:16,5
AMD,nal4siy,Neither of those links were fires.,hardware,2025-08-25 14:12:14,1
AMD,nb03hh2,Neither of those are related to the connector,hardware,2025-08-27 19:26:23,1
AMD,na2zqy5,"After a lot of comments from ""very smart people"" in the last few months which state a 5090 is gonna burn your house down, actually yes. Dust seems even like a quite unrealistic point.",hardware,2025-08-22 14:44:53,-5
AMD,na3o2u4,"> Then they don't really know what they're doing, isn't it? We shit on people who use water to clean their motherboard.  Well, we probably shouldn't do that either, as demineralised water is a perfectly fine way to clean electronics, and in certain ways superior to isopropyl alcohol.  > Why are we giving a pass to people who treat their hardware in a way that is way out of spec and out of norm?  Well, because they're used to electrical devices being built to a much higher standard.  The required standards for electrical devices in Germany would normally require that if multiple wires share a load passively, either each wire needs to be fused individually, or each wire needs to be able to take the full load itself.  With 12VHPWR, that'd require AWG10 per wire, or if each wire is individually fused, AWG15. For reference, 12VHPWR uses in reality AWG16.  So it's not that people's expectations are wrong, it's that PCI-SIG is applying significantly lower standards than people are used to and can reasonably expect.",hardware,2025-08-22 16:46:27,5
AMD,naddsi2,I own one and I am not at all. The topic is just massively overblown and 99% of the cards with that connector will just fwork fine for their entire lifespan.,hardware,2025-08-24 06:14:29,1
AMD,na4g41b,The dust can be one of the triggers for the problem. It's so badly designed it can have multiple causes.,hardware,2025-08-22 19:05:36,4
AMD,n83vb6h,Still happy I got it for cheap as hell for my old AM4 mATX build for like 140 euro with coupons from Aliexpress last December,hardware,2025-08-11 13:25:00,121
AMD,n84020s,Last summer it was still a very good AM4 upgrade path at 180â‚¬ for the tray variant but at 240â‚¬ it hasn't been  a viable product in quite a while unless you really dread replacing your motherboard and all that it entails.  For 240â‚¬ you can comfortably get a decent B650 board and 32GB of DDR5 RAM. Chances are you can sell your old AM4 platform for enough money to cover the cost of a Ryzen 7600 and then you're good to go with your AM5 upgrade.,hardware,2025-08-11 13:51:57,34
AMD,n83vinb,Itâ€™s hard to justify spending significant money on it at this point unless youâ€™ve got an incredibly top of the line AM4 motherboard with a ton of ram that you want to keep using. A Zen 4 7600 is usually similar in performance or similar enough that moving to AM5 is more enticing.,hardware,2025-08-11 13:26:10,59
AMD,n84bg17,"New <$200 5700x3ds have been gone for many months, 5800x3d and new <$300 5800x3ds have been gone for almost a year",hardware,2025-08-11 14:50:33,16
AMD,n840vgk,"Salute, to the budget king.  I'll run a round of 7zip benchmark in its honor.  [And I'll deliver](https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Farjb7tudceif1.png%3Fwidth%3D1024%26auto%3Dwebp%26s%3D512b51cbd0cc79cda957c007fca260fb17fe6a78)",hardware,2025-08-11 13:56:23,25
AMD,n84in34,"i can't fucking believe all the times i chose to hold off buying it at MC because I told myself it would eventually be dirt cheap  so fucking dumb, and now my plans to ride AM4 until AM6 are harder because this 5700x won't last nearly as well as the x3d would have",hardware,2025-08-11 15:25:50,11
AMD,n83toy7,It's double the price of a 5700 non x here in AU. At what point is X3D not worth it?,hardware,2025-08-11 13:15:47,24
AMD,n83xc2c,"I used to own one of these and was able to sell it off for more than what i actually paid for just a few months ago when i switched to AM5, and I think this is likely the reason why their prices have went up. the stock is drying up and they are getting more expensive even on used market to the point they don't make any sense anymore.  So, for people planning to upgrade or especially newer builders I think it's best to give up AM4 at this point and switch to AM5, because the overall price of the new build will end up being nearly the same anyway and you are also in a newer platform that is not dead end and DDR5 which has higher bandwidth.",hardware,2025-08-11 13:36:33,10
AMD,n843aob,If you looked at the recycle rate in China(Basically a bunch of clearinghouse that would accept any functioning CPU). 57X3D saw a rose since 2024-09 AMD exceeded fair price for performance at 2014-12,hardware,2025-08-11 14:09:03,4
AMD,n83xpqy,Low bin sells on low stock and runs out? Water is wet? Why is this news?,hardware,2025-08-11 13:38:43,11
AMD,n848j96,"Aww, nuts. I upgraded my 3 sons' PCs from Ryzen 3600 to the 5700x3d last November for $180 each. That was a huge value improvement. I couldn't justify swapping out my 5800x.  I've been on the AM4 train since the end of 2017. What a ride!",hardware,2025-08-11 14:35:57,2
AMD,n84962o,So glad I grabbed one when they were going cheap on aliexpress it's the heart of my 3d printed SFF LAN PC with a 4070 super.,hardware,2025-08-11 14:39:08,2
AMD,n84mmhh,Been out of stock since early this year. All resellers.,hardware,2025-08-11 15:45:27,2
AMD,n85o5p9,5700X3D will be missed,hardware,2025-08-11 18:48:59,2
AMD,n8hofec,And thanks to this announcement the price is skyrocketing,hardware,2025-08-13 16:15:18,2
AMD,n8436qr,"This CPU is amazing, basically felt like what happened when I went from an old I5 gpu to Ryzen 1800x, and I was going from a 3800x to this one. Just upgraded to a 9800x3D and honestly not all that much of a jump with a 4090 @ 1440P.  Still, I wanted to use my 4 m.2s and change my PC case so I upgraded since I can pass this chip and ram to my fiance.",hardware,2025-08-11 14:08:28,4
AMD,n840lge,It's been nearly 1 year since this was worth getting. I think it's good that it's off the market now since the price was more horrible. Am4 owners will now have to make the best upgrade choice which is going am5 and NOT missing the boat on future 11800x3d/11700x3d discounts.    I picked up the 5700x3d + 32gb ram on newegg for $180. I don't think you can beat that value really. Sold my 5600x + 16gb ram for $120 making it the most effective price:perf upgrade ive ever done,hardware,2025-08-11 13:54:54,2
AMD,n85hlmi,"I remember when 5800x3d first came out i was contemplating upgrading from a 5800x since the perf difference back then seemed rather trivial. Especially since I used to rock a 1080ti, but years later i am so so happy i upgraded since now i got a 6800 xt and thr CPU is so freaking good basically keeping up with the latest mainstream CPUs in gaming apart from the very top end ðŸ˜€",hardware,2025-08-11 18:15:40,2
AMD,n843uut,I wish I had gotten one when they were on sale last year lol,hardware,2025-08-11 14:11:59,1
AMD,n84emi8,"That is strange,  I still see it in online stores",hardware,2025-08-11 15:06:06,1
AMD,n84xt40,"Still OK with the fact that I went for 16C Zen 3 over 8C with 3D-vcache... $200 for 16 ""not bad"" cores is solid.",hardware,2025-08-11 16:40:35,1
AMD,n864y6z,\*sweats in 5800x3d and pulling the trigger to sell on ebay\* I legit checkd they are selling for $300-$450. WHO BUYS a 5800x3d for $450!!!! used!!! mines just in the box chillin ever since i upgraded to am5,hardware,2025-08-11 20:14:16,1
AMD,n86d0e3,I'm looking forward to the 9600X3D dropping later this year.,hardware,2025-08-11 20:52:52,1
AMD,n8bx78z,The budget CPU that could.,hardware,2025-08-12 18:21:40,1
AMD,n8fws7f,Why is it being discontinued?,hardware,2025-08-13 10:01:20,1
AMD,n8j7v7h,Guys would a 5600x be worth upgrading to a 5700x3d? Like what would be a worthy upgrade for it?,hardware,2025-08-13 20:40:17,1
AMD,n9v8xcb,Damn ik heb te lang gewacht met deze CPU te kopen,hardware,2025-08-21 10:10:08,1
AMD,n83w1w2,"> ... lower priced alternative to the 5800X3D, which AMD retired later that year in October 2024...  Surprised that the 5800X3D went away first although I guess in a way it makes sense to set low expectations with the 5700X3D, or it's more consistent to disable cores down than try and use perfect 5800 chips and add the X3D on top as an added risk (or that the market segments don't align, gamers probably don't need that many cores anyway so a 5700 is enough processing grunt).  Wonder how the future phaseouts on the newer 7000 X3D series will go eventually too, although probably not for a few years either.",hardware,2025-08-11 13:29:13,1
AMD,n8409wb,The death of AM4 begins...,hardware,2025-08-11 13:53:09,1
AMD,n83vm61,"Not surprised. In U.K. the 5700X3D will cost you Â£80 more than the 5700X. 5700X is Â£140, 5700X3D Â£220. Now Â£80 for extra cache isnâ€™t a good deal, especially at the entry level where this is aimed. That same money is the cost of 16GB RAM, or a 1TB NVME.   So extra cache, with same overall performance as 5700X, or 5700X, and an extra part? Financially, the latter makes more sense.",hardware,2025-08-11 13:26:44,-1
AMD,n843a96,"Same, I was able to sell my 5600X for like 80 bucks so the whole thing was a net $60 purchase. Going to use this thing until AM6 unless it dies",hardware,2025-08-11 14:08:59,28
AMD,n87jyk2,Well if anyone wants cheap 5700X3D like performance then they'll have to buy Raptor Lake   It's dirt cheap and it's on a dead socket   Unless you can find Zen-4 or Zen-5 at a similar price then Raptor Lake would be the best option,hardware,2025-08-12 00:52:41,7
AMD,nbp2l7s,"Damn. Last autumn I was going through a car project (daily) and totally missed it when it hit it's lowest at around 190 â‚¬. Though I don't know If I had bought it even at that price, but it quickly climbed back to around 220-240 â‚¬, and now in the last stretch it was near 300 â‚¬ with the stock running low. I guess the demand was still there.",hardware,2025-08-31 17:30:01,2
AMD,n8gzhs4,I paid this amount for mine at BF last year it was a good deal,hardware,2025-08-13 14:13:57,1
AMD,n8hum0j,"Same I got mine Black Friday from Amazon for $199.. no special holiday sale. Then watched the price hike up to about $275.. then it settled around $260. Now it's gone?!  Now I just wish I had purchased 2 of them.. I wanted one as soon as they came out but waited, the 5800x3d was a bit more expensive the 5700x3d was perfect for me. But I watched both Amazon and Microcenter run out of stuck on the 5800x3d fairly early.  So this stinks.. I guess I'd have to try and get lucky and find one on eBay if something ever happens to this one.. Hopefully it lasts forever like all my other AMD CPUs lol.  I feel like this is another attempt to try and get us to upgrade to AM5.. I don't want to build an AM5 system yet. Everything I play in 1440p plays great and performs well enough for me w/ my 3070 heh.",hardware,2025-08-13 16:45:03,1
AMD,n8sk1fu,"same, got mine for like 118 or 130 USD back then too.",hardware,2025-08-15 07:20:23,1
AMD,n841dkk,"My 7500f was 97, is better and I can upgrade to 10800x3d :)",hardware,2025-08-11 13:59:01,-16
AMD,n843g1l,mind sharing do I need to reinstall windows?,hardware,2025-08-11 14:09:49,5
AMD,n86pqej,"I got it for 125Â£ on some insane deal and I'm sorted for at least 3 years probably more, it's Rock solid and runs everything so well 120+ fps cpu bound!",hardware,2025-08-11 21:58:00,1
AMD,n8aw69f,"I hate reselling stuff one issue thats user error and they nonstop DM you. ""1 star sold me motherboard that was damaged"" even tho they damaged it. CBA to do that my old stuff goes to my daughter, for troubleshooting issues down the line and reserve hardware incase of failure.",hardware,2025-08-12 15:20:47,1
AMD,n85ed1s,"With my 7900xtx/5700x3d/32 gb 3200 CL16 ram im benchmarking in 1440p pretty close to what AM5 builds are doing in the same games, and that's on a Gen 3 b450-f. This cpu is black magic for AM4",hardware,2025-08-11 17:59:45,22
AMD,n846c6w,"ehh not really, the X3Dness of it means that if you got 3200 CL16 DDR4 you are more or less good, not great and all, but it works well enough with that without spending more on ram.  which is a easy speed to hit with very meh motherboards, hell it would work with some of the older stuff that you don't think is good.  without X3D this is far more true than not, but with it, honestly not a bad idea with older boards and not so blazing ram.",hardware,2025-08-11 14:24:43,22
AMD,n86n2qn,"I got one a couple of months ago to replace a 3700x on X570-E with 32 gb of 3600 CL16. So yeah, I guess there's at least one of us.",hardware,2025-08-11 21:44:01,3
AMD,n8b4bgr,"I got the 5700X3D. By the time I'm needing to upgrade, AM6 will already be out.",hardware,2025-08-12 16:00:54,1
AMD,n84cdy1,"<silently salutes, tear slowly making its way down my cheek>",hardware,2025-08-11 14:55:05,5
AMD,n8666ye,"Sorry, but you're reaching here. No need to beat yourself up, your CPU is not gonna stop being viable suddenly. Yes, we're 3 years from AM6, but as I said, it's gonna keep serving you well for what I can gather; the only reason people but these 3D chips is for gaming, so.",hardware,2025-08-11 20:20:13,13
AMD,n888o1v,"I'm in a similar boat, they were cheap late last year but I didn't really need to upgrade yet so I thought I would wait, now they're $150 more than what they were before. I'm on a 3700X but now I'm unsure if it's worth the upgrade for this price.",hardware,2025-08-12 03:26:11,3
AMD,n8hcyvk,I just started planning to upgrade for a 3600 to the 5700x3d for battlefield 6 because I didnâ€™t want to completely upgrade my mobo and ram yetâ€¦ looks like that just went out the window,hardware,2025-08-13 15:20:14,2
AMD,n8hws7o,"I'm going to be really frustrated if something happens to my 5700x3d, I figured they'd be around a bit longer at least.  Just feels like another maneuver to try and get us to upgrade.. and I am trying to avoid building an AM5 for another year or two. Maybe even all together like I skipped AM3, my AM2+ Phenom9850 just got the job done until it was deemed obsolete by gaming developers lol.  Crap for $200 on Black Friday I should have purchased 2 and kept one safe on the shelf.",hardware,2025-08-13 16:55:33,2
AMD,n8n76sm,"While I don't think that the days of dirt cheap parts will ever be truly over, I don't think it is as likely as it used to be.  Manufacturers have been able to dial in their parts logistics to know exactly how and where to sell things to maximize sales/profit.  The 5700 was likely, if not certainly I forget if it was officially stated, a lesser binned 5800 die and as such was always going to be tied to the quantity of 5800's they made.  And this increased binning process has allowed manufacturers to really to, going back to my original point, know how and where to sell their parts.  If you see a part at a good price these days I think that is the time to buy if you need it.  When you see a part at a dirt cheap price that is likely going to be a much more one off occasion.",hardware,2025-08-14 12:58:32,1
AMD,n8486ww,"5700 is made from mobile Zen3 dies, so it's 10% slower than regular 5700X which is usually best pick for cheap AM4 upgrade.  Getting 5700X3D depends a lot on what GPU you have. Don't overrate one component price, if your whole system costs $1000, you need to take that into account when calculating if better performance is worth additional money for better CPU, not just compare one CPU price versus another.",hardware,2025-08-11 14:34:12,22
AMD,n851l0x,Cost of getting a 7500F or 7600X + motherboard + RAM - (resale value of your AM4 set)  In Canada I can buy a 7600X bundle for $490 or a 5700X3D at scalped pricing on Amazon for $385,hardware,2025-08-11 16:58:55,3
AMD,n8620oi,"Last year in 2024, you can import the 5700X3D for about A$220 (check OzBargin), which is similar to the local price for the 5700   But seems like stock has dried up this year",hardware,2025-08-11 19:59:33,2
AMD,n88rll9,>At what point is X3D not worth it?  depends entirely on your use case. in some tasks X3D can triple performance if you end up with great cache hit rates and were CPU bottlenecked (think online MMOs). In others it may be completely useless.,hardware,2025-08-12 05:55:14,1
AMD,n83u3bo,Aus here  Avoid am4 entirely at this point. Get 12400f and ddr5 if i have to bs honest atp am4 is just way expired,hardware,2025-08-11 13:18:04,-12
AMD,n84734y,"I mean, that is the thing right, it isn't for new builders for a long time now, its for people with older AM4 mobos to upgrade and maybe skip AM5 entirely.  Because X3D only needs 3200 CL16 to work well enough, you don't need a super high end board and the power needs are also not that high vs the 16 core stuff.  so a lot of older boards work well enough with it and it becomes the best drop in upgrade, 5800 or 5700 X3D  Like https://www.youtube.com/watch?v=uo4WxpwHs3s a B350 board and a 5800X3D is a viable coupling and that can make someone just skip AM5 at this point. Hell, because you can't OC the chip or it would burn it self out, I think A320 can do it too...",hardware,2025-08-11 14:28:33,5
AMD,n87lqpx,In stock everywhere in the UK.,hardware,2025-08-12 01:03:24,1
AMD,n8ifrdo,"im trying to pick up a 5700x3d, but the price on amazon in the UK went from 200GBP, to 207GBP, to 210GBP and finally to 220GBP on amazon.  Trying to find a really good deal for under 200 but its not looking good. Might have to call it a day and stick to my i7 9700k or get a 5700x. Hopefully some really good last min deals appear, though im doubtful",hardware,2025-08-13 18:25:02,1
AMD,n84w8qj,"they're honestly designed at people with midrange gpus at 1080 and some 1440, you're not going to see much uplift when a 4090 completely creams 1440 but if you had a 4070 super or something you'd definitely notice it",hardware,2025-08-11 16:32:51,3
AMD,n88jhsz,which mobo and Ram are you using for your 9800x3d. Also 850W or 1000W power supply for a 9800X3D+4090?,hardware,2025-08-12 04:45:35,1
AMD,n84i6l2,"> future 11800x3d/11700x3d discounts  This will only happen if intel can deliver. 9800x3d is still at msrp, and the 78x3d was ~$100 off msrp at this point in its life.",hardware,2025-08-11 15:23:36,5
AMD,n8837ic,"AM4 boards don't evaporate. It is worth getting for a simple cpu swap upgrade right now, today.",hardware,2025-08-12 02:50:06,1
AMD,n840fyh,"It took a while for the 5700x3d stock to be depleted, probably the 5800x3d was premium binned CPUs so the stock wasn't that big to begin with. I haven't seen 5800x3d in retail in a long while.",hardware,2025-08-11 13:54:03,6
AMD,n856j76,People were saying that three years ago. It will rise again.,hardware,2025-08-11 17:22:30,-1
AMD,n83yzit,If you're focused on gaming (which is the whole point of the X3D lineup) then the 5700X3D kind of demolished the 5700X and it's not close. You're talking a jump to 12900K levels of performance in gaming. It's pretty good value if that's your objective.,hardware,2025-08-11 13:45:56,11
AMD,n84zjfh,"I do kind of wish I snagged a 5700x3D for that price, but Iâ€™m content waiting until Nova Lake (depending on if the HEDT-level IO rumors are true or not) or Zen 7 with my 5600. Maybe Iâ€™ll end up regretting it closer to 2027, but it seems to be holding up for now.",hardware,2025-08-11 16:49:06,7
AMD,n8hvbib,I did the same thing.. Bought the 5700x3d for about $200 new off Amazon and sold my 5600x to my neighbor.  I also upgraded me ASUS B350 from 2017 w/ a new ASUS B550-F I found on eBay new sealed in box for $100. I sold the 5600x w/ the B350 to my neighbor for about $130.  So I technically only spent about $170.. whatever money I saved went towards my new Samsung EVO 990 Plus 2TB NVMe and it's probably the fastest drive I've ever owned lol.,hardware,2025-08-13 16:48:26,1
AMD,n846i46,The 7500F would have been a GOATED CPU if only it was released in every region,hardware,2025-08-11 14:25:34,6
AMD,n84gtzc,"Well when it comes to gaming performance, 7500F is 10-12% slower than the 5700X3D at official jedec configuration (RAM). It can gain something by using 6000 MT/s, but at best it would match 5700X3D in non cache sensitive games.  Still, of course for people who don't have AM4 mobo already, 7500F is no-brainer, as buying into AM4 in 2024/2025 makes no sense.",hardware,2025-08-11 15:16:58,5
AMD,n844oyp,In my experience - yes.  When I upgraded to AM5 I at first tried the cloning thing and transferring the SSD and I had horrendous performance problems until I reinstalled the operating system.,hardware,2025-08-11 14:16:18,14
AMD,n847i6o,"If you can't be bothered can always try with old windows install to see if it's fine, for me it was fine last year with lga 1700>AM5, but your mileage may vary.",hardware,2025-08-11 14:30:40,5
AMD,n8hwe58,I will only sell to local buyers 90% of the time.. I make them come to me. If they really want it they'll come fairly quickly.  I'll sell a few things on eBay but I don't really like to lose a % to them.. so I have to hike the price up a little bit. I list the same stuff on FB Marketplace in hopes someone will search there as well and we can both avoid eBay's taxes/fees hehe.,hardware,2025-08-13 16:53:38,1
AMD,n8f6wox,"I've got a similar build with a 7900 GRE instead, running a 34"" Ultrawide at 1440x1440.  Yah, after the CPU/GPU upgrade I'm content staying here until AM6 comes.  I've got a huge backlog and the only games on the horizon I care about is BF6 and the next Jedi Survivor/Fallen order sequel.  The latter probably won't come out before AM6, anways.",hardware,2025-08-13 05:49:44,1
AMD,n86cizc,"They said   > ton of ram  Not fast RAM. IE, if someone has 96/128GB of RAM in their AM4 system for productivity programs. 128GB of DDR5 isn't exactly ""cheap,"" even now. It would add a lot of money to the cost of upgrading from a working AM4 platform to AM5.",hardware,2025-08-11 20:50:33,10
AMD,n87ennk,"I waited for AM5, but ultimately saved over $1000 upgrading things I put on this AM4 socket ASUS TUF Gaming X570.  Chip arrives tomorrow.",hardware,2025-08-12 00:20:51,1
AMD,n86bd8i,"The games I play specifically benefit from x3d, mainly esports titles. So yeah, it was a miss on my part to not upgrade for cheap when I had the chance.",hardware,2025-08-11 20:44:59,2
AMD,n8hxhqm,"I can't even find a 5700x3d anymore, unless it's on eBay and I'm afraid to look at the prices people are going to want now.   Seen a few on eBay just now for $250'sh which is kind of where Amazon had them last priced, think I saw $260 last time.. it was up to $280 for a minute.  I'll just cross my fingers it lasts me forever like every one of my other AMD CPUs.. freaking Thunderbird/Athlons/Phenoms all probably still run but I retired most of them.",hardware,2025-08-13 16:58:59,1
AMD,n8hxnfn,Check eBay before the weekend you might find one for $250'sh.. Before the sellers realize and hike them up to about $400.  Already seen some listed on NewEgg for $400.. absolutely ridiculous.,hardware,2025-08-13 16:59:44,1
AMD,n8hwyz6,"Yeah, I was planning to dodge am5 entirely. We'll see if I can last another few years lol",hardware,2025-08-13 16:56:28,1
AMD,n84nw2w,"Yeah, IIRC the 5600 will beat the 5700 in most games because of this. The mobile/monolithic Zen 3 CPUs have a lot less cache.",hardware,2025-08-11 15:51:39,10
AMD,n84a8e0,Didn't know that. At least 5700x isn't too far off the price.,hardware,2025-08-11 14:44:31,5
AMD,n8521mj,"the only time its worth it is if you are on AM4 on something like a 1700/X or 1600/X that gamed at 1080 and now wants to do so at 1440p or even 4k.  then you can drop the money on a 5700X3D, 32 GBs of 3200 ram and then a nice 9070/XT or 5070/ti and call it a day till at least AM6 if not midway thru AM6 when you would consider replacing those GPUs after 2 gens.  otherwise, if you really want HFR 1080 you need to go up, for 4k ultra the difference of 5700X3D vs 9800X3D is tiny lol, and 1440p it would be worse but not that much worse. esp if your upgrade / total budget is 1k",hardware,2025-08-11 17:01:06,3
AMD,n88268u,"Damn, wish I had done that. They're like $429 AUD locally  https://www.scorptec.com.au/product/cpu/amd-socket-am4/108156-100-100001503wof  Don't mind supporting local businesses, but not when it's 2x more",hardware,2025-08-12 02:43:29,1
AMD,n83ugl8,"Unless you're already on the AM4 platform and just looking to get a bit of a performance uplift rather than doing a total rebuild. I'm waiting another generation before I do that, trying to get my value for dollar out of this build lol.",hardware,2025-08-11 13:20:09,27
AMD,n83zjlg,12400f is also on a dead plateform and slower than a 5700x.,hardware,2025-08-11 13:49:06,21
AMD,n842jcl,"Was a good buy last year. No way I'd buy my 5700x this year though, I also needed to upgrade rather than want.    Now I'm waiting for that sweet 2md hand 5800x3d",hardware,2025-08-11 14:05:05,1
AMD,n881dzd,"The article is referring to US. But it also has been discontinued worldwide, so the only remaining inventory is the stuff still on the shelves. There will be no more new restocks once that runs out.",hardware,2025-08-12 02:38:30,1
AMD,nayvh78,400 on Amazon now RIP,hardware,2025-08-27 15:58:46,1
AMD,n85dloh,"Yeah I figured, it was more that my older asus tuf wifi mobo was limited, I couldn't even use the PCIE for an expansion slot for m.2 because the gpu blocked all but the bottom gen 1 slot.  I use a lot of AI applications though, so I'm hoping that the 9800x3D has some uplift there, and I assume it will in music production as well.",hardware,2025-08-11 17:56:05,1
AMD,n88pby4,GIGABYTE X870 AORUS Elite WIFI7 ICE and I have a 1000W plat PSU carried over from my last build  I think 850 watts SHOULD be ok but I figure that that price you can usually get a solid 1000w anyway and just future proof the one component that actually DOES last 10+ years  Ram is Patriot Viper Venom RGB DDR5 RAM 64GB (2X32GB) 6000MHz CL30 1.35v,hardware,2025-08-12 05:34:48,1
AMD,n85fn2f,"9800x3d was selling at $400, 7800x3d at $300 at microcenter btw.",hardware,2025-08-11 18:06:00,0
AMD,n88kc7d,Absolutely not. Sell the motherboard and ram and upgrade to a 7600. You can get an am5 bundle around 300-400 usd rn. There is no reason to spend 290 on a goddamn 5700x3d,hardware,2025-08-12 04:52:23,0
AMD,n840zfn,"Well uh yeah, October 2024 was a while ago ðŸ˜… practically 10 months at this point.  And exactly yeah, binning strats is basically what I was getting at.",hardware,2025-08-11 13:56:58,1
AMD,n857wdy,With a global 5400x3d release,hardware,2025-08-11 17:28:56,3
AMD,n852etk,"Yeah ultimately my 5600x was fine, I looked up my receipt and I actually got to he x3D for $123 lmao  I hope Intel can turn the ship around, hell, not even that, just stop the bleeding. While I love what AMD is doing if they become the dominant player in the CPU space we're going to be in for a rough time.",hardware,2025-08-11 17:02:51,8
AMD,n846sda,AliExpress same as op,hardware,2025-08-11 14:27:01,0
AMD,n8abhgx,I honestly feel like it could be a decent choice to buy  a new am4 system with a 5700x3d as long as you don't plan to upgrade in the next few years.   That way you could just completely skip am5 and buy an am6 system in like 4 years or so with a zen 7 cpu.,hardware,2025-08-12 13:32:58,1
AMD,n84icem,7600 matches 5800x3d is most reviews i've seen. I use m-die 6200 cl28.,hardware,2025-08-11 15:24:23,-3
AMD,n88rf08,"I had no issues with windows 10 booting after motherboard/CPU/RAM replacement on any of the machines i did it. Windows just thought a bit longer on first boot and seems to have set itself up for the new system, then just worked as usual. And i had everything ready for reinstall first time too, but didnt need it. I havent tried that on windows 11 year, maybe its different.",hardware,2025-08-12 05:53:36,5
AMD,n8hyhby,"Same.. Heck I've still got my 1600x running on an old A320M next to this system.  AM4 is like my new AM2.. as long as it works I'm sticking with it.  It literally took game developers deeming my Phenom9850 obsolete when No Mans Sky and RE: Biohazard got release in November 2017 for me to finally upgrade haha.  I got a good deal on my 1st AM4 (B350/1600/16GB) for about $300 or less, just re-used my tower, PSU, GPU, and HDDs until I could eventually upgrade all of those as well.  Now I'm on my 2nd AM4.. I tried a 3600x and a 5600x.. wasn't feeling the 3600x but the 5600x was nice, I just really wanted the x3d.",hardware,2025-08-13 17:03:46,1
AMD,n88rpij,"yeah, if you are building a new system now dont use AM4 anymore. if you are on AM4 and want a replacement CPU its another matter.",hardware,2025-08-12 05:56:11,1
AMD,n8mbuz4,"Uh  When DDR4 prices match ddr5, why AM4?",hardware,2025-08-14 09:02:19,1
AMD,n840w32,Its a bit cheaper than the 5700 however.,hardware,2025-08-11 13:56:29,1
AMD,nb1jeo4,"i just checked after i saw this comment and my jaw fucking dropped  i ended up upgrading to am5, i got an insane deal, spent at max likeee 350-400, got a 9600x, b650, 6000mhz ddr5 cl 30 ram and OH MY GOD its the greatest decision ive ever made.  i cant believe i was literally 1 click away from buying 5700x3d, glad i didnt",hardware,2025-08-27 23:47:07,1
AMD,n865bi0,dont worry the 5800x3d is selling from 300-450 on ebay actual insanity...i should sell mine not using it!,hardware,2025-08-11 20:16:02,1
AMD,n841oy9,Too bad i didn't get a cheap 5700x3d before upgrading to a 5600. When i looked to upgrade in february the 5700x3d was a lot more expensive than getting a 7600 in my country lmao.,hardware,2025-08-11 14:00:40,1
AMD,n853u48,"I donâ€™t think Intelâ€™s position in the market is *that* dire (yet). Certainly a hell of a lot better than AMD during the Piledriver and Excavator days. As long as Nova Lake isnâ€™t too expensive, and has differentiating factors besides raw performance (like the IO rumors), then I think theyâ€™ll be able to find a place in the enthusiast market. I bought first gen Ryzen, which had a similar ethos, and I could see myself going that direction again.",hardware,2025-08-11 17:09:38,6
AMD,n8hvnj0,"I really liked my 5600x but I debated over and over towards getting a 5700x3d and eventually did. I do not regret it considering all I do is game (1440p).  The few games like Ubisoft's titles (Jedi Survivor) and others that gave me any slight stutters remedied themselves, quite a few games I played w/ RT enabled also saw some improvement.. nothing crazy but noticeable.",hardware,2025-08-13 16:50:03,1
AMD,n88r5tq,"aliexpress? so 10% chance of receiving a CPU you bought, 20% chance of recieving a different CPU and 70% chance of receiving a rock?",hardware,2025-08-12 05:51:17,-2
AMD,n85eaxw,"For most games, it's similar, but not the same. The 3D cache makes extreme differences in certain games. Tarkov and Guild Wars 2, I can think of off the top of my head.",hardware,2025-08-11 17:59:28,2
AMD,n84pz3k,"From the very latest CPU demanding games and what I've seen it was like \~5-8% better for 5800X3D, but against a 7500F with 5200 MT/s RAM. If you're using 6200 CL28, than it certainly at least matches or even overcome 5800X3D.",hardware,2025-08-11 16:01:57,1
AMD,n8mce3u,"I said 'if you're ***already on*** the AM4 platform', meaning someone who already has something like a 3900X, doesn't want to do a complete rebuild right now, but needs a performance uplift. The 5700X3D is a drop-in replacement that will give them a big uplift.",hardware,2025-08-14 09:07:32,1
AMD,n845t2j,"Also if you can somehow find an ECLK board like B760M riptide for cheap, easy 5Ghz+ OC. Yea SA voltage is still locked so not gonna oc ram as easily as an alder lake K-chip, but still above 6000 at least.",hardware,2025-08-11 14:22:00,2
AMD,n877yyf,yeah wtf. Sell that shit and get your free upgrade.,hardware,2025-08-11 23:41:42,1
AMD,n85wcyj,"Intel only loses to AMD in retail market that is less than 10% of desktop market and less than 5 % of PCs overall. Intel comfortably keeps grabbing majority of sales even now, as evidenced by their revenue. AMD still has less than 30 % of the market. There's really no reason to feel bad about their processor division.  The problem is just their fabs. They have to turn them into a foundry or they have to ditch them somehow. But you can pretty much think of that as a separate business they have on top of being AMD competitor. Sure, it weighs them down but they own the x86 market anyway, it's a mistaken view that it's now AMD that has upper hand.",hardware,2025-08-11 19:30:36,10
AMD,n85786h,"For sure - I'm looking more at Intel laying off a ton of folks and the board already turning on the new CEO   On the other hand, too big to fail? In the US at least, national security consideration might keep them afloat",hardware,2025-08-11 17:25:47,1
AMD,n8hzw54,"The best thing about it, IMO, is the 1% lows - they're much better so the random stutter is way less noticeable than it used to be",hardware,2025-08-13 17:10:38,3
AMD,n88ybod,Yes AliExpress the site where you can pay after receiving the package.,hardware,2025-08-12 06:57:27,1
AMD,n8afhqc,"That's not true. If you buy from verified sellers with a lot of reviews and EU warehouse stock, there is almost zero chance to receive a fake unit.",hardware,2025-08-12 13:54:58,1
AMD,n8dbeph,I bought  2  AMD CPUs at discount prices form aliexpress and received what i ordered.,hardware,2025-08-12 22:27:10,1
AMD,n8avkdw,Looking at you boeing.,hardware,2025-08-12 15:17:48,1
AMD,n9phnxz,"What was the difference anyway ? Isn't a ""new"" Promontory chipset just relabel of the ""old"" one ?  All of the differences I know of are in the PCB, not chipset.",hardware,2025-08-20 13:20:02,72
AMD,n9pkc0x,B650 dies earlier than B550 lol,hardware,2025-08-20 13:34:21,31
AMD,n9q5x8z,"Kinda glad to get rid of minor options and variations.  Makes it too hard to compare MBs for all the slight GPU, M.2, and USB connectivity options.",hardware,2025-08-20 15:22:00,10
AMD,n9qrd3e,I think the industry is ready to â€œforceâ€ PCIe gen5 on more and more motherboards such that future GPUs donâ€™t have to be designed with Gen4/3 connectivity in mind.,hardware,2025-08-20 17:03:54,6
AMD,n9pg5py,"Hello NGGKroze! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-08-20 13:11:45,1
AMD,n9zr8ym,"So what's going to be the new ""good"" budget board now?",hardware,2025-08-22 00:43:34,1
AMD,n9pq641,B650s are dogshit value compared to the jump in feature set on the B850 of like segment for like... 20 CAD more generally? And the good value B650es are mostly sold off anyway. No skin off a wise builder's nose.,hardware,2025-08-20 14:04:42,-16
AMD,n9rt14z,does this mean no cpu drivers anymore for these boards ?,hardware,2025-08-20 20:05:29,0
AMD,n9pjc5s,Itâ€™s the same. AMD just mandated PCIe 5 and USB4 on X870(E) and B850 boards compared to the 600 boards.,hardware,2025-08-20 13:29:03,55
AMD,n9plmx5,"Yes and no.  Promontory 21 (A620, B650, B650E, B840, B850, X870) are identical silicon, but are not the same as the AM4 chipsets with the same name.",hardware,2025-08-20 13:41:15,9
AMD,n9pmtwt,"No, the chips physically have a different model #     [https://pbs.twimg.com/media/GrJ43ECXoAA96Sa?format=jpg&name=large](https://pbs.twimg.com/media/GrJ43ECXoAA96Sa?format=jpg&name=large)",hardware,2025-08-20 13:47:31,9
AMD,n9x9116,"As an owner of a b650 board and have built multiple machines for people with them, good. So many of these boards were fucking lemons, I genuinely regret suggesting these.",hardware,2025-08-21 16:57:58,3
AMD,n9x98a1,"You have to have gen 4 and 3 compatibility anyway, that is how pcie works.",hardware,2025-08-21 16:58:56,9
AMD,n9ssbak,"Considering we are still getting new chipset drivers for AM4 boards, you shouldn't worry about it right now.",hardware,2025-08-20 23:09:32,5
AMD,n9pl53y,"Yeah it seems like the right move. When Iâ€™m building a new pc I want all the newest connectivity. Thats why I sprung for an 870. I also wanted my pc Russian roulette mode on, thatâ€™s why I chose Asrock.",hardware,2025-08-20 13:38:39,57
AMD,n9puhe7,So the changes are trivial for AMD.  Not much more than a label change.,hardware,2025-08-20 14:26:19,7
AMD,n9q9l65,B840 uses PROM19 (B550).,hardware,2025-08-20 15:39:48,5
AMD,n9r93oz,"It's likely all the PROM21 chips are identical when manufactured, but AMD then blows fuses to set the PCIe ID for each model and prints a different code on the top. That's very common in the semiconductor industry.  There's no sense in building multiple chips that have the same functionality, that would just drive up costs needlessly.",hardware,2025-08-20 18:27:32,9
AMD,na3fasr,What's so bad about the b650 board series? I was thinking of getting one once they go cheaper over time as an upgrade for my Asus B550 A-AC Prime in 2027 (bought in 2022).,hardware,2025-08-22 16:02:37,2
AMD,n9ptspr,"AMD pulled a quick one, just upselling B650E as X870 and resetting prices.Â  On B650E and X670E, PCIe slots where already 5.0. USB4 is not implemented natively, so it reduces the amount of freely available fast PCIe lanes. You could add USB4 on 600 boards via a PCIe add-on card.Â  B850 just adds mandatory PCIe 5.0 for the primary M.2 slot. No mandatory 5.0 for the PCIe x16 slot or USB4. Basically just AMD resetting prices on the mainboards and upselling one chipset.",hardware,2025-08-20 14:22:57,33
AMD,n9q6z5s,"I don't know if the connectivity part is sarcastic, but I personally think it's a complete waste of PCIe lanes. We only have 28 lanes in total. On a typical board they'll be allocated like this:   4 of them goes to the USB controller, 4 goes to the chipset and the remaining 16 + 4 goes to the first PCIe slot and M.2 slot.  Almost all the X870 boards I looked at allocated a pitiful 1x to each of the PCIe slots which is practically useless for any kind of expansion. Only MSI seemed to be willing to allocate 4x to one of the slots.   If USB 4 was optional those 4 lanes could have been used for one of the PCIe slots where the user could then install a USB 4 addin card if they needed that, and if not, use the slot for something else.  Of course if USB 4 was super popular and something you could expect most users to need, then the story would be different, but as it is now I just don't see much of a reason for users to use all that bandwidth from a USB port on a desktop PC.",hardware,2025-08-20 15:27:07,10
AMD,n9psa27,"Like, after those German current gen GPU benchmarks under RAM scarcity and the current GPU market... there's no excuse for not dropping the cash for something with fully enabled PCIE 5.0, it may well squeeze significant extra useful life out of a GPU and help its successor.  Especially for when in Canada you can get a B850 for like only 20ish bucks more more then a like B650, renders the last gen pointless.",hardware,2025-08-20 14:15:19,3
AMD,n9rt7v0,Looks like you worked out all the angles,hardware,2025-08-20 20:06:25,1
AMD,n9ryg4r,I've always wanted the latest connectivity with a new motherboard or GPU purchase.,hardware,2025-08-20 20:31:55,1
AMD,n9szhhh,yeah all the separate chipsets are likely a byproduct of binning and silicon lottery. its a pretty cost efficient way of making multiple product lines.,hardware,2025-08-20 23:50:57,2
AMD,na6o65w,"So the asus b650 boards were genuine crap. But i suspect app b650 boards have issues. The asus boards run the voltages on the x3d cpus way too high. And while amd and asus claimed to have ""fixed"" the issue i still notice from time to time my 7950x3d's vsoc is going over what amd recommends. Even disabling expo does not consistently fix this issue. I have to reset the bios and then hand tune the vsoc voltage. This is also true for my friends with asus b650 boards (no samples of x670 they were way too expensive). The memory stability of these boards is total crap, I get random crashes and have replaced everything but the cpu and motherboard because it is a ton of work. I have to disable expo and even then the pc is still not stable. And the stability issue is more widespread among my friends and not just asus owners. These boards are fucking cursed.",hardware,2025-08-23 02:41:55,1
AMD,n9qucoq,Iâ€™m looking at a B850 board to replace my X870 AsRock partly because of better balanced PCIe speeds,hardware,2025-08-20 17:17:51,2
AMD,n9y1987,"Are there any benchmarks of this? The dedicated USB4 seems absurd to me, it kind of reeks of some theoretical future proofing that probably will not turn out to be needed, while having full capacity for SSD seems more practical now. But I guess if it turns out it hardly matters for SSD performance anyway, I may be persuaded.",hardware,2025-08-21 19:11:50,1
AMD,nakhyxf,"id rather we get more m.2 and SATA lanes, none of that nonsense of installing m.2 disables SATA ports because they share  the same lanes. I currently have 7 storage devices connected, i need those connections on.",hardware,2025-08-25 11:56:25,1
AMD,n9py99c,luckily my Asus B650m-e which is only spec'd as pcie 4.0 on the product page is actually 5.0 :),hardware,2025-08-20 14:44:56,2
AMD,n9wlbs2,"Other reason is you don't want your 9800x3d to melt? ;)  If you don't care for USB4 and want more 16x slots that are more than just 1x, msi x670e tomahawk has 16x(gen5) plus x4 & x2 gen4. I plan to re-use my gaming rig build for my nas in 4-5 years, need gpu/hba/10gbNIC so that was the only board that ticked all the boxes.  Using those gen4 x16 slots will disable some of the nvme slots tho.",hardware,2025-08-21 15:04:45,1
AMD,n9y6ly8,"Benchmarks of what? 4x PCIe 5 gives about 126 Gigabits of bandwidth. USB ports can go up to 40 Gigabit. If I look at the rear ports on my motherboard I have:  * 1x 40G * 1x 20G * 2x 10G * 2x 5G  That's 75 gigabit in total. So in theory, if you expect all those ports to be fully utilized you really do need to allocate 4x PCIe 5 lanes. Like I said before, I completely agree that it's a waste and I'd rather have those lanes be freely available, but if they think high bandwidth USB is so important then there's not really a way around this.  I'm just thankful MSI managed to make a board suitable for my setup (with that 4x slot from the chipset). I have a 25G NIC that I wouldn't really have been able to utilize if not for that slot.",hardware,2025-08-21 19:37:45,1
AMD,n9y91pt,"Yeah, I get that you don't want networking to in any way be theoretically hampered before it possibly could hit any other bottlenecks.  I'm on a X670E board which has 2 M2 slots with full capacity, but I don't really know if that is needed for any regular workload across those drives. So I'm curious if any workloads *other* than maxed sequential loads (like lots of connections for networking, or random r/w for SSD's) have reduced performance in a shared scenario or is PCIE completely agnostic and has full duplex or whatever you would call it?",hardware,2025-08-21 19:49:29,1
AMD,nb8jouv,I'd personally suggest the 9060xt.  The 9000 series are spectacular tbh...,buildapc,2025-08-29 01:08:31,12
AMD,nb8khvg,If it's the 16gb 9060 xt I'd say the upgrade is worth it,buildapc,2025-08-29 01:13:15,1
AMD,nb8tz26,If its the 16gb 9060xt then absolutely that. Thats the one i just got for my first pc,buildapc,2025-08-29 02:11:04,1
AMD,nb8z9g2,"It honestly depends on what your needs are... Fps, solo campaign, rpg, streaming, vid rec., etc ... Do u have an interest in A.I. or M.L.? RX is amazing for raw frames n streaming. ARC is amazing for AI n ML so it depends on what your use cases are",buildapc,2025-08-29 02:43:36,1
AMD,nb9it9e,9060 XT and get the 16GB version.,buildapc,2025-08-29 05:05:01,1
AMD,nb8jzrh,is the current price still good with it?,buildapc,2025-08-29 01:10:18,1
AMD,nb8lgjv,the pricing would be my real question as it seems to be too much for both,buildapc,2025-08-29 01:19:03,1
AMD,nb8kctz,"they're both overpriced, is this USD?",buildapc,2025-08-29 01:12:27,2
AMD,nb8kk4l,Nah. MSRP is 350. +/-50 is what I'd accept. 100+ more is crazy. That's 30% upcharge.,buildapc,2025-08-29 01:13:38,1
AMD,nb8ohbu,What country are you in? It would depend on local pricing and what alternatives are priced at. Sometimes countries are just expensive unfortunately,buildapc,2025-08-29 01:37:21,2
AMD,nb8kjao,usd equivalent of php,buildapc,2025-08-29 01:13:30,1
AMD,nb8lbsr,where can we get msrp ones im from the philippines.,buildapc,2025-08-29 01:18:15,1
AMD,nb8l2bd,"if I had to overpay for something, it'd be amd over Intel.  Especially because their new CEO has been threatening to discontinue their GPUs",buildapc,2025-08-29 01:16:41,5
AMD,nb8lyby,I see. Are the 9060 xt around those price?,buildapc,2025-08-29 01:22:02,1
AMD,nb8lddy,thanks for that info ill keep that one in mind,buildapc,2025-08-29 01:18:31,1
AMD,nb8m7z8,on stores here yeah around those price,buildapc,2025-08-29 01:23:36,1
AMD,nb8nu3n,I see. Then I guess it's fine to buy it. It's a much better card that the b580 by 25%.,buildapc,2025-08-29 01:33:25,2
AMD,nbmgs8d,"Intel CPUs of that generation have a question mark over their reliability, specifically possible thermal degradation over time.   Therefore, these Intel chips are so cheap because shops are desperately trying to move their stock. This will impact their resale value in the second-hand market as well so you'll need to keep that factor in mind as well.   I'd probably choose the more expensive Ryzen option for the peace-of-mind, upgradability and future resale value.",buildapc,2025-08-31 06:58:08,6
AMD,nbmjv60,# 7600X3D,buildapc,2025-08-31 07:27:25,8
AMD,nbmvdwp,"I think the 7600x3d would warrant the price for the performance alone, then again because Intel will probably require you to upgrade the motherboard if you ever want to in the next generation, but is there a reason why you want the 7600x3d specifically?    If the 7600x3d isn't significantly cheaper for you and you're willing to pay the extra money, you might be better off considering the 7800x3d since it can be found for only $30 more",buildapc,2025-08-31 09:20:31,2
AMD,nbnevjg,"Iâ€™d recommend a better motherboard if you go with the 14600K, the one you have picked has an incredibly weak VRM setup.",buildapc,2025-08-31 12:12:00,2
AMD,nbock19,"I had the 14th gen K chips and replaced it with a 7600x3d recently (kids and wife are using a 14900k, lol). Performance in gaming is about the same at 1440p and above. The other stuff I do, Intel is better and the 14600k is a screaming deal. Iâ€™d take the savings and get a better gpu.",buildapc,2025-08-31 15:21:22,2
AMD,nbmnvdz,"7600X3D is slightly better in gaming, while 14600K wins in productivity by a lot.",buildapc,2025-08-31 08:06:40,3
AMD,nbmept2,[https://www.youtube.com/shorts/B9OmKHTqBMQ](https://www.youtube.com/shorts/B9OmKHTqBMQ)  this video sums it up perfectly imo  7600x3d = better upgradability but the 14600k can be a great option still,buildapc,2025-08-31 06:38:51,1
AMD,nbn775h,"If youâ€™re just gaming, 7600x3d. If youâ€™re doing productivity stuff, 14600k",buildapc,2025-08-31 11:12:18,1
AMD,nbminlz,"The 14600k is a beast of a chip. The only downside that it can be a little hot, but your AiO can handle it easily. You probably will not have to upgrade your chip in the next 4-5 years or so at 1440p (GPU will be the thing you'll need to upgrade first). So don't worry about ""upgrade path"", or resale value.   At that price difference, I'd get the 14600k. AMD chips are good, but they've been a little bit overpriced right now (following the same footsteps of Intel in the past).  Yes. Downvote me, AMD fanboys. You guys are ignoring the fact that AMD has become a luxury brand, and are suggesting something almost 2x the price for very similar performance (or even worse in normal usage).",buildapc,2025-08-31 07:15:48,-4
AMD,nbmtt2z,Intel by far...,buildapc,2025-08-31 09:04:45,-6
AMD,nbnj6in,The 7800x3d is about â‚¬50 more expensive. But I think that one is a bit overkill for the rx9060xt.,buildapc,2025-08-31 12:41:14,2
AMD,nbnypgo,"Yes, if going with an dead socket, live a little and get a nice motherboard at a discount. Deals are out there. Though not everyone likes used, so YMMV with new LGA 1700",buildapc,2025-08-31 14:12:10,1
AMD,nbohv1b,That is a interesting point. I could upgrade the 9060xt to a rx5070 for about the same money,buildapc,2025-08-31 15:48:06,1
AMD,nbnjkpj,But is it â‚¬130 more better than the Intel one?,buildapc,2025-08-31 12:43:51,1
AMD,nbnrmpn,"Yeah.. not sure why you're getting mass downvoted.  The 14600K is only $150 right now via Newegg and comes with BF6 for ""free"".  **Effectively $80 for the CPU itself assuming you were planning on buying the game at launch.**  Considering he's playing on 1440p, the FPS difference won't really be super high unless its some CPU bound esports game or ms flight sim.  This is obvious due to cache.  People might argue AM5 upgradability, but this only makes sense if you're planning on keeping a platform for like 5+ years.  Selling off the mobo and CPU every 2-3 years is more logical IMO.  The 14600K performs a little better than the current AMD 8 core single CCD's ATM for MT related stuff.  I wouldn't exactly look at this like a 6 core CPU.. It's generally competitive with a 12900K due to clock/ring speed differences, minus some MT performance being left off the table.  There's nothing wrong with AMD, but if OP is planning on buying BF6.. it's kind of a no brainer to go 14600K with a half decent board.   He should be able to net min $200\~ in the next few years when he wants to upgrade.  Power draw in most games is actually within 40W of each other when left stock... Only outlier is likely battlefield where the game will leverage a full CPU and draw like 150W max.. but it's really only this game engine.  I'll go against the grain a little.. I don't think this CPU runs hot.. at least relative to the 8+8 alder/raptor configs.. which tend to hit 100C pretty easily.  Edit: just realized hes in Europe. Changes some aspects, but it will generally come down to price still.",buildapc,2025-08-31 13:32:49,2
AMD,nbny5tu,"Yeah, this sub hates Intel lol. I have used AMD for the last 5 years, but there is a certain price where getting 5 years on a 14600k is worth it. $100 after selling BF6 is pretty damn close to that price!   I also love how everyone parrots ""spend more - who cares how much, because you can upgrade on the same socket and save a few dollars!"" Like bro, save the money up front instead, get a great performer now, and just upgrade when you need it. It comes out to similar cost in the end, since everything will be cheaper (ie cost / frame) in 5 years. If you need to upgrade sooner than that, you probably made compromises in your initial CPU choice, or you are an enthusiast and wouldn't be making this post.Â    As I prepare for the downvotes, I will add I feel good about 14600k, but not 14700k or higher. At that point I would just get AMD. I might go AMD anyways, but for anyone who wants to save money up front or wants the best deal, 14600k rips",buildapc,2025-08-31 14:09:16,2
AMD,nbmxeb7,100% 14900kf is cheaper than 9800x3d rn.,buildapc,2025-08-31 09:40:44,-1
AMD,nbo0zts,You might find this video interesting â€¦ compares a lot of CPUs performance with the 9060 XT.  https://youtu.be/NqRTVzk2PXs?si=bbn7bpSd1jHb9Bgz,buildapc,2025-08-31 14:23:51,3
AMD,nbo07bz,For 50bucks it is well worth it to upgrade.,buildapc,2025-08-31 14:19:46,2
AMD,nbqcceh,"I'm actually surprised you saw the 7600x3d for a good price because that was a limited-run CPU with limited stock.    I think you can go as low as the AMD Ryzen 5 7600x and it would be JUST good enough for the 9060xt if you're on a strict budget. You should be able to find the Ryzen 5 9600x for about the same price and that should beat the Intel i5-14600k in most games, assuming you won't need the multi-thread performance for anything like video editing or rendering",buildapc,2025-08-31 21:23:03,1
AMD,nbqf99m,"I'd recommend going with AMD just so you aren't hard locked into the Intel upgrade path, and then because AMD uses much less power and takes less investment to keep cool.    I think both the 7600x3d and the 7800x3d would give you all the performance you want and then some for the future, but if you don't want to pay extra and still want better gaming-specific performance there's still the 9600x",buildapc,2025-08-31 21:39:03,1
AMD,nbojtmn,Unfortunatly second hand mobo in the EU market is not as big as in the US. What micro-atx board would you recommend?,buildapc,2025-08-31 15:57:54,1
AMD,nbnk1wo,That's a subjective thing to say. There's plenty benchmarks on youtube pitting both CPUs against each other. Just check their comparative performance at the resolution applicable to you and make a decision :),buildapc,2025-08-31 12:47:01,3
AMD,nbojdxk,"The Intel was at the moment of writing about â‚¬130. Now back to â‚¬145. Still good. (no free bf6 sadly). My last few pc's i've kept about 8 years, that has been about the point when there are a few titels that get me to upgrade. This time it was bf6 with their stupid anti cheat haha...  An other redditor had the idea to put the â‚¬130 in a better gpu instead of a cpu, that would be a upgrade to a rx5070.",buildapc,2025-08-31 15:55:42,2
AMD,nbqzswm,"Because in the last few years, people have become AMD fanboys (with good reasons). They have forsaken all logic and just immediately go ""intel sucks, buy AMD"".  Intel has been reducing their pricing by a lot and many of their chips, especially in the budget range, are now way better choices than AMD counterparts.",buildapc,2025-08-31 23:40:31,2
AMD,nbuf7d7,"Where I live the 265K costs literally 30% less than a 9700X and people here are still recommending AMD cuz ""dead socket"" as if anyone is gonna need to upgrade a 265K for 1440p gaming before AM6 comes out.",buildapc,2025-09-01 14:41:31,1
AMD,nboevcf,Interesting video! Thanks for sharing!,buildapc,2025-08-31 15:33:06,3
AMD,nbsnhvs,The 9600x is availeble for â‚¬196. 7600x3d â‚¬273 14600k â‚¬139,buildapc,2025-09-01 06:37:29,1
AMD,nbol6tg,"Can you check eBay? I'm not sure what's available to you, but get something that has a little more than what you need for features, and probably get something that you think looks nice and has a built in io shield. Usually those models have better VRMs. If you don't know what that is, just get something that looks heavy haha.Â    If you can't find a good motherboard at a good price, don't bother with 14600k and go AM5.",buildapc,2025-08-31 16:04:45,1
AMD,nbome4r,"Ah yeah.. that's a tad more competitive and I can see both CPU's being valid in this case.  Like I said, unless a game is super CPU bound, they're competitive fps wise give or take up to \~40w in most games.  I would view it this way.. If you play competitive games (low res, optimal settings) and want a long term platform, then maybe AMD makes more sense here.  I view the 14600K as the better ""all arounder"" atm (better productivity/streaming), but it does lack in specific situations.",buildapc,2025-08-31 16:10:42,2
AMD,nbqzygo,"Yep, a gpu upgrade will do you much more than cpu (and the 7600x3d is not even better than the 14600k in most games at 1440p)",buildapc,2025-08-31 23:41:28,1
AMD,nbuvnsg,"The only argument that's valid to me is esports gamers. There are situations where running an    X3D just nets much higher FPS, especially on low res.     1440P and high graphics? More than not, these two CPU's are quite close... bar the intel chip running slightly more wattage at lower res.     I've been building PC's since 2002ish and socket longevity never really mattered. People would always hover to the price to performance option on the latest release.",buildapc,2025-09-01 16:02:23,1
AMD,nbsqxmh,"The price range is between us is bigger than i thought. The best price for the 14600k shows as $165 for me, while the 9600x is $189, but I could've sworn I saw it recently for $10 or $20 less   I was actually trying to watch the 7600x3d to see if it ever comes back in stock and I gave up when I saw $450 when the 7800x3d dipped close to $300, but the 7600x3d is now showing as $310 for me. I wanted to help and even made another list to make sure it stays in your price range, but the range is too much ðŸ˜­",buildapc,2025-09-01 07:08:38,1
AMD,nbssgrq,Could you share the list you made? With conversion a lot of the prices get pretty close. I could look up the build you made and see what it would have cost here.   14600k: $165 = â‚¬140 - eu price â‚¬139 9600x: $189 = â‚¬161 - eu price â‚¬196 7800x3d: $410 = â‚¬383 - eu price â‚¬316 7600x3d: $310 = â‚¬264 - eu price â‚¬273,buildapc,2025-09-01 07:22:52,1
AMD,nbssr70,Give me a min. I just realized the app version doesn't have a share function and I gotta remake it in the website list,buildapc,2025-09-01 07:25:34,1
AMD,nbsu6vd,"In case you don't see the end, I forgot to save the list earlier and quickly put that together.    The original was $100 more than yours, and I don't know why this one is $60 more than the other one   Edit: Think I found it. The app looks to be using outdated or inaccurate prices",buildapc,2025-09-01 07:39:10,1
AMD,nbkx19b,"Skip the 4060 and 5060, slow core performance and only 8GB of VRAM. A 9060XT 16GB is 360-400 euro, much better value.  https://de.pcpartpicker.com/products/video-card/#c=596&P=17179869184,51539607552&sort=price&page=1  PS, RTX 4000 cards support DLSS 4. The only thing you lose is multi-frame gen, which has little value.",buildapc,2025-08-31 00:11:44,8
AMD,nbkx79o,I ran a 5700X and a 4070 for a couple of years and it ran perfectly fine.  What is your budget for your GPU?,buildapc,2025-08-31 00:12:47,2
AMD,nbkxhue,5700X is still a good CPU for the level of card you want.  Get a 5060Ti 16GB or 9060XT 16GB though.,buildapc,2025-08-31 00:14:40,2
AMD,nbkxtqb,"If you're playing at 1440p and mostly GPU-intensive games, the CPU won't be a bottleneck.   The RTX 5060 or RX 9060XT are also good choices, but only the 16GB version.",buildapc,2025-08-31 00:16:45,1
AMD,nbkyk7c,Please don't get the 5060,buildapc,2025-08-31 00:21:23,1
AMD,nbkyow7,Get a used 3080 on ebay for 350. Or a 6700XT for 250.,buildapc,2025-08-31 00:22:12,1
AMD,nbl1g6j,Why would you buy a worse GPU because of a CPU? Just buy the best GPU your money can get you.,buildapc,2025-08-31 00:39:44,1
AMD,nbkyg0q,Yes but i play 1080 p so 8 gb is anough than right,buildapc,2025-08-31 00:20:40,0
AMD,nbkycp3,I only got like around 350,buildapc,2025-08-31 00:20:05,1
AMD,nbkydji,I play 1080p,buildapc,2025-08-31 00:20:14,1
AMD,nbkzfkg,Why not,buildapc,2025-08-31 00:26:54,1
AMD,nbkzhhk,Thats not as much of a upgrade form the 3050 right,buildapc,2025-08-31 00:27:14,1
AMD,nbl2gu9,Wich one bro i got max 400,buildapc,2025-08-31 00:46:16,1
AMD,nbl0zrm,Depends on the game. Some already need 10-12GB for max quality settings. Paying 350 euro for an 8GB card now is sabotaging yourself.,buildapc,2025-08-31 00:36:48,3
AMD,nbkyxwx,How much is the reaper 9060 XT 16 gb where you live? Would try to bump that budget up so you can get that. Future proof and the best card for the buck that you can get.,buildapc,2025-08-31 00:23:47,1
AMD,nbkypnc,"Still if mostly GPU-intensive games, there is no problem.",buildapc,2025-08-31 00:22:20,1
AMD,nbkztbz,"Bro, the 6700XT is light-years ahead of the 3050. Especially the 3080.",buildapc,2025-08-31 00:29:18,1
AMD,nbl3l7w,"The one that gives you the most performance in the games you play and in games intend to play in the future.  There are many GPUs right now which are excellent in specific games, but flat fall on their face in newer titles due to the rise in VRAM barf.",buildapc,2025-08-31 00:53:20,1
AMD,nbkzept,417,buildapc,2025-08-31 00:26:45,1
AMD,nbl0xja,Thats the ti version your talking about rtx 5060 doesnt have 16 gb,buildapc,2025-08-31 00:36:24,1
AMD,nbl10bq,En the 9060 xt,buildapc,2025-08-31 00:36:54,1
AMD,nbl5x4l,I am getting the rx 9060 xt 16 gb,buildapc,2025-08-31 01:07:43,2
AMD,nbkzm26,What games do you play? Do you play only a few titles or do you play a lot of different games?,buildapc,2025-08-31 00:28:02,1
AMD,nbl1nl3,"Well, you already gave the obvious answer.  Well you aready",buildapc,2025-08-31 00:41:03,1
AMD,nbl1i7m,"9060 XT with fsr4 vs the 6700 XT with FSR 3, 9060 XT probably wins. But at rasterization, 6700XT wins. And if you're playing online, raw power (rasterization) is better",buildapc,2025-08-31 00:40:05,1
AMD,nbl7mfu,"If I was choosing between 5060, 4060 and 9060 XT, I would've gone for the 9060 XT as well.  For me personally the choice is simple - I can't stand frame generation, and I prefer minimal upscaling. With that, the 9060 XT's raw power wins even considering its slightly weaker RT performance.  Also, from just purely my memory - AMD's driver CPU overhead is lower than NVidias, at least currently. AMD is also well known for their GPUs aging like fine wine - driver updates increasing performance over time.  FYI - I am saying this as a 4070 owner (and a long term nvidia user overall).",buildapc,2025-08-31 01:18:08,2
AMD,nbl0qdo,Depends what i olay now i siege somethimes play call of duty warzone en stuff but like once a month fortnite and just shooting games overall but mostly r6,buildapc,2025-08-31 00:35:08,1
AMD,nblwvq4,9060xt is faster than 6700xt in raster,buildapc,2025-08-31 04:06:08,1
AMD,nbl16cl,Used 4060 ti an alternative?,buildapc,2025-08-31 00:37:59,1
AMD,nbm0btd,"In most games, barely. Not worth the premium.",buildapc,2025-08-31 04:33:15,1
AMD,nbl2jqb,Used aint a option,buildapc,2025-08-31 00:46:47,1
AMD,nbl2m9j,Is the 9060 xt not good i see benchmarks its preetu good ngl,buildapc,2025-08-31 00:47:13,1
AMD,nbm11s0,"9060xt is basically same as 7700XT, theyâ€™re significantly faster than 6700XT. Not to mention feature set on 9060xt makes 6700xt garbage value for money",buildapc,2025-08-31 04:39:05,1
AMD,nbl2men,5060 bruchacho,buildapc,2025-08-31 00:47:15,2
AMD,nbl61te,Why?,buildapc,2025-08-31 01:08:32,1
AMD,nbl2sd5,The 9060 xt is an amazing card for itâ€™s price. Would recommend saving up for that.,buildapc,2025-08-31 00:48:17,1
AMD,nbl4adp,I can afford but like is it worth it,buildapc,2025-08-31 00:57:42,1
AMD,nbuajii,5700X/5800X are already a decent improvement over the 3600. They are quite a bit faster. If you can get a normal 5700X for 100-130â‚¬ that's really not bad. For the money it's a good upgrade.,buildapc,2025-09-01 14:17:53,30
AMD,nbubpsy,"There is no component shortage, AMD donâ€™t make the 5700X3D anymore as of August",buildapc,2025-09-01 14:23:55,14
AMD,nbuavp4,5700X would still be a decent upgrade if your gaming is currently CPU limited. Not much else you can do without upgrading to AM5 now.,buildapc,2025-09-01 14:19:38,7
AMD,nbuaih7,For a small boost you can get a 5000 series CPU or you can go AM5 for a larger boost.  This isn't actually a component shortage.  It's just the lifecycle of computer parts.,buildapc,2025-09-01 14:17:44,7
AMD,nbubr9p,I would keep an eye out for the 5500x3d to see on what regions it releases,buildapc,2025-09-01 14:24:07,7
AMD,nbul1bv,Prob used 5700x3d/5800x3d or whatever 5700x/5800x line chip if uts for pure gaming going am5 is similar as the iPhone and clock speed improvements of am5 bring it to near am4 x3d chips level and as they are the current main stream they are a bit cheaper to get would mean platform change but you get more future upgrades so its different trade offs.,buildapc,2025-09-01 15:10:20,3
AMD,nbub8g7,"Just a standard 5600 I think?  [https://de.pcpartpicker.com/list/RQTPrM](https://de.pcpartpicker.com/list/RQTPrM)  If you want to do AM5, you can put a budget rig together surprisingly easily. I am deliberately using one stick of good quality, so you can add a matching one easily later.  [https://de.pcpartpicker.com/list/QfJtGJ](https://de.pcpartpicker.com/list/QfJtGJ)  After you sell the old rig, it's probably pretty close.",buildapc,2025-09-01 14:21:26,3
AMD,nbuhyaw,5700x is  %30-40 faster than 3600. You decide if it's worth it or not.,buildapc,2025-09-01 14:55:08,3
AMD,nbujapw,"Just sorting by P-core boost clocks:  1. 5800XT (4.8 GHz, 8C) 2. 5800X (4.7 GHz, 8C) 3. 5600XT (4.7 GHz, 6C) 4. 5700X (4.6 GHz, 8C) 4. 5600X (4.6 GHz, 6C)  Pricing is weird right now, so some of these that are faster may even be cheaper. Depends on your market.",buildapc,2025-09-01 15:01:43,3
AMD,nbuasu8,I wanted an upgrade to my 5600x without replacing my MOBO and RAM. Settled on a 5800x3d on Ebay.,buildapc,2025-09-01 14:19:13,2
AMD,nbug235,5800X is still perfectly fine! the 5600x3D is coming as a 6core with the 3d cache.   usually how it works is that 5800x which has 8 cores. when it has 2 defect cores they will gert disabled and it turns into a r5 5600 6core     there were people who bought a 5600 and it actually still had 8cores.,buildapc,2025-09-01 14:45:45,2
AMD,nbum2ft,Iâ€™d just bite the bullet and get b650 Matx motherboard + 32GB RAM + 7600x. it would really close in both $ and performance.,buildapc,2025-09-01 15:15:28,1
AMD,nbumlq7,See if you can pickup second hand 5700x3d or 5800x3d they're def worth it and best offering for am4,buildapc,2025-09-01 15:18:07,1
AMD,nbumxyc,5500x3d/5600x3d or 5700x/5800x,buildapc,2025-09-01 15:19:46,1
AMD,nbunic6,if its possible for you to get a 5500x3d at around 200 bucks it would be your best bet   if not... 5600/5700x are below 200 bucks.,buildapc,2025-09-01 15:22:33,1
AMD,nbuqgj0,"5800X/XT is a good alternative to more expensive X3D options, coming from a 3600 it's a substantial improvement and silicon quality may let you tighten up RAM timings if you are so inclined.  Or save up for AM5.",buildapc,2025-09-01 15:36:58,1
AMD,nbuqmd2,Go for a 5700X or 5800X or the newer refresh of the 5700XT/5800XT.  Depending on budget and availability in your area.,buildapc,2025-09-01 15:37:47,1
AMD,nbuwlgz,I'd just save a bit more and go for am5. Something like 7700X/9700X will serve for a long time,buildapc,2025-09-01 16:06:55,1
AMD,nbuyz3m,I just found a 5900xt for 249 shipped for my wife's birthday. Made a nice upgrade for her 5600x since some games would make it sit at 100% usage.  Edit: US pricing,buildapc,2025-09-01 16:18:26,1
AMD,nbv1anv,"If you want something comparable for AM4, it's the 5800X3D. Neither are easy to find, or cheap. If you want one, you're likely going to have looks on the second hand market, or go with a non X3D chip.  That being said, there's no shortage, the 5700X3D is EoL. They discontinued them, and unless someone finds some hiding in the back of a warehouse, they're not really going to come back in stock.",buildapc,2025-09-01 16:29:37,1
AMD,nbv7gnz,They recently announced a 5500x3d but I haven't looked at it yet,buildapc,2025-09-01 16:59:36,1
AMD,nbubk6g,"So if you are pure gaming, you can try 5800x3D, almost same level as 5700x3D, a bit more power thirst than 5700X3D, but that's about it.  If you are using it for workstation PC, you can try 5900X or 5950X, not that perfect for gaming(No V-cache), but much better in multi tasking.  Edit: 5700X is actually a good option as a alternative, good arounder, can also save you some money if you are not looking for that ultimate gaming performance.",buildapc,2025-09-01 14:23:06,-1
AMD,nbuzcmu,I recently went from a 3600 to a 5800x . Thought about the 5700x3d but I was able to get the 5800x for like 120$ and at the time the 5700x3d was almost 300. Am not disappointed with the power uplift but man my new cpu gets toasty,buildapc,2025-09-01 16:20:16,1
AMD,nbufcij,Theyâ€™re EoL and approaching $300 new,buildapc,2025-09-01 14:42:13,3
AMD,nbubw9e,"yes, you are right! my fault",buildapc,2025-09-01 14:24:49,2
AMD,nbujodo,I thought that was South America only,buildapc,2025-09-01 15:03:36,2
AMD,nbuq181,"> the 5600x3D is coming as a 6core with the 3d cache.  5600X3D is exclusive to one US retailer, where it's been out of stock for a while.",buildapc,2025-09-01 15:34:55,1
AMD,nbuc1hy,"i will love to try 5800x3D but is out of stock too. Like other comments are saying, they no longer make that CPUs ):",buildapc,2025-09-01 14:25:35,5
AMD,nbv752w,"Yeah did the same thing. Got a new cooler, that helped, but still gets up there  But Iâ€™m happy with it. Donâ€™t feel any need to upgrade it despite getting a 5070 TI recently",buildapc,2025-09-01 16:58:00,1
AMD,nbug8qh,"Here in Europe I can find a 5700x tray for 105â‚¬ new, so Im wondering where you got that info",buildapc,2025-09-01 14:46:40,10
AMD,nbutryz,It is,buildapc,2025-09-01 15:53:10,2
AMD,nbucdwz,check the used market!,buildapc,2025-09-01 14:27:19,3
AMD,nbus6w9,160 in Sweden,buildapc,2025-09-01 15:45:27,1
AMD,nbunqh2,"the 5800x3d doesnt exist and used the 5700x3d its 250+ at that point, its not worth it   unless you were at a ryzen 2600 or 1600 where the improvement is so big and even then.. maybe better to buy am5",buildapc,2025-09-01 15:23:38,1
AMD,nbv9noa,Paid 85â‚¬ trough aliexpress here in Italy (shipping from eu),buildapc,2025-09-01 17:10:13,1
AMD,naeojsw,"Sure. You're basically putting a 5600/X with a RX6600, and that's a good combo. Just make sure the price is right, I wouldn't pay more than $120 for a 7400F/8400F.",buildapc,2025-08-24 13:10:16,1
AMD,naeso3c,"I'd avoid the RX6600 if you think you'll be playing more modern titles. It's has about the equivalent raw power of a $350 GPU released in mid-2019 that can be snagged for $150-200. The 7400f is fine with something like a Phantom Spirit on-air cooler.    The *16gb* 9060 xt is well-worth every penny if you can fit an extra $100 into budget. [Here are benchmarks of the 6600 vs. 9060 XT](https://www.youtube.com/watch?v=4dj0gsUjHaI) with a very similarly performing CPU. Watch performance across several titles, as the intro seems to be forgiving to the 6600.   If budget is strict, even something like a used $200 3060 Ti should beat out the RX6600.. but there may be more optimal options in that price range (I just haven't looked recently). And yes: for the extra $100 I'd definitely still go 9060 XT.",buildapc,2025-08-24 13:35:49,1
AMD,naftanq,It's fine.  Like every computer ever built it will of course have bottlenecks.  Like most every other computer it will be perfectly usable for its intended usage.,buildapc,2025-08-24 16:50:18,1
AMD,nahjcet,"Try to snag a 6700/6700xt with 12gb of VRAM, seriously (or whatever 7000-series card has 12gb).  That or a used version of that card on ebay (or amazon), sometimes you can even get them with warranty.  If you are open to nvidia, a 12gb 3060 Ti will work also, again a used card.  Just try to get 12gb of VRAM if possible, as what is going to hit you first and foremost is the 8gb VRAM limit issues which is a right-now current event.  However, if that price is too much for you to swing (or the price of any 12gb cards), then 8gb will have to do.  If you are targeting 1080p resolution, or 1440p on games older than a year or two (or three), you should be OK at somewhere in the neighborhood of medium to high settings, which you can lower if things get too choppy.  If it is REALLY slow, just knock texture detail down to medium first, as this helps 8gb cards a lot, and keep those ray tracing settings down to low or off.  Get a 9060 or better if you want to even try RT effects on AMD cards to any decent amount.  8gb VRAM cards will be fine for most e-sports games or Minecraft/Roblox/Kids games, though, if that's it's intended use-case.  In-fact Minecraft can nearly run on your refrigerator or toaster at this point... 20 years after it came out!   A used 7700, 7600, or 7500/7400 CPU can be had cheaply, just check if it comes with a cooler or not.  If it doesn't, you'll have to buy one.  Be careful when installing your CPU, not to bend the pins on the CPU socket, otherwise your PC motherboard will be damaged and you will be forced to fix it yourself (the manufacturer won't do it for free with mangled CPU socket pins!) or buy another - and they're NOT easy to fix.   GOOD LUCK!   Amazon/Ebay do sell used stuff and it can come with warranty, there's even replacement-cost policies you can buy when you buy hardware for usually around 10% the purchase cost per part that are sold by separate insurance companies (like Allstate and others) if the given sold-as warranty is not enough.  Most failures occur in the first 90 days aside of hard drives, which tend to increase in failure rates after the first 2\~3 years depending on manufacturers.  Fans can be like this too, so get decent ones to keep the rattles at bay and keep the noise down.   Edit: Used Radeon 6800 16gb cards are getting CHEAP!   Oh and when buying used, look at the seller ratings.  Don't buy from someone with no ratings or nearly no ratings, or someone that hasn't sold in a lot of years, or from anyone with an unrealistically low price.",buildapc,2025-08-24 22:13:15,1
AMD,najjryw,Better to take the 7500F,buildapc,2025-08-25 06:43:39,1
AMD,najjv3k,8400F and 8700F are the worse AM5 cpu... Even the 7400F is better...,buildapc,2025-08-25 06:44:30,2
AMD,naeorby,Thanks,buildapc,2025-08-24 13:11:36,1
AMD,nafxgl2,Thank you,buildapc,2025-08-24 17:11:11,1
AMD,najkgtc,"Absolutely. Itâ€™s hot garbage but itâ€™s easy to get.Â   Weâ€™re making a 5600/X DDR4 rig on DDR5, not a good DDR5 PC.Â   Good to know about the 7400F, genuinely thought people were typoing the 8400F. Iâ€™ll recommend that moving forward, thanks!",buildapc,2025-08-25 06:50:24,1
AMD,najtl1l,"If it is with the Ryzen 5 5600x, it is 60 bucks in used. 90 for the motherboard and 32gb of ram in used.Â    150 at all and no bottleneck stick with the RX 9060 XT 16gb, if you can find at 350 it can be a good pc.",buildapc,2025-08-25 08:22:00,1
AMD,nboos0m,"I would strongly suggest avoiding any asus products, and going for at least a b850 motherboard.  I wouldn't pay for noctua personally but if it's a must then that's your choice.    Looks fine, but I'd personally get a cheaper SSD, but it's not the end of the world.",buildapc,2025-08-31 16:22:27,-1
AMD,nbottg2,Whatâ€™s wrong with asus?,buildapc,2025-08-31 16:47:50,3
AMD,nbqi3hr,"Nothing especially wrong with Asus. They are said to have terrible warranty/support int he USA, but here in the EU (at least in hungary) they are actually probably the best in terms of warranty.  I personally have been suffering from their B550-e mobo, but that's on intel too (absolute garbage ethernet chip/driver. 2.5gbps I-225V. Had to buy a separate network card. Weirdly low publicity on the problem, apparently very niche.)  Considering you plan for longer term, I suggest trying to stretch the budget for a gpu with better cooling, especially one with PTM on the Core. Asus and MSI (and seemingly Galax, rare though) are the only nvidia partners who use PTM.  So the Asus Prime would be a great step up, both in overall cooling/noise and longevity. Won't ever have to be repaste it, which you would almost certainly have to do with a Zotac card in 5 years. Not a huge deal, but it's a peace of mind.   Don't get me wrong, the Zotac SFF is perfectly fine. But long term... if you care about noise, it's worth a bit more investment. Just a bit more though.  Or an MSI Gaming Trio, but that's probably unreasonably expensive.   So... of course it heavily depends on the price, but I would buy a card with a bit better cooler.  The cases fans are mostly enough, at worst case you might have to add another 140mm one to the front and you'll be good.",buildapc,2025-08-31 21:54:30,1
AMD,nbqn3xs,What you think about MSI GeForce RTX 5070 Ti VENTUS 3X OC 16GB GDDR7 Reflex 2 RTX AI DLSS4? It would be better ?,buildapc,2025-08-31 22:23:08,1
AMD,nbqs9zf,"No, or at least not really. Unfortunately the Ventus isn't great, because it has pretty loud fans. Only the Inspire, Gaming Trio and Vanguard use MSI's good fans.  ((bracketing this part in light of the card having to be very small.   It's weird to say it, but based on [this video,](https://www.youtube.com/watch?v=aFodA-N_SsI&t) the Gigabyte Eagle is probably a better choice than the Zotac SFF. Bit better cooling, bit better PCB. Windforce SFF is basically the same but without rgb.   Zotac offers a 5 year warranty, Gigabyte offers 4, if you register the product within 30 days of purchase.))  To be clear, any of these cards will be ""fine"" if you undervolt and power limit them. But even then they won't be very quiet.  **DAMN** just checked the case's compatibility. With front fans, the max. possible GPU lenght is 306mm.   The Zotac SFF, Gigabyte Windforce SFF/Eagle and Asus Prime technically fit with 304mm lenght. That's uncomfortably close imo...  The best model that comfortably fits is the MSI Inspire, but I assume it's much more expensive. Surprisingly quiet card, despite being tiny. It has MSI's good fans.   Inno3D also fits relatively well (300mm long), but its cooling is probably worse than the Inspire.  Man it's hard to fit a good and cheap cooler into such a small case:D",buildapc,2025-08-31 22:54:25,1
AMD,nbqz7kk,Yeah i found out that this case i chose donâ€™t match with the motherboard. Thx for ur help btw,buildapc,2025-08-31 23:36:53,2
AMD,nbh9jui,Sounds like a motherboard issue. Can you RMA the motherboard?,buildapc,2025-08-30 12:12:29,25
AMD,nbh9kdg,"Sounds like you might have just got a flaky motherboard. AMD has been fantastic for millions of people, it's the recommended go-to platform atm, so jumping off for Intel would be a bit hasty imo",buildapc,2025-08-30 12:12:36,36
AMD,nbh9wd7,"From what I've been reading, AMD has taken the crown from Intel. Evidently Intel is not doing so hot as a company.   Lately I keep seeing that the 9800X3D is the CPU to get.",buildapc,2025-08-30 12:14:58,19
AMD,nbhchbp,Can confirm the tuf and the tomahawk are rock solid boards.,buildapc,2025-08-30 12:32:30,10
AMD,nbifglh,"Sounds like a motherboard issue. Your WiFi is not part of your CPU. It's part of your motherboard. So that's not a CPU issue.  The board came out September 2022. That's three years ago. Google says Gigabyte Aurus Elite AX X670 has 3 year warranty. If you independently confirm that to be true, contact gigabyte **immediately** for RMA fix or replacement.  If it's no longer under warranty, replace motherboard. The rest of your system is still perfectly suitable for modern gaming. No reason to get rid of it.",buildapc,2025-08-30 16:07:47,7
AMD,nbhbxu2,"MSI B850 Tomahawk for $210, etc?",buildapc,2025-08-30 12:28:50,4
AMD,nbi9ipo,"I once had stability issues that were solved simply by updating my chipset drivers. I don't see you mention chipset drivers specifically, maybe you should try updating them first?  Also, did you daisy chain your GPU's power cables? That can lead to instability also.",buildapc,2025-08-30 15:37:55,5
AMD,nbix9w3,"Ignore the brand. Don't think of it as AMD Vs Intel or Nvidia Vs AMD.  Check the benchmarks and reviews and base your purchase on that. Reviewers dedicate so much time and energy into comparing products. It's the best was to purchase.  By the product, not the brand.",buildapc,2025-08-30 17:35:56,3
AMD,nbhakht,"Thanks guys, could I have some specific recommendations on which AMD board is the best for my set up and which is the most stable for heavy streaming and gaming? MSI, ASUS or any other brands? Think I might avoid Gigabyte for now.",buildapc,2025-08-30 12:19:38,2
AMD,nbijcko,Team Red,buildapc,2025-08-30 16:27:03,2
AMD,nbild7j,"Probably already done this but, are you running the latest stable BIOS for your current board?    However for future upgrades one of the 800 series MoBos from MSI or Gigabyte will let you reuse almost everything in your current build.",buildapc,2025-08-30 16:37:05,2
AMD,nbizso7,If you want something that's future proof I would stick with an am5 socket because they plan to support and provide upgrades well into the future.,buildapc,2025-08-30 17:48:29,2
AMD,nbj1kqc,AMD!,buildapc,2025-08-30 17:57:27,1
AMD,nbj70pz,have the tuf x670e and 7950x3d. Love it.  Zero issues.,buildapc,2025-08-30 18:24:59,1
AMD,nbldtp6,"Try updating your UEFI / BIOS. You can also pull the dGPU to see whether you have similar issues with on-board graphics. There's some isolation you can do. And WiFi cards like Intel AX210 and AX211 are fantastic, cost effective choices for a 6 GHz upgrade.  Reddit will always recommend AMD lately, but Intel is running some great deals that make the 265K look an appealing value. If you are gaming on 4K display, you are GPU bound anyway. Fall in love with the board first!  But my more specific recommendation is Framework Desktop tho. That's a clever build. The most interesting option and the only one that is Copilot+ PC.",buildapc,2025-08-31 01:56:19,1
AMD,nbkv5x0,"Yeah, but they want me to wait weeks to get it ""checked"" to see if it's faulty. Which means i'll have no motherboard till then. I only bought it 1.5 years ago. So it's definitely under warrenty. But I guarantee they will fight tooth and nail to say it's not faulty to not get a refund. So i'm going to buy one now and see where that path takes me.",buildapc,2025-08-30 23:59:42,1
AMD,nbkupzh,"Yeah, I want to replace it first. Because the place where I bought it wants to send it away to check if it's faulty. Which I know it is. Which they intentionally do to keep you without a motherboard for a week at least so you have to buy another one. So I will get another one and get it sent away. If they don't refund it then I know they're lying.",buildapc,2025-08-30 23:56:51,1
AMD,nbj6i8y,"I was having similar issues (wifi not working upon boot up and failing to load but no crashing) on a Gigabyte B650, updated chipset and wifi drivers and has been working fine last couple weeks.",buildapc,2025-08-30 18:22:28,1
AMD,nbkvkya,I tried to update it. Would crash every time. I sent it to the computer store to get it fixed. They also had the same problem. Had to reset it to default stock settings and it works decently sometimes but not all the time. I don't know what's going on with it. But definitely seems like a hardware issue. I was a little reluctant to get gigabyte. But I wanted to go with MSI because my 4080 is a MSI also. Don't know if that means much. But may add a few points to compatibility.,buildapc,2025-08-31 00:02:24,1
AMD,nbhccm3,"It's either MSI or Gigabyte.  I have a similar GB board to yours (X670E AORUS PRO) and I don't experience these issues.  MSI Mag Tomahawk are good boards, the Carbon wifi too.",buildapc,2025-08-30 12:31:37,6
AMD,nbhbmrj,"B850/X870 TUF, X870/B850 Tomahawk, X870 LiveMixer",buildapc,2025-08-30 12:26:48,3
AMD,nbihpyu,"I've had an MSI board for 6 years now, giving MSI a +1",buildapc,2025-08-30 16:18:57,1
AMD,nbkw59d,"Tried updating it but because it won't let me restart and crashes on restarts but lets me shut down and start up fine, I don't want to run the risk of updating it because it needs to restart to complete the process. The computer repair store tried it too and it kept crashing. Rather get a more stable motherboard for the long term than have this keep happening.",buildapc,2025-08-31 00:06:01,2
AMD,nbkvreo,That's what mine does. But because my computer won't restart and crashes on restarting. I don't want to risk bricking my computer more than it already is until i have something to fall back on.,buildapc,2025-08-31 00:03:34,1
AMD,nbiljwd,I have a tomahawk and its a great choice,buildapc,2025-08-30 16:38:02,2
AMD,nbhq3cs,Avoid the Live mixer board for now. Weâ€™re still uncertain whether Asrock resolved their issue yet.,buildapc,2025-08-30 13:55:50,4
AMD,nbl31i8,"That does sound like there's something odd with your motherboard.  Going with a B850/X870/X870E chip-set board will give you the best (current) upgrade path while staying in the AM5 family.  Depending on how much they'll charge you, you might consider asking your local shop to ""stress test"" your CPU/GPU and memory in one of their known good systems.  That will let you know what you can plan on re-using in your new system.",buildapc,2025-08-31 00:49:53,1
AMD,nbplf42,For gaming itâ€™s AMD right now.   The more important question is what your budget is,buildapc,2025-08-31 19:04:10,119
AMD,nbpm6pu,"AMD is far superior right now, the ""more core"" is actually E cores which doesn't help much in gaming, the P core is the main workforce and it's basically has the same amount of P cores as the AMD counterpart",buildapc,2025-08-31 19:08:05,69
AMD,nbppwer,"These years AMD makes just better CPUs overall: raw gaming performance, price/performance ratio. It happens sometimes that the former underdog gets to the top.  But Intels are great as well. It's not like you're gonna be able to play with Intel, no. More likely you'll get a 5% less performance (highly depending on game, some will prefer Intel) or spend about hundred dollars more, or have to gey better cooling, or have more expensive motherboard... But the differences in performance will be negligible in the same price range.  If you're into some specific game and you absolutely need to know, will you have, say, 120 or 129 fps, you can search for performance test videos for that game. There's a lot of them.   If you're a general gamer-for-fun, just ignore the CPU, as GPU will define most of the performance.",buildapc,2025-08-31 19:26:53,11
AMD,nbpm875,"For gaming an X3D CPU is currently unmatched because it uses so little power and delivers top tier gaming performance. Also: clockspeeds do not matter as much as other specs on the CPU. Games do not profit from extra cores and threads as much as productivity software. In that case there are also AMD CPUs with higher core count, but the x3d variants are very expensive.  Knowing your budget would also be helpful.",buildapc,2025-08-31 19:08:17,28
AMD,nbpozgb,"More cores and threads do not translate to better gaming more cores generally slows down gaming performance,amd ryzen 7 7800 x3d best gaming cpu in the market dollar for performance,if you want bragging rights,go for the 9800 x3d but you gain in minimal for the extra $120",buildapc,2025-08-31 19:22:12,7
AMD,nbpnnpb,X3d,buildapc,2025-08-31 19:15:29,12
AMD,nbpnccc,AMD on top,buildapc,2025-08-31 19:13:55,14
AMD,nbpqdt3,"If you're playing more at 1080 and 1440p then the x3d, if 4k any top of the line cpu will work",buildapc,2025-08-31 19:29:24,2
AMD,nbq10rt,Intel 14600K is cheapest cpu for serious gaming,buildapc,2025-08-31 20:24:36,4
AMD,nbplo8z,There's this thing called *benchmarks* from places that are *reputable* that use *generally controlled environments* to measure performance of these components where you can decide if it's worth your money.    https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/19.html  They even have a section on price to FPS.  Imagine that.,buildapc,2025-08-31 19:05:29,6
AMD,nbpoe8j,"Intel is on the decline, I build 2 amd systems last week with x3d chips, gaming has been amazing.",buildapc,2025-08-31 19:19:13,1
AMD,nbpoggs,"It depends on the budget, for example, I will assemble a setup with an i5 14400f which is â‚¬130, the AMD equivalent which is the Ryzen 5800x is â‚¬150, the latter is inferior. So I will pay for less for performance.",buildapc,2025-08-31 19:19:31,1
AMD,nbpr30r,For budget... AMD. For performance... also AMD. Don't bother getting more than 8 cores. If you find fantastic intel deals they can still game well.,buildapc,2025-08-31 19:33:00,1
AMD,nbptbws,"If itâ€™s just for gaming and a tiny bit of productivity tasks for work, AMD all day (specifically the Ryzen 7 9800X3D).   If you do more work/film, photo or music editing with a tiny bit of gaming, then Intel with more cores.",buildapc,2025-08-31 19:44:36,1
AMD,nbpthb8,What resoution you wanna play?,buildapc,2025-08-31 19:45:23,1
AMD,nbpusp3,"TLDR: AMD with the 3D L3 cache (X3D) is the fastest right now.  The truth is; it depends.  If I pair a the fastest AMD and Intel CPU with a mid-range graphics card to play games at 4K resolution with ultra settings, both CPUs will perform identically.  It might very well be that you can pick a lower priced Intel CPU and pair it with a higher priced GPU to get the best value for your money.  Sorry if I burst your bubble but it not as simple picking the fastest CPU especially if you are on a budget.  To have peak performance on the games you play , you need to research the which CPU and GPU perform the best and then try to min-max your setup.  With the current prices I suggest a budget split of 2(GPU)/1(everything else minus Keyboards, mouse and monitor).",buildapc,2025-08-31 19:52:15,1
AMD,nbpv44a,i used to think Intel was the best. my thinking changed once i moved to AMD 5800x3d many many years ago.,buildapc,2025-08-31 19:53:57,1
AMD,nbpvdhh,I've had my first Ryzen CPU ever for the last two years and I'm very satisfied. No issues at all.,buildapc,2025-08-31 19:55:19,1
AMD,nbpw72l,"My method before upgrading pc is... go to some pc tech sites like guru3d, check benchmarks. Sure 9800x3d is the king. But it cost twice as the i5 14600k. And the perf difference is the same? Nope. And if you play on 4k the perf is almost the same for both. It all depends what is your budget. If you have the money, go for x3d.",buildapc,2025-08-31 19:59:38,1
AMD,nbpw736,You gonna be fine with either of them,buildapc,2025-08-31 19:59:38,1
AMD,nbq0klb,For gaming it's an AMD X3D chip no contest.,buildapc,2025-08-31 20:22:20,1
AMD,nbq1p1w,"Despite AMD is generally superior, I think the 14600K has pretty good value right now, depending on your country, use case, budget and situation. It's below 200 with a Battlefield 6 in bundle here. You need X3D AMD processors to get better performance, and they cost much more. As I'm coming from 12gen Intel and I already have an LGA1700 motherboard, it's a logic upgrade. On the other hand, unless you build a high end PC with an expensive gpu, you won't get much worse performance with a 7600/7700 in the price range of the 14600K, with better upgrade options.  So I would say AM5 is generally superior with the fastest X3D gaming cpus, but these are expensive and 14600K is also an option, but only at this moment due to the bundle.",buildapc,2025-08-31 20:28:02,1
AMD,nbq6w1v,"Intel prob has good deals right now, they are pretty desperate. Ya AMD has the best performance these days if you want the bleeding edge of gaming performance, but seriously, just shop good deals, the GPU choice matters much more for gaming than the CPU.",buildapc,2025-08-31 20:54:17,1
AMD,nbq9yb8,"Iâ€™ve seen you guys wanting to know my budget.  $1,800",buildapc,2025-08-31 21:10:11,1
AMD,nbqagbu,AMD for gaming!,buildapc,2025-08-31 21:12:51,1
AMD,nbqao3f,Why even ask this on reddit? You will only get one answer and opinion.,buildapc,2025-08-31 21:13:59,1
AMD,nbqb3ct,"AMD has X3D cache, more cores and higher clockspeeds in their top models compared to intel.  also higher clockspeed doesnt mean higher performance.  it is about IPC (instructions per clock) in which AMDs highest end CPUs are also better.     to keep it simple if money is no issue then get a 9800x3D or 7800x3D",buildapc,2025-08-31 21:16:16,1
AMD,nbqbw8q,Any 9000x3d chip takes a dump on the best Intel chip in gaming,buildapc,2025-08-31 21:20:36,1
AMD,nbqgc7l,Definitely AMD since latest Intel is a dead platform. Amd is still supported for another 2 years.,buildapc,2025-08-31 21:45:00,1
AMD,nbqh922,AMD has the same frequency as Intel both can hit ~5.5 stock.  Intel has more slower cores.  AMD beats Intel by a significant margin for gaming right now and it doesn't look like that will end anytime soon,buildapc,2025-08-31 21:49:56,1
AMD,nbqj4n5,Pick an AM5 mobo and you can upgrade later. Pick a 1851 and...you need a frame...,buildapc,2025-08-31 22:00:15,1
AMD,nbqlf4p,"The 9800X3D is the best gaming CPU on the market but honestly any modern AMD or Intel CPU will likely be more than good enough for every game on the market. Maybe if you're running a 5090 or doing high-frame rate stuff, the difference might matter, but for most games any contemporary CPU will be plenty good.",buildapc,2025-08-31 22:13:17,1
AMD,nbqp03j,it depends on your budget and how often you are going to upgrade.,buildapc,2025-08-31 22:34:24,1
AMD,nbrnlo9,"I believe the quick way to sum it up is that Intel cores excel at productivity, but can handle gaming if thats what you want to build for. AMD's cache excels at gaming, giving better performance for less power   The deciding factor that people who know much better than I usually highlight is price since some Intel components can he found cheaper, but since parts stay interchangeable longer with AMD, I very frequently see gaming build options at the same price range with AMD either so close it doesn't matter or actually gives better gaming-specific performance",buildapc,2025-09-01 02:07:26,1
AMD,nbsvwtl,"As the others say.  AMD's X3D chips have absolutely murdered Intel's closest chips.  Price per performance, they are king. Parity performance, they are king.  And they have relatively low powerdraw to boot.  For gaming I can only recommend AMD.",buildapc,2025-09-01 07:55:36,1
AMD,nbsy3mx,"I bought a PC with an Intel 13900ks about a year and a half ago before the bios problems were all worked out, and despite a ton of tweaks to stabilize it, I finally have to replace my processor and MB because the frequency of the crashes kept increasing.   Went with an AMD 9800 X3D just because of how bad of an experience I had with the Intel chip. With the bios all good, more than likely the Intel chips would be good now, but pretty sure my processor had degraded with the early overvoltage/heat issues. Looking forward to seeing how it runs once the local custom build shop gets it back to me.",buildapc,2025-09-01 08:17:00,1
AMD,nbszcyk,"AMD easily clears from a pure pc building standpoint the 9600x and 9700x are the best value imo, but the older intel chips that cost  like 100-150 dollars are fire for smaller pc projects like emulation machines",buildapc,2025-09-01 08:29:15,1
AMD,nbpmalm,Amd is better at the top end with the x3d chips but rn intel is better price to perf with the 14600k at 150 with bf6 for free  However by going with intel you also give up upgradability so even if the price to perf is better it may be a better idea to spend more for ryzen,buildapc,2025-08-31 19:08:37,1
AMD,nbpv6ig,For gaming AMD is the winner hands down in the CPU market,buildapc,2025-08-31 19:54:18,1
AMD,nbpvmcz,"Some people say they have no issues with AMD X3D others have stuttering problems which they dont know how to fix over months/years. I tried the 7800X3D and the 7900X3D on 3 different x670 mobos Msi carbon/asrock taichi and gigabyte Aorus Master. I had only issues with stutters, delay and crashes. Tried all possible to fix it , after 18 months i gave up and switched to Ultra Core 265K, since now all is running buttery smooth no issues at all. If i read all the posts at r/AMDHelp the issues still exist. I wouldnt care if amd or intel, all i want is a working pc",buildapc,2025-08-31 19:56:37,1
AMD,nbpw0ri,AMD & Nvidia.,buildapc,2025-08-31 19:58:43,1
AMD,nbpx2ul,"AMD is the obviously superior CPU right now for almost all use cases, including productivity. Intel has some niche software wins. For gaming its not even debatable. (like its not a situation where X or Y game favors Intel, nothing favors Intel, and in most games its not even close AMD performs a generation beyond Intel)  AMD also means you are buying into AM5. Which has 1 and perhaps as 2 more CPU generations coming for it. So if you buy a 9800x3d or any other AM5 CPU. You will be ready to drop a Zen6 into the same rig. If rumors are true we may also get a 6+ or even zen 7 on AM5 as well. Intel ultra and 14th are both DOP (Dead on purchase). Intels NEXT gen, it is rumors Intel will finally learn and plans to support 3 or 4 generations on the same socket. Right now as it stands that isn't the case. Any rig you buy now is tapped out. ZERO doubt that a Zen6 drop in replacement in a year or two is going to destroy anything you can buy from Intel today in all things.",buildapc,2025-08-31 20:04:17,1
AMD,nbq2ykq,AMD. Intel is quickly falling behind and has less long-term support.,buildapc,2025-08-31 20:34:23,1
AMD,nbpo1kg,Either AMD or Intel will work great. Have you done any research into YouTube videos that compare them?,buildapc,2025-08-31 19:17:26,0
AMD,nbpp0jf,If you dont want spend too much money then Intel,buildapc,2025-08-31 19:22:21,-2
AMD,nbpu79o,"Amd X3D if you want to squeeze everything out of your system. Other than x3d everything is kinda on pair. Where single core performance matters Intel is better, but runs hotter and power draw is significantly higher.",buildapc,2025-08-31 19:49:09,0
AMD,nbpnqbx,"***The Ryzen 9 9950X3D is an easy recommendation with very few caveats. It's fair to say this new Ryzen chip has cemented itself as the best all-around processor for those seeking ultimate performance for both work and play. If productivity is your primary focus, the 9950X3D is likely the best option***  ***In workloads like***Â [***Cinebench***](https://www.techspot.com/downloads/7579-cinebench-r23.html)***, the 9950X3D and***Â [***Intel's 285K***](https://www.techspot.com/review/2934-intel-core-ultra-285k-vs-intel-core-14900k/)Â ***were neck and neck, with the Ryzen processor consuming just over 10% less power. In general, they should be highly competitive for productivity tasks, but as always, it's best to research specific application performance based on your workflow.***  ***Where Intel falls behind significantly is in gaming. Here, the 9950X3D delivered, on average, 35% more performance.***  [https://www.techspot.com/review/2965-amd-ryzen-9-9950x3d/](https://www.techspot.com/review/2965-amd-ryzen-9-9950x3d/)  ***If you have the funds and are looking to build a purely gaming computer, we think you should scale it down and go for a***Â [***9800X3D***](https://www.amazon.com/AMD-9800X3D-16-Thread-Desktop-Processor/dp/B0DKFMSMYK?tag=gamersnexus01-20)***. Itâ€™s just not that big of a difference as the 9800X3D often trades places with the 9950X3D and you save some money.***Â   ***Intel, on the other hand, is out of this conversation right now. They are not part of the high-end expensive CPU for gaming build scenario at the moment.***Â   [https://gamersnexus.net/cpus/amd-ryzen-9-9950x3d-cpu-review-benchmarks-vs-9800x3d-285k-9950x-more](https://gamersnexus.net/cpus/amd-ryzen-9-9950x3d-cpu-review-benchmarks-vs-9800x3d-285k-9950x-more)  AMD's flagship processors, has just as much cores and threads as Intel's. It's just as competitive as Intel's latest in productivity, consumes less power, and is much faster for gaming.",buildapc,2025-08-31 19:15:52,-1
AMD,nbptjrr,"The biggest bottleneck is the GPU, not the CPU.  Nowadays AMDs x3D chips are far superior if you only look at gaming.  But they are also more expensive now, at least where I live. Roles have switched a bit. An intel ultra 7 265kf is far cheaper than the much older 7800x3D from AMD, at least here, it's worse in gaming performance but overcomes the AMD chip in productivity with ease.  If you really focus on gaming take the 7800x3D or 9800x3D, depending on the budget.  But the CPU is really one of the least important bottlenecks if they are already decent and the higher the resolution the more you utilise the GPU which then becomes the bottleneck.",buildapc,2025-08-31 19:45:46,0
AMD,nbqi91q,Best value chip is 265K. Also comes with free Battlefield 6 which is a $60 game that almost everyone wants. Making the CPU dirt cheap.,buildapc,2025-08-31 21:55:21,0
AMD,nbroleb,"Have a 285K and 9800X3D and 5090's.  Game at 4k, Can't tell the difference between the 2 in games, but since I actually do other work on my computer..........the 285K smokes the AMD ""great"" gaming chip, so instead of having a one trick pony, the 285K is my main system that makes zero compromises.",buildapc,2025-09-01 02:13:27,0
AMD,nbplozt,"AMD if ur only playing games on the PC. Intel for everything else, like 3D design programs, any sort of development software, or other productivity items.",buildapc,2025-08-31 19:05:35,-7
AMD,nbppeyv,"X3D, but slow at non-gaming, and you will NOT notice it at 4k.",buildapc,2025-08-31 19:24:23,-2
AMD,nbpslz7,I'm not sure if there is any real difference in using a Intel CPU vs anAND CPU. What I have heard is that there are some najor differences in game play when using an AMD based GPU vs an INVIDA<sp> based one,buildapc,2025-08-31 19:40:55,-2
AMD,nbpr74y,"""Intel often has more cores and threads and higher clock speeds."" no intel offers more E cores and it super bad for gaming because for some games you need to go in bios and disable E cores or games will lag",buildapc,2025-08-31 19:33:36,-3
AMD,nbpsg8h,"In gaming, not productivity and not regarding the value.   But in gaming yes.  Edit: Bad formulation from myself: they are not bad in productivity but in many regions worse especially when calculating the value.",buildapc,2025-08-31 19:40:06,-34
AMD,nbrzy1s,">price/performance ratio  [Not really.](https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/28.html) Retail pricing adjust fairly quickly to correct this kind of stuff. Note that the data in the linked review is using pricing data from November 2024 when it was published, so there have been some noticeable shifts.   Things are generally pretty consistent over time though. Newer, higher end chips pretty much inevitably offer substantially worse performance/$ than older/budget chips. The 9800X3D ($470) has an absolutely terrible price/performance ratio, because it's the 'top dog' and AMD can easily demand a significant premium for it - you're literally paying more than twice the $ per frame compared to something like a 9600X ($200), or even an Ultra 7 265K ($260). The 14600K can be found at $150 right now and pretty much blows everything else out of the water in terms of value.   AMD's dominance caused Intel's consumer sales to crater (in terms of CPUs sold to builders, because they're still milking tons of contracts with OEMs), so retail pricing on those parts has dropped significantly to accomodate. That $260 Ultra 7 265K launched at $395, which even then was *still* able to offer marginally better $ per frame vs the 9800X3D. At $260 it's in the same value realm as the 9600X. And that's before the various microcode and scheduler fixes were implemented, though those generally delivered fairly marginal performance uplifts.",buildapc,2025-09-01 03:28:19,3
AMD,nbrew81,The 7600X3D crushes the 7800X3D if you're just going for performance / price,buildapc,2025-09-01 01:13:15,1
AMD,nbr0mjk,Imagine thinking owning an AMD product is bragging rights lmao. What a joke.,buildapc,2025-08-31 23:45:31,-8
AMD,nbs0oon,Definitely a pretty fantastic value at $150USD right now.,buildapc,2025-09-01 03:33:34,1
AMD,nbprgzg,"Yeah, the 245k is absolutely a better buy than the 9700x. Intel also has better memory controllers.",buildapc,2025-08-31 19:35:02,3
AMD,nbs0z2x,Many many years meaning... 3? Assuming you bought quickly after launch.,buildapc,2025-09-01 03:35:39,1
AMD,nbppaxf,"The difference is pretty minimal in 1440P and non-existent in 4K.  Nobody pairs a 9950x3d or 285k to a 1080 Monitor and cheapo GPU. My 9950x3d is paired to a 5090 with a Super Ultrawide Monitor. Minimum people will get is something like a 5070Ti for this pairing, so at least 1440P.",buildapc,2025-08-31 19:23:49,3
AMD,nbs8f2f,"You'll get the same performance with a ryzen 5600 or i5 12400 at 4k in demanding games. Why don't you just get that, I'll trade my system for yours. Also maybe you should give csgo a try and see the diff between the 9800x3d and 285k.  There's a reason why cpu testing is done at 1080p, and/or when there's not a gpu bottleneck.",buildapc,2025-09-01 04:30:56,1
AMD,nbpmafb,It really depends on how much of those things you actually do. Itâ€™s just a hobby a few hours a week itâ€™s not a big deal.   If itâ€™s your full time job then yes,buildapc,2025-08-31 19:08:36,5
AMD,nbpucyo,"Amd CPUs also runs well in productivity as long as it doesn't need that special intel iGPU encoder, the 9950x3d is a perfect example.",buildapc,2025-08-31 19:49:59,27
AMD,nbt6dnv,As an intel owner I regretted buying this crap,buildapc,2025-09-01 09:37:30,1
AMD,nbts734,"Where I live even the 265K costs like 30% less than a 9700x, it has an incredible price to performance right now. It's harder to recommend since it's a ""dead"" socket with no upgrade path, but if you think you're gonna keep the same PC for like 4 years without upgrading, definitely look into intels prices. They're much more competitive now in terms of price to performance than at launch, often outright beating AMD for budget and midrange builds.",buildapc,2025-09-01 12:32:30,2
AMD,nbpt8ip,"At 4k yes, but thereâ€™s still a sizable difference at 1440p. The point is, the OP asked which brand has the better processor for gaming, and right now itâ€™s AMD.",buildapc,2025-08-31 19:44:07,1
AMD,nbqeqtq,">  Nobody pairs a 9950x3d or 285k to a 1080 Monitor and cheapo GPU.  Plenty of people do exactly this. Esports gamers don't need much GPU, since they're often *only* interested in FPS at 1080p Low.  I have my 9800X3D running with a 6800XT at 1440p right now, and the CPU is still my limitation in a majority of games. I don't play anything particularly demanding on GPU horsepower.",buildapc,2025-08-31 21:36:15,0
AMD,nbpnhit,I think AMD is great for purely gaming. in my experience the CPUs get beat out by Intel multi core performance and stability. but that was well over 3-4 years ago and I could be wrong about their performance now. what's your experience?,buildapc,2025-08-31 19:14:38,-4
AMD,nbpxlyi,"Yeah they are good, but not a great value, at least here where I live. The 9950x3D is like 650â‚¬, the ultra 7 265kf costs only 279â‚¬ and the Ultra 9 265K is ""only"" 530â‚¬.  That's like 120â‚¬ plus. The 9950x is with 520â‚¬ near the ultra 9. So the ultra 7 for productivity and gaming is just a steal. The 7800x3D is available for 350 and the 9800x3D for 450â‚¬.  So many hate on intel now, but for years AMD had nothing to compete with intel. Now they surpassed intel, especially in gaming and got more expensive than intel, so intel actually became the efficient great value underdog AMD was before. But instead of boosting intel or keeping up the competition people just rant on intel permanently.",buildapc,2025-08-31 20:07:02,-20
AMD,nbtv8wc,"Where I'm at the 9700x and 265k cost about the same. And yeah, I could easily see myself going four years without upgrading. Computer shit is expensive where I live and wages kinda suck.",buildapc,2025-09-01 12:51:49,1
AMD,nbsk329,"I never said amd was better at intel for rendering and video editing. But what percentage of people buying these chips actually do that?   If you're argument is that intel is better at AMD in both rendering AND gaming that's false. And you don't have to play at 1080p, why don't you try comparing your 285k and 9800x3d in cs2,wow at 4k?",buildapc,2025-09-01 06:07:35,1
AMD,nbso264,"Hello, your comment has been removed. Please note the following from our [subreddit rules](https://www.reddit.com/r/buildapc/wiki/rules):  **Rule 1 : Be respectful to others**  > Remember, there's a human being behind the other keyboard. Be considerate of others even if you disagree on something - treat others as you'd wish to be treated. Personal attacks and flame wars will not be tolerated.    ---  [^(Click here to message the moderators if you have any questions or concerns)](https://www\.reddit\.com/message/compose?to=%2Fr%2Fbuildapc&subject=Querying mod action for this comment&message=I'm writing to you about %5Bthis comment%5D%28https://www.reddit.com/r/buildapc/comments/1n52z7q/-/nbshphe/%29.%0D%0D---%0D%0D)",buildapc,2025-09-01 06:42:32,1
AMD,nbq03j8,"They fucked up their launch and price which was around $400 at the time also for extra $150Â± you get more threads and core and also a CPU that can game I'd gladly pay for it.   Whats funny is it's cheaper to get AM5 in the long run too as Intel already abandoned the lga 1851, meaning you have to buy a whole new motherboard for the next core ultra",buildapc,2025-08-31 20:19:53,15
AMD,nbq0itd,We'll shill for Intel when they deliver a decent product that doesn't either underperform like mad or self-destruct.,buildapc,2025-08-31 20:22:05,9
AMD,nbq23tu,"Issue is Intel products arenâ€™t reliable. We still donâ€™t have enough data to prove the kinks are gone. I would rather pay more for AMD products with much more proven reliability than Intel atm. So, buying into 13th and 14th gen is still a risk. Is buying an unreliable car for cheaper than a reliable one â€œbetter valueâ€? For me, no.",buildapc,2025-08-31 20:30:04,2
AMD,nbrhpls,"Amd is NOT more expensive than intel lol, at least in terms of price to performance. The 9950x3d is $699 and is 35% faster in games than any cpu intel has ever put out.  The 9800x3d is $479, and is 36% faster than any cpu intel has put out in gaming. The best intel cpus are $500+ rn",buildapc,2025-09-01 01:30:46,1
AMD,nbs9i99,"> Whats funny is it's cheaper to get AM5 in the long run too as Intel already abandoned the lga 1851, meaning you have to buy a whole new motherboard for the next core ultra  It depends. Both the 265K and LGA 1851 motherboards are cheaper than similar AM5 chips/mobos. Assuming you're using a like-for-like motherboard (like, the MSI MAG B850M MORTAR and MSI MAG B860M MORTAR), a Core 7 265k system is $243.83 cheaper than a 9800X3D system. The 265k is $93.83 cheaper than the 9700x. There's maybe a little savings lost since the Intel's can (and should) be outfitted with faster member than AM5, but that should hopefully be re-usable.  The 265k is definitely not as fast as the 9800X3D in most games, but, unless tariffs completely f motherboard market, you'll be able to get a AM6 or LGA xxxx board in the future",buildapc,2025-09-01 04:39:29,-1
AMD,nbq51tg,"The first abstract is a bit unclear, could you explain further?   Well the upgrade thing is an aspect. But I actually don't know if it's really that important. I have read multiple times, that most people upgrade the mainboard anyway after a long time pc.   And I actually don't know many people buying frequently and upgrading frequently. Most I know buy a CPU and GPU and keep them for years.",buildapc,2025-08-31 20:44:57,-13
AMD,nbq6reg,"Naw AMD fans will never stop.  Regarding your comment: the ultras are decent, they are neither prone to get the instability problem nor are they bad. They perform very well in gaming and even better in productivity while being much cheaper than AMDs CPUs.   The mainboard aspect is the biggest disadvantage but I think keeping a CPU long-term will increase the probability that you'll upgrade the mainboard and ram anyway.   The instability is from my knowledge gone and AMD has had some problems lately. Of course this was no big deal...",buildapc,2025-08-31 20:53:39,-8
AMD,nbq637d,"Okay so first you would probably buy the ultra anyway. These CPUs have great value, are easy to cool as I have heard and even though they have a higher max TDP they run efficiently.   The thing about the 13th and 14th gen is from my knowledge more an emotional thing right now. As far as I know it got fixed with updates and a bios upgrade.   But you still get a 2 year extra warranty so there shouldn't be a big problem, you will get 5 years of update warranty that's kinda impressive.   So I don't see any problem buying an intel. I actually see many pros buying an ultra 7 for example.",buildapc,2025-08-31 20:50:10,-1
AMD,nbt7g44,"I said where I live. AMD is incredibly expensive here and I said that multiple times.  The 5800x3D is over 600â‚¬ here for all Americans over 700 dollars. And yeah it's more powerful in gaming, depending on the scenario also quite a bit.  https://www.tomshardware.com/pc-components/cpus/intel-core-ultra-9-285k-vs-amd-ryzen-7-9800x3d-faceoff-battle-of-the-gaming-flagships#section-gaming-benchmarks-and-performance-intel-core-ultra-9-285k-vs-amd-ryzen-7-9800x3d The Intel's get destroyed in gaming, no doubt, but the prices here are vice versa where I live.  And in productivity intel wins while being highly efficient.  There are even better productivity cpus from AMD that beat Intel's best allrounders, but they are then worse in gaming and also not quite as cheap. And just check this out:  https://youtu.be/zbbVU101Gc4?si=u00Rz9sI5fOAdRBQ While the ultra is 100 bucks less where I live than the 9900, a a hundred...",buildapc,2025-09-01 09:47:27,1
AMD,nbqkspy,"Plenty of people who are recommending AMD aren't ""AMD fans"". Being a fan of a corporate brand is dumb! I've had multiple AMD and Intel CPUs over the last 30 years, and try to choose based on who provides the best value at my price point at the time.  Right now, for gaming at least, it's AMD. My previous 2 PCs were Intel.  As for the upgrade thing, going from a 2600 to a 5600X without needing to buy RAM and motherboard was a great upgrade. But you are obviously free to choose for yourself.",buildapc,2025-08-31 22:09:42,9
AMD,nbq7q1u,"How are you claiming that the Ultras perform ""very well"" in gaming when even the 285K is middling at best? They perform worse than Raptor Lake, and are even beat by the 5600X3D in some cases.  What issues did AMD have? I haven't heard of anything.",buildapc,2025-08-31 20:58:31,1
AMD,nbrkf28,As an IT professional amd work so much better in an office setting. Intel has been on a downward slide for a min. Their server processors even make server updates and migrations far quicker. I try to push people to AMD for most applications.   Gaming theyâ€™ve been dominating for literally a decade easy.,buildapc,2025-09-01 01:47:53,1
AMD,nbri8m0,"Iâ€™m not an â€œamd fan.â€ Its just that amd is doing **GAMING** better than intel rn, which is what the post is about. Intel also has worse frametimes and worse hitching issues in games, when 9800x3d and 9950x3d do not.  My productivity build is intel, fyi. But for gaming, ALL DAY its amd, like objectively",buildapc,2025-09-01 01:34:05,1
AMD,nbqlpcr,"I'm not talking about the people recommending AMD, I talk about the people promoting AMD. The YouTube shorts with meme pictures and the frame rate counters with ""dope emojis"" to show how badass AMD is.  Also the flames in posts that sound like pure bait.   And yes I also think it's about value. If you really focus on gaming and if you have a bit more money, go for AMD, clear choice.   If you want a good and cheap allrounder, intel is also a good choice and you won't lack too much gaming performance, depending on the game and settings.   AMD actually also beats intel in the Threadripper segment.   The only place intel still wins is max OC Performance, from my perspective a sad development and intel made mistakes, but a lot of hate is actually over exaggerated and misleading.   Because it also leads to high AMD prices. A 7800x3D is a great CPU, but it's old and more expensive than it should be, just because of the promotion.   Intel on the other hand gets cheaper and cheaper and if intel falls too much behind we won't gain anything of it. This happened to AMD against Nvidia and we see the outcome now.   It would be better if intel could get their stuff together to battle AMD and it's not a bad choice to buy intel in some situations.",buildapc,2025-08-31 22:14:57,2
AMD,nbq8qkx,"What benchmarks did you check out and you know that Intel got a performance boost update?  The CPU is also one of the least important bottlenecks, the GPU is way more important, especially with higher resolution.  So a CPU performing great is overall overinterpretation in most scenarios.  Anyway I would like to see your sources.   Search for AMD - ASRock issues. AMD claimed ASRock made the mistake, ASRock denied it and I think while it was actually mainly an ASRock problem, it still happened on other boards but less frequent.",buildapc,2025-08-31 21:03:49,3
AMD,nbt8nsy,"AMD is definitely not dominating gaming since a decade. AMD became competitive before but it's dominating intel around 2019 or so. Intel lost the crowns like 2021 roughly. And I don't talk about server CPUs, I have also mentioned that AMD is superior even in high core count cpus.  It's funny cause everything positive I say about AMD still gets attacked. That's how AMD gets defended, even if they don't get attacked.   Where I live intel is quite an amount cheaper, while offering better all-round performance than similar AMD CPUs.  That's what I said. AMDs old CPUs get quite expensive cause they are so hyped for gaming and Intel's get cheaper and cheaper.",buildapc,2025-09-01 09:58:44,0
AMD,nbt7ifx,"I also never meant you.  And I also never attacked AMD but still all people feel they need to defend AMD. I don't get why this is so extreme with AMD.  I pretty much said, AMD wins every battle, in gaming and in productivity even with the threat rippers.  If you have the money and if you focus on gaming like it seems to be the case in this post, then always go for AMD. I just said intel is not as bad as many people try to convince other people...  Where I live intel is cheaper and I know this is also true for many other regions. And the new intel CPUs are better allrounders than AMDs older gaming CPUs while being cheaper. That was my whole point.",buildapc,2025-09-01 09:48:03,1
AMD,nbq9ol0,"[GamersNexus CPU Comparisons](https://gamersnexus.net/cpus/best-cpus-2024-intel-vs-amd-gaming-production-budget-efficiency)  Feel free to check them out. LTT's benchmarks are pretty consistent with these results. The article's dated to November '24. The April performance boost manage to eke out an extra 7% according to what I've seen, which is still not enough to overturn these results by any significant margin.",buildapc,2025-08-31 21:08:47,4
AMD,nbtcjb1,"Nobody is sayinh that intel is bad, brother. Weâ€™re saying that in recent years, its just not worth its price point.  Also, sorry to hear that amd is expensive in your country. That genuinely sucks to hear",buildapc,2025-09-01 10:34:20,1
AMD,nbqhcp7,"Well thank you, but it's a bit outdated, especially since Intel got a free performance boost. We talk about ram up to 8000+ MHz and free OC tuning to gain extra percentages.  But let's assume that's no real thing because we maybe talk about 5-10% extra performance.  First there is only one game, Baldurs Gate at 1080p which is better because it utilises the CPU more, but it's not really representative overall. The second strange thing: the 14900K which is actually a very powerful CPU in gaming (not as powerful as AMDs top notch CPUs, but still very strong), and pretty close to the 7800x3D falls strangely behind, if I'm not mistaken it's sometimes even better than the 7800x3D while being much more expensive obviously.  If I search for more benchmarks in more games I get very different results, also when I check videos and comparisons.  Anyway let's assume we only have this piece of information... Then we still see the ultra series in the upper midfield or top field. The ultra 9 should have way better values nowadays but the 7 is the great value CPU and it should perform comparable to the 5700x3D which is actually a very decent performance at 1080p. What's the performance uplift here, if it is CPU demanding 10-25% based on the game? Well here comes the important point, that's a big factor. I could take multiple sources, but I found actually one of gamersnexus and if you scroll through the games, you get a better view on the ranked CPUs and in many games they perform even better.  https://gamersnexus.net/cpus/intel-core-ultra-7-265k-cpu-review-benchmarks-vs-285k-245k-7800x3d-7900x-more So let's break that down, you get a CPU, that's maybe 20/25 percent worse in gaming, 75 bucks cheaper but vice versa 15-30% better in productivity while being newer and supporting faster ram and overall it's easy to cool and not massively less efficient than the 7800x3D.   And all of the gaming performance is less important if you play on 1440p or 4K anyway.   For me the intel sounds like a decent option and it's by far not a bad cpu.  Oh and who buys a 7800x3D or even 9800x3d and no GPU capable of playing 1440p or higher - maybe if you want to play super competitively.",buildapc,2025-08-31 21:50:29,1
AMD,nbtdckd,"Not in this comment section, true, but the flame on intel is massive. All I wanted to point out: intels new CPUs can actually be a valid choice if you focus on value and good allrounders. That's all.   I could buy myself an intel for much less than an AMD here and while it might lack the gaming performance, which becomes neglectable with higher resolution, it's very good as a workstation CPU.  That's all.",buildapc,2025-09-01 10:41:31,1
AMD,nbrdv91,Temps,buildapc,2025-09-01 01:07:01,1
AMD,nbrfaqw,"CPU temps at Idle in BIOs are 34(ish),  Windows running at Idle are 48C, Case has 6 intake fans/4 exhaust fans (3 AIO and 1 on rear of case) Internal ambient case temps are 28C",buildapc,2025-09-01 01:15:45,1
AMD,nbrfisj,"Check temps under load. That's what matters most.  After that I'd be doing individual component testing. Cinebench for CPU, Superposition for GPU, CrystalDiskMark for SSDs, Memtest86 for memory. Etc.  If that doesn't help narrow it down, I'd remove the GPU completely and connect the monitor to the motherboard (to run off the iGPU). See if that makes any difference.",buildapc,2025-09-01 01:17:08,1
AMD,nbritzt,"seems kind of warm, did you take off the plastic cover from the coldplate of the AIO?",buildapc,2025-09-01 01:37:52,1
AMD,nbrg8k6,"Thanks, Ill give it a shot, I currently have the onboard GPU running a second monitor as well as my primary monitor on the 9070XT.  But Ill see if i can get temps under load.",buildapc,2025-09-01 01:21:33,1
AMD,nbrprkq,"I did, yes.",buildapc,2025-09-01 02:20:43,1
AMD,nbrggdw,"It's more to completely remove the GPU from the equation. It could be a faulty GPU, or it could be a PSU that is acting up because it can't power the GPU properly (for whatever reason).  When troubleshooting, it's best to eliminate as many variables as possible.",buildapc,2025-09-01 01:22:55,1
AMD,nbq07zh,For 27 dollars grab the 5060 ti it is a no brainer just the raster advantage alone is worth that much,buildapc,2025-08-31 20:20:32,12
AMD,nbq05p7,"They are basically equivalent gpu's, I wouldn't waste the hassle of returning and getting the new gpu.",buildapc,2025-08-31 20:20:12,4
AMD,nbq070w,"5060 Ti 16GB, but I'd just get the cheapest brand",buildapc,2025-08-31 20:20:23,4
AMD,nbquo39,"OC doesn't really matter, you can go into AB and just turn up the clock. Triple fan is more than enough to cool this small chip.",buildapc,2025-08-31 23:09:28,2
AMD,nbq309i,"If microcenter is a reasonable drive for you Iâ€™d go with the 5060 Ti, I think itâ€™s worth the extra $27.",buildapc,2025-08-31 20:34:38,1
AMD,nbqx6nw,For 27$ more the 5060 ti is a no brainer.,buildapc,2025-08-31 23:24:35,1
AMD,nbq2g9u,"It looks like the 5060Ti is marginally faster than the 9060XT in most games, but the 9060XT does chalk up some wins as well. But yes, the $27 price difference here is inconsequential.",buildapc,2025-08-31 20:31:49,3
AMD,nbq1qhe,+ DLSS  Really squeezes extra enjoyable lifespan of a GPU,buildapc,2025-08-31 20:28:15,2
AMD,nbq3fri,"Microcenter is a 14 minute drive from my home, and I'll be only 6 mins from it tomorrow as I am running an errand in that area.",buildapc,2025-08-31 20:36:48,1
AMD,nbq3aw5,[https://www.reddit.com/r/hardware/comments/1l8yden/amd\_radeon\_rx\_9060\_xt\_meta\_review/](https://www.reddit.com/r/hardware/comments/1l8yden/amd_radeon_rx_9060_xt_meta_review/)  yeah based on the meta review the 5060 ti 3-7 percent faster depending on the resolution and about 15 percent faster with rt. Then with pt it can be over double as fast but in most cases not worth using with a 5060 ti unless in lighter titles like doom or maybe CP with the ultra plus mod.  Then ofc better upscaling and no need to mess with optiscaler and better frame gen as well.,buildapc,2025-08-31 20:36:07,3
AMD,nbgjjg8,"[https://pcpartpicker.com/list/K9d6HW](https://pcpartpicker.com/list/K9d6HW)  A bit of improvement. Unless you really need the IO from the X870. In that case I would still bite the bullet and get a tomahawk board. They tend to be the more balanced ones, even if a bit more costly.  Also, Montech century PSUs are quite good. A tier, actually.  As for the rest of the post, unfortunately, I have to agree that nvidia is the better bet. But  linux support is better than it was before. Advise is, get a distro that is already prepackaged for nvida (cachyOS, bazzite I think?)",buildapc,2025-08-30 08:18:27,2
AMD,nbgghoo,"Unfortunately Nvidia is the only real option for consumer AI at the moment due to tensor cores. I say unfortunately because Nvidia drivers kind of suck on Linux. They're not as bad as they used to be, but the experience is far from ideal.",buildapc,2025-08-30 07:49:10,1
AMD,nbggdmk,"Nvidia won't lose you much gaming performance, theyre just more expensive in terms of gaming performance in general. And their drivers, while worse than AMD, are quite solid on Linux now, only very slightly behind. I was running a 3060 Ti on Linux for quite a while until upgrading to the 9070 XT recently. Driver experience is smoother on AMD but Nvidia was still fine. I'm not too sure about AI performance on Amd vs Nvidia though sorry. But anyways looks like a nice build, though probably an overkill cooler for a 9600x",buildapc,2025-08-30 07:48:05,1
AMD,nblst3y,Thanks for the advice!,buildapc,2025-08-31 03:35:53,1
AMD,nbm3lls,Thanks!  Is there a better fit for an air cooler you would recommend?,buildapc,2025-08-31 04:59:58,1
AMD,nbma354,Anything part of Cooler Master's Hyper 212 series is nice. Thermalright has some nice stuff too at low prices. Generally just get any single tower cooler and you should be good. The 9600x is really easy to cool. No need to spend over 30-40 dollars unless you really like the look of one. Also make sure to buy thermal paste if your cooler doesn't come with it,buildapc,2025-08-31 05:56:31,1
AMD,nbsxy2l,Yes  7600x can handle basically any modern GPU at the resolution it would likely be used for,buildapc,2025-09-01 08:15:29,4
AMD,nbsz3lk,Ye that's a great combo for 1440p,buildapc,2025-09-01 08:26:44,3
AMD,nbtjst1,"That's more than enough.  I'm still using my 5600X with a 5070 ti. Perfectly fine, as long as I don't want to do stable 120+ fps gaming (which I never cared about).",buildapc,2025-09-01 11:33:55,1
AMD,nbu2o07,Good combo.,buildapc,2025-09-01 13:35:18,1
AMD,nbd0hb3,A 5060 will run fine in this system. Any higher might be stretching it and won't fit your budget.,buildapc,2025-08-29 18:28:35,5
AMD,nbd1fel,"Maybe u can get a 4060 or atleast a 3060 used, for amd u can maybe try to find a 6650xt or 6750xt used, but those need a bit more power so i can't say for sure since u didn't list ur psu",buildapc,2025-08-29 18:33:18,2
AMD,nbefgho,"Okay i could be wrong here ,but hear me out... - the 6800xt is a card from half a decade ago maybe 2020 I guess okay ! .. - And its been taking hits on Nvidia 3080 for ages okay .. - and its been tested firmly by alot of gamers nowadays and streamers including "" Zworms"" on many games that came this year, considering its an old gpu and it ran pretty damn good on alot of games . Including cyberpunk , Rdr2, and battlefield 6 at 1440p high settings running at 60fps or more on certain titles . -  which brings us to ""This GPU is still 1440p monster and even at 1080p is still overkill"" - only problem being you find a good deal on it , i only suggested it cause last week I saw a great deal for it on ebay, was around 370 cad ,so yea its a stretch. But hope you fine a better deal with even a 7000 card or Nvidia  - GOOD LUCK BRO",buildapc,2025-08-29 22:56:19,2
AMD,nbd2enp,"5060 is not worth getting for its price, unfortunately you might not be able to find a gpu with 12gb or more, but i think the best price to performance for $300 would be the [9060 XT](https://www.bestbuy.com/product/gigabyte-radeon-rx-9060-xt-gaming-oc-8g-gddr6-pci-express-5-0-graphics-card-black/J3ZW9X7XQX/sku/6633167?ref=212&loc=22883127098&gclsrc=aw.ds&gad_source=1&gad_campaignid=22883127098&gbraid=0AAAAAD-ORIhJ5AlmUGZRfR9jXM7_kr6rd&gclid=Cj0KCQjwn8XFBhCxARIsAMyH8Bvi0EhyLo08x6Mwsfo6BRwr5kAmpKudprDO4tm9zeUgLJgyqr9dnVsaAi4IEALw_wcB). My recommendation is to save a little bit more and upgrade to the [16gb 9060 XT](https://www.newegg.com/xfx-swift-rx-96tsw16bq-radeon-rx-9060-xt-16gb-graphics-card-double-fans/p/N82E16814150914?item=N82E16814150914&source=googleshopping&utm_source=google&utm_medium=paid+shopping&utm_campaign=knc-googleadwords-mobile-_-pla-_-video+cards+-+amd%2fati-_-N82E16814150914&id0=Google&id1=19147136377&id2=150981347704&id3=&id4=&id5=pla-2427911115116&id6=&id7=9031950&id8=&id9=g&id10=m&id11=&id12=Cj0KCQjwn8XFBhCxARIsAMyH8Bts8nhm-16jhdZuQUv4x_svGlIeoCippAj4uSQmB6VrW8ZZz2T7t7caAjwsEALw_wcB&id13=Y&id14=Y&id15=&id16=639366582949&id17=&id18=&id19=&id20=&id21=pla&id22=8438988&id23=online&id24=N82E16814150914&id25=US&id26=2427911115116&id27=&id28=&id29=&id30=4738762246540646711&id31=en&id32=&id33=&id34=&gclsrc=aw.ds&gad_source=1&gad_campaignid=19147136377&gbraid=0AAAAAD-YhmMkwGaUJZqiagiRPlEq1EP5t&gclid=Cj0KCQjwn8XFBhCxARIsAMyH8Bts8nhm-16jhdZuQUv4x_svGlIeoCippAj4uSQmB6VrW8ZZz2T7t7caAjwsEALw_wcB)",buildapc,2025-08-29 18:38:16,1
AMD,nbd85m9,personally id get a 4060 open box/used off of ebay,buildapc,2025-08-29 19:06:55,1
AMD,nbdbe9h,I have a 600 w psu btw,buildapc,2025-08-29 19:23:14,1
AMD,nbg74v1,"Not much info.  If I'm not mistaken $300 CAD is about \~$210-220 USD.  Buying new you don't have many choices, Radeon RX 6600 would be my choice. Maybe Arc B570 if you can find for that price.  Used you can maybe get 3060 Ti (I'm not familiar with ebay prices), that would be the best.",buildapc,2025-08-30 06:20:38,1
AMD,nbd3lww,"Misread the post and just saw you put 300 cad, in that case iâ€™d go with the [3060 ti](https://www.jawa.gg/product/91202/gigabyte-rtx-3060-ti-gaming-oc?utm_campaign=share-listing&utm_source=jawa&utm_medium=website&utm_content=share-button), I love that card, was my previous card before upgrading",buildapc,2025-08-29 18:44:18,3
AMD,nbdpdmt,$300 i would say either 7600 xt or 5060,buildapc,2025-08-29 20:34:06,2
AMD,nbdcloo,Used or new?,buildapc,2025-08-29 19:29:22,1
AMD,nbfctmt,"Used 3060 ti, you can find a few now at that price point",buildapc,2025-08-30 02:23:14,1
AMD,nbv0sqo,"Get a b850 instead for pcie 5.0.    PSU is fine.  Just about *any* modern case that isn't a solid wall in the front panel is going to cool just as good as another.  That's an expensive case I would not pay for unless you're set on looks.  Get CL30 RAM.  No compromises.  L3 cache is NOT the same as having faster RAM.  While L3 *mitigates* slower RAM, RAM is still the bulk of your storage and why reduce the effectiveness of x3d with slower RAM to bottleneck transient events?  Don't let people tell you otherwise, literally you're saying maybe a few bucks for nothing.  Team Create Expert is cl30 6000 and while it's not RGB is faster.",buildapc,2025-09-01 16:27:14,1
AMD,nbv1cf2,"I'd change the case to the lian li 217, or watch gamers nexus review of the meshify 3 and look at the charts as they include most popular cases   Going to B850 etc gets you pcie5  5080 you want at least 850w. The montech century II 1050w is an A rated psu and like $110 instead of $180  The case and psu change brings the list under $2200",buildapc,2025-09-01 16:29:52,1
AMD,nbv1lpv,"About the case, the torrent is simply the best in airflow. Because of this, it's also extremely expensive. It's honestly not worth it unless you're an airflow nerd and want to brag about having literally the best case. Any mesh case will be fine and way cheaper, especially Phanteks or Lian Li",buildapc,2025-09-01 16:31:06,1
AMD,nbv2fwd,KLEVV ddr5 6000mhz cl30 can be found at less price along with better latency than the corsair on Amazon (CRAS V rgb version for 101.99 usd and FIT V nonrgb for 86.99 usd). They are also guaranteed Hynix A Die (best module as of now) since KLEVV is brand name for SK Hynix. You might also want to consider RAM clearance if you plan on using dual tower cpu cooler and FIT V model is almost the stock height.,buildapc,2025-09-01 16:35:12,1
AMD,nbv6zz6,"[PCPartPicker Part List](https://pcpartpicker.com/list/4nK2yW)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 9800X3D 4.7 GHz 8-Core Processor](https://pcpartpicker.com/product/fPyH99/amd-ryzen-7-9800x3d-47-ghz-8-core-processor-100-1000001084wof) | $472.02 @ B&H  **CPU Cooler** | [Thermalright Phantom Spirit 120 SE ARGB 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/MzMMnQ/thermalright-phantom-spirit-120-se-argb-6617-cfm-cpu-cooler-ps120se-argb) | $37.90 @ Amazon  **Motherboard** | [MSI MAG B850 TOMAHAWK MAX WIFI ATX AM5 Motherboard](https://pcpartpicker.com/product/vjpD4D/msi-mag-b850-tomahawk-max-wifi-atx-am5-motherboard-mag-b850-tomahawk-max-wifi) | $209.49 @ Amazon  **Memory** | [\*Silicon Power Value Gaming 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/cCKscf/silicon-power-value-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdeae) | $84.97 @ Newegg Sellers  **Storage** | [Samsung 990 Pro 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/34ytt6/samsung-990-pro-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-mz-v9p2t0bw) | $159.99 @ Abt  **Video Card** | [PNY OC GeForce RTX 5080 16 GB Video Card](https://pcpartpicker.com/product/7z7MnQ/pny-oc-geforce-rtx-5080-16-gb-video-card-vcg508016tfxpb1-o) | $999.99 @ Amazon  **Case** | [Fractal Design Torrent RGB ATX Mid Tower Case](https://pcpartpicker.com/product/Y9RYcf/fractal-design-torrent-rgb-atx-mid-tower-case-fd-c-tor1a-04) | $245.98 @ Newegg  **Power Supply** | [Montech CENTURY II 1050 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/yJkqqs/montech-century-ii-1050-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-1050w) | $109.90 @ Newegg   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$2320.24**  | \*Lowest price parts chosen from parametric criteria |  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-01 12:57 EDT-0400 |",buildapc,2025-09-01 16:57:19,1
AMD,nbmobkp,5600G is enough for 6600 and you won't get any upgrade with a new CPU(few % max).  You could sell your CPU-MBO-RAM for 160$ or so(idk really) and then when you sell it add the money to your 200$ and get a new AM5 platform for example on Newegg 7600+Gigabyte Eagle B650+32GB 6000MT DDR5 is 415$ or get a bundle on Ebay for 350$ with the same specs. With that you could be future proof when you get a new GPU later.  Or just save money as it is better than shelling 160$ for 5800X for example,buildapc,2025-08-31 08:11:02,4
AMD,nbmmmae,Nothing really worthwhile price/performance wise to upgrade to anymore tbh. Especially when using a 6600.,buildapc,2025-08-31 07:54:22,2
AMD,nbmlk3e,Do you have a gpu?,buildapc,2025-08-31 07:43:59,1
AMD,nbn0nkd,"Oh this might actually work, Ill think about selling my things to upgrade to am 5.",buildapc,2025-08-31 10:12:29,1
AMD,nbmn2kc,Oh,buildapc,2025-08-31 07:58:50,1
AMD,nbmm15u,SorryI forgot to put but its Rx 6600,buildapc,2025-08-31 07:48:38,1
AMD,nbn5paa,"Nice, good to know your options and how you feel about it depends on you. A year or so after you can sell 6600 for a bit over 100$ and update that for 100$ more to something 50% stronger.",buildapc,2025-08-31 10:59:35,1
AMD,nbmmni7,"No idea about US prices, but you should be able to choose any 5000 series processor.",buildapc,2025-08-31 07:54:41,0
AMD,nbqf52z,Needs more RGB,pcmasterrace,2025-08-31 21:38:25,1
AMD,nbujneb,"$400 is about part cost. Itâ€™s a fair deal but not a great one, maybe $350?  Itâ€™s not a beast though. But I think itâ€™s still worth it. Youâ€™ll have to ship of Theseus that thing though, meaningful CPU upgrades will also require a change of motherboard + RAM.Â   I wouldnâ€™t upgrade it Iâ€™d use it.",pcmasterrace,2025-09-01 15:03:28,2
AMD,nbukbne,"Ohh whatâ€™s the point of I have to change mobo and rams that will mean I have to change the cpu too??  Should I go for it? The pc works fine I have tested it.  Should I ask him to lower the price too? Or itâ€™s fine priced he said he bought it for 1200 from Costco last year., Edit; my bad okay I understand thanks",pcmasterrace,2025-09-01 15:06:49,1
AMD,nbul4vu,"Just because someone else overpaid for a prebuilt doesnâ€™t mean you should. Check /r/suggestapc if you want something new. Costco can have good deals, but last year, $1200 USD for a PC like that wasnâ€™t a good deal. $400 is though.",pcmasterrace,2025-09-01 15:10:49,2
AMD,nam642h,This is how I wanted my acid trips to look,pcmasterrace,2025-08-25 17:16:52,2
AMD,nal63k2,"I would go back to the driver you had installed previously.  Thatâ€™s a crazy effect, too bad you canâ€™t print screen glitches and make it into a background. I love it, and also sorry you are having this problem.",pcmasterrace,2025-08-25 14:18:59,1
AMD,nan25f5,Itâ€™s beautiful,pcmasterrace,2025-08-25 19:51:34,1
AMD,nbnc0dg,"How long did you wait after turning it on? Memory training can take some minutes. During that time, you wont see anything",pcmasterrace,2025-08-31 11:50:55,2
AMD,nbk8apq,Did you say you were using the motherboard displayport?  Try plugging it into the GPU port,pcmasterrace,2025-08-30 21:42:59,1
AMD,nbk97yt,any led indicators or error code on the motherboard when you boot the pc? my mate built pc with same board and 9700x,pcmasterrace,2025-08-30 21:48:09,1
AMD,nbl0zf7,"Did you follow the manual?   And just to be sure, did you connect the 8 pin and the 24 pin ATX power to the motherboard?",pcmasterrace,2025-08-31 00:36:45,1
AMD,nbnnj1u,"Good reminder, especially with DDR5, but I waited a long time. I am pretty sure it's a motherboard problem, unfortunately.",pcmasterrace,2025-08-31 13:08:31,1
AMD,nbk90y0,"Also tried the GPU HDMI port, no joy.",pcmasterrace,2025-08-30 21:47:04,1
AMD,nbk9o3w,Have you tried reseating the RAM? Blow into the slots in case there's any debris in there too.,pcmasterrace,2025-08-30 21:50:36,2
AMD,nbkaoad,"Many times, thanks for answering.",pcmasterrace,2025-08-30 21:56:11,1
AMD,nbkbg7x,"Welp, sorry I can't help much. The only other thing I could think of is making sure the GPU is seated properly and the power cable is pushed all the way in, otherwise reseat the cpu and check for pin damage. If the diagnostic LEDs are displaying anything, it might be worth googling what it means.",pcmasterrace,2025-08-30 22:00:35,2
AMD,nbnpgxn,Thanks for your thoughts on this. At this point I am pretty sure it's a motherboard issue.,pcmasterrace,2025-08-31 13:20:08,1
AMD,nbu1ejz,"Great machine, I think we have a similar case in terms of space... Which case is yours? Mine supports a maximum of 310mm of VGA. I don't know if yours is around, it doesn't look very big. Congratulations.",pcmasterrace,2025-09-01 13:28:08,2
AMD,nbucqia,"Well, if you still want to continue with the AM4, consider switching to a 5700X3D if your focus is on GAMES, because you are leaving a lot of FPS behind. Here is a comparative video. [https://www.youtube.com/watch?v=MHr4jqe01U0](https://www.youtube.com/watch?v=MHr4jqe01U0)",pcmasterrace,2025-09-01 14:29:06,1
AMD,nbu01j0,Kinda funny to get a 5080 for 1440p. Nice build though. How much fps you get on cyberpunk 2077 with path tracing ? With frame gen and DLSS,pcmasterrace,2025-09-01 13:20:22,1
AMD,nbu1mlb,It's Corsair 4000D AIRFLOW. This is the old one so around 2020 I think. There is a new one that's bigger.  Thank You,pcmasterrace,2025-09-01 13:29:24,1
AMD,nbudjym,"I stream on OBS and occasionally do discord calls while streaming.  I am waiting for 9950x3d sale for complete upgrade but for now 5900x has been serving well with extra cores. Having said I want to stream 60+ fps so extra frames I don't mind missing out since youtube goes wacky with stream 60+ fps.   Thank you for the suggestion but as for now with the setup I have, it will will serve me.",pcmasterrace,2025-09-01 14:33:14,1
AMD,nbu1790,Didn't want to spend $2k on AM5 with new GPU and wasn't done with AM4. I do only play on 1440p so it's more than enough for me. DLSS Quality looks beautiful  All settings Maxed out Path Tracing and Ray Tracing  DLSS Quality 65.80 FPS  DLAA: 38.52  I did full benchmarking here https://youtu.be/tNREKH7TJ_U?si=LDej-epcnd8ynD5g,pcmasterrace,2025-09-01 13:26:59,3
AMD,nbu1p27,Solid performance. Have fun bro,pcmasterrace,2025-09-01 13:29:47,2
AMD,nbu1qhr,Thank You,pcmasterrace,2025-09-01 13:30:00,1
AMD,nbhd1so,"Swapping RAM is a bad idea. DDR4 is overpriced since we aren't getting any more stock. Also, you can probably just clock up your RAM manually with some trial and error (e.g. I got a 3000/15 kit and clocked it to 3600/16).  As for the GPU: An RX 9070 should be around 30% faster than a 3080 on average. For Cyberpunk specifically, the RX 9070 should even be close to 50% faster. And you'd save some money due to lower consumption on top of that (usually \~2-3ct/h gaming, which won't make a big difference at that cost, but it is something to keep in mind).",pcmasterrace,2025-08-30 12:36:16,3
AMD,nbkicu3,">My idea is to upgrade the GPU to a RX 9070XT (most important factor being 16Gb VRAM)  a RX 9070XT would be about a 40% improvement in 1440p it's in the range where you really fell the boost.  is it worth is another question, up to you to know if 650-700â‚¬ for a 40% lift is enough.  in your own exemple of recommended spec you can play at 1440p High with a 3060 Ti and DLSS balance so the 3080 a still lots of life for that resolution and I would keep it. The only reason I changed mine is because I brought a 4K screen in the mean time.  >and to swap my RAM for a DDR4-3600 CL18 (32Gb) kit.  I would not invest in DDR4 ram at that point, better to keep the money aside for the next CPU/RAM/MB update.",pcmasterrace,2025-08-30 22:40:47,2
AMD,nbn5us9,I wouldn't. Save more money for a completely new platform (AM5) and new generation of GPUs. This PC is good enough for at least 2 years still. Games you listed run well enough on your PC.,pcmasterrace,2025-08-31 11:00:53,2
AMD,nbgothd,Extra info: this would cost me â‚¬900 at todays prices,pcmasterrace,2025-08-30 09:10:30,1
AMD,nbosxmn,"Personally I probably wouldn't move from 3080 to 9070XT - while it is an upgrade, I don't think it's good enough upgrade to justify current pricing.  I agree what someone else here said, that saving up for a new platform (AM5 - to also upgrade RAM from 16 to at least 32GB) would be wiser for now as I have RTX 3080 12GB and it works fairly well for 4K (obviously not ultra on everything, but generally even medium or high looks great these days + you can always use DLSS) and then maybe wait for Nvidia Super cards or one more generation if the GPU is working for you. (it's not like when I had to move from GTX1070 because new games were dropping support for it completely and drivers as well later this year)",pcmasterrace,2025-08-31 16:43:24,1
AMD,nbhlfy5,"Thank you for the response.  It's about â‚¬140 for the RAM kit (Kingston Fury). Wouldn't it be a substantial upgrade going from 16Gb to 32Gb? I see a lot of games switching to 16Gb minimum and 32Gb recommended.  30% to 50% increase in some titles seems like a decent improvement, better power efficiency is a very nice bonus! Despite still having good framerate on my 3080, I think the 10Gb is not gonna be enough sooner than later at 1440p.  The 5800X3D is a beast and doesn't fall far behind the newer flagship AMD CPU's, so I'm very comfortable staying on AM4.",pcmasterrace,2025-08-30 13:28:36,1
AMD,nbn4ho9,">Â overpriced since we aren't getting any more stock.  Not true, samsung restarted production as the demand is still high.",pcmasterrace,2025-08-31 10:48:51,1
AMD,nbmqg4e,"Thanks for the response!  It's nice to have an idea what the idea % wise would be. From what I can find in benchmarks and what people are saying it falls somewhere between 30-50% depending on the game.  I'm aware that the 3080 still is a good card nowadays, the only 'annoyance' is the 10Gb of Vram (Nvidia kinda fucked up there and made the 3080 12Gb later on).  I'm playing the new update in PoE2 and put my metrics on so I can see the amount of load and Vram usage. If I run into no issues and have some headroom I will keep it. Will do the same thing once BF6 launches or when playing other demanding games.  Cheers",pcmasterrace,2025-08-31 08:31:39,1
AMD,nbn9xhr,Yes they do run well even on high/ultra still. Maybe I'll even make it to AM6 and UDNA could be amazing if AMD starts competing again in the high end GPU segment. I'm keeping an eye on my Vram usage.,pcmasterrace,2025-08-31 11:34:44,1
AMD,nbp1hxc,"Aye that's what I figured by now. So far highest Vram usage I've seen in PoE2 act 1 is 5,9Gb so all good still.  Might even last me even longer past AM5. We have a very similar setup, only difference is the 2Gb of Vram.",pcmasterrace,2025-08-31 17:24:50,1
AMD,nbhovzd,">It's about â‚¬140 for the RAM kit (Kingston Fury). Wouldn't it be a substantial upgrade going from 16Gb to 32Gb? I see a lot of games switching to 16Gb minimum and 32Gb recommended.  You can always free up your Windows install and remove bloat. Minimum for Win 11 is like 2-4 GB base RAM usage, so you should have enough left for gaming. Really only becomes an issue when you do stuff like open Chrome with dozens of active tabs and have like 12GB base RAM usage before even opening the game.  140â‚¬ is just a horrible price for outdated RAM. I got my 32GB kit (which is running at 3600/16 right now) for \~110â‚¬ 6 years ago.  >The 5800X3D is a beast and doesn't fall far behind the newer flagship AMD CPU's, so I'm very comfortable staying on AM4.  Well, 5800X3D is around 7600X performance in most games. Maybe slightly better whenever the 3D-cache matters. Usually, you get \~10% per gen, but just the one gen step to the 7800X3D is already 20% faster.  There certainly is no big reason to upgrade right now, but it also makes no sense to get new DDR4 RAM. Realistically, you could just sell RAM+MoBo+CPU and get like 300-400â‚¬ for it. Which you can then use to get an AM5 system with a 9600X or so. You might even be able to get a 7800X3D system if we include those 140â‚¬ that you would be willing to spend on new RAM. So imo IF you want an upgrade for the RAM, just upgrade the whole thing.",pcmasterrace,2025-08-30 13:48:54,1
AMD,nbiunzi,The X3D chips don't really care about memory speeds as much as other AMD/recent Intel chips.  You could just drop in another 16GB kit or go for a cheaper 2x16GB kit that's 3200MHz.  Another Redditor in r/AMD tested it and saw only a 1.8% increase in **synthetic** benchmarks with overclocked and tuned memory. I guarantee real world results will be even less.,pcmasterrace,2025-08-30 17:23:02,1
AMD,nbn7zqr,They just extended the deadline. Others producers like Micron have already stopped.,pcmasterrace,2025-08-31 11:18:57,1
AMD,nbmucpg,">I'm aware that the 3080 still is a good card nowadays, the only 'annoyance' is the 10Gb of Vram (Nvidia kinda fucked up there and made the 3080 12Gb later on).  people talk about that way more than games actually need it doubly so in 1440p not ultra.",pcmasterrace,2025-08-31 09:10:13,1
AMD,nbhpryd,"My 5700 xt isn't supposed to run the new Indy game, but thanks to the new raytracing emulator introduced to the Linux drivers, I can play it anyways at some low to medium mixed settings and it manages a fairly stable [1080p@60](mailto:1080p@60) (no framegen). Dunno if the new DOOM game also works but it still is cool.",pcmasterrace,2025-08-30 13:54:01,87
AMD,nbi01uq,"Benefit of having one unified driver project and open source drivers.  even if the manufacturer gives up on it, people have a choice to keep it going if they have the knowledge to do so.",pcmasterrace,2025-08-30 14:50:03,37
AMD,nbhhjn9,"R300? That's the legendary Radeon 9700 Pro isn't it? One of the legendary GPUs.   Also it's not 20 years old, it's 23 years. Almost a quarter of a century.",pcmasterrace,2025-08-30 13:04:49,36
AMD,nbhj93a,https://preview.redd.it/35v22irvp5mf1.jpeg?width=640&format=pjpg&auto=webp&s=12864e45f134c854523f71005ea21f5f8e8f7cfe,pcmasterrace,2025-08-30 13:15:20,55
AMD,nbhibfs,Nothing out of the ordinary in the Linux community.,pcmasterrace,2025-08-30 13:09:37,21
AMD,nbkiqu9,Thats what makes linux community great.   Supporting old hardware and give it the longest life as possible.  I only wish this open source drivers could be back ported to windows.,pcmasterrace,2025-08-30 22:43:07,5
AMD,nbjo21n,And here I am with vega 56 which still rocks and even supports rocm.,pcmasterrace,2025-08-30 19:53:52,1
AMD,nbiygnu,"Yes, Linux support for old GPUs is nice... unless you have an old Nvidia card (old like Fermi) :P",pcmasterrace,2025-08-30 17:41:51,1
AMD,nbjcms7,"Meanwhile, I still have bitter memories of having to replace three Radeon cards (in 3 computers) that were all of six years old because AMD decided to stop doing drivers for them.  Never bought another Radeon again and I never will.",pcmasterrace,2025-08-30 18:53:35,-8
AMD,nbik1qs,What game ?,pcmasterrace,2025-08-30 16:30:31,14
AMD,nbnbn01,Have you ever tried running a Linux program compiled 25 years ago? Good luck with that.,pcmasterrace,2025-08-31 11:48:08,4
AMD,nbn4udu,"i wish windows was that simple. the reality is very old stuff is a pain to run on modern computers, particularly if it had hardware requirements. like requiring glide.",pcmasterrace,2025-08-31 10:52:00,0
AMD,nblf2mr,Can you at least get it to cook an egg in Linux?,pcmasterrace,2025-08-31 02:04:13,0
AMD,nbik99u,The new Indiana Jones game,pcmasterrace,2025-08-30 16:31:33,19
AMD,nbkqx6d,Time comes for us all,pcmasterrace,2025-08-30 23:32:45,2
AMD,nbimdu0,Oh my dumbass thought you meant a indie game not the new indiana jones game yeah that makes more sense now.,pcmasterrace,2025-08-30 16:42:12,34
AMD,nbq99rt,Saw it after a session of oblivion remastered. Refreshed hwinfo ran cinebench but couldnâ€™t recreate it no throttling from cinebench,pcmasterrace,2025-08-31 21:06:36,1
AMD,nbqebk2,"The author of HWiNFO answered, [""If only PROCHOT EXT is asserted, it means it wasn't the CPU that is overheating, but some other component (most likely the SIO or Embedded Controller) that issued this signal. If all temperatures are OK, then it might be caused either by some very short spike that wasn't caught by HWiNFO or some bug on the mainboard (BIOS/firmware, sensor).""](https://www.hwinfo.com/forum/threads/r5-3600-regarding-prochot-cpu-ext.6151/#post-23483)",pcmasterrace,2025-08-31 21:33:56,1
AMD,nazy6rd,Psu?,pcmasterrace,2025-08-27 19:01:07,3
AMD,nb0hjrn,"if its constant and chronic and nothing seems to help, theres a chance the card is faulty.",pcmasterrace,2025-08-27 20:31:57,1
AMD,nb36edy,Fans full speed black monitor = PSU. Constantly crashing while gaming aswell. Fixed my issue when I replaced my psu never happened again,pcmasterrace,2025-08-28 06:28:58,1
AMD,nb25k2k,Turn off freesync,pcmasterrace,2025-08-28 01:56:07,0
AMD,nb7ykkn,"But not all games crash. What causes the crashes if itâ€™s PSU, does the power draw get too high? How can I test this",pcmasterrace,2025-08-28 23:06:39,1
AMD,nb7yl8q,"But not all games crash. What causes the crashes if itâ€™s PSU, does the power draw get too high? How can I test this",pcmasterrace,2025-08-28 23:06:45,1
AMD,nb9gqdf,Process of elimination really,pcmasterrace,2025-08-29 04:48:01,1
AMD,nbcut7b,Sick build man I'm loving that case you went with!,pcmasterrace,2025-08-29 18:00:31,6
AMD,nbengai,Looks neat,pcmasterrace,2025-08-29 23:43:24,3
AMD,nbf24kf,Effing CLEAN and strong. Well done.,pcmasterrace,2025-08-30 01:13:54,3
AMD,nbcv2yj,ðŸ”¥,pcmasterrace,2025-08-29 18:01:50,2
AMD,nbe1p5v,Pretty slick. I like your fans and cooler choice,pcmasterrace,2025-08-29 21:37:46,2
AMD,nbfqwer,wow! love the case,pcmasterrace,2025-08-30 04:02:40,2
AMD,nbfwrz9,Nice bro,pcmasterrace,2025-08-30 04:49:52,2
AMD,nbh21zx,"I love the case, the little slant on the bottom is awesome for the design!",pcmasterrace,2025-08-30 11:15:38,2
AMD,nbd3vlw,"Thatâ€™s a sick case, now I want one! Is it comfortable and practical to build a PC in? Would you recommend it?""",pcmasterrace,2025-08-29 18:45:40,1
AMD,nbeav8t,"People often overlook the TUF cards cause Asus makes the STRIX line, but they are HUGE and can pack a punch. Not as flashy, but definitely high quality (IMO)     Source: I also own Asus TUF cards",pcmasterrace,2025-08-29 22:29:12,1
AMD,nbf1vcy,"https://preview.redd.it/k4ifi1ae42mf1.jpeg?width=4284&format=pjpg&auto=webp&s=d54ee1c2fe580db92a46bf30fa910e00b6448f42  V100 brothers! I originally wanted the black case but it wasnâ€™t in stock, love your build!",pcmasterrace,2025-08-30 01:12:17,1
AMD,nbg4987,My dick moved,pcmasterrace,2025-08-30 05:54:26,1
AMD,nbgin6f,"Good first build, probably 1440p ultra",pcmasterrace,2025-08-30 08:09:46,1
AMD,nbi74ju,not sure about airflow but nice otherwise,pcmasterrace,2025-08-30 15:26:00,1
AMD,nbcx5to,Yep i searched a lot to find this. I loved this when I saw this on computex 2025. Since then i was searching everywhere and I found a seller who ordered this for me. I got this in the first shipment of this case.,pcmasterrace,2025-08-29 18:12:08,1
AMD,nbfqe5a,It is very practical case. It is small yet it can support TUF gpu along with 2 bottom fans. Only limitation is you can install only one SATA. like all other lian li case build quality is good.,pcmasterrace,2025-08-30 03:58:52,1
AMD,nbfmv58,"There is virtually no performance difference between STRIX and TUF...it's all rgb crap and supposed slightly better components, which i doubt. It's marketing markup for people who need to flex dollars spent.",pcmasterrace,2025-08-30 03:32:39,1
AMD,nbfr43c,I love white build.  Is it hard to maintain because i am seeing lots of dust in black. Because it is black it is not that visible. What about white build?,pcmasterrace,2025-08-30 04:04:20,1
AMD,nbgl1tn,I think you have to see a doctor ðŸ¤£.,pcmasterrace,2025-08-30 08:33:14,2
AMD,nbgj3ur,"Nope, runs 4k ultra like a charm. No ray tracing though.  Cyber punk - 110+ fps  COD MW3 - 160+ fps  wuchang fallen feathers  - 90+ fps",pcmasterrace,2025-08-30 08:14:16,1
AMD,nbit2og,I have tested by running synthetic benchmark on both cpu & gpu at same time. At that time i saw power consumption goes to 810w. I ran the test for 20.min thermal was stable. Gpu is maintaining 64 - 70'c.   For cpu i have enabled PBO @ 85'c so it won't go beyond 85. I must mention that I got lucky my silicon is running at -25mv undervolt. It is silicon lottery. So some one have to confirm the temp for stock setup.  This case support upto 9 fans. So airflow won't be a problem.,pcmasterrace,2025-08-30 17:15:20,1
AMD,nbfqtbe,I think TUF has bigger heatsink. Which will help in long gamming sessions. Mine consumes 380w in 4K gamming and stayed 61'C for 3 hr session.,pcmasterrace,2025-08-30 04:02:02,1
AMD,nbmznjx,What does Windows Reliability Monitor say? You might see some errors being reported there.,pcmasterrace,2025-08-31 10:02:57,1
AMD,nbpqj2d,"I would recommend sticking with NVIDIA graphics cards, as they are way better than any competing products. Look at the GeForce RTX 5070 Ti. That is about the best for the price range you're looking for. With NVIDIA you have DLSS, Multi Frame Generation, G-Sync, and other features that you simply can't get on other brand cards.",pcmasterrace,2025-08-31 19:30:09,1
AMD,nbqtjg1,Radeon RX 7900 XTX has been discontinued.  Get Radeon RX 9070 XT,pcmasterrace,2025-08-31 23:02:24,1
AMD,nbg2qi2,"PC builder and technician from Argentina here, keep in mind that most of the parts to build a nice PC aren't as cheap as you guys think here in SA, for example my setup which is in my flair would easily cost 1000 USD or more converted from my currency (Arg. pesos)   Aside that we use payments and promotions when we get new hardware and most of the times people want a computer at a budget and as of now the upgrade combo that is so popular here at least in Arg. is an AM4 one, the famous A320 chipset motherboard and 5600G that later gets upgraded to a B550 motheboard and a better 5000 series chip (I didn't went this route but I helped some friends to upgrade)  So kudos for AMD to give us a last hand when it comes to an almost 10 year old socket, hope AM5 recieves the same treatment down the line",pcmasterrace,2025-08-30 05:40:48,177
AMD,nbg7nub,"My friend, there is *still* a very active market for LGA1150 and LGA2011-3, Intel's Haswell and Broadwell generations. You can buy new motherboards with all manner of cool things. Eight channel memory and two CPUs? Cheap as chips. You can build a fully loaded 28 core production powerhouse for less than Â£â‚¬$200.  This is being driven because there are just *so many* Haswell/Broadwell Xeons out there being ""recycled"".  Similarly, there are so many AM4 CPUs out there that it'll still be being used well into the 2030s.",pcmasterrace,2025-08-30 06:25:32,55
AMD,nbg612p,"I just did my first build today, on an AM4 socket.  Ryzen 5600 paired with a Rx 9060 xt. Huge change from my previous Ryzen 3600 and GTX1650.",pcmasterrace,2025-08-30 06:10:32,22
AMD,nbg4n05,ryzen 3 5200x3d and 5300x3d coming soon 2026** (only available on Bermuda Triangle region),pcmasterrace,2025-08-30 05:57:51,38
AMD,nbg40xk,ppl still playing with a ryzen 1600 or below and ppl saying this is dead? lol this is a great value cpu.,pcmasterrace,2025-08-30 05:52:20,16
AMD,nbg34ex,Still on 5700x and don't see myself upgrading soon.,pcmasterrace,2025-08-30 05:44:15,9
AMD,nbfzun9,"It's only available in certain parts of South America, and it's not a new architecture. AM4 has essentially been dead for years. AMD is just getting rid of sub-par silicon by ~~throwing some extra cache on it~~ disabling cores, lowering the frequency, and selling it instead of throwing it away.   To be clear, I'm not saying this is a bad thing. I'm just countering the claims that I keep seeing about AMD continuing to support the AM4 socket. These aren't improvements.",pcmasterrace,2025-08-30 05:15:48,60
AMD,nbg0ob8,its fairly close to dead. the 5700x3d was keeping it alive.,pcmasterrace,2025-08-30 05:22:55,28
AMD,nbg2ovg,Still using 3900x,pcmasterrace,2025-08-30 05:40:24,3
AMD,nbhqznm,"If building from scratch, I would just get am5. But I love the longevity of am4. Back before COVID, I bought a 3600+2060 which I upgraded half a year ago to a 5700x3d+ 7800xt. The fact that an almost decade old generation is still getting updates is amazing.",pcmasterrace,2025-08-30 14:00:59,3
AMD,nbgm07u,AM4 will never die! Only the actors that portray it.,pcmasterrace,2025-08-30 08:42:33,5
AMD,nbge122,"I'm in latam is not available here in my country, anyway, I build a brand new pc with the 5700X3D during amazon black friday 2024",pcmasterrace,2025-08-30 07:25:16,2
AMD,nbh22x7,LEGENDS NEVER DIEEEEEE,pcmasterrace,2025-08-30 11:15:50,2
AMD,nbh2iuc,"My Ryzen 7 7800x3d looking at my FX 8350Â    ""_chuckles_ im in danger :)""",pcmasterrace,2025-08-30 11:19:27,2
AMD,nbh41rv,omg this is insane! good job amd!,pcmasterrace,2025-08-30 11:31:49,2
AMD,nbh4mi8,I hope am5 will have the same future,pcmasterrace,2025-08-30 11:36:24,2
AMD,nbhh563,So evil for the CPU designers. Being stuck on a 10 year old platform is tough for those engineers.   Really glad they're still doing it anyway. Intel would have quit 8 years ago...,pcmasterrace,2025-08-30 13:02:21,2
AMD,nbi2rbv,what about am6,pcmasterrace,2025-08-30 15:03:55,2
AMD,nbkniys,"I'm sitting here stressing wondering how/when I am going to get all of my kids and my wife from AM4 to AM5. Nah, I think I can chill. Probably will upgrade my Wife though.",pcmasterrace,2025-08-30 23:11:58,2
AMD,nbg2xrq,"Iâ€™m confused. The 5500 was a 5600G without an igpu, so it was basically a neutered 5600 because it had half the L3 cache (16MB vs 32MB) but if they have an X3D version of it is it still limited to PCIE 3? Does it have the full 96MB of L3 cache that all other X3D chips have?",pcmasterrace,2025-08-30 05:42:36,3
AMD,nbgot8z,"AM4 has quite a upgrade path, like i upgraded from r5 2600 to r9 5900x since i needed more cores",pcmasterrace,2025-08-30 09:10:26,3
AMD,nbgd3un,"It's dead as in not being developed anymore, but considering the higher end AM4 cpus are plenty capable still, it's relevant to a lot of people, especially those with older AM4 cpus who can just upgrade their setup to the higher end AM4 cpus by just doing a bios update. I did that and went from a 3600 to 5800x3d to save a lot of money by not needing to upgrade my ram and mobo as well",pcmasterrace,2025-08-30 07:16:33,2
AMD,nbfw2s9,Someone probably said the exact thing with am3,pcmasterrace,2025-08-30 04:43:59,1
AMD,nbgpylj,5600x + 3070ti here and still always impressed with how my games run....but then I was gaming way back when getting 25-30 fps for Half life and a mixed bag of settings was a good time - I marvel at how things perform these days and still understand how to tweak settings to get the best out of my set up.,pcmasterrace,2025-08-30 09:21:56,1
AMD,nbgrjlc,I bought r7 5700x for 126 euro I m happy with it,pcmasterrace,2025-08-30 09:37:59,1
AMD,nbhnbk3,"I think it's okay, im hunting on fb marketplace for second hand itx rig.",pcmasterrace,2025-08-30 13:39:46,1
AMD,nbhq23n,"I have a 3700x for few years now, If I fell the need to upgrade it will be to a used 5700XD3 or something similar, not to AM5. It's probably good enough for 1080p gaming for another 10 years.",pcmasterrace,2025-08-30 13:55:38,1
AMD,nbfy9o2,What do you think about 5700x?,pcmasterrace,2025-08-30 05:02:21,1
AMD,nbfxbs5,Brazil only (edit; got caught by correction actually Latin America).,pcmasterrace,2025-08-30 04:54:26,-2
AMD,nbgplk8,"Oh no it's already dying. With how expensive AM5 PC parts/build currently, they stop making new AM4 boards (at least no re-stocks in my area) so people have no choice but to build a AM5 PC.",pcmasterrace,2025-08-30 09:18:16,-2
AMD,nbgag8m,"Stop with this please. AM4 has been dead since the launch of the 5800X3D which was the last ""upgrade"" to the platform. A platform remains alive due to upgrades and not ""new names"".   AM4 remains a ""relevant"" platform because the best performing parts on it still do a great job at running modern games. But that also shouldn't be surprising. The 5800X3D was launched 3 years ago with an MSRP of $450 and had class leading gaming performance. A high end CPU like that remaining relevant for 3+ years is totally normal.",pcmasterrace,2025-08-30 06:51:40,-7
AMD,nbgn8qb,"AM4 is slooooowly disappearing. I myself upgraded to a 5700x3d from a 3600 last year and now they are stopping production of 5700x3d, so guess thats it;)",pcmasterrace,2025-08-30 08:54:52,-2
AMD,nbgtgrw,Releasing worse models of old CPUs doesn't exactly keep the platform alive.,pcmasterrace,2025-08-30 09:57:10,-5
AMD,nbgfo3p,"So far am5 is looking really promising, we got x3d refreshes of entry grade processors and 2 more generations of releases, so like 3-4 more years of generational support on a what? 3 year old socket? Plus with the decrease in chip size I donâ€™t think weâ€™ll need to upgrade from b650 like how people had to upgrade to b550 for stuff like overclocking. Maybe if amd releases a 32 core desktop processor (so 64 threads) then maybe b650 might not be able to deliver enough wattage.",pcmasterrace,2025-08-30 07:41:17,15
AMD,nbhb2kw,"Also people forget not every country has similar cost of living. Some products might be the same as in North America or in Europe, but salary wise people earn less than richer countries.",pcmasterrace,2025-08-30 12:23:01,5
AMD,nbgt6gj,My friend has been using the same AM4 motherboard since AM4 came out. He got the 5700x3D a while back and hasn't looked back since.,pcmasterrace,2025-08-30 09:54:21,18
AMD,nbjnqwe,"> 28 core production ~~powerhouse~~ Power Bill Inflater 5000  I joke as like a 14 core Broadwell E5-2680 v4 CPU sits at like 3.3 Ghz turbo but 120W TDP. So a 28 core dual CPU setup might put 250w into the CPUs.  A 2021 EPYC 7453 with 28 cores is rated at like 225w and sees a 3.45ghz boost, but the IPC is higher. A newer Epic like the 9335 can hit 4.4ghz at about 210w TDP. Not bad but not much less power consumption at load.",pcmasterrace,2025-08-30 19:52:12,1
AMD,nbgyfbo,"2680v2 the goat. 20â‚¬, 10 cores, runs cool enough to not need thermal paste (yes I tried that using a z420 stock cooler and it runs at max 70C under load which is the same as my laptop), does it for a lot of people.",pcmasterrace,2025-08-30 10:44:00,1
AMD,nbg9x07,Why did you not just buy a 5700xt?,pcmasterrace,2025-08-30 06:46:40,-28
AMD,nbk2f5n,Dual core x3d,pcmasterrace,2025-08-30 21:10:21,1
AMD,nbh1vxq,"Just waiting for mine to ship after getting it on discount for $110. Upgrading from 3600, Iâ€™ve basically locked myself in to AM4 for the remainder of my PCâ€™s life",pcmasterrace,2025-08-30 11:14:15,2
AMD,nbgbumj,">silicon by throwing some extra cache on it and selling it for several times the value it would otherwise have.  you can't just glue extra cache to already made CPUs,    it's ryzen 7 5800x3D/5700x3Ds that didn't make the cut (damaged core, unstable at required clocks etc) and have 2 cores disabled",pcmasterrace,2025-08-30 07:04:43,22
AMD,nbgiim5,"\> AMD is just getting rid of sub-par silicon by ~~throwing some extra cache on it~~ disabling cores, lowering the frequency, and selling it instead of throwing it away.  that's pretty much every CPU/GPU except the very top ones",pcmasterrace,2025-08-30 08:08:33,10
AMD,nbg2fro,It's available at great prices here in India,pcmasterrace,2025-08-30 05:38:11,11
AMD,nbgnb01,"Supporting doesnâ€™t mean improving so they are right, AMD still is supporting AM4.",pcmasterrace,2025-08-30 08:55:30,0
AMD,nbgehis,"Yeah, last CPU family for AM4 was released in 2020 (Vermeer).  1H 2022 was vcache variant.  Writing a slightly different speed/price on the box of something that already existed 3-5 years ago doesn't really extend the longevity of the platform.",pcmasterrace,2025-08-30 07:29:41,-4
AMD,nbgtazh,Still using a 5700x3D in my build. Runs all my games at well over 100 FPS and is paired with a 3090. I've heard that it can even drive a 5080 or 5070 ti at 4k without being a bottleneck.,pcmasterrace,2025-08-30 09:55:36,8
AMD,nbgevib,"The 5500/5600G is Cezanne, which is a different CPU die. This one (and everything with vcache) will be Vermeer like the regular 5600.  That means that it will have pci-e 4.0, the regular amount of cache, the chiplet layout instead of all being on one chip etc.",pcmasterrace,2025-08-30 07:33:28,9
AMD,nbg4mmc,"Yes, actually.  On the other hand, it has lower base and max clocks than 5500 non-X3D",pcmasterrace,2025-08-30 05:57:44,1
AMD,nbgu8z8,"I just went from 1800x to 5800xt, a cheap upgrade  to an 8 year old motherboard.  In 2017, I sent back an i7 4 core cpu set up to amazon and purchased a ryzen 8 core monster. If I had kept that Intel setup, I would have had to buy a new mother board / ram / cpu combo years ago due to the lack of cores.   Amd has always supported cpu sockets for longer than Intel, and I am gaining the benefits right now of the longevity of the am4 platform.",pcmasterrace,2025-08-30 10:04:45,3
AMD,nbfx1iw,Nah AM3 died pretty quickly after AM4 released.  AM4 is still pretty strong even after AM5 has been out for 3 years already,pcmasterrace,2025-08-30 04:52:04,30
AMD,nbfyojg,"Am3+ was like, ""hmm this processor here costs less than the i5 and has twice as many physical cores? ok, I'll take it"". Nobody really liked or disliked them, but Am4 was a cool turning point, it converted every possible Intel user.",pcmasterrace,2025-08-30 05:05:53,8
AMD,nbg35ep,No one said the same thing about Bulldozer...,pcmasterrace,2025-08-30 05:44:30,2
AMD,nbj5fm3,"Until you start to wish for DDR5 support, PCIE 5.0 support, better efficiency and power draw.  I'd also say you won't really need a 5700X3D if you just copped a 1440P monitor and loaded your GPU a tad bit more if it's a capable card.",pcmasterrace,2025-08-30 18:17:03,1
AMD,nbfzv66,If you have one not worth to upgrade to the 5500x3D.  This is for people on older am4 like a 3600.  I thought about it but the performance is comparable to my 5800X,pcmasterrace,2025-08-30 05:15:55,0
AMD,nbg67ak,If youâ€™re dropping 2-3k for a gpu itâ€™s probably worthwhile to upgrade to AM5,pcmasterrace,2025-08-30 06:12:07,5
AMD,nbfzmzp,Latin America* I can get one just fine in Uruguay.,pcmasterrace,2025-08-30 05:13:59,4
AMD,nbgnm8f,"Yeah stop it and fuck the people who arenâ€™t rich, they donâ€™t deserve any happiness.",pcmasterrace,2025-08-30 08:58:35,2
AMD,nbj520d,This is a better gaming oriented version of the god awful 5500 lol. Only thing worse is it's clockspeeds.,pcmasterrace,2025-08-30 18:15:08,1
AMD,nbgoqzw,"That whole AI thing could cut it short, at least as the latest socket. The main reason to go AM5 is DDR5 RAM. Language model stuff needs basically nothing but RAM bandwidth, the CPU itself can be rather crappy. So I bet there are big incentives for another RAM generation jump and personally I would hope for quad channel RAM to become the standard.  Coincidentally you need the same things for the APU stuff that AMD is pushing.",pcmasterrace,2025-08-30 09:09:49,11
AMD,nbhe4to,"I dont think itll be a problem, as 9000 series is hilariously efficient and even the 16-core 9950X is fairly tame in its power draw. So if Intel can get away with the 14900, AMD can get away with a 32-core 9000 series CPU.",pcmasterrace,2025-08-30 12:43:26,0
AMD,nbhi557,We earn 10x less in average in Argentina   A 120k a year salary is considered rich by our standards and our cost of living is only low when it comes to services everything else costs the same or more as it does in the US because of ridiculous taxes on absolutely everything,pcmasterrace,2025-08-30 13:08:33,4
AMD,nbgxj76,"Same story for the PC I built my brother back when Ryzen launched.  1500x, 3600, 5700X3D  I'm in a similar boat having hopped onto B450 in 2019.",pcmasterrace,2025-08-30 10:35:41,8
AMD,nbh7t8s,"Heck yeah I got this same processor at microcenter earlier in the year. Breathed new life into my 3080, Iâ€™m hoping to get at least 2 more years out my system I built in 2020!",pcmasterrace,2025-08-30 12:00:06,3
AMD,nbjrg4t,"Running a 2690V4 here in my server, full load takes it up to around 135 watts. PL2 is 160 watts.  The newer EPYCs are a lot more efficient, of course, but it would take several decades to save enough in task energy for them to pay for themselves!",pcmasterrace,2025-08-30 20:12:05,2
AMD,nbga11w,Because I was building on a budget and more cores donâ€™t necessarily mean better gaming.,pcmasterrace,2025-08-30 06:47:46,17
AMD,nbixut3,"What a coincidence, I'm also waiting for my 5700X to ship. Coming from a 3500X though, but damn if AM4 isn't such a good and flexible platform for upgrading",pcmasterrace,2025-08-30 17:38:48,3
AMD,nbllsrt,"Coincidence or not, the 5500X3D arrived just in time to fill the gap caused by the lack of 5700X3Ds - and it costs the same as the 5700X3D did. It isn't that good of a deal.",pcmasterrace,2025-08-31 02:46:54,2
AMD,nbgdpar,"You're right, that was a mistake. I'll edit to fix.",pcmasterrace,2025-08-30 07:22:06,0
AMD,nbgmeay,Even the top ones are somewhat disabled,pcmasterrace,2025-08-30 08:46:27,6
AMD,nbiqjhr,"I'm aware. But instead of selling it all at once back in 2022, they've severely staggered the releases for maximum profit.",pcmasterrace,2025-08-30 17:02:52,1
AMD,nbg3slc,where? which website,pcmasterrace,2025-08-30 05:50:17,2
AMD,nbg7azj,I don't see it anywhere,pcmasterrace,2025-08-30 06:22:11,1
AMD,nbgfv9o,"While it may not extend the longevity it still provides a upgrade path at every price point, itâ€™s probably the best thing you can do with a old socket.",pcmasterrace,2025-08-30 07:43:13,9
AMD,nbp8286,"yes, im running it with a 9070xt. Its a good chip but imo it was the last am4 chip worth upgrading to. Now with it pretty much gone I dont see the point of upgrading to a 5500x3d unless maybe you are extremely budget bound and you have something like the 3600 or older.",pcmasterrace,2025-08-31 17:56:10,1
AMD,nbgdvza,"Interesting, according to [AMD](https://www.amd.com/en/products/processors/desktops/ryzen/5000-series/amd-ryzen-5-5500x3d.html) this thing is actually not cezanne like the regular Ryzen 5 5500, and gets PCIE 4.0. I guess this is a down binned version of the 5600X3D instead? Naming conventions are all over the place.",pcmasterrace,2025-08-30 07:23:54,6
AMD,nbgergb,"> Nah AM3 died pretty quickly after AM4 released.  AM3 was quite irrelevant for a long time before Zen's release. They released Piledriver in 2012, and didn't bother releasing architectural updates that they made for mobile until Zen came out in 2017.",pcmasterrace,2025-08-30 07:32:22,3
AMD,nbg1403,''twice as many physical cores''   https://preview.redd.it/va6fph2td3mf1.png?width=259&format=png&auto=webp&s=113a2f1ebe8de26ad668fafd09bcbddd9485d236  My face when I understood how FX architecture works,pcmasterrace,2025-08-30 05:26:41,12
AMD,nbfz0ds,"ya sorry i dont know that much about old hardware. only started after am4,",pcmasterrace,2025-08-30 05:08:38,2
AMD,nbg09qu,Ok,pcmasterrace,2025-08-30 05:19:24,-1
AMD,nbgpld7,Dude wtf are you even talking about?,pcmasterrace,2025-08-30 09:18:12,-5
AMD,nbj9dlp,"This is not an upgrade for a 5800X3D, 5700X3D or 5600X3D. It doesn't add to the lifetime of the platform at all, since those 3 have been and are still the limit.",pcmasterrace,2025-08-30 18:36:50,0
AMD,nbgtqix,"Quad channel used to be then am5 came busted. I believe when we start using ddr6 itâ€™ll be fixed. Tho I disagree I believe that any of the ai CPUâ€™s are going to strictly be for small form factors and laptops due to not having a powerful gpu most of the time, do models can be ran on the go (if that makes sense)   Weâ€™re still promised zen7 on am5 and the am6 socket doesnâ€™t come out till after zen8. If we do get specialized ai CPUâ€™s that arenâ€™t the laptop chips on a larger motherboard I believe it to be on a specialized socket like how threadrippers are so that it can have stupid amounts of ram.",pcmasterrace,2025-08-30 09:59:49,4
AMD,nbgt1bb,"Wait... People still use dual channel ram...? I've been using quad channel for almost 7 years in each of my pc builds because I like maxing out the ram slots on my motherboard  Edit: why is this being downvoted? I asked a question because I didn't think dual channel was still the majority, not because I wanted to ""flex"" quad channel. Judging by the replies it's not even that much different from dual channel.",pcmasterrace,2025-08-30 09:52:57,-15
AMD,nblv4bl,I also did the 3600 to 5700X3D route. It went from my main machine to my backup game PC when I wanted an AM5 but it still gets a lot of play time.,pcmasterrace,2025-08-31 03:52:42,2
AMD,nbjs4zq,I moved back to a lower TDP AM4 setup in my home server recently due to the summer heat. The cheapo 12700k I found a few years back was not expensive to run until I had to pair it with an AC...,pcmasterrace,2025-08-30 20:15:49,1
AMD,nbkacdm,"5700 XT is a GPU, and as an owner of one, I still think that'd be an extremely goofy choice vs. the 9060 XT. You made the right call, there's no reason to get a 5700 XT if you can get an actually modern GPU. RDNA1 fucking sucks.  i wouldn't recommend it to anyone unless you absolutely have to budget for a card that you can find used for $150.",pcmasterrace,2025-08-30 21:54:21,2
AMD,nbjfnil,"Yeah, the entire point of me choosing AM4 in 2019 was to be able to upgrade CPU without replacing the motherboard. Seemed a shame (and a waste) to just go to AM5 when the 5700X plus whatever GPU I decide on upgrading to in a couple years will suit my needs just fine",pcmasterrace,2025-08-30 19:09:24,1
AMD,nbg29uf,"And the funny part doesn't stop there, they had a good cost but what you saved on the price of the processor you lost on the cost of the motherboard, which were more expensive than Intel's, because the video memory of the Am3+ was on the motherboard, like, wtf",pcmasterrace,2025-08-30 05:36:45,1
AMD,nbgnskw,"The FX name is cursed, the GeForce FX line was also very bad.",pcmasterrace,2025-08-30 09:00:19,1
AMD,nbg2vjo,AM3 was basci6 dead on arrival. AM4 brought the company back from a decade of near extinction,pcmasterrace,2025-08-30 05:42:03,2
AMD,nbg0rct,"You hurt my feelings now, made me feel old, haha. But just to give you a rough idea of â€‹â€‹what it was like, Intel was 100% dominant in the processor market. Notebooks, Macbooks, premade desktopsâ€”they all typically came with an Intel processor; coming with an AMD processor was practically synonymous with low quality. It was also common for an entry-level processor to be the Intel Core i3 (which is still weak today). With the arrival of the AM3+ (Bulldozer architecture), AMD began to gain more acceptance among gamers because they were very cheap compared to Intel and compensated for the lack of ""polish"" with brute force, making performance very similar. But despite this, they had heating issues, which bothered a lot of people. Still with this architecture, AMD launched the A-Series, which was a very cool APU (basically a Ryzen G prototype) that increased the presence of AMD chips in notebooks. Then with AM4 AMD overtook Intel in desktops, and years later, Apple launched the M1, also leaving Intel behind. Now they basically only dominate the notebook sector (and I imagine the server sector).",pcmasterrace,2025-08-30 05:23:38,1
AMD,nbgyezb,Wtf are you talking about? Why should they stop making cpus for affordable builds? How does this hurt you?,pcmasterrace,2025-08-30 10:43:54,-1
AMD,nbj9oi1,"It still adds life to the platform as the newer, lower end X3D chip draws users towards the platform over AM5, especially in the region of the world these are actually sold in (Latin America) where they're significantly cheaper then AM5 chips, and 5700/5800X3DS. Any addition to a platform, still adds a small lifespan to the socket.",pcmasterrace,2025-08-30 18:38:24,1
AMD,nbgt94e,Basically am5 has busted memory controllers to where quad channel is significantly worse than the same amount in dual. (It might be a ddr5 problem I donâ€™t remember),pcmasterrace,2025-08-30 09:55:05,7
AMD,nbgvqic,"Motherboards have 4 slots since forever, but plugging in 4 banks instead of two doesn't make it quad channel. 4 banks are usually slower on a double channel system. quad channel is supposed to be twice as fast (when it comes to bandwidth).",pcmasterrace,2025-08-30 10:18:55,2
AMD,nbju1rw,"In winter I get heating, and I tend to load the old girl a lot lighter in summer!  At the end of the day, task energy is what drives post-sale cost of ownership. Low TDP AM4 is really, really, *really* good for task energy. PPT limit a Zen 3 CPU and it's right up at the top of things.",pcmasterrace,2025-08-30 20:25:53,2
AMD,nbg2oru,"fx6300 was my first CPU (that i bought for my own money)   I played The Witcher 3 and other games from that era. So I'm not complaining.  In my country, Intel was much more expensive back then. (even motherboards)",pcmasterrace,2025-08-30 05:40:23,2
AMD,nbh0qin,Tbh AMD's Athlon FX did pretty well and they wanted to reuse that name considering the speed of the Athlon FX line compared to the Pentium 4.  That aged horribly tho,pcmasterrace,2025-08-30 11:04:30,2
AMD,nbhb4zp,"No one is saying they should stop making AM4 CPUs, way to miss the point which is that new CPUs being made doesn't count towards the longevity of the platform.",pcmasterrace,2025-08-30 12:23:29,-4
AMD,nbgvud2,It's perfectly normal that using 4 banks instead of 2 is slower in what still is dual channel hardware.,pcmasterrace,2025-08-30 10:19:54,2
AMD,nbhn5o8,"I see, makes sense",pcmasterrace,2025-08-30 13:38:48,1
AMD,nblwzwl,I had a 5600X in a tiny itx system that ran at 95%+ of normal boost while on the 45w Eco mode setting. So efficient. That generation made high performance itx possible.,pcmasterrace,2025-08-31 04:07:01,1
AMD,nbi3gas,"Didnâ€™t know you made the rules. Yeah Iâ€™m not playing by your rules, why should I lol. Products being made for a platform means the platform is still alive.",pcmasterrace,2025-08-30 15:07:31,1
AMD,nbgvyb6,"Yeah but if I remember correctly it wasnâ€™t supposed to be dual channeled, it just kinda didnâ€™t come out properly and never got fixed",pcmasterrace,2025-08-30 10:20:55,1
AMD,nagklxq,Would be kinda funny if they did,pcmasterrace,2025-08-24 19:08:28,1236
AMD,nagli9e,can't wait for the Radeon PTX 7090 Ti,pcmasterrace,2025-08-24 19:13:12,478
AMD,naglsfy,Im guessing you watched Moores video where he explicitly and very clearly says that he has not hear this from any source but is a suggestion from HIM,pcmasterrace,2025-08-24 19:14:42,367
AMD,naglqa2,People actually think this is real,pcmasterrace,2025-08-24 19:14:23,34
AMD,nagl17i,AMD's Radion marketing team haven't had a single original thought in over a decade so I wouldn't be even slightly surprised.,pcmasterrace,2025-08-24 19:10:41,136
AMD,nagmurk,PTX 11090 XTX,pcmasterrace,2025-08-24 19:20:23,11
AMD,nagluap,Would become even more funny if they start naming them the same as their nVidia counterpart  PTX 6090 vs RTX 6090,pcmasterrace,2025-08-24 19:14:59,26
AMD,nago8r4,Just call it RAYDeon and call it a day,pcmasterrace,2025-08-24 19:27:49,20
AMD,nagljyf,Is it really surprising after GPUs like RX 9070?,pcmasterrace,2025-08-24 19:13:27,31
AMD,nagotfc,Can\`t we just fuckin keep the goddamn naming scheme for more than a couple generations,pcmasterrace,2025-08-24 19:30:51,7
AMD,nagpbne,Raydeon is better.,pcmasterrace,2025-08-24 19:33:32,5
AMD,nagpxed,Source: random youtuber,pcmasterrace,2025-08-24 19:36:45,6
AMD,nagmrin,Ready for Path Tracing. Yeah Okay. ðŸ¤£ðŸ¤£,pcmasterrace,2025-08-24 19:19:54,26
AMD,nah5kw9,Bring back the cool names and the 3D rendered random characters on the shroud and the box.,pcmasterrace,2025-08-24 20:57:39,4
AMD,nahysqt,"Still better than the 50 series lineup, lol. Lmao, even.",pcmasterrace,2025-08-24 23:45:08,5
AMD,nai9dmh,Love it. Â And who says the tech world has no drama and is all 1â€™s and 0â€™s.,pcmasterrace,2025-08-25 00:51:04,3
AMD,nahal8p,"Naw it's going to be PX10070xt or they will start over with a whole new lineup.  CPU's the new OMEGA 1000x series and the new PX 1070xt or somesuch.   Preferably I kind of like how Intel did it with CPU's  Starting with a simple indicator for what their it was (i5)Generation (14) and then where it lined up on the stack (700) along with a a letter to indicate if it was unlocked or low power or didn't have a iGPU (K or U or F or whatever).  The industry should come together and agree on some kind of standard.. maybe base it on tier.year.placement.specifics so basically the same except the generation would be the year it was released so ""i5 25700k""  Maybe to give some distinction between GPU and CPU and APU and NPU start with the letter C For classic CPU and then A for apu/iGPU N for NPU and G for GPU  So.. Intel Ci5 25700K? They would have to use something new   AMD CR5 25700X   GPU GRU the R for Radeon and the U for UDNA 25700xt.  Anyway probably never will happen and for many it is still confusing.",pcmasterrace,2025-08-24 21:24:32,3
AMD,nahdsd0,The PTX name idea was from Moores law is dead. He used it as a placeholder name and not something from his connections.,pcmasterrace,2025-08-24 21:42:11,3
AMD,nahg7no,AMD HIGHERUPS : ME COPY NVIDIA ME MAKE STOCK GO NORTH OF THE ECONOMIC CHART,pcmasterrace,2025-08-24 21:55:30,3
AMD,naitoo2,Let's not forget howAMD copied Intel for the CPU naming convention.,pcmasterrace,2025-08-25 03:02:47,3
AMD,najfdl6,AMD is known for having terrible naming so I can see this happening,pcmasterrace,2025-08-25 06:00:59,3
AMD,nai3ato,PT for path tracing and then X because X,pcmasterrace,2025-08-25 00:12:50,4
AMD,nagnzpe,"if you can't beat your competition, copy their name so people buy you by accident",pcmasterrace,2025-08-24 19:26:28,8
AMD,nahjlpg,https://preview.redd.it/1f30olwnk1lf1.jpeg?width=1080&format=pjpg&auto=webp&s=5675009a02462cb583d90c106ad2df80812554a1,pcmasterrace,2025-08-24 22:14:46,4
AMD,nahchbt,The 6000 and 7000 series had such a good name and they destroyed it whit the 9000 series.  Who the f@ck made this decision?,pcmasterrace,2025-08-24 21:34:54,2
AMD,nahmny7,"Yeh should just lean into the 7xxx series GPU and bring back the XTX, just put it in the front of the name.  XTX 10090   XTX 10070   XTX 10060   etc",pcmasterrace,2025-08-24 22:32:49,2
AMD,naiembn,But Radeon already does xtreme tracing x since rdna3,pcmasterrace,2025-08-25 01:23:48,2
AMD,naiwcuq,Cant wait to get PNY PTX PTSD card with nvidia connector.,pcmasterrace,2025-08-25 03:21:45,2
AMD,naizn7a,"[AMD Ryzenâ„¢ AI Max+ 395](https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-max-plus-395.html)   and    [AMD Ryzenâ„¢ AI 9 HX 375](https://www.amd.com/en/products/processors/laptop/ryzen/ai-300-series/amd-ryzen-ai-9-hx-375.html)   yes, these are officials",pcmasterrace,2025-08-25 03:46:11,2
AMD,najhr4d,What is company's problem with names? Can't they just get their naming conventions right? Why is this so confusing,pcmasterrace,2025-08-25 06:23:48,2
AMD,najqypa,How often can a company change their GPUs' naming?   AMD: Yes.,pcmasterrace,2025-08-25 07:55:36,2
AMD,nak133f,But its not true,pcmasterrace,2025-08-25 09:37:45,2
AMD,nammgli,"I'd be more interested in AMD not having sham MSRPs.  The 9700 XT is no where near a $600 card, and I think AMD knew it wouldn't be.",pcmasterrace,2025-08-25 18:35:28,2
AMD,namwtb6,"I don't believe this rumour. However, it's typical from AMD to change the naming of their products to be similar to their competitors', which tells us they're not confident in their own products.",pcmasterrace,2025-08-25 19:26:02,2
AMD,nanxyka,"AMD is simply better than NVIDIA, everywhere.",pcmasterrace,2025-08-25 22:30:03,2
AMD,nar892x,Now if only NVIDIA would cheat from AMDs homework have get drivers that work well.,pcmasterrace,2025-08-26 12:52:37,2
AMD,nagwqbs,"We love brand tribalism, donâ€™t we folks?",pcmasterrace,2025-08-24 20:12:49,4
AMD,nagpnsn,"Us: Mom, can we get RTX?  Mom: We have RTX at home.",pcmasterrace,2025-08-24 19:35:20,3
AMD,nah8nbw,"It was GT for Nvidia for years, then they went to GTX. AMD was RX for such time and still is. Nvidia changed the G to R and they approached AMD becoming RTX Now AMD is to blame?",pcmasterrace,2025-08-24 21:13:58,5
AMD,nagpwyx,"Would be kinda pathetic seeing them brag their GPUs can do pathtracing in 2027 when Nvidia has been doing it at that point for like the last 5 years, ever since Cyberpunk released it's PT patch.",pcmasterrace,2025-08-24 19:36:41,4
AMD,naho873,i don't care what they name their gpu's as long as they keep making budget friendly gaming gpus with really good performance,pcmasterrace,2025-08-24 22:42:19,2
AMD,naiwpc5,"AMD's naming department (well, marketing) needs to be liquidated.  No, seriously. Re-seat them somewhere else in the company, and just start the fuck over. xD I am aware this is kind of a joke, but dude, I am scared - because every oh-so-funny joke has a tiny grain of truth to it.  ...and I do not want that truth. o-o",pcmasterrace,2025-08-25 03:24:15,2
AMD,nagqx9t,Nah i still rock my Ryzen 4070 man,pcmasterrace,2025-08-24 19:42:00,1
AMD,nagr873,like brother like sister,pcmasterrace,2025-08-24 19:43:37,1
AMD,nags2ec,path tracing and ray tracing was created by movie studios long before real time tracing was possible.  it's also how companies bake lighting.,pcmasterrace,2025-08-24 19:48:09,1
AMD,nags33u,Isn't the next one UDNA?,pcmasterrace,2025-08-24 19:48:16,1
AMD,nagwg82,"what is ""path Traced"" games?",pcmasterrace,2025-08-24 20:11:23,1
AMD,nagze0n,Why not? Is it shameless? Sure. But it works for team green and to their credit the 9070 has been AMDs best launch in years.,pcmasterrace,2025-08-24 20:26:09,1
AMD,nah2a29,"Hey, if you can't beat them, join them.",pcmasterrace,2025-08-24 20:40:41,1
AMD,nah3784,That will most likely be my next card. hope they make flagship models again,pcmasterrace,2025-08-24 20:45:22,1
AMD,nahq355,PTX 6060 just to confuse buyers between a RTX 5060 and  PTX 6060.,pcmasterrace,2025-08-24 22:53:31,1
AMD,nahrv61,"guess i will buy some AMD stocks too then. but stop it with the games, show us the AI plan.",pcmasterrace,2025-08-24 23:04:05,1
AMD,nai180b,Can't wait for my Radeon PTX 10060,pcmasterrace,2025-08-24 23:59:54,1
AMD,nai1l97,"Iâ€™m just confused how they do their naming scheme, make something cohesive, so they change to 9070? Wild jump that still boggles me to this day",pcmasterrace,2025-08-25 00:02:12,1
AMD,nainpdy,What Id we follow Nvidia but with XT On all GPU   RTX 9070 XT (Radeon-Trace),pcmasterrace,2025-08-25 02:21:50,1
AMD,naiyuqn,not so subtle huh,pcmasterrace,2025-08-25 03:40:08,1
AMD,naj15eu,AMD should take AiTX before nvidia does :D,pcmasterrace,2025-08-25 03:57:49,1
AMD,naj1zef,Can see it happen lol.,pcmasterrace,2025-08-25 04:04:09,1
AMD,naj6b6g,Come on if you really want to do a funny make it RXT,pcmasterrace,2025-08-25 04:39:18,1
AMD,naj6zvv,"Well, they've always done this, surprised?",pcmasterrace,2025-08-25 04:45:04,1
AMD,naj70lm,I don't really care what the naming convention will be. I just want an affordable graphics card that won't catch on fire in my next system!!!,pcmasterrace,2025-08-25 04:45:13,1
AMD,naj9xmt,yep more rt/pt that  does not follow the standard for both. yes there is a standard.,pcmasterrace,2025-08-25 05:10:26,1
AMD,najdvck,"Before long they'll be like pickup trucks where it doesn't matter if it's a Chevy or a Ram, they all use the 1500, 2500, 3500 naming scheme. Ford is nearly the same with F-150, F-250, F-350.    If only they'd just leave the generation out of the model number and switch to model years the way vehicles always have been.   2040 RTX 90 series sounds a lot better than RTX 26090",pcmasterrace,2025-08-25 05:46:39,1
AMD,najjnrp,"As long as it's OSS drivers work well, I don't care. Also, I hope they get ROCm figured out by then.",pcmasterrace,2025-08-25 06:42:30,1
AMD,najklmu,Nah sales gimmicks like ray tracing is nvidias thing haha,pcmasterrace,2025-08-25 06:51:42,1
AMD,najle8c,Just name Radeon P9800 P9800X,pcmasterrace,2025-08-25 06:59:30,1
AMD,najz4qc,If they're cheaper than equivalent Nvidia cards and have more VRAM I'm all for it! ðŸ˜†,pcmasterrace,2025-08-25 09:18:16,1
AMD,nak1cbq,"To be fair, AMD were already using RX and then Nvidia came along and were like ""lol R**T**X get rekt"".  So if anything I'd argue that Nvidia ""started it"".  Perhaps AMD should go from RX to RXT. No, lets be realistic, it's AMD so they'd go for RX-X\_\_\_\_\_XT instead.   /s",pcmasterrace,2025-08-25 09:40:13,1
AMD,nak3oar,PTR - Path Tracing Ready This is just my suggestion.,pcmasterrace,2025-08-25 10:02:02,1
AMD,nak84km,Why not just Raydeon ? Scnr,pcmasterrace,2025-08-25 10:41:33,1
AMD,nakhzgo,Until it's confirmed there is no point in any of this. This generation should have been rx8800xt for the top GPU.,pcmasterrace,2025-08-25 11:56:31,1
AMD,nakid9t,I thought they said the next Gen would be UDNA and that RDNA is basically over with this gen.,pcmasterrace,2025-08-25 11:59:08,1
AMD,nal27xm,"RTX, short for ""Radeon Technical Xcelence""",pcmasterrace,2025-08-25 13:58:36,1
AMD,nal3f9x,Reminder that the CEOs of Nvidia and AMD are cousins,pcmasterrace,2025-08-25 14:05:10,1
AMD,nal7efq,"The ptx thing is just an idea by mlid, not an actual leak.",pcmasterrace,2025-08-25 14:25:38,1
AMD,nalaufc,Radeon RZX sounds gud tho,pcmasterrace,2025-08-25 14:43:15,1
AMD,nalbkxe,I for one think it's a good thing when it's easy to understand which numbers mean what when cross shopping for people who are inexperienced.,pcmasterrace,2025-08-25 14:46:55,1
AMD,nalujve,This is why I swapped back to Nvidia and went big with the 5090... I don't think prices will come down any time soon.,pcmasterrace,2025-08-25 16:20:28,1
AMD,nalyzhr,"Pay Tracing, monthly subscriptions",pcmasterrace,2025-08-25 16:42:29,1
AMD,nam0abn,"This wasn't a leak, this was a marketing suggestion by MLID. There is no evidence AMD is actually doing this. He just made the point AMD could maybe push path tracing into their marketing to jab Nvidia.",pcmasterrace,2025-08-25 16:48:55,1
AMD,nam78fj,"Whomever is in charge of marketing is just pathetically incompetent at this point.  It was still bad back when amd wasn't even a threat to challenge for market share in any department and they just copied the number and added 1 just to try to pick up the buyers who knew so little about hardware that they would be duped by something so lame.  At this point their cpu department has been ahead for a couple of generations, and they need to distinguish themselves.  Not for nothing, a little simplicity wouldn't hurt anyone.  I don't care what spin they want to put on it, it looks petty and absolutely insipid at this point.  It's taking away from their brand, not adding to it.",pcmasterrace,2025-08-25 17:22:10,1
AMD,namkwdh,I'm down for it.,pcmasterrace,2025-08-25 18:27:42,1
AMD,nammde9,This would be awesome. I hope they do it!  Screw Nvidia. May the fleas of a thousand camels infest Jensen Huang's underwear.,pcmasterrace,2025-08-25 18:35:02,1
AMD,namuphf,nvidia would be so mad if they beat them to PTX,pcmasterrace,2025-08-25 19:15:56,1
AMD,naqk7dx,PTX1090XT,pcmasterrace,2025-08-26 10:06:22,1
AMD,nagr0dl,Ryzen 4070 when?,pcmasterrace,2025-08-24 19:42:27,1
AMD,nah5ol3,"I play games. It takes a GPU to play the games I want to play. I buy the GPU. Nothing more, nothing less.",pcmasterrace,2025-08-24 20:58:11,1
AMD,naj8es0,I mean it would be an improvement. I always found Nvidiaâ€™s naming scheme way easier to understand at a glance.,pcmasterrace,2025-08-25 04:57:00,1
AMD,nah9l8i,"Given PT is just RT, but computationally more intensive (by calculating multiple light rays per pixel), the card is just going to take RT cores it would have anyway, and itll have more of them.   So stop pretending like this is some sort of intellectual property that AMD is stealing. Its just a slightly specific upgrade for the GPUs raw horsepower.",pcmasterrace,2025-08-24 21:19:05,0
AMD,naj8tf8,How long until they rebrand to nvidiAMD?,pcmasterrace,2025-08-25 05:00:34,0
AMD,nagp4cw,I love nVidia,pcmasterrace,2025-08-24 19:32:27,-2
AMD,nagsbgr,The hell do I care what they call them . It works ? Great,pcmasterrace,2025-08-24 19:49:31,0
AMD,nagx2ut,"AMD should do mid-cycle refreshes and call them ""Elite"" instead of super. Radeon PTX 10090 XT Elite, 512-bit 48GB GDDR7 lol.",pcmasterrace,2025-08-24 20:14:35,0
AMD,nahviyl,"""I believe they were concentrating on raytracing LAST year""",pcmasterrace,2025-08-24 23:25:47,0
AMD,naizsn9,AMD loves the X and T letters ðŸ™„ Soon the GPUs will have names like PTX RXXXR XTX XXTTXTXTXTXTX 9999 TTTXXTXTX XTreme,pcmasterrace,2025-08-25 03:47:22,-1
AMD,najg0ps,"Just let them have something lol, still gonna be two gens back lol. Fake Frames tho.... oop lol",pcmasterrace,2025-08-25 06:07:02,-1
AMD,nagy1iw,not only will it be named PTX it will be 30% slower in Path Tracing then the Nvidia equivalent MSRP for 20% less while costing 40% more then Nvidia at retail.  AMD will rejoice for they have done something smart! and can't understand why there market share sucks.,pcmasterrace,2025-08-24 20:19:25,-5
AMD,nah21uy,"they do seem to enjoy fucking with their competitors through the channel that is naming conventions, since they sorta just do whatever they want to begin with on that front  i seem to recall they named their mobile AI APU line just like intel's but a gen ""ahead""",pcmasterrace,2025-08-24 20:39:32,483
AMD,nang8i8,Personally I can't wait for the PTX6090. That would be the ultimate troll,pcmasterrace,2025-08-25 20:58:12,3
AMD,nago455,"Nah, weâ€™re getting the PTX 9190 XT Super, you gotta copy just a little bit because too much feels heavy",pcmasterrace,2025-08-24 19:27:07,137
AMD,nagru4t,Then Intel Arc QTX 8090 Ti,pcmasterrace,2025-08-24 19:46:55,10
AMD,najdl7d,What is this Radeon brand? Did you mean RVIDIA?,pcmasterrace,2025-08-25 05:43:59,5
AMD,najkg64,\*Pateon PTX 7090 Ti,pcmasterrace,2025-08-25 06:50:13,1
AMD,nak9gte,"I wouldn't mind if everyone kept a somewhat comparable naming convention. xx70 to xx70, XT to Ti, Super to Super, etc. Now Intel needs to jump on board, too, and we'll be set.",pcmasterrace,2025-08-25 10:52:28,1
AMD,nago29z,"AMD will keep it to 4 digits and change the branding again, since it's their business model to confuse customers",pcmasterrace,2025-08-24 19:26:50,143
AMD,nah91xa,"People in this sub will pretend MLID claimed something as fact and continue to hate on him, when very clearly he states the opposite in almost all of his vids with 100s of disclaimers lol. Wouldn't surprise me if people also take this as ""rumor fact claim""",pcmasterrace,2025-08-24 21:16:10,11
AMD,najyxqf,"Mate, Radeon literally changed their naming strategy to align to Nvidia's with the 9000 series. Justify that if you struggle to believe this meme could become reality.",pcmasterrace,2025-08-25 09:16:17,16
AMD,nako6pw,"They changed their naming scheme for laptop 2 times in the last 3 years. The Ryzen 7 7840HS is following  the 6800HS but changes rules.   The 6800HS was : 6 for generation, 8 for the product, HS for the power target thus performance target  Then, 7840HS was : 7 because 2023, 8 for either a Ryzen 7 or Ryzen 9 class, 4 for the Âµarch Zen so Zen 4 here, 0 for the lower or higher tier of this specific variant, HS same as before  They kept it for a single generation after that, the 8000s ( a pure refresh of the 7000s ) because just after that, that came with another one : Ryzen/AI    So now, laptops are coming with a Ryzen 7 250 or 350 etc...  A Ryzen 7 250, is the exact same chip as the 8840HS with the new name and that CPU was also a refresh of the 7840HS, just with a NPU added.   They changed for that, only because Intel got the Core 100 naming for their ""AI ready SoC"" and 200 is more than 100.  They did the exact same for Radeon GPUs, RX 9070 only because 9070 is more than 5070 and they admitted it to the press.   In the past decade only, AMD changed 5 times their GPU names.   We got : R9 2/3xx/Fury > RX 4/5xx > RX Vega > Radeon VII > RX 5xxx/6xxx/7xxx > RX 90xx   At this point, AMD could change anything about their naming scheme and it would be believable.",pcmasterrace,2025-08-25 12:37:14,7
AMD,nagooqt,"I'm pretty sure Raja Koduri was on something when marketing Radeon VII. Something about it having a spirit then jumping ship immediately after it released, IIRC.",pcmasterrace,2025-08-24 19:30:10,20
AMD,nagotoc,Did nvidia make a card called the 3075 GRE? Didn't think so.,pcmasterrace,2025-08-24 19:30:53,12
AMD,nagqwjj,Why should they.  They do not want to neat nividia. They control the bottom other half of the market. Consoles.  They just show up to the game. But I never expect them to be trailblazers.  But I have always fondly been a fan that they show up to the fight .,pcmasterrace,2025-08-24 19:41:54,0
AMD,nah9kut,"The nvidia cult members havent had a single thought at all. If better value cards arent enough, they gotta try to get the clowns somehow...",pcmasterrace,2025-08-24 21:19:02,-1
AMD,nahir9h,PTX Pro 1 PTX Gamer 1+ PTX Esports,pcmasterrace,2025-08-24 22:09:53,5
AMD,nagoamt,"When you think about it they do, AMDâ€™s Ti is XT and yeah thatâ€™s it (except first number), 9060 XT = 5060 Ti, 9070 = 5070, literally just changed it to 9 for no reason what is wrong with 8",pcmasterrace,2025-08-24 19:28:05,17
AMD,nagpyvt,The 9070 does PT just fine.  The next wave of hardware is targeting PT specifically.  It will be the baseline for the next consoles.  I hope they also provide high VRAM so we can run LLM NPCs locally.  That would be better than PT IMHO.,pcmasterrace,2025-08-24 19:36:58,-17
AMD,nahbav7,Oh God the next CPUs can get pregnant??? What hath science wrought????,pcmasterrace,2025-08-24 21:28:22,0
AMD,naib2rh,Cause X gonna give it to ya!,pcmasterrace,2025-08-25 01:01:42,5
AMD,namwxhw,Ouch!,pcmasterrace,2025-08-25 19:26:35,0
AMD,nahjopw,Agreed pretty ridiculous,pcmasterrace,2025-08-24 22:15:16,-3
AMD,naguvdg,More like 3 years. The 4090 was the first GPU actually capable of path tracing. Before that it was more of a technically possible thing.  Even now path tracing is reserved for higher end GPUs.,pcmasterrace,2025-08-24 20:03:17,1
AMD,nah7n4v,"Path Tracing is a more advanced, hardware demanding and accurate version of Ray Tracing, which means simulating light (rays, photons) from light sources to the camera in games for more realistic lighting, shadows, reflections and even sound waves (acoustics)",pcmasterrace,2025-08-24 21:08:36,3
AMD,nagxl4f,Ryzen Core Ultra,pcmasterrace,2025-08-24 20:17:09,1
AMD,najdilg,"gt, gs, gtx, rtx, ti, titan, super, ti super...",pcmasterrace,2025-08-25 05:43:18,0
AMD,nagxez4,PTX XT PTXT PTSD,pcmasterrace,2025-08-24 20:16:18,4
AMD,nah2d2l,Like the 9060 and 9070 amd gpu. What is Nvidia gonna do lmao,pcmasterrace,2025-08-24 20:41:07,234
AMD,nakl0ks,"The desktop chipsets Zen too were sorta named to troll Intel. Intel used the Bx50 naming for their mid-range chipsets up to the Kaby Lake gen with B250, but then AMD rolled out with their B350.",pcmasterrace,2025-08-25 12:16:48,2
AMD,nai3ncd,Add a lot of X because it's amd.Â Â    Amd Radeon XPTXÂ² 42069 XTX TIX SuperX,pcmasterrace,2025-08-25 00:14:59,45
AMD,nagr9ry,XTXY*,pcmasterrace,2025-08-24 19:43:51,26
AMD,naisqe0,"I agree with this, This won't impact the X per Card metric and keeps XFX in the lead with their XFX PTX 9190 XT Super.  Or, the can do XFX PTX 9190 XT Xtreme to maximise their X per Card metric.",pcmasterrace,2025-08-25 02:56:06,7
AMD,namrywk,*Pateon PTX 7090 Pi,pcmasterrace,2025-08-25 19:02:26,2
AMD,nagqv4x,"Yes, the 01007 XT will be the midrange GPU to rival the 6070 TI.   I will admit though, using the same core naming scheme as Nvidia, XXYY where XX is the generation and YY is the class of products has made explains the AMD line up a lot easier. XT is TI so the whole system is neat.",pcmasterrace,2025-08-24 19:41:41,53
AMD,naj96fy,Would be even more confusing if they branded their GPUs as Dvinia PTX 9080,pcmasterrace,2025-08-25 05:03:45,3
AMD,naja1tf,the issue is many sites use him as a fact sources. also most of his reporting is throw at wall. 99% never sticks.,pcmasterrace,2025-08-25 05:11:28,5
AMD,nagq1kg,"AMD have Golder Rabbit Edition for China, Nvidia have Dragon.  Both animals from the Chinese zodiac.  Edit. The rabbit is pretty appropriate for AMD  >The fourth Trine consists of the Rabbit, Goat, and Pig. These three signs are associated with the element of wood, said to have a calm nature and a somewhat **reasonable approach.** They seek aesthetic beauty, are artistic, well-mannered, and compassionate, yet they can also be detached and **resigned to their condition.** The three are described as caring, self-sacrificing, obliging, sensible, creative, empathetic, tactful, and prudent. However, they can also exhibit traits such as being **naive**, pedantic, insecure, selfish, **indecisive**, and pessimistic.",pcmasterrace,2025-08-24 19:37:22,14
AMD,najg8l3,The 8000 series was made at the same time as the 8000 cpu series but it was never meant to be a consumer chip it was all ai accelerators for machine learning so amd had already used there 8000 series,pcmasterrace,2025-08-25 06:09:06,4
AMD,nagukpb,It might be that they wanted a clean slate and rename the gpus next generation.  UDNA is meant to be a new architecture. RDNA 4 is plain RDNA pushed to its limits. UDNA is meant to signify change so they want to change the naming scheme to reflect that.  It also allows for their CPU and GPU series to be on the same naming scheme in a way.  Since I don't think they've ever gone to 10000 as a series number it's meant to mark the end of this line of GPUs. 8000 would look and feel awkward to abandon a series at.,pcmasterrace,2025-08-24 20:01:42,6
AMD,nagu0rs,I'm skeptical the next gen consoles will be able to do PT in any wide capacity. Maybe at 30-40fps in certain optimized titles.  And sure RDNA4 can do PT but still loses massively to Nvidia in that respect. I think 5070 Ti has something like a 50% advantage in PT compared to the 9070 XT.,pcmasterrace,2025-08-24 19:58:45,14
AMD,nah2m4y,4090 launch in 2022. It would be 5 years old in 2027....,pcmasterrace,2025-08-24 20:42:24,5
AMD,nagvx28,3090 and 3090 Ti are both capable of PT at 1080p and 1440p with DLSS. I use PT all the time on my 4070 Ti Super even at 4K with DLSS and frame gen. People really overexaggerate the hardware requirements for it.,pcmasterrace,2025-08-24 20:08:40,5
AMD,nah8iyw,i can smell empty wallets,pcmasterrace,2025-08-24 21:13:19,5
AMD,najgdl3,"Youâ€™re deliberately mixing up nomenclature to make it sound more complex than it is.   You wouldnâ€™t put â€œTiâ€ in the same category as â€œGTXâ€. One is a prefix, the other a suffix. Itâ€™s like trying to argue names are confusing because â€œMr.â€ and â€œJohnsonâ€ are both parts of names. Nobodyâ€™s first name is â€œMr.â€",pcmasterrace,2025-08-25 06:10:27,1
AMD,nahlqxr,"NVIDIA might not care, by the time we get to the RTX 9xxx series the RX 9060/9070(XT) are going to be relics. It doesnâ€™t help that AMD never released a 9080 or 9090 either, so these names are essentially still free real estate",pcmasterrace,2025-08-24 22:27:22,145
AMD,nahhvh1,"Nvidia will employ the classic strategy of ""7 8 9""",pcmasterrace,2025-08-24 22:04:49,42
AMD,nai4kc7,Good chance theyâ€™ll drop gaming gpus by then cos all their money is in AI now,pcmasterrace,2025-08-25 00:20:43,3
AMD,najzgk9,Copy NOAA's hurricane-naming convention?  RTX Albert  RTX Bertha  RTX Corey,pcmasterrace,2025-08-25 09:21:35,4
AMD,nau2krj,"The original Threadripper chipset did that too, being named X399 around the release of the infamous X299.",pcmasterrace,2025-08-26 21:06:46,1
AMD,najjg7v,Amdâ€™s stock will skyrocket due to Elon investing all his money in it !,pcmasterrace,2025-08-25 06:40:24,18
AMD,naju372,And the X3D version of it,pcmasterrace,2025-08-25 08:27:15,4
AMD,najz5m5,If any GPU company gives us a 42069 I donâ€™t care about the VRAM or anything else. I will get it.,pcmasterrace,2025-08-25 09:18:31,6
AMD,nagzs4v,I don't know why they don't just use the model year + 2 digit product tier. They could release 100 products a year and be fine.  Ryzen 2590. Radeon 2580.,pcmasterrace,2025-08-24 20:28:07,19
AMD,nagsp9e,"Easier to explain other than the fact that generation kinda doesn't mean shit cause the ""accidentally"" named it 9070 so it sounds better than Nvidia's 5070 because it's a 90 not a 50",pcmasterrace,2025-08-24 19:51:37,5
AMD,najeaql,"on speculative stuff, to some degree maybe (ofc 99% being hyperbolic obviously), but when he presents direct statements from insider sources about chip layouts etc he's almost always right on the money, and he always has disclaimers for almost everything. I would really not say he's someone who throws stuff at the wall to see what sticks. And I say this as someone who is already critical of things he presents.  His job is talking about tech, so of course he will talk about tech. Some is speculation, here obviously things will be hit or miss. Other things are reportings which are rarely miss -even if things change it's usually from unforeseeable circumstances.  But you're absolutely right, the main issue is all the sites that quote (and often misquote) him and use his presented content as facts and rumor verifications, rather than actually being honest about the nature of what and how it was actually presented. And he always has to go out of his way to correct misstatements by these ""news"" sites lol.  To me the main annoyance with MLID is his display of self confidence even in cases where he is a bit out of his depth. Usually he doesn't do this as he tends to speak on things he knows of, but sometimes he misses or blows things up speculatively where things really just needed objective reporting and not too much speculation yet.",pcmasterrace,2025-08-25 05:50:44,9
AMD,najfwk0,I hope your specs are bs why have 2 2080s if no sli and why tf do u have 200tb of storage are you pirating all of netflix,pcmasterrace,2025-08-25 06:05:56,0
AMD,najfx2i,I hope your specs are bs why have 2 2080s if no sli and why tf do u have 200tb of storage are you pirating all of netflix,pcmasterrace,2025-08-25 06:06:04,0
AMD,nagr12h,"Yeah, but they did at least have one original thought of not naming it the Dragon. So be nicer to the poor indie gpu team they have at amdðŸ˜¡",pcmasterrace,2025-08-24 19:42:33,-1
AMD,najjdwx,"ryzen 7600, rx 7600 :",pcmasterrace,2025-08-25 06:39:45,1
AMD,nah18nf,Also they need a Ray Reconstruction competitor. It really does clean a scene up.,pcmasterrace,2025-08-24 20:35:25,7
AMD,nai7v8v,"I can run PT in Linux on CP2077 at 1440P and get 60FPS with FSR4 performance.  If I turn on framegen it's 120fps.  I agree Nvidia is a bit faster, but there's no reason to release new consoles unless they can PT.   It's coming, that's what all the AMD patents say.  All the new consoles will have FSR4+ and will be able to run RT as baseline with PT being common.  Downvote me to oblivion, but that's what's coming in 2027.    We will be on a new node, which means faster chips.  We've been on 5nm for 5 years+ at this point.  3nm is a new node that will bring higher performance.  Should be a pretty good leap.  We'll probably be stuck on 3nm into 2030 or longer.  Nodes bring faster chips and it'll be easier to run PT on them.  When consumer switches to 2nm, we'll see another large boost, not again that will likely be 2030+.  Servers and AI GPU will get 2nm first.  Followed by 1.8nm (which is a revision of 2nm).  Then we should start seeing glass substrate as a means to improve inner chip latency.  Lots of new tech coming in 2 years.  The 5090 won't be that impressive when this stuff drops because of the new node.",pcmasterrace,2025-08-25 00:41:36,2
AMD,nah26pq,"I think in terms of raw raster the most recent next gen console leaks have it in the 5070 to 5070 Ti ballpark, so yeah I think PT being a baseline for the next lot of consoles is a pipe dream.   The days of consoles being anywhere near the bleeding edge are long gone.",pcmasterrace,2025-08-24 20:40:13,0
AMD,nahocu1,never released a 9080 or 9090 yet,pcmasterrace,2025-08-24 22:43:06,63
AMD,nahlfxz,"Its simple and it works tbh. Everyone, even non techies know what a 2060, 3070, 4080 etc are. Very easy to know what gen/series they are.",pcmasterrace,2025-08-24 22:25:36,45
AMD,nao3ynr,"I wondered this too, I don't know much about chip fabs but I imagine the 5090 is just RTX 6000 rejects, so I think there will always be a market for GPUs in some form but to say that GPUs are not Nvidias main interest is accurate.",pcmasterrace,2025-08-25 23:05:37,1
AMD,nah1hwk,"Wait until they introduce the Radeon 2567 and the Ryzen 2667GXT.   That's AMD we're talking about, they'll fuck up that naming scheme too somehow.",pcmasterrace,2025-08-24 20:36:43,15
AMD,nak1kya,*Samsung has entered chat*,pcmasterrace,2025-08-25 09:42:30,2
AMD,nagu3ep,"Eh, donâ€™t really think so. They synced CPU and GPUs so itâ€™s the Ryzen 9 and the Radeon 9000. Where it was 7900 for the seven thousand series they would have been the 7090 using the new scheme. Yeah, I am sure it didnâ€™t hurt to have the higher number, but I donâ€™t think that was the primary intention.",pcmasterrace,2025-08-24 19:59:09,8
AMD,najg2xr,Workstation / home server vm game.  Check out craft computing,pcmasterrace,2025-08-25 06:07:36,1
AMD,nahp8yc,"I know that's something that's coming ""soon"" with Redstone.",pcmasterrace,2025-08-24 22:48:29,1
AMD,naiuvz0,Rtx 10090 not going to look good at all on the side.,pcmasterrace,2025-08-25 03:11:22,45
AMD,najk24s,"Reminder that the the strongest 90X0 of amd is a **midrange** current gen card  God i hope they make a 10080 or 10090, would be so funny",pcmasterrace,2025-08-25 06:46:26,12
AMD,nai2bhe,Kinda why I hate AMDs naming scheme theyâ€™re really all over the place,pcmasterrace,2025-08-25 00:06:41,39
AMD,nazmm0o,Yeah . Most of my friends can easily identify the latest Nvidia gpu. Amd though?? Takes different manuals and YouTube videos to decide which one is the latest,pcmasterrace,2025-08-27 18:05:42,2
AMD,naotv7c,"> I don't know much about chip fabs but I imagine the 5090 is just RTX 6000 rejects  The 6000 uses a TU102 die, while the 5090 uses a GB202 die. They're different chips entirely.  Contrast that with the 5080 and 5070ti, where the 5070ti uses reject/poorly binned 5080 GB203 chips.",pcmasterrace,2025-08-26 01:39:54,2
AMD,nah1ohl,"Yeah you're probably right. Even then as long as they keep the year + higher number better paradigm, it's a net improvement.",pcmasterrace,2025-08-24 20:37:39,2
AMD,naj9csa,"Please keep it civil, at least they are better than a company naming their gaming displays",pcmasterrace,2025-08-25 05:05:19,1
AMD,nagw8n6,This makes sense. I didn't see that before.  Now its time for my Radeon RX 9090 X3D (Like that makes any sense),pcmasterrace,2025-08-24 20:10:19,4
AMD,najfr5r,Yea and a 8000 series does exist consumer markets just dont really have it as they are all ai centric and server grade chips technically exists tho,pcmasterrace,2025-08-25 06:04:32,1
AMD,najghjv,I will it just is a crazy storage total if your running something like a server tho it makes sense ig I didnt think of that just due to idk why you got a good enough cpu and enough ram for it,pcmasterrace,2025-08-25 06:11:29,1
AMD,naj1tis,RTX series 90s and the RTX series 90x,pcmasterrace,2025-08-25 04:02:54,11
AMD,nak6uy2,"They should just go full meme and call it the ""Over"" series and write 9006, 9007 on the side.",pcmasterrace,2025-08-25 10:30:38,4
AMD,nakaubl,ptx 1080,pcmasterrace,2025-08-25 11:03:26,1
AMD,naq6mkx,"It sounds like they will, with a card thatâ€™s planned to be as good as a 6090 for half the price (I believe $1500 was the rumored msrp). It is also a card thatâ€™s can be sold to datacenters so it may take a little while for everyone who wants one to get one, I doubt weâ€™ll see a launch as voluminous as the 9000 series for a while.",pcmasterrace,2025-08-26 07:55:17,1
AMD,naitb4u,Lowkey better than sticking to one like Intel. People are getting swindled left and right with old Intel CPUs because it's an i7.,pcmasterrace,2025-08-25 03:00:08,16
AMD,naouqy9,Thank You! ðŸ™,pcmasterrace,2025-08-26 01:44:55,2
AMD,najgonu,"Redundancy.   With 3 lvl of storage speed to . Super  fast , fast normal.",pcmasterrace,2025-08-25 06:13:24,1
AMD,naq7ij8,"A 6090 (or 5090 idk idc) equivalent for 1.5 sounds like a dream  I P R A Y one of the retailers makes a version that looks like the ROG astral 5090, such a beautiful card",pcmasterrace,2025-08-26 08:04:02,1
AMD,najf736,"S/O to Microsoft for the Xbox One, Xbox One S, Xbox One X, Xbox Series S and the Xbox Series X",pcmasterrace,2025-08-25 05:59:20,14
AMD,naksg8l,Theyâ€™d have to do the same amount of research on that as they would looking up what a random name from AMD is so meh same thing to me.,pcmasterrace,2025-08-25 13:03:08,3
AMD,naw4mu5,"By the way you were actually right. I was looking at the Quadro RTX 6000. The 6000 you're thinking of actually does use the GB202, same chip as the 5090!",pcmasterrace,2025-08-27 04:20:37,2
AMD,najh0bc,"Ah so nvme ssd, sata ssd and had then?",pcmasterrace,2025-08-25 06:16:35,1
AMD,naq7ppa,"Honestly Iâ€™ve been dreaming about making my own shroud for a gpu, I got a resin printer and cad software so like, why not?",pcmasterrace,2025-08-26 08:05:58,1
AMD,nakvzpg,"not when they don't research it lol, lotta ppl see RTX/i7 and click buy no matter the specs, especially if it's 'cheap'",pcmasterrace,2025-08-25 13:23:45,2
AMD,najiuoc,Hdd. All 3 types,pcmasterrace,2025-08-25 06:34:31,1
AMD,nakwu5y,Than thereâ€™s probably no helping them regardless then,pcmasterrace,2025-08-25 13:28:34,2
AMD,nbqcvwm,Amazing build. Prepared for LCD fan headache.,AMD,2025-08-31 21:26:02,20
AMD,nbqjbov,Looks nice man ðŸ‘Š  Is that the HAVN 420 vgpu case?   Which OLED did you go with? Iâ€™m talking myself in and out of about 3 different ones right now ðŸ¤£,AMD,2025-08-31 22:01:21,6
AMD,nbr8l3r,"This looks incredible, that 3080ti looks great in it. Congrats on the upgrade!",AMD,2025-09-01 00:34:33,4
AMD,nbsnulf,"What are you CPU idle, gaming load, and synthetic bench temps? Sick build btw.",AMD,2025-09-01 06:40:39,2
AMD,nbsy91f,This set up is fireeeeeee ðŸ”¥ðŸ”¥ðŸ”¥,AMD,2025-09-01 08:18:28,2
AMD,nbqjwo4,"I always found this setup interesting, like, the gpu is not receiving air directly at all, I know it looks nice and all but, at what point we missed the purpose of getting the best temps we could lol",AMD,2025-08-31 22:04:39,5
AMD,nbqslgf,"Ah, you had a socket 939 back in the day, huh? I've still got a socket 939 Opteron sitting around, maxed out with 4GB of RAM. I ran it way longer then it had any right too.",AMD,2025-08-31 22:56:25,2
AMD,nbr2k5p,What fans are those? Love the Gundam aesthetic!,AMD,2025-08-31 23:57:21,2
AMD,nbs85hy,sick rig.,AMD,2025-09-01 04:28:53,2
AMD,nbs8g36,"I love how there are so many types of pc builds. I've loved heavily into the SFF space with absolutely minimalist design, and then these is the complete opposite. Yet we can all appreciate the spectrum",AMD,2025-09-01 04:31:09,2
AMD,nbs8ktx,This build is so sick,AMD,2025-09-01 04:32:11,2
AMD,nbv353d,OLOCO QUE MÃQUINA TOP,AMD,2025-09-01 16:38:35,1
AMD,nbrjmjr,Glad to have you back and nice build,AMD,2025-09-01 01:42:56,1
AMD,nbr97oe,Big issues for the LCD Lian Li's?,AMD,2025-09-01 00:38:24,6
AMD,nbr9usq,"Never had an issue with them other than using too much CPU on animated or monitoring hardware.  As for the troubles it's likely that most people try to split the USB port inside. They really need their own ports alone. That's why I used cables that connect to ports outside in the back, and I kept my 2 internal USB headers for my other stuff. Now, not sure if they released their ""wireless"" version since then.",AMD,2025-09-01 00:42:24,3
AMD,nbty3zp,It's Lian Li things,AMD,2025-09-01 13:09:02,1
AMD,nbr3rqg,Yep! Really like the case a ton.   LG Ultragear OLED Curved 39â€ (39GS95QE). They have a 45â€. Canâ€™t even imagine. Wouldnâ€™t fit on my desk. Haha,AMD,2025-09-01 00:04:45,2
AMD,nbr94fw,Thank you! Feels so nice to be done and it all working.,AMD,2025-09-01 00:37:51,1
AMD,nbu4uga,"Haven't done much benchmarking yet. CPU idles at around 35-40C and 50-55C during intense games with the GPU around 75-80C. (Darktide, Cyberpunk, Kingdom Come 2). Haven't OC'ed anything yet. Just been enjoying the build before I do.",AMD,2025-09-01 13:47:22,1
AMD,nbu5edl,Thanks!,AMD,2025-09-01 13:50:24,1
AMD,nbr2osl,I doubt it really cools the GPU that much more. But I really like the design of the angled fans. Cable management is mega nice too. Had a Lian Li O11 Dynamic case before. Wanted something different.,AMD,2025-08-31 23:58:09,3
AMD,nbrg1re,"It has no front-facing fans, so all the air is going to slipstream across to the clear panel and out the exhaust fans, leaving an area of negative pressure around the motherboard. That makes the back fans and bottom fans contribute little to cooling. The lack of air across the motherboard and VRM heatsinks might cause some problems.",AMD,2025-09-01 01:20:21,1
AMD,nbqm5s1,Yea this case always looked so weird to me. I get that it's popular but it just looks so strange and isn't optimal for the gpu. However to each their own.,AMD,2025-08-31 22:17:39,0
AMD,nbr3xoj,"Yeeep, lasted a long while!",AMD,2025-09-01 00:05:46,2
AMD,nbr4zl8,Lian Li TL LCD Wireless. Love the sides of the fans and more chill RGB with the lines. Last fans I had were some Corsairâ€™s where the entire blade lit up. Wanted something a bit more stealthy.  And thanks over the Gundam comment. Iâ€™m big into the model kits as a side hobby. Mega relaxing.,AMD,2025-09-01 00:12:14,3
AMD,nbrothw,Oh yeah. I had 4 return them with in 3 weeks..... check lianli sub reddit for more info.,AMD,2025-09-01 02:14:50,5
AMD,nbrp3vc,I was dealing with software issues. My wireless wiring was correct. I switch back to SL infiniti after 3 weeks,AMD,2025-09-01 02:16:37,1
AMD,nbr51p3,That 45â€ (5k version) is $2k and Iâ€™ve had to walk away from it in my cart multiple times ðŸ¤£  I love that case. Almost bought it (the non-vGPU model) but 11 Noctua fans with the case just added too much cost to my build (for now).   Looks great man ðŸ”¥,AMD,2025-09-01 00:12:36,2
AMD,nbsahlh,"please switch the GPU to normal orientation, the case is not intended to combine the glass airflow guide with vertical GPU mount",AMD,2025-09-01 04:47:22,-3
AMD,nbr6stw,Crazy pricey!   Itâ€™s my first time to have a vertical mount for the GPU. Just dig the look of that and the bottom fans angled. And yes! It adds up so quick.   Thanks!,AMD,2025-09-01 00:23:23,2
AMD,nbsl56o,... What? The glass airflow guide is *only* included with the vertical GPU version of the case. The horizontal GPU version has a normal horizontal fan bracket that doesn't angle the fans. What are you talking about?,AMD,2025-09-01 06:16:42,3
AMD,nbqds1z,Looks good. Play on playa ðŸ˜Ž,AMD,2025-08-31 21:30:57,6
AMD,nbqkajo,Love the build. Might go this route myself once I upgrade.,AMD,2025-08-31 22:06:51,5
AMD,nbrpscl,#HUGE FANS,AMD,2025-09-01 02:20:51,4
AMD,nbuawsy,GOATED case. I love this case so much.,AMD,2025-09-01 14:19:47,3
AMD,nbtdz2j,Perfect ðŸ”¥,AMD,2025-09-01 10:46:56,2
AMD,nbrrmwu,Strong ðŸ’ªðŸ¼ congrats!,AMD,2025-09-01 02:32:32,1
AMD,nbs1pl7,"I love this case so much. My current rig is in it as well and I havent had to dust the inside in years, the mesh is perfect. Looks great!",AMD,2025-09-01 03:40:56,1
AMD,nbs5p3x,beautiful.,AMD,2025-09-01 04:10:17,1
AMD,nbsa6i7,idk man Iâ€™m kinda happy that AMD is the King of the hill again,AMD,2025-09-01 04:44:55,1
AMD,nbtkhig,"I have the non-RGB 216.  Sometimes I think about swapping to the RGB fans, but I'm not sure if they can be bought separately.",AMD,2025-09-01 11:39:07,1
AMD,nbue0jv,"Can only recommend it, was really easy to build!",AMD,2025-09-01 14:35:33,2
AMD,nbudgpr,"Yeah i really like it! Excellent airflow, easy to build in, good price what do you want more? And no fishtank like 90% these days.",AMD,2025-09-01 14:32:46,2
AMD,nbtsv5v,I don't think you can buy them. Maybe direct from Lian Li.,AMD,2025-09-01 12:36:52,1
AMD,nb4regh,"I have not seen a full Wraith with a CPU for a while now, nor can I see I see Spires very often, seems all I see are the Stealth which was always a poor cooler. With the price of decent coolers now being so low, I would rather see the chips solid without a cooler a small price reduction.",AMD,2025-08-28 13:45:42,169
AMD,nb4yc1s,"It's a shame they're discontinuing the Spire and not the Stealth, that's a slightly better cooler for the current crop of lower end AMD stuff. But I understand the stealth is much cheaper to build and ship out, it's just that it was always borderline e-waste :-/",AMD,2025-08-28 14:20:07,27
AMD,nb4o12d,"Without competition, amd is doing the exact same shit Intel did with their stock coolers.   Remember, amd is not your friend.",AMD,2025-08-28 13:27:54,157
AMD,nb68xu7,"I've used the same Wraith Prism on a 2700X, 5600, and now a 5700X3D. It's served me well.",AMD,2025-08-28 17:58:05,9
AMD,nb4y82c,"i didnt know that they even still put coolers in their cpus since all the am5 cpu box pics looked too thin to include one. I havnt looked deep at am5 since im waiting for at least 2nd gen am6 till i upgrade again and honestly thought that they stopped including them after am4.   Every am4 chip iv bought iv gotten the prism, or the original wraith with the r1700, and just thought it was the standard and that the spire and stealth were phased out after am3, its not like i was going to research it further for worse heatsinks but looking at them now the spire and stealth were only am4/5",AMD,2025-08-28 14:19:34,5
AMD,nb4xye7,"I think more lower end processors should come with them. Basically all Ryzen 5â€™s other than maybe those with 3d cache. The 5800xt however does not need a wraith prism and should really be cooled by a better cooler. I think adjustments being made are a good thing, but it should be both give and take, not just taking away stock coolers.",AMD,2025-08-28 14:18:16,7
AMD,nb6gvd7,I gave my Wraith Prism away. A Hyper 212 was $20 and performed much better than it.,AMD,2025-08-28 18:35:52,4
AMD,nb7gtnv,I still use my wraith prism. It is so awesome for a stock cooler,AMD,2025-08-28 21:29:33,4
AMD,nb4sgmg,Does anyone know who builds the coolers?,AMD,2025-08-28 13:51:08,3
AMD,nb79nk3,I had a wraith prism on my 2700x. Was a good looking and decent cooler.,AMD,2025-08-28 20:54:49,2
AMD,nb9fvof,"AMD's box coolers used to be a big deal back in the early days of Ryzen, but thankfully better aftermarket coolers can be had for dirt cheap these days so I can see why they're being discontinued. There is still an argument about saving money by not needing to buy a cooler in the first place, but with how much you're already spending on a brand new PC are you really that bothered by another $20-30?",AMD,2025-08-29 04:41:08,2
AMD,nb4reo2,I donâ€™t know it still came with anything. I thought AM4 was the end of them outside of stealth coming with the R5 cpus. Iâ€™m clearly out of the loop.,AMD,2025-08-28 13:45:44,3
AMD,nb5hm3g,"prism, like one of the best ones",AMD,2025-08-28 15:51:19,1
AMD,nb6ho90,"I guess my main problem with this is how unstable the market has been for entry-level coolers.  For example, right now in Canada here are the cheapest regular price AM5 coolers I can easily get:   \-Newegg.ca: ""JONSBO CR1400"" for $42 CDN   \-Memory Express: Arctic Cooling FREEZER 7 X CO $34.99 CDN   \-Canada Computers: DeepCool GAMMAXX AG400 $24.99 CDN   \-Amazon.ca: Vetroo V3 $19.99 CDN  (Not counting any of the Hyper 212s currently on sale)  Like if someone puts together a build guide for a budget PC, whatever great value cooler they name is a crapshoot whether or not you're going to be able to buy it here. It was insanely bad during COVID, and the current global trade situation is probably not going to help in the long run.  My secondary problem with this is that I always thought the Wraith Prism was super cool with some of the best lighting effects I've ever seen, a good cooler design for its small package... it would have been cool to see AMD/CoolerMaster to develop it a little more. Oh well. I least I have a spare one, waiting for a good excuse to be cycled back into operation",AMD,2025-08-28 18:39:53,1
AMD,nb7g51w,"Thermalright Phantom Spirit 120 EVO, under $50 and one of the best air coolers on the market.",AMD,2025-08-28 21:26:02,1
AMD,nb893yv,"I think it's good. Those stock coolers, although not terrible like the ones Intel had, weren't good. If they can decrease the price of the boxed processor by 5-10$, then a 15-20$ cooler from Amazon will be a much better choice for 99% of people building a pc.  It also makes sense marketing wise. If someone who doesn't know much about cpus and coolers buys an amd cpu and uses the stock cooler, he will complain about the high temps / loud noise. This won't happen if he instead just buys a way better cooler for like 5-10$ more.",AMD,2025-08-29 00:06:27,1
AMD,nb8np2y,"They're getting rid of the two better ones but keeping the worst one lolol. I have used all 3, the prism and spire are better than the stealth.  In one case, the stealth just did the bare, bare minimum for a R5 5600x and I needed to buy an aftermarket cooler. In another rig I'm using a prism for an R5 5600  and it does an amazing job believe it or not lol",AMD,2025-08-29 01:32:34,1
AMD,nb906mt,"You can get a decent cooler for so cheap nowadays, is amd still going to package with some CPUs or going the way of the dodo. Stock coolers do a good job for the majority of folks",AMD,2025-08-29 02:49:26,1
AMD,nb9feyi,"Sucks for those real tight on cash but oh well, the aftermarket cooler space is great anyway considering effective tower ones are still rather cheap, like 50AUD, 30USD is getting you a dual tower. And you can still go cheaper with a single tower that's still gonna beat a stock cooler and most Ryzen users probably already have a cooler so most models won't need them and the poor coolers will be e-waste or if they're lucky sold off on some marketplace cheaply lmao.   Of course it's good if AMD keeps some CPUs to have a stock cooler, and I'd imagine in more mass purchase scenarios it could be useful.",AMD,2025-08-29 04:37:28,1
AMD,nba0ubl,i saw the 9600 has a wraith included in it... anyone know how it performs? wanted to see tests if it was enough for certain uses,AMD,2025-08-29 07:47:35,1
AMD,nbahgl5,No problem. Just get whatever Thermalright makes. No big cost.,AMD,2025-08-29 10:22:18,1
AMD,nbi2fo7,I think I have 3 or 4 Wraith Prisms...  I use 2 on my old Tyan S2927 Board (Dual Opteron) and also 1 for my Athlon X2 5200+ EE. They do wonderful on chips that are under 90W TDP.,AMD,2025-08-30 15:02:15,1
AMD,nbkh8m3,"I bought 2 wraith prism and 1 Wraith Max they're good cpu coolers kept my FX, Athlon 860K, and 3900X systems cool",AMD,2025-08-30 22:34:10,1
AMD,nbsfe9p,Still installing them on AM4 paired with 5700x3d (prisms) along with some AM5 builds as they are honestly one of the best looking and quiet stock coolers that more than handled the tasks.,AMD,2025-09-01 05:27:38,1
AMD,nb4trp3,"Also, AM5 is backwards-compatible with AM4 coolers and, since many AM4 owners are upgrading, or more likely to upgrade, to AM5, therefore reusing their coolers, bundling a cooler doesn't make much sense now (especially the Stealth, which I promptly replaced it with a Deepcool Gammaxx 400 because my 3600 was reaching 85Âº with that thing).   >I would rather see the chips solid without a cooler a small price reduction.  But will they really reduce the price?",AMD,2025-08-28 13:57:45,31
AMD,nb4t0fi,"Yeah, 20 bucks gets you a proper tower cooler.",AMD,2025-08-28 13:53:56,49
AMD,nb7hljn,"It's funny how the ""Stealth"" was the fucking loudest fan ever lol",AMD,2025-08-28 21:33:38,5
AMD,nb52g4w,Some years back you could buy TRAY variants of the CPUs that had no coolers.   It is just an issue with warranty because those are supposed to go in bulk to system builders so they don't have the usual 3 year warranty. If you are in the EU you still get the 2 years EU warranty which is fine (either you have a dud and it won#t take 2 years till you have issues or its fine and an extra year won't do much).  Nowadays those TRAY variants seem to not be available to the public anymore. Or all of the vendors I checked just stopped selling them.,AMD,2025-08-28 14:39:49,8
AMD,nb899wk,Stealth was more than sufficient for early Ryzens. I had one for my 1600 and it did it's job adequately.,AMD,2025-08-29 00:07:24,1
AMD,nb5i92v,"Wraith prism were made by coolermaster, pretty good too. Shame.",AMD,2025-08-28 15:54:18,17
AMD,nb4q0jj,"Best option would be to have an option of buying it with the cooler or not, but that would be a logistical nightmare so it's either this or continuous e-waste which I'm against. My stock cooler is just gathering dust and will eventually end up in a landfil.",AMD,2025-08-28 13:38:25,62
AMD,nb4r1og,I always find it funny when people are like i know nvidia is better than amd and even for the same price i ll go amd because i dont wanna give my money to greedvidia ... acting like amd is running a charity,AMD,2025-08-28 13:43:51,4
AMD,nb6uf1d,"No corporation is your friend, especially private, for profit ones.   They only care about quarterly profits. Exclusively. Thatâ€™s it. If they get that by being nice to you, then sure, theyâ€™ll be nice. But the second they make more money by screwing you, they absolutely will.   Itâ€™s funny, because if this were an individual human, we would call that person a narcissist. But with corporations, weâ€™re supposed to *expect* them to screw us",AMD,2025-08-28 19:41:52,1
AMD,nb4q56o,"lmao if I'm not wrong, back when the 3600 released with no competition from intel their prices were like $300.",AMD,2025-08-28 13:39:05,-4
AMD,nb7imb7,"Same here, the old prism with vapour chamber (Ryzen 1000 & 2000) has no problem cooling my 5700x3d.",AMD,2025-08-28 21:39:08,6
AMD,nb53uqg,Yea AMD introduced them with the 2000 Ryzen series. Like you said before that there was just the original Wraith and the older square coolers from AM3(+).,AMD,2025-08-28 14:46:21,6
AMD,nb53ji9,"If they lower prices by 20-30USD (depending on the cooler that came with it) I think most people would be fine with it. I bet a lot of people that assemble their own PCs change that default cooler on the higher SKUs anyways, so a bit less cost would be nice.  And I agree, give all the 65W CPUs the spire and they would be more than fine. Stealth sucks and should be discontinued, even 65W CPUs are running 95Â°C with it.",AMD,2025-08-28 14:44:55,0
AMD,nb4u13g,"Once upon a time I read the ODM was Cooler Master, but don't quote me on that.",AMD,2025-08-28 13:59:03,8
AMD,nb4u8lc,I think it was cooler master,AMD,2025-08-28 14:00:06,2
AMD,nbb3yk0,"Even the Stealth was a beefy looking cooler compared to the small, stock coolers with 70mm fans that came with pre Ryzen AM3 CPUs.",AMD,2025-08-29 12:55:03,1
AMD,nb4nmh6,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-08-28 13:25:47,1
AMD,nb8vlyj,"The hole pattern goes all the way back to AM2 at the very least, so just about any damn cooler works on AM5. I like that they did it like that.",AMD,2025-08-29 02:20:57,5
AMD,nb5lvm5,"Seriously, my Freezer 36 keeps my 5700x under 60c no problem",AMD,2025-08-28 16:11:29,7
AMD,nb5lrf3,"Â£16 gets you a proper tower cooler, a tube of thermal compound instead of a tiny sachet and best of all, a digital screen for displaying thermals.Saves me having to setup a separate small screen(even an old smartphone can be used) for metrics and figuring out where the most aesthetically place is to put it inside the case.",AMD,2025-08-28 16:10:57,5
AMD,nbpo19j,"The Stealth was alright for the couple of months I used it on my R5 3600, but not out of the box. The stock fan curve had an annoying tendency for the fan speed to ramp up and down. Making the fan curve less steep reduced how annoying it was drastically.",AMD,2025-08-31 19:17:23,1
AMD,nb5626f,"Depends on the location I guess, but most stores in my country  sells tray versions of AMD CPUs. They have a small stock of CPUs with boxes, but most stock usually are tray versions.",AMD,2025-08-28 14:56:33,10
AMD,nb6wgb5,">Some years back you could buy TRAY variants of the CPUs that had no coolers.  I bought a R7 7700 for my test-bench in May as Tray version, just the cpu in that plastic holder.  >If you are in the EU you still get the 2 years EU warranty which is fine  I'm in the EU and I got it with 3 years warranty, Monster Hunter Wilds game came with it too.  There are plenty of shops selling Tray/Oem version from the cheapest 8400F all the way to the 9950X3D.",AMD,2025-08-28 19:51:55,2
AMD,nb7asdj,"Aliexpress and other marketplaces are the places to look for tray CPUs, still going strong",AMD,2025-08-28 21:00:08,2
AMD,nbc2vqr,"In Chinese markets TRAY variant CPUs are still readily available, provided that you put a bit of effort searching for it. We also tend to buy motherboard and CPU in a set which has a small price reduction, and they let you select TRAY variant for another usually 100CNY (roughly $13) reduction.  And we still a 3-year warranty if the TRAY seller is still running and you have your records. If they go down there is still a chance in legal aspect too.",AMD,2025-08-29 15:47:13,2
AMD,nbacxl8,"Yes, in Germany you can buy a tray version of the Ryzen 7700 for 190â‚¬. The price was around 200 for more than a year already.",AMD,2025-08-29 09:42:37,1
AMD,nb4v9cl,Many people use the stock cooler though. They're pretty good if you're not going to do anything crazy with it.,AMD,2025-08-28 14:05:09,26
AMD,nb50nzi,"For many AMD CPUs there is the option to buy the boxed (with cooler) or tray (without cooler) version, atleast in my region/my favourite e-/retailers.",AMD,2025-08-28 14:31:17,2
AMD,nb7iham,There is a big difference at least right now between nvidia practices and AMD. So people will go with AMD when they clearly see that nvidia is very evil in comparison.  What you might be eluding to but worded it very wrongly is that people start idolising corporations and celebs/people in high positions. Which is wrong to do.  One needs to remember is that big corporations are never your friend. However if there is business you must do with one then do it with the least evil one.  Although with AMD(so far) is that they make stupid decisions rather than outright being evil like Intel has and nvidia is. However in due time AMD growing will eventually start doing same things. So one has to always be careful with big corporations and never side with any of them to a great extent.,AMD,2025-08-28 21:38:24,3
AMD,nb4w1k0,Show me one person who acts that way.,AMD,2025-08-28 14:09:02,2
AMD,nb4wiz5,intel was very competitive at the time and 3600 was 199$ so you re wrong on both statements,AMD,2025-08-28 14:11:23,7
AMD,nb62gij,"Intel 10th gen was still competitive back when the Ryzen 3000 series released. It was only after with Rocket Lake where Intel took a step back on the CPU front (as rocket lake decreased core counts specifically on the top end, and the rest of the stack didn't change much. 11th gen's saving grace was updated pci-e and that tiger lakes igpu made intel igpu not garbage)  the 3600 was popular because it was priced very well. intels competitor to that (the i5-10400) would come out much later.",AMD,2025-08-28 17:28:15,1
AMD,nb53y8j,$20-30 price decrease on a $10-15 retail price cooler would be wild. They probably pay $2 or less for the cooler. Where they are really saving money is shipping costs being able to ship the processors more densely packed.,AMD,2025-08-28 14:46:49,4
AMD,nb7c9pl,"If they lower prices by 20-30USD      they will not lower the price lmao      Its a company, they love money and they take all the money they can get.",AMD,2025-08-28 21:07:11,1
AMD,nb5zok6,"I know they made prism coolers, but I'm not sure about the spire or stealth.",AMD,2025-08-28 17:15:33,2
AMD,nb8ksd9,For Â£16? No way!,AMD,2025-08-29 01:15:01,4
AMD,nb530va,"I'm still using a Wraith Max, it's great.",AMD,2025-08-28 14:42:32,1
AMD,nb4wun6,They're really not good at all. They're *adequate*.  And they are no longer *adequate* for the TDP that CPU's have reached.  Aftermarket coolers have always been a necessity for anything outside of spreadsheeting and internet browsing.,AMD,2025-08-28 14:12:58,-2
AMD,nb8p7s7,Be it stupid decision or not . Custommer should look for his own interests not adopt the mentality of lets go for the underdog becoz i think x company is badall corporation are bad its just that amd isnt leading the market . Let mr give u an exemple when i bought my 7900xt  till today amd isnt proposikg any upscaler of the level of xess . They did shrunk their drivers team ... u may judge its a stupid decision i  see it as forcing u to go buy new gpu to get new fsatuews that u can get a lite version of those jf they wanted . So they re going the same nvidia road in the end u should go for the better product when the price is same or close enough . Then its up to amd to fjx their stupidity if they do care abt their share and inovate .,AMD,2025-08-29 01:41:51,-1
AMD,nb5g447,"The prism has a solid copper plate and heatpipes I bet you its a lot more than $2 that AMD pays for one. I was only talking about the higher end SKUs that had a cooler, not the CPUs coming with the stealth.  All the tests I have seen the prism was a fine cooler, keep in mind it is also very low profile. It surely isn't a fit for a 150+W CPU or if you wanna OC, but for a 105W CPU it is decent.  I.e. the Silverstone Krypton KR01: costs 21â‚¬ here and is worse than the prism.   An ARCTIC Freezer 8A CO is 17â‚¬ and about the cheapest tower cooler you can get that is decent for its price. But I wouldn't change a wraith for that because you wouldn't gain much but a bit more silence in prolonged high stress scenarios.   There is a reason the prism goes for like $40, it IS one of the best flat coolers, its just most people don't need that.",AMD,2025-08-28 15:44:15,3
AMD,nb8wxxs,Shop online and its there right in front of you.,AMD,2025-08-29 02:29:09,2
AMD,nb4xe3m,Tdpâ€™s have been pretty stable or even going down. The problem now is that the dye size is so small and the heat is so concentrated in one small area that even with an IHS itâ€™s not able to transfer the heat as effectively as in the past with a larger die.,AMD,2025-08-28 14:15:34,4
AMD,nb50q2v,A ton of OEMs use the stock coolers on their low-mid range systems aimed at gaming as well.   There's always someone willing to spend $20 on an extra 5% performance and even more people willing to save $20 to lose 10% off advertised boost or whatnot it seems,AMD,2025-08-28 14:31:34,1
AMD,nb50uj1,"You made the claim, you have to back it up with a source.  If you can't do that, or don't want too, then don't make a fool of yourself by making a claim you can't back up with evidence in the first place.",AMD,2025-08-28 14:32:10,0
AMD,nb6o4ia,"Thatâ€™s fair, the prism does have copper and would be worth more than the spire. I still gotta believe manufacturing costs would be in the $10-15 range. Itâ€™s also kind of in a weird spot in terms of height. Itâ€™s not particularly low profile so wonâ€™t fit in most Itx cases, but isnâ€™t as big a standard basic tower cooler.",AMD,2025-08-28 19:11:03,2
AMD,nb96543,What cooler with a screen is that cheap?,AMD,2025-08-29 03:29:22,5
AMD,nb5097c,"Ryzen 7 3800x TDP: 105w  Ryzen 7 5800x(3D) TDP: 105w  Ryzen 7 7700x TDP: 105w (Ryzen 7 7800x3D TDP: 120w)   Ryzen 7 9700x TDP: 105w (Ryzen 7 9800x3D TDP: 120w)  We can clearly see that the 3D cache requires more wattage to power. And as such, has increased the TDP of these chips.  These are the most popular chips right now, and a spire cooler is only rated for a TDP of 95w.  Without a complete redesign, the spire cooler has become obsolete.",AMD,2025-08-28 14:29:19,0
AMD,nb51kzv,"I'm still waiting on sub-zero chilling becoming a requirement for high end computing. For a while at least.  For some reason I just see heatpumps in the near future (like in a decade) for a while after some semi-solved materials breakthrough for exotic parts, like qubits but maybe not as esoteric by that time or something haha.",AMD,2025-08-28 14:35:43,0
AMD,nb51dsj,"Yes, but why stoop to call8ng me a fool? What warrants being such a nasty person to strangers? Is your home life bad?",AMD,2025-08-28 14:34:46,0
AMD,nb51pmd,You can't justify being that rotten can you? People treat you like trash irl so what's the harm in insulting strangers to let the stress out?,AMD,2025-08-28 14:36:20,-1
AMD,nba4zvm,I think they are talking about the Thermalright Assassin Spirit 120 Vision or the Thermalright Assassin X 120 R Digital.  Â£16=$21.56 USD and they are $27.90 and $24.90 on Amazon which is very reasonable.     https://pcpartpicker.com/product/bmdG3C/thermalright-assassin-spirit-120-vision-argb-6617-cfm-cpu-cooler-assassin-spirit-120-vision-white-argb  https://pcpartpicker.com/product/tWbypg/thermalright-assassin-x-120-r-digital-argb-7084-cfm-cpu-cooler-assassin-x-120-r-digital-argb-black  .................  Thermalright has the entire air cooler market locked down right now and are almost always the best choice at every price point. Pcpartpicker has 208 AM5 compatible Thermalright coolers which is the same as Noctua+Cooler Master+Corsair combined.,AMD,2025-08-29 08:27:04,4
AMD,nb51kcf,"Give the spire to all 65-95W CPUs, the prism for 105-120W CPUs and everything better than that people will buy a 3rd party cooler anyways.  Then with the next CPU generation they can do a redesign on the coolers to save some money. Or well CooleMaster can do the redesign, AMD doesn't make those themself.",AMD,2025-08-28 14:35:38,4
AMD,nb512z0,"9700x tdp is 65w. Only after release and after people complained did AMD allow motherboard manufacturers to add a setting in the bios to increase the TDP to 105 W.  But other than that, thank you for confirming that the same class of chip has been stable or decreasing just like I said, lol",AMD,2025-08-28 14:33:19,-1
AMD,nb54yi8,Where did you get these TDP values from? 7800X3D 120W? [It never goes past 85W](https://tpucdn.com/review/amd-ryzen-7-7800x3d/images/power-per-application.png). And 9800X3D can pull up to [160W](https://tpucdn.com/review/amd-ryzen-7-9800x3d/images/power-per-application.png),AMD,2025-08-28 14:51:30,-1
AMD,nb5294r,"I disagree. Computer parts are becoming ever more efficient and drawing fewer and fewer watts. Itâ€™s far more likely that mobile ships will start to take over the low end and medium range Desktop chips. Sooooo much more energy efficient and it decreases sku count for what they need to manufacture and when paired with even a wraith stealth, the chips can boost higher in a desktop form factor due to much better cooling.",AMD,2025-08-28 14:38:55,0
AMD,nbawary,"The one with the plain, non RGB fan is cheap as chips.  The screen only shows 1 metric (temp or load) and its either GPU or CPU, not both.Consider a dual tower cooler with more screen estate if you want to see more readings displayed.",AMD,2025-08-29 12:09:52,3
AMD,nb530fp,"Personally, I think they *are* redesigning the cooler.  But it would seem they also don't want to produce a cooler that is obsolete in the meantime either.  I really don't get what the big deal is personally, Aftermarket coolers have ways been hugely better then stock, and this only really affects people who buy thousands of CPU's at a time.",AMD,2025-08-28 14:42:28,2
AMD,nb51ekw,"You're missing the point, but whatever.",AMD,2025-08-28 14:34:53,1
AMD,nb55d2n,"Google. And TDP is a calculation of heat generated, not of watts used.",AMD,2025-08-28 14:53:21,-2
AMD,nb52oq6,"I did say high-end. There's always a divergent path for HEDT where they don't look at maximising EE like for mobile or datacenter/SC cluster use.  It's just the loose threads they gather up at the high end and deliver regardless. Besides, if there were some material breakthrough it'd be for use in something we're not doing now anyway haha. So who knows what form it could take.  It's all just imaginationland when looking out that far heh.  ETA: And besides that, as density goes up so does power use, moors law doesn't scale like that lol. We get more compute but it's not free... Computers are using more electricity than ever. Home computers used to use like less than 20W. They're more efficient now to be sure and we can pull a lot of performance from 20W but that's not what they want to sell us.  Now they want everyone running computationally expensive local AI models so they can keep selling us hardware haha.",AMD,2025-08-28 14:40:56,0
AMD,nb5df4q,"Hey OP â€” Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-08-28 15:31:19,1
AMD,nb52x2t,I think you could use a break if you spend your time here insulting strangers over computer parts companies.,AMD,2025-08-28 14:42:02,-1
AMD,nb6nkhn,"Well whatever you pulled out of google is clearly wrong, and I have no idea what you're trying to say with ""TDP is a calculation of heat generated, not of watts used.""  The amount of watts used is directly related to the amount of heat generated.",AMD,2025-08-28 19:08:20,0
AMD,nb53hrj,"I've never seen someone so triggered over being called a fool before, I'm definitely gonna do it more often, after this interaction, lol.",AMD,2025-08-28 14:44:41,1
AMD,nb6nxt4,"No it isn't, I double checked the sources and they were all correct.  Also, you're wrong.  https://en.m.wikipedia.org/wiki/Thermal_design_power",AMD,2025-08-28 19:10:09,1
AMD,nb53qp9,Im honestly having fun too <3,AMD,2025-08-28 14:45:50,1
AMD,nax2neb,Welcome back 7900 GRE 16GB,AMD,2025-08-27 09:33:01,153
AMD,naxcipv,"The rumor is wild  * Improved AMD Radeon RX 9070 GRE * Modest VRAM increase, with a larger memory bus and bandwidth * Probably the same amount of cores, but could be more approaching the AMD Radon 9070 non-GRE core count   * Nothing like a Nvidia Super increasing VRAM by 50% * Nothing to take away the top-dog status of the AMD Radeon 9070XT  Take away, that China market must be special to spin out a refresh, the rest of the world would love a 9070XT Super",AMD,2025-08-27 10:56:51,59
AMD,nax52bb,"In my market the premium 9070 are as expensive as the cheaper 9070XT. I like the idea, but at least here the difference is so small that anyone that went for the GRE would just go for the XT.",AMD,2025-08-27 09:55:00,57
AMD,naxb62x,"So basically EXACTLY the same as RX 9070, with slightly less cores.",AMD,2025-08-27 10:46:32,22
AMD,nax3rp3,"Might be an interesting choice, between RX 9070 and RX 9060XT is still a price gap.",AMD,2025-08-27 09:43:37,24
AMD,nayoqlm,That the kind of card i've been waiting for. The 9060 128 bit bus doesn't inspire longer term confidence and the 9070 was way out of my budget.,AMD,2025-08-27 15:26:28,5
AMD,naxyhx9,449 do it AMD .,AMD,2025-08-27 13:16:18,5
AMD,naxa0w0,"not sure about the gre this time.  watched a comparsion video recently between 9060 xt, 9070 GRE, 9070 and 9070 XT.  the 9070 GRE was only marginally faster then the 9060 xt and it only had 12 gb of vram. It looked like a card that makes no sense at all.  at least this one has 16 gb. but as the performance is very close to the 9060 xt, the question what sense this card makes remains. plus selling two cards with different ram sizes with the same name is a crappy trick amd should have left to nvidia.",AMD,2025-08-27 10:37:18,7
AMD,nay64tm,That's GREat news!,AMD,2025-08-27 13:56:15,2
AMD,naysark,"Thats nice and all but WHERE IS THE 32GB VERSION for ai work?? Like seriously? AMD sounds like they REALLY want to give all the AI market to nvidia without doing a thing about it.  If they did a 32gb version of the 9060XT and 9070XT im sure a bunch of consumers would buy it for at home AI work... thats the next market... because WHO wants to let big corporations control everything you do with AI? And even have to PAY them for it anytime you want to do something? Hell no. The future is running AI locally and for that you need AT LEAST 24gb of vram. Whoever provides consumers with 32gb vram for AI work for reasonable prices first gets a lot of sales. And lets be real, 16gb of GDDR6 costs like 35$ at most. So they could sell a 32gb 9060XT-9070XT for 50$ more than the normal version and still make a profit... and it would make nvidia mad as hell... because it would make it look like the greedy bastards they are to every gamer on the planet.  You cant run local AI on 16gb. You need at least 24gb of vram. AMD needs to get that through their thick head.",AMD,2025-08-27 15:43:47,2
AMD,naxn5zu,What does GRE stand for?,AMD,2025-08-27 12:10:20,1
AMD,naxxtuc,If they can make a short 9070 it would still be a significant improvement over available options. Afaik right now the only short-length new cards are 5060s and 9060s unless you go Watercooled.,AMD,2025-08-27 13:12:39,1
AMD,naye0cb,Will it too be 100-200 above MSRP as well as the other 9070 class GPUs?,AMD,2025-08-27 14:35:10,1
AMD,nazshvn,"In Germany the 9060XT cost 350â‚¬, 5070 is 550â‚¬ and 9070 is 570â‚¬  Would be nice to have something around 450â‚¬",AMD,2025-08-27 18:33:05,1
AMD,nb4lygu,If this actually has 16GB it could be my next gpu(with 9070 non XT as the other choice).,AMD,2025-08-28 13:17:00,1
AMD,nayie9g,cant wait,AMD,2025-08-27 14:56:18,0
AMD,nb11xun,Hopefully it follows the same path. The GRE was a great value GPU,AMD,2025-08-27 22:11:34,15
AMD,naxyjlw,Should be quite favorable against the 18GB 5070 super in price performance.,AMD,2025-08-27 13:16:32,6
AMD,naxhyu1,The 9070GRE is a cut down of the 9070. So it should be along the lines of the low end of the 9070 cards,AMD,2025-08-27 11:36:11,45
AMD,naxsxh5,"Such an odd one. The gre was less than the xt for the 7900xt, better than the 7800xt  But now that we already have a card that's non xt, the gre is less than that? Such an odd place to sit. But I guess there's far more room between the 9060xt and 9070 than there is between the 9070 and xt",AMD,2025-08-27 12:45:04,6
AMD,naxt01j,If it delivers it'll be a good gpu for $450,AMD,2025-08-27 12:45:29,7
AMD,nb126dk,$400 to crush NVIDIA,AMD,2025-08-27 22:12:45,3
AMD,nb4m348,If it's in the $400 range I would definitely buy it!,AMD,2025-08-28 13:17:40,1
AMD,nayopb2,The 7900 gre (which is the performance equivalent to the 9070 gre) is 25% faster than the 9060 XT. It's not a negligible gap. The 9070 GRE with more memory would be maybe 30% faster in cases where it was previously memory limited.  It makes sense as a card. The gap between 9070 and 9060 XT is too large.,AMD,2025-08-27 15:26:17,5
AMD,naxzb1k,The 12GB version only had a 192 bit memory interface due to the left out memory chips. Maybe the 16GB performs better.,AMD,2025-08-27 13:20:41,3
AMD,naxio3x,"If performance is close to a 9060XT, then it's just a way to bin faulty 9070XT/9070 dies that didn't make the cut",AMD,2025-08-27 11:41:02,2
AMD,nb381w3,">the 9070 GRE was only marginally faster then the 9060 xtÂ   I don't know what video you saw, but 9070 GRE is **29% faster** than 9060 XT. Source: [computerbase.de](https://www.computerbase.de/artikel/grafikkarten/amd-radeon-rx-9070-gre-china-test.93409/seite-2#abschnitt_performancerating_mit_und_ohne_rt_in_2560__1440) review of 9070 GRE",AMD,2025-08-28 06:44:34,2
AMD,nayys0u,"They have a 32GB version under the pro brand (workstation).  But much more expensive, because they (as well as the other GPU companies) know entreprise customers can pay the higher price, and AI entreprise market is way bigger than rich gamers market",AMD,2025-08-27 16:14:36,4
AMD,naxnv1g,It used to be Golden Rabbit Edition to commemorate the Year of the Rabbit in Chinese Calendar in 2023 but has since been changed to Great Radeon Edition since the Year of the Rabbit has passed,AMD,2025-08-27 12:14:35,10
AMD,naxy1mc,I see. So it's between 9070 and 9060xt?,AMD,2025-08-27 13:13:50,18
AMD,naxy8dx,"The 9070 is already 10% cut down from the full chip. I doubt they have that many bad chips for the GRE, and now with them also getting 16GB, the only real manufacturing price savings are also gone.",AMD,2025-08-27 13:14:51,10
AMD,naxyv7w,"maybe they want to keep the price of the 9070 up, so they push the 16GB gre between there so the don't have to price drop the original model.",AMD,2025-08-27 13:18:19,2
AMD,naxv8v0,Would have loved to see the 9070 GRE 12GB if that means it could have been cheaper than 9070 GRE 16GB.  Don't really need 16GB of vram unless I am paying more than let's say ~$450  And then with pricing looks like this in mind:  9060 xt 16GB ~$350  9070 GRE 12GB ~$450  9070 GRE 16GB ~$525  9070 ~$600   9070 XT ~$700   I know 9070 and 9070 XT were supposed to be $50 lower... but reality is different,AMD,2025-08-27 12:58:22,2
AMD,nb1d712,$380 to make Jensen angry and gamers happy,AMD,2025-08-27 23:12:26,1
AMD,nayy1rt,"it is extremely optimistic to assume that a 25% increase in memory bandwith will result in 30% higher performance. lets wait for the benchmarks.  \> The 7900 gre (which is the performance equivalent to the 9070 gre) is 25% faster than the > 9060 XT.    does not reflect test results. the performance differene between the 9060xt and 9070 gre where a few percent, not 25%.   \> It makes sense as a card. The gap between 9070 and 9060 XT is too large.   maybe. have not seen test result for the 16gb version. the 12 gb version hardly fills that gap because its too close to the 9060xt.",AMD,2025-08-27 16:11:06,0
AMD,naxlznf,"i understand amds motivation to sell the card. i do not see me being motivated to buy it. matter of pricing, of course.",AMD,2025-08-27 12:02:53,1
AMD,nbk1d3k,"Well thats nice and all but soon (but really... already today) everyone will want to run AI locally... because nobody wants to give money to an AI corporation to do AI stuff, they control everything, censor it, record your info... its only downsides.  But yeah as usual, enterprise are gonna get the AI goodies first then later the consumers are gonna have them... just wished for once that a company would prioritize normal people instead of rich assholes. Guess not",AMD,2025-08-30 21:04:39,1
AMD,nazdj7j,I like to imagine we are in a groundhog day situation and its going to be year of the rabbit forever,AMD,2025-08-27 17:24:46,2
AMD,naxoysg,Cool thanks!,AMD,2025-08-27 12:21:18,1
AMD,nb6o1im,Wish they didn't change it. Then we could have the GCE in 2029.,AMD,2025-08-28 19:10:38,1
AMD,nb8akww,"We don't know, but just expect its gonna be cheaper than the 9070 unless the VRAM pushes it up. Its more about demand I think, and if there is suppy. Ultimately the smartest thing to do when buying a GPU is to wait a while after launch to see how prices look.",AMD,2025-08-29 00:14:54,2
AMD,nay3hpq,"Looking at the European prices where the 9070XT is already closing into 630-650â‚¬ territory while the 9070 is barely going below 600â‚¬, I can totally see an scenario where the 9070 is discontinued and all the cut-down SKUs (both 9070-worthy chips and all of them that are too defective to make a 9070) are suddenly turned a 9070 GRE 16GB for 480-500â‚¬ (around $450 in the US) to undercut the 5070 with a similar performance.",AMD,2025-08-27 13:42:41,9
AMD,nazhs54,Ok but they can only charge what people are willing to pay for it. It's lower class than a 9070 so people won't pay the same as a 9070,AMD,2025-08-27 17:43:57,2
AMD,naz8ph7,"I'm not sure who you watched but the 25% figure comes from Tech Power Up who do frequently test their cards to reflect any changes and they're reputed.   So, no. It's not gonna increase performance by 30% but it doesn't have to. It's just to alleviate the cases where a lack of memory is a bottleneck.",AMD,2025-08-27 17:02:12,2
AMD,nbv7jws,"Itâ€™s not inherently user error since it is the boardâ€™s doing, but like, why on earth would you see your motherboard blow up your CPU, then get a replacement and plug it into the SAME motherboard??",AMD,2025-09-01 17:00:01,1
AMD,nbv8g7y,Lightning seems to strike twice ðŸ˜µâ€ðŸ’«,AMD,2025-09-01 17:04:23,1
AMD,nbv8kdr,This got to win some Darwin award.,AMD,2025-09-01 17:04:57,1
AMD,nbv7g6e,This is why I like msi,AMD,2025-09-01 16:59:32,1
AMD,nbv8jvk,"Both products under warranty, no risk losing any substantial money, just time.  If you have a lot of free time and said system is not critical to any work related assignments its not exactly detrimental.  Consumer protection and return policies does not exist in every country, sometimes you just have to live with the product you bought till the manufacturer fixes it.  Of course you can just sell the board and get a different non faulty one if you cherish your time.",AMD,2025-09-01 17:04:53,1
AMD,nbud37w,still waiting to see what the new Xbox Ally handhelds will cost,AMD,2025-09-01 14:30:52,7
AMD,nbuiofs,"Really hope one of them would release a mini, like same size as a switch lite with the power of a steam deck",AMD,2025-09-01 14:58:42,1
AMD,nbv8i1e,Isnâ€™t this expected to not be significantly faster than the Lunar Lake equipped one? What is MSI doing?,AMD,2025-09-01 17:04:38,1
AMD,nbuap6w,"Â£850 for handheld gaming on steroids?! Take my money, MSI! The future of portable PC gaming is looking brighter by the day!",AMD,2025-09-01 14:18:42,1
AMD,nazbygu,"CPU releases seem to be less interesting nowadaysâ€¦ more ram, more bandwidth please!",AMD,2025-08-27 17:17:44,33
AMD,nazktxq,Honestly Iâ€™d love a igpu thatâ€™s actually pretty competent at a non exorbitant price,AMD,2025-08-27 17:57:33,26
AMD,nb1akig,"It really depends on the usecase when CPU releases are interesting or not.  I'm more interested to see how much cpu performance they can pull out in power limited situations. strictly power limited situations like pc handhelds is a fun usecase to look at, and is one of the current reasons why the Z1 and Z2 series of apus for handhelds are kinda ass. (CPU arguably takig too much of the power budget)",AMD,2025-08-27 22:58:10,2
AMD,nb58731,"If Nova lake is releasing in 2027 like the leak says, it could quite possibly be using DDR6 which should massively increase bandwidth and capacity. It would also make sense if they are planning to support that platform for multiple generations, and would be able to feed the massive core counts they are advertising. I see nova lake as more of a non pro threadripper killer than a gaming cpu but it could be that as well.",AMD,2025-08-28 15:06:30,1
AMD,nb5p66a,and that would require more bandwidth more than anything,AMD,2025-08-28 16:27:07,5
AMD,nb1laq3,"Agreed. Thereâ€™re still some niches like HPC on desktop or efficient but powerful mobile/handheld CPUs, but in general 10% performance improvements are not interesting enough anymore. Iâ€™d like to see their GPU/NPU be more usable for real AI applications.",AMD,2025-08-27 23:57:46,0
AMD,nb5pss7,I can see AMD increasing the number of memory channels in TR both Pro and non-Pro with the rumour of Zen 6 Epyc maxing out at 16 channels,AMD,2025-08-28 16:30:07,1
AMD,nb61egx,"So personally I donâ€™t think it needs more (for a number of reasons but the proximity to the cpu is one of them). Now Iâ€™m not saying it shouldnâ€™t have a good amount of bandwidth, just that it doesnâ€™t need more, itâ€™s not gonna be a extremely high end chip (in terms of desktop counterparts). I just want the ai max in a laptop for like $1200",AMD,2025-08-28 17:23:24,4
AMD,nb843d0,And then you proceed to name the most high end chip with the most expensive packaging in the whole consumer cpu world,AMD,2025-08-28 23:37:37,6
AMD,nb8auuz,"Yeah because that matches my workload? Like I know itâ€™ll most likely take a few generations but at the same time the laptops with similar performance are like $1000. so something that costs double the competitors is exorbitant. And Iâ€™m saying that it doesnâ€™t need more bandwidth, it just needs to be cheaper.",AMD,2025-08-29 00:16:29,2
AMD,nbcirf2,>	at the same time the laptops with similar performance are like $1000.  Which laptops?,AMD,2025-08-29 17:03:56,2
AMD,nbclque,Rtx 4060 laptops  Amazon has a Acer ryzen 7 8845 and 4060 for $999  I do acknowledge the difference in cpu core count and ram of this specific laptop but the raw performance (specifically for games) is about the same (the cpu is about half the cores and I acknowledge that too but that should be about a $200 (maybe 300 and at most 400) difference if weâ€™re looking at price to performance but the price difference is like $1000 for a laptop)   https://www.amazon.com/Octa-Core-Capable-Processor-GeForce-ANV16-41-R5J0/dp/B0CXY573W9/ref=mp_s_a_1_12_maf_2?adgrpid=1342506175986059&dib=eyJ2IjoiMSJ9.TuSD4mcnoZSWpbnCM2lU8nXTNISKySTFcx0yltxptmgScGohIkfOG9a1qrTVCyijRXdpRAA9Yiaco6AEiqkgeD8msq0YI5OErr3rFmsgYaZluc88J9-45igPq22Vj12B8QOhc-KloNe8pv9pZhIjyUiTPgoEG8FlCQPN5olVYxq6gyGhgilMGjEpCYY9MTmJoI8TXFBlmN9_e1KnSBJ6iA.4y1Aenlq876Zrq7HDCs2EtOxlRYJYLOOxel_3ZvqT2c&dib_tag=se&hvadid=83906910696745&hvbmt=be&hvdev=m&hvlocphy=103497&hvnetw=o&hvqmt=e&hvtargid=kwd-83907759466379%3Aloc-190&hydadcr=20133_13293593&keywords=rtx+4060+laptop&mcid=5906934aa0b031049a674246492fe619&msclkid=09e186aad2c31ea18c2dee1b0d5d543e&qid=1756487587&sr=8-12  Hereâ€™s a link if you want.,AMD,2025-08-29 17:17:48,1
AMD,nbdfn8c,If youâ€™re only concerned with gaming performance of course itâ€™s not price competitive to other options.,AMD,2025-08-29 19:44:55,2
AMD,nbdh7py,"I understand that, I have other reasons for it but I do want itâ€™s relative performance to be comparable. If that makes sense. Like I completely understand why itâ€™s so expensive I just hope that the next generation will be more reasonable for the general public instead of effectively having a niche. If that makes sense.",AMD,2025-08-29 19:52:50,1
AMD,nbn46kq,If they are launching this late why not go with the Z2 models? 48.5Wh probably won't be amazing with a full 370.,AMD,2025-08-31 10:46:01,27
AMD,nbnl38v,Oh FFS... just launch with official SteamOS support. Why must the wheel always be reinvented?,AMD,2025-08-31 12:53:42,47
AMD,nbpnwxo,"This release the device this time ? Because Zone 1 was never release in many EU country, despite German origins of Zotac...",AMD,2025-08-31 19:16:47,2
AMD,nbolxwd,Lol they are going to sell like 200 worldwide cause thats how much units they produced with the first zone handheld..,AMD,2025-08-31 16:08:29,4
AMD,nbo1o04,7â€ screen is an instant non-starter for me.,AMD,2025-08-31 14:27:17,4
AMD,nbv2yhz,"Looks a lot like the steam deck, but it's a win win for Linux gaming",AMD,2025-09-01 16:37:42,1
AMD,nbnxrnj,"Horizon Forbidden West imo is best experienced played in 4K on a 100"" projector screen.   It will visually give you the feeling that you are in the world as Aloy.",AMD,2025-08-31 14:07:08,-13
AMD,nbs1h6j,"Hell, even the Z2 would leave me wanting more given it really opens up around 20-23w TDP and draws around 30w at the battery for that power. 1-1.5 hour handhelds kinda suck, if I'm gonna need to keep an outlet within 6 feet I'll just grab my laptop.",AMD,2025-09-01 03:39:15,2
AMD,nbu029g,"I'd prefer a larger battery for sure. But they showed excellent scaling with the first Zone already:   https://tpucdn.com/review/zotac-zone/images/zotac-zone-power-scaling-final.png   From TechPowerUp: ""Compared to the ROG Ally, the Zone is in another tier at 15 W and below. It's more than 50% faster at 10 W and more than 30% ahead at 12 W, mopping the floor with ASUS's handheld.""   And that's with Windows. I'm sure they'll achieve even better results with the less demanding Linux OS. It won't be enough to really compensate a larger battery though, unfortunately. At least it'll mitigate it a bit. No way around a power bank for longer sessions on the go though.",AMD,2025-09-01 13:20:29,1
AMD,nbumb79,Spec doesn't matter. It's needs to be low price and have enought battery life with minimal viable performance in modern games.,AMD,2025-09-01 15:16:41,1
AMD,nbo8ezw,How did Manjaro even get this deal is beyond me.,AMD,2025-08-31 15:00:36,25
AMD,nbqevby,"Any linux will do. No need for steamos, and if you really need you could install it yourself. The good stuff is you have alternatives, and I'm excited to see Manjaro on a hh.",AMD,2025-08-31 21:36:56,9
AMD,nbpfllr,"Iâ€™m desperately in need of a 6â€ pc handheld. 7â€ is too big even with zero bezels, and all these 8â€+ models are fucking ginormous. The only decently portable handheld is the Ayaneo Air and even then thatâ€™s a 5.5â€ screen in a chassis that could easily fit 6â€.",AMD,2025-08-31 18:34:00,5
AMD,nbonj72,"I was so bummed when I found out the Xbox Ally is sticking to 7"" because the ergonomics look awesome. It's pretty crazy how much a difference going from a 7"" 1920x1080p screen to an 8"" 1920x1200 makes.",AMD,2025-08-31 16:16:23,5
AMD,nboczrs,it's really the size of a phone screen and that's fine.,AMD,2025-08-31 15:23:35,2
AMD,nbu0kqk,"7"" is perfect for me for such a low performance device tbh. With many modern games, you'll have to rely on FSR and its ugly artefacts, no need to blow that up to an even larger screen. 7"" is big enough to be pleasant and detailed, small enough to reduce some of the issues.",AMD,2025-09-01 13:23:26,1
AMD,nbobgqz,Rookie numbers. If it isn't IMAX my cones and rods can't even see the light,AMD,2025-08-31 15:15:49,9
AMD,nbqpg43,"They threatened to have pamac download content from Zotac    For those that don't get it [this link, section ""Poor QA""](https://manjarno.pages.dev/)",AMD,2025-08-31 22:37:04,5
AMD,nboph9k,Yeah same!,AMD,2025-08-31 16:25:55,1
AMD,nbpj9y1,"Phone screens themselves are just too small; its annoying that phone makers basically stopped before the 7"" mark and the only way to go bigger is the pricey & fragile folding models.   I have a Legion Go with a 8.8"" screen; if they just slapped a cellular modem in it well I would use it as my cell phone too.",AMD,2025-08-31 18:53:02,2
AMD,nbu0o6h,Fair enough.,AMD,2025-09-01 13:23:59,2
AMD,nboodzr,You're telling me you guys don't own a private Sphere like the one in Vegas?,AMD,2025-08-31 16:20:32,7
AMD,nbv51iq,"They're not pricey for flagship phones, just the NA models are. I paid like 8 or 900 for a vivo fold and 1200 for honor. That's with a little bit of a sale that are common, but Tecno hits the market with \~1000 msrp. Hell, zte have a 200$ flip. You pay more for a flagship slab.  But when you have 3 companies and only 2 of them make a product, they can allegedly team up and gouge you instead of competing in my opinion. And since this is the amd sub, not talking about GPUs this time.",AMD,2025-09-01 16:47:49,1
AMD,nbook55,I'm still paying off the 15 minutes of avatar film reel I was able to find.,AMD,2025-08-31 16:21:23,3
AMD,nboouw4,Pity. You can stay on my private 6th yacht rent free to save some money.,AMD,2025-08-31 16:22:51,3
AMD,naosjtk,This is awesome! I wonder how it compares to the Beelink GTR9 Pro. Since the GTR9 Pro is double the height for more cooling.     edit: the Beelink has an integrated power supply.,AMD,2025-08-26 01:32:20,12
AMD,naq06x4,Strix Halo is going to be awesome in a mini pc,AMD,2025-08-26 06:52:28,6
AMD,narcbft,This is so awesome. I needs it my precious.,AMD,2025-08-26 13:14:56,2
AMD,nas2hpg,"Give me RDNA4 or UDNA! I want a powerful APU for gaming, not AI. This is great for AI but I am still waiting and hoping they come out swinging with UDNA.",AMD,2025-08-26 15:24:49,1
AMD,naxrncn,"tbh I don't particularly want this. Give me an A9 ""slim"" with hawk point 220/230 at 15mm height and a single 1gig ethernet is fine for $300",AMD,2025-08-27 12:37:29,1
AMD,nb4prs8,Another non-laptop...,AMD,2025-08-28 13:37:08,1
AMD,nb516f7,"On principle, I will wait to buy this class of computer until AMD incorporates 3D V-cache.",AMD,2025-08-28 14:33:47,1
AMD,nao6cbm,Iâ€™m sorry but what use are mini PCs? Theyâ€™re nowhere near as powerful as a real PC and portability wise are almost as bad,AMD,2025-08-25 23:22:35,-21
AMD,nawgl9l,"I own the other Ryzen AI MAX 395+ device and I do observe the development of all the related devices, and the Beelink GTR9 is actually the top tier, probably the best Chinese NUC 395+ with vaporchamber air cooling, together with the Thermalright 395+ watercooled. But there's some concerns about the device, it's their BIOS, because it shares the same BIOS with all other Chinese NUC 395+ which is Sixunited's BIOS, this BIOS is very barebone, it can't undervolt and set fan curves, which is a deal breaker for many people, having good BIOS is very important, which only HP, Asus and Framework currently provide, but they're also much more expensive.",AMD,2025-08-27 06:01:27,2
AMD,nao9tl1,Almost as bad? Have you seen one? They are tiny lol,AMD,2025-08-25 23:43:25,19
AMD,nawgy5m,"> nowhere near as powerful  You know this Mini PC scored 43000 Cinebench R23, do you ? You think mid tier PCs have a chance to beat that score ?",AMD,2025-08-27 06:04:44,2
AMD,naqcqie,Strix Halo is more powerful than the majority of â€œrealâ€ PCs while being more portable and power efficient.  The main problem with Strix Halo is cost.,AMD,2025-08-26 08:55:37,2
AMD,naqibjf,"\> portability wise are almost as bad  Lol, a ""real PC"" is cumbersome to carry even in a car trunk/seat let alone in your bag/lap. You can carry some mini PCs with just your one hand's palm.",AMD,2025-08-26 09:49:07,2
AMD,nbglljv,"Only cheap Mini PCs around $100 make sense when you simply need a computer for less demanding tasks (i.e., not rendering 3D scenes) and entertainment (movies, old games). They have low energy consumption - economical operation.  Everything else has a terrible price/performance ratio and many other negative characteristics like bad cooling, no support (drivers, BIOS..). All AMD APUs are one or two generations behind.",AMD,2025-08-30 08:38:34,0
AMD,nawh9qx,Huh I didn't realize how bare bones the BIOS is for the Chinese nuc models. But I also don't see that much of a price difference between the Chinese models and more notable branded ones at MSRP. Do the Chinese models go at a deeper discount?,AMD,2025-08-27 06:07:43,1
AMD,nao9xbu,"Yeah but you still have to take a monitor, mouse, and keyboard with it when traveling",AMD,2025-08-25 23:44:01,-8
AMD,nawiwty,"Chinese models go from $1600 to $2000 in price. The GTR9 is probably in the $2000 range, which is the highest of all Chinese NUC 395+s.  The cheapest you can get is Nimo 395+ or Bosgame 395+, both in range of $1600+, and from what I know build quality of both device are good, so no point buying anything higher than $1600 unless you want to try out varporchamber/water cooling from Beelink/Thermalright.",AMD,2025-08-27 06:22:54,2
AMD,naoakls,Plug it in to any TV.  Foldable keyboards are also tiny. Talk about making a problem out of thin air ðŸ˜‚,AMD,2025-08-25 23:47:45,16
AMD,napl9nr,"I travel often theres normally a desk in the hotel, tv. Mini pc is like half the weight of a laptop, very compact.",AMD,2025-08-26 04:40:20,3
AMD,naxdfh6,"> no point buying anything higher than $1600  Well the Geekom A9 has at least two 2.5GbE ports as opposed to the single port on SixUnited ODM devices (Bosgame M5, Corsair 300, etc.). This can come in handy for some use cases.  The Beelink GTR9 even has two 10 GbE ports which is nice.",AMD,2025-08-27 11:03:35,2
AMD,nawmdkq,Hmm ok that's good to know. Has thermalright launched their model yet. I remember seeing coverage of it at computex this year but nothing since.,AMD,2025-08-27 06:55:13,1
AMD,naoapxn,In that case just build a mini itx thatâ€™s way more powerful and modular,AMD,2025-08-25 23:48:36,-14
AMD,naob06z,This post really proves you don't know how small these are! lol,AMD,2025-08-25 23:50:15,16
AMD,naobaxo,I do but the gap in performance really isnâ€™t worth it. A similarly priced or even less expensive laptop is way more powerful than this and you donâ€™t need to carry around a mouse and keyboard with them. These just seem like solutions looking for problems,AMD,2025-08-25 23:51:59,1
AMD,naocb24,The obsession with carrying around a keyboard and mouse when you suggest building a PC instead is hilarious lol  This chipset can match 4060/70 laptops GPU wise. And 128GB/8000Mhz of shared system ram is also nothing to be sniffed at.,AMD,2025-08-25 23:57:49,13
AMD,naplfnr,"Laptop is double the weight, and more thermal limited.",AMD,2025-08-26 04:41:40,4
AMD,naocmb6,Im saying both a mini pc and a full pc require peripherals so in that case just go with the most powerful one.   You can get those exact same specs in a tablet form factor,AMD,2025-08-25 23:59:38,-1
AMD,naod3iy,"You are also underestimating the Ryzen AI MAX+ 395 pure CPU performance. I know the tablet form factor, I own one....",AMD,2025-08-26 00:02:28,12
AMD,nbhm5fn,"Is this like Hairworks, but for AMD gpus?",AMD,2025-08-30 13:32:47,99
AMD,nbiol6y,Hopefully we'll see more attention brought to smaller details like this now that it's more accessible. TressFX is amazing.... just rarely utilised,AMD,2025-08-30 16:53:07,18
AMD,nbhadat,More stutter. YEAAAAH,AMD,2025-08-30 12:18:16,-108
AMD,nbhn17c,"For any GPU, without trying to sabotage the competition with bad code.",AMD,2025-08-30 13:38:03,268
AMD,nbhn2jp,"Yes, it has existed for a long time. And it can run on any GPU, like Hairworks, but is much more efficient. Hairworks was deliberately inefficient on AMD GPUs.",AMD,2025-08-30 13:38:16,46
AMD,nbhpkzs,Someone who doesn't know wtf is treesfx and just comments brainlessly. HURRRAY,AMD,2025-08-30 13:52:54,67
AMD,nbhsqwc,UE5 is dogshit but middleware like this isnt the reason,AMD,2025-08-30 14:10:48,27
AMD,nbm5znf,Idiot,AMD,2025-08-31 05:20:13,5
AMD,nbhnbdg,Imagine if this was the norm.,AMD,2025-08-30 13:39:44,90
AMD,nbj0gko,"An indie dev has managed to get dynamic fluid simulation running at real-time on any midrange GPU, something which is usually measured in seconds per frame and not frames per second: [https://www.youtube.com/watch?v=LJSADKf2150](https://www.youtube.com/watch?v=LJSADKf2150)  And yet nVidia managed to cripple flagship GPUs by badly simulating a few strands of hair.",AMD,2025-08-30 17:51:50,8
AMD,nbhrc9k,"From what I remember, Hairworks made liberal use of tessellation, while TressFX was a pure compute shader. Nvidia dedicated significant amounts of transistors to their chips to ensure they could handle tessellation efficiently, AMD didn't.  Does this mean Hairworks was deliberately trying to sabotage AMD, or was it Nvidia playing to their strengths without considering AMD? I'm not sure",AMD,2025-08-30 14:02:56,-10
AMD,nbkwmjh,"Hairworks is inefficient on all the GPUs not just AMD, which is why it wasn't widely adopted unlike TressFX that is part of most engines nowadays.",AMD,2025-08-31 00:09:06,10
AMD,nbhwazs,Wasnâ€™t it more that AMDâ€™s cards didnâ€™t handle tesselation well compared to Nvidia and Hairworks was utilizing tesselation?,AMD,2025-08-30 14:30:10,-22
AMD,nbhqy4k,TressFX is 13 years old and has been used in multiple games ever since. Hairworks was actually an Nvidia answer to it.,AMD,2025-08-30 14:00:44,102
AMD,nbk7vyp,If people bought more AMD GPUs (and if AMD had better GPU products) it would be the norm.,AMD,2025-08-30 21:40:40,1
AMD,nbkfik4,Thanks for sharing that video.   NGL probably the most exciting enhancement I've seen in ages that I hope really pushes its way into the mainstream.,AMD,2025-08-30 22:23:59,3
AMD,nbi0tl8,"nVidia had Hairworks do like 64 passes of tessellation, when there was barely any visual difference between 4 passes and 8. If you modded the files of games that used Hairworks to only do 4, 8, or 16 passes, Hairworks barely affected performance on AMD and previous generation nVidia cards.   It was very much an intentional sabotage of both AMD products and their own older generation products.",AMD,2025-08-30 14:53:58,36
AMD,nbhvvoi,"nVidia added absurd levels of tessellation specifically to make it run worse on AMD than on nVidia.  With no visual benefits.  AMD's tessellation, which was added by ATI in 2001, long before DX11 adopted it, was perfectly fine for sensible tessellation levels.  So the answer to your question is obvious, and was obvious to everyone immediately at the time.",AMD,2025-08-30 14:27:56,25
AMD,nbi71gb,"Man, the misinformation galore on this one.       The Nvidia marketing really got to you. This is the problem with the average internet computer enthusiast/fanboy, they intake surface level knowledge and run away with it.       The fact that you've been holding onto this misinformation all this time is a testament to how good Nvidia is at manipulating the average ignorant ""enthusiast,"" and how bad AMD is at marketing their own stuff.",AMD,2025-08-30 15:25:34,10
AMD,nbjtbfq,efficiently? you misspelled excessively lmao,AMD,2025-08-30 20:22:02,1
AMD,nbierb8,"Exactly  And who invented tessellation? ATI!   AMD marketing are a bunch of whiners that preferred be the victim to make it seem like nvidia was sabotaging their performances when within 48 hours a fan made a driver mod on AMD side that made performance good.   CDPR had warned them 18 months ahead, from SiGGRAPH the year before, they would use hairworks. CDPR wanted to work with AMD on an alternative hair solution for AMD and there was never an answer. Then AMD acts surprised after 18 months that thereâ€™s game works?  https://www.forbes.com/sites/jasonevangelho/2015/05/21/amd-is-wrong-about-the-witcher-3-and-nvidias-hairworks/?sh=15433a65e33a  AMD victimization is the biggest internet brainwash of the 2010s. Every single instances are AMD fucking up.",AMD,2025-08-30 16:04:17,-7
AMD,nbhzbvu,"As others had mentioned, Nvidia helped with the development in The Witcher 3 and Crisis 2 and in doing so added obscene amounts of overdraw tessellation for the water, even when hidden for Crisis 2 Maximum Edition as well as for the hair in The Witcher 3.  In the case of the Witcher 3, mods where applied to paid back the hair works tessellation where in it looked indistinguishly the same but now could run efficiently on AMD gpus of the time.",AMD,2025-08-30 14:46:16,26
AMD,nbngs7m,"AMD cards handled 32x tessellation just fine, but not 64x, in pretty much all cases the 64x added literally (and I use that word literally) nothing, except poorer performance. Could be different now with 4K resolutions being doable.",AMD,2025-08-31 12:25:17,1
AMD,nbhrtg7,"Well, TIL!   Crazy how I've never seen it in games settings.",AMD,2025-08-30 14:05:37,27
AMD,nbilams,Very clean explanation.   Nicely done.,AMD,2025-08-30 16:36:44,1
AMD,nbmlzh9,"Same.  Physics draw me in way more than just about any feature in a game, my jaw was on the floor through most of the video. So many games are held back by simplistic water physics.  Here's a link to the demo if you want to play around with it, watching it run firsthand at 250-300 FPS is even more impressive than the video: [https://imaginaryblend.itch.io/fluid-flux](https://imaginaryblend.itch.io/fluid-flux)",AMD,2025-08-31 07:48:10,2
AMD,nbk8iea,"[https://cdn.wccftech.com/wp-content/uploads/2015/05/witcher3\_2015\_05\_19\_16\_55\_26\_867.png](https://cdn.wccftech.com/wp-content/uploads/2015/05/witcher3_2015_05_19_16_55_26_867.png)     this is the visual difference between the tessellation levels,     it could be switched between in the AMD driver, there was also funny business involving Crysis 2 tessellation, they tessellated the water but didn't cull it when it was under the map same with random pieces of geometry, so the GPU was having to render all these triangles that couldn't be seen     [https://youtu.be/IYL07c74Jr4](https://youtu.be/IYL07c74Jr4)",AMD,2025-08-30 21:44:10,3
AMD,nbidqyr,"You didn't mod the game files to force 16x tessellation, you used the driver switch in the CCC. Something you'd know if you had a Radeon GPU at that time and decided to try for yourself. 16x mode also caused the hair to be somewhat thinner than normal from what I remember, while 8x and 4x rapidly dropped into absolutely cursed territory.",AMD,2025-08-30 15:59:13,-6
AMD,nbibevl,"Right... Care to show the AMD tech demos of tessellation from 2001, since you're obviously such an expert /s  For your information. AMD added tessellation with unified shaders in 2007, the first approach was certainly acceptable for that time. The 7870 and 7970 had something like 4x the tessellation performance of an HD 2900 XT, the improvement was pitiful, and it was by no means a priority for ATI/AMD at the time.  It certainly fit the narrative of AMD fanboys at the time to call out Nvidia for ""gimping performance through excessive tessellation"" though. Let's just ignore that even a 290X struggled with The Witcher 3 without enabling the completely optional Hairworks setting.",AMD,2025-08-30 15:47:29,-8
AMD,nbi8p7e,"The misinformation galore in what respect? That Hairworks used tessellation, TressFX didn't use tessellation, or that it was supposedly tuned to run horribly on AMD GPUs?  From personal memory, Hairworks ran rather poorly on GCN and Kepler, but that doesn't fit the wanted truth, so I guess we're rewriting history instead in order to make Nvidia look bad.",AMD,2025-08-30 15:33:52,-2
AMD,nbk5qh6,"Well, AMD has followed Nvidia's capabilities with RDNA, so?",AMD,2025-08-30 21:28:36,1
AMD,nbiiaf4,Don't forget the infamous Final Fantasy XV benchmark.,AMD,2025-08-30 16:21:49,5
AMD,nbk0aft,They handled correctly implemented tessellation fine but they didn't have very good culling techniques so tessellation that didn't even get seen (like in many Nvidia sponsored titles that did it on purpose) was still getting rendered on AMD while Nvidia was culling most of it and not having to render as much.,AMD,2025-08-30 20:58:52,1
AMD,nbifhz2,"I remember trying Hairworks with a 16x tessellation limit on my 7970 back in 2015, from what I remember Geralt's hair became noticeably thinner. And before you say that must have been 8x, I'm pretty sure 8x was the beginning of absolutely cursed territory.",AMD,2025-08-30 16:07:58,0
AMD,nbihemb,"This is an oversimplification. Nvidia didn't go in and quadruple the tessellation factor of the water in Crysis 2 and they didn't force the devs to make hair obscenely tessellated in Witcher 3. Nvidia gave the devs early access to new GPUs and their new SDKs. These games were both on pretty harsh dev cycles and were targeting both maxed out PC's and much weaker consoles. Optimization is the first thing to be cut in favor of an earlier release date, and what we got was horrible scaling on ATI/AMD cards.  Doing things like lowering the tessellation of hairworks and limiting tessellation factors on the driver level for ATI didn't actually solve the problem but rather masked the issue of weak tessellation on ATI/AMD cards of the time. Hairworks looked worse with lower tess factors, but was worth the performance gain. ATI limiting tess factors on the driver side helped Crysis 2 run *better* but still way worse than similar Nvidia cards of the time.",AMD,2025-08-30 16:17:23,-7
AMD,nbhs8ke,"The first game to use it was 2014 Tomb Raider Definitive Edition. But yes, it's not commonly an user-facing preset to be selected like Hairworks but rather used as standard library for hair/fur rendering simulation by some developers. Not too dissimular to Kawaii Physics plugin.",AMD,2025-08-30 14:08:01,55
AMD,nbjus42,"It was kind of heavy for Nvida in it's early days so there were a few games that included a toggle but since it's open source, it's a lot more optimized now for all vendors and isn't really something that would cause a noticeable drop in fps compared to other stuff going on in modern rendering so there's no point to include a toggle.",AMD,2025-08-30 20:29:46,6
AMD,nbj7o7y,There are a lot of TIL's about things AMD does first and Nvidia later copies.  And things NVIDA does first and AMD copies.  And things Intel does first and both copy.,AMD,2025-08-30 18:28:09,5
AMD,nbkw202,"TressFX is used in majorly of games nowadays, there is no setting for it cause is quite performant.",AMD,2025-08-31 00:05:26,9
AMD,nbl3kwz,"Yeah, 8x is already good enough, but there is a clear difference in all of them in the quality",AMD,2025-08-31 00:53:17,2
AMD,nbk7sup,"yeah, there was a dropdown in the driver, Default was ""AMD optimised"" which was 32x i believe?",AMD,2025-08-30 21:40:11,1
AMD,nbifkek,"""Actually..."" Rather than ATI, I think it was a technology developed by ArtX just before ATI purchased that company and closed a deal with Nintendo. The Game Cube had that tech, called TrueForm. On PCs it was available with the older Radeon 8500. From what I understand it was abandoned, no one actually used it, and AMD substituted it with the standard DirectX tessellation.",AMD,2025-08-30 16:08:18,5
AMD,nbj6tv7,Here you go:  https://web.archive.org/web/20101118054955/http://www.anandtech.com/show/773,AMD,2025-08-30 18:24:05,1
AMD,nbiazgm,Others have already answered these questions so I'm not going to regurgitate facts that have been known for over a decade. Perhaps try replying those instead.,AMD,2025-08-30 15:45:19,8
AMD,nbjfb8h,"This is what I'm talking about in my other comment. I also had a non-maxwell gpu that really struggled with hairworks so I needed to drop the tessellation factor, but it did make a noticeable difference to the hair. As you say, below 8x it's better to just disable it. Geralt's beard became a sleep paralysis demon.",AMD,2025-08-30 19:07:36,1
AMD,nbidb3a,What's crazy is that the only games I can think of that had Hairworks in the settings are FFXV and Tomb Raider 2013.,AMD,2025-08-30 15:57:02,10
AMD,nbk71a5,"the F1 games by codemasters used TressFX hair for a while, not sure if they still do but i remember F1 2018 and F1 2019 having the Tress FX branding show up on one of the splash screens at start up",AMD,2025-08-30 21:35:52,5
AMD,nbq8iyj,"""majority of games'  Lol, ok. Calm down.",AMD,2025-08-31 21:02:43,-4
AMD,nbkbapz,"AMD Optimized was supposedly adjusted per-game, so if a game was known to have ""excessive"" tessellation factors the AMD driver would tone it down automatically.  I know the setting remains, but I suspect they've dropped the game profile system with RDNA",AMD,2025-08-30 21:59:44,1
AMD,nbihkuh,"I didn't know that, but I remember Terascale had dedicated fixed-function hardware which was (for it's time) relatively potent. The same fixed-function block was also present in the Xbox 360 from what I remember, and was actually used in some games like Mass Effect for the Xbox 360.",AMD,2025-08-30 16:18:15,1
AMD,nbibxe5,"So in essence, you don't know yourself, but feel comfortable in regurgitating hatred towards other people who don't see the world as you do.",AMD,2025-08-30 15:50:05,-1
AMD,nbie5q0,"Witcher 3, the Metro games, and Far Cry 4 also had Hairworks. The list of all the games that had it is very short though.",AMD,2025-08-30 16:01:15,22
AMD,nbmsx71,Tomb Raider 2013 had tressfx. It was the first big game to use it. Later games they branched the code for tressfx added their own changes and started calling it pure hair.,AMD,2025-08-31 08:55:59,7
AMD,nbqpn1q,"There nothing to calm down, is clear you understanding of game engines is null.  Unreal Engine, Frostbite, Foundation, Ego, Dawn Engine and more are using TressFX, you simply won't be see the option anymore nor banner, since is a properly optmised industry standard with custom implementations.  But I don't expect you to understand such basic concepts, since is clear you never touched on any game engine.",AMD,2025-08-31 22:38:13,6
AMD,nbid4vd,"No, I just didn't see the point in writing the same thing again. Perhaps you shouldn't so readily assume the other person doesn't know what he's talking about.",AMD,2025-08-30 15:56:10,8
AMD,nbqpx80,Sure brah.,AMD,2025-08-31 22:39:54,-4
AMD,nbifxqd,"The problem is that this debate on whether Nvidia purposefully sabotaged ATI/AMD has been going on for almost 2 decades at this point. It's a topic rife with misinformation and regurgitated forum rants.  The reality, as is still the case for the games industry, is that developers optimize their games as well as they can. But if those developers are all using Nvidia cards, and they can get away with insane amounts of tesselation and don't have the time to target other hardware, you'll get lopsided performance scaling. Is that the entire story? No. Is it out of the question that Nvidia purposefully told devs to do this? No. Do we have any facts proving anything? No.",AMD,2025-08-30 16:10:09,2
AMD,nbieo3u,"So did you have a Radeon 7000 or 200-series back in early 2015 when this topic was hot? I had a pair of 7970s, which conveniently enough suffered especially from a lack of tessellation performance due to the imbalanced CU:SE ratio, as well as The Witcher 3 being one of the first games to use TAA which broke CFX compatibility.",AMD,2025-08-30 16:03:49,-1
AMD,nbin638,"But it is true that Hairworks uses a metric ton of passes for absolutely no good reason besides the implied anti-AMD sentiment. It's good and all the hardware is there to do it, and may prove useful in some niche, 3D animation projects but in gaming it serves no purpose at all.  In this case, forcing that many passes pedantically just to out-perform the competition is a blatant anti-competitive practice, period. It's not like RT/PT where AMD is at a clear disadvantage of their own making, because these technologies do make a tangible difference.  While Nvidia never ""forced"" devs to implement Hairworks, it's obvious there was bribery at work because quite often alternative technologies were straight up missing. Thankfully Hairworks as a bespoke tech had died in a ditch and isn't relevant any more for reasons of Nvidia's own making.",AMD,2025-08-30 16:46:06,4
AMD,nbimgmr,"Yessir. My first Radeon card was an ATi 9200SE, and I've had several ATi and AMD cards since. I've also had a 980Ti for a couple years until my whole PC burned from a bad PSU.",AMD,2025-08-30 16:42:35,3
AMD,nbn0ddv,Yea dog thatâ€™s clean. Nice work.,AMD,2025-08-31 10:09:47,22
AMD,nbmyfet,Gorgeous,AMD,2025-08-31 09:51:09,8
AMD,nbncllk,The reference model is just peak imo,AMD,2025-08-31 11:55:18,5
AMD,nbn0w4b,A very nice aesthetic. Well done ðŸ‘,AMD,2025-08-31 10:14:46,3
AMD,nbn906q,I...I dont see anything that really stands out and screams batman beyond here. It just looks like a blacked out build.,AMD,2025-08-31 11:27:15,7
AMD,nbnb7tk,Where is my Goth girl?,AMD,2025-08-31 11:44:55,2
AMD,nbo81h8,"<standing next to PC glowing rainbow like a thousand suns> guys why does his use 30W less power than mine, I can't figure it out",AMD,2025-08-31 14:58:45,2
AMD,nbrrf7f,Here i am looking for the PSU. The case hides it well.,AMD,2025-09-01 02:31:09,2
AMD,nbscjnw,I bought that same build 2 years ago and its still going strong.  Congrats.,AMD,2025-09-01 05:03:56,2
AMD,nbsxdlc,If you don't mind me asking.. what power cables are you using (the GPU ones)?,AMD,2025-09-01 08:09:55,2
AMD,nbv3mhp,Clean build man looks good,AMD,2025-09-01 16:40:54,1
AMD,nborh3i,Looks like a little sagg,AMD,2025-08-31 16:36:00,1
AMD,nbmxqpe,The Reference design red stripe always reminds me of the BMW M badge.... Cant unsee it.  Like RX6000s being the Mustangs rear xD,AMD,2025-08-31 09:44:15,1
AMD,nbonoa9,"Love it, just wish you didn't need to overspend on NZXT for the design.",AMD,2025-08-31 16:17:05,0
AMD,nboy4av,White build = Shit ton of dusts but barely noticeable Black build = Small amount of dusts and it looks like its 10 years old,AMD,2025-08-31 17:08:42,0
AMD,nbn94tu,Black with a little bit of red.  What I should've done was have a Batman Beyond Bat on the LCD screen.,AMD,2025-08-31 11:28:18,3
AMD,nbt9vfv,The stock cables that come with the Corsair RM1000e 2025 Cybenetics Platinum PSU.,AMD,2025-09-01 10:09:59,2
AMD,nbsyuxh,"Hey OP â€” Your post has been removed for not complying with rule 10.   Asking for free PCs, sponsorships, components and other e-begging activities are not allowed on r/AMD.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-09-01 08:24:23,1
AMD,nbpzpop,Does it look dusty?,AMD,2025-08-31 20:17:54,1
AMD,nbp9q4l,"Definitely needs more red imo, if you could get the pump red it would be better imo",AMD,2025-08-31 18:04:24,3
AMD,nbtcu4s,"Oh yeah that would be cool.   Even if you bought one of those Batman Beyond emblems that you can display inside the case. Like those new cases shown at Gamescom where it has a display ""stage"" for figurines and stuff. Would be good for photos.",AMD,2025-09-01 10:36:58,2
AMD,nbtb434,Thanks for the reply... will check it out.,AMD,2025-09-01 10:21:22,2
AMD,nbr97lm,hold that thought,AMD,2025-09-01 00:38:24,1
AMD,nbpccsa,Thank you.,AMD,2025-08-31 18:17:33,1
AMD,nbevqzq,"> The Legion Go 2 will use **Ryzen Z2 Extreme**, a special version of the Strix Point APU featuring Zen 5 and RDNA3.5 architecture.  Strix Point supports up to 12 Zen 5(c) cores, but the Z2 Extreme uses 8 cores.  The focus is on graphics, powered by the **Radeon 890M with 16 RDNA3.5 Compute Units**.",AMD,2025-08-30 00:34:07,7
AMD,nbecueo,Anyone know what the performance dif is between the Z1 and Z2 extreme?,AMD,2025-08-29 22:40:52,4
AMD,nbp7xa9,I was just looking up yesterday if there's any more information about the Lenovo Legion Go 2. This is great.,AMD,2025-08-31 17:55:30,1
AMD,nbfk2no,I badly want to buy it. A worthy upgrade for my Zotac Zone handheld. Been waiting since January.,AMD,2025-08-30 03:12:50,-1
AMD,nbet4r8,Long story short [early videos](https://youtu.be/ku8gm6vXrQs?si=mlr3a6MvZ21PbrQd&t=304) seem to indicate 10% average increase real world. Some games as low as 5% and rarely some above 20%. Unless there is a godly firmware update before launch I wouldn't bother swapping a Z1E handheld out for it. For people getting into PC handhelds it's nice to see progression but price/performance won't be worth it until sales start or the old devices sell out.,AMD,2025-08-30 00:17:59,8
AMD,nbgaggv,"Biggest difference will probably come from getting twice the RAM.   The OG Legion Go suffers a lot with just 16GB, mostly because Microsoft never really bothered to optimize RAM allocation in APU systems.",AMD,2025-08-30 06:51:43,3
AMD,nbetpdr,at 15W around maybe 20%.,AMD,2025-08-30 00:21:27,3
AMD,nbedj5f,There are reviews of other devices equipped with the Z2E out there with performance metrics if youâ€™re curious.,AMD,2025-08-29 22:44:55,1
AMD,nbfti07,Yeah that makes it a nice entry for newbies but not really an upgrade path,AMD,2025-08-30 04:22:53,2
AMD,nbhor45,So torn on this. I like seeing a smallish bump when owning a current device. Doesnâ€™t give me a desire to upgrade every gen and you get a good few years of use. On the other hand Iâ€™m a techie and like seeing things make large improvements each generation. But then Iâ€™d feel the urge to upgrade more frequently.,AMD,2025-08-30 13:48:07,1
AMD,nbftf89,Alright not terribly interesting then I suppose,AMD,2025-08-30 04:22:15,1
AMD,nbftyea,Cool. Doesnâ€™t seem like all that much. Nice little bump for the product but not a big enough jump to make interesting for those of us with one already.,AMD,2025-08-30 04:26:34,0
AMD,nbia5ub,I think this is why Valve doesnâ€™t seem to be updating the Steam Deck for this generation of APU. Upgrade is not big enough yet.,AMD,2025-08-30 15:41:08,2
AMD,nbk7jxn,"Also, these chips use considerably more power to reach that performance, and are presumably more expensive than the steam deck APU. I reckon Valve wants a more efficient solution thatâ€™s a significant improvement to their current hardware at a similar price point.",AMD,2025-08-30 21:38:48,1
AMD,nast5f7,Man my first computer was an Acer. I think it was 3000 years ago Gandalf.,AMD,2025-08-26 17:31:08,30
AMD,nate5my,"I saw it show up in my local store a couple days ago and was baffled by how expensive it was lol, not sure if this is just the local price or if it's the same in other countries but it was by far the most expensive AIB model, like 50 euros more expensive that Sapphire Nitro which already overpriced",AMD,2025-08-26 19:10:39,10
AMD,naszeot,I wonder what Sapphire has to say about that naming scheme,AMD,2025-08-26 17:59:31,32
AMD,naurd23,Acer sells GPUs?,AMD,2025-08-26 23:17:47,3
AMD,nb15lm9,That no one will buy if itâ€™s anything like their Intel GPUs,AMD,2025-08-27 22:30:28,1
AMD,nb8qx4g,No way right after I bought my 9060 xt MERC,AMD,2025-08-29 01:52:11,1
AMD,navc3vl,"I wonder if someone will ever dare to back away from the oversized cooler trend. These flow-through designs on lower-end products seem wholly unnecessary.  One of these days, maybe we can get an RDNA card with support for 6+ displays. The 9060 family (maybe a 9050 below it) seems like a respectable chance to put a bunch of ports on a low-profile card once again.",AMD,2025-08-27 01:17:04,0
AMD,naw3zid,their arc b580s are still $50-70 usd more expensive than msrp where you can find multiple other manufacturers at or only $10 more than msrp so its an acer thing,AMD,2025-08-27 04:15:43,4
AMD,nat2p0j,Nitro has long been one of Acerâ€™s gaming naming scheme.,AMD,2025-08-26 18:15:04,21
AMD,nat1hc8,Maybe Acer is such a small fish compared to Sapphire video card sales that Sapphire isn't worried about it.,AMD,2025-08-26 18:09:16,14
AMD,nb2sce2,"To be fair, Sapphire canâ€™t do anything about it. Acer has been using the Nitro brand for a while in their gaming lineup, and since Nitro is Acerâ€™s entry-level brand, I doubt Sapphire even cares.",AMD,2025-08-28 04:27:55,1
AMD,nauwn1m,"i think they first started with the intel A series, the Acer bifrost a770 was the go to intel card when it launched since it was at msrp or around $10 more for a cooler looking model compared to all others and it seemed like it had better thermal performance with 2 different style of fans but their b580's are standard looking and always at least $50-$70 over msrp and still are today. I dont expect them to price a 9060xt reasonably from their b580 prices and currently you can get other manufacturer 9060xt 16gb models at msrp with free shipping.  Heavily edited to fix the big grammer mistakes and added in a small amount of more context so it reads way better than before",AMD,2025-08-26 23:47:17,4
AMD,naw48ox,The powercolor reaper 9060 is only 200mm long and works totally fine.,AMD,2025-08-27 04:17:39,2
AMD,nat59ya,Nitro has long been one of Sapphires gpu naming scheme.,AMD,2025-08-26 18:27:19,4
AMD,nat4z21,"Maybe they think that it'll help sales of their ""NITRO+ Radeon RX 9060 XT OC"" if people see Acers ""Radeon RX 9060 XT NITRO"" and think the Nitro+ is the upgrade.  I wouldn't be surprised if there will be a lawsuit. Acer has their Nitro gaming lineup, Sapphire has their Nitro gpu line. I wonder if someone messed up their trademark or their legal teams are asleep.",AMD,2025-08-26 18:25:53,14
AMD,naw6wlt,"No, it fails at every aspect of what I mentioned. It only drives 3 monitors. The HD 7750s we use at my work have 4- and 6-port variants. They're also smaller and operate off board power only, whereas the 9060 uses PCIe power.  You can think whatever you want about viability in a DIY HTPC build, but for offices using SFF workstations, it's not even close to meeting the needs of what things like the 7750 and the Nvidia T1000s that you see in a lot of business workstations can give. We're not halving our monitor support on a workstation for a system adminstrator because you think it's ""totally fine.""",AMD,2025-08-27 04:38:30,0
AMD,natz7m3,"Acer used Nitro for displays and laptops prior to Sapphire launching its Nitro series of GPUs (2015), so nobody's going to win this one.",AMD,2025-08-26 20:50:47,12
AMD,nauaw9k,"It doesn't lol, Acer Nitro+'s lineup came before Sapphire launched their Nitro+ lineup, this is insane levels of reaching",AMD,2025-08-26 21:47:53,8
AMD,natuw2k,It might be that neither of them can claim it as an exclusive trademark by now.,AMD,2025-08-26 20:30:36,5
AMD,nb7b2dw,"I doubt any company cares about supporting 6 display outputs any more. Just get them a few big monitors if they can abandon their architect from the matrix fantasy. If not, there's always dp chaining, 2 gpus, or displaylink.",AMD,2025-08-28 21:01:28,1
AMD,nax2wyf,"Displays and Laptops are not graphics cards. Shell also has NiTRO+ gasoline, that doesn't allow them to sell graphics cards with the name. Zotac once had a Nitro OC controller which also doesn't give them the right to sell GPUs, laptops or monitors with that name.   Wikipedia doesn't even mention Acer Nitro until [2018](https://en.wikipedia.org/w/index.php?title=Acer_Inc.&diff=prev&oldid=873008616) because prior to 2017, Acer only sold a few Acer Aspire laptops with Nitro in the name. Acer also only recently filed a trademark for Nitro gpu products.",AMD,2025-08-27 09:35:35,-3
AMD,nausad2,"It will just become like the ""pro"" line for all these manufacturers, almost every one of them has a pro line of motherboard/gpu and nobody is throwing any lawsuits around.",AMD,2025-08-26 23:22:55,1
AMD,nawxtcy,"I have not seen any ""Nitro+"" branded items from Acer ever. They only have Nitro. But its a different thing if you have Nitro branded Laptops and Monitors and the other one Nitro branded GPUs its a different thing than two companies having Nitro branded gpus.  For fun i have checked the EUs trademark database and Sapphire does own the ""[Sapphire Nitro](https://www.tmdn.org/tmview/#/tmview/detail/US500000086569677)"" and ""[Sapphire Nitro+](https://www.tmdn.org/tmview/#/tmview/detail/US500000088643083)"" trademarks, at least in the US. Acer does have the Nitro trademark for many product groups, but the [filing for GPUs](https://www.tmdn.org/tmview/#/tmview/detail/US500000099288173) has only been submitted recently and is still pending. All are marked with ""Distinctiveness in commerce: false"". No clue what that means. All of Acers trademark filings came long after Sapphires.  I don't see how a company not being happy with a competitor using their brand for the same product counts as ""insane levels of reaching"".",AMD,2025-08-27 08:46:19,1
AMD,naw90sk,incorrect. sapphire nitro came before acers laptops. 2016 vs 2018,AMD,2025-08-27 04:55:28,1
AMD,nauynqc,It's not obscure at all. You can Google hardware reviews for Acer Nitro products which go back to the mid 2000s  Edit: actually shows it in the mid 1990s,AMD,2025-08-26 23:58:46,1
AMD,naxn5yn,"Why do you care so much, who gives a shit. Unless you work for one of those companies.",AMD,2025-08-27 12:10:20,0
AMD,nay3rng,"Apparently you do or why the comment? I just found it interesting that there is now two companies using the same branding for their cards. Especially with Ntiro+ being the high end of Sapphire, a brand that so far delivered quality products while Nitro is the entry level from Acer, a brand whose products i found to be manufactured e-waste.",AMD,2025-08-27 13:44:07,1
AMD,naz5fcr,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Intel,2025-08-27 16:46:20,47
AMD,nazj7k0,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Intel,2025-08-27 17:50:18,15
AMD,naz95i7,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Intel,2025-08-27 17:04:24,1
AMD,naz6p5r,Just hodl until you get the biscuits,Intel,2025-08-27 16:52:25,33
AMD,naz7hoi,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.Â    It means 18A will arrive later than TSMC 2N.Â    Nova lake will likely arrive after Ryzen successors.Â    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Intel,2025-08-27 16:56:15,9
AMD,nazkvn7,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Intel,2025-08-27 17:57:46,4
AMD,nazb0q3,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Intel,2025-08-27 17:13:24,3
AMD,nb2ag3p,"why even panther laker when it was only for mobile, cancelled it and just released nova lake next year",Intel,2025-08-28 02:25:25,0
AMD,nazazfk,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Intel,2025-08-27 17:13:14,30
AMD,nb330i7,I thought Arrow Lake refresh was in the cards for 2025.,Intel,2025-08-28 05:57:34,1
AMD,nb04wuk,Wasnâ€™t Panther Lake going to use TSMC for a portion of the volume?,Intel,2025-08-27 19:33:00,-2
AMD,nazl60x,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Intel,2025-08-27 17:59:03,3
AMD,nb8e4jm,This entire thing is a mobile roadmap so why are you here?,Intel,2025-08-29 00:35:26,2
AMD,nazbb9i,"Yeah if it's Surface roadmap, it's a nothing burger.",Intel,2025-08-27 17:14:47,7
AMD,nb1w9ji,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Intel,2025-08-28 01:01:42,2
AMD,nb02q5l,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Intel,2025-08-27 19:22:51,1
AMD,nb171ho,I think it was for the iGPU but I donâ€™t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Intel,2025-08-27 22:38:31,3
AMD,nb1xsvd,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Intel,2025-08-28 01:10:35,1
AMD,nb1vmub,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Intel,2025-08-28 00:58:05,3
AMD,n73y5u9,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Intel,2025-08-05 20:10:52,34
AMD,n73iurw,"I like how they just throw random words around to pad their ""article"".",Intel,2025-08-05 18:43:24,68
AMD,n73t59w,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Intel,2025-08-05 19:42:58,28
AMD,n741jok,"""leaks""",Intel,2025-08-05 20:28:54,5
AMD,n74lp88,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Intel,2025-08-05 22:16:47,5
AMD,n77fxeb,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Intel,2025-08-06 10:20:26,1
AMD,n7koh28,Scared of their rumor?  Lets release our rumor!,Intel,2025-08-08 09:59:01,1
AMD,n758aa3,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Intel,2025-08-06 00:22:30,1
AMD,n76sf5r,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Intel,2025-08-06 06:42:22,-1
AMD,n74sw92,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Intel,2025-08-05 22:56:59,-5
AMD,n75af32,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Intel,2025-08-06 00:34:46,5
AMD,n754qqc,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Intel,2025-08-06 00:02:20,8
AMD,n7582e3,bLLC is not stacked cache,Intel,2025-08-06 00:21:15,3
AMD,n767h6n,What do you consider random? The article was perfectly clear.,Intel,2025-08-06 03:54:03,10
AMD,n748zl3,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Intel,2025-08-05 21:08:09,14
AMD,n741l8k,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Intel,2025-08-05 20:29:08,2
AMD,n76jhxw,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Intel,2025-08-06 05:24:22,1
AMD,n7s4doe,Yeah. Definitely just you,Intel,2025-08-09 14:25:48,3
AMD,n757qwt,You could literally make that claim with any CPU performance increase.,Intel,2025-08-06 00:19:24,9
AMD,n7615sm,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Intel,2025-08-06 03:11:36,7
AMD,n77a60y,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Intel,2025-08-06 09:28:27,1
AMD,n76ahn8,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Intel,2025-08-06 04:15:00,5
AMD,n757yiy,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Intel,2025-08-06 00:20:37,1
AMD,n749ios,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Intel,2025-08-05 21:11:01,15
AMD,n76z9ct,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Intel,2025-08-06 07:45:28,2
AMD,n77cfm2,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Intel,2025-08-06 09:49:46,5
AMD,n77cdot,Yea but Iâ€™m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Intel,2025-08-06 09:49:17,6
AMD,nam3zf1,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Intel,2025-08-25 17:06:43,0
AMD,n75cvtk,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Intel,2025-08-06 00:48:56,2
AMD,n7ddfzp,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Intel,2025-08-07 06:10:13,1
AMD,n7dcz9u,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Intel,2025-08-07 06:06:08,1
AMD,nax8p6j,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Intel,2025-08-27 10:26:21,1
AMD,n7ddyd1,"I play Warthunder almost daily. Itâ€™s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  Iâ€™ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. Iâ€™m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Intel,2025-08-07 06:14:41,1
AMD,n7deaek,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Intel,2025-08-07 06:17:37,1
AMD,n9hwp92,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Intel,2025-08-19 08:52:24,63
AMD,n9hupxb,It's not sorcery. Its just Intel doing the game developers work.,Intel,2025-08-19 08:32:04,76
AMD,n9i0i3y,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Intel,2025-08-19 09:30:05,7
AMD,n9ic94k,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Intel,2025-08-19 11:11:41,9
AMD,n9k23nq,Never count out Intel. They have some very talented people over there.,Intel,2025-08-19 16:43:12,8
AMD,n9hygne,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Intel,2025-08-19 09:09:52,8
AMD,n9qyqfh,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Intel,2025-08-20 17:38:05,2
AMD,n9tmgam,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Intel,2025-08-21 02:04:20,1
AMD,nab3aup,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Intel,2025-08-23 20:50:51,1
AMD,naohj1m,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Intel,2025-08-26 00:28:42,1
AMD,nattrzq,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Intel,2025-08-26 20:25:28,1
AMD,n9j60kj,Congratulations on getting a CPU that didnâ€™t fry itself.  Consider yourself one of the few lucky ones.,Intel,2025-08-19 14:10:38,-8
AMD,n9iw99o,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Intel,2025-08-19 13:18:38,10
AMD,n9ihm3d,How is this doing the game developers work?,Intel,2025-08-19 11:50:25,-6
AMD,n9imvpt,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Intel,2025-08-19 12:24:19,3
AMD,n9ipgu8,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Intel,2025-08-19 12:39:59,4
AMD,n9ktypv,The intel software team is pure black magic when they allowed to work on crack.,Intel,2025-08-19 18:54:19,4
AMD,n9iml8f,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Intel,2025-08-19 12:22:32,4
AMD,n9s6nj5,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Intel,2025-08-20 21:12:41,6
AMD,n9iyl6t,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Intel,2025-08-19 13:31:21,8
AMD,n9u57ww,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Intel,2025-08-21 04:07:48,5
AMD,na9spi1,Balanced in full load will just do the same thing as High Performance.,Intel,2025-08-23 16:43:30,1
AMD,n9lhg6u,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Intel,2025-08-19 20:46:34,20
AMD,nah6zpc,Or... Just process lasso.,Intel,2025-08-24 21:05:07,1
AMD,n9ij60y,Because itâ€™s optimizations on how it can efficiently use the cpu.,Intel,2025-08-19 12:00:43,21
AMD,n9iv66n,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Intel,2025-08-19 13:12:40,1
AMD,n9iwird,"All of this stuff is super situational. For when itâ€™s supported for my 14th G, I love it.  To call it better than X3D though? Thatâ€™s a bit much.",Intel,2025-08-19 13:20:05,4
AMD,n9mv7uv,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Intel,2025-08-20 01:17:18,4
AMD,n9iojt9,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Intel,2025-08-19 12:34:27,1
AMD,n9y05el,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Intel,2025-08-21 19:06:28,1
AMD,n9lzy5i,"Great explanation that makes sense!Â  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 wouldÂ perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Intel,2025-08-19 22:19:43,3
AMD,n9mhmhz,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Intel,2025-08-19 23:59:05,1
AMD,nah75n6,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Intel,2025-08-24 21:06:00,1
AMD,n9kb8x1,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Intel,2025-08-19 17:25:29,-6
AMD,n9ivrcr,You need to download the Intel Application Optimization app from the Windows store,Intel,2025-08-19 13:15:54,1
AMD,n9iy6ma,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Intel,2025-08-19 13:29:09,6
AMD,n9t6inr,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Intel,2025-08-21 00:31:46,1
AMD,n9xzseo,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Intel,2025-08-21 19:04:43,1
AMD,n9ipdm4,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Intel,2025-08-19 12:39:26,4
AMD,n9opxwe,Where to download this APO,Intel,2025-08-20 10:12:53,1
AMD,n9ktn8y,Except its THE game devs job to optimize games for multiple cpus and gpus.,Intel,2025-08-19 18:52:47,7
AMD,n9ks0ao,Itâ€™s literally their job to do so? wtf you talking about?,Intel,2025-08-19 18:45:02,3
AMD,n9mt1qu,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Intel,2025-08-20 01:04:38,1
AMD,n9iw056,"Will do, I got a 265K. Performance is already great tbh.",Intel,2025-08-19 13:17:15,1
AMD,n9iz3ih,I love Intel but with how the company is doing right now Iâ€™m in a â€œIâ€™ll believe it when I see itâ€ type of skepticism.,Intel,2025-08-19 13:34:10,3
AMD,n9irrhk,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Intel,2025-08-19 12:53:21,1
AMD,n9q1a42,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Intel,2025-08-20 14:59:31,2
AMD,n9kvap0,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Intel,2025-08-19 19:00:36,-4
AMD,n9p3czn,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Intel,2025-08-20 11:55:36,1
AMD,n9kt4hv,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Intel,2025-08-19 18:50:20,-3
AMD,n9mx3nm,That's not simply what APO does.,Intel,2025-08-20 01:28:12,1
AMD,n9iwijq,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Intel,2025-08-19 13:20:03,2
AMD,n9j8cqp,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Intel,2025-08-19 14:22:18,3
AMD,n9iublo,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Intel,2025-08-19 13:07:57,4
AMD,n9pa899,i am not talking about older games. i am talking about newer games.... really dude?,Intel,2025-08-20 12:37:54,0
AMD,n9l8ne6,Okâ€¦all CPUs include Intel CPUsâ€¦I donâ€™t get your thought process,Intel,2025-08-19 20:05:16,3
AMD,n9iyd7e,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Intel,2025-08-19 13:30:09,6
AMD,n9iuumd,Thanks a lot.,Intel,2025-08-19 13:10:53,4
AMD,n9pc7ju,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Intel,2025-08-20 12:49:28,2
AMD,n9lby71,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Intel,2025-08-19 20:20:50,2
AMD,n9pd885,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Intel,2025-08-20 12:55:15,0
AMD,n9lgtiq,Okâ€¦? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still donâ€™t get your thought process,Intel,2025-08-19 20:43:35,2
AMD,nadrsxq,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Intel,2025-08-24 08:29:02,0
AMD,n9li080,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Intel,2025-08-19 20:49:11,0
AMD,n6bumwm,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Intel,2025-08-01 11:02:22,62
AMD,n6bnc4i,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Intel,2025-08-01 10:01:27,33
AMD,n6chmtn,Did the DP4A version also improve from 1.3 to 2.0?,Intel,2025-08-01 13:26:20,5
AMD,n6i4k9c,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Intel,2025-08-02 10:05:35,1
AMD,n6bj9ci,Okay but why would I want to use that instead of NVIDIA DLSS?,Intel,2025-08-01 09:23:32,-23
AMD,n6bwcya,Itâ€™s the least you should get after not getting FSR4.,Intel,2025-08-01 11:15:16,17
AMD,n6ck8qn,You'd use it over FSR if that's available too?,Intel,2025-08-01 13:40:07,3
AMD,n6i0iq2,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Intel,2025-08-02 09:25:17,1
AMD,n6c91ju,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Intel,2025-08-01 12:38:17,12
AMD,n6btrfp,And they expect people who bought previous RDNA to buy more RDNA,Intel,2025-08-01 10:55:34,4
AMD,n6nl7sh,Not by a significant amount.,Intel,2025-08-03 06:43:34,1
AMD,n6i0oud,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Intel,2025-08-02 09:27:02,0
AMD,n6bp1iu,for the games that dont support DLSS,Intel,2025-08-01 10:16:31,30
AMD,n6bul99,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Intel,2025-08-01 11:02:02,21
AMD,n6bqgih,10 series cards will benefit from this,Intel,2025-08-01 10:28:41,11
AMD,n6bnz56,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Intel,2025-08-01 10:07:08,1
AMD,n6cmxm3,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Intel,2025-08-01 13:54:01,17
AMD,n6d0adu,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Intel,2025-08-01 14:58:55,6
AMD,n6i1gij,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Intel,2025-08-02 09:34:56,2
AMD,n6emb0z,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Intel,2025-08-01 19:37:45,1
AMD,n6guokn,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Intel,2025-08-02 03:16:52,1
AMD,n6nl9o1,DP4a is cross vendor.   XMX is Arc only.,Intel,2025-08-03 06:44:04,3
AMD,n6bqj7p,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Intel,2025-08-01 10:29:20,-9
AMD,n6cx6cb,1080ti heard no bell,Intel,2025-08-01 14:44:10,3
AMD,n6i2ufc,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Intel,2025-08-02 09:48:51,1
AMD,n6r59ev,where did amd touch you bud?,Intel,2025-08-03 20:25:24,2
AMD,n6d2yn8,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Intel,2025-08-01 15:11:40,-1
AMD,n6f09jv,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Intel,2025-08-01 20:47:10,3
AMD,n6iu4q0,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Intel,2025-08-02 13:21:11,2
AMD,n6klfjc,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Intel,2025-08-02 19:05:15,24
AMD,n6q6qct,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Intel,2025-08-03 17:28:08,2
AMD,n7ht68l,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Intel,2025-08-07 21:45:26,1
AMD,n72229n,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Intel,2025-08-05 14:15:36,1
AMD,n7ewvah,You sound like an intel stock holder lmao The markets don't care about gaming at all.Â  They only care about AI slop,Intel,2025-08-07 13:19:41,2
AMD,nanq5zn,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Intel,2025-08-25 21:47:41,3
AMD,n7m0sli,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Intel,2025-08-08 14:57:07,1
AMD,n6m90qi,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance.Â CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.Â    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Intel,2025-08-03 00:48:59,6
AMD,n6nytko,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Intel,2025-08-03 08:55:06,3
AMD,n6l6ttw,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why ðŸ¤·",Intel,2025-08-02 21:03:36,5
AMD,n7m8w5n,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate ðŸ«  But I have not spend that much time optimizing my settings so I think I can work around it",Intel,2025-08-08 15:35:34,1
AMD,n8s3s3l,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Intel,2025-08-15 04:54:15,1
AMD,n6orqc9,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Intel,2025-08-03 12:59:19,7
AMD,n6qeres,Thank you :),Intel,2025-08-03 18:07:58,1
AMD,n4v3wzh,Wouldn't the 300 series actually be Arrow Lake Refresh?,Intel,2025-07-24 08:11:30,41
AMD,n4v46fh,"Videocardz is wrongÂ      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Intel,2025-07-24 08:13:59,42
AMD,n4y92ft,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Intel,2025-07-24 18:56:57,10
AMD,n4w6o9h,This is pat's work,Intel,2025-07-24 13:07:04,11
AMD,n4v6nq7,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Intel,2025-07-24 08:37:59,4
AMD,n4vlaga,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Intel,2025-07-24 10:49:31,8
AMD,n4vi19u,Large L3 cache without reducing latency will be fun to watch.,Intel,2025-07-24 10:22:42,18
AMD,n4w07wg,Is Intel finally making a comeback with their cpus? I hope so,Intel,2025-07-24 12:30:48,10
AMD,n4v3ue7,the specs sure do shift a lot..,Intel,2025-07-24 08:10:48,4
AMD,n4z4yed,it's just a bunch of cores glued together - intel circa 2016 probably,Intel,2025-07-24 21:27:20,2
AMD,n5798px,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Intel,2025-07-26 02:39:20,2
AMD,n4v8k6w,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Intel,2025-07-24 08:55:55,2
AMD,n4w3arv,Bout time,Intel,2025-07-24 12:48:32,1
AMD,n4w52bq,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Intel,2025-07-24 12:58:18,1
AMD,n510yns,Will it be available in fall of this year or 26Q1?,Intel,2025-07-25 04:05:49,1
AMD,n525u2m,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Intel,2025-07-25 10:03:56,1
AMD,n54i8vo,Noob question:  Is this their 16th gen chips?,Intel,2025-07-25 17:44:39,1
AMD,n5c2bcs,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Intel,2025-07-26 21:56:10,1
AMD,n5r7teb,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Intel,2025-07-29 07:47:01,1
AMD,n5rui8a,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Intel,2025-07-29 11:14:18,1
AMD,n4yqz30,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Intel,2025-07-24 20:20:50,1
AMD,n55s05h,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Intel,2025-07-25 21:27:44,1
AMD,n4xrnev,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Intel,2025-07-24 17:35:44,0
AMD,n4vu505,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Intel,2025-07-24 11:52:38,0
AMD,n4wlmkp,"So alder lake on cache steroids ?  If the heavier core count chips donâ€™t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Intel,2025-07-24 14:23:50,0
AMD,n4vi1cf,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Intel,2025-07-24 10:22:43,-11
AMD,n50egti,"""E-Cores"",  Ewww, Gross.",Intel,2025-07-25 01:40:31,-2
AMD,n4y77tz,"**Hi,**  Iâ€™m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPUâ€™s clearly starting to bottleneck a bit, but itâ€™s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Intel,2025-07-24 18:47:57,0
AMD,n4vx5x8,NVL will definitely be the 400 series. PTL is 300.,Intel,2025-07-24 12:12:08,12
AMD,n4vqabe,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Intel,2025-07-24 11:26:19,26
AMD,n4x2p0n,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Intel,2025-07-24 15:43:01,7
AMD,n4vnjk4,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Intel,2025-07-24 11:06:28,41
AMD,n4vbjjr,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Intel,2025-07-24 09:24:22,8
AMD,n4vqnzd,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Intel,2025-07-24 11:29:01,13
AMD,n4xnlen,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Intel,2025-07-24 17:17:52,2
AMD,n4y6q9e,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Intel,2025-07-24 18:45:35,2
AMD,n9a1zq5,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Intel,2025-08-18 01:51:09,1
AMD,n54f312,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Intel,2025-07-25 17:30:13,0
AMD,n50iof9,More like *despite* him.,Intel,2025-07-25 02:05:32,-4
AMD,n4wx0ud,All the SKUs rumored so far are BLLC,Intel,2025-07-24 15:17:14,1
AMD,n4w5o0o,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Intel,2025-07-24 13:01:33,10
AMD,n4vwjon,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Intel,2025-07-24 12:08:14,-3
AMD,n51lyb6,Why not 8 P cores with HT + even more cache and no e cores at all,Intel,2025-07-25 06:56:21,-1
AMD,n4wel6l,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Intel,2025-07-24 13:49:07,15
AMD,n4w5tga,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Intel,2025-07-24 13:02:22,4
AMD,n4wwsgj,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Intel,2025-07-24 15:16:10,2
AMD,n8gsgik,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Intel,2025-08-13 13:37:20,2
AMD,n4xic5y,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Intel,2025-07-24 16:54:39,0
AMD,n51u6hg,Nova lake? More like 26Q4.,Intel,2025-07-25 08:13:29,1
AMD,n5272v5,Only Pantherlake for mobile,Intel,2025-07-25 10:15:01,1
AMD,n52fjpx,"It doesnâ€™t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Intel,2025-07-25 11:22:11,2
AMD,n4yb3g9,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Intel,2025-07-24 19:06:33,1
AMD,n4vwh5f,It is really happening. The only question is when? Or can they release it on the next year without delay?,Intel,2025-07-24 12:07:46,1
AMD,n4wxgvv,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Intel,2025-07-24 15:19:15,3
AMD,n4wm6y8,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Intel,2025-07-24 14:26:35,1
AMD,n514twx,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Intel,2025-07-25 04:33:51,1
AMD,n5153vl,"E cores are the future, P cores days are numbered.",Intel,2025-07-25 04:35:54,5
AMD,n50iukn,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Intel,2025-07-25 02:06:36,2
AMD,n514kx3,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Intel,2025-07-25 04:32:00,1
AMD,n65knix,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.Â  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.Â Â    You can definitely wait a bit.",Intel,2025-07-31 12:35:52,1
AMD,n4vw5ub,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Intel,2025-07-24 12:05:46,9
AMD,n4vrhxo,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Intel,2025-07-24 11:34:53,19
AMD,n588bec,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Intel,2025-07-26 07:21:16,2
AMD,n4xxr8n,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Intel,2025-07-24 18:03:15,3
AMD,n50iwkc,The former ceo,Intel,2025-07-25 02:06:56,3
AMD,n4xy0ww,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Intel,2025-07-24 18:04:31,3
AMD,n4x3wyl,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Intel,2025-07-24 15:48:35,9
AMD,n4xgvaw,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Intel,2025-07-24 16:48:02,5
AMD,n4wb0qc,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. Youâ€™re using dated info.,Intel,2025-07-24 13:30:18,12
AMD,n4vzlh2,No different to 12-14th gen then.,Intel,2025-07-24 12:27:02,-2
AMD,n4wathc,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Intel,2025-07-24 13:29:14,-1
AMD,n51m5t6,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Intel,2025-07-25 06:58:12,3
AMD,n5267k5,E core is 30% faster than hyper threading,Intel,2025-07-25 10:07:14,1
AMD,n9ruz42,HT is worse than E cores,Intel,2025-08-20 20:15:01,1
AMD,n4wi0su,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Intel,2025-07-24 14:06:24,5
AMD,n5m42rc,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Intel,2025-07-28 14:26:44,1
AMD,n4xxluj,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Intel,2025-07-24 18:02:33,4
AMD,n50irzt,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Intel,2025-07-25 02:06:09,2
AMD,n526u6b,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Intel,2025-07-25 10:12:51,1
AMD,n514fo0,ARM chips regularly do this sometimes on a yearly basis.,Intel,2025-07-25 04:30:55,0
AMD,n5maqpy,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Intel,2025-07-28 14:59:12,2
AMD,n6ijxrr,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Intel,2025-08-02 12:13:51,1
AMD,n4yg12q,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Intel,2025-07-24 19:29:54,-2
AMD,n4wewa9,">Â They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Intel,2025-07-24 13:50:43,2
AMD,n4x5x1h,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Intel,2025-07-24 15:57:43,0
AMD,n5m9vma,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Intel,2025-07-28 14:55:00,1
AMD,n5awbw7,"Lmao, some of my games still runs on e cores",Intel,2025-07-26 18:06:33,1
AMD,n50x83s,People are still disabling e cores for more performance.,Intel,2025-07-25 03:39:29,1
AMD,n525z7c,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Intel,2025-07-25 10:05:10,1
AMD,n5m4vc2,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Intel,2025-07-28 14:30:40,2
AMD,n511bh4,fym no different they're 50% faster,Intel,2025-07-25 04:08:20,5
AMD,n5m5ymk,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Intel,2025-07-28 14:36:05,3
AMD,n528glp,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Intel,2025-07-25 10:27:04,1
AMD,n4wm3t2,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Intel,2025-07-24 14:26:09,6
AMD,n4wln0q,"As a advice, Nova Lake will further increase memory latency.",Intel,2025-07-24 14:23:54,-3
AMD,n50qnfg,In which gen iteration?,Intel,2025-07-25 02:55:31,0
AMD,n526q2r,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Intel,2025-07-25 10:11:49,1
AMD,n5m8bv4,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Intel,2025-07-28 14:47:35,1
AMD,n4xhpq9,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Intel,2025-07-24 16:51:53,1
AMD,n4xw2bt,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Intel,2025-07-24 17:55:25,8
AMD,n4zgeej,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Intel,2025-07-24 22:26:02,5
AMD,n52evsy,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.Â    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.Â    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Intel,2025-07-25 11:17:22,6
AMD,n588e12,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Intel,2025-07-26 07:21:58,2
AMD,n5266cc,They don't pay attention,Intel,2025-07-25 10:06:55,2
AMD,n5m6uax,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Intel,2025-07-28 14:40:21,1
AMD,n5dzs6f,That is more so bc the 9950x3d isnâ€™t really a 16 core cpu it is two 8 cores,Intel,2025-07-27 05:47:15,1
AMD,n51sxrb,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Intel,2025-07-25 08:01:41,3
AMD,n50ijg8,How do you think they're doing 2 compute tiles if the memory controller is on one?,Intel,2025-07-25 02:04:42,2
AMD,n4wml1q,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Intel,2025-07-24 14:28:27,5
AMD,n5m2xjb,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Intel,2025-07-28 14:21:04,2
AMD,n528fgg,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Intel,2025-07-25 10:26:47,3
AMD,n5m9sth,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Intel,2025-07-28 14:54:37,1
AMD,n4xx7gl,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Intel,2025-07-24 18:00:40,2
AMD,n51508q,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Intel,2025-07-25 04:35:11,1
AMD,n526cpd,That didn't stop Intel with N3B,Intel,2025-07-25 10:08:30,0
AMD,n55ndnc,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Intel,2025-07-25 21:04:09,-1
AMD,n5qxo88,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Intel,2025-07-29 06:12:03,1
AMD,n80drlt,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Intel,2025-08-10 21:49:03,1
AMD,n4yjbs7,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Intel,2025-07-24 19:45:28,0
AMD,n55qjrv,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.Â    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix theÂ problem. It doesn't happen very often but when it does it's very annoying.",Intel,2025-07-25 21:20:18,5
AMD,n4z1sm2,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Intel,2025-07-24 21:11:47,2
AMD,n4z2fz2,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Intel,2025-07-24 21:14:56,1
AMD,n526jxa,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Intel,2025-07-25 10:10:18,1
AMD,n4znijc,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.Â   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Intel,2025-07-24 23:05:11,0
AMD,n53d0lp,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Intel,2025-07-25 14:33:18,0
AMD,n3haicz,Will it also introduce Lunar Lake successor?,Intel,2025-07-16 17:36:05,15
AMD,n3htfns,I wonder if this could become avaliable on LGA 1954 because that'd be a good alternative for budget builds potentially if the iGPU is potent enough.,Intel,2025-07-16 19:02:16,8
AMD,n3h5hhc,Is anyone left?,Intel,2025-07-16 17:13:41,20
AMD,n3hw0vs,I would love to see a Nova Lake-A (strix halo like) APU with an 8+16 tile + 20Xe3 cores which is equal to 2560 FP32 lanes or 40 AMD CU's with 16-32mb of memory side cache to handle igpu bandwidth demands,Intel,2025-07-16 19:14:21,4
AMD,n3h8963,"Wonder what node such a product, if it does exist, would use for the iGPU tile.   AFAIK, rumor is that the iGPU tile for the standard parts would be on 18A or an 18A variant. However, if this is a flagship part, I would assume they would be willing to pay the extra cost to use N2...   However that would also presumably involve porting the architecture over to another node, which I don't think Intel has the resources, or wants to go through the effort, to do so.   Also wonder if such a product will finally utilize the ADM cache that has been rumored since like, forever.   And this is a bit of a tangent, but I swear at this point in the leak cycle for a new Intel product, there's always usually a ton of different skus that are leaked that may or may not launch - right now we have NVL bLLC, NVL standard, and now this, and a bunch of them end up not launching (whether they were cut internally, or the leaks were just made up). So it would be pretty interesting to see if any of the more specialized rumored NVL skus end up actually launching.",Intel,2025-07-16 17:26:01,5
AMD,n3hcp5u,These designs are going to be the go-to for creator and mid-range gaming at some point in the future. Having it all integrated together should provide opportunities for optimization and form factors that are difficult with discrete cards,Intel,2025-07-16 17:45:45,5
AMD,n3jhgvd,How long do I have to wait for something that is more powerful than an AMD 7840HS and still cheaper?,Intel,2025-07-16 23:53:49,1
AMD,n3k7s38,Good to hear! I have a strix halo asus tablet the 390 one and its only got 32GB ram but its fast!  Would love to see an intel versionâ€¦,Intel,2025-07-17 02:31:14,1
AMD,n3mdvwq,"This is great move by Intel since they aren't making dGPU for laptop so they can get some marketshare by making strong Intel ecosystem. If Nova Lake AX released then people don't need to buy overpriced laptop with Nvidia dGPU anymore, not to mention with Nvidia shitty small vram.",Intel,2025-07-17 12:44:29,1
AMD,n4c03u2,"I'm still using Alder Lake, I suppose all these chips will be released in 2026 ?",Intel,2025-07-21 12:54:03,1
AMD,n3huqus,That's Panther Lake in a few months,Intel,2025-07-16 19:08:25,11
AMD,n3ik298,Lunar lake was always confirmed even by Intel as a 1 off design with soldered memory and many of its design choices.,Intel,2025-07-16 21:05:21,8
AMD,n3lcumc,"No, the memory config wouldn't work.",Intel,2025-07-17 07:51:04,1
AMD,n3h7xps,No not really.  It's pretty f'n bleak atm.,Intel,2025-07-16 17:24:37,17
AMD,n3i46cl,"That isn't that different from the AI MAX 395+ already released, which has 16 Zen 5 cores and 40CUs and some MALL cache (I think 20MB) to help with bandwidth.  For this to be a meaningful upgrade, you'd want more than 20 Xe3 cores, which means you need more bandwidth, which means you need probably LPDDR6 memory to get it high enough as otherwise the cache demands would be prohibitive and LPDDR6 memory isn't coming to client until probably 2027.",Intel,2025-07-16 19:53:15,7
AMD,n3h9kp1,It's N2.  Sadly Intel Inside has almost entirely become TSMC Inside,Intel,2025-07-16 17:31:54,4
AMD,n3l4fgi,"Nova Lake-AX will not be released, and DLLC will not be released, and nova Lake will not be released.",Intel,2025-07-17 06:35:04,-1
AMD,n3irdqf,"Unless they can find some way to bring down the cost.....these aren't gonna get much traction, the same applies to Strix Point of course. While more efficient than your typical 2000$ laptop with a decent GPU.....the traditional laptop is still gonna outperform them based on pure specs.  You get more battery hour and less heat but people who use these kind of laptop are used to having it plugged 24/7. Feels more like experimental models but they have a long way to go before they see proper adoption. Particularly the cost.",Intel,2025-07-16 21:39:27,7
AMD,n3hywlr,"> mid-range gaming  Strix halo is only in $2000 systems. high end price, mid-range performance.  Good for stock margins i guess.",Intel,2025-07-16 19:27:56,5
AMD,n3k42qb,I'm sure that Xe2 in Lunar Lake is faster than the 7840HS iGPU on average if you run it with the max performance mode in the OEM software that comes with your laptop.,Intel,2025-07-17 02:08:16,7
AMD,n3k6ewf,The 7850HS can't compete with the Xe2 igpu because it's bandwidth starved.   The 8mb of memory side cache insulating the 4mb of L2 from main memory allows the Xe2 Arc 140V igpu in LL clocked at 1950mhz to beat the RDNA 3.5 890m clocked at 2950mhz as it only has 4mb of L2 as a last level of cache before hitting memory.   TLDR: RDNA 3.5 is bandwidth starved on strix point due to lacking memory side cache,Intel,2025-07-17 02:22:43,3
AMD,n3jnkuo,PTL's not really a LNL successor.,Intel,2025-07-17 00:29:37,2
AMD,n48vx1p,"Yeah, it was just to prove a point that ARM is overrated.",Intel,2025-07-20 22:58:27,1
AMD,n3h8tf7,"I hear you, questioning my decision to return under Patâ€™s hire-back spending spree. Dodged this oneâ€¦ but this is hitting differently.",Intel,2025-07-16 17:28:32,13
AMD,n3k5a4s,What about an 8 + 16 big LLC tile and 32Xe3 cores (4096 FP32 lanes or ~60 AMD CU's) + 64mb of memory side cache?   Or even better a mid range part with an 8+16 tile and 16-20Xe3 cores + 16-32mb of memory side cache,Intel,2025-07-17 02:15:39,1
AMD,n4sfvrm,Just read about the JEDEC spec for lpddr6 of 14400mt/s.Â  That is wild!,Intel,2025-07-23 21:35:03,1
AMD,n3kgv3j,Who said it's It's entirely TSMC ? they have been pretty open about majority of tiles Intel the CPU Tile will be shared with N2 ofc though,Intel,2025-07-17 03:30:53,5
AMD,n3jgnpe,"It should be Intel here, TSMC there.  Or a little bit of Intc, a little bit of TSMC.",Intel,2025-07-16 23:49:16,4
AMD,n3hafqh,IDM 2.0: Where we proudly declare our fabs are world-classâ€”while quietly handing the crown jewels to TSMC.,Intel,2025-07-16 17:35:45,3
AMD,n3jfseq,"The BOM is lower, so the question is where the markup is coming from.",Intel,2025-07-16 23:44:28,2
AMD,n3k5vvd,Consoles have big APUs and they have better value than anything. I know they're subsidized however.,Intel,2025-07-17 02:19:23,2
AMD,n3k8ay7,A mid range Nova Lake-A SKU with a 6+12 tile and 16/20Xe3 cores with 16-32mb of memory side cache would be sick,Intel,2025-07-17 02:34:31,2
AMD,n3khaz8,Thats the problem with AMD sticking an additional 8 cores instead of infinity cache to fix the bandwidth bottleneck.,Intel,2025-07-17 03:33:57,1
AMD,n3k63l5,>still cheaper,Intel,2025-07-17 02:20:44,1
AMD,n3k776n,"It's the closest that we will get to a Lunar Lake successor   Really, panther lake is a combined successor to both Arrow Lake-H and Lunar Lake",Intel,2025-07-17 02:27:36,9
AMD,n3ugqr7,"Low volume and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference.  Combine that with not moving a lot of them compared to the rest of the lineup, and each one of those units has to make up more of the costs from things like bad dies or general spin up costs for a new chip.",Intel,2025-07-18 16:50:56,1
AMD,n3l78dq,It already has 32MB infinity cache.,Intel,2025-07-17 06:59:18,2
AMD,n3k81ho,The 7840HS is cheaper because it is older.,Intel,2025-07-17 02:32:52,4
AMD,n3lcowt,"It competes in the U/H lanes, so what is today ""ARL""-U and ARL-H. But yes, in practice it should bring *most* of the LNL goodness over, but PTL-U is still not a true successor in the original ~10W envelope LNL was designed for.",Intel,2025-07-17 07:49:34,1
AMD,n3ut4ta,"> and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference   It's no bigger than the equivalent dGPU, and you save on memory, package, and platform costs. Half the point of these things is to use the integrated cost advantage to better compete with Nvidia dGPUs.",Intel,2025-07-18 17:47:58,1
AMD,n3kh6ij,The new Ryzen AI 250/260 with 780M is still cheaper because it doesn't have copilot+ .,Intel,2025-07-17 03:33:05,-1
AMD,n3levrg,I will see the performance envelope of PTL U and decide should dump my LNL or not,Intel,2025-07-17 08:10:07,3
AMD,n3uubsu,"The low volume part of that quote is important. If you're making heaps of them, each can take up a little bit of the startup costs for that design. If you only make a few, those costs get concentrated on what you do make.  Low volume also means a single defect is promotionally more of your possible units, and hits the margins harder. You do get more total defects in a larger run, but at sufficiently high quantities of those defects, you start making lower bins to recover some of the defects. If you hardly make the volume to cover one widespread SKU, you aren't likely to have enough to make a cheaper one with any degree of availability. At some point you hit a threshold where it's not worth selling what would be a lower bin because there's not enough of them.",Intel,2025-07-18 17:53:36,1
AMD,n3ki3cu,Because putting a 40 TOPs NPU for the Copilot+ certification is costly.,Intel,2025-07-17 03:39:32,4
AMD,n3l7fsl,"It doesn't have a 40 TOPS NPU, it's the same 16TOPS one in the 8840HS it's literally just another rebrand. It's cheap because it's from 2023.",Intel,2025-07-17 07:01:04,1
AMD,n3laz95,Exactly. Hence your Xe2 lunar lake being faster is irrelevant to the question because it won't be cheaper regardless of age when they have a NPU that takes close to 1/3 of die space.,Intel,2025-07-17 07:33:37,1
AMD,n3l7mu1,I think I didn't say anything that deviates from what you just said.,Intel,2025-07-17 07:02:48,1
AMD,n3lach0,"Maybe I misread or misunderstood, I thought you said the 260/250 had a 40TOPS NPU",Intel,2025-07-17 07:27:39,1
AMD,n1d5sl1,"â€œYeah just push it to production nothingâ€™s gonna happen, no need to QA/QC thisâ€",Intel,2025-07-04 20:26:29,19
AMD,n1d44wc,Haha wow that is hilariously bad.   And here I thought ASUS newer bios update was bad due to some higher temps,Intel,2025-07-04 20:17:30,15
AMD,n1hmxgn,"The new Gigabyte BIOS also has booting issues for me, and memory is even more unstable.  At least the UI is entirely in English now.",Intel,2025-07-05 16:10:42,5
AMD,n1je3rl,"who needs quality control, what can go wrong?",Intel,2025-07-05 21:53:07,3
AMD,n1fn5q9,"This is why you don't rush to update software, let others do the testing for two weeks",Intel,2025-07-05 07:05:46,3
AMD,n4zj8x9,Gigabyte is trash,Intel,2025-07-24 22:41:41,1
AMD,n1j4fq9,The Elon Musk method,Intel,2025-07-05 20:58:12,4
AMD,n2m6czi,"> â€œYeah just push it to production nothingâ€™s gonna happen, no need to QA/QC thisâ€  Seems to be the motto of a lot of the tech world",Intel,2025-07-11 20:40:54,1
AMD,n25f2x8,"Yup indeed.   I would even go as far as one full month   Hence, why I learned to avoid installing new Windows Update automatically",Intel,2025-07-09 10:28:21,1
AMD,mzzaf4q,Intel really needs to be able to compete with X3D or they're going to continue getting dominated in the enthusiast consumer market. I like Intel CPUs and was happy with my 12600K for awhile but X3D finally swayed me to switch over.,Intel,2025-06-27 00:15:57,65
AMD,n023uge,"Intel has had plans for big ass L4 cache for almost a decade now, just that it never made it past the design board.  Supposed to be marketed as Adamantium. But it got ZBBâ€™d every time I suppose due to cost.  For Intel to implement Adamantium, regular manufacturing yield has to be good enough I.e cost is low so they can splurge on L4.  Of course now they are forced to go this way irrespective of cost. Iâ€™d love 16p + L4 CPU.",Intel,2025-06-27 13:03:00,9
AMD,mzzyzii,"Honestly, good. I've been using AMD for a while now but we need healthy competition in the CPU space for gaming otherwise AMD will see a clear opportunity to bring prices up",Intel,2025-06-27 02:44:24,13
AMD,mzz7wta,"Something interesting is that the extra cache isn't rumored to be on a base tile (like it is with Zen 5X3D), but rather directly in the regular compute tile itself.   On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.   I think Intel atp desperately needs a X3D competitor. Their market share and especially revenue share in the desktop segment as a whole has been ""cratering"" (compared to how they are doing vs AMD in their other segments) for a while now...",Intel,2025-06-27 00:01:01,16
AMD,n029b76,Hasnâ€™t this been on their roadmap for a while now? Iâ€™m pretty sure they said 2027 is when theyâ€™ll have their version of x3D on the market,Intel,2025-06-27 13:33:26,3
AMD,mzz8z0y,"These core count increases could be a godsend at the low end and in the midrange. If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming, Intel will have a massive advantage on price.  That being said, if leaked Zen 6 clocks (albeit theyâ€™re from MLID, so should be taken with a grain of salt) are accurate, Nova Lake could lose to vanilla Zen 6 in gaming by a solid 5-10% anyway.",Intel,2025-06-27 00:07:18,11
AMD,n07rvul,"Funny how non of this news posted on reddit hardware sub or even allowed to be posted. Guest what? R amdhardware will always be amdhardware! It's painfully obvious that unbearable toxic landfills sub is extremely biased to Amd. Meanwhile all Intel ""bad rumors"" got posted there freely which is really BS!  I still remember i got banned from that trash sub for saying ""People need to touch grass and stop pretending like AMD is still underdog because they aren't"" and the Amd mods sure really mad after seeing my comment got 100+ upvotes for saying the truth, but that doesn't matter anymore because i also ban those trash sub!",Intel,2025-06-28 09:38:27,2
AMD,n0tqugl,Intel should be ahead of the curve on things not looking to compete on previously created tech,Intel,2025-07-01 20:43:31,1
AMD,n0x6eyy,"Very fine-tuned ARL-S almost reach 9800X3D performance. Extra cache could help to close the gap   Given people are willing to overpay for price-inflated 9800X3D, I wonder if it could work given buyers need an entirely new platform. 9800X3D users are fine for a pretty long time like 5800X3D users did",Intel,2025-07-02 10:56:54,1
AMD,n5g03d8,"Lol, requires a new socket. Intel is such trash.",Intel,2025-07-27 15:10:56,1
AMD,n05c074,Intel will simply always be better than amd,Intel,2025-06-27 22:40:39,2
AMD,mzzj3f4,"AMD gains tremendously from X3D/v$ because the L3 cache runs at core speeds and thus is fairly low latency, Intel hasn't seen such low latency L3 caches since skylake, which also has much smaller sizes, so the benefits of this could be much less than what AMD sees.   Only one way to find out, but I advise some heavy skepticism on the topic of ""30% more gaming perf from 'intel's v$'""",Intel,2025-06-27 01:08:00,-8
AMD,n00a0en,"Either more cache or resurrecting the HEDT X-series... Doesn't matter, as long as there is an affordable high-end product line.",Intel,2025-06-27 03:59:29,21
AMD,n01ajgx,"The 12600k was a fine chip, but AMD had the ace up Its sleeve. I upgraded from a 12600k to a 7950x3d and it was one of the best PC upgrades I ever made.",Intel,2025-06-27 09:25:00,9
AMD,n03naj2,I mean 9800x3D and 14900K offers basically the same performance in the enthusiast segment. Going forward though it would be nice to have more cache so normal users doesn't have to do any sort of memory overclocking just to match 9000x3D in gaming.,Intel,2025-06-27 17:33:15,5
AMD,n022v6j,4070 ti wonâ€™t cut it man - upgrade!,Intel,2025-06-27 12:57:19,-2
AMD,n067n87,Broadwell could have been so interesting had it planned out.,Intel,2025-06-28 01:50:48,4
AMD,n0254vg,"I want a 32 Core/64 Thread 3.40 GHz Core i9-like CPU. Not Xeon like with Quad-Channel and stuff, just 40 PCIe 5.0 lanes and 32 Power-Cores instead of little.big design. ðŸ˜¬",Intel,2025-06-27 13:10:22,6
AMD,n06s4h5,">Otherwise AMD will see a clear opportunity to bring prices up  AMD already did, as you can see zen 5 x3d is overpriced as hell especially the 8 core CPU. Zen 5 is overpriced compared to zen 4 which is already more expensive than zen 3. Not to mention they did shady business like keep doing rebranding old chip as the new series to fools people into thinking it was new architecture when it wasn't and sell it with higher price compared to chip on the same architecture in old gen.  Intel surely needed to kick Amd ass because Amd keep milking people with the same 6 and 8 cores CPU over and over with price increases too! Not to mention radeon is the same by following nvidia greedy strategy.  Edit: Some mad Amd crowd going to my history just to downvote every of my comments because they are salty as hell, i won't be surprised if there are from trash sub r/hardware. But truth to be told, your downvote won't change anything!!",Intel,2025-06-28 04:12:23,5
AMD,n028uwl,"Even though it's not stacked, I believe it's still going to fix the last level cache latency issue MTL and ARL have.Â   Ryzen CPUs have lower L3 latency than Intel because each CCX gets their own independent L3, unlike Intel's shared L3. Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3, so possibly giving the existing cores/tiles their own independent L3, improving latency and bandwidth over shared L3.  But one thing intrigues me. If this cache level has lower latency than shared L3, wouldn't this more properly be called L2.5 or something below L3 rather than last level cache? Will NVL even still have shared L3 like the previous Intel CPUs? I know the rumor that it will have shared L2 per two cores, but we know nothing of the L3 configuration.",Intel,2025-06-27 13:30:56,6
AMD,n014nai,"> On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.  It is already a non-issue since AMD moved the 3D V-Cache to underneath the compute tile.",Intel,2025-06-27 08:27:08,11
AMD,n00xlat,"Adamantaium was on the interposer, did they change plans?",Intel,2025-06-27 07:17:41,3
AMD,n02fp39,"Don't remember them saying anything like that, but by around that time their 18A packaging is supposed to be ready for 3D stacking.",Intel,2025-06-27 14:06:40,6
AMD,n0r9clc,"Nova lake= skip of it's just as good as zen, you would be looking at 2 gens after that and then swap from AM5 to intel.",Intel,2025-07-01 13:40:00,1
AMD,n027hjt,"> If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming  Doubt that since it'll probably lack hyperthreading and the E-Cores are slower, even 6C12T CPUs are starting to hit their limits in games in the last few years, faster cores won't help if there's much less resources to go around, it kinda feels like intel went backwards when they removed hyperthreading without increasing the P-Core count.",Intel,2025-06-27 13:23:25,-1
AMD,mzzxdtg,Intel managed to run Sandy Bridge's ring bus clock speeds at core clocks which resulted in 30 cycles of L3 latency.   Haswell disaggreated core and ring clocks allowing for additional power savings.   Arrow Lake's L3 latency is 80 cycles with a ring speed of 3.8ghz,Intel,2025-06-27 02:34:12,17
AMD,n05blhb,"I'd like to see the HEDT X-series come back too, but Intel would have to come up with something that would be competitive in that area.  It's not hard to see why Intel dropped the series when you take a look at the Sapphire Rapids Xeon-W lineup they would have likely been based off of.  I think AMD would also do well to offer something that's a step above the Ryzen lineup, rather than a leap above it like the current Threadrippers.",Intel,2025-06-27 22:38:16,6
AMD,n01isnc,Well it was a downgrade on system snappiness as intel have way higher random reads than amd.,Intel,2025-06-27 10:39:30,15
AMD,n0421zg,> I mean 9800x3D and 14900K offers basically the same performance  LMAO,Intel,2025-06-27 18:42:47,11
AMD,n5exp59,"Huh? 9800x3d is universally known to be like 20-25 percent faster, even in 1 percent lows.   https://www.techspot.com/review/2931-amd-ryzen-9800x3d-vs-intel-core-14900k/",Intel,2025-07-27 11:11:38,1
AMD,n835hud,Maybe that is your experience.  Neverthelesss if you compare most gamers who switched to 9800x3D they report a significantly noticeable uplift in fps and 0.1 fps. Maybe a negligible few reported a decrease. And this has very likely nothing to do with the x3D CPU but other causes.,Intel,2025-08-11 10:24:29,1
AMD,mzzqcpb,"Ah youâ€™re missing the final piece. As far as iâ€™m aware this pretty much requires controlling the OS as well (or at least solid OS support). Consoles get their own custom operating system, Apple built a new version of MacOS for M chips. Intel and AMD though donâ€™t control windows.",Intel,2025-06-27 01:51:25,21
AMD,mzzxpgo,UMA is such a hassle That's why I don't see it much except for calculation purposes (HPC/AI)...,Intel,2025-06-27 02:36:14,8
AMD,mzztuv0,"Application developers are supposed to try to avoid copies from GPU memory to CPU memory, instead letting it stay in the GPU memory as much as possible",Intel,2025-06-27 02:12:29,7
AMD,n014s2d,">so there is still the cost of useless copies between system RAM vs allocated GPU ram.    There is none, AMDGPU drivers have supported GTT memory since forever, so static allocation part is just to reduce burden for app developers but if you use GTT memory you can do zero-copy CPU+GPU hybrid processing.",Intel,2025-06-27 08:28:28,3
AMD,n0r948t,"Intel needs something decent because AMD has taken a page out of intel (up to gen7) playbook, same cores no changes. Intel now provides more cores but it's the 100% core increase Vs AMD 50% and bLLC that should shake things up, hopefully they keep the temperature down as I don't want to have to replace case and get a 360mm rad just to not throttle, and not ever again do a 13th and 14th gen degradation show.   If all goes well going back to intel for a few years then AMD, brand loyalty is for suckers, buy what's best for performance and value. Hopefully intel i5 has 12P cores and i7 18-20P cores that would be nice to have",Intel,2025-07-01 13:38:44,1
AMD,n03y3vh,"bLLC is just a big-ass L3$ and since Intel does equal L3 slices per coherent ring stop, it'll be 6\*12 or 12\*12 with each slice doubling or quadrupling. The rumor is 144MB so quadrupled per slice, probably 2x ways and 2x sets to keep L3 latency under control.",Intel,2025-06-27 18:23:40,7
AMD,n0798ym,"Intel and AMD have effectively the same client L3 strategy. It's only allocated local to one compute die. Intel just doesn't have any multi-compute die parts till NVL.   > Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3  8+16 is one tile, in regardless of how much cache they attach to it",Intel,2025-06-28 06:37:39,5
AMD,n01j4io,It is a massive issue for amd. You're voltage limited like crazy as electron migration kills the 3D cache really fucking fast. 1.3V is already dangerous voltage for the cache.,Intel,2025-06-27 10:42:13,8
AMD,n01g781,"I still think there's a slight impact (the 9800x3d only boosts up to 5.2GHz vs the 5.5GHz of the 9700x), but compared to Zen 4, the issue does seem to have been lessened, yes.   And even with Zen 4, the Fmax benefit from not using 3D V-cache using comparable skus was also only single digits anyways.",Intel,2025-06-27 10:17:10,7
AMD,n01fnuq,"Adamantium was always rumored to be an additional L4 cache IIRC, and what Intel appears to be doing with NVL is just adding more L3 (even though ig Intel is calling their old L3 the new L4 cache? lol).   I don't think Intel can also build out Foveros-Direct at scale just yet, considering they are having problems launching it for just CLF too.",Intel,2025-06-27 10:12:22,9
AMD,n02ibwo,"I'm an e-core hater but arrow lake e-cores are really performant and make up for the loss of HT. arl/nvl 4+8 would wildly beat 6c12t adl/rpl.  HT was always a fallacy anyway. If you load up every thread, your best possible performance is ~60% of a core for a games main-thread.  I would much rather pin main-thread to best p-core in a dedicated fashion and let the other cores handle sub threads. Much better 1% lows if we optimize for arrow lake properly (still doesn't hold a candle to 9800X3D with HT disabled though).",Intel,2025-06-27 14:19:43,8
AMD,n045qi7,"Yeah, I somewhat agree with this. I suppose it depends if Intelâ€™s latency problem with their P+E core design is at all a fixable one - 4c/8t is still shockingly serviceable for gaming, but 4c/4t absolutely is not.",Intel,2025-06-27 19:00:29,2
AMD,n3fijqh,It's the same ratio as 285K 8P+16E vs AMD 16P and we know that 285K is competitive despite no hyperthreading,Intel,2025-07-16 11:51:08,1
AMD,n0ap7lk,"Sooo a few months ago, I helped a buddy of mine troubleshoot a black screen issue on his newly built 9800X3D and RTX 5090 rig, a fairly common issue with Nvidiaâ€™s latest GPUs.  While working on his PC, I'd notice a series of odd and random hiccups. For example, double clicking a window to maximize it would cause micro freezes. His monitor runs at 240Hz, and the cursor moves very smoothly, but dragging a window around felt like it was refreshing at 60Hz. Launching League of Legends would take upwards of 10+ seconds, and loading the actual game would briefly drop his FPS to the low 20s before going back to normal. Waking the system from sleep had a noticeable 2-3 seconds delay before the (wired) keyboard would respond, which is strange, considering the keyboard input was what wake the system up in the first place.  Apparently, some of these things also happen to him on his old 5800X3D system, and he thoughts that these little quirks were normal.  I did my due diligence on his AMD setup: updated the BIOS and chipset drivers, enabled EXPO profile, made sure Game Bar was enabled, set the power mode to Balanced. Basically, all the little things you need to do to get the X3D chip to play nice and left.  But man... I do not want to ever be on an AMD system.",Intel,2025-06-28 20:15:41,9
AMD,n02j14s,did they measure responsiveness and timed the click to action? and was it significantly different? how much difference are we talking about?,Intel,2025-06-27 14:23:07,11
AMD,n04xv5b,"can you explain exactly what you're talking about here? are you talking about a situation where the system needs to do random reads from an ssd? aka: boot time, initial game load time?",Intel,2025-06-27 21:22:28,6
AMD,n0429de,"How was ""system snappiness"" measured?",Intel,2025-06-27 18:43:46,8
AMD,n01uxxc,No.,Intel,2025-06-27 12:08:29,11
AMD,n08zo5w,"Now both AMD and Intel chips are â€œdisaggregated â€œ which means between cpu and other agents like memory controllers, pcie, and storage there is higher latency than the 12/13/14th gen parts. AMD has higher latency due to the larger distances involved on the package.  Also Intel is not really improving the CPU core much. There wonâ€™t be a compelling reason to upgrade from a 14700 until DDR6 comes out. At least not in desktop. Nova lake high cache parts will cost $600 or more so value/dollar will be low.",Intel,2025-06-28 14:51:50,2
AMD,n02l2ku,So? Major upgrade for everything else,Intel,2025-06-27 14:33:03,2
AMD,n0x0cru,"I had an 12600 not k. I had the opposite experience, I upgraded to a 7800x3d and the snappiness was a night and day upgrade. I can recommend a x3d to anyone. Pair that cpu with Windows 11 IoT LTSC and you have a winner <3",Intel,2025-07-02 10:05:39,0
AMD,n046538,"Meant to say ""Gaming Performance""  >Higher avg on X3D  >similar or same 1% lows on both platforms >Higher .1% lows on Intel.",Intel,2025-06-27 19:02:27,8
AMD,n043af1,"any comment that starts with ""I mean..""  I never go any further, its like some weird reddit think where everyone with ignorant comments seems to start out with this,  at least often anyway.",Intel,2025-06-27 18:48:47,-2
AMD,n5fdqig,"""Enthusiast Segment"" my good sir. All the benches you see are poorly configured or stock 14900K. With tuning it's a different story. Intel craptorlake scales with fast ram.",Intel,2025-07-27 13:07:23,0
AMD,n84bzza,"As a long time AMD user I know that Intel needs to be tuned to perform best. So when you tune the 14900K or even 285K you get like 20% performance uplift vs stock. X3D just performs great out of the box because of the huge L3 Cache. At the very least if you do not like microstutters or frame drops and want consistent gaming performance Intel 14th gen is superior vs current AMD's offering. Anyone with a specific board like Apex, Lightning, Tachyon, or even Gigabyte Refresh boards + i7/i9 13-14th gen with decent memory controller can achieve similar gaming experience. I'm speaking from experience since I also have a fully tuned 9950x3D/5090 on my testbench. For productivity task Intel feels much better to use as well. I feel like Intel is just better optimized for Windows and Productivity too.",Intel,2025-08-11 14:53:13,1
AMD,n1f1onn,"Actually Intel thermal is already better than Amd ever since Arrow Lake and Lunar Lake released. Even Core Ultra 7 258V is arround 10c cooler than Amd Z2E and Strix Point on the same watt.   On MSI Claw 8 AI+, Lunar Lake temp at 20w is just arround 62c while the Amd version is arround 70c. I wouldn't have a doubt Nova Lake and Panther Lake will also have good thermal because it will have 18A node with BPD and RibbonFET GAA which is more advance than traditional silicon when it comes to power delivery and efficiency.",Intel,2025-07-05 03:53:59,1
AMD,n0jysi1,Ah so bLLC on both tiles is a possible configuration? Any chance Intel actually goes for this?,Intel,2025-06-30 10:29:27,1
AMD,n02dy44,You can very simply get 9800x3D to 5.4 with little effort,Intel,2025-06-27 13:57:44,5
AMD,n0qy2r3,I haven't seen any of those issues on AMD where the underlying cause wouldn't also cause those issues on Intel.,Intel,2025-07-01 12:35:15,4
AMD,n02jgm4,Difference between 85MBps and 140MBps in q1t1 random reads and writes.,Intel,2025-06-27 14:25:11,-2
AMD,n01v9di,Lets just ignore the whitepaper WD and Intel did about this.,Intel,2025-06-27 12:10:35,3
AMD,n5fmwbs,So what configuration (tuning and ram settings) can a 14900k match a 9800x3d?,Intel,2025-07-27 14:00:49,1
AMD,n006ga4,That (Apple Silicon is good) and UMA are different stories I already know that Apple Silicon is good,Intel,2025-06-27 03:34:22,9
AMD,n03645p,On which benchmark(s) / metrics?,Intel,2025-06-27 16:12:47,5
AMD,n1jzzta,"Haven't kept up with mobile since AMD 5000 and intel 10th gen, all I remember is intel needing XTU undervolting and then intel blocking XTU so using third party undervolt programs, AMD like I said 5000 never needed undervolting.   Desktop side, AMD 7000 is a hot mess, like seriously ridiculous. Seems that was sorted on 9000,Â    Let's wait and see on nova lake and zen6, like I said, brand loyalty is stupid, bought into AM5, so it's cheaper for me to go for zen6 with 244Mb of L3, but no am6-7 that will be intel.",Intel,2025-07-06 00:05:28,1
AMD,n0ksjbi,"In theory, yes. For packaging reasons and market segmentation, probably not.",Intel,2025-06-30 13:51:44,2
AMD,n02kkbn,"what's that on in terms of percentage, or seconds to person? was it noticeable?  i'm not techy enough, but is random reads and writes for clicking things and accessing data, and less so on copying and pasting a file?",Intel,2025-06-27 14:30:33,10
AMD,n0226kr,where is this whitepaper,Intel,2025-06-27 12:53:19,6
AMD,n02akpt,"You can send white papers all day but if most people buy these for gaming or productivity, AMD is winning in both categories.",Intel,2025-06-27 13:40:12,7
AMD,n5g6rxb,Stock clocks 5.7/4.4/5.0 HT Off With DDR5 7600-8000 on Windows 10 22H2 or Windows 11 23H2 is enough to match 9800x3D at 5.4ghz with 6000 c28/6200 c28,Intel,2025-07-27 15:44:33,0
AMD,n7xvp9r,So MLID leaked in his most recent video about this (and he ranks it with a blue color code - 'very high confidence')  What do you think about this?,Intel,2025-08-10 13:55:29,1
AMD,n023hsg,[https://youtu.be/0dOjvdOOq04?t=283](https://youtu.be/0dOjvdOOq04?t=283) This explains it.  Gonna find the whitepapers link again.,Intel,2025-06-27 13:00:57,2
AMD,n5gbwrf,Based on what evidence?   I looked online for a few moments and found:  (1) Buildzoid from actually hardcore  overlocking did a 12 hour live stream where they couldn't even get a 8000 mhz overclock stable on the 14900k. No wonder why people haven't benched this lol  https://www.youtube.com/live/bCis9x_2IL0?si=ht3obVoBLcRFCyXI  (2) Plenty of benchmarks where an overclocked 9800x3d is about 10 percent faster than an overclocked 14900k with 7600 ddr5,Intel,2025-07-27 16:09:50,1
AMD,n007dg1,"Haven't you been listening? The conversation is strange (Confused) You first brought up the story of UMA, right?",Intel,2025-06-27 03:40:49,6
AMD,n03knav,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-06-27 17:21:00,2
AMD,n046z2m,"That channel has a lot of videos and even on this specific video it would help if you point the specific time you're referring to.  Now regarding AI, I assume you are talking about token generation speed and not prompt processing or training (for which Macs are lagging due to weak GPU compute).  I happen to have expertise in optimizing AI algorithms (see https://www.reddit.com/user/Karyo_Ten/comments/1jin6g5/memorybound_vs_computebound_deep_learning_llms/ )  The short answer is that consumers' PCs have been stuck with dual-channel RAM for very very long and with DDR5 the memory bandwidth is just 80GB/s to 100GB/s with overclocked memory.  Weakest M4 starts at 250GB/s or so and M4 Pro 400GB/s and M4 Max 540GB/s.  Slowest GPUs have 250GB/s, midrange about 800GB/s and 3090~4090 have 1000~1100GB/s with 5090 having 1800GB/s bandwidth. Laptop GPUs probably have 500~800GB/s bandwidth  LLMs token generation scales linearly with memory bandwidth, compute doesn't matter on any CPU/GPU from the past 5 years.  So by virtue of their fast memory Macs are easily 3x to 8x faster than PC on LLMs.  The rest is quite different though which is why what benchmark is important.",Intel,2025-06-27 19:06:33,3
AMD,n7xwq6a,"I'm highly suspicious, for packaging reasons if nothing else. I'm not going to call it impossible, but that would put enormous strain on the X dimension. Think about it, it's something on the order of 3-4x what the socket was originally for.  And obviously goes without saying, but MLID ""very high confidence"" doesn't mean shit.",Intel,2025-08-10 14:01:29,1
AMD,n5ghx4j,"Based on my own testing , the games I play and benchmark results. Bullzoid is not relevant to this conversation. You can keep downvoting me for speaking the truth it's okay. I can't blame you because youtube and mainstream techspot are 99% misinformation. If you believe 9800x3D is 30% faster then go buy it, nobody is stopping you. I have investments with AMD and you're doing me a favor by supporting them.",Intel,2025-07-27 16:38:58,0
AMD,n13yjw6,Just to be clear compute and hardware support for things like tensor cores have a massive impact. HBM is king but on older cards like the mi100 (released five years ago) can be out paced by a mid range card like the 4070.  All I wanted to convey is llm and token generation is a complex topic with limitations and struggles beyond memory bandwidth.,Intel,2025-07-03 11:37:55,2
AMD,n5gkgn2,"I haven't downvoted you at all, what are you even talking about.   You want people to believe in some conspiracy theory where any other information is a lie, and only you provide the ""real truth"".",Intel,2025-07-27 16:51:17,1
AMD,n0081vw,"Besides, UMA wasn't first developed by Apple. Even if Intel introduces it, the software side or the software frameworkâ€¦ Moreover, the OS side has to deal with it, so it is necessary to consider it a little. That's what you said earlier",Intel,2025-06-27 03:45:34,10
AMD,n049nak,"I take no side there. I'm a dev, I want my code to be the fastest on all platforms  I have: - M4 Max so I can optimize on ARM and MacOS - Ryzen 9950X so I can optimize with AVX512 - in the process of buying an Intel 265K so I can tune multithreaded code to heterogeneous architecture.  The problem of Intel and AMD is segmentation between consumer and pro.  If Intel and AMD want to be competitive on AI they need 8-channel DDR5 (for 350~400GB/s), except that it's either professional realm (Threadripper's are 8-channel and EPYC's are 12-channel) with $800~1000 motherboards and $1500 CPUs and $1000 of RAM.  Or they make custom designsvwith soldered LPDDR5 like the current Ryzen AI Max 395, but it's still a paltry 256GB/s.  Now consumer _need_ fast memory. Those NPUs are worthless if the data doesn't get fetched fast enough. So I expect the next-gen CPUs (Zen 6 and Nova Lake) to be quad-channel by default (~200GB/s with DDR5) so they are at least in the same ballpark as M4 chip (but still 2x slower than M4 Pro and Max).  I also expect more soldered LPDDR5 builds in the coming year.",Intel,2025-06-27 19:19:40,2
AMD,n5gmhbv,"Have you not seen HardwareUnboxed 9070 XT Finewine video? Tell me why should I trust someone like that? I'm just sayin if you want real information test it yourself. I just don't trust product influencers like GamersNexus, HUB , Jayz2cents and other mainstream channels. Id rather buy the product and test it myself. Im not forcing you to believe what I said, its for people that actually knows what Raptorlake is capable off. If you want to see a properly tuned 14900k check out facegamefps on youtube. This is a very capable platform.",Intel,2025-07-27 17:01:05,0
AMD,n0rzbmt,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-07-01 15:46:52,1
AMD,n1s9740,Is it safe to install an Arrow Lake CPU without a third-party contact frame over the long run?,Intel,2025-07-07 10:28:03,1
AMD,n212egt,"Edit: solved. Windows is capable of disabling e-cores on boot. The setting to turn them back on in MSCONFIG, boot tab, cores dropdown. Whatever feature causes this, I do not know but would love to find out, and furthermore their is no legitimate purpose for this feature, and Intel should prevent Microsoft from doing this, in my opinion, if there is any legal means available. Unless it interferes with Intelâ€™s revenue.  Poor performance, incredibly low benchmarks. Beginning to suspect bad CPU.  I have tried bone stock with cleared CMOS settings. I have loaded multiple overclocking profiles, clearing CMOS in between. I have tried undervolting, and adjusting Load Line. I have changed Lite Load settings. I have tried with and without XMP enabled. Changed windows power plans. Nothing gets my cinebench r23 score above 11k. CPU-Z benchmarks it as 43% as fast as the previous generation CPU. I never overclocked this cpu, and I changed to the updated bios with new microcode the day it was available. CPU Purchased from Intel, retail box.  Windows 11 Build 26100.4484  CPU: Intel I9 14900K, stock clock, 360 AIO liquid cooling  RAM: Patriot Viper Venom DDR5 32GB (2x16) 7200MT/S CL34  GPU: MSI Ventus NVIDIA RTX 4080S  Motherboard: MSI Z790-S Pro Wifi with most recent bios  Storage: WD Black 2TB NVME on CPU; Toshiba 20GB X300 Pro  PSU: NZXT C1000 PSU (2022)  Display: Samsung Odyssey G93SC  I am at my wits end with this thing. I first noticed games crashing to desktop a few months ago. I thought it was just poorly coded (Helldivers 2). I checked my FPS in the game, and while previously I had to frame limit it 10 144FPS, I was now getting close to 60-80FPS.  Is my CPU a toaster or is there something I'm missing?  CPU-Z output, and HWiNFO64 sensor readings:  [https://imgur.com/a/ROuZOKS](https://imgur.com/a/ROuZOKS)",Intel,2025-07-08 18:18:07,1
AMD,n2cfmmi,"I bought a 265kf CPU from Amazon. On the checkout page it clearly stated that my purchase qualified for the Intel Spring Gaming Bundle and that I would receive an email with a Master Key. Well, I didn't receive anything and I spent all day being transferred from one Amazon support staff to another to no avail.  The promotion is literally still active and if I try to buy the CPU again it shows the same offer.  But somehow no one on Amazon or Intel support can tell me why I didn't get the email.",Intel,2025-07-10 11:27:48,1
AMD,n2h44v7,Does anyone know if the upcoming Bartlett Lake-S 12 p-core no e-core CPU will suffer from the same stability issues as Intel 13th gen and 14th gen CPUs?,Intel,2025-07-11 01:52:33,1
AMD,n4at6sf,Any news to share regarding this link? https://www.reddit.com/r/intel/s/Bg4QnVzIdD,Intel,2025-07-21 06:57:04,1
AMD,n4vm1wc,"Bug report: Latest ARC driver 32.0.101.6972 causes crashing using Speed Sync  I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  Monitor is non VRR compatible (older gsync monitor ASUS PG348Q)  Hope this gets resolved as it sounds like a great feature.",Intel,2025-07-24 10:55:22,1
AMD,n5c8s6s,"Hello,  I own a i9-14900K, mobo MSI MAG Z790 Tomahawk. I am wondering if itâ€™s possible a specific program can cause my cores to be power limit exceeded, and is there any fix to that? Currently it drops the cpu speed to 0.8 GHz. This one program LeagueClient.exe for a game, League of Legends, has recently started to cause this problem, the only fix is to restart the computer. I have updated my bios, changed power limits within the bios, disabled c state, disabled EIST, disabled e-cores, tried to go into safe boot but the program wonâ€™t launch, reinstalled multiple times.",Intel,2025-07-26 22:34:12,1
AMD,n663btu,Has the degradation of the 13th and 14th gen CPUâ€™s been fixed yet?,Intel,2025-07-31 14:14:55,1
AMD,n6pmegm,"Been getting the following error A LOT while playing Expedition 33, and now after 18 hours I can't start the game without it crashing:  `LowLevelFatalError [File:C:\Hw5\Engine\Source\Runtime\RenderCore\Private\ShaderCodeArchive.cpp] [Line: 413] DecompressShaderWithOodleAndExtraLogging(): Could not decompress shader group with Oodle. Group Index: 760 Group IoStoreHash:52bcbf8ac813e7ee35697309 Group NumShaders: 29 Shader Index: 9301 Shader In-Group Index: 760 Shader Hash: 3BF30C4C9852D0D23B2DF59B4396FCC76BC3A80. The CPU (13th Gen Intel(R) Core(TM) i7-13700KF) may be unstable; for details see` [`http://www.radgametools.com/oodleintel.htm`](http://www.radgametools.com/oodleintel.htm)     Am I just screwed because I bought the wrong generation of Intel CPUs 2 years ago?",Intel,2025-08-03 15:45:17,1
AMD,n7b4kpy,"I'm not sure if this matters, but when runningÂ `garuda-inxi`Â on my laptop, it tells me that my i3-8130U is Coffee Lake Gen 8, not Kaby Lake Gen 9.5. I've seen similar problems reported with users using CPU-Z, is this just a known bug or is there something else going on?  I'm going to be tweaking my CPU performance soon, so I'd like to make sure of the CPU capabilities/options first.",Intel,2025-08-06 21:44:11,1
AMD,n85zrk7,"I've been planing to build a PC from scratch for video editing and Ultra 7 265k and Arc B580 are good in my price bracket. Now, I'm worried with all the talk of Intel potentially going bankrupt or having layoffs, is it a good idea to still buy their products? Will we lose support with drivers and stuff like that?",Intel,2025-08-11 19:48:10,1
AMD,n8auefg,My gaming laptop Intel core i7-12700h runs at a constant temperature 95 . Doesn't matter if I'm playing a high end game like cyberpunk or some indie game like hollow knight. Is this thing suppose to always run at this temp?,Intel,2025-08-12 15:12:02,1
AMD,n8i5ccq,"con el nuevo microcodigo de intel , los juegos de ubisoft (ASSASINS CRREED ODYSSE) tienen tiempos  de carga excecivamente altos, diria que al menos 10 veces mas de lo normal, Al devolverr la bios al microcodigo anterior todo funciona bien, cuando lo van a parchar?",Intel,2025-08-13 17:35:56,1
AMD,n8k6lv9,is it worth the upgrade for ai preformance cus my 5070 ti wont work for some reason so now my cpu is my main accelertor and user so should i upgrade to ultra 9? and i  will remove 5070 ti from my flair soon so yeah .,Intel,2025-08-13 23:38:35,1
AMD,n9pv76c,"ok now this is something. my Intel HD Graphics Control Panel is no longer there? no idea how long its been gone but i clearly remember it being there at a point.  using a Lenovo G510(i7-4700MQ, HD Graphics 4600)   Windows 10 Home 22H2 (Build 19045.6216)  windows apps in settings shows the intel driver but not the control panel   drivers from [lenovo's website](https://pcsupport.lenovo.com/in/en/products/laptops-and-netbooks/lenovo-g-series-laptops/lenovo-g510-notebook/20238/downloads/ds103802-intel-vga-driver-for-windows-10-64-bit-lenovo-g410-g510?category=Display%20and%20Video%20Graphics) are dated 16jul 2015. version seems to be 10.18.15.4240   drivers from [intel's website](https://www.intel.com/content/www/us/en/download/18388/intel-graphics-driver-for-windows-10-15-40-4th-gen.html?wapkw=intel%20hd%20graphics%204600) are dated 9jan 2015. version is 15.40.7.64.4279  now whats funny is that my device manager shows that my driver is dated 8mar 2017 which is version 20.19.15.4624   i also have another driver that i can see in the update drivers menu(drivers already present on my device) along with this one which is dated 29sep 2016 version 20.19.15.4531  i have tried reinstalling the driver from the update driver menu using the 8mar 2017 version, no change at all.  my drivers dont seem to be DCH drivers.   intel also says that they [discontinued the ms store version of the control panel](https://www.intel.com/content/www/us/en/support/articles/000058733/graphics.html) anyway.  this is all that i could think of writing here. any other details required just ask.   any help would be good lol",Intel,2025-08-20 14:29:49,1
AMD,na637rq,"my i5-14600KF is being throttled at low temps and refuses to go past 0.8ghz of clock speed. Nothing I do seems to get it to stop throttling. According to throttlestop i have a red EDP OTHER ongoing throttle under CORE and RING. My average CPU temp is 31 degrees C across all cores and im getting 0.69 Voltage to my CPU  CPU: Intel i5-14600KF stock settings no overclock   GPU: Intel Arc B580 ONIX Odyssey BAR resizing enabled   Motherboard Gigabyte Ultra Durable Z790 S WIFI DDR4   RAM: Corsair Vengeance DDR4 16 GB x 2 (32GB)   Storage: 1TB Corsair MP600 CORE XT SSD + 2 TB WD Black SN770   PSU: Cooler Master MWE Gold 850 V2  EDIT: NEW INFO ACQUIRED   When running in safe mode and when booting into BIOS settings my CPU acts normally and receives typical voltage. Something running on my computer is throttling my CPU as i boot into windows. If i open Task Manager quickly after booting, I see system interrupts consume a mild amount of CPU before quickly going away, rather than sticking around when their is an ongoing hardware issue. I have reason to believe a program, either maliciously or due to error, is fucking with my CPU. Also worth noting is that Intel Graphics Software reports my CPU utilization as far higher than task manager, anywhere between 2-50% higher.",Intel,2025-08-23 00:28:09,1
AMD,nand9o5,"Currently in the planning/purchasing phase of a small NVR/Steam Cache server. Information on VROC on X299 is pretty limited. so far I've seen mixed information on the drives supported. Before I purchase x4 Intel P4510 drives, I was hoping someone on here has a similar configuration that works.  The mobo manual states that only Intel based drives are supported but doesn't clarify which intel drives. Also saw on the intel forum that X299 CPU raid is further limited to only Optane based NVME drives. This drive will not be booted to, and I dont want to do a windows based raid.  My planned specs are:  CPU: 10900X  Mobo: X299 Taichi CLX - One of the few that seems to support bifurcation & VROC  Drive: Intel P4510 1TB  VROC Key: VRoc Standard  Are the Intel P4510 supported for Vroc on the x299 platform?",Intel,2025-08-25 20:44:18,1
AMD,natm1ml,"My Intel I210 ethernet device has device id 1531 meaning unprogrammed. The freebsd ethernet driver does not work with 1531. It needs device id 1533 meaning programmed. (Can I use a different driver? No, it's an embedded system that only supports this driver.)  I was linked this:  https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html  but 1) have no idea how to do it 2) cannot access the .bin file and tool it requires  Does anyone have ELI5 steps for getting the device to show devid 1533? Where can I get the .bin?",Intel,2025-08-26 19:48:39,1
AMD,nb5mwcf,"The RMA process for Intel is absolutely atrocious. I don't know if  anyone can give advice on this but here is what is happening:  Based in Germany, for geolocation info. I was one of the early adopters of the 13th gen processors, but as I don't follow tech news too strongly I didn't find out about the issues with these chips until autumn 2024 when the issues I was having with my PC escalated to a point I couldn't ignore any further. Identified the CPU as the likely culprit and started the RMA process.  Firstly, Intel would not offer any solution where I could continue to use my PC whilst they analysed my CPU. As I use my PC for work, having it out of action for weeks/months was simply not an option, so I was forced to pause the RMA ticket whilst I saved up for a new cpu way ahead of my expected timeline.  With that aside, I reopened my ticket and the requirements they lay out are near impossible to meet - they wanted a clear photo of the matrix on the front of the chip and the matrix on the pcb itself.  The front of the chip was simple enough but on this series Intel printed the matrix in dark grey on a dark green pcb. It's barely visible just with the naked eye and I've tried so many ways to take a picture of the matrix with my smartphone and nothing I do is getting a clear picture.  I don't understand how your average consumer can possibly meet this requirement - solutions online apparently suggest purchasing a special type of expensive scanner or a macro lens for your smart phone? Which is ridiculous to me.  There is no way they cannot verify my chip with the rest of the information I have been able to give them, as well as far as I am aware, the matrix on the side of the chip on the 13th gen is literally the same as the one on the front of the chip.  The whole experience is proving to be awful, time consuming, feels like it should be illegal and has completely put me off ever using Intel again, or recommending it to anyone I help spec builds for.",Intel,2025-08-28 16:16:20,1
AMD,n22cmoi,"u/Progenitor3  yes, it is generally safe to install an Arrow Lake CPU without a third-party contact frame over the long run. CPUs are designed to function properly with the standard mounting mechanisms provided by the manufacturer. Third-party contact frames are optional and may offer additional stability or cooling benefits, but they are not necessary for the safe operation of the CPU. Always ensure proper installation according to the manufacturer's guidelines to maintain optimal performance and safety.",Intel,2025-07-08 21:50:47,1
AMD,n22gizs,"u/TerminalCancerMan  Intel cannot comment or interpret results from third party benchmark tools. RunÂ [IntelÂ® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html)Â to confirm if there are any issues with the CPU. You may try this, If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.Â  If the instability ceases with Turbo disabled, it is likely that the processor need a replacement.",Intel,2025-07-08 22:10:13,1
AMD,n2h78m4,"u/Progenitor3  Just fill in your info, and itâ€™ll automatically create a ticket for you. Our team that handles those items will get in touch within 3â€“5 business days.Â   [Software Advantage Program](https://softwareoffer.intel.com/Support)",Intel,2025-07-11 02:10:36,1
AMD,n2jp4dz,Gotta wait for real-world tests to know for sure tho.,Intel,2025-07-11 13:32:10,1
AMD,n4nvw3m,"u/Special_Ad_7146 To stay on top of Intel news,Â **visit**Â ourÂ [Newsroom](https://newsroom.intel.com/).",Intel,2025-07-23 05:31:29,1
AMD,n523x8x,"u/Hippieman100  just checking can you confirm which software you're using? From what Iâ€™ve seen in the latest [ReleaseNotes\_101.6972.pdf](https://downloadmirror.intel.com/861295/ReleaseNotes_101.6972.pdf), there doesnâ€™t seem to be a â€œSpeed Syncâ€ feature in the current version that comes with this driver. It does support V-Sync and Adaptive Sync though. Just wanted to clarify are you referring to the older Intel Arc Control software or the Intel Graphics Command Center? Just making sure weâ€™re on the same page!",Intel,2025-07-25 09:46:51,1
AMD,n5hzwbu,"u/TheCupaCupa to better understand and isolate the issue, I kindly ask for some additional information. Please find the details requested below:Â   1. When the CPU drops to 0.8 GHz, do you notice anyÂ **error messages or warnings**Â in Windows or BIOS? 2. Does the issue happenÂ **only with LeagueClient.exe**, or have you seen it with other programs too? 3. Have you checkedÂ **CPU temperatures and power draw**Â when the issue occurs? 4. Can you checkÂ **Event Viewer**Â for any critical errors or warnings around the time of the slowdown? 5. Are you using anyÂ **custom power plans**Â in Windows, or is it set to Balanced/High Performance? 6. IsÂ **Intel Turbo Boost**Â enabled in BIOS? 7. **When did this issue first start happening?**Â Has it occurred before? 8. Have you made any software or hardware changes to the system recently?Â   Once I receive this information, I will be able to properly assess the situation and provide further assistance.",Intel,2025-07-27 21:08:02,1
AMD,n6sn19c,"u/Alloyd11 Not all 13th and 14th generation processors show instability issues. Just to better assist you are you planning to buy or use one of these processors, or do you need help with your current system?  Let me know how I can support you!",Intel,2025-08-04 01:26:43,1
AMD,n6sobcw,"u/amitsly â€œcrashesâ€ is a pretty broad term, and not every system issue points directly to the CPU. There are quite a few steps we go through to fully isolate the problem before concluding itâ€™s a processor fault.  To help us assist you better, could you please share a bit more info about the crashes?  * When did the issue first start happening? * Have you made any recent changes to the system either hardware or software? * Is there any visible physical damage to the system? * What troubleshooting steps have you already tried? * Have you noticed any signs of overheating? * Have you tested the processor in another working system, or tried swapping it out to see if the issue follows the CPU?  The more details you can provide, the quicker we can get to the bottom of it!",Intel,2025-08-04 01:34:29,1
AMD,n7crjzp,"u/Jay_377 its  likely comes from Intelâ€™s naming convention. The i3-8130U is part of the 8th gen, but it falls under â€œKaby Lake Refreshâ€ (for mobile chips), not â€œCoffee Lakeâ€ (which is for desktops). Tools likeÂ `garuda-inxi`Â orÂ `CPU-Z`Â might label it differently based on how they categorize architectures, not a bug, just naming differences.  [IntelÂ® Coreâ„¢ i3-8130U Processor](https://www.intel.com/content/www/us/en/products/sku/137977/intel-core-i38130u-processor-4m-cache-up-to-3-40-ghz/specifications.html)",Intel,2025-08-07 03:21:15,1
AMD,n7j7cu2,It is because Kaby Lake CPU's are 7th gen processors for laptops and 8th gen to 9th are Coffee Lake check rhis link for better understanding:https://www.intel.com/content/www/us/en/ark/products/codename/97787/products-formerly-coffee-lake.html,Intel,2025-08-08 02:36:25,1
AMD,n88eg87,"u/Reality_Bends33 Intel has been around for a long time and is known for making solid, reliable products, so you can feel confident about choosing them for your PC build. The Ultra 7 265k and Arc B580 are great picks for video editing, offering the performance you need without breaking the bank. While thereâ€™s been some discussion about Intel facing challenges, remember that big companies like Intel usually keep up with driver updates and support, even if theyâ€™re going through changes. The tech world is always evolving, and Intel is investing in new technologies to stay ahead. So, youâ€™re likely to get the support you need for your products.",Intel,2025-08-12 04:07:16,1
AMD,n8jfndw,"u/unknownboy101 Itâ€™s normal for your processor to heat up during heavy tasks like gaming. Intel CPUs are built to manage heat by adjusting power and speed, so they stay safe and avoid damage. However, running at a constantÂ **95Â°C**Â on your Intel Core i7-12700H even during light gaming isÂ **not ideal**Â and could indicate a cooling issue. While Intel CPUs are designed to handle high temperatures and will throttle performance to avoid damage, consistently running near the thermal limit can shorten the lifespan of your components and affect performance. **Feel free to check out this article for more info or steps to try. Just a heads-up this is specifically meant for boxed-type processors. You can still take a look, but I strongly recommend reaching out to your laptopâ€™s manufacturer to get help with the overall system configuration.**  [Overheating Symptoms and Troubleshooting for IntelÂ® Boxed Processors](https://www.intel.com/content/www/us/en/support/articles/000005791/processors/intel-core-processors.html)  [Is It Bad If My IntelÂ® Processor Frequently Approaches or Reaches Its...](https://www.intel.com/content/www/us/en/support/articles/000058679/processors.html)",Intel,2025-08-13 21:16:44,1
AMD,n8lchtt,"u/Frost-sama96 Tenga en cuenta que solo puedo apoyarlo en inglÃ©s. He utilizado una herramienta de traducciÃ³n web para traducir esta respuesta, por lo tanto, puede haber alguna traducciÃ³n inexacta     **To help me dig a little deeper into the issue, could you share a few details?**  * Whatâ€™s theÂ **make and model**Â of your system? Is it aÂ **laptop or desktop**? * Do you rememberÂ **when the issue first started**Â happening? * WhichÂ **BIOS version**Â are you referring to, the one that works fine? If you can share the exact version , thatâ€™d be super helpful.",Intel,2025-08-14 03:51:06,1
AMD,n8lcvrc,"u/Fluid-Analysis-2354 If your 5070 Ti isn't functioning and your CPU is now your main accelerator, upgrading to the Ultra 9 could be beneficial. The Ultra 9 offers enhanced AI performance, which can significantly improve your computing tasks. If AI capabilities are a priority for you, the upgrade is worth considering.",Intel,2025-08-14 03:53:51,1
AMD,n9tzi8c,"u/BestSpaceBot , As with all good things, your product has reached the end of its interactive technical support life. However, you can find [IntelÂ® Coreâ„¢ i7-4700MQ Processor](https://www.intel.com/content/www/us/en/products/sku/75117/intel-core-i74700mq-processor-6m-cache-up-to-3-40-ghz/specifications.html) recommendations at [Intel Community forums](https://community.intel.com/) and additional information at the [Discontinued Products](https://www.intel.com/content/www/us/en/support/articles/000005733/graphics.html) other community members may still offer helpful insights or suggestions.. It is our pleasure to continue to serve you with the next generation of Intel innovation atÂ [Intel.com](http://www.intel.com/). You may also visit this article for more details [Changes in Customer Support and Servicing Updates for Select IntelÂ®...](https://www.intel.com/content/www/us/en/support/articles/000022396/processors.html)",Intel,2025-08-21 03:26:48,1
AMD,nagz2n7,"u/ken10wil  Â To help figure out whatâ€™s going on with your CPU throttling issue, Iâ€™d like to ask a few quick questions thatâ€™ll help me dig deeper:  * When did you first start noticing the problem? * Have you made any changes to your system recently like installing new software, updating drivers, or swapping hardware? * Is there any visible damage to your PC or loose connections? * Have you updated your BIOS to the latest version for your Gigabyte Z790 board? * Did you try resetting the BIOS to default settings to see if that helps? * Have you tried reapplying thermal paste to the CPU? Just to rule out any cooling contact issues. * Are there any startup programs or services that might be messing with your CPU? * What background processes pop up right after boot? You can check Task Manager or use Reliability Monitor to trace anything unusual. * Can you run theÂ [IntelÂ® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) and let me know if it passes or fails?  Let me know what you find happy to help you troubleshoot further once we have a bit more info!",Intel,2025-08-24 20:24:35,1
AMD,nap6uz8,u/PM_pics_of_your_roof   Interesting thanks for pointing that out! Let me check this on my end and Iâ€™ll post an update here once I have accurate information.,Intel,2025-08-26 02:57:55,1
AMD,navntia,u/UC20175 Let me check this on my end and Iâ€™ll post an update here once I have accurate information.,Intel,2025-08-27 02:26:11,1
AMD,nb7ks0h,"I don't think that there it is awful, I mean in the past where the issue blew out and many consumers are reporting. People at Intel can barely accommodate them but they still give the replacement needed for the consumer.   If i am right, that image is necessary for them to get your serial number and other stuff. Unless you have the box of the processor, or you did not purchase it as a tray then I think you'll be fine.",Intel,2025-08-28 21:50:30,1
AMD,n4u7q4r,I fixed the problem. It was windows msconfig disabling e-cores on boot. You should forbid them from doing this.,Intel,2025-07-24 03:41:03,1
AMD,n527mwk,"I can't remember which (not at my pc), I think it's intel graphics command center. I get a list of v-sync options, off, on, smooth, smart and speed (from memory). Speed was not available in previous versions of the software/driver. Underneath there is an FPS limiter and a control for latency improvement (I can't remember the name) with off, on, and on + boost options available.",Intel,2025-07-25 10:19:53,1
AMD,n53dndg,"Just got back to my PC, it's actually neither Arc Control or Intel Graphics Command Center. I'm using IntelÂ® Graphics Software (25.26.1602.2), should I be using something else?",Intel,2025-07-25 14:36:18,1
AMD,n5i96k3,"Hello, thank you and I'll try my best.  1. No error messages or warnings in Windows, unsure how to check BIOS. 2. Yes, currently I have only found this happens when LeagueClient.exe is started. 3. Using HWInfo64, right when I open LeagueClient.exe, all P and E cores have ""Yes"" in the Current column for ""Power Limit Exceeded"". Core temperatures are: Current- 33C Minimum- 31C Max- 72C Average- 42C. CPU Package Power, minimum is 65.699 W, maximum is 129.230 W. Upon starting the program, the maximum value doesn't change. 4. For Event Viewer, no warnings come up when LeagueClient.exe is started. 5. I have tried setting it to Balanced, High Performance, but I'm mainly on Ultimate Performance. 6. Yes, Intel Turbo Boost is enabled. 7. The issue first started happening 2 days ago, 07/25/25. No it has not occurred before. 8. I have not made any new hardware changes to the system. I did have a windows update, 2025-07 Cumulative Update Preview for Windows 11 Version 24H2 for x64 based Systems (KB5062660) (261.000.4770) installed on 07/25/25, however I installed this update later on during the day after the problem had already started.",Intel,2025-07-27 21:57:08,1
AMD,n6tt5m6,"Well, I didn't immediately blame the CPU. The crash message specifically mentions the issue that point the finger to the CPU, plus the provided link a saying that's the root cause.  Anyhow, here are more details:  1. The issue first started happening about 2 hours into my Expedition 33 playthrough and has happened at least 20 times since (in about 18 hours of gameplay) 2. I have not made any changes whatsoever to software or hardware before it started happening. Last night, after the 20th crash, I did run an update for various drivers & the BIOS (including the 0x12F update) using Gigabyte's CC software. 3. No physical damage that I could observe through the PC's glass window 4. I have tried everything the internet had to offer about this specific issue regarding Expedition 33. This includes verifying game files, changing config and settings, reinstalling, lowering the CPU clock speed and more.  5. I didn't notice the temperature when the crashes happened but I'll be on the look out. 6. I don't have a way to swap out the CPU nor do I have a replacement CPU",Intel,2025-08-04 06:32:53,1
AMD,n7jewjr,"Weird, mine isn't in that list.",Intel,2025-08-08 03:26:09,1
AMD,n8urw7c,"I5 14600k desktop  When i try Odysse ACC, \*game\* I notice the start up and loading in to the game was so slow, like 8 or 10 minutes to Load.    \--VersiÃ³n 182011.06 MB2025/05/21SHA-256 ï¼š493D40A2351EED41FCF60E51346B9065880E87F986C1FD1FB1A3008E8C68DA26  ""Update the Intel microcode to version 0x12F to further improve system conditions that may contribute to Vmin Shift instability in Intel 13th and 14th Gen desktop-powered systems.   Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later. ""-- With this BIOS frrom ASUS page, The game do that   But, when i rollback to>   VersiÃ³n 181211.04 MB2025/03/28SHA-256 ï¼š98528167115E0B51B83304212FB0C7F7DD2DBB86F1C21833454E856D885C7EA0  ""Improve system performance and stability      Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later."" Â Intel microcode 0x12B t  The game Load A lot faster and run well.  I dont know what this happens, but is a error caused by New microcode",Intel,2025-08-15 16:11:42,1
AMD,nai5re4,"\- I started noticing the problem between august the 19th and august the 20th.  \- I updated the system around the time I \*noticed\* the problem, but I am not sure if that is when the issue started. I reverted the machine to a state prior to the update and the problem still occurs. Any other driver updates or software installs happened after I had already noticed the problem, and were initiated trying to solve the problem  \- There is no visible damage to my pc or wires  \- my Bios is up to date, and aside from turning on resizable BAR months ago for my GPU, my bios settings are default. I reset to default and turned resizable BAR on again just to be sure, and the issue still occurred.  \- I have not, mostly because it would be a hassle and because the CPU has remained very cool. I have manually overclocked it as a temporary solution to the problem and even under these conditions it is averaging 35-40 degrees Celsius when idle and hovers around 50 degrees when under stress  \- While the issue did not occur in Safe Mode, I am not sure which program is causing the throttling. I have disabled all installed startup programs and still get throttled.  \- No unusual processes pop up right after boot, though the sum of all processes hits 15% CPU utilization, stays there for a while and then the CPU throttling begins (all happens in less than 30 seconds after boot). I could see what happens after disabling even the security related startup programs and see if there is any difference  \- I pass the PDT tests but I have abysmal performance on various benchmark tests, and the PDT takes a long time to complete. Overclocking leads to more expected results.",Intel,2025-08-25 00:28:15,1
AMD,napdcv6,"Thank you. I spent some more time looking and it appears VROC on x299 was discontinued sometime in the past two years? Seems intel pushed people towards RST. I guess the question still stands since VROC on X299 has moved into sustaining mode, so drives that maybe came out during that time should still work.      [https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html](https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html)",Intel,2025-08-26 03:41:24,1
AMD,naxwb58,What I've gathered from support so far is documents needed by section 2.14/2.15 of https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html are behind a premier account registration. I'll try registering an account.,Intel,2025-08-27 13:04:17,1
AMD,nb7ugs2,"I'm mostly referring to the fact they've designed the matrix to be printed near invisibly with it being absolutely miniscule and basically the same colour as the pcb, then made it a mandatory requirement for the RMA process.   It reeks of a company trying to dodge responsibility for faulty products by making the hoops so difficult to jump through theres no reasonable expectation the average consumer will be capable of complying.   Why do I need to go out and purchase a macro lens for my smart phone? Its putting me out of even more money than I already am.",Intel,2025-08-28 22:43:46,1
AMD,n5hx86x,"u/Hippieman100 **Intel Graphics Command Center**Â andÂ **Intel Arc Control**Â have been the go-to software for many users, but theyâ€™re being phased out soon, and support will be limited moving forward.  Since system (not your PC) includes anÂ **Intel Arc B580**, I highly recommend switching toÂ **Intel Graphics Software** instead. This newer software is bundled with the graphics driver package, which you can find at the link Iâ€™ll provide-[IntelÂ® Arcâ„¢ & IrisÂ® Xe Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html). Before making the switch, please take a moment to read through all the details and driver descriptions on that page.  Also, based on my checks, it looks like you're currently usingÂ **driver version 25.26.1602.2 on your PC**, which is outdated.  Let me know if you have any questions.",Intel,2025-07-27 20:54:17,1
AMD,n5q60xv,"u/TheCupaCupa If the motherboard BIOS allows, disable Turbo and run the system to see if the issues continues.Â  If the instability ceases with Turbo disabled, please let me know.",Intel,2025-07-29 02:44:56,1
AMD,n6y9ah3,"u/amitsly For further analysis, please provide the crash dump or log files generated by the game. You can follow the guide and scroll down to the section titledÂ **""Crashing/Freezing Issues/BSOD""-**[Need help? Reporting a bug or issue? - PLEASE READ THIS FIRST! - Intel Community](https://community.intel.com/t5/Intel-ARC-Graphics/Need-help-Reporting-a-bug-or-issue-with-Arc-GPU-PLEASE-READ-THIS/m-p/1494429#M5057)Â for instructions.  Once youâ€™ve obtained the files, kindly notify me so I can send you a private message to collect the logs.  For isolation purposes, please try the following step and let me know the outcome:   **If your motherboard BIOS allows it, disable Turbo Boost and observe whether the system crashes continues.**",Intel,2025-08-04 22:17:06,1
AMD,n7jf4yf,Yeah that's weird try checking qith laptop manufacturer about it,Intel,2025-08-08 03:27:44,1
AMD,nanj5h3,"u/ken10wil  Thanks for the detailed info - this is really helpful for narrowing things down. What I'm seeing here points to a software issue rather than hardware failure. The sudden onset timeline, passing PDT tests, and the fact that safe mode works fine all suggest you're dealing with a software or driver problem.   Your CPU temps are totally normal too, so I can rule out thermal throttling. That 15% CPU usage spike right before throttling kicks in is actually a big clue - something's definitely triggering this behavior.     [Information about Temperature for IntelÂ® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [How to Know the Idle Temperature of IntelÂ® Processor](https://www.intel.com/content/www/us/en/support/articles/000090343/processors.html)  [What Is Throttling and How Can It Be Resolved?](https://www.intel.com/content/www/us/en/support/articles/000088048/processors.html)  For overclocking: Note that the motherboard does have an impact on the ability to overclock.Â Some are better quality and more capable than others. Furthermore, the way the motherboard is set up also impacts the overclockingÂ ability of a particular system (such as liquid cooler vs fan). Please note that if the system was overclocked, including voltage/frequency beyond the processor supported specifications, your processor voids warranty.Â   **Next steps to try:**Â â€¢Â **Check Windows power settings**Â \- sometimes updates mess with power profiles. Set to ""High Performance"" and see if that helps â€¢Â   **Look at that 15% CPU spike**Â \- open Task Manager right after boot and sort by CPU usage to catch what's eating those cycles â€¢Â   **Try disabling Windows security temporarily**Â \- sometimes antivirus can cause weird throttling behavior â€¢Â   **Check Event Viewer**Â \- look for any error messages around the time throttling starts â€¢Â **Check Reliability Monitor**Â \- go to Control Panel > Security and Maintenance > Reliability Monitor to spot any anomaly issues or critical events around August 19-20thThe fact that overclocking fixes it temporarily suggests your CPU is being artificially limited by software, not hardware.   Since you've already tried the obvious stuff like BIOS reset and driver rollbacks, you're probably looking at a Windows service or background process gone rogue. Keep me posted on what you find with that CPU usage spike and reliability monitor - those are likely our smoking guns!",Intel,2025-08-25 21:12:03,1
AMD,navgo8f,"u/PM_pics_of_your_roof  You're absolutely correct about VROC on X299 being discontinued. Intel moved VROC for X299 platforms into sustaining mode within the past two years and has shifted focus toward Intel RST (Rapid Storage Technology) for consumer applications. While VROC is no longer actively developed, it remains supported in sustaining mode for existing users, which means NVMe drives that were released during VROC's active development period (roughly 2017-2022) should still function properly. However, newer drives may work but won't receive official validation or certification. For anyone building new systems, Intel recommends using RST instead of VROC, but existing X299 users can continue using VROC with supported drives from the qualified vendor list. ***Support is now limited to sustaining mode with no new features or drive certifications planned, as confirmed in the support article you referenced.***",Intel,2025-08-27 01:43:49,1
AMD,nb2m74y,"u/UC20175  Per your inquiry:  The necessary tools and firmware files, but they are only accessible through Intelâ€™s Resource and Design Center (RDC), which requires a Premier account.  1. Register for an Intel RDC Premier Account 2. Visit: Intel RDC Registration GuideÂ [https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html](https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html) 3. Use a corporate email address for faster approval. 4. From the RDC, search following Content IDs:  EEPROM Access Tool (EAT) â€“ Content ID: 572162  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162)  Production NVM Images for I210/I211 â€“ Content ID: 513655  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655)",Intel,2025-08-28 03:42:30,1
AMD,nb7yt2f,I guess you are the only one who is having trouble on that part. I mean I haven't seen anyone get mad about it. So I don't really seem to find any fault I mean it is their preventive measure for fake processors that are out there. Just ask for some help to take a picture it's not that hard,Intel,2025-08-28 23:07:57,1
AMD,n5hzs51,"You have misunderstood, I am using intel graphics software already as I said, 25.26.1602.2 is the version number of that software. My driver's are up to date according to intel graphics software.",Intel,2025-07-27 21:07:25,1
AMD,n5u22od,"Hello,    great news, it solved itself. I opened the program this morning and everything was fine. If this happens again, I will try disabling Turbo and see what happens. Thank you for the help!",Intel,2025-07-29 18:03:17,1
AMD,navgz13,So what is the QVL list for x299 VROC? I canâ€™t find that information. I can find QVL for C621,Intel,2025-08-27 01:45:34,1
AMD,nb80g5j,"[This is literally what I am dealing with.](https://imgur.com/a/WBowyjO)  I'm not even worried about posting this image because please tell me how to get a better photo than this with a regular smart phone, and you can barely see there's anything there.",Intel,2025-08-28 23:17:04,1
AMD,n5i13kx,"u/Hippieman100 Ah, I see I overlooked that part! Looks like this is the installer version, my apologies for the confusion since we were discussing three different software options earlier.  Alrighty, since you're using the latest version now, how can I help? Are you running into any issues with the new application, or is there a specific feature that's not working as expected?",Intel,2025-07-27 21:14:17,1
AMD,n5why78,"u/TheCupaCupa Great to hear it fixed itself! If it happens again, trying Turbo off sounds like a good plan.",Intel,2025-07-30 01:29:50,1
AMD,navj4sy,"u/PM_pics_of_your_roof  You're encountering a common issue, Intel doesn't maintain a centralized QVL (Qualified Vendor List) specifically for X299 VROC at the platform level. Since X299 is a consumer chipset, the VROC compatibility lists were typically maintained by individual motherboard manufacturers rather than Intel directly. The C621 QVL you found is for Intel's server/workstation chipset, which has more formal validation processes. For X299 VROC compatibility, you'll need to check with your specific motherboard manufacturer (ASUS, MSI, Gigabyte, etc.) as they would have maintained their own compatibility lists during VROC's active period. However, since X299 VROC is now in sustaining mode, many manufacturers may no longer actively update these lists. Your best bet is to search for your specific motherboard model's support page or contact the manufacturer directly, though given the discontinued status, this information may be limited or archived.",Intel,2025-08-27 01:58:06,1
AMD,nbi5a7l,Relax ðŸ˜… and follow this:https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html,Intel,2025-08-30 15:16:56,1
AMD,navjsxu,"Thank you for the reply. Sadly asrock doesnâ€™t have a QVL for nvme drives in U.2 form factor or enterprise grade drives, just m.2. Let alone a QVL for VROC support. Looks like Iâ€™ll be the test subject for this.",Intel,2025-08-27 02:02:02,1
AMD,nbknxhv,"That is what I'm following, I provided support everything on that list, including decoding the matrix on the front of the chip but they don't proceed with the ticket until they get a picture of the code on the pcb which is nigh on impossible.  I wound up taking it into the office to use a high resolution scanner thats used to digitise paintings and just about got something readable. I remain firm in my stance that whilst wanting to verify a serial number isn't outrageous, making the process in which you verify said information virtually impossible is.   I simply don't understand how regular people can take such a detailed photo using smartphone technology. My phone is 4 years old and was absolutely not capable (Samsung S20+)",Intel,2025-08-30 23:14:21,1
AMD,navmgeg,u/PM_pics_of_your_roof  **Best of luck with your configuration!**Â Your pioneering work might just pave the way for others looking to implement similar setups.,Intel,2025-08-27 02:17:54,1
AMD,nbpwuw1,"u/Danderlyon  **just sent you a message, check your inbox when you get a chance!**",Intel,2025-08-31 20:03:06,1
AMD,mtbh26z,"LPCAMM2 and CAMM2 are not the same thing, CAMM2 utilizes the same DDR modules as conventional DIMMs, LPCAMM2 it for laptops and it utilizes LPDDR modules, you don't want LPDDR memory on your desktop, it has awful timings/latency.",Intel,2025-05-20 16:18:33,24
AMD,mte086a,">Faster DDR5 DRAM chips are coming, even at high capacities, but existing DIMM standard is a bottleneck due to signal degradation.   Intel is already running MRDIMMs at 8800 MT/s with 12800 MT/s planned on a conventional DIMM socket. The DIMM format can handle the frequency, but bottleneck is the DRAM itself, so MRDIMMs basically RAID two 4400 MT/s memories (and in future 6400 MT/s) on a single DIMM to get around the the limited speed of the DRAM. Signal integrity is evidently not a problem past 10 GT/s, so don't expect CAMM to suddenly unlock more performance from existing DDR5 DRAMs or to make the performance hit from multiple modules dramatically smaller (although it may help somewhat).   >Does that mean it would be prudent to wait a bit with new MoBos purchase ?   IMO no, at least not with DDR5. We will see if DDR6 at >12 GT/s runs into issues with DIMM, but that is still a ways off.Â    >LP/CAMM2 apparently brings many other benefits, not just frequency bump. It shoudl be compatible with LP/DDR6, allow line-load-reducing, clk regen ( like CUDIMM), MRDIMM data rate doubling (friggin cool!), registering, ECC etcetc   The format allows those memory types, but putting a CAMM with LPDDR into a system that doesn't support LPDDR is not going to work. Similarly to DIMM, the CPU still has to support a feature, so don't expect things like MRDIMMs to suddenly work on consumer hardware.",Intel,2025-05-21 00:03:20,4
AMD,mtioser,"We need more speed, further, faster, farther. Where is the easy button for all this?",Intel,2025-05-21 18:34:50,3
AMD,mtmrmwm,"I'm at Computex right now talking with both motherboard makers and memory makers. At this point, they're not seeing a ton of demand for CAMM2 modules in the enthusiast space, so I don't think this year will be the year for mainstream adoption. I could see adoption picking up for laptops and for full systems (see the ASUS TUF T500 that launched a couple days ago).   I don't have a crystal ball, but I'd almost think the major transition will happen with DDR6 - at this point, if you have DDR5 in a DIMM already and are upgrading your rig while reusing the same DDR5... why would you re-buy it as CAMM2?",Intel,2025-05-22 10:37:06,3
AMD,mtltk38,">you don't want LPDDR memory on your desktop, it has awful timings/latency.  Ryzen AI Max uses LPDDR5 and achieves 256GB/s memory bandwidth vs 100GB/s at most for 2-channel DDR5 on regular PC.  Apple M4 is the same. M4 Pro is 400GB/s and M4 Max is 540GB/s. Ultra is 800GB/s reaching recent GPU speed.",Intel,2025-05-22 05:08:17,-5
AMD,mysj6u2,"Yeah, this is unfortunately the big problem.. Convincing mobo makers to switch.  I think it will have to start with Laptops right? And it'll have to come from consumers not wanting normal SODIMMs or soldered RAM.. So maybe a few years yet :( .",Intel,2025-06-20 11:12:39,1
AMD,mtnk7rt,"Yeah and they still have awful memory timings, you are confusing memory channels and bandwidth with memory latency.",Intel,2025-05-22 13:44:09,5
AMD,mtnsgwd,"Latency kills any graphics application that need to display at 60fps, 144fps or more, one blip and frames drop.",Intel,2025-05-22 14:26:30,1
AMD,mp4gz3w,"Guess, I'll stay on bios v1701 on my Strix Z890-H then. I have already set my cache ratio to 40 and the NGU and D2D to 32...  My E-cores are set to 48 and my RAM is running at 8200MT/s in gear 2 with the XMP Tweaked profile.  Haven't touched any voltage settings and my system is stable. I have no idea if this can be improved but I'm quite happy with the performance at the moment.",Intel,2025-04-26 10:21:42,6
AMD,mp7y8iu,How to retain my previous undervolting settings along with this new S200 Boost mode? I have an ASRock Z890 Riptide WiFi board and a Core Ultra 9 285K.,Intel,2025-04-26 22:29:27,2
AMD,mrvcwk6,I'm having issues since I enabled S200 boost mode. I can't seem to get my RAM to run stable without xmp. If I run both RAM settings and boost mode I hard crash at random. What RAM settings do you guys use when using boost mode?,Intel,2025-05-12 06:01:02,1
AMD,mp3qblb,"Yes for xtu if 200s is off, but once 200s boost is active, xtu doesn't let you do anything. No undervolting, no multiplier. Even if vmd and undervolt protection is off, xtu wom't let you. 200s boost is useless, just use performance intel bios setting, xpm profile and undervolt in xtu amd get a bigger boost than 200s boost.",Intel,2025-04-26 05:57:56,1
AMD,mp32e3z,"Yeah, 200s boost is trash. I had better performance with an undervolting 0 OC, than no possible undervolting with XTU. The 265k sees no benifit from this. I disabled it, undervolted, and added 53x and 47x and have better perf.",Intel,2025-04-26 02:56:15,-3
AMD,mp3qxe4,"Sigh, I reverted to old bios only allow me to use xtu.",Intel,2025-04-26 06:03:28,2
AMD,mp88wl6,"200s boost is basically Intel sanctioned overclocking that does not void your warranty. Ive enabled the feature, and I'm pretty happy with how much my ram latency dropped (9.6ns) and how I'm also getting improved 1% lows as a result.   You are free to disable it, over clock and tune until your heart's content. But calling it useless is a bit of a stretch.",Intel,2025-04-26 23:30:34,4
AMD,mp3pxdq,Are you able to undervolt using xtu even 200s boost is disabled?,Intel,2025-04-26 05:54:24,1
AMD,mnw7x5w,"Rumors and alleged hardware id captures suggest that yes, more discrete GPU models are coming.Â  It appears that Intel has cancelled the higher end Battlemage Xe2 GPUs though, which considering the performance of the B580, it's a shame if we don't get to see big Battlemage.Â      But Celestial is allegedly part of the tile set for the upcoming Panther Lake chips as Xe3 cores, and Druid (Xe4) is supposedly pretty far along in production if not close to being finished.     But all of that is just rumors at this point.Â  Who the heck knows what will happen?Â  The company got a new CEO and it looks like he's trying to make Intel slim down a bit.",Intel,2025-04-19 07:52:30,36
AMD,mnznpaw,I'm not sure Intel itself knows that all things considered,Intel,2025-04-19 21:17:33,6
AMD,mnweflc,"dgpus/igpus are a marketable side effect of producing server farm gpus, so probably.",Intel,2025-04-19 09:01:22,11
AMD,mnxb4g2,Even most people at Intel probably donâ€™t know at this point.  Intelâ€™s new CEO did say that Intel will get rid of its â€œnon-core businessâ€ to focus on its â€œcore businessâ€ and â€œexpand that using AI and Software 2.0â€.,Intel,2025-04-19 13:38:19,13
AMD,mo02xsw,"Intel's [got job listings for the Arc division](https://www.pcguide.com/news/intel-isnt-giving-up-on-discrete-gpus-as-new-job-listings-reveal-the-company-is-shooting-for-much-much-higher/) and have stated they're ""shooting for much much higher"" so presumably we're at least going to get Celestial and Druid. If they both flop then maybe Arc'll get canned but otherwise I'm fairly certain Arc's here to stay.",Intel,2025-04-19 22:45:46,6
AMD,mnxmmux,* I see similarities between the engineering philosophy of Intel GPU's to AMD  Bro can share me some of your weed ?,Intel,2025-04-19 14:44:28,4
AMD,mo723i1,Given that Tan has said that he believes in you need to ship products to learn from them.  I don't see him canceling many chip programs that have future product development needs.  Intel needs ship products to get people into their eco system and GPU development is needed for AI chip development.  Either way I would not expect much rumors until closer to Thursday and Intel announces earnings.,Intel,2025-04-21 02:44:00,1
AMD,mo1ntev,"In the land of HPC, AI workloads, GPU is king. If they stop making GPU then they stop being relevant. They already goofed up big time by not making GPU last two decades. If they deprioritise dGPU now then they will never be able to catch up.",Intel,2025-04-20 05:04:13,1
AMD,mo238pi,"I do hope Intel continues.  Intel HAS to develop shaders and tensors for iGPU and NPUs anyway. Might as well get more out of it by releasing dGPU cards as well.  Nvidia completely abandoned the low end of the market. It's likely a low margin product, but with high volumes. The market used to be mostly sub 300 â‚¬ GPUs.  As for mid tier and high tier card, Nvidia sells their VRAM like it's made of money, and AMD is hopelessly behind in software. Words can't describe how bad AMD is at accelerating anything.   Give how quickly Intel figured out driver, I am more hopeful Intel can figure out an acceleration stack to make use of their NPU units.",Intel,2025-04-20 07:34:57,1
AMD,mnzdjuj,"My best is they'll slim down the division and not release discrete cards. They need to keep the r and d up to make their igpu competitive.   Intel need to save money first and foremost, so I would think layoffs are coming fairly soon",Intel,2025-04-19 20:21:08,0
AMD,mo2lg06,"I hope they keep trying. While not perfect (of course)  , they had good showing with ""only"" 2 gens of products. But they really need to fix gpu usage, which seems kind of low right now. They would need to fix this before launching high end gpus, because right now going larger doesn't seem to scale very well for their arch.",Intel,2025-04-20 10:48:47,-1
AMD,mo42t8z,"Do they even produce B570/580?   Not even available, or for a stupid price.",Intel,2025-04-20 16:32:11,-1
AMD,mnw9fnk,"> But Celestial is allegedly part of the tile set for the upcoming Panther Lake chips as Xe3 cores  PTL has an Xe3 iGPU. Celestial is the name for a (once) future dGPU line. Xe3-derived dGPUs are dead, however. The last shred of hope would be something Xe4-based years down the line.",Intel,2025-04-19 08:08:03,-11
AMD,mnxca1g,And it has also allowed intel to make proper integrated graphics on the laptop side,Intel,2025-04-19 13:45:24,11
AMD,mny9o1l,"Clearly,  As we know, network administrators pass their time by playing video games on server farms they manage.",Intel,2025-04-19 16:47:05,1
AMD,mnxx22p,"He mostly means, Mobileye etc. Not GPU's... Though that might not also make the cut.  I think new CEO is more likely to split the foundry than to cut GPU's given their importance in AI. I don't think either of these is going to happen.",Intel,2025-04-19 15:40:22,10
AMD,mo7yliu,"Yeah, but GPUs and WiFi chips are both part of the intel package, even if they dropped discrete gpus they still need to develop igpus and ai accelerators, so it's probably not actually that big of a cost",Intel,2025-04-21 07:23:18,1
AMD,mnxhvuk,"Yep, GPUs were Patâ€™s project, most likely gone with the new guyâ€™s focus on essentials.",Intel,2025-04-19 14:18:19,-5
AMD,mocligm,"> AMD is hopelessly behind in software  If Nvidia drivers are anything to go by, they may be trying to catch up to AMD's Glory days.",Intel,2025-04-22 00:25:48,4
AMD,mohjbi4,"How can they save money if they have to develop the software parts all by themselves ? They can't, and that's why they need as much help from the open-source community as possible. That means putting GPUs into their hands.",Intel,2025-04-22 19:49:07,1
AMD,mnxkmy0,â€œHardware Unboxed said the same.â€ ðŸ¤£,Intel,2025-04-19 14:33:37,9
AMD,mnxqokt,Hereâ€™s to hoping that the overhead issue gets fixed soon so that we could have better Intel gpuâ€™s,Intel,2025-04-19 15:06:30,4
AMD,mnxwcvv,"Or much simpler explanation. It didn't scale well to make economic sense?  If anything overhead issue is less of an issue on a premium higher performance cards, as people are more likely to overpay for their cpu in higher price ranges.",Intel,2025-04-19 15:36:43,4
AMD,mny05l2,"Overhead issue is when we use older cpus like 9400 or 3600  If intel is to make high end GPU , it would be obviously bought buy those having good enough cpu as well like 5700x or 12600k  It would be stupid imo to buy a high end GPU (whether it's amd .intel or nvidia ) with older CPUs  The real reason could be potential b770 wasn't giving satisfactory performance uplift to be considered as mid-high end GPU that's why they cancelled b770 and now looking forward c770 i.e celestial",Intel,2025-04-19 15:56:28,4
AMD,mnxwr31,"I have not seen anything that indicates that it's cancelled except -more often wrong than right- Moore's law is dead.  In fact Intel product head, said that they would continue with discrete in this January. Though didn't specify if they will ever go for a higher end card.",Intel,2025-04-19 15:38:47,6
AMD,mnwb2gs,"I can't find any good info saying Celestial cards with Xe3/Xe3p are cancelled or dead.Â Â Peterson himself said the work for Celestial dGPUs is finished, and they have been spotted in data pulls on Linux.Â  I don't believe either one of those things mean there will never be a Xe3/Xe3p dGPU.",Intel,2025-04-19 08:25:21,10
AMD,mo2m8dl,stop watching Moore's law is dead and stating his videos as facts,Intel,2025-04-20 10:56:18,3
AMD,mnz0om3,"He is referring to any money-losing side project.  To survive, Intel needs to tighten its belt to conserve money, just as AMD did between 2011 and 2017.  There is no way that Intel would abandon GPU development, as ML/AI is a very lucrative market, but most users here are talking specifically about gaming GPUs.",Intel,2025-04-19 19:09:22,8
AMD,mnxify7,"There will still be Intel GPUs for ML/Al (because thatâ€™s where the money is), but I donâ€™t think that that is what most users here are talking about.",Intel,2025-04-19 14:21:27,8
AMD,mny2bpe,"> Or much simpler explanation. It didn't scale well to make economic sense?  That's pretty much the entire Arc lineup  >If anything overhead issue is less of an issue on a premium higher performance cards, as people are more likely to overpay for their cpu in higher price ranges.  Imagine that it makes twice as many draw calls.  Even the Ryzen 7 9800X3D is going to be overloaded.",Intel,2025-04-19 16:08:00,2
AMD,mny284q,Imagine that it makes twice as many draw calls.  Even the Ryzen 7 9800X3D is going to be overloaded.,Intel,2025-04-19 16:07:28,0
AMD,mnxyklt,"> I have not seen anything that indicates that it's cancelled except -more often wrong than right- Moore's law is dead.  That would be a broken clock right twice a day kind of situation. Gelsinger cancelled Celestial a few months before he was fired.   > In fact Intel product head, said that they would continue with discrete in this January  The wording was ""continued investment"", which means precisely nothing at all. A single future driver update for BMG would count.",Intel,2025-04-19 15:48:08,0
AMD,mnwbn77,"> Peterson himself said the work for Celestial dGPUs is finished  No, he said nothing at all about dGPUs. He didn't even name Celestial at all. His comments were that Xe3 was essentially finished, because that's the PTL iGPU, and we know that's more or less done from a hardware standpoint.",Intel,2025-04-19 08:31:35,-3
AMD,mo3978v,Who ever said I got that from him?,Intel,2025-04-20 13:49:09,-1
AMD,mohiqio,"Not to mention, nVidia is also coming to eat Intel's lunch, with an APU of its own (and surely there'll be consumer versions too).   It would be absolutely suicidal for Intel to get rid of its GPU business unit.",Intel,2025-04-22 19:46:15,1
AMD,mo19jy5,"You don't know what you're talking about, you never did, I always read your posts and you act as if you know what you're talking about.  Intel's job hiring proves you're just a clueless redditor making things up https://www.pcguide.com/news/intel-isnt-giving-up-on-discrete-gpus-as-new-job-listings-reveal-the-company-is-shooting-for-much-much-higher/",Intel,2025-04-20 03:10:41,5
AMD,mnwdjn7,"Fair enough, but that still does not make the point that there will be no Celestial dGPU release.Â  If xe3 is all the way done and allegedly popping up on high performance test strings on Linux, it would follow that sometime soon we will see Celestial in the graphics card market.     Again, there seems to be no evidence that Celestial is cancelled on dGPUs.Â  The only evidence found is that the uArch is developed and will, in tile form, be an igpu.",Intel,2025-04-19 08:51:47,4
AMD,mo1az4z,"Lmao, people continue to be in denial about the things everyone else everyone knows. Tell me, is it still ""FUD"" that Intel's using N3 for ARL? Or that BMG is 4060-tier?  Btw, you can't even find that job link.",Intel,2025-04-20 03:20:48,-1
AMD,mnwdvqa,"No *public* evidence, at least. Intel has been cagey on this for a reason. Would simplify things if they just admit what their roadmap (or lack thereof) is.",Intel,2025-04-19 08:55:28,-3
AMD,mo2s9s5,"Notice how you started to move the goal posts, you can literally find the job postings yourself with a simple Google search.  Also, maybe you should do something else in life than sitting on reddit for over a decade spewing BS.",Intel,2025-04-20 11:49:52,3
AMD,mo0dctd,"Yeah if they layed out a real honest roadmap and stuck with it, it would be better for everyone.",Intel,2025-04-19 23:46:35,1
AMD,mocou56,Iâ€™m runningÂ 1703Â with theÂ Z890 EXTREME. Zero issues.,Intel,2025-04-22 00:45:13,3
AMD,mofvmdv,Release date for 200s boost bios?,Intel,2025-04-22 14:58:25,2
AMD,mogpgun,"I don't usually get the board list/release notes until Friday. However, roll out starts during that week, so you can start checking Tuesday - Thursday, but I don't have a better date for you at this time.",Intel,2025-04-22 17:22:56,4
AMD,moh375i,it's available if you have an apex:  https://dlcdnets.asus.com/pub/ASUS/mb/BIOS/ROG-MAXIMUS-Z890-APEX-ASUS-1801.ZIP  probably soon otherwise tho,Intel,2025-04-22 18:29:00,2
AMD,mohl3j7,Thank you but actually i am looking for Z890 TUF-PLUS.,Intel,2025-04-22 19:57:48,1
AMD,moixtuw,Intel 200S boost available on version 2001  [https://dlcdnets.asus.com/pub/ASUS/mb/BIOS/TUF-GAMING-Z890-PLUS-WIFI-ASUS-2001.ZIP?model=TUF%20GAMING%20Z890-PLUS%20WIFI](https://dlcdnets.asus.com/pub/ASUS/mb/BIOS/TUF-GAMING-Z890-PLUS-WIFI-ASUS-2001.ZIP?model=TUF%20GAMING%20Z890-PLUS%20WIFI),Intel,2025-04-23 00:16:59,1
AMD,mojg2y4,"One thing also to note is that because of the way our website syncs, sometimes you might find a newer BIOS on a different regional website (although this doesn't happen often). So, if you see someone say they see it, and you don't, try to find out which regional website they saw the BIOS update listed.",Intel,2025-04-23 02:04:03,3
AMD,n626b4v,b860 tiene soporte para 200s boost?,Intel,2025-07-30 22:09:47,1
