brand,comment_id,text,subreddit,created_utc,score
Intel,nxxeocb,Great.  Does this mean AMD will finaly stop pricing Strix Point as if it was made out of gold ?,hardware,2026-01-06 01:29:45,288
Intel,nxxrlux,"One of the biggest things the current AMD driven handhelds lack is a decent upscaling option, so getting native XeSS support on a fast GPU would be a HUGE performance boost.",hardware,2026-01-06 02:39:52,93
Intel,nxy2sll,"I think the LPE cores and them going at chiplets a second time after Meteor lake is paying off. This chip is more efficient than lunar lake, a chip that could do 0.62W idle lol.",hardware,2026-01-06 03:42:57,31
Intel,nxy9wxm,"This is exciting. Hope some decently priced handhelds can drop, RAM prices notwithstanding.",hardware,2026-01-06 04:26:42,10
Intel,nxxr090,am confused. this is battlemage too right? because its a B series. but its supposed to be all new. and the old gen was battlemage too on the 200V series. so what is going on here?. is this just a bigger GPU or is this Xe3 so that would be Celestial.,hardware,2026-01-06 02:36:35,22
Intel,nxxhg0r,brah they straight up claiming it's equivalent to a 4050 on stream >!(a 60W RTX 4050)!<,hardware,2026-01-06 01:44:36,47
Intel,nxyc1w5,Xps is a huge seller for Dell and they are straight up using Panther Lake and XE3. They are exclusively going intel. Intel is 100% securing up there dominance in Labtops. In the process also taking business away from Nvidia.,hardware,2026-01-06 04:40:47,16
Intel,nxykdxb,"Even if that claim were overstated by 2x, would still be a colossal L for amd.",hardware,2026-01-06 05:39:32,18
Intel,ny1ev6y,XESS and native frame gen is going to make handhelds monsters with Panther Lake in them.,hardware,2026-01-06 17:14:42,3
Intel,nxxgruz,I hesitate to trust Intel's charts. But I am interested if Intel will actually get companies to adopt panther lake for their handheld pc. They did not have much luck with lunar lake.,hardware,2026-01-06 01:41:01,25
Intel,ny1ifg6,"Assuming intel also keeps those mobile CPUs a good price, this could be really good. Hopefully as well they add the B390 in their high power desktop CPUs, seeing a core ultra 5 with an iGPU like this would really mitigate the need for a dedicated GPU right away Mostly because iGPUs on other generations were bad, and only a select few Ryzen CPUs had the 890M. Budget systems could become much better for gaming on the low side for graphical intensive games",hardware,2026-01-06 17:30:54,3
Intel,nxz4ji6,>Intel reference platform; Memory: 32GB LPDDR5 9600;  I wonder how much difference that makes and if we'll even see laptops with such RAM in this economy...,hardware,2026-01-06 08:34:38,5
Intel,ny34ie7,"I'd love to see benchmarks comparing it to lower end discrete GPUs (like 5050, B570, etc). Could be a boon for ultra low cost builds depending on what price point it lands at.",hardware,2026-01-06 21:54:52,2
Intel,nxxyz1h,How many compute units does it have?,hardware,2026-01-06 03:20:58,3
Intel,ny0ibg4,I think people need to be ready for the fact that OEMs aren't going to use lpddr5x-9000,hardware,2026-01-06 14:42:06,1
Intel,nxxd70q,"We'll see. Every year they claim they're faster, and every year they have been proven not to be",hardware,2026-01-06 01:21:44,-26
Intel,nxxgaz9,NOTE: this might be because it has MFG (Multi-Frame-Generation).  We have to see reports to see if its true or not.,hardware,2026-01-06 01:38:30,-11
Intel,nxxoge8,"This ain't gonna matter. It's the sku with 50% more igpu cores compared to lunarlake, which already has an igpu that's larger than the hx 370, it's real expensive. Imagine a hx370 with 26cu instead of 16, that's the price range you're lookin at  Any system built with this is gonna need to run at extremely high mem speed to feed the really large igpu which in the current market with insane ram prices is gonna be priced out of most people's budget. Are ya prepared for a gaming handheld that costs north of $1500?  And since this is gonna compete against nvidia's entry level mobile gpus oems are gonna have to choose between nvidia and intel for the gaming brand on laptops. Amd learned this through the hard way that most oems would choose nvidia over a large igpu.",hardware,2026-01-06 02:22:32,-13
Intel,nxykt3q,Haven't they been making similar claims for all their failed GPU's?,hardware,2026-01-06 05:42:44,-12
Intel,nxyax11,"I mean Intel has never fudged the numbers before when they were behind, or do something crazy like literally bribe people.... Oh wait.... Uh.... Oh.....   Jokes aside, with what the current and future state of the market looks like, people might have to get used to iGPU graphics.",hardware,2026-01-06 04:33:13,-14
Intel,nxyljzq,"To be fair it kind of is, the die size is huge, larger than an RTX 5070 die",hardware,2026-01-06 05:48:23,51
Intel,nxxotcs,"Intel laptops were already better tbh, AMD had nothing to compete with Lunar Lake, and Arrow lake pretty much was better at high perf efficiency. Zen 6 better not be delayed or AMD will be buried under intel, qualcomm and apple all launching a real next gen shortly",hardware,2026-01-06 02:24:30,84
Intel,nxz1mq8,"Nah, because it's an ""AI chip"" and AMD will market it as so. AI equals fancy even though the AI capability can't match a regular desktop computer for far less.  Intel is probably gonna strategically match AMD in price.",hardware,2026-01-06 08:07:06,5
Intel,nxxvwrz,"With how wide the memory bus is, how much RAM it requires, nah the price is going up.",hardware,2026-01-06 03:03:31,21
Intel,nxzxy26,it has soldered ram ... so it's better then gold!,hardware,2026-01-06 12:45:44,2
Intel,ny04wij,I guess that depends on Intel pricing too. Considering it's using both the latest TSMC and Intel foundries in one chip package. Not to mention the LPDDR5 9600.,hardware,2026-01-06 13:28:38,2
Intel,nxxhzsm,You mean OEMs.,hardware,2026-01-06 01:47:35,-13
Intel,nxyn46u,No?  People will still value AMD more ue to brand so Intel will have to rely on volume for revenue  For reference only yesterday on this sub we had people talking about Intel lacking efficiency in comparison in mobile space,hardware,2026-01-06 06:00:32,-13
Intel,nxy8gkb,Crazy AMD haven't updated their iGPU to RDNA 4. I know they're probably waiting for UDNA but it would have been almost 3 years on the same architecture by the time we get the UDNA refresh next year (if they even bring it to their APUs right away). Sort of disappointing.,hardware,2026-01-06 04:17:21,56
Intel,nxxun0v,"tbf the most important issue is, few games implemented XeSS, just like AMD FSR.  And I think XeSS 3 being implemented in more games is a net positive for AMD GPU too.",hardware,2026-01-06 02:56:27,24
Intel,nxz85mh,"With everything happening around NVIDIA´s price increases and AMD´s lack of providing updates where it hurts, it **feels like** AI-Datacenters are more important right now for them (like the last 2 years).  But who can resent them as Intel had products that where not so much competitive that time.    Arrow lake (Desktop) at least closed on efficiency, but lacks a bit of gaming performance still, hopefully Nova Lake will be the step required to push more competition.   On GPU the same, AMD does not compete with NVIDIA in higher segments while NVIDIA is fairly comfortable with their setup and increases prices because they want to milk customers to increase their ridiculious margins (up to 70%) that they are used to from AI-Chips.   And now Intel also provides Multi-Frame generation, while a niche for me still, starting to compete with AMD and closes up to NVIDIA in terms of Software support, which they lacked the most and fixes a lot of problems.   Now let them release a B770 that is rumoured to be fairly mid/high range and we can hope for competition that actually learned from bad products recently and tries to make it better.",hardware,2026-01-06 09:09:24,10
Intel,nxz92l2,"Handhelds is a tiny tiny market, basing your product stack around them would be monumentally stupid.",hardware,2026-01-06 09:18:25,1
Intel,ny0x7rg,"It may not beat LNL in very low power envelopes (LNL was designed for ~10W, PTL for 15W+), but it's a much, much better baseline than what Intel's historically had in client. Even just extending vaguely LNL-tier efficiency across the stack is a very big deal. Looks like Intel finally has a respectable SoC architecture. Now just need to get the cores and such in shape.",hardware,2026-01-06 15:54:14,7
Intel,ny4y069,"I mean the Xbox Ally X handheld is considered a $999 ""console"" so it sets the floor for what the Steam Deck and other handhelds would be priced at.",hardware,2026-01-07 03:37:21,1
Intel,nxxwdna,"It's branded as a Battlemage for some reason, but the architecture is Xe3. It's much closer to Celestial than it is to Battlemage.",hardware,2026-01-06 03:06:08,48
Intel,nxxt4ce,"Battlemage is the brand name. The actual architecture of Lunar Lake is Xe2, same as desktop Battlemage, but they never explicitly called it Battlemage, only “Arc Graphics”.  What is meant to be desktop Celestial is Xe3P, but desktop Celestial is likely cancelled or significantly scaled back. Alchemist was a massive flop, and by the time the B580 came out to salvage Arc’s reputation the axe had probably already swung.",hardware,2026-01-06 02:48:07,8
Intel,nxxpiop,"They claimed ""10% faster"" than 4050   https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Famd-is-done-v0-8op4m6l6bmbg1.jpeg%3Fwidth%3D1851%26format%3Dpjpg%26auto%3Dwebp%26s%3Df229e1ff0e364a6db90715de23ba799261ffe9e3",hardware,2026-01-06 02:28:19,50
Intel,nxxqhlr,APUs are always way worse at gaming than synthetics when compared to a DGPU due to memory bandwidth limitation and power sharing with the CPU among other things like cache set up etc.  when they compare them to GPUs its always synthetics unless you get benchmarks of games,hardware,2026-01-06 02:33:43,21
Intel,nxxl8hj,"Where's the bandwidth coming from?   Reviewers were saying that the 890m was bandwidth starved, so how can this chip be neck and neck with a recent dgpu with multiple memory channels",hardware,2026-01-06 02:05:05,7
Intel,nxy0hn7,60W is the laptop power draw. It looks like 30W for the 4050  this is the laptop they used for the comparison https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor,hardware,2026-01-06 03:29:36,5
Intel,nxy6cgs,"At best, it’s a 16% difference between a 100 watt and 60 watt RTX 4050 I believe, based on synthetic performance  Edit: Intel used a 30 watt 4050, this comment is incorrect",hardware,2026-01-06 04:04:08,-1
Intel,ny0j8e4,"I got downvoted everytime I brought this up, but this is precisely why Nvidia wanted a deal to have an Nvidia iGPU tile on an Intel APU: Large iGPUs in thin and lights are going to get good enough over the next few years to make them the new entry-level graphics option for people. This directly threatens Nvidia's consumer laptop volume in the entry segment. Intel is claiming close to 4050 performance at this lower TDP, and that's certainly good enough for many to not have the tradeoffs of having a dGPU in their laptop.  If the new market is moving towards putting GPUs on the CPU package instead of discretely on the board, Nvidia doesn't want to place all of their hopes on WoA becoming better, and are hedging by doing both their own SoC *and* an x86 APU with Intel.  The XPS line dropping Nvidia discrete all together is proof of this. In these sub 70W total laptop power markets, a discrete GPU is just eats into the power budget too much.",hardware,2026-01-06 14:46:50,8
Intel,nxzgk6d,"In the ultraportables market (like XPS), integrated graphics just make so much sense (energy envelope; cooling system required; battery life; etc); and that's already substantial and before considering the cost of a NVIDIA mobile dGPU itself.  I don't understand why AMD decided to price Strix Point and Strix Halo so ridiculously -- it's their market for the taking.",hardware,2026-01-06 10:28:32,7
Intel,nxzf0cx,I think they’re trying to take away business from Qualcomm/arm on windows before it takes off,hardware,2026-01-06 10:14:30,6
Intel,ny0ce80,What do you mean that a refreshed Strix Point can’t compete with an updated architecture?,hardware,2026-01-06 14:10:24,5
Intel,nxxj6sm,Intel charts for their gpus have been pretty on point   Msi claw with lunar lake is one of the best handhelds,hardware,2026-01-06 01:54:01,65
Intel,nxyetge,"Lunar Lake was a expensive product which didn't make sense in handhelds, Intel just didn't have anything else so they slapped that on the MSI Claw. Now the options should be much better considering they are selling a lower core Xe3 version for cheap too",hardware,2026-01-06 04:59:20,12
Intel,ny2yll4,"I'm just curious how they handle the need for such high speed RAM on desktop though? I guess this is an application where CAMM2 will be required, I don't think DDR5-9600+ is possible without it and this is presumably pretty key to the performance.",hardware,2026-01-06 21:27:43,5
Intel,nxzgr0g,"It certainly would make a huge difference because iGPUs are very memory bandwidth bound; and as the name suggests, LPDDR5 9600 has literally twice the bandwidth of the JEDEC standard 4800.  Unfortunately I doubt we'll see reasonably priced laptops with LPDDR5 9600 -- even as an add-on option. I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800; and many SKUs that were 2x16GB are now 1x16GB; yes **single channel**.... they charge you extra if you want 2x8GB.",hardware,2026-01-06 10:30:16,12
Intel,nxyivuh,"It has 12 Xe3 cores. Intel doesn't use the term Compute Units, AMD does.",hardware,2026-01-06 05:28:14,12
Intel,nxzfdoo,X9 and X7 have 12 Xe cores and the best Ultra 5 has 10 Xe cores,hardware,2026-01-06 10:17:51,3
Intel,nxxf8he,??? lunar lake has already shown to be faster than the 890m.  73 percent though seems like a bit much since panther lake was claimed to be around a 50 percent increase over lunar lake,hardware,2026-01-06 01:32:47,66
Intel,nxxh29b,Example?,hardware,2026-01-06 01:42:33,14
Intel,nxxhysy,"They did make a graph specifically to compare the performance of HX 370 and this Arc B390 while they were both using 2x upscaling, which is where this 73% number comes from. In another graph featuring supposedly ""native"" 1080p, they claimed Arc B390 was 82% faster than the HX 370 (why don't they just call it Radeon 890M though...)",hardware,2026-01-06 01:47:26,28
Intel,nxxhhap,"No, intel claims 73% with upscaling (both) and 82% native",hardware,2026-01-06 01:44:47,20
Intel,nxy83by,"If it was only a 73% gain *including* MFG, then that would be a serious performance regression. If they were using MFG in their graphs, it would easily be 200% - 300% ""faster"" at the same ""real"" performance",hardware,2026-01-06 04:15:06,7
Intel,nxxhnbn,"The graphs all listed games and I didn't see any synthetic benchmark scores were listed, so yeah.",hardware,2026-01-06 01:45:42,23
Intel,nxxsvz8,The relative proportion of the die isn't as important as the die size itself and the node ofc.   Lunar lake for example has an estimated die size smaller than the hx370 so even if they did make the die bigger I don't think that is going to massively raise the price. Not to mention intel owns the foundry unlike AMD who are outsourcing to TSMC. This isn't in the realm of a strix halo competitor with a 300 mm\^2 + die size.   Dell for example has already refreshed the XPS line with intel panther lake and cut out the option for a dedicated gpu.,hardware,2026-01-06 02:46:52,18
Intel,nxxhf0d,"There are 50% more GPU cores here than on Lunar or Arrow Lake. CPU is still 16 cores as well compared to Arrow Lake, just shifted from 6+8+2 to 4+8+4.",hardware,2026-01-06 01:44:27,27
Intel,ny08rcj,"Do you mean Strix Halo?  Halo is made up of THREE dies. Two are regular CCD and one is a ~300mm2 graphics die. Total die area is around 440mm2 IIRC.  It's expensive, but not THAT expensive.",hardware,2026-01-06 13:50:17,27
Intel,nxxs32m,Doesn't this depend on use case? AMD laptops are more capable for gaming and the iGPU can also use the lesser version of FSR. Intel is obv better for productivity.,hardware,2026-01-06 02:42:30,16
Intel,nxzgbtp,"I disagree, Arrow lake HX seems to be more expensive than AMD HX as least on Lenovo Legion Pro setup.   I would have buy Arrow lake for the same price but AMD is cheaper by $200.",hardware,2026-01-06 10:26:24,3
Intel,nxy0jsm,Eh? It's a standard 128 bit memory bus.​,hardware,2026-01-06 03:29:57,30
Intel,nxz8yd7,"You can't price it higher than people are willing to pay, how high that is I have no idea, people bonkers buying CPU only laptops at these high prices if gaming is something they really want to do.",hardware,2026-01-06 09:17:16,1
Intel,nxyjv70,Oems magically dont price intel variants as if they were made out of gold?,hardware,2026-01-06 05:35:35,8
Intel,nxz7zsg,"Reddit isn’t indicative of anything really, most casual laptop buyers don’t even know what AMD is.",hardware,2026-01-06 09:07:48,8
Intel,ny2zhn1,"Lmao check the data, Intel has 79% of the laptop market share currently",hardware,2026-01-06 21:31:48,2
Intel,nxzevm6,"I wouldn't be surprised if this is because the team has chosen to focus efforts on UDNA because that's the architecture next-gen consoles would use. They only have so much talent and headcount on their graphics division after all, and consoles have much higher volume (even tho low margins) and thus take priority.",hardware,2026-01-06 10:13:17,33
Intel,nxy8549,Not an ideal solution but Optiscaler exists,hardware,2026-01-06 04:15:24,15
Intel,nxzf3cy,"NVIDIA has increased margins but they haven't been that terrible. Part of the compounding issue at play is limited TSMC capacity; with both gaming and DC on the same TSMC node.  Ampere (crypto bubble ignoring) was priced well and many excellent cards in there since it was on Samsung, a cheap fab; while DC/workstation chips got TSMC.",hardware,2026-01-06 10:15:15,5
Intel,ny0vvvv,Who said anything about basing the entire product stack around handhelds?,hardware,2026-01-06 15:48:11,3
Intel,ny13z79,"That's not quite right. Power levels are determined by the frequency of a given CPU core. The LPE cores in Lunar and Panther lake both clock up to 3.7 GHz, so given the added IPC of the new Panther lake e-cores and better process node, it is more efficient. Base power levels tell you nothing really.",hardware,2026-01-06 16:25:13,4
Intel,nxz3zvx,"[It's actually closer to Battlemage than Celestial. Straight from Tom Petersen](https://youtu.be/P2AsCkKi-vs?t=1576)  >""Unfortunately that Xe3 name got decided years ago, it's actually spread around the Linux stack. Changing the name of that would have been very, very painful. So, that's why you're seeing this disjointedness abut Intel Arc ""B"" series. **Well, [Panther Lake] is B series because it's similar to Xe2** and we want to be transparent with our customers. Panther Lake has a new and improved GPU, that GPU is bigger and **it's very similar to B series.**""",hardware,2026-01-06 08:29:22,17
Intel,nxyzwrf,"Xe3 isn't Celestial, only Xe3P will be. See [https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake](https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake)",hardware,2026-01-06 07:51:09,11
Intel,ny0y47o,"Yeah, it's a proper generational jump. Intel marketing is just dumb, and the comments from Peterson claiming Xe3 is somehow a smaller jump than Xe3p are just laughable.",hardware,2026-01-06 15:58:20,3
Intel,nxyhb1s,The reason is marketing (the Battlemage brand is hot and filled with good will ATM so resetting to celestial so soon is not ideal regardless of panther Lake being xe³) Peterson addressed this a bit ago.,hardware,2026-01-06 05:16:46,1
Intel,nxysdu1,Xe3p was alr confirmed coming im sure Celestial happens,hardware,2026-01-06 06:43:57,2
Intel,nxzg5fp,"We will see, while Intel's PR and marketing is extremely confusing, Intel did confirm Xe3P will come to desktop; and at least from driver updates (as a very happy B580 owner) driver support has been constant and lively.  I had some issues with an older Civ game, I reported an issue in [their app](https://www.intel.com/content/www/us/en/support/articles/000057021/graphics/other-graphics.html) with screenshots/etc, and while I never got any notification, the game works perfectly now a few months later. Dunno if they read those reports, but my card keeps getting better.  I actually think a MSRP B580 is another card that will age like fined wine -- YMMV depends on games you play, but in Australia they have been regularly sold slightly below international MSRP and represent phenomenal value in the price class.",hardware,2026-01-06 10:24:47,0
Intel,nxzdyw4,That's bloody good for an iGPU. It's been nice to see them finally get to respectable performance over the last few years. Intel in particular has really upped their iGPU game & it shows.,hardware,2026-01-06 10:05:03,19
Intel,nxxpg2g,not to mention it is a little skewed as they threw in a title which pushed the vram limit on the 4050 making the b390 over 800 percent faster in that title which obviously messes with the average.,hardware,2026-01-06 02:27:55,18
Intel,nxxsozj,It's 10% faster geomean across 45 games,hardware,2026-01-06 02:45:47,25
Intel,nxzfaqu,That can be resolved if either Intel or AMD decides to unlock quad-channel on consumer chips and mobos. It's artificial market segmentation; the die area needed to deliver more (LP)DDR5 channels is absolutely minuscule; for a huge boost in iGPU performance.,hardware,2026-01-06 10:17:06,3
Intel,nxy0rol,Cache. Lots of it.,hardware,2026-01-06 03:31:13,24
Intel,nxxpegk,"They are using 9600mt/s lpddr5x, could also have a lot of cache, (iirc 890m configs were nerfed in cache because they wanted to put a npu instead), and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.",hardware,2026-01-06 02:27:40,15
Intel,nxxumwa,Panther Lake still has a 128 bit memory bus so only models with 9600 mt/s will get slightly faster shared memory bandwidth than Lunar Lake.   I wonder how this will manifest in games as the only performance leaks have been from Geekbench and 3DMark which may not be as bandwidth intensive as real games and applications.,hardware,2026-01-06 02:56:26,5
Intel,nxy8e9t,"This seems to be correct, since checking NotebookCheck for the 30 watt 4050 shows that it’s around 70% faster than the HX370 in games, which is roughly where Intel places their iGPU.  The performance difference between a 30 watt 4050 and full 140 watt 4050 is around 41 percent performance based on Time Spy",hardware,2026-01-06 04:16:57,17
Intel,nxy2xyi,"basically cheating tho, rtx cards in dell laptops are barely getting enough watts to even turbo",hardware,2026-01-06 03:43:51,4
Intel,nxzpm42,"That's the only way to do a fair comparison, really.   Because the 45 watts that Intel chip uses is shared for the entire chip.   So it's still 45w Intel + igpu vs 60w Intel+gpu",hardware,2026-01-06 11:45:18,1
Intel,ny2zoh2,The thing about that... what sort of tile are we expecting them to package up? As you say if we can get 4050ish performance from an Intel iGPU then they really can't be far off 5050M... and maybe even 5060M performance in future.  Do you think they'll offer something like a 5070 tile? that almost seems excessive (and difficult to actually package from a thermal point of view in a laptop) but it seems like the 5050/5060 sort of tier is going to be pretty well covered as a traditional iGPU soon.,hardware,2026-01-06 21:32:40,1
Intel,nxzk1m0,AMD actually introduced lower tier Strix Halos in this CES; and the first budget laptop thats gonna use it is [the Asus TUF A14](https://youtu.be/h27w0PXFBgk?si=Pa7UQhinywF-uFMj&t=306),hardware,2026-01-06 10:59:13,5
Intel,ny0na4n,"It’s a super strong generational gain though, it’s like the jump from Vega 8CU to rdna2 12CU. The kind of single gen gain you see once in 5 years at most",hardware,2026-01-06 15:07:11,9
Intel,nxxklk7,They're a lot more accurate than whatever the fuck Nvidia has been doing where you have to decode their bar graphs for proper scaling lmao,hardware,2026-01-06 02:01:38,46
Intel,nxzfsf4,"I'm pretty sure Intel threw lots of ""marketing money"" for the MSI Claw too. There were heaps of MSI Claw promotional booths / draws at shopping malls / public places in Australia and it was heavily discounted.  I picked one up for about $550 AUD (after rebates; tax included), which is like $369 USD inclusive of tax.",hardware,2026-01-06 10:21:31,2
Intel,ny13km9,> Lunar Lake was a expensive product which didn't make sense in handhelds   What do you mean? All the tradeoffs LNL made were pretty good fits for a handheld.,hardware,2026-01-06 16:23:22,1
Intel,ny13z0b,"> I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800   You're looking at normal DDR, not LPDDR. LPDDR5-9600 *is* a JEDEC spec, and already available in mobile.",hardware,2026-01-06 16:25:12,6
Intel,ny020mx,"The majority of the lineup still only has 4. Will be interesting to see what the pricing and performance is on those since these will likely be quite limited. What's also a bit crazy is there's three different nodes being used for the various GPUs, and the full 12 unit one is probably on N3.",hardware,2026-01-06 13:11:40,2
Intel,nxxlxrw,"I doubt it's exactly 73% outside of cherrypicked games, but it should not be shocking that it's significantly faster than rehashed rdna3.",hardware,2026-01-06 02:08:52,-1
Intel,nxxjlfd,"Every Intel marketing benchmark for like a decade or so, but especially their GPUs seem to do far better in their benchmarks than they do in reality.",hardware,2026-01-06 01:56:13,-17
Intel,nxxhxjk,"Ice lake, Alder lake, Metor Lake",hardware,2026-01-06 01:47:15,-20
Intel,ny0aepy,"Different poster than OP.  Compute tile on Lunar Lake is 140mm2 on N3E with a small 46mm2 controller N6 tile. Strix Point (HX 370) as a whole is 233 mm2 on N4P. Lunar Lake is clearly cheaper, but given the newer node and packaging not massively so, likely by around 20-25%.  Panther Lake, with the B390, is going to be significantly more expensive than Lunar Lake. The B390 GPU has 50% more CUs, and that is very likely still on N3 or some variant (Intel only labeled this as external on their deck). CPU size 4xP+12xE as opposed to Lunar's 4+4, which should still be significantly more area with the core upgrades despite being on A18.  The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.  Intel Foundry in general isn't any cheaper than TSMC. With Intel being practically the only user and development expenses it's likely more expensive than TSMC despite TMSC's margins. For all purposes it's an accounting trick to hide CCG's and DCAI's 5-15% operating margins if you divide the foundry losses per group revenue.",hardware,2026-01-06 13:59:25,1
Intel,nxxpvi7,"Yes, although the actual low power 8 core successor to Lunar Lake is the 335/365 with half GPU cores and slower RAM.",hardware,2026-01-06 02:30:19,-7
Intel,ny0b9nu,"even if you exclude the CPU CCDs the graphics die alone is bigger than a RTX 5070Ti mobile GPU, which also retails for \~$2000-$3000, same as Strix Halo laptop",hardware,2026-01-06 14:04:11,12
Intel,ny393aj,"This is slightly splitting hairs but the 8c CCDs in Strix Halo is actually NOT the same chiplet as the ones in desktop zen 5 parts. It iirc is produced on a smaller node, slimmed down, and has different ( or no) TSVs.  It is similar to design and cache sizes to desktop however, but the changes to the CCDs were done to improve low power performance characteristics. They are likely a bit more expensive than Desktop CCDs.  I believe it is discussed in a chips&cheese deep dive.",hardware,2026-01-06 22:16:26,3
Intel,nxycvko,"AMD have largely been ""winning by doing nothing"" due to their better driver support stack for gaming on iGPUs, rather than actually throwing superior hardware at it.  It's almost ironic how AMD's mobile chipsets are now the ""Intel 14nm+++++"" of this generation.  Constant minor refreshes or even straight-up re-badges of old chips.  Now that Intel Arc has been around a while now and is getting quite capable.  I suspect Intel have a real opportunity to overtake AMD this generation in the iGPU space (ie. handheld and mini-PCs), especially since the new AMD APUs are just **another** refresh with a clock boost and Strix Halo is not scaled or priced to be actually affordable by normal people in that market.  XeSS can also act as a massive force-multiplier in power-constrained scenarios like handhelds.  AMD really shot themselves in the foot by either not building or not allowing FSR4 to function on RDNA3/3.5, which all current and now next gen AMD handhelds are stuck on.  Given how effective DLSS is on the Switch2, one could only imagine how kickass a Nvidia chip in a handheld PC could be with the far more ubiquitous DLSS support.",hardware,2026-01-06 04:46:15,76
Intel,nxyz1xa,Now they are not. The panther lake igpus are undisputed winners (excluding the 395+ from amd since it's just not gonna be mainstream). You can get a 358H or 368H and you'll have solid laptop for igpu gaming far cheaper than the 395+,hardware,2026-01-06 07:43:17,13
Intel,nxz8aui,"For business apps laptops have been good enough for 10 years now, iGPU and battery life is really the only differentiator.",hardware,2026-01-06 09:10:50,4
Intel,nxxvlpa,"Intel is plenty competent for gaming, and has XeSS which is way better than FSR3.",hardware,2026-01-06 03:01:47,37
Intel,nxxx4hv,Lol? No 6 or 8 core 3dvcache laptops and no 5080 or 5090 laptops. Strix Halo is a joke for gaming as well,hardware,2026-01-06 03:10:21,-8
Intel,ny1dwcs,The 9955HX + 5070Ti is $2240 and the 275HX + 5080 is $2540.   When both 5070Ti configurations are on sale they should be the same price.,hardware,2026-01-06 17:10:18,2
Intel,nxy23yz,My mistake I was thinking of Strix Halo,hardware,2026-01-06 03:39:00,30
Intel,ny0f479,"It's about compromise. I don't *want* a 4lb laptop. I don't want a laptop that runs hot when web browsing. Or a laptop that has loud fans, or gets poor battery.  I have a desktop for gaming and other demanding tasks. For a laptop, I, and most of the market, want it focused on portability. Light weight. Cool running. Long battery. These big iGPU PTL laptops are really interesting because they provide *good enough* gaming without sacrifice to the non-gaming livability of the device.",hardware,2026-01-06 14:25:02,1
Intel,ny02zjc,"It took me way too long to convince my sister the AMD laptop I bought her isn’t going to blow up in her face and lose all her data, the Intel(and now Apple) CPU brands are very strong.",hardware,2026-01-06 13:17:30,2
Intel,ny0wejw,"> most casual laptop buyers don’t even know what AMD is  We're past that point now. Even ""normies"" have heard of AMD from news.",hardware,2026-01-06 15:50:32,1
Intel,ny006yw,"its always ""fix it next generation"" with AMD.",hardware,2026-01-06 13:00:12,18
Intel,nxyndbc,"I mean yeah it's not ideal, but you could argue it's the same with XeSS or FSR 4 on RDNA 3. Since the OP said ""there's no decent upscaling on AMD handheld"", therefore I assume Opsticaler is out of the question too.",hardware,2026-01-06 06:02:32,6
Intel,nxzib6q,"Well you said it, it's TSMC capacity, meaning also a priority issue. They prioritize AI over consumers and then increase the price by reducing availability, meaning the same chip costs more, meaning more margin.  Seeing they increase the 5090 to roughly 5k (USD) is just the beginning and as I know all companies will use the increasing memory prices to say they must increase the product price, just not proportional to the memory costs.  next step: then they will use this to move more to streaming instead of owning",hardware,2026-01-06 10:44:14,-3
Intel,ny15kho,"> Power levels are determined by the frequency of a given CPU core   There are SoC and platform level targets that depend on a lot more than just clock speed for the same cores. Consider how LNL's PMICs scale vs FIVR/DLVR. Or what operating point benefits the most from the on package memory.   Especially at really low power, the cores are not your big concern. Consider the difference at 10W between 50% of your budget available for compute and 80%.   > so given the added IPC of the new Panther lake e-cores   We're talking a couple percent. DKT is a tick.    > and better process node   Very much unproven.    If you want to give credit somewhere, pretty much all of it should go to the SoC and GPU teams.",hardware,2026-01-06 16:32:30,0
Intel,ny0idmq,"Xe2, Xe3, etc. are the ""real"", more accurate names. Battlemage, Celestial are the marketing names.  Intel's decision to label the new Xe3 iGPUs as ""Battlemage"" is certainly an interesting (odd) choice - my best guess for this decision is that next year, Xe3P discrete will launch alongside Xe3P iGPU in NVL, and they're saving the new Celestial naming for that launch event.  Xe2 -> Xe3 is the bigger change.",hardware,2026-01-06 14:42:24,6
Intel,ny1hu62,"Peterson states explicitly it's to take advantage of good Battlemage branding, around 1:30 of this video. [Intel Talks Xe3 Improvements For Gaming - YouTube](https://www.youtube.com/watch?v=Bjdd_ywfEkI)",hardware,2026-01-06 17:28:12,1
Intel,ny0xhzi,"> Xe3p was alr confirmed coming  Not for client dGPUs, which are what get the Battlemage/Celestial brand.",hardware,2026-01-06 15:55:31,2
Intel,ny0xj91,> Intel did confirm Xe3P will come to desktop  They have not.,hardware,2026-01-06 15:55:41,1
Intel,nxy8fjz,"They also might be getting better value out of the ""2x scaling"" choice for benchmarking. Notice how they are behind Nvidia in all the none scaled titles except Dota2 that I saw.  Still very good results for a iGPU, but they are not entirely honest numbers either.",hardware,2026-01-06 04:17:11,13
Intel,nxxshuz,It's 1 game out of 45 in geomean which devalues outliers. ~~9.9% faster instead of 10% faster if you take it out.~~  Edit: Oh no it's actually 6 FPS on the 4050. Yeah that's way too big for geomean to smooth out.,hardware,2026-01-06 02:44:44,22
Intel,nxy7k6w,And the fact they showed 45 games shows how confident they are in this product.  I remember the Intel slides with 5 hand picked titles we used to get just a few years ago.,hardware,2026-01-06 04:11:44,26
Intel,ny17afz,You mean in desktop? Or do you want mainstream mobile to go quad channel?,hardware,2026-01-06 16:40:20,2
Intel,nxzgpgy,I wonder how 96MB cache would do had Intel put that much on it.,hardware,2026-01-06 10:29:53,2
Intel,nxyig6s,They have 16 MB of L2 just for the GPU alone lmfao,hardware,2026-01-06 05:25:03,11
Intel,nxyiiu1,"Also there's like 45 games on display here, it's not just 3dmark",hardware,2026-01-06 05:25:35,5
Intel,ny08xk6,> and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.  They're benching 45 games dude.,hardware,2026-01-06 13:51:15,6
Intel,nxy8oam,41 percent difference in performance compared to a full 140 watt in Time Spy. Honestly a bit surprised it isn’t more performance difference.,hardware,2026-01-06 04:18:43,3
Intel,ny55i5l,"No, it's disingenous. Because everyone would think 60w 4050 = 60w on gpu alone",hardware,2026-01-07 04:22:58,1
Intel,ny384o8,"Not really sure. I believe it's Hammer Lake that's debuting the Nvidia tile, and that's rumored for a 2029 launch, so still quite a ways off, and 2 generations ahead of Blackwell.  The only rumors I'm aware of that it's going to be a pretty big iGPU",hardware,2026-01-06 22:11:51,4
Intel,nxzkp7q,Fantastic -- but at least six months too late ;),hardware,2026-01-06 11:04:52,4
Intel,nxz5md2,You don't like graphs with zero scale claiming their latest 100W GPU is somehow a gazillion percent better than a 4090 or something?,hardware,2026-01-06 08:45:06,9
Intel,nxy8v6h,Wattage limited 4050 to 30 watts is the only slide that’s suspect.  It’s around a 41 percent performance loss based on Time Spy from the 140 watt 4050.,hardware,2026-01-06 04:19:56,9
Intel,ny0l0pv,The standard 4Xe models use the extra die space they save to have more PCIe lanes. that large iGPU adds cost and doesn't make much sense to use that chip if you're gonna add an Nvidia dGPU,hardware,2026-01-06 14:55:55,3
Intel,nxxp4au,"yeah just looking at the game sample I can see a few that really don't perform well on RDNA architecture at least relative to nvidia(idk what really constitutes an ""intel favoured"" title)   Like stalker, csgo 2, civ vii, dying light the beast, and delta force ik run a lot better on nvidia relative to amd so im guessing the same holds true for intel vs amd.   A couple titles amd does well in were thrown in there too though like God of war and Cod but im guessing the real performance difference is more like 40-60 rather than the claimed 70-80.   Pretty large sample though which is nice so the numbers can't be that off.",hardware,2026-01-06 02:26:09,9
Intel,nxxtyhj,Why not?   It's 50% more cores + architectural improvements + clock  speeds,hardware,2026-01-06 02:52:42,9
Intel,nxzgxn6,"Please provide a **single** example in the past ~5 years of an Intel marketing benchmark that is materially inaccurate or untruthful.  NVIDIA is the one playing it loose with BS charts, AMD generally has a good track record (with some exceptions), and Intel on the GPU side has been pretty accurate. For example, these benchmarks have 45 games (!!) and use geomean to reduce outliers.  While I disagree with their choice of LPDDR5 9600MHz (hah, imagine a single consumer product shipping with that in this DRAM market), it is not untruthful.",hardware,2026-01-06 10:31:54,9
Intel,nxxkps6,All were pretty accurate.,hardware,2026-01-06 02:02:15,18
Intel,nxxkqio,But lunar lake igpu actually perform better than 890M.Like comparison of core ultra 7 and z2 extreme in handheld like msi claw.,hardware,2026-01-06 02:02:23,16
Intel,ny0lzbq,">The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.     The mainstream unit that's more directly comparable to LNL is the same core count (4+0+4) with a smaller iGPU tile. It'll be cheaper.  The 4+8+4 w/ 4Xe is the direct replacement to ARL-H, and that should also be cheaper than ARL-H.",hardware,2026-01-06 15:00:42,1
Intel,nxxvizf,"True, but then still, that's not a removal of CPU cores like they said it was.",hardware,2026-01-06 03:01:22,12
Intel,ny3ft4h,That is due to the Nvidia tax and AI bubble rather than the production cost of the chip. Even Apple ships cheaper silicon than that.,hardware,2026-01-06 22:48:46,2
Intel,nxyibby,"This is very topical and cyclical of Intel/AMD. Intel did really poorly for like a half a decade which was unusual but usually they go back and forth. One gets lazy and incompetent, the other curated a masterful product that becomes dominant for a while and then they get lazy and it flips around.  Intel is planning on socketing a ton of cache on their next breed of chips which will massively boost their gaming perfomance and they have pretty darn efficient chips now too.",hardware,2026-01-06 05:24:04,13
Intel,nxygca9,Thank you for the thorough explanation! Very excited for the future of miniPCs and handhelds since there's so many games I'd like to play on the go.,hardware,2026-01-06 05:09:52,2
Intel,nxz2wtv,"Yup, I am very happy to learn how wrong I was thanks to other people in this thread as well.",hardware,2026-01-06 08:19:04,6
Intel,nxzn2om,"For business apps 10 years ago yes, now even Office has bloated itself up so much it's genuinely taxing even on the Apple chips  And well, the better the chip, the more outrageous the user workload gets. I appreciate the modern laptop chip's ability to import a CSV the size of Excel's row count limit and make a pivot table out of that, but now that it *can* do that I'm *expecting* that to be possible as quickly and as efficiently as possible.",hardware,2026-01-06 11:25:00,3
Intel,nxxy7wk,I was under the impression that XeSS needed a dedicated GPU? If it can run on iGPU that's a whole different story.,hardware,2026-01-06 03:16:38,-13
Intel,nxz8pk3,"Their GPU's only look good when compared to 1 generation old bottom tier GPU's of their competitors. Its wild the praise they get.  Same thing will happen here, AMD will release a new iGPU architecture and Intel will be left comparing to out of date CPU's no one buys anymore.",hardware,2026-01-06 09:14:52,-7
Intel,nxxynsf,Sorry I should have specified that I'm talking about budget laptops with iGPUs.   I would sooner build a pc than even think about a 5080 laptop with 3dvcache options.,hardware,2026-01-06 03:19:11,17
Intel,ny01arz,"Yeah; meanwhile NVIDIA just released DLSS4.5 for **every single RTX GPU**... yes all the way back to Turing. It runs a lot better on more recent cards, but it's available on every single RTX GPU if you want to.",hardware,2026-01-06 13:07:13,14
Intel,ny0x2bs,"XeSS and FSR 4 on RDNA 3 both use downgraded versions of those upscalers, that either look worse, perform worse, or both. In the case of FSR 4, it's a leaked one-off model that people got their hands on. All I really meant by ""decent"" was having an officially supported modern upscaler without all the downsides.  An Intel GPU running XeSS would presumably get the full version of XeSS without the performance hit and with good visuals.",hardware,2026-01-06 15:53:33,2
Intel,nxzipnz,I can currently buy a brand new 5090 in Australia for $2841 USD with express postage included; I'm not sure why it's 5k USD in your region; but there's no reason you should be paying 5k USD. Which country are you in?,hardware,2026-01-06 10:47:45,5
Intel,ny4uh4q,"The ultra X9 388H has a base TDP of 25W and minimal assured power draw of 15W. Meanwhile the ultra 7 155U has base TDP of 15W and minimal assured power draw of 12W. Both these numbers are lower for the meteor lake chip, yet the Panther lake chip is waaay more efficient (+2x). The base power level doesn't mean anything. It might be the point where the chip had the most perf/watt, but that doesn't mean that the performance at lower wattages is the same.",hardware,2026-01-07 03:17:13,0
Intel,ny0xx01,"Battlemage, Celestial, etc are named they (usually) use only for the dGPUs, even if that does correlate with the B/C-series naming. I think at some point this is just reading the tea leaves. The name's misleading for the tech difference.",hardware,2026-01-06 15:57:25,3
Intel,nxyibw6,XeSS FG has lower overhead than Nvidia IIRC,hardware,2026-01-06 05:24:11,2
Intel,nxyaxye,If you actually do the maths it'd go down to (1.1^(45)/9)^(1/44) = 1.049 = 4.9% faster,hardware,2026-01-06 04:33:24,9
Intel,nxysgiw,Mobile rtx 4050-70 maxes out at roughly 90-100 watts due to voltage limitation,hardware,2026-01-06 06:44:37,7
Intel,ny3bk9e,"Oh wow that's a lot later than I expected, I was thinking this year or next.  Yeah no clue in that case.",hardware,2026-01-06 22:28:12,3
Intel,nxzfk6p,Infinity percent better at a feature the older GPU used for comparison does not support!,hardware,2026-01-06 10:19:28,1
Intel,ny0kqkf,"I don't really think that's ""suspect"". They said they're limiting the total laptop power on the 4050 to match the total laptop power of the PTL chip. If you want stronger performance out of a 4050, you're gonna need to have much higher power draw than the PTL laptop",hardware,2026-01-06 14:54:29,2
Intel,nxyj1tk,"CSGO is known for running like utter shit on Intel Arc, you can check r/IntelArc for details LOL. The game selection looks pretty reasonable to me.",hardware,2026-01-06 05:29:27,7
Intel,nxy8v14,"Yeah all depends on pricing, 6 core ultra 5 model is however technically downgrade from last generation and the same core config as the i3 1315U.",hardware,2026-01-06 04:19:54,-2
Intel,nxyk9c8,Honestly not that unusual. It takes an average of around 4-5 years to develop a processing unit from the ground up. If we assume each one does this when they get mushroom stamped by the other for being lazy it accounts for the 5 years gaps till they show back up with something to sell.,hardware,2026-01-06 05:38:33,20
Intel,ny0dmdk,"I think AMD is getting a bit lazy when it comes to consumer graphics. I think their attempts at laptop have been really half-assed given just how good their IP portfolio is.  But when it comes to their core businesses, they're definitely been keeping the heat on and have been quite aggressive. They're datacenter first and foremost, and that trickles down to amazing desktop CPUs too. They're heavily focused on building out their Mi series too...but they're just dropping the ball in laptop and consumer GPU",hardware,2026-01-06 14:17:03,5
Intel,ny39k1v,Pantherlake also has an oddity in that it has MUCH higher L2 cache than even desktop zen 5 parts. I'm curious to see its CPU performance in low resolution scenarios.,hardware,2026-01-06 22:18:39,1
Intel,nxxyz2v,"The good version of XeSS runs on any chip with XMX units (Intel's version of tensor cores). Lunar Lake, Arrow Lake mobile, and now Panther Lake have GPU tiles with XMX units, so they get the same XeSS as discrete Arc cards.",hardware,2026-01-06 03:20:59,28
Intel,nxyty97,"Dedicated hardware, not dedicated GPU. The new Intel CPUs have iGPUs with the necessary hardware.",hardware,2026-01-06 06:57:17,5
Intel,nxy622y,"It needs dedicated GPU hardware to run faster, but they’ve started incorporating it on Lunar Lake and Panther Lake",hardware,2026-01-06 04:02:21,4
Intel,ny0e8rw,Intel has been very aggressive in the iGPU space. AMD isn't going to have any real updates to their iGPUs until 2027 the earliest.,hardware,2026-01-06 14:20:20,8
Intel,ny2bpjc,"> Same thing will happen here, AMD will release a new iGPU architecture   ... Based on what history? AMD's iGPU has not significantly changed in years. It's still hugely memory bottlenecked and no matter how many times they add an extra 2 CU's, it will still be memory bottlenecked.  IIRC someone disabled 2 CU's on their 7000 series APU and their in-game FPS almost didn't change because the bottleneck was actually memory access.  Intel ARC is actually very good on this metric. Intel doesn't exactly need to sling anything better than ""slightly more Battlemage on a better transistor"" to completely swamp out AMD iGPU in this space.",hardware,2026-01-06 19:42:26,1
Intel,ny2y1c0,"Intel Panther lake base tdp is 25w, around the same as AMD Strix Point/ Gorgon Point. Why will they compare it to a 55w tdp Strix Halo?",hardware,2026-01-06 21:25:10,1
Intel,nxy5y1b,"Even on the budget laptops category the new Ryzen 7s suck compared to the Intel Lunar Lake options, they seem to be priced closer with Lunar Lake getting stuff like nice displays. In the really budget category I feel like they are tied on value and I don't know how sales affect that. This is partially cause AMD went cheap on the mid-range kraken point chips and also had to fit in the still dead weight 40 tops NPU for Microsoft. So it only has 8 GPU cores.",hardware,2026-01-06 04:01:39,11
Intel,nxzj5en,"First custom design OEM are fast, here 4400€ on Amazon https://amzn.eu/d/idxVW9M  And you know how this goes, one starts the other follow.  Here in the US for a normal founders edition for 4.2k USD + TAX… one article from the first of January quoted ot that time being at 3.7, like 5 days ago.  https://www.newegg.com/nvidia-founder-edition-900-1g144-2530-000-geforce-rtx-5090-32gb-graphics-card-double-fans/p/1FT-0004-008V4?source=f",hardware,2026-01-06 10:51:32,-1
Intel,ny0ypvn,">The name's misleading for the tech difference.  Yeah, that's my point. People are reading too much into the ""B series"" naming scheme for B390.  As you said, ""(usually) use only for the dGPUs"". So if Xe3P is launching as a discrete Celestial Card, then it would make sense to have Xe3P tile be part of the ""Celestial"" launch, rather than Celestial Discrete being ""one year later than Celestial integrated""",hardware,2026-01-06 16:01:03,3
Intel,nxyf825,Oh dang you're right lmao.  The 4050 has SIX (6) FPS at 540p high. I thought OP was exaggerating with 800%.,hardware,2026-01-06 05:02:05,4
Intel,nxyahr2,Yeah those kinda suck. Should be Ultra 3s given they're basically WCL spec.,hardware,2026-01-06 04:30:27,2
Intel,ny2mxg2,"More like 10 years, 5 to realized that they are getting stomped in the face, and another 5 to actually make something of it.",hardware,2026-01-06 20:34:21,1
Intel,nxyak62,"Man, there are so many older and less demanding titles I'd love to play through on the go, but knowing that Lunar Lake laptops have better displays for the price is really good. Thanks for the info!",hardware,2026-01-06 04:30:54,4
Intel,nxzkmr7,"That's a marketplace listing, it's basically eBay, because Newegg is out of 5090FEs directly.  You can get it on the overpriced StockX for far cheaper: https://stockx.com/nvidia-geforce-rtx-5090-32gb-graphics-card-900-1g144-2530-000",hardware,2026-01-06 11:04:15,6
Intel,ny12fxe,"That would make some sense if they *did* plan a Celestial launch, but that's a big ""if"" and is just creating confusion for now. And it'll be even worse when NVL mixes Xe3 and Xe3p.    You also have Intel marketing actively making the situation worse like that Peterson interview people keep quoting to justify this nonsense. As if Xe3p isn't much more incremental than Xe3.    It's a particular shame when the product itself is actually good.",hardware,2026-01-06 16:18:11,2
Intel,nxyb20u,"Yeah, the first 6 core i/u5 series since 11th gen. :/",hardware,2026-01-06 04:34:08,-1
Intel,nxyoap3,"Yeah idk why but they typically got OLEDs exclusively, though could be a US market thing. I would also note I was mostly looking at decently built midrange to high-end laptops. I think AMD is more common in the plastic crap box design and may be a better value there, but those also typically seem to have a ton of older rebadged processors instead of the newer Kraken Point unless something changed.",hardware,2026-01-06 06:10:02,2
Intel,ntyjuf7,Why does this need an article? It's a tweet by an official account praising their own product.,hardware,2025-12-14 10:28:41,116
Intel,ntynem1,"The B580 has 200W TDP, in a perfect world and TDP scales linearly, the B770 would be 50% faster, that would put it around the 5060Ti/9060XT.  If the price also scales linearly, that would be around 375€, seeing that the 9060XT is going for 350€ now, it's gonna be tough competition.",hardware,2025-12-14 11:02:45,43
Intel,ntypvez,Im really looking forward to panther lake X. 4-4-4 core configuration and Xe3 iGPU with sr-iov is perfect for running a Linux-Windows mixed vm environment without having to get a gaming laptop with a dedicated GPU for virtualisation.,hardware,2025-12-14 11:26:24,7
Intel,ntyixk6,I hope the Linux driver support and performance is good in these,hardware,2025-12-14 10:19:48,17
Intel,nu8l0vn,"Intel ARC needs to maintain their momentum. They have an excellent pricing strategy and genuinely compelling features, it's time they released a card that competes in the midrange. And no, I don't count the A770. As a B580 owner, increased ARC adoption rates will be sure to benefit all cards in the range, so I really hope that intel is committed for the long-haul here. They are not in the position to be burning consumers anymore",hardware,2025-12-15 22:51:09,3
Intel,nu1xwz6,"Releasing a GPU more than 1 year after the B580 came out seems weird to me. Unless this is a new architecture, or is using Intel's own process, and fabs.",hardware,2025-12-14 22:08:28,5
Intel,ntyjaqa,"4070 performance for $350-400, I'm calling it now.",hardware,2025-12-14 10:23:22,10
Intel,ntyxo3a,Hopefully they've seen Nvidia and AMD fuck things up by having two VRAM configurations and know not to do that.,hardware,2025-12-14 12:35:17,7
Intel,ntyfbh6,"Hello Revolutionary_Pain56! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-14 09:44:11,1
Intel,nu28x8z,They wouldn’t need a B770 or mystery GPU if they actually released more than just a B50 to the masses.,hardware,2025-12-14 23:06:54,1
Intel,ntzwyfc,"I don't know what the driver situation is like a year later, but B580 was anywhere between a 4060ti and a 3060 (or less if the driver really choked), so comparing B770 to a single Nvidia point of reference probably isn't the whole story.  Intel has been selling a big chip with a lot of hardware relative to what they charge, so when the drivers work Battlemage can punch way above its price class. I expect the same this time.",hardware,2025-12-14 16:10:14,1
Intel,nu3z9nt,"It's been deleted, so it might even be inaccurate.",hardware,2025-12-15 05:31:41,9
Intel,nu1v2ky,Ad revenue.,hardware,2025-12-14 21:53:55,8
Intel,ntzk2x5,Trying to apply logic or rules to the internet is a waste of time.,hardware,2025-12-14 15:02:28,18
Intel,ntyxt68,"Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds. That however will mean a bigger die and viability might be questionable (considering they're already massive for the performance).",hardware,2025-12-14 12:36:26,26
Intel,ntzjh8i,"Price doesn't scale linearly because die sizes make defects scale quadratically. so pricing is the same, 2 50mm\^2 dies are cheaper than 1 100mm\^2 die     However in GPUs there is a fixed cost for every GPU so there is a sweet spot",hardware,2025-12-14 14:59:01,14
Intel,ntz8bta,"As always, TDP is a semi-arbitrary figure and has little to do with what the GPU requires.  Most GPU's of today have heavily inflated TDP's simply to try and juice benchmarks on review day as much as possible.",hardware,2025-12-14 13:51:23,7
Intel,nu40836,"The BMG-G31 is supposed to have 32 Xe cores in 8 render slices on a 256-bit memory bus, compared to the 20 Xe cores and 5 render slices on a 192-bit memory bus for the BMG-G21. Unless Battlemage is seriously memory bandwidth-limited, it should be almost 50% more performant.  The only question is die size. If it's 50% larger than the 270 mm^2 BMG-G21, that would exceed 400 mm^2. The GB203 in the RTX 5080 is 378 mm^2 for context.",hardware,2025-12-15 05:39:14,2
Intel,nu49a7b,"That ""perfect world"" of yours seems to violate basic physics though...",hardware,2025-12-15 06:56:09,1
Intel,ntywdzf,With tdp of 300 w it better be RTx 5070 or 9070 territory for much low price,hardware,2025-12-14 12:24:39,-1
Intel,nvdrawd,Intel never confirmed SR-IOV on Panther Lake - did they?,hardware,2025-12-22 15:23:42,1
Intel,nu14qjw,"You can choose between high performance and crashes (xe) or low performance and stable (i915), and with Intel firing linux devs left and right I wouldn't expect much improvement any time soon.",hardware,2025-12-14 19:45:02,9
Intel,ntz5xy4,That would be an amazing value proposition.,hardware,2025-12-14 13:35:49,5
Intel,ntypyrq,Rtx 5070 16gb for 380$,hardware,2025-12-14 11:27:18,0
Intel,nu2970r,I’d be happy if they didn’t gate the Arc Pro B60 behind bad distributors.,hardware,2025-12-14 23:08:26,3
Intel,ntz0y7e,"So banking on the hope, that *everyone* ***else*** *somehow falls behind by accident*, only for Intel to succeed?  If that's their business-plan (looking at their foundry-woes, it seems it is), that's an awfully idiotic business-model.  ---- Last thing I heard, was redditors moaning about en masse that monopolies are bad. *Which one is it?!*",hardware,2025-12-14 13:00:48,-23
Intel,nu8khh7,"The B50 is not a gaming GPU and actually underperforms in gaming tasks compared the the B580. They need to have an actual range of cards, not just a budget option, and even more budget option, and a server/workstation GPU. The B770 is essential to compete in the midrange",hardware,2025-12-15 22:48:12,1
Intel,nu8kn1x,"A year later the drivers are fantastic, seriously not even a single hiccup. Been playing Hogwarts legacy at 4k 60fps with Xess Quality upscaling, and no frame gen.",hardware,2025-12-15 22:49:02,1
Intel,ntyyf0i,"> Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds.   The problem with this idea, is that this would cost them far more money, as you need more die space, which they already use relatively inefficiently compared to nVidia.  They can't really afford not to use every bit of die space they have for all that its worth.",hardware,2025-12-14 12:41:19,21
Intel,nu40pti,"Battlemage doesn't have the ability to add more Xe cores per render slice, this is something Intel has changed for Xe3. The BMG-G31 will have 128 ROPs, the same as an RX 9070 XT, or more than an RTX 5080.",hardware,2025-12-15 05:43:07,2
Intel,nu49ukp,"FWIW that is not how pricing structures work. It's a bit more complex than that.   E.g. defect rates scale with die size, that is true. But larger dies also have more budget for DFM structures, that can lead to fewer overall functional faults than smaller dies and more variability and binning opportunities. So although smaller dies tend to be on average cheaper, it is not necessarily that 2 dies half the size will be cheaper than the twice as large single die.",hardware,2025-12-15 07:01:13,1
Intel,nvdwqwo,"afaik it works on every iGPU since skylake, but the driver is not in the mainline kernel",hardware,2025-12-22 15:51:22,1
Intel,nu2hk6w,I'm using an Arc A770 right now in Linux.  With i915 performance was unusably (for me) low.  With xe it's been fine.,hardware,2025-12-14 23:54:28,6
Intel,nu2tqb3,"the driver is already open source right? i think it will get better over time on virtue of being open source, but relying on intel to fix it now probably isnt gonna pan out.",hardware,2025-12-15 01:01:50,0
Intel,ntz7z9x,"Well it kinda has to be, the 4070 came out nearly three years ago.",hardware,2025-12-14 13:49:08,23
Intel,nu096mb,5060 performance for twice the price isn't a good deal.,hardware,2025-12-14 17:11:40,-2
Intel,ntz10sx,"I could see that. Nvidia really bailed out Intel by making the 5070 not much faster than the 4070 without using MFG to cheat lol   Edit: for all the Nvidiots downvoting, [the truth hurts](https://www.techspot.com/review/2960-nvidia-geforce-rtx-5070/#RT-1440p-png)",hardware,2025-12-14 13:01:20,5
Intel,ntz7opy,This seems an absurd overreaction. All I'm saying is they don't do a 5060ti or 9060xt situation where there's a 8 gig model and a 16 gig model.,hardware,2025-12-14 13:47:16,18
Intel,nubvjrs,"It’s not but the B50 is the only Arc Pro that isn’t gated behind a bad vendor like Hydratech.    If they can’t properly launch the B60, why should I trust Intel or it’s partners with the B770 or some mystery GPU?",hardware,2025-12-16 13:27:34,1
Intel,nu1491m,5060 is not nearly as performant as the 4070,hardware,2025-12-14 19:42:34,18
Intel,nt8w4et,"Interesting results. If this is representative for consumer laptops, Panther Lake is a much bigger upgrade than most here, including me, expected. But it almost seems too good to be true somehow.",hardware,2025-12-10 06:01:38,25
Intel,nt7pdwj,is Geekbench a CPU or a GPU benchmark?,hardware,2025-12-10 01:17:56,13
Intel,nt7pbu4,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-10 01:17:35,1
Intel,ntafnry,How does this compare to the Snapdragon X2 Elite?,hardware,2025-12-10 13:56:04,1
Intel,nuahcx5,4 pcores  8ecores 4 lpcores..,hardware,2025-12-16 06:05:52,1
Intel,ntcj3az,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:16:10,1
Intel,ntbju2x,"Is Intel just ""squeezing the toothpaste"" again ? Even a low-frequency single-core 288V gets 2,700+ on Geekbench, while the 285H gets 2,600+ in single-core and 14,785 on multi-core. Therefore, TL;DR: I don't see Panther Lake being a huge improvement over the current Alder/Arrow Lake pairing. We will have to wait and see the power consumption, though.",hardware,2025-12-10 17:23:27,1
Intel,nte6d5x,"I’m sorry, but that’s awful? Only 9% better single core when it has a better node and a newer architecture? Compared to what Apple and Qualcomm achieve every year, that’s pathetic",hardware,2025-12-11 01:36:38,0
Intel,nt8wwpg,"Probably because GeekBench 6 only scales to a certain point, where more cores won’t help with improving performance compared to improving core IPC",hardware,2025-12-10 06:08:21,6
Intel,nt9ghvf,"Fr, I really need to get a new light laptop (bc my old one's hinge is broken), but starting to feel  like I'd be better off waiting for Panther Lake than compromising with a bulky gaming laptop....",hardware,2025-12-10 09:13:58,5
Intel,ntcj6hz,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:16:37,-2
Intel,nt7pvq6,cpu,hardware,2025-12-10 01:20:55,15
Intel,nt7t1it,"Probably one of the worst benchmarks out there for multicore tbh, I’d be more curious about the cb r24 scores",hardware,2025-12-10 01:40:02,15
Intel,ntdv8jz,"Panther Lake doesn't bring any major changes to the cores. It's mainly about bringing node shrink, redesigned SoC, and new iGPU.",hardware,2025-12-11 00:29:31,1
Intel,ntcpemm,"It’s obvious that this is the viewpoint of an outsider. Professionals would never look at it this way. Professionals first evaluate a processor based on its specifications, features, and process technology. Lunar Lake and Arrow Lake are *not* using some outdated process — they use TSMC’s then-most-advanced N3B node, Intel’s first time adopting it. Meanwhile, Panther Lake uses Intel’s own **18A** process.  Based on the current benchmark results, Intel’s 18A appears to outperform TSMC’s N3B by at least the same margin that **Intel 4** trailed behind N3B — which is an astonishing result.  Every day you hear people saying how much TSMC has advanced, how far ahead its processes are, how “outdated” Intel’s nodes are, how AMD’s processors using TSMC have excellent efficiency. These kinds of statements have been repeated endlessly over the past decade.  Yet today, Intel is using its newest process node to **clearly surpass** TSMC’s top process from just one year ago.",hardware,2025-12-10 20:47:39,-1
Intel,ntep84w,"the real test will be how many watts the X9 388H needs to achieve its scores, because the 285HX needed like 90 watts to achieve its scores  so if the X9 could hit its scores while on its base TDP (65 watts) then thats a \~40% increase in efficiency, not bad",hardware,2025-12-11 03:32:35,4
Intel,ntfpawi,"But it does that while clocked almost 6% lower, so the IPC gain is actually decent. Especially considering most people expected Panther Lake to be a side grade because of the small architectural changes on the cores.",hardware,2025-12-11 08:27:12,2
Intel,nthte2d,"For the use cases of PTL, Geekbench (which is mostly consumer focused) is a good indicator.  It doesn't assume that its workloads are perfectly parallel, it assumes some threads are used more heavily than others, so its value in nT is influenced by its 1T.    If someone is using this for rendering or other highly parallelizable workloads they might want to look into a subtest or into an alternative benchmark, but for typical consumers it seems like Geekbench is a good approximation of their experience.",hardware,2025-12-11 17:03:33,2
Intel,ntckb4u,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:22:19,0
Intel,ntgubsm,"Is macOS not an option? Because IMO, MacBook Air is *the* thin and light laptop to get, hands down.",hardware,2025-12-11 14:03:57,1
Intel,ntfhexr,"Couple of subtests leverage some new arm vector instructions and get huge scores, but those have limited influence to the overall score. Apple is better across the board, though the difference isn’t as big as the overall score suggests.   One difference is that since geekbench is distributed as binary it’s compiled more directly for apple architectures specifically while others use more generic targets. But that has very limited effect.",hardware,2025-12-11 07:10:24,2
Intel,nt7rc6b,But they have a gpu compute test too,hardware,2025-12-10 01:29:45,13
Intel,nt8452s,"Geekbench claims it's much more realistic than those multicore tests that scale nearly perfectly with tons of cores, and I think that's a fair take. It's not as if they didn't know how to create a benchmark that scales like other nT tests do, geekbench 5 nT does that.   I wouldn't call it worse, just different.",hardware,2025-12-10 02:45:40,26
Intel,ntfm9jb,Geekbench runs common workloads as they are commonly implemented. It gives you a score on how well a multi core implementation of that workload would actually run in that CPU.   I think that is far more useful than some perfectly parallel workload measuring max power and core count.,hardware,2025-12-11 07:57:01,5
Intel,ntco5ms,"I have carefully compared the various models across Geekbench, PassMark, and the differences between Meteor Lake, Arrow Lake, and Lunar Lake. If my judgment is correct, the theoretical peak performance of the 484 in Cinebench R23 should reach around **24,500**; the 285H scores **22,500**. Compared with the 285H, it should be easier for the 484 to achieve high scores because its power requirements are significantly lower than the previous generation built on TSMC N3B.  Its peak performance will not be extremely strong because the frequency is not high. IPC is likely improved by around **10–11%**, but clock speeds drop by about **6%**. Overall, that means single-core performance should only rise by **4–5%**.  The improvement will be most noticeable in Geekbench. Since PassMark single-core also shows gains, the IPC uplift and resulting single-core increase should be quite certain. If Geekbench were the only source, it would still be questionable, but PassMark is more solid and has higher reference value.  Overall, in terms of peak performance, the uplift is average—around **10%**, close to that figure.  However, the real key is the **efficiency gains**. I believe they will be excellent. Compared with the 285H, which requires **65 W** to reach **20,000** points in Cinebench R23, I estimate that the **388H** may only need **40–45 W**.  I also estimate that the Cinebench R24 score should fall around **1300–1400**. Compared with Qualcomm’s X Elite 2 at **1950**, there is still a significant gap—but the two products differ drastically in scale.  Overall, Panther Lake’s greatest achievements lie in several aspects:  1. **Energy efficiency** — likely the best among all x86 products. 2. **Performance per mm²** — excellent. For example, the 484: if you look at its die shot, the total area of the CPU (including the CPU tile’s 4P and 4 LPE cores and all caches) is essentially equal to the die area of a traditional monolithic 8-core design. That means the 484 uses the same silicon resources as past 8-core chips, yet **no AMD mobile 8-core processor surpasses it**, either in raw performance or efficiency. 3. It also offers better performance-per-area than Qualcomm’s processors. The X Elite 2 has **18 cores**, including **12 “very large” cores**—similar in size to Intel P-cores—and **6 large cores**, each larger than Intel’s E-cores. The die area of this chip is **2.5× larger** than Panther Lake 484’s.",hardware,2025-12-10 20:41:27,-2
Intel,nthu15b,"I mean, it does bring SOME changes to the cores, both are a next generation, it just isn't a more radical change like will be happening with NVL.  A mid single digit improvement is still pretty decent.",hardware,2025-12-11 17:06:47,2
Intel,ntd3lso,">The benchmark is very friendly to ARM and least favorable to AMD.   How so?   >The only valid reference is **same-generation, same-architecture comparisons**,  Geekbench is nice because it explicitly allows cross ISA comparisons. You don't have to take my word on it either, Intel and AMD themselves have used geekbench before to compare themselves to the ARM competition.   Same thing applies to spec and cinebench 2024.",hardware,2025-12-10 21:57:00,11
Intel,ntsl9xc,My old 14900hx gets 35k multi core in cinebench r23,hardware,2025-12-13 10:15:38,1
Intel,ny53e0z,better go for cb2024 cuz r23 is being less relevant these days,hardware,2026-01-07 04:09:43,1
Intel,ntfgs1k,"What's with the Cinebench fascination? At any rate. Geekbench 6 runs a raytracing test, and the 388H leak shows it at 29700 points compared to a 285H scoring 25300 points. That would place the Cinebench R23 scores at about 20% higher for the 388H  https://browser.geekbench.com/v6/cpu/15500755  https://browser.geekbench.com/v6/cpu/15474224  At any rate, the reason Geekbench doesn't scale perfectly with more threads is because a lot of workloads hit scaling limits due to Amdahl's Law, or memory bandwidth limitations. This applies to SPECint and SPECfp results for multiple threads as well.",hardware,2025-12-11 07:04:32,1
Intel,ntgxa5r,"lemme know once UTAU and Fighter Maker 2002 works on Arm macOS    (my point is that I work with a lot of old abandonware apps that barely even run on x86, so there's no chance in hell they gonna work on macOS)",hardware,2025-12-11 14:20:52,1
Intel,nt9ouc7,"Thus useless to compare high CPU core counts.  If you actually need more than 8 cores you also have workloads that scale much better than Geekbench 6. It's especially dumb to claim this CPU is close to a 16 core, 32 thread zen 5 cpu based on Geekbench...",hardware,2025-12-10 10:37:37,2
Intel,nvdnrp5,I have workloads that scale fine with 16 threads and would scale fine with 32. People who actually buy high end multicore CPUs have a use for them.,hardware,2025-12-22 15:05:12,1
Intel,nt8fz34,It runs for far too short a time to reflect accurate multi-core performance.  People don't get a multitude of cores to run a task for a few seconds.  They do it for tasks that take minutes or hours to complete.  I'd argue it spends too little time on single-core tests as well.  I don't trust it to provide any useful information about anything other than transient performance.,hardware,2025-12-10 04:00:09,-2
Intel,nt8esdi,I agree but the problem is it's being mindlessly used to compare MT scores as in this article.,hardware,2025-12-10 03:52:12,1
Intel,ntd4133,Bro there's no need to spam this same comment like 4x in the same post's comment section T-T,hardware,2025-12-10 21:59:01,7
Intel,ntduxlv,"Why's the scaling ""problematic""? Its nT scaling is by design because GB6 is trying to replicate common consumer workloads which are rarely embarrassingly parallel. If you wanna see how well nT scaling for rendering is, there's cinebench for that.",hardware,2025-12-11 00:27:43,5
Intel,ntwk3p9,"was your 35,000 score achieved with power consumption above 100W? Can you try it now at 80W and see how many points are left? Also, limit it to 40W and check if it can reach 20,000 points. Because I estimate that the 388H has a chance to hit 20,000 points at 40W.",hardware,2025-12-14 00:54:40,2
Intel,ntmv22s,"Ok, a simple no would have been fine.   Seems like those extremely old apps would run on any old POS x86 machine, if anything harder to run on modern hardware but hey what do I know. Best of luck.",hardware,2025-12-12 12:29:41,1
Intel,nt8x38v,"I mean, shortness is more of a problem if a device can cool itself properly or not rather than a problem of the CPU itself, unless said CPU in question is impossible to cool in that form factor",hardware,2025-12-10 06:09:56,15
Intel,nt9ufxf,Geekbench correlates with SPEC really well while taking a fraction of the time to run. Making it run for more minutes changes nothing,hardware,2025-12-10 11:28:32,2
Intel,nt8wp7g,"It provides useful information about the chip itself to real computer architecture enjoyers. Idk if gb6 changed it but geekbench has historically correlated with spec scores. Longer running programs like cinebench test the whole system including the thermal solution but geekbench gives a much better view into the pure performance of the cpu itself (and the associated memory system :/). Besides, you can always slap on a bigger cooler if thermals are that limiting.",hardware,2025-12-10 06:06:35,9
Intel,nt8mbai,"It's being used for comparison because that's what we have. AFAIK, this is the *only* 388H benchmark we have",hardware,2025-12-10 04:44:37,10
Intel,ntgnaao,That looks like an AI post to me,hardware,2025-12-11 13:22:24,2
Intel,nt9re78,It has been going hayway since SME just like GB5 had issues with AES Skewing results,hardware,2025-12-10 11:01:14,1
Intel,nt92449,Fair enough but I'd rather they kept something similar to GB5 multicore test in addition to their new 'more realistic' one.,hardware,2025-12-10 06:54:12,-1
Intel,nvfvk61,Incredible hardware news. Thanks for the share.,hardware,2025-12-22 21:50:44,18
Intel,nvl1r29,"This was sarcasm, by the way. A video from Usagi Electric on how computers count isn't hardware related but this is? OK mods.",hardware,2025-12-23 18:20:13,12
Intel,nu6hk69,"Frankly, I wouldn't buy one for gaming, though I must admit Battlemage is pretty sweet for video editors thanks to 10-bit AV1 and 4:4:4 chroma on the HEVC side + you also get two codec engines (at least on the B580 with the same G21 core).  For perspective, you'll have to move up to Nvidia GB203 (RTX5070Ti), or better, to get your hands on two or more NVENC engines for the same 10-bit AV1 + 4:4:4 H.265.  If I was a serious video editor, this is *the* graphics card I would get.",hardware,2025-12-15 16:37:33,46
Intel,nu6rlfl,The main bit that intrigues me about these ARC GPUs is their Linux gaming performance & how they compare to their windows performance.,hardware,2025-12-15 17:26:31,13
Intel,nu7ksue,Not in the same system the 1080 ti was in.,hardware,2025-12-15 19:47:27,7
Intel,nue205x,"One interesting data point is he's testing with 7500f. We have no comparison with contemporaries or higher end CPU to examine CPU bottleneck, but it's a realistic scenario and system for the card.  Interesting how that 1gig made all the difference in TLOU2",hardware,2025-12-16 20:01:26,3
Intel,nu6oprg,"idk why, intel gpu is so expensive in my country like bruh that gpu perform worse than cheaper nvidia/amd. those sucker trying to scam buyer just cause ""intel"" name in it.",hardware,2025-12-15 17:12:19,0
Intel,nu6jvho,Yeah intel's quick sync is very good at video editing and streaming as well. Even preferred over nvenc in streaming (no idea about video editing),hardware,2025-12-15 16:48:43,13
Intel,nuacl0f,"I would and I did (Intel B50 gpu).  So far, zero regrets and zero issues on linux.  Edit: Fedora for those that are curious.",hardware,2025-12-16 05:26:38,16
Intel,nugufgj,Rumour has it Linus torvalds uses an Intel card because he wanted something on a budget that could drive dual 6k screens.,hardware,2025-12-17 05:53:53,2
Intel,nu8m2qq,"For desktop use Intel on Linux is great, but gaming performance and compatibility is horrifically bad.",hardware,2025-12-15 22:57:03,2
Intel,nwpbxq3,Iceberg's intention always been to show realistic performance and average user would get with prices imaginable lol. Otherwise for highest possible performance people would prefer gn or hub.,hardware,2025-12-30 09:19:38,1
Intel,nu75g90,"it's the retailers, they don't sell well and need higher margins",hardware,2025-12-15 18:32:51,12
Intel,nwpc21t,"Yeah retailer margin and taxes made arc GPUs very less desirable, in my country b580 is close to rtx 5060 in price",hardware,2025-12-30 09:20:46,2
Intel,nuitvat,Its not a rumor lol he did a video with Linus tech tips and specifically requested they put a b580 in the PC they built him  [link to video](https://youtu.be/mfv0V1SxbNA?si=jT_3dFy1H40vrVjk),hardware,2025-12-17 15:13:42,7
Intel,nuc491o,"Performance is a little worse than on windows, but compatibility is not horrifically bad. It's pretty much the same as on windows.",hardware,2025-12-16 14:18:10,8
Intel,nuj4t5k,"I believe Linus Torvalds wanted an ARC Pro B50, but settled for the B580 because that's what LMG could get their hands on",hardware,2025-12-17 16:07:30,4
Intel,nuzrua9,can you imagine a bunch of nerds whispering about which graphics card an old man uses?,hardware,2025-12-20 06:16:26,1
Intel,nuf0jlu,"I'm curious as to how much worse. I've considered an upgrade to a B570 due to them being seen for £150 new, putting it into used RX 6600/6600 XT territory, but if the Linux performance of say a B580 on Linux falls closer to either or, then it's probably not a worthwhile choice over the used AMD options for me.",hardware,2025-12-16 22:58:15,3
Intel,nskpbhm,"TLDW:    GPU Models Tested: MSI Shadow 2X RTX 5050, Intel Arc B580 FE      16 games average:    1080P, High-Ultra Settings:     Native TAA: Arc B580 is 14% faster, 23% faster at 1% lows due to higher VRAM        DLSS 4 Quality vs XeSS Ultra Quality: Arc B580 is ~11% faster     DLSS 4 Quality XeSS Quality: Arc B580 is ~20% faster     DLSS 4 Balanced XeSS Balanced: Arc B580 is ~15% faster     DLSS 4 Performance vs XeSS Performance: Arc B580 is ~14% faster",hardware,2025-12-06 10:33:27,74
Intel,nskzrhs,"""There was a time, about a decade ago when the $250 price tag offered solid products, but the world has changed""  Yep, inflation. $250 in 2015 money is $342 in todays money. And you can get a very solid product at that price tier, the RX 9060 XT is $369 on Newegg.  GPU prices haven't gone up, you money is just worth way less.",hardware,2025-12-06 12:12:49,135
Intel,nsl4bqs,"5050 really has no right to exist at the price it does. B580 is obviously being sold at near cost or even a loss however, it's not exactly a fair comparison but that doesn't matter to consumers.  If you just want to game then I can't see any reason to consider anything else at this price point.",hardware,2025-12-06 12:50:37,46
Intel,nspebmq,I'd still probably go nvidia here as I don't trust intel's compatibility with older titles and the like.   Still it would probably be better to spend $20 more on a 9060 xt 8 gb or $50 more on a 5060 than either of these.,hardware,2025-12-07 03:40:56,8
Intel,nsm5bxh,"If UE5 games generally run this poor on Intel GPUs, there might be trouble ahead as there are lots of those games in the pipeline.  You still couldn't get ~~more~~ me to buy an Intel GPU, even if I was desperate for a cheap GPU right now. I'd just adjust my settings.",hardware,2025-12-06 16:32:46,9
Intel,nszc9eq,"The B580 is decent enough, but it might be better to just save a bit more and get a 16GB 9060 XT for $350 or something. That card is likely to last 10 years flat at this point, and it will definitely last at least 5.  And yes the 5050 is not good. Getting something with a half-decent iGPU would be a better use of your money at that point.",hardware,2025-12-08 18:55:08,1
Intel,nt0zpd3,The biggest issue is that he did not test PCIE 3.0 vs 4.0 vs 5.0. Those GPUs are very likely to go into budget builds or as upgrades to older motherboards like the B450.,hardware,2025-12-09 00:04:12,1
Intel,nsnsg7n,Imagine spending $250 on a GPU when you could literally just save $100 more for like a 100% percent more performance.,hardware,2025-12-06 21:49:06,-6
Intel,nskuu1l,Wait isn't xess a lower resolution per quality setting?,hardware,2025-12-06 11:27:34,34
Intel,nsuy6fa,"yeah there is a reason why there are 5050s for 210  the thing is a sub 200 dollar GPU, which matches it capability and vram well, its more or less the I want to step up from igpu deal",hardware,2025-12-08 00:59:57,5
Intel,nslxyj3,Maybe for the low end but high end I can’t even buy a card at msrp outside of America.,hardware,2025-12-06 15:53:05,32
Intel,nspes5s,"*ignores that this is a 50 tier product and should be compared with the 950 and 1050*  This kinda of ""but but inflation"" virtue signalling I'd very unhelpful to these kinds of discussions. It's as if you're saying people should stop complaining gpus are several times more expensive than they used to be with the actual low end market completely destroyed.",hardware,2025-12-07 03:43:50,7
Intel,nsl2hua,Tech is supposed to beat inflation. Look at monitors or TVs or SSDs (before now) or CPUs or ....,hardware,2025-12-06 12:36:00,14
Intel,nt09q01,All the tech tubers are just turning into old men shouting at clouds. They will probably all be replaced by younger people living in the now soon enough.,hardware,2025-12-08 21:40:56,1
Intel,nslaf1q,"First of all 250 euros bought way more gpu in 2015 than 360 does today. And the lower end and midrange gpus were much less cut down vs the high end chipa today.  A 5050 sits where the 750ti did when the 980ti was out. Now you get entry level performance for mid end prices  Have wages actually increased that much? Because that is the only useful measure of ""inflation"". Everything else is just corporate profits   If prices for everything go up but wages don't then that leaves less money for frivolous shit like ram and storage and laptops and consoles, not more.  Even in my country where our wages are automatically indexed to match inflation, our purchasing power has dropped because the actual cost of living isn't properly represented in whichever calculation is used for the inflation number.  Houses have gone up by 100+ percent since 2015, rents have gone up by over 60 percent, grocery prices have more than doubled, utility prices have risen sharply, public transport has more than tripled in cost.  Minor expenses like clothing or a tv you buy every ten years have stayed flat, but that isnt what people are spending 80 percent of their income on.",hardware,2025-12-06 13:33:32,-10
Intel,nsl9m9r,"Shh, everyone knows that prices only go up on luxury goods due to evil corporations, after all how will people live without their computer not being 800% faster than last year?",hardware,2025-12-06 13:28:13,-6
Intel,nsmb10n,"> Yep, inflation. $250 in 2015 money is $342 in todays money.   People really need to stop using CPI. I can bet you that GPUs don't make it to the market basket. Yes, your money's value has fallen but not by that much.",hardware,2025-12-06 17:02:45,-7
Intel,nslqts2,You're getting downvoted for speaking the truth.  The RTX 5050 should be a $150-180 GPU for the price and value it offers but unfortunately people are gonna defend the price tag that the card was set for by Nvidia,hardware,2025-12-06 15:13:39,26
Intel,nspdyuf,"For a while you could get them for $229, which would be more acceptable vs a 5060 for $299, making it the same FPS/$. But the 5060 is actually the one on sale right now for only $30-$35 more. 30% faster for like 12% more money.",hardware,2025-12-07 03:38:38,1
Intel,nsn70s5,"I'd say that the cheapest new GPU that I'd blanket recommend with no ifs, buts and caveats is the 9060XT 16GB, everything below that either struggles with outright performance, VRAM or software issues like Arc.",hardware,2025-12-06 19:49:40,12
Intel,nsmn51w,"Intel checks all the right boxes on paper (generous VRAM, decent pricing compared to competitors, an alternative to the duopoly) but the recent CPU overhead stuff coupled with the crapshoot that is trying to play older games and it just isn't worth it",hardware,2025-12-06 18:06:54,7
Intel,nsl1d2d,"Yeah they cover this at the start of the video https://youtu.be/lLe5AP6igjw?t=229   XeSS 1.3 shifts everything down a tier, so their quality scaling ratio is everyone elses balanced ratio.  Older versions of XeSS match DLSS/FSR scaling ratios.",hardware,2025-12-06 12:26:39,40
Intel,nskx602,"I know quality is, not 100% sure about others. dlss quality preset uses higher resolution than XeSS and FSR quality presets",hardware,2025-12-06 11:49:24,0
Intel,nsxjyy3,Its fine for people who need a dGPU but not a beast for work. think stuff like CAD or Photoshop. It will also be fine for people who only play competitive multiplayer games.,hardware,2025-12-08 13:22:59,1
Intel,nsxk5xm,Nvidia cards are bellow MSRP here in eastern europe. AMD cards slightly above MSRP.,hardware,2025-12-08 13:24:15,2
Intel,nt0aiwr,"They are all selling below MSRP in the UK. £979 is MSRP for a 5080 and I can buy 3 in stock models for less than that price without much searching, at scan.co.uk.  If you are in South America its probably your countries insane import taxes, protecting their home grown GPU market lol.  29 upvotes from children who have not bothered to check or do any kind of reasoning.",hardware,2025-12-08 21:44:54,1
Intel,nsl44tr,"> Tech is supposed to beat inflation.  And it does, wtf are you trying to claim?  $100 CPUs these days run circles around 6700K which was the flagship in 2015. A B580 is faster than a GTX 980 Ti, which was the flagship card of 2015.",hardware,2025-12-06 12:49:07,74
Intel,nslgt6n,"It does. For the price of a 1993 CRT TV, you can get a flat-screen LED thrice the size and with 10 times the resolution.  SSDs? A 2 TB nvme is a fraction today than a 128 GB Sata one was a little over a decade ago.   What actually changed is inflation, and that the buying power of today's middle class person decreased significantly relative even to the 2000s.",hardware,2025-12-06 14:14:46,19
Intel,nssphno,Gpus are way more expensive to produce,hardware,2025-12-07 18:04:53,1
Intel,nsxk9lk,"> Tech is supposed to beat inflation.   It does, despite wafer prices increasing the last 10 years.",hardware,2025-12-08 13:24:53,1
Intel,nslrykc,Wrote a fucking who? The fact that TVs are cheaper in nominal terms than they were 15 years ago does not mean it has to be the same thing with every other tech product. TVs are not products manufactured necessarily on cutting-edge expensive nodes.,hardware,2025-12-06 15:20:04,0
Intel,nsridpx,"It does. For the same amount of money, you get way better GPU(unless your braindead thinks the gtx970 has same performance of 9060xt)",hardware,2025-12-07 14:17:24,0
Intel,nsmyw1o,"It does, but also, inflation has been extremely bad for 5 years.",hardware,2025-12-06 19:06:53,-1
Intel,nslef0x,"You are just making shit up at this point. NVIDIA GeForce GTX 760 (2013) release price: $250.   That was a shit card, arguably a worse product than the 9060 XT is today, when you compare it to contemporary rivals. How do i know it was shit? I had it.",hardware,2025-12-06 13:59:32,36
Intel,nsmzdz9,TSMC inflation is FAR higher than CPI. You are half right,hardware,2025-12-06 19:09:24,11
Intel,nsxnqe7,"according to US bureau of labour staticstics that measures the CPI it includes  all personal computers (desktops, laptops, tablets) and related equipment (printers, monitors, smartwatches, smartphones). It does not look at GPUs specifically, but the effect of that will be visible.",hardware,2025-12-08 13:46:23,2
Intel,nslsb0f,I doubt ppl are gonna defend the 5050 considering a 5060 or an 8GB 9060XT is not much more and a fair bit faster.,hardware,2025-12-06 15:22:04,11
Intel,nsuys01,"i mean, the 5050 off of amazon rn is 210, so it is getting there as a sub 200 dollar GPU for improving over iGPU right",hardware,2025-12-08 01:03:40,1
Intel,nsmjt4j,I pretty much but there's nothing else people *trust* in the category because apparently Arc cards are for professional nerds or something whereas I haven't had a real bad driver issue in over 2 years with my A770  People are also conditioned to fear older gen GPUs so 6xxx and 7xxx parts are sitting on shelves waiting for blowout discounts. People would still rather spend more on a basic nvidia from the 5000 series.,hardware,2025-12-06 17:50:02,0
Intel,nsokkai,B580 user here. I coupled it with a R5 5500 and as of the recent updates the card just seemed to run much better vs when I got it last July.  There was a video before which also revealed that the CPU overhead is now being addressed in subsequent updates.  https://youtu.be/gfqGqj2bFj8?si=PyAfB2NhqZKWWVXY  I’d say it’s getting better and that I’d recommend it over a 5050 since the overhead is now fixed/negligible.,hardware,2025-12-07 00:31:34,10
Intel,nsxo5ua,Intel is in their second GPU generation. Its going to take a lot longer to catch up with the institutional knowledge and practical application in videogames that the others were developing for over 20 years. The CPU overheard was not an issue in Intel iGPUs and Alchemist because GPUs never got fast enough to matter. It is only now that they noticed that issue since the GPU is far enough to create it.,hardware,2025-12-08 13:48:58,1
Intel,nsq49gv,And that only in terms of native resolution and does not mean equal final image quality.,hardware,2025-12-07 06:54:10,5
Intel,nsljs69,\>dlss quality preset uses higher resolution than XeSS and FSR quality presets  Not exactly. FSR and DLSS are evenly matched in internals at all quality presets.,hardware,2025-12-06 14:32:33,21
Intel,nt3tjxg,I’m in Australia lol. The msrp for the 5090 is 1999 USD which translates to 3011 AUD The cheapest 5090 is 4800 AUD. That’s not even close at all to the msrp…,hardware,2025-12-09 13:05:09,1
Intel,nsq2h5v,"It sure is funny every time all those 6700k/8700k era CPU's pop up on used parts sites or FB Marketplace and still expecting close to initial prices.  Who even buys them anymore? At least a Q6600 has retro value ,but those are just obsolete.",hardware,2025-12-07 06:37:50,9
Intel,nslou4e,"Not if you consider how the workloads being run on them have also changed. A GTX 970 ($450 inflation adjusted) would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  In other words, demand for performance has outstripped performance improvements, and those improvements are not felt as much.",hardware,2025-12-06 15:02:28,-9
Intel,nsm6adc,"A 100$ CPU in 2015 would easily run 2015 made software. A 100$ CPU in 2025 would barely run the electron JS slop. This includes Windows.  In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Yes yes yes it is very good on benchmarks but I don't stare at benchmarks all day. I use my computer for things you do at computer. Don't force my CPU to crunch how much digits of Pi it can compute.  A 980 Ti can easily run top 2015 games. Now? My laptop barely runs modern AAA games without looking a blurry mess. I simply can't fucking understand how you people look at the glorified motion blur and call ""yup it is the pinnacle of computer graphics"". How the fuck majority of modern AAA games look any better than RDR2 can anyone fucking tell me?",hardware,2025-12-06 16:37:49,-10
Intel,nstfa7l,"Why aren't you comparing relative buying power of 2014/2015 vs now then?  $650 got you what in 2014, a GTX 980TI?  $330 got you what in 2014? How close to the top end are both these things?    That's $900/$455 today, thereabouts.  What does $900 get you today?  Does that buy you anywhere near the top end?  And how does that product compare in relation to others above and below it?  Because the $330 product in question ($455 today) got you about ~75% to top end performance for ~half~ MSRP of the 980TI.  How does a $455 product of today square up relative to the top end?   Why don't we throw in a GTX 980TI vs a GTX 280 comparison while we're at it.  Make things really interesting.  I'll let you fill in those blanks (along with the $330 card in question) hoping you actually learn something in the process here.  The bar is very low, try not to trip.    The underlying point that user was making was pretty obvious if you read the comment they responded to.",hardware,2025-12-07 20:08:33,-7
Intel,nsm3fs2,"cmon man, give him some slack, he just made shit up cause it's convenience for his argument.",hardware,2025-12-06 16:22:46,14
Intel,nslu7ja,How is it comparable? The 9060 XT is a very good card for 250. Ideally it'd be around 200 or below but for 250 you get a card that's a bit overkill for even 1080 P gaming.,hardware,2025-12-06 15:32:39,-8
Intel,nsluf98,In quite a few countries the 9060 XT is at or below the RTX 5050s MSRP.,hardware,2025-12-06 15:33:51,18
Intel,nslugvd,Then why did the comment above mine get multiple downvotes? It's Reddit and that's how it goes unfortunately,hardware,2025-12-06 15:34:05,2
Intel,nsnx0y4,6x and 7x are priced far too high for old stock and are poor value compared to nvidias 50 series. They really haven't had a good price/performance low-mid end card since the 6700XT which are extinct at retail.,hardware,2025-12-06 22:14:23,4
Intel,nspdbpk,"I think they must have to go into every game, and adjust that t fix it, because it's not a universal fix it seems. Maybe per-game optimizations .",hardware,2025-12-07 03:34:19,3
Intel,nsxjt6i,yep. XeSS 1.3 is closer to DLSS 3 rather than DLSS 4 in terms of image quality. Its good enough to game on in my opinion.,hardware,2025-12-08 13:21:58,5
Intel,nsqehxx,"> Who even buys them anymore?  The best SKUs on sockets have always demanded a premium in the used market. Since that's where people upgrading old machines will go.  And many machines from OEMs are not readily upgradable with just new boards/CPUs combos. Since they use custom form factors etc. So it's either a in socket upgrade or replace the whole machine. The socket 6700k is on is also especially affected by the ""premium"" factor. Since there's no lower end SKU with 4C/8T. You either get the 6700/7700 variants or are stuck with lower thread count.",hardware,2025-12-07 08:33:16,11
Intel,nsm3au4,"> would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  I think your memory is impacted by the expectations at the time. And the problem of reviews often using older titles inflating numbers, ffs some are still benching with GTA V to this day.   The [970](https://tpucdn.com/review/nvidia-geforce-gtx-1060/images/witcher3_1920_1080.png) couldn't even get 60 fps in witcher 3. Which was released in 2015.  And the performance it got in Witcher 3. Was not much better than what the 5060 Ti got in [Black Myth Wukon](https://tpucdn.com/review/msi-geforce-rtx-5060-ti-gaming-16-gb/images/black-myth-wukong-1920-1080.png)  Which even including 2025 titles. Is one of the hardest/heaviest titles with the worst performance. You can expect much better performance in almost every title. Just like the 970 was doing better than it did in Witchers 3.   But to argue that we got a lot better performance back then in the games releasing at the time, that is just false.",hardware,2025-12-06 16:22:03,37
Intel,nsp8t2z,Yes 3.5GB of memory in 2015 was soooo much better than 12GB today /s,hardware,2025-12-07 03:04:36,3
Intel,nsm6g3n,"All the GPU makers are betting on you using DLSS/FSR/XeSS as part of your usage to play games. Maybe even frame generation along with Relex, and all the other tech they ship GPUs with. They used to only rely on you using regular AA techniques.   If you ignore all those options you have today, and pay like it's 2015, it might be worse a lot of the time. If you use those options, you're generally way ahead of where a GTX 970 would fall. So it depends if you're willing to adopt new rendering tech, or rejecting it.",hardware,2025-12-06 16:38:39,3
Intel,nsmzr6o,"No, not really, in 2015, you happily accepted 45 FPS on not the highest settings at 1080p",hardware,2025-12-06 19:11:17,4
Intel,nsn5d71,"This don't sounds like CPU problems at all, more like either Win11 is a vibe coded pile of bugs, or ACPI problems.",hardware,2025-12-06 19:40:49,9
Intel,nsmgtnv,"> Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Something is wrong. I keep reading people's experiences of stuff like this and I haven't experienced it, I'm not doubting it but I'm so curious as to what is wrong.  In particular I read a lot of people saying Windows Explorer takes forever to open etc",hardware,2025-12-06 17:34:05,7
Intel,nsmsf6b,"> In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  That's Windows for you.",hardware,2025-12-06 18:33:48,1
Intel,nsxkhp8,it does not matter how close to the top GPU is. its a completely useless comparison.,hardware,2025-12-08 13:26:19,2
Intel,nsm74h5,I'm not sure what you're saying. The 9060xt is $250 only in 2013 money. They are arguing it's better value than a GTX 760.,hardware,2025-12-06 16:42:14,1
Intel,nslzd1j,"Ppl up/downvote kinda randomly, doesn't really mean much post can go from +/-20 to the opposite real quick sometimes.  Anyways It's at +8 currently was at +something(2 maybe?) when i commented so who cares.",hardware,2025-12-06 16:00:40,3
Intel,nsxodfi,"Nvidia and AMD do a lot of per-game optimization in the driver as well. In some cases very brutally, for example Nvidia is known for grabbing all games DX12 drawcalls and rearranging them in driver because the way game handles it is inefficient.",hardware,2025-12-08 13:50:12,1
Intel,nsm6ry8,I remember upgrading to a 970 in 2016 and still being unable to max Witcher 3 at 1080p60 but got close enough,hardware,2025-12-06 16:40:22,5
Intel,nsxkob8,"funnily enough, GTA 5 Enhanced Edition can be quite a benchmark for ray tracing nowadays. But it took to this year for it to be released. I think we can consider it a testbed for whats going to be implemented in GTA 6.",hardware,2025-12-08 13:27:28,1
Intel,nsmnevw,"Bringing up 1080p no RT Wukong benchmarks sort of makes the point for me: the only way these cards look comparable is if we pretend features and standards are the exact same they were a decade ago.  High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen. RT was not a thing in 2015, now it is and Nvidia marketing really wants you to use it. It's like you're comparing Witcher 3 on Ultra settings to Wukong on Medium or High settings, and acting like it's apples to apples.  The moment you take modern displays and features (including DLSS to be fair) into account, it paints a picture where technology has moved on, developers and players would love to move on, and GPUs are struggling to make that jump.",hardware,2025-12-06 18:08:19,-10
Intel,nsxl3k9,"if you run out of memory today the game swaps textures and continues running, it just looks uglier.   If you run out of memory in 2015 it starts using the superslow 0.5 GB and everything breaks.",hardware,2025-12-08 13:30:09,2
Intel,nsxl8jt,Nvidia is certainly expecting DLSS+FG to be the typical use case. The vast majority of their benchmark and marketing material is with those two.,hardware,2025-12-08 13:31:02,1
Intel,nsmwsat,"I've got nothing against DLSS, I use it whenever I can, but sometimes it's just not enough to bridge that gap.  Another user brought up Witcher 3 and Wukong as an example of a graphically advanced 2015 game vs a graphically advanced 2025 game. The 970 would get 50+ fps on Ultra settings Witcher 3. Max out Wukong on a 5060 Ti and no amount of DLSS will make that card stop crying and screaming.",hardware,2025-12-06 18:56:07,-4
Intel,nsoze8r,"Like, Debian has no issues running on a n150 with multiple docker containers without instantly spiking the cpu to 100%.",hardware,2025-12-07 02:04:41,3
Intel,nsxmt3k,"I found a way to sort of kinda make file explorer slow. But its really a perfect storm thing. Have multiple screens, one of which is running in HDR and another in SDR. Have the file explorer tree open. Have a HDD, slower the better.  When you browse folders it refreshes the tree. When it refreshes the tree it asks connected devices if they are online, including the HDDs. Now move the window back and forth between your screens. When the explorer moves into HDR screen, it gets redrawn. Same when it moves to SDR screen. I suspect but cannot confirm there is a bug where the old instance is not cleaned correctly. So now when you browse it asks all devices if they are online 10 times. 100 times. At some point youll start noticing actual delays in opening folders.  Works even better if you havent restarted for a month.",hardware,2025-12-08 13:40:46,1
Intel,nsxn5gg,"the opening notepad thing, if you use taskbar it has a bad habit of not actually opening notepad until it finishes the online search for apps called notepad or whatever you typed. Disabling online search in start makes it fly really fast.",hardware,2025-12-08 13:42:53,1
Intel,nsm8g0h,The 9060 XT 8GB is currently retailing for 250$ in many areas. I'm saying that the 760 isn't an ARGUABLY worse product. It is a worse product for it's time straight up.,hardware,2025-12-06 16:49:09,5
Intel,nslzlxy,"Fair point, but isn't lower and competitive prices good for us?",hardware,2025-12-06 16:02:01,1
Intel,nsmqlcg,">  and acting like it's apples to apples.  Apples to apples would be comparing W3 performance for both cards.  Wukong even without RT is a CONSIDERABLY more advanced game graphically than original W3.   >High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen.   And? Better monitors showing up doesn't change the laws of physics and basic economics. It doesn't make scaling with die shrinks suddenly increase. With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.   And before you start harping on about die sizes. The die in the 5060 Ti is actually more expensive than the die used on the 970. Wafer price increases more than compensates for the size difference.",hardware,2025-12-06 18:24:26,16
Intel,nspgewd,"Looked it up. Wither 3 got 52 FPS at Ultra settings, no Nvidia Hairworks turned on, for a GTX 970. Wukong gets 42 FPS at the cinematic preset native resolution, which is actually intended for cinematics, but developers allow people to enable anyways. As Digital Foundry has said, they maybe shouldn't.  Gets over 70 FPS if you turn the preset down 1 notch to high. No upscaling, or frame generation, or hardware RT, which is like what Nvidia Hairworks was for Witcher 3. It's really not hard to get Wukong to run at 90 FPS on a 5060ti with some minor tweaks.",hardware,2025-12-07 03:54:39,6
Intel,nsxlsss,According to TPU review maxed out Wukong with DLSS got 42.3 fps. Not exactly the 50 fps you remmeberr for witcher but close. Heres a link to the review: https://www.techpowerup.com/review/black-myth-wukong-fps-performance-benchmark/5.html,hardware,2025-12-08 13:34:33,1
Intel,nsxop63,"That's very interesting thank you, I can definitely see why I haven't experienced it.  You genuinely wonder how Microsoft are testing these days.",hardware,2025-12-08 13:52:10,1
Intel,nsmvfxi,"> With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.  Except in practice from 2005 to 2015 you got considerably more advanced graphics *and* higher resolutions *and* generally higher framerates too. Now either you pay up or you gotta pick one.  As for the rest of your post, it's more of a digression. All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.",hardware,2025-12-06 18:49:17,-2
Intel,nsxm00p,"> As Digital Foundry has said, they maybe shouldn't.  hard disagree. As someone who does not have a lot of time for videogames and often end up playing older games with newer cards, those beyond high settings are great as it allows me to make use of my newer card and make the old game look better.",hardware,2025-12-08 13:35:48,1
Intel,nsrkwkw,"I disagree with comparing RT to Hairworks, when the visual impact as well as the emphasis put on it by Nvidia is so much bigger. I also disagree with using 1080p as a reference for Wukong, when high res and high refresh rate monitors are as cheap and plentiful as 1080p was back then.  Imagine you went back to 2015 and told the GTX 970 guy he's supposed to play his games at 2005 resolution and turn off antialiasing, how do you think he'd react?",hardware,2025-12-07 14:32:28,1
Intel,nsncxlc,">All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.  Why complain about something that there are valid reasons for lol",hardware,2025-12-06 20:22:05,5
Intel,nsyfiin,"It just makes such a small difference in UE5, it's really not worth losing 30% performance over for this engine. They would agree with you for a lot of other games, and Avatar Pandora kind of has a hidden setting, they'll maybe make available in menu at some point. Right now you need to modify a config file to enable it. Maybe they just need to wait until the final patch of a game to show those settings, years after launch, or just name them ""next gen"" or ""experimental"" with the setting below called ""ultra"".",hardware,2025-12-08 16:15:19,1
Intel,nss9lt6,"You can use DLSS and frame generation to play at higher resolutions. That's their intent. Especially UE5 games, because TSR was developed by Epic for a reason. The games on UE5 are really never intended to be run at a native resolution. I don't tell people to run UE5 at a 2013 resolution, but I also don't tell them to have the 2013 mindset that everything has to be run at native resolution, and that's the only way to play it. 1440p Balanced DLSS should give you around 50-60 fps without frame generation.",hardware,2025-12-07 16:44:57,2
Intel,nt9j82j,"The performance loss does not mater for future (re)plays.  The config settings are usually hidden because during testing there were instabilities found that they didnt think was worth fixing. There were some games that had settings beyond ultra with names like ""Extreme,"" ""Nightmare,"" or ""Insane"".",hardware,2025-12-10 09:42:20,1
Intel,nosu5u7,"3050 performance at half the power, can't argue with that.  I do wonder how long it will be before the low-end dGPU disappears entirely from laptops, suspect these new chips probably aint it due to price but it's presumably coming.",hardware,2025-11-14 12:51:04,54
Intel,noshwzt,"While Geekbench's OpenCL isnt super popular for GPU testing;  I still cant believe a mainstream integrated GPU is actually competing with laptop 3050Ti, and these results still aren't with final optimizations!  by the time Panther Lake comes out, Intel claiming that its as fast as laptop RTX 4050 might actually be true lol",hardware,2025-11-14 11:16:17,73
Intel,notzu22,Are these better than Ryzen igpu? How do they compare to RTX 4050 AND 5050 in non demanding game?,hardware,2025-11-14 16:35:58,7
Intel,notsbf4,"I'm excited sure but if the claims are true how can anyone here expected these to be reasonably priced and sell well? If it competes with a laptop dedicated GPU it will be priced 2-3 times that laptop. The 285H isn't even that popular and its expensive, hope the 385H is affordable AND the claims are true.",hardware,2025-11-14 15:58:55,6
Intel,noudeop,"The score is slightly above a GTX 1650 Super which in turn is slightly above a GTX 1060 which is barely matched by current most performing 128 bits iGPU (Lunar Lake / Strix Point)  Even if we expect 50% improvement compared to 890M, that's still half the performance level of a full power mobile RTX 3060. (roughly half a PS5 too)",hardware,2025-11-14 17:44:26,4
Intel,npgcjdt,This would still no where be close to M4/M5 in single core and GPU,hardware,2025-11-18 06:45:37,1
Intel,nvtf2fd,I sincerely hope Samsung will work on their camera and make it better in this version.,hardware,2025-12-25 03:06:03,1
Intel,nosfy6e,"Hello 6950! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-14 10:58:08,1
Intel,nouc8xh,"The problem with buying a device relying on Intel graphics drivers, is that you're getting a device relying on Intel graphics drivers.  I'd rather not.",hardware,2025-11-14 17:38:32,-5
Intel,nosxwa8,"iGPUs have already killed off the MX series, suppose it’s a real possibility other low end dGPUs also get killed off",hardware,2025-11-14 13:14:59,44
Intel,note1cz,"More likely what is considered a low end dGPU will shift, just as a current low end dGPU would have been an high end model a decade ago.",hardware,2025-11-14 14:47:09,13
Intel,np3ssro,"iGPUs is why low end desktop parts dont exist anymore, its going to do the same for mobile parts.",hardware,2025-11-16 06:06:05,1
Intel,npclgdx,"It'll happen eventually but they have a few issues.  First is memory bandwidth, they'd need to put a wider bus on it to address which is costly.  Second is the memory itself, they need something with higher bandwidth (but that might be addressed with LPDDR6).  To address the bandwidth issue they often need to put on extra cache, the 12Xe3 core is basically maxing out the bandwidth of the 'standard' chip, and if they went to say 20Xe3 cores (which would be 4060+ level) then they would need a bigger memory bus and more cache (Strix Halo put 20mb of MALL cache to try and address, and a 256 bit bus) and that is more design work.  They'd also have an issue with the socket - the APU would be too big to fit into a standard socket and would need a custom one which means a custom motherboard which increases costs.  Another is heat - while more efficient overall, all the heat is in one spot so the computer needs to be thoughtful about moving that out.  A think that in 2028 big APU solutions can compete well against XX50 series and most XX60 cards.  It isn't happening before then however.",hardware,2025-11-17 17:36:51,1
Intel,nosr91b,Apple's M5 already beats the 3050Ti though.,hardware,2025-11-14 12:30:46,30
Intel,notfij9,I mean the early Iris pro onboard GPU’s traded blows with gtx 650m at a lower power draw. It’s been done before. Still nice to see but nothing ground breaking,hardware,2025-11-14 14:54:51,9
Intel,nou9dky,We will have to wait for the actual laptops to release & see how they do in real games; performance in synthetic benchmarks does not always line up in real tests.,hardware,2025-11-14 17:23:51,10
Intel,nov4acg,PTL should have comparable or possibly even lower production costs than ARL. There's some room for optimism.,hardware,2025-11-14 19:59:19,8
Intel,noydico,this GPU is also coming into the 355H btw (more specifically there's a Ultra X7 version of the 355H with the full 12-core GPU)     and iirc last time the 255H was pretty popular,hardware,2025-11-15 09:37:36,2
Intel,npcp3di,"Way we should think about it in my mind is that it's got 12Xe3 cores, and 1 Xe2 core = 2 RDNA 3.5 cores.  Strix Halo has 40 RDNA 3.5 cores (though with a bigger bus and enhanced cache) which is kind of like 20 Xe2 cores (maybe a bit more), and Strix Halo basically ties with a 4060.  If you assume a 5-10% improvement from the architecture going Xe2 to Xe3 to offset the bus and cache (which isn't *as* limiting with only 60% of the hardware), then you're probably at about 60% of a 4060 power, which is more or less where this looks to fall.  4060 mobile passmark = \~17,400  4050 mobile passmark = \~14,300 (81% 4060)  60% 4060 mobile passmark = \~10,500  3050 mobile passmark = \~10,100  That puts it pretty much at a 3050.  With a little sprinkle of cutting edge upscaling and a bit of frame gen it'll be just fine for light titles, but won't be a dedicated monster.  It's just a good solid chip for light-weight long-battery general purpose use that can play some titles on the side (or play older titles with confidence).  I suspect it'll review pretty well, since the reviewers tend to be excessively games-focused and this will appeal, but it's not a games machine - it's a surprisingly zippy long-life chip that's a good fit for all-day laptops.  EDIT: 8060S (Strix Halo) is \~18,000 for reference.  EDIT 2: 890M (strix point's top end) about 8,100, so the PTL 12Xe chip will smoke that.  I'm excited about it, it looks like a great chip!",hardware,2025-11-17 17:54:36,0
Intel,nvy72ph,"I swear, the only issue i have with the book 5 is the camera, this shit doesn't deserve the book 5 hardware",hardware,2025-12-26 00:34:46,1
Intel,noui1dy,"It is not 2023 anymore. Intel Arc drivers work quite well, and Lunar Lake graphics are already pretty solid in the low power category. No need to keep repeating this nonsense when you haven't even used the devices.",hardware,2025-11-14 18:07:17,19
Intel,not3o9j,"Only sticking point seems to be cost I suppose, implementing a truly competitive iGPU looks like it will be expensive if Strix Halo is anything to go by.  I think the Intel/Nvidia partnership might at least come close to making it a reality to have a 50/60 tier integrated GPU in a laptop that isn't much more expensive than one with a dGPU currently... in theory it could/should be cheaper.",hardware,2025-11-14 13:49:22,10
Intel,nou51fd,Not really? Even optimistically speaking this won't be close to a 5050 laptop when accounting for bandwidth limitations in real world tasks that use the cpu and gpu at the same time.,hardware,2025-11-14 17:01:51,3
Intel,nou5hpq,"It's a good point. There really isn't any truly low end dGPU options out there now as far as I know, Nvidia abandoned the GT line and someone up there mentioned MX is dead etc.",hardware,2025-11-14 17:04:08,7
Intel,nouozrl,I’m assuming the caveat to posts like this is “running a version of windows/linux”,hardware,2025-11-14 18:41:25,10
Intel,nossn02,"and the 780M when fed enough power also comes within range, and the 890M also beats it.",hardware,2025-11-14 12:40:39,22
Intel,np3szy6,"It could beat a 5090, it would still be useless until the bootloader is open.",hardware,2025-11-16 06:07:46,2
Intel,nougqlr,Wasn't the previous Intel igpu really good for games and efficient?,hardware,2025-11-14 18:00:43,9
Intel,nouo5af,"Even if what you said was true, I know enough about Intel to know that even if it worked great, they'd find a way to fuck it up eventually. That's what they do.",hardware,2025-11-14 18:37:14,-5
Intel,noteu83,">if Strix Halo is anything to go by.  I think Strix Halo skews how expensive a competitive iGPU could be. Strix Halo was very low volume and was also, imo, overbuilt. I think it's definitely possible to have a more cost competitive big-iGPU SoC",hardware,2025-11-14 14:51:22,14
Intel,np2vxz8,Prices for Strix Halo are rumored to fall in the new year; I'm curious to see if it ever reaches 1000-1500 dollar laptops like I've seen suggested it could. Seems like a ridiculous idea right now.,hardware,2025-11-16 02:14:32,1
Intel,nox0o6z,And in gaming.,hardware,2025-11-15 02:39:40,8
Intel,noxmkdx,I mean...technically Asahi Linux exists for Macs. Though not the M5,hardware,2025-11-15 05:15:54,3
Intel,nost857,Then there's the 8050S & 8060S,hardware,2025-11-14 12:44:41,11
Intel,np2p16a,"How so? It is not even beating, at best barely even with 3050 35W (non-Ti).",hardware,2025-11-16 01:33:18,1
Intel,npfpm2g,I'm sure thats the first thing that comes to mind when people purchase a thin and light.,hardware,2025-11-18 03:44:31,1
Intel,novhbul,"The Arc 140V like in Lunar Lake 268V? it's about 3050 Mobile (35W) sort of performance in real tests. There's a 140T in some chips too, I think it's less powerful than the V.",hardware,2025-11-14 21:08:26,6
Intel,noto02z,Strix point is the better comparison,hardware,2025-11-14 15:37:54,3
Intel,nou57r7,"I hope so. I was a little surprised by the price but as you say, they clearly have a market for that thing and it isn't particularly price sensitive.",hardware,2025-11-14 17:02:45,2
Intel,nosxd97,they are not really comparable to the traditional APUs,hardware,2025-11-14 13:11:43,19
Intel,not3ot0,These are 256bit bus devices and have even fatter GPUs .....,hardware,2025-11-14 13:49:27,17
Intel,npbumwg,"Wait, is the 780/880m close to the 3050? I had heard that it was close to the 1050 at some point... As an 780m owner, it sounds like I can suddenly play way more games than I imagined?",hardware,2025-11-17 15:24:12,1
Intel,novka84,yes that one,hardware,2025-11-14 21:23:45,2
Intel,notxkw1,"Right, but Strix Point suffers from the same problems that PTL-X will: 128b bus chocking the memory bandwidth.  If iGPUs want to truly replace entry level dGPU, there needs to be wider bus variants",hardware,2025-11-14 16:24:49,10
Intel,npcclrj,"Might seem so, but mobile 3050 is way slower than let's say 1660 laptop.  Still, you can play many games, just at lower res and settings. You can look up benchmarks/tests for handhelds with this gpu (or ryzen z1/z1 extreme)",hardware,2025-11-17 16:53:04,1
Intel,nour2eo,"No doubt , I meant in terms of availability",hardware,2025-11-14 18:51:36,1
Intel,nngoz2c,Why can't they have a low core count CPU but still keep the full iGPU?,hardware,2025-11-06 18:43:01,67
Intel,nngf3u0,"While this has pretty much been known for a while, you really do get the feeling when looking at these frequency and power stats that Intel hoped for more from 18A.  It's a good node and a good chipset, I think it's targeted and designed well and will do well in the market, but it doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.",hardware,2025-11-06 17:56:37,44
Intel,nnk8kqz,"6 different top models with just 100mhz between them is so stupid, that should be 1 or MAYBE 2 models for that core configuration  But just hope the 356H is cheap then",hardware,2025-11-07 07:43:58,6
Intel,nnku3ds,Is this desktop or laptop?,hardware,2025-11-07 11:19:09,3
Intel,nnglnmb,">16C (4P + 8E + 4LP)  I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  Two clusters make sense. What am I supposed to do with three?!  Unless I'm mistaken, AMD seems to be sticking with full fat Zen 5 cores in both Strix Halo and ""Fire Range,"" and frankly, it sounds like Intel is just trying to keep up in the core count race with its plethora of 'baby' cores.  Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.   I use my laptop for convenience and my desktop for compute heavy work.",hardware,2025-11-06 18:27:30,-6
Intel,nnhbq11,"16 cores, but only 4 of them will be fast. The rest will be slow cores.",hardware,2025-11-06 20:34:10,-3
Intel,nngan3d,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-06 17:35:20,1
Intel,nniwn7f,"Nice. My a770 still slaying 4k ultra gaming in 2025. No stutters no freezing just responsive beautiful gaming. Ofcourse it’s paired with 14900k so my igpu technically has identical specs but I keep temps down this way. Also have an RTX 50 series. I love arc graphics. Bang for buck AT HIGHER RESOLUTIONS — that’s where it’s at. And I’ve had 4090 (sold), amd 9070(sold). Never would I pay what they are asking for a 5090. Just because I wasn’t 100 percent thrilled with price per FPS with the 4090. Once you have had all of them you will see Intel is the way to go with 4k gaming(unless you like paying thousands of dollars). Arc doesn’t have shadows and artifacts . RTX does. Bad. Blurs my picture even on best settings possible.",hardware,2025-11-07 01:45:56,-6
Intel,nngrua7,Upselling,hardware,2025-11-06 18:56:23,50
Intel,nnh35ny,IMO it's probably not economical for them to do it.   Gaming laptops/handhelds are already niche enough as it is -- doesn't make sense for them to package both and for OEMs to carry both for just for a ~$1-200 delta between the units. Those who want the full GPU will just pay for it.,hardware,2025-11-06 19:51:36,15
Intel,nnntnit,"You gotta remember these CPU’s don’t have multi-threading. AMD gets 16 threads out of their 8 core z1 and z2 extreme but with these core ultra’s you get one core and that’s it. A full on physical core is better than a virtual core, but the same amount of cores with multi-threading is much better than just a same core-count CPU without it. Intel are offering up to 16 cores which is 16 cores/16 threads, but generally should offer better multi-core performance vs an 8-core 16-thread. If anything it’s nice Intel have the ability to improve core count offerings without significantly raising the power requirements.   I for one veered away from the MSI Claw 8 AI+ as a standalone PC solution because it lacked the multi-core performance I needed with it only having 8 physical cores and no multi-threading. A 16-core offering is exactly what I need.",hardware,2025-11-07 21:03:16,3
Intel,nnhhojx,Because nobody would buy it.,hardware,2025-11-06 21:03:48,5
Intel,no4wbjw,"yeah, sucks cause they said handhelds was a priority for them",hardware,2025-11-10 17:11:40,1
Intel,nngiuzv,"Not great, not terrible. In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.",hardware,2025-11-06 18:14:25,21
Intel,nngq1l8,According to jaykihn0 it's a heat issue: BSPD and transistor density is flattening the VF curve on the high-end. The ST power curve is substantially improved at lower frequencies over Lunar Lake and Arrow Lake.,hardware,2025-11-06 18:48:00,10
Intel,nngpld3,"If anything, seems strictly worse than N3, even compared to older iterations. That's not great for a node that was supposed to go toe to toe with N2. Especially if the production cost is more in line with the latter.",hardware,2025-11-06 18:45:52,11
Intel,nounnja,"18a is initially launching with its lower power variant, which is why the only nova cpus that will use it for the compute tile are the laptop ones. The higher power variant comes later",hardware,2025-11-14 18:34:48,1
Intel,nnitrfm,"Jaykihn, whose tweets these leaks are based on, says that the design characteristics cannot be extrapolated to the node itself.   So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.",hardware,2025-11-07 01:28:24,1
Intel,nngofbt,"> […] It doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.  Well, we already had rumors of Intel struggling to hit actual intended frequency-goals, no?  Though especially the actual performance-metrics of the node will be quite sobering I think.  The power-draw of these SKUs is supposed to be a TDP of *officially* 25W – In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4) and 65/80W (4+4+4 and 4+8+4) respectively.  So it must be seen, if these parts are any more efficient in daily usage, or just a side-grade to Lunar Lake.",hardware,2025-11-06 18:40:28,-3
Intel,nngjg8l,"Panther Lake is pretty much a pipe cleaner, so it should still improve a bit.  Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD. If that is the case, it will only really be allowed to shine in high powered desktop and server chips, while power limited chips won't be able to reap as many of the benefits of 18A, since laptops tend to target the efficiency sweet spot, rather than clocks.  My understanding is that 18A will scale much better with extra power than N3 and N2. Once you pass the sweet spot, 10% extra performance on N2 might take 30% extra power, while 18A should continue to scale much more linearly for a good while longer.  So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones, especially in use cases where SRAM density is less of a concern. AFAIK Intel CPUs aren't as cache-dependent as AMD, so SRAM density might not be the highest priority, whereas slugging it out with AMD and trying to beat them on frequency might?  From what I've heard, Intel is still just about matching AMD in terms of IPC, while Arrow lake is very efficient, just not great for gaming. If Intel wants the CPU crown back, simply pushing more power could go a long way in clawing back the deficit, assuming 18A does indeed scale well with more power. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.",hardware,2025-11-06 18:17:12,-7
Intel,nnpnb6l,> 6 different top models with just 100mhz between them is so stupid  They've always done that because of the OEMs who like a million segments.,hardware,2025-11-08 03:41:25,3
Intel,nnkuisr,Laptop.,hardware,2025-11-07 11:22:50,5
Intel,nngwxp8,Since when is any user determining what cores they want to use and when?,hardware,2025-11-06 19:21:09,22
Intel,nngrg35,It makes sense to have three as the low-power cores prioritize keeping the power consumption low for battery life but to achieve this they aren't attached to the ring bus which hurts performance so 8 regular E cores are also used,hardware,2025-11-06 18:54:32,12
Intel,nnhqy1t,">I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  From a user's perspective, CPU's are a blackbox and the inner-working details are irrelevant. What matters is results. They want, typically, a balance of performance, battery life, heat / fan noise, and cost, placing greater emphasis on one of these categories over the others.  The idea is that P cores = performance optizied  E cores = Area optimized  LP-E Cores = power optimized.  The theory being to have different core types focus on different parts of PPA.  E cores and LP-E cores are the same microarchitecture. Just LP-E cores are off-ring so the ring (and rest of the cores) can be powered down in light-load scenarios.  Intel's biggest problem is that looking at each of the cores, the P cores perform their designated role the worst. 200% larger than E cores for \~10% more IPC and \~15% more clockspeed is a pretty rough trade off - and by the time you've loaded all P cores and need to load E cores too, that clockspeed difference diminishes hard.  Intel's better off (and all rumors point this way) to growing the E cores slightly and making that architecture the basis of the new P and E cores (like Zen vs Zen-C)",hardware,2025-11-06 21:48:50,9
Intel,nngrdc7,"> Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.  You are the niche use case. The average/median user is using H-class laptops.   These are meant to beat Strix Point and its refresh, which it will do and at better economics for Intel than Arrow Lake H.",hardware,2025-11-06 18:54:11,9
Intel,nnhcb0r,"And with all the issues Windows already has with the scheduler, I fully expect it to never be able to properly utilize the different cores. I expect a similar situation as with previous versions, where it's sometimes necessary to disable the slow cores to eliminate stuttering in games, because they sometimes offload tasks to them which slows the whole game down to a crawl.",hardware,2025-11-06 20:37:09,0
Intel,nngp9jv,"> I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  > Two clusters make sense. What am I supposed to do with three?!  Ain't these Low-Power Efficiency-Cores aren't even usable by the user anyway in the first place (and only through and for Windows' scheduler to maintain)? As I understand it, LPE-cores are essentially placebo-cores for marketing.",hardware,2025-11-06 18:44:22,-8
Intel,nnhcsuh,Darkmont should really not be that far off Cougar Cove. Certainly not *slow*.,hardware,2025-11-06 20:39:41,26
Intel,nnhjsmd,The E-Cores are plenty fast for most use-cases by now and there are 8 of them in there. Aren‘t they even ahead of the skylake IPC by now? 8 will be doing a great deal with a good clock frequency.  Only the 4 LP-Cores are kind of weak. But they aren‘t attached to the ring-bus and really only used for offloading low req Backgroundtasks and an idling machine. So that you can turn off the faster cores when they aren‘t necessary.  But for performant gaming and productivity it is basically a 12 core machine.,hardware,2025-11-06 21:14:06,5
Intel,nngt3ry,I hope there's a 4+0+4+12 for handhelds but they might not consider adding a whole new tile configuration packaging line worth it over just giving the OEM a discount.,hardware,2025-11-06 19:02:26,11
Intel,nnpqhax,I find that weird. Who buys a 20 core CPU for GPU performance equal to like an RX 580?,hardware,2025-11-08 04:04:39,2
Intel,nnj0g84,"Yeah as of now only MSI looks committed to using Intel for their handheld, and not only are they not that successful but they had also branched out to AMD for another line of their gaming handheld.",hardware,2025-11-07 02:08:54,7
Intel,nnpmmi9,Gaming handhelds are niche but not gaming laptops (relative to gaming desktops). Steam hardware survey reveals a large number of mobile gpus very high up in the charts.,hardware,2025-11-08 03:36:31,1
Intel,nnhnhmd,Both Apple's M series and LNL use decent iGPUs for their 4+4 parts.,hardware,2025-11-06 21:31:54,13
Intel,nnnppbh,"This is a minority use case, but is a very valid one. I run a home server (/r/homelab) and could use some extra GPU power for video encoding and some basic AI stuff. Though this is just a home server so I don’t want to put in a dedicated GPU due to the desire to not spend all of my discretionary income on my power bill. Right now I have a 9th gen i7 that is good enough for the basics of what I am doing, but I’ve been keeping my eye out for a replacement. I’m not alone in that need, but I do get this is a minority group. I’m sure there are other use cases though. Saying no one would buy it is wrong.",hardware,2025-11-07 20:42:57,1
Intel,nngm2dr,"IIRC Arrow Lake's compute tile is fabbed on N3B, which is pretty much TSMCs earliest N3 node for mass fabrication.  Its other tiles are on a mix of mature 5nm- and 7nm-class nodes.",hardware,2025-11-06 18:29:25,21
Intel,nnh1yr8,"> In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.  Arrow Lake was the pipe cleaner for 20A which in turn was deemed unnecessary to move into production because everything was going so well that they could use that groundwork to get a head start and move straight to 18A. That's right from Intel's mouth. Now the goalposts shift once again.",hardware,2025-11-06 19:45:51,20
Intel,nngr2a1,"> Not great, not terrible.  For what it's worth, I'm glad and somewhat relieved, that the never-ending charade of 18A (and ultimate sh!t-show it eventually again amounted to, after their blatant Vapor-ware 20A-stunt) finally is about to come to some lousy end, after years of constant shady re-schedules, even more bi-weekly Intel-lies and basically +2 Years of delays again.  So it will be a bit of … 'consolation' I guess, not having to constantly read that 18A-sh!t in every darn Intel-news.",hardware,2025-11-06 18:52:46,-7
Intel,nnh0tuv,"I'm curious to see the core's power curve itself. Or the core+ring, I forget what exactly Intel measures here for their software power counters.   Intel claims PTL has outright lower SoC power than LNL and ARL, so if the power savings are coming from better uncore design and not the node gains themselves....",hardware,2025-11-06 19:40:21,10
Intel,nnh18hs,"> BSPD and transistor density is flattening the VF curve on the high-end   He doesn't make this claim. BSPD in particular was supposed to help the most at high voltage, and the density does not appear to meaningfully differ from N3, which it still regresses relative to.    Best case scenario, the node, for other reasons, can't hit as high voltages as N3, and the top of the curve is just capped. But you wouldn't expect thermals to be such an issue then.    Doesn't seem to be any evidence of the core-level VF curve benefiting from the node either.",hardware,2025-11-06 19:42:19,11
Intel,nniukba,"Jaykihn attributes only density to the supposed heat issues, not BSPD. Which makes sense because the process guy and Stephen Robinson both talked about having to space out the signal and power wires when working with BSPD.  If they prioritized density, then it is possible that frequency could not reach the theoretical maximum due to crosstalk or heat issues.",hardware,2025-11-07 01:33:21,2
Intel,nngraag,"18A has always been low density, but expected to compensate for it with very efficient frequency scaling past peak perf/W. It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Wait for the desktop/server chips before you call it. With fewer thermal constraints and much higher power budgets, they should be able to push well past the perf/W peak, where it should continue to scale well and for a long time before it starts hitting severely diminishing returns.  I'm not talking 300W monster CPUs, but the scaling between a 65W and 105W 18A CPU should be more significant than on current nodes, where you might gain like 10-15% additional performance from that 50% power bump. I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.",hardware,2025-11-06 18:53:47,7
Intel,nnklc8p,> So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.  Why would they suddenly decide to underclock it to such a degree?,hardware,2025-11-07 09:55:39,2
Intel,nngrcwg,"Tbh, LNL battery life (note: not the same thing as loaded efficiency) with -U/-P/-H perf levels and market reach would still be a very good thing. It's just a shame to see Intel finally straighten out their SoC architecture only to be hamstrung by a subpar node.",hardware,2025-11-06 18:54:08,13
Intel,nnhns9s,">In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4)      Intel's existing 2+8 parts have a PL2 of 57W, so seems to me OEMs are permitted to target the same PL2 they've been targeting on U series.  >and 65/80W (4+4+4 and 4+8+4) respectively.  Which is a fairly large drop compared to existing H series. ARL-H PL2 is up to 115W, PTL-H PL2 will be up to 65/80W",hardware,2025-11-06 21:33:20,5
Intel,nnh0cvv,">So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones,  NVL-S being external adds serious question marks to this.   I wonder if post NVL-S they go back to using further iterations of 18A though for even desktop, if they feel confident enough that they can hit high enough Fmax (even throwing away power and density) on those compute tiles.   Maybe for RZL or Titan Lake in 28' (going off memory for codenames lol).   >especially in use cases where SRAM density is less of a concern.  On paper, Intel has caught up to TSMC (even N2) in SRAM density.   >AFAIK Intel CPUs aren't as cache-dependent as AMD,  Generally speaking Intel uses way higher capacity caches than AMD, AMD uses smaller but faster caches.   >. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.  This seems *extreme*. Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?",hardware,2025-11-06 19:38:01,10
Intel,nnh6cvf,"I actually suspect otherwise - speculation on my part, but I suspect that they were running into unexpected process issues when they pumped the frequency and/or voltage higher so they had to scale back performance.  It almost feels like a process bug that they discovered when making the actual chips.  They've also talked about kind of weak yields on 18A on their latest earnings call.  That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  If they have a 'double upgrade' opportunity in both fixing the discovered 18A issues and implementing the originally planned 18AP improvements then it could end up being a somewhat bigger jump.  Don't think it'll be an N2 beater though.  18AP *might* beat the best of N3 in some applications, we'll see.  For that you'd want to look ahead to 14A where the CFO mentioned on the earnings call (and he was somewhat realistic in other areas so it's more believable here) that 14A has been a positive surprise and they're actually pretty excited about it.  It's possible that if Intel gets more experience on High-NA versus TSMC that they might start having an edge later on.  Maybe.",hardware,2025-11-06 20:07:18,8
Intel,nngqma7,"> Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD   And yet we see the exact opposite here. The leaker claims these go to to 65W, even 80W TDP. There's plenty of power to hit any ST boost the silicon can handle, yet it *regresses* vs N3B ARL. And that's with a year to refine the core as well.    > If that is the case, it will only really be allowed to shine in high powered desktop and server chips   Server chips are low voltage. Even lower typically than mobile chips. High-V only matters for client.    > My understanding is that 18A will scale much better with extra power than N3 and N2   There is no reason to believe that at this time. Notice that Intel themselves are using N2 for their next flagship silicon, including desktop.   Edit: Also, for the ""pipe cleaner"" rhetoric, remember how they cancelled 20A claiming 18A was doing so well and ahead of schedule? Even more obviously a lie now.",hardware,2025-11-06 18:50:42,14
Intel,nnj8gs7,Having to roll the dice on the scheduler doesn't make things better.,hardware,2025-11-07 02:56:58,4
Intel,no3fsuj,"Since release of Windows 7, which allowed users to easily set which cores are used on per application basis in first party software (task manager).",hardware,2025-11-10 12:23:22,1
Intel,nnh9r6a,"It's been a thing for a while as a means to circumvent the Windows kernel's shitty scheduling, especially on processors with asymmetric core or chiplet arrangements. Tbf, Windows has slowly begun to catch up and run cores a little more efficiently, but some people still swear by apps like Project Lasso for manually controlling their process affinity.",hardware,2025-11-06 20:24:17,-3
Intel,nnjjmma,"Users aren't ""determining"" anything because they don't have to.   On smartphones, they've virtually zero control over their own hardware.   And on the x86 side, I don't think people are buying big.LITTLE hardware in droves, and even if they are, I doubt the multiple core clusters are their deciding factor.   And if I had a big.BABY CPU, I’d definitely be tempted to play around with it, and yes, I don’t see the big idea behind having three clusters, aside from marketing gimmicks and artificially inflated MT benchmark scores.  From what I've heard, Windows Scheduler has trouble dealing with just two clusters as it is. On Linux, the experience is, at best, tolerable.  At the very least, I've certain issues with Intel marketing these CPUs has having ""16 cores."" I know a lot of 'normies,' first hand, who have fallen prey to this deceptive marketing tactic.",hardware,2025-11-07 04:09:16,-3
Intel,nni9yge,"> E cores and LP-E cores are the same microarchitecture   In this case, they're even the same physical implementation. 100% identical to the compute cores. Also, for MTL/ARL they were neither power nor area optimized.",hardware,2025-11-06 23:31:35,6
Intel,nnhpwee,The average user is using U series,hardware,2025-11-06 21:43:43,7
Intel,nnh2hm0,">, which it will do and at better economics for Intel than Arrow Lake H.  Doesn't seem like this will be the case till the end of 26' though. At least from the earnings call a few weeks ago.",hardware,2025-11-06 19:48:23,5
Intel,nnlj5r4,"The average user runs apps like Teams, entirely off of the LPE core of Lunarlake and Pantherlake",hardware,2025-11-07 14:06:45,2
Intel,nngrbmr,"LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  4 Darkmont LPE, in therory, should be a significant improvement.",hardware,2025-11-06 18:53:58,14
Intel,nngrskq,"They are definitely more than usable if it’s anything like LNL, which basically defaults to them.  If it’s more like ARL or MTL, it’s placebo except for S0 sleep.",hardware,2025-11-06 18:56:10,10
Intel,nngrybw,"The LP cores in Meteor Lake/Arrow Lake were too weak to be usable as multithreaded boosters, this isn't the case with Lunar Lake and Panther Lake will be no different",hardware,2025-11-06 18:56:56,11
Intel,nngqpfh,"Windows are supposed to be using these LPE cores for lighter task such as web browsing or Word documentation or idling so that the they won't have to activate the rest of the cluster -> Reduced power draw and therefore good battery life. Meteor Lake originated with these but they were so slow they were pretty useless outside of idling. Lunar Lake also had 4 LPE cores functionally and we see how good those cores are, and these Panther Lake are supposed to be built on that but with additional 8 E cores for people that really value multi-core performance and to also solve the one major weakness of Lunar Lake, if it is even that major in the first place.",hardware,2025-11-06 18:51:06,9
Intel,nnhryps,>Aren‘t they even ahead of the skylake IPC by now  They're definitely ahead of Skylake now. Cougar Cove IPC vs Darkmont is gonna be like 10% better,hardware,2025-11-06 21:53:47,16
Intel,nni8z4w,> Only the 4 LP-Cores are kind of weak   Not for LNL or PTL. They're not crippled like the MTL/ARL cores. Should be full speed Darkmont. That's like 12th or even 13th gen big core.,hardware,2025-11-06 23:25:55,12
Intel,nnj1ufe,"The LPE cores being weak is probably the point anyway, or at least partially. It doesn't need to usurp an insane amount of wattage to get to full speed because it doesn't scale well, but it stays fast enough at lower wattage to default to that power profile -> Improved battery life.",hardware,2025-11-07 02:17:05,5
Intel,nnlhwmo,They are ahead of Raptor lake P cores and Zen 4 in IPC under 40W,hardware,2025-11-07 13:59:45,1
Intel,nngwvfw,Honestly I wonder if the 4 e-core cluster on the compute tile is outright more power efficient than the 4 p-cores.    I wish some reviewers would do power efficiency testing with different core count configurations enabled on LNL and ARL. Just for curiosities sake.,hardware,2025-11-06 19:20:53,14
Intel,nnihgw0,"At this point, I just hope they have a handheld chip with just 8 e cores (ensuring the CPU tile is as efficient as it gets) and a decent GPU. Maybe in time for an Intel + Nvidia chip, which could be a dream handheld chip.",hardware,2025-11-07 00:15:32,7
Intel,no19s52,Gaming laptop with iGPU should be niche. Most people just get Nvidia in their gaming laptops.,hardware,2025-11-10 01:40:12,1
Intel,nnin9fr,Apple gets their margins in upselling memory and storage they don't need to worry about the added complexity of more SKUs for the sake of price laddering,hardware,2025-11-07 00:48:54,11
Intel,nngr3we,"Perhaps fully mature was an overstatement, but it wasn’t the first product on N3B, the node was already in HVM for over a year at that point IIRC.",hardware,2025-11-06 18:52:58,11
Intel,nnhhz5t,"We all know that's a lie and 20A was a disaster. Why repeat it now? 18A may not be perfect, but it exists and these chips are comming soon.",hardware,2025-11-06 21:05:14,14
Intel,nnkdpn4,What delays? 2025 node in 2025?,hardware,2025-11-07 08:35:46,0
Intel,nnhmjz8,">BSPD in particular was supposed to help the most at high voltage  Says who?  BSPD improves signal integrity, lowers IR Drop, and helps improve density, at the expense of heat dissipation.   Idk how a technology that is known to increase heat density was supposed to *improve* fMax",hardware,2025-11-06 21:27:24,3
Intel,nngweib,"> but expected to compensate for it with very efficient frequency scaling past peak perf/W   Quite frankly, the only people ""expecting"" that seem to be people on the internet unwilling to admit Intel underdelivered with 18A. There's been no objective reason to believe that was even a target for the node. If anything, the exact opposite. This was supposed to be the node where Intel focused more on low voltage for data center and AI.    > where you rarely push past the best perf/W due to thermal and battery constraints   That is not the case. ST turbo in -H series laptops has always been high, nearly on par with the desktop chips. At the power budgets being discussed, there's plenty available to hit whatever the silicon is capable of, and Intel's never been shy about pushing the limits of thermals.    And again, when you adjust for the power/thermal envelope, you *still* see a clock speed regression vs even ARL-H. The best possible outcome for Intel is actually that they can't hit the same peak voltage but look ok at low to mid.    > Wait for the desktop/server chips before you call it   Server is typically low voltage, or mid voltage at most. It's lower down than even laptop chips. And what desktop? Intel's using N2 for their flagship chips for NVL, which should really have been an obvious indicator for how 18A fairs.    > I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.   Intel had numbers in their white paper. BSPD, all else equal, was maybe a couple percent at high V. It's not going to produce dramatically different scaling, and again, we have direct evidence to the contrary here.    In general, Intel's had a lot of process features over the years that look a lot better on slides than they end up doing in silicon.",hardware,2025-11-06 19:18:34,18
Intel,nnguuaj,"> 18A has always been *Xyz* …  There's always some excuse for Intel's stuff the last years, to accidentally NOT perform as expected, no?  > It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Hear, hear. Isn't that all to convenient …  > Wait for the desktop/server chips before you call it.  … and when is that supposed to happen? Nova Lake will be TSMC's N2.  What *Desktop* CPU-line will be on Intel 18A anyway then?",hardware,2025-11-06 19:10:51,7
Intel,nnkoi67,Give reasons why you claim this is an 'underclock' that was done 'suddenly'.,hardware,2025-11-07 10:27:15,1
Intel,nngu3yg,"Of course, Lunar was quite competitive, yet it was mainly so on performance due to Intel basically sugar-coating the living benchmark-bar out of it via the on-package LPDDR5X-8533 RAM. That OpM for sure *majorly* polished its efficiency-metrics by a mile …  Though looking back the recent years, that's how Intel always masked rather underwhelming progress on their process-technology – Hiding the actual architectural inefficiencies and shortcomings behind a invisible *wall of obfuscation* by only ever deliberately bundling it with other stuff like newer PCi-Express versions of 4.0/5.0 or newer, faster, crazy high-clocking RAM-generations.  So Lunar Lake while very strong, was mainly so due to being propped by OpM, and TSMC's processes of course.",hardware,2025-11-06 19:07:17,-10
Intel,nnhp8md,"The article claims it's ""TDP"", which would typically refer to PL1. Might be leakers getting their terminology mixed up, but I wouldn't necessarily assume a large drop in power limits.",hardware,2025-11-06 21:40:24,3
Intel,nnkfy09,AFAIK Intel was at 165W in mobile back then …,hardware,2025-11-07 08:59:02,0
Intel,nnh5e4g,"> Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?  [6%](https://web.archive.org/web/20240901235614/https://www.anandtech.com/show/18894/intel-details-powervia-tech-backside-power-on-schedule-for-2024/2)",hardware,2025-11-06 20:02:30,6
Intel,nnhouzs,"I wonder if the issues Intel's facing with 18A partially explain TSMC's very conservative approach to adopting ~~it~~ BSPD.  I image in reality, 18A (and AP's) fMax limits are almost entirely the reason for N2 in NVL-S",hardware,2025-11-06 21:38:35,4
Intel,nnh96c0,"> That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  There's no doubt that given enough resources they can fix the node to the point where it's good enough for HVM in high performance chips. It's what they did with 10nm after all but it took them several years to go from poorly yielding Cannon Lake/Ice Lake to okay Tiger Lake and good Alder Lake.  If we take the talk about 18A ""margins"" from the recent analysts call to be a euphemism for something like node yields and a stand in for when it will be ""fixed"" then we could be talking 2027 before 18A gets to that point. A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.",hardware,2025-11-06 20:21:25,8
Intel,nnh7vd8,"Yeah I know they talked about yield issues, but everything indicated that it was not a case of manufacturing defect density, but rather something else.   That would imply issues with hitting target frequencies, which as you say, they should be able to iron out in due time. That is as long as it isn't a fundamental issues, which I have a hard time believing, given how unconcerned they seemed about it.   I'm fairly confident it will be ironed out before Clearwater Forest hits mass production.",hardware,2025-11-06 20:14:54,0
Intel,nngu5ix,"There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery. It offers better voltage control and less current leakage, leading to higher efficiency at any given voltage, but especially past the point where current leakage starts becoming more of an issue, i.e. at very high frequency.  A pipe cleaner running at low frequencies is to be expected. Wait for the desktop chips and you will see P cores hitting well over 6+ GHz advertised all core boost, possibly 6.5 GHz. I wouldn't be surprised to see some users hitting 7 GHz stable.",hardware,2025-11-06 19:07:30,-1
Intel,nnlize0,The modern CPU cores schedule all cores equally with E core priority because nowadays the difference between E core and P core is less than between AMD X3D CCD and higher clocking CCD,hardware,2025-11-07 14:05:46,-1
Intel,nnhabba,"Windows Scheduler has come a long way but I mean come on, this is pretty pedantic. The commenter above is clearly not using a tool to assign cores to specific tasks they are just being annoying.",hardware,2025-11-06 20:27:06,8
Intel,nnh1new,4 skymont already seem pretty good in LNL,hardware,2025-11-06 19:44:19,10
Intel,nnh5cdj,> LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  Makes you think why Intel even went all the way to integrate those and waste precious die-space doing so …,hardware,2025-11-06 20:02:16,2
Intel,nniocav,They could be useful if they could check your email and stuff while the laptop is effectively sleeping.,hardware,2025-11-07 00:55:19,1
Intel,nnhrk78,PLT's LP-E cores take after LNL. ARL/MTL's LP-E cores design is dead,hardware,2025-11-06 21:51:48,3
Intel,nnh5ttp,I hope those are powerful enough to eventually act as the low-power booster these were once supposed to.,hardware,2025-11-06 20:04:40,1
Intel,nnh666b,> Meteor Lake originated with these but they were so slow they were pretty useless outside of idling  The last info I had on the back of my mind was that these couldn't even be associated by the user and were basically reserved for Windows itself. Is that still the case now?,hardware,2025-11-06 20:06:23,1
Intel,nnj4lq1,"Yeah after being reminded that the LPE cores are also Darkmont like the E-Cores I looked up the layout in detail and the differences is simply the P and E cores are on a large Cluster with L3 Cache and a fast ring-bus, but the LPE cores are seperated from that Ring-bus and don't have any L3 Cache (just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster).     They are for sure also much more limited in power-budget and frequency, but they can be very fast in general. The main usage is being able to disable the large cluster in Idle or low cpu usage to heavily reduce power draw.     The communication in between the clusters is much slower. So anything that relies on synchronisation and communication between cores is only really fast (latency-wise) if it is kept within the same cluster.      The LPE cores therefore can be very fast actually, but only if the task stays within the cluster. Modern demanding games will therefore 99% soley run on the large cluster, because they would run slower if they spread across all of them. The small cluster could be used to offload background tasks to themselves (if the powerlimits aren't hit), which could at least improve 1% lows and fps stability.",hardware,2025-11-07 02:33:41,5
Intel,nngxyok,"It's not just more power efficient, but also much faster at low power which you kinda want since you'd be mostly GPU bottlenecked. The good thing is that they've made that part of their thread director tech.",hardware,2025-11-06 19:26:15,10
Intel,nnlikjx,Yes it is. Memory uses power and IPC of LPE should be past Zen 3,hardware,2025-11-07 14:03:29,2
Intel,nnkc1a7,Intel makes e-core only CPUs (N100 etc) but at this point they're based on several gen-old e-core design. They're good enough for what they are so they might not update them for a while still though.,hardware,2025-11-07 08:18:15,1
Intel,nnjcalp,"even 4 e-cores is enough, dont forget we use to pair up i7-7700K with GTX1080/1080Ti.  4 e cores + even stronger iGPU probably a better combo for handheld.",hardware,2025-11-07 03:21:21,8
Intel,nnoc1fy,Apple absolutely does price ladder their SoCs.,hardware,2025-11-07 22:41:38,1
Intel,nngy7u4,"True. But the node itself may be kinda mid. It seemed much, much more complex than N3E, and it's not as if the node was any sort of highlight in the products Apple used it in.   Going back to A17 reviews and such, there were serious questions presented about N3B vs N4P.",hardware,2025-11-06 19:27:29,16
Intel,nnhibo4,I'm fairly sure that's what u/ProfessionalPrincipa was also implying.,hardware,2025-11-06 21:06:56,7
Intel,nnhovl3,> We all know that's a lie and 20A was a disaster. Why repeat it now?   Probably in response to all the people assuming there must be some kind of upside.,hardware,2025-11-06 21:38:39,3
Intel,nnkl8kv,"It was supposed to be a 2024 node. And given the perf downgrade and admitted yield problems, seems more like they're only delivering the originally promised metrics in '26 or even '27.",hardware,2025-11-07 09:54:38,6
Intel,nnl32z1,"> What delays? 2025 node in 2025?  What del— *Seriously now!?* Where you living under a rock the last years by any chance? o.O  18A is neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: *'cause Yields again!*) will be 1H26 and most likely even end of first half of 2026.  So it's a 2H24-node, which Intel is only able to offer actual volume basically +1–2 years later. That's called *»delay«*.  **Edit:** And not to mention the actual massive performance- and metric-regression u/Exist50 pointed out already. 18A is basically 20A in disguise. Since 20A wasn't actually knifed, but just relabeled as ""18A"" instead.",hardware,2025-11-07 12:29:34,-1
Intel,nnhohos,> Says who?   Intel. It was part of their own published results about PowerVia on the Intel 4 test chip. The gains range from ~negligible at low-V to around 6% at high-V.    Also explains why TSMC is not adopting it in the same way.,hardware,2025-11-06 21:36:46,8
Intel,nnkdym7,Nova Lake is full on N2?,hardware,2025-11-07 08:38:24,0
Intel,nnocn3l,"It's literally a regression from the prior gen, for a node that was supposed to be ""leadership"", mind you.    I don't know why it's hard to acknowledge that 18A is simply underperforming.",hardware,2025-11-07 22:45:03,1
Intel,nngzzny,"Nah, credit where credit is due. LNL made a ton of fundamental design changes that PTL should also benefit from. Yes, they also benefitted a lot from both having an actually decent node and on-package memory, and no, they're not on par with the likes of Apple or Qualcomm, but merely making ARL monolithic on N3 would not have delivered these gains.",hardware,2025-11-06 19:36:12,11
Intel,nnhs780,They definitely have their terminology mixed up. There's no chance PTL-H has an 65W/80W PL1,hardware,2025-11-06 21:54:56,4
Intel,nnpn3zz,Which was never the PL1 but rather the PL2.,hardware,2025-11-08 03:40:00,3
Intel,nnhxmr5,"Didn't TSMC delay their BSPD node too? Ik they changed it from appearing in N2P vs A16, but unsure if there was a timeline shift in that as well.   Honestly, what's going on with N2P/N2/A16 seems to be kinda weird, timeline wise.",hardware,2025-11-06 22:22:47,5
Intel,nnhappm,2027 just seems like hitting baseline parametric yields. Being able to get to an actual improvement in terms of performance is a whole other thing. Not an easy road.,hardware,2025-11-06 20:29:05,3
Intel,nnkqty9,">A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.  How it can be 24 product in 27 when it was 25 node in 25 with products on shelfs in january 26, rofl?",hardware,2025-11-07 10:49:31,0
Intel,nnhg56o,Intel themselves claim they won't be hitting industry acceptable yield till 2027 for 18A.,hardware,2025-11-06 20:56:18,3
Intel,nngyuum,"> There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery.    No, it's not. The main long term advantage of BSPD is to reduce the pressure on the metal layers as they do not scale like logic does. Better PnP is another advantage, but secondary. No one cares much about high-V these days.    > A pipe cleaner running at low frequencies is to be expected   As a reminder, ARL-20A was supposed to be the pipe cleaner, and they claimed they cancelled it because 18A was doing so well! PTL was supposed to be the volume product, a year after 18A was nominally supposed to be ready.    > Wait for the desktop chips   What desktop chips? PTL-S was cancelled long ago, and for NVL, they're moving the good compute dies (including desktop) back to TSMC on N2. A decision which should demonstrate the node gap quite clearly...   Also, ST boost for -H chips is in the same ballpark as desktop ones. They're already at the high end of the curve.",hardware,2025-11-06 19:30:37,11
Intel,nnoxxwo,"They aren't the same though, hence the roll of the dice to see if you actually get the performance you paid for.",hardware,2025-11-08 00:53:15,2
Intel,nnh7b83,Because they still improve battery life under very light loads.,hardware,2025-11-06 20:12:05,8
Intel,nnj1km0,Yeah I think you had to utilize tool such as Project Lasso. I tried to assess the LPE cores to HWINFO64 and it was rather painful to use so I understand why these cores are never used otherwise lol.,hardware,2025-11-07 02:15:28,1
Intel,nnj555b,"Yeah. The efficiency and overall battery life will probably comes down to how Windows manages threads, which seems to be pretty decent now with Lunar Lake seeing as they got pretty good battery life with the same design philosophy.",hardware,2025-11-07 02:36:58,2
Intel,nnkn219,"> just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster  It's also accessible by the large cluster, fyi.",hardware,2025-11-07 10:12:52,2
Intel,nnkd975,"For those games back in the day, yeah. But modern games can and do use more than 4 cores.   Depends how low end/power you want to go, of course. 4 cores + eGPU cuold be enough too depending on your games: [https://youtu.be/XCUKJ-AgGmY?t=278](https://youtu.be/XCUKJ-AgGmY?t=278)",hardware,2025-11-07 08:30:54,7
Intel,nnn8cyi,"The e-cores are unlikely to be hyperthteaded though. 8 e-cores gives you 8 threads just like a 7700K did, though none of them share cores. It feels like a sweet spot for a handheld in 2025 imho.   This is while AMD competitors use 8c/16t of full Zen. An 8 e-core solution would remain weaker but more than sufficient on the CPU side, while allowing enough power for what truly matters (a capable GPU). If paired with an Nvidia iGPU, that could be the perfect handheld chip, if their partnership produces one like that.",hardware,2025-11-07 19:12:43,3
Intel,nnhq813,"Those ""people"" are either bots or trolls. No sane person thinks Intel is crushing it with 18A.",hardware,2025-11-06 21:45:18,7
Intel,nnkrne6,"Whats the name of the close the gap initiative?  5 nodes in 4 years... announced in 2021  2021+4=2025  You can even read 3rd party articles from 2021 talking about 18A in 2025.  https://www.tomshardware.com/news/intel-process-packaging-roadmap-2025   >Intel didn't include it in the roadmap, but it already has its next-gen angstrom-class process in development. 'Intel 18A' is already planned for ""early 2025"" with enhancements to the RibbonFET transistors.",hardware,2025-11-07 10:57:07,1
Intel,nno64t2,">neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: 'cause Yields again!) will be 1H26 and most likely even end of first half of 2026.  How it aint 25 node when there will be products on shelves in january 26?",hardware,2025-11-07 22:08:46,2
Intel,nnljpf8,FMax at same power consumption is higher true. But the temperature is just too high. Both temperature and power consumption are important considerations,hardware,2025-11-07 14:09:47,2
Intel,nnklaca,"High end NVL is N2, low end is 18A. At least for compute dies.",hardware,2025-11-07 09:55:07,3
Intel,nnl268a,"That's what we know so far, yes. At least the performance-parts. The lower end is supposed to be 18A, I guess?",hardware,2025-11-07 12:23:00,0
Intel,nnhcrle,"> Nah, credit where credit is due.  I said verbatim, that Lunar Lake was quite competitive, and I meant it, unironically. It took long enough.  > LNL made a ton of fundamental design changes that PTL should also benefit from.  I haven't disputed that —  The modular concept was surely ground-breaking for Intel, and urgently needed!  Though, it's kind of ironic, how Intel all by itself proved themselves liars, when it took them basically +6 years since 2017, for eventually coming up with only a mere chiplet-copycat and their first true disintegrated chiplet-esque design …  For a design, which *in their world-view*, Intel was working on their tiles-approach already since a decade, which is at least what they basically claimed when revealed by 2018 – I called that bullsh!t the moment I first heard it. Surprise, surprise, they again straight-up *lied* about it …  They most definitely did NOT have had worked on anything chiplets/tiles before, if it took them *this* long.  Just goes to show how arrogant Intel was back then, letting AMD cooking their Zen in complete silence since 2012/2013, for their later Ryzen. *Still boggles my mind, how Intel could let that happen* …  > Yes, they also benefitted a lot from both having an actually decent node and on-package memory […]  In any case, we can't really deny the fact, that Intel basically cheated on Lunar Lake using OpM, eventually creating a halo-product, which ironically was quite sought after, but yet expensive asf to manufacture.  If AMD would've been to cheat like that (using OpM) in a mobile SKU, while dealing some marked cards in such a underhanded manner, it would've been ROFL-stomped Lunar Lake …",hardware,2025-11-06 20:39:31,-2
Intel,nni6v1y,"Willing to believe that, though then I have to wonder why it would be so hard to cool at such a substantially reduced PL2. Also if/how they could pitch this as an HX replacement.",hardware,2025-11-06 23:13:41,3
Intel,nnq97pr,"Yes, I think it was 45- or 65W-parts PL1-wise. The spread was still obscene and outright insane.",hardware,2025-11-08 06:42:53,0
Intel,nnhgp5t,I'm excited for 18A-P. Intel subnode improvements have always seemed to bring decent uplifts (Intel 10SF being a notable example).,hardware,2025-11-06 20:58:58,2
Intel,noaeih3,"The difference is no longer stark. As I said, its now smaller than between 2 CCDs in the most popular CPU line ever made in actual raw performance. ie it's not something entirely unique at the scale the differences are",hardware,2025-11-11 14:51:59,1
Intel,nnheh7k,"How even, if these weren't even used with MTL!?",hardware,2025-11-06 20:48:06,-2
Intel,nnkfcmn,"Yup, pretty much paper-cores for marketing-reasons alone basically.",hardware,2025-11-07 08:52:54,1
Intel,nnlies1,We already have games tested on Skymont E cores. They are very fast,hardware,2025-11-07 14:02:35,4
Intel,nnoujai,"Yes we know modern games can use more cores, but having smaller amount of cores means saving power budget. Those power consumption saved are better off use to juice iGPU.   Lets not forget the E-Cores here are skymont, not skylake. Skymont is definitely more powerful cores. The exact same reason why i5-7600K is more power than i7-2600K.  given how iGPU for handheld are no way near GTX1080 yet. I think it is better to pump more juice for better iGPU.",hardware,2025-11-08 00:32:08,1
Intel,nnhveh1,Go to the intel stock subreddit lol,hardware,2025-11-06 22:11:05,9
Intel,nni71qc,"You'd be surprised how many people are willing to take Intel at their word on it.  Just look at previous threads here. Every time Intel posts nonsense slides, lot of people come crawling out of the woodwork.",hardware,2025-11-06 23:14:46,1
Intel,no3ffkc,"They are doing about as much as the ""sane"" people expected from 18A.",hardware,2025-11-10 12:20:33,0
Intel,nnku399,"> 5 nodes in 4 years... announced in 2021  It was supposed to start with Intel 7 in '21 and end with 18A readiness in H2'24.   https://img.trendforce.com/blog/wp-content/uploads/2023/10/17144859/intel-4-years-5-nodes.jpg  And now in the way end of '25, we're getting something more like what they originally promised for 20A, *if that*, and now they're saying yields won't be ""industry standard"" until as late as 2027. It's a disaster by any objective standards. Their failure with 18A literally got the CEO fired...",hardware,2025-11-07 11:19:07,4
Intel,nnpmsrt,"> How it aint 25 node when there will be products on shelves in january 26?  It seems, you don't actually grasp the concept of the term »difference«. *2025 and 2026 is actually NOT the same!*",hardware,2025-11-08 03:37:46,1
Intel,nnkm46i,so the igpu is propably tsmc too?  do we know if thats because of poor performence or poor manufacturing capazity?,hardware,2025-11-07 10:03:27,1
Intel,nni73m8,I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   I think they'll probably have ARL-R for HX,hardware,2025-11-06 23:15:04,3
Intel,nnqeasg,"Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage. Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.",hardware,2025-11-08 07:33:02,1
Intel,nnhrg1s,"The LP-E cores were a failure in MTL/ARL's design.  They're very useful in LNL and that trend follows with PTL/NVL.  They're definitely not just ""placebo cores"" - LNL uses them more often then the P cores and they can be activated and used in all core workloads as well.",hardware,2025-11-06 21:51:15,5
Intel,nnhp1ul,"They were, just not as often as Intel would have liked.",hardware,2025-11-06 21:39:29,3
Intel,nnkr81u,"For MTL and ARL, yes. For LNL and PTL though, I haven't test them out personally but seeing the battery results of LNL chips made me think that the LPE cores of those things seem to be legit.",hardware,2025-11-07 10:53:10,1
Intel,nnm0kgv,"Oh year, right, I think I missed (or forgot) that. Darkmont should be even better but I'm guessing not by that much.",hardware,2025-11-07 15:37:40,2
Intel,nnhvpi9,Kinda making my point.,hardware,2025-11-06 22:12:40,10
Intel,nnsiatm,"Bro xd  Node readiness and product readiness and product on shelves are 3 different date  Node must be ready before product, product must be ready before being on shelves.  Officially 18A was ready around Q2 25",hardware,2025-11-08 16:59:43,4
Intel,nnkmum2,"> so the igpu is propably tsmc too?  18AP, iirc. Good *enough* (or at least, assumed to be when originally planned) not to be worth the extra cost of TSMC.  > do we know if thats because of poor performence or poor manufacturing capazity?  Poor performance. Intel can't afford to be constantly a node behind. It's just too big of a gap.",hardware,2025-11-07 10:10:47,3
Intel,nni82fk,"> I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   But that's exactly it. They said 65W vs 80W was in relation to cooling challenges, but either is more than sufficient for max ST boost. Idk, maybe reading too much into too little information.    > I think they'll probably have ARL-R for HX   At one point it sounded like they wanted to pitch PTL as a partial replacement. Guess it's not good enough though.",hardware,2025-11-06 23:20:38,3
Intel,nnqhka1,"> Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage.  Of course, I already knew that. Binned Desktop.  > Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.  Well, up until 2018/2019/2020 Intel actually didn't really uncapped any of them and those parts were hard-limited to only pull their max 45–65W TDP.  Only later on when they were trying to hold pace with AMD, Intel insanely increased the TDP to pull to insane numbers (90W, 135W, 165W) … Other than that and prior, you had to resort to modded BIOS.  So until even Apple burned their thick skin on some i9 (**Cough* i9-9980HK in the 16"" MacBook Pro 2019), you didn't had such insane TDPs to begin with anyway (except for BIOS-mods).",hardware,2025-11-08 08:06:08,0
Intel,nnkfnwq,"As said, I owned a MTL-machine back then, and back then you couldn't even associated these cores after all, since you couldn't pin anything on them – So yes, *back then*, these were placebo-cores and they were in fact pretty much 'on paper-cores for marketing-reason' and basically useless.  Seems Intel managed to improve their performance a lot and make them actually useful, which is good!",hardware,2025-11-07 08:56:07,0
Intel,nnl1o21,"Yes, MTL's LPE-cores were basically a dud, LNL was fairly workable and PTL will be hopefully potent.",hardware,2025-11-07 12:19:17,1
Intel,nnhvsow,haha,hardware,2025-11-06 22:13:07,5
Intel,nnxlj8j,> Officially 18A was ready around Q2 25  Which is still at least 4 quarters *past* its due-date actually …,hardware,2025-11-09 14:16:26,1
Intel,nnqk8rl,"That's not strictly true, the (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  https://www.ultrabookreview.com/27215-asus-rog-g703gx-review/#a5  >That comes with a few drawbacks, though, and one of the most important is the noise development. Asus provides three different power modes for this laptop, which you can use to juggle between performance, thermals, and noise:  Silent – CPU limited at 45 W and 4.2x multiplier, GPU limited at 140 W, fans only ramp up to about 40-43 dB in games; Balanced – CPU limited at 90 W and 4.2x multiplier, GPU limited at 140 W; Turbo – CPU limited at 200 W and 4.7x multiplier, GPU limited at 200 W, fans ramp up to 55-56 dB in games.  It was always up to the laptop OEM to configure the power limit and there was no hard cap enforced by Intel.",hardware,2025-11-08 08:33:24,1
Intel,nnndfa5,"The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, negating the entire point of them.  In addition, MTL/ARL had the LP-E cores on the SoC tile which made them functionally useless for full nT load.  LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks *can* stay entirely within the LP island, *and* they're on the same compute tile, so they're also used in full nT workloads.",hardware,2025-11-07 19:38:34,3
Intel,nnqndqo,"> That's not strictly true …  Yes it actually is.   > The (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  Dude, you're basically confirming exactly what I wrote, by literally picking a **i9 9980HK**-equipped notebook of 2019, which is even exactly the very infamous SKU I was talking about …  I was talking exactly on (among else) that very CPU, even Apple couldn't tame and had to undervolt (still without being able to prevent the later sh!t-storm) until eventually abandoning Intel altogether.  ---- All I'm saying is, all the years prior with only quad-core, Intel \*never\* (nor any OEMs for that matter) went above and beyond the official TDP – The only lone exception from this, were those super-bulky Schenker Desktop-replacements.  That only suddenly changed by 2017–2019, when Intel suddenly increased core-count in mobile swiftly from Quad- to Hexa- and ultimately Octa-cores, while pushing the TDP in quick succession  into insane territory.  *That was the time, Asus took about a year to manage applying liquid-metal en masse, for Intel-notebooks!*  You are right with your sample here, yet you only confirm what I said before. Only past quad-cores it was.",hardware,2025-11-08 09:05:43,0
Intel,nnprgvn,"> The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  Do you happen to know, if that (ring-bus related) was the same principle on MTL?  > The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, **negating the entire point of them**.  Yeah, talking about throwing some completely untested sh!t onto the market, irregardless of the fact, if the product makes sense or not — A true classic, I'd say. Just Intel being Intel.  Since dropping those LPE-cores on MTL, would've actually made the mask and thus needed die-space *smaller*, which in turn would've actually *increased* their yields \*and\* in return profit-margins already …  But muh, benchmark bars and ""AMD has moar corez!!"", I guess.  > LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks can stay entirely within the LP island, and they're on the same compute tile, so they're also used in full nT workloads.  Is there any greater penalty (latency or cache-flush-wise) for moving threads off the LP-island to P-cores?  *Thanks for the insides so far though!* I can't really see through anymore to all these constant arch-changes.",hardware,2025-11-08 04:12:06,1
Intel,nmflglf,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",hardware,2025-10-31 20:39:13,30
Intel,nmdl0rr,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,hardware,2025-10-31 14:32:50,56
Intel,nmm72sv,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,hardware,2025-11-01 23:31:51,2
Intel,nmf7n07,They said their high idle power is an architecture issue so they can't fix that,hardware,2025-10-31 19:25:22,16
Intel,nme3l5e,God forbid Intel supports Day 1 GPU drivers longer than 5 years,hardware,2025-10-31 16:03:57,-21
Intel,nmfdha7,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",hardware,2025-10-31 19:56:31,12
Intel,nmvlitt,So just like AMD then.,hardware,2025-11-03 13:41:12,2
Intel,nmfexoy,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",hardware,2025-10-31 20:04:15,26
Intel,nmhjwz4,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",hardware,2025-11-01 04:34:17,5
Intel,niqvrj6,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",hardware,2025-10-10 09:51:49,30
Intel,nis4lzk,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",hardware,2025-10-10 14:45:18,11
Intel,nisn7g8,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,hardware,2025-10-10 16:15:59,4
Intel,niz80tx,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,hardware,2025-10-11 18:17:30,1
Intel,njgcnqq,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",hardware,2025-10-14 14:31:48,1
Intel,njxoogl,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",hardware,2025-10-17 07:35:47,1
Intel,nir35ti,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",hardware,2025-10-10 10:58:36,22
Intel,nir6blp,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",hardware,2025-10-10 11:23:30,16
Intel,nis0as0,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,hardware,2025-10-10 14:23:44,4
Intel,nisagox,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",hardware,2025-10-10 15:13:44,3
Intel,njr6343,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",hardware,2025-10-16 06:29:51,1
Intel,nit88y1,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",hardware,2025-10-10 17:58:19,7
Intel,njxozxf,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,hardware,2025-10-17 07:39:00,1
Intel,niruidx,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,hardware,2025-10-10 13:53:56,5
Intel,nircpja,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,hardware,2025-10-10 12:08:58,31
Intel,nisb3yn,That's not what Peterson was talking about context wise when he addressed this in the video.,hardware,2025-10-10 15:16:53,4
Intel,nito8rq,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",hardware,2025-10-10 19:19:56,-4
Intel,nis0ezd,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),hardware,2025-10-10 14:24:18,16
Intel,nirfv6f,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",hardware,2025-10-10 12:29:32,17
Intel,niw1ezc,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",hardware,2025-10-11 04:09:31,5
Intel,njr69nx,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",hardware,2025-10-16 06:31:38,1
Intel,nisec0d,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",hardware,2025-10-10 15:32:43,1
Intel,nizcijr,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,hardware,2025-10-11 18:41:45,1
Intel,nitxcqr,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",hardware,2025-10-10 20:07:36,1
Intel,njgdemn,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,hardware,2025-10-14 14:35:38,1
Intel,nitzoza,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",hardware,2025-10-10 20:19:35,1
Intel,nj6xna9,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,hardware,2025-10-13 00:16:37,2
Intel,nj7rqtc,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",hardware,2025-10-13 03:27:42,1
Intel,nifgt95,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",hardware,2025-10-08 14:41:36,39
Intel,nighktm,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",hardware,2025-10-08 17:39:48,31
Intel,nihvtct,MLID must have an aneurysm seeing the guy still employed at Intel,hardware,2025-10-08 21:50:09,14
Intel,nifmhnd,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",hardware,2025-10-08 15:09:23,31
Intel,nihklk5,Igpus not discrete gpus,hardware,2025-10-08 20:51:31,12
Intel,nigirli,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",hardware,2025-10-08 17:45:26,11
Intel,nii5519,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,hardware,2025-10-08 22:44:00,9
Intel,nifuaux,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,hardware,2025-10-08 15:47:29,8
Intel,niis0eh,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,hardware,2025-10-09 01:01:50,5
Intel,nigp77c,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,hardware,2025-10-08 18:16:46,13
Intel,nigl085,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",hardware,2025-10-08 17:56:04,0
Intel,nilqzn1,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,hardware,2025-10-09 14:41:05,3
Intel,nig3u1g,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,hardware,2025-10-08 16:33:52,18
Intel,nijp5fy,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,hardware,2025-10-09 04:46:48,4
Intel,nigpqtx,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",hardware,2025-10-08 18:19:29,9
Intel,nihx213,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",hardware,2025-10-08 21:56:59,3
Intel,nigh9jb,You stole what I was going to say... take my upvote.,hardware,2025-10-08 17:38:17,6
Intel,nijrll5,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",hardware,2025-10-09 05:07:37,9
Intel,niixhey,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",hardware,2025-10-09 01:35:25,72
Intel,nij1t7l,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,hardware,2025-10-09 02:02:14,31
Intel,nijk1dw,that's insane vram density,hardware,2025-10-09 04:05:01,10
Intel,nijb3xq,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",hardware,2025-10-09 02:59:53,16
Intel,niiwt1u,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-09 01:31:12,2
Intel,nimo4dq,Is there a fork of chrome that runs on gpus,hardware,2025-10-09 17:24:22,2
Intel,nindfe4,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",hardware,2025-10-09 19:31:05,2
Intel,nikln6u,but is that faster than a single 5090?,hardware,2025-10-09 10:10:09,2
Intel,nik40r8,Is this enough VRAM for modern gaming?,hardware,2025-10-09 07:06:00,-2
Intel,nil0agd,Nvidia: ill commit s------e,hardware,2025-10-09 12:07:51,-7
Intel,nil8gj0,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",hardware,2025-10-09 12:59:34,17
Intel,no4mb2m,"I doubt we'll see any UALink this year, but perhaps  on the 160GB  Crescent Island card next year.  Intel hasn't announced anything, but it seems obvious.",hardware,2025-11-10 16:22:54,1
Intel,nikh8oe,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",hardware,2025-10-09 09:25:55,-14
Intel,nij5zhc,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",hardware,2025-10-09 02:27:37,43
Intel,niml0xn,I don't think servers are supposed to stay idle for long.,hardware,2025-10-09 17:09:18,2
Intel,niqkqmc,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",hardware,2025-10-10 07:57:46,2
Intel,nijeto3,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,hardware,2025-10-09 03:25:25,44
Intel,nilc2tl,Could that make it very cost effective for any particular use cases?,hardware,2025-10-09 13:21:04,4
Intel,nim2kjx,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",hardware,2025-10-09 15:38:17,4
Intel,nij8lcp,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,hardware,2025-10-09 02:43:33,30
Intel,nijhbrl,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",hardware,2025-10-09 03:44:18,3
Intel,nilfpel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",hardware,2025-10-09 13:41:48,4
Intel,nin9mev,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",hardware,2025-10-09 19:11:33,3
Intel,nijt662,At least its a human hallucination and not AI hallucination.,hardware,2025-10-09 05:21:24,16
Intel,nik407y,Can also be bad translation.,hardware,2025-10-09 07:05:52,2
Intel,nijj2tk,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",hardware,2025-10-09 03:57:40,15
Intel,nilgnxj,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",hardware,2025-10-09 13:47:09,2
Intel,nili8e7,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",hardware,2025-10-09 13:55:46,4
Intel,niokcbz,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,hardware,2025-10-09 23:22:33,12
Intel,nilzeoj,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",hardware,2025-10-09 15:22:43,26
Intel,nime2oj,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",hardware,2025-10-09 16:35:12,18
Intel,nilf97b,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-10-09 13:39:16,1
Intel,nim7yeq,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,hardware,2025-10-09 16:04:46,-4
Intel,nilnsw1,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",hardware,2025-10-09 14:24:56,-24
Intel,nionxww,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,hardware,2025-10-09 23:43:45,5
Intel,nimlaz8,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",hardware,2025-10-09 17:10:41,4
Intel,nimhv2z,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,hardware,2025-10-09 16:53:51,5
Intel,nimkx2o,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",hardware,2025-10-09 17:08:47,22
Intel,nin877l,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,hardware,2025-10-09 19:04:21,8
Intel,nir42up,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,hardware,2025-10-10 11:05:57,6
Intel,nirvt3d,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,hardware,2025-10-10 14:00:42,2
Intel,nixqbsp,"Yeah prob, but by how much. Looking forward to the post CES reviews and Xe3 better be a major leap over Xe2.",hardware,2025-10-11 13:25:20,1
Intel,nimkw5p,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,hardware,2025-10-09 17:08:39,6
Intel,nilrz1f,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",hardware,2025-10-09 14:45:58,22
Intel,nilwkhh,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",hardware,2025-10-09 15:08:45,11
Intel,nioq01t,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,hardware,2025-10-09 23:56:01,5
Intel,nirvexf,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",hardware,2025-10-10 13:58:40,4
Intel,nimlpyk,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",hardware,2025-10-09 17:12:42,5
Intel,nimm20x,Celestial was based on Xe3p.,hardware,2025-10-09 17:14:19,6
Intel,niokiba,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",hardware,2025-10-09 23:23:33,1
Intel,niy2dcz,I don't think Mac minis support hardware AV1 _encoding_. Apple only recently supported hardware AV1 _decoding_ so their devices aren't the best suited to AV1 media,hardware,2025-10-11 14:38:03,3
Intel,nimmzhl,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,hardware,2025-10-09 17:18:48,13
Intel,nirvxip,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",hardware,2025-10-10 14:01:21,3
Intel,nin3vrr,That's not what my colleague's say,hardware,2025-10-09 18:42:31,3
Intel,nilz2fy,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,hardware,2025-10-09 15:21:03,25
Intel,nim3npl,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",hardware,2025-10-09 15:43:35,-13
Intel,ninrarq,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",hardware,2025-10-09 20:40:49,-4
Intel,niy1tvt,">Intel can't abandon GPU development in the short term  Well Intel now uses the same architecture across both their iGPU and dGPU, so even if their dGPU products get killed, iGPU demand(indirectly, through Core Ultra processor demand) can fund continued GPU architecture dev. Intel still has the majority market share in the iGPU market, so they have the demand to justify continued R&D into GPU dev for iGPU alone.",hardware,2025-10-11 14:35:03,1
Intel,nj09dos,Xe3p is a significant architectural advancement says Tom Petersen.,hardware,2025-10-11 21:46:07,2
Intel,nim48wq,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",hardware,2025-10-09 15:46:27,-14
Intel,nim6va1,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",hardware,2025-10-09 15:59:21,7
Intel,nimks6h,"So much buzzwords, yet it sounds like a stroke.  You need help.",hardware,2025-10-09 17:08:07,7
Intel,nip4uap,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",hardware,2025-10-10 01:23:47,4
Intel,nim4t3o,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",hardware,2025-10-09 15:49:13,13
Intel,nino52j,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",hardware,2025-10-09 20:24:55,-2
Intel,nirwlsr,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,hardware,2025-10-10 14:04:53,1
Intel,ninor5p,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",hardware,2025-10-09 20:27:56,-7
Intel,ninre0y,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",hardware,2025-10-09 20:41:15,5
Intel,nixpud1,Because it's most likely still the same foundation. Xe3 just have additional optimizations and HW blocks along the lines of OMM (confirmed) and whatever new tech Intel decides is worth pushing for higher end parts.    But perhaps it's more of a RDNA 3.5 situation. Some early Druid tech backported to XE3P.    Yeah I have zero clue just guessing xD,hardware,2025-10-11 13:22:13,1
Intel,ninu5gl,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",hardware,2025-10-09 20:55:03,-1
Intel,nio14ia,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",hardware,2025-10-09 21:31:45,8
Intel,nfxvgt2,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,hardware,2025-09-24 13:00:51,30
Intel,nfxs3mr,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",hardware,2025-09-24 12:41:08,38
Intel,nfxrqaw,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,hardware,2025-09-24 12:38:53,15
Intel,nfz9psy,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",hardware,2025-09-24 17:11:03,7
Intel,nfyjkgh,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",hardware,2025-09-24 15:06:03,4
Intel,nfxz0x9,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,hardware,2025-09-24 13:20:54,2
Intel,nfz9315,we are witnessing the downfall of pc gaming in real time,hardware,2025-09-24 17:08:01,5
Intel,ng021ru,Why should they? They are going to buy NVidia GPUs for everything now.,hardware,2025-09-24 19:27:35,2
Intel,nfxotxk,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-09-24 12:20:56,1
Intel,nfxyif8,This could be awesome more completion The better,hardware,2025-09-24 13:18:01,1
Intel,ng09fp7,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,hardware,2025-09-24 20:03:42,0
Intel,nfxyqy3,"Multi-Post News Generation, with three articles interpolated per source.",hardware,2025-09-24 13:19:21,43
Intel,nfxswgf,I'm excited for Intels new GPU,hardware,2025-09-24 12:45:57,7
Intel,ng28c6g,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",hardware,2025-09-25 02:43:17,9
Intel,ng09kyr,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",hardware,2025-09-24 20:04:24,3
Intel,nfy71ww,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,hardware,2025-09-24 14:03:45,-6
Intel,nfxzd6m,Competition *,hardware,2025-09-24 13:22:47,1
Intel,nfydv2b,Obligatory article quoting reddit post quoting another article quoting original reddit post.,hardware,2025-09-24 14:38:06,11
Intel,nfy8e3s,"Fake frames, fake articles! /s",hardware,2025-09-24 14:10:35,7
Intel,ng9nabn,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,hardware,2025-09-26 07:24:34,5
Intel,nfy9ky6,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",hardware,2025-09-24 14:16:39,5
Intel,ngt9vbz,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",hardware,2025-09-29 11:53:54,1
Intel,ngt9z2u,"patents expire after 15 years, they will have to share it then.",hardware,2025-09-29 11:54:37,1
Intel,nfykmyd,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,hardware,2025-09-24 15:11:13,11
Intel,nfyiwdp,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",hardware,2025-09-24 15:02:46,10
Intel,ng9ndqw,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,hardware,2025-09-26 07:25:31,3
Intel,ngta2hh,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,hardware,2025-09-29 11:55:17,1
Intel,ngtlhim,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",hardware,2025-09-29 13:07:31,1
Intel,ngtvb8k,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",hardware,2025-09-29 14:02:39,1
Intel,ng9nh90,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,hardware,2025-09-26 07:26:29,3
Intel,nf42stl,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,hardware,2025-09-19 18:12:40,35
Intel,nf2t8pz,"Okay then, what's the Xe GPU roadmap looking like then?",hardware,2025-09-19 14:34:57,75
Intel,nf43nvh,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",hardware,2025-09-19 18:16:54,13
Intel,nf4q7tq,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,hardware,2025-09-19 20:09:01,11
Intel,nf31f8h,They barely lived on before the deal.,hardware,2025-09-19 15:13:57,35
Intel,nf53r9t,"It'll live on our hearts, yes.",hardware,2025-09-19 21:17:43,9
Intel,nf3f8y9,Lol if you believe that I have a bridge to sell you in Brooklyn,hardware,2025-09-19 16:20:00,15
Intel,nf7xz74,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",hardware,2025-09-20 09:42:21,3
Intel,nf8zcs6,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",hardware,2025-09-20 14:09:11,1
Intel,nfacpei,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,hardware,2025-09-20 18:18:08,1
Intel,nf8psx7,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,hardware,2025-09-20 13:16:21,0
Intel,nf605ki,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",hardware,2025-09-20 00:24:16,7
Intel,nfcc5gm,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",hardware,2025-09-21 00:57:40,3
Intel,nf2xqty,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",hardware,2025-09-19 14:56:28,31
Intel,nf53uu0,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,hardware,2025-09-19 21:18:14,6
Intel,nf69muk,"Always selling out actually, they just can't produce that much",hardware,2025-09-20 01:23:31,11
Intel,nf3gd7v,And I have another if you think nVidia is capable of keeping this deal running for that long...,hardware,2025-09-19 16:25:26,4
Intel,nfho1ld,Intel will hopefully split their fab business from the rest of the company either way.,hardware,2025-09-21 21:10:12,1
Intel,nf3itwc,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",hardware,2025-09-19 16:37:23,-22
Intel,nfe3v2w,> they just can't produce that much  because they are losing money on them,hardware,2025-09-21 09:31:45,5
Intel,nfkj39o,Trade bridges.,hardware,2025-09-22 09:21:12,1
Intel,nf6x83x,Intel's arc is dead with or without nvidia deal.,hardware,2025-09-20 04:05:06,1
Intel,nf3naf4,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",hardware,2025-09-19 16:58:38,22
Intel,nf41fhg,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,hardware,2025-09-19 18:05:57,15
Intel,nf5ky00,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",hardware,2025-09-19 22:54:47,7
Intel,nfjjjlm,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",hardware,2025-09-22 03:50:01,1
Intel,nf72gvm,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",hardware,2025-09-20 04:47:19,1
Intel,nf5bseq,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",hardware,2025-09-19 22:01:34,4
Intel,nf5z77h,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",hardware,2025-09-20 00:18:20,3
Intel,nfjkc77,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,hardware,2025-09-22 03:55:54,2
Intel,nf73hs7,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",hardware,2025-09-20 04:55:37,0
Intel,nf6x2qk,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",hardware,2025-09-20 04:03:55,1
Intel,nflcaft,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",hardware,2025-09-22 13:04:14,2
Intel,nf729ff,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",hardware,2025-09-20 04:45:38,3
Intel,nf74cdn,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",hardware,2025-09-20 05:02:36,3
Intel,nx5hhax,It's a good gpu for its price. The driver issues that ruined A-series seem to be gone. Just be sure to enable ReBAR: BattleMage needs to to perform best,buildapc,2026-01-01 22:20:13,5
Intel,nx5gj70,[https://www.youtube.com/watch?v=00GmwHIJuJY](https://www.youtube.com/watch?v=00GmwHIJuJY)   [https://www.youtube.com/watch?v=npIpWFSfmv4](https://www.youtube.com/watch?v=npIpWFSfmv4)  Not sure what kind of options you have where you are. 3060 is pretty long in the tooth unless your getting a smoking deal. No low priced 9060s or or anything around where you are?,buildapc,2026-01-01 22:15:11,3
Intel,nx6wr9i,At 250 its a crazy good deal. Its not top of the line but its winning competition is the fact that it beats out every other card in its weight class at its price point.,buildapc,2026-01-02 03:18:01,2
Intel,nx7n3le,It’s great,buildapc,2026-01-02 06:21:14,2
Intel,nxbtc77,"In comparison to other GPUs within its range, what would it be comparable to in terms of performance?",buildapc,2026-01-02 21:45:50,1
Intel,nxbtinv,"In comparison to other GPUs within its range, what would it be comparable to in terms of performance? Especially if the driver issues are no longer present with the more recent updates",buildapc,2026-01-02 21:46:42,1
Intel,nxbt4xl,"A bit out of my price range, even for a used one. I’m only 19 and I’m building this by myself with my own money as my first ever build. From where I’m from, the 3060 12 gb is a bit manageable price-wise, I just wanted to check out other options.",buildapc,2026-01-02 21:44:51,1
Intel,nxo75v2,"i remember hearing that the overhead issues are less of/not an issue now, due to driver updates",buildapc,2026-01-04 18:49:23,1
Intel,nxbtrn7,[similar to a 3060ti](https://www.techpowerup.com/review/intel-arc-b580/32.html) but with newer features,buildapc,2026-01-02 21:47:56,2
Intel,nxbzpsr,Fair. If your on a tight budget. Consider some of the RDNA3 AMD cards as well. I mean if your looking at a 3 generation old 3060 anyway. Something like the 7600 XT 16gb might be on the market where you are at a good price. Its probably 20% or so faster then a 3060.,buildapc,2026-01-02 22:17:35,1
Intel,nvursni,r/homelab,buildapc,2025-12-25 10:48:55,1
Intel,ny4phvu,"You could consider a used GPU since your budget is limited.  Not really sure how much difference you are going to notice for your GPU between PCIe 5 and PCIe 4, especially in a budget setup.  I love that you had a 1070!  I have just upgraded from my 9yr old 6600k|1070 rig.",buildapc,2026-01-07 02:49:45,3
Intel,ny4pbns,PS :    Specs - Ryzen 7700   B850M   2x16GB ram CL30   SSD Crucial P3 1 TB M.2-2280 PCIe 3.0 X4 NVME   GeForce GTX 1070   Screen > Acer ED240Q S3 1920x1080 180hz,buildapc,2026-01-07 02:48:49,1
Intel,ny4t2il,"If you want a new card, you can't do better than [the B580](https://www.techpowerup.com/review/gigabyte-geforce-rtx-5050-gaming-oc/33.html). The extra vram (and performance) matters more than the pcie bandwidth here, especially since you're playing at full HD.  Do you mind the used market? HEre in the states, I can pick up a second-hand 3080 for about the same price as a new B580. A 3080 is faster than the B580 and the more expensive 9060xt.",buildapc,2026-01-07 03:09:21,1
Intel,ny4pm05,5050 is your best bet under 300.,buildapc,2026-01-07 02:50:21,0
Intel,ny4skvb,"i see thanks ! Ahah still was running a i5 7400 on a b250m with the 1070 in early 2025, OG setup.",buildapc,2026-01-07 03:06:36,1
Intel,nwmhbvr,"The 9060 xt should be 30-35% faster than the b580 depending on games and resolution. To me, that's definitely worth a 14% increase in cost.  8 GB does mean the card won't age as well, but it doesn't mean the card is suddenly worthless. There are very few games where 1440p simply won't run on 8 GB of RAM. With the RAM crisis only going to get worse, my hope is that developers will put a little bit of effort into optimizing vRAM usage.  (edit: and based off your pricing, the 16 GB should only be about $50 USD more. That is probably worth it because not only will it perform better now, but you'll get a couple years longer out of it, and it will have higher resale later.)",buildapc,2025-12-29 22:17:02,11
Intel,nwmjhz4,"VRAM usage is based on your resolution, mostly, in conjunction with game settings at that resolution.  If you plan on gaming at all at 1440p, do **not** get a card with 8GB of VRAM. This isn't a future issue, this is a now issue. It currently does not have enough vram for 1440p gaming in nearly any new heavy titles, including battlefield 6 (just as a recent example).   Hell, at 1080p my 8gb card was over capacity on battlefield 6. There's an argument to be made that it isn't enough for future 1080p gaming as well.  Can you do plenty of gaming on an 8gb card? Yes. Can you run every game at max settings? No, not even on 1080p. You'll have to turn settings down. Which frankly I'm in the boat of ""not usually a big deal"", but it's not good when you pay 300+ bucks for a new piece of hardware that can't run your games at full tilt.   It is more powerful than the B580, yes. The B580 will last longer because of VRAM, also yes.   My suggestion is really think about how long it will be until your next upgrade and make your decision based upon that. Longevity, go with intel. More power now for just a couple of years (2 or 3), consider the 9060XT/5060 (at 1080p only, skip entirely for 1440p). Or look at their 16gb variants.   You could also look at used options. You can find a 6750XT 12GB for 200-250 USD typically, as an example.",buildapc,2025-12-29 22:28:10,2
Intel,nwmnkd8,"in that price range I think you can find good used deals. also maybe take a look at the rx 6800 xt (or generally gpus from 1 or 2 generations ago). I have one since 3 years or so and works like a charm (you will lose fsr 4 compared to the 9060 xt, but it has 16gb vram and better performance) in my country (in central europe) I saw some sold for about $250",buildapc,2025-12-29 22:49:17,2
Intel,nwmiajn,"It depends on what games you play and refresh rate not just resolution.  As you mentioned, it's not just a matter of the amount of VRAM, otherwise a 1070 or 1080 Ti would still be used -I bring it up because I just upgraded from a 1070 :)  I would spend the extra $40. I think the RX 9060 XT 8GB would also have more resale value.",buildapc,2025-12-29 22:21:58,1
Intel,nwmjwp3,16gb,buildapc,2025-12-29 22:30:14,1
Intel,nwmu4vj,"9060xt 16gb, is the one you should get if you are planning for 1440p in the future. Get that one that future you will thank and appreciate in the long run.",buildapc,2025-12-29 23:24:40,1
Intel,nwmv68o,"definitley wait and save up for the 16gb of the 9060xt, it will be worth it",buildapc,2025-12-29 23:30:20,1
Intel,nwn4giv,If you're on PCIe 3 the 9060 is the only choice here.,buildapc,2025-12-30 00:20:57,1
Intel,nwn5ndl,"So recently I bought a new prebuilt that came with a 5060 TI 8GB. For my old system, shortly before, I'd purchased a 9060 XT 16GB.  I haven't had an AMD card in many a year, and I was inclined to be generous to NVIDIA, thinking ""Hey, they've got DLSS, they've got multi-frame gen, yadda yadda yadda...""  So I'd been playing Borderlands 4 on my old rig with the 9060 XT when the 5060 TI system came in a few days ago. I tried playing Borderlands 4 on the new PC with DDR5 RAM and a much better processor, and for every game I tried (Witchfire, Borderlands 4), it was a much worse experience.  DLSS frame gen treated me well on my old 4060 so I thought  multi-frame gen would be rad and close the 8GB-16GB gap, but holy shit the lag was like improperly configured Lossless Scaling. It was awful, and I am NOT generally sensitive to latency. I really learned that without enough native frames to feed frame gen, it's not even a feature worth using.  I suspect had my new PC had the 16gb version of the 5060 TI it probably would have run everything as well as, if not better than, the 9060 XT.  As an example, on my new PC with the 5060 TI, Borderlands 4 was running like less than between 35-50 fps native on medium settings. With the same processor, same RAM, but a 9060 XT 16GB, it runs natively in the 70s and 80s on high settings.  Now, I do only game at 1080p, mind you, so that factors in as well, I suppose.  I was a true believer that 8GB was still enough in late 2025, especially with AI assistance, until this past weekend. I would urge you to go with more VRAM. I know, personally, I'll never buy another GPU with 8GB VRAM again after I see how impactful it is.",buildapc,2025-12-30 00:27:21,1
Intel,nwn9c6w,"The whole vram issue is valid. But, ultimately, all you can do is look at the games you want to play. For the vast majority of current games, the 9060xt performs better and not by a tiny amount. I don't get why you'd take 30% worse performance now for the hope that in 3 years' time you might get better performance.",buildapc,2025-12-30 00:47:14,1
Intel,nwntzzr,Especially if 1080p is still your target then I'd go with the 9060 no doubt about it.,buildapc,2025-12-30 02:41:24,1
Intel,nwmhqtw,"Idk about these specific cards but a few years ago I got a 1650 4gb and my buddy got a 1060 3gb  When he played games the fidelity was great but had some stutters here and there, just cruising at 70ish percent  Mine would be screaming at 99% usage with lower fidelity but boy oh boy was it significantly smoother  I'd go with more vram",buildapc,2025-12-29 22:19:09,1
Intel,nwn3tz7,"Neither, get the 9060xt 16gb version. Stay away from 8gb.",buildapc,2025-12-30 00:17:36,0
Intel,nwmr1mk,Tell OP they can get the 9060Xt in 16gb,buildapc,2025-12-29 23:07:52,3
Intel,nxf1w8v,1080p yes on medium settings. 1440p probably if you turn down all the settings to low. 4K if you like slideshow.,buildapc,2026-01-03 10:37:04,2
Intel,nxf3qbn,"of course it will, runs comparable to a 3060. You can expect 70fps on AAA titles at 1080p mid-high settings.",buildapc,2026-01-03 10:52:35,1
Intel,nxf92eu,Why a 14600k with ddr5? At that point you might as well just get onto am5 with a 7600x,buildapc,2026-01-03 11:37:26,1
Intel,nxfiey9,It will be a great setup for 1080p,buildapc,2026-01-03 12:49:52,1
Intel,nxfxfre,Listen if I can run new games with a i5 4690k with ddr3 this is a non starter. Lol,buildapc,2026-01-03 14:22:12,1
Intel,nxezd8p,50/50 chance,buildapc,2026-01-03 10:15:34,0
Intel,nxf63rb,just give you a reference point. i'm using 225f (a cpu even weaker than yours) + b580. i play games at 4k medium quality. [here](https://www.reddit.com/r/IntelArc/comments/1q0diio/effects_of_driver_and_firmware_update/)'s one of them,buildapc,2026-01-03 11:12:36,0
Intel,nxf4fn1,Great. thank you :3,buildapc,2026-01-03 10:58:27,1
Intel,nxflrgz,Most comparisons seem to put 14600K on top though?,buildapc,2026-01-03 13:12:28,1
Intel,nxf0hlk,what does that mean xD,buildapc,2026-01-03 10:24:59,0
Intel,nxkv2gf,"40fps, worse than a console.",buildapc,2026-01-04 05:47:14,1
Intel,nxg25bc,"they trade blows, mainly depends on the game being played",buildapc,2026-01-03 14:48:19,1
Intel,nxf1hve,Hope and pray they optimize their game 😆,buildapc,2026-01-03 10:33:38,1
Intel,nxlrk9m,which console,buildapc,2026-01-04 10:29:33,1
Intel,nxm9lvg,"Series X, PS5 pro.",buildapc,2026-01-04 12:59:14,1
Intel,nxmzk7n,"how many fps do they get at 4k medium quality for acs?  you have to compare them with same resolution, same image setting for the same game",buildapc,2026-01-04 15:30:08,1
Intel,nxnnp61,"60fps, they have 60fps modes with equal to or better settings than that.",buildapc,2026-01-04 17:22:41,1
Intel,nxntih2,prove it,buildapc,2026-01-04 17:49:43,1
Intel,nxnu9h3,Digital foundry video on it,buildapc,2026-01-04 17:53:08,1
Intel,nxnw0p3,"as said, you have to compare them with same resolution, same image setting for the same game  a lazy google search [result](https://www.google.com/search?q=ps5+pro+assassin%27s+creed+shadows+4k+medium+quality+how+many+fps)  The game offers several graphics modes on the PS5 Pro:   * Performance Mode: Targets 60 FPS at an upscaled 2160p (4K) resolution, and includes standard ray-traced global illumination throughout the world. * Balanced Mode: Targets 40 FPS at an upscaled 2160p (4K) resolution (requires a 120Hz display with HDMI 2.1 support) and includes extended ray tracing features, such as ray-traced reflections. * Fidelity Mode: Targets 30 FPS at an upscaled 2160p (4K) resolution with the full suite of extended ray tracing features for the highest visual quality.  it looks like you're (mistakenly) read the ""Performance Mode""  here's my game screen. my upscaling mode is ""quality""  https://i.ibb.co/7xCzKnF3/acs.jpg",buildapc,2026-01-04 18:01:04,1
Intel,nxnw9i9,And the performance mode is higher fidelity than you're playing at.,buildapc,2026-01-04 18:02:11,1
Intel,nx7mwmw,$2k pc is a very budget pc now?   Broke ass me can't even afford that,buildapc,2026-01-02 06:19:38,7
Intel,nx7t182,\->Budget minimal rig   \->9800x3d,buildapc,2026-01-02 07:12:15,6
Intel,nx7jdb2,I did have a lot of stability issues with amd gpus but I hear gamersnexus saying nvidia have drivers issues on the 50s cards. So keep that in mind.,buildapc,2026-01-02 05:51:11,1
Intel,nx7ky17,"I would really look into Intel Arc GPU's, I'm running a b580 with many games at 4k60 max settings (although with AI upscaling in some more recent cases e.g. Cyberpunk with RT and ultra perf XeSS = 55-60+ fps at 4k, no FG needed) it has great support for comfy ui (I'm on Linux for my genAI) and can create 1024*1024 SDXL images in around 2 seconds each and Flux Q4ms (iirc) in around 5-10 sec. I'm not so into LLM's and just use ChatGPT for free, but as far as I can tell it's pretty fast with LLM's as well.  It supports XPU without any issue out of the box if you install the drivers from Intel and I've not seen any issues myself.  So, by only looking at Nvidia you're severely limiting yourself.  Also, as a Lenovo server tech support, I see a lot of companies adding Intel compute units (which are based on Xe cores) to their AI-server clusters as they are way more cost effective and aren't that much slower compared to the Nvidia compute units. All of these servers have no issues running XPU based ML/AI.",buildapc,2026-01-02 06:03:40,1
Intel,nxd91c9,"Sorry but with this component list, it's not a budget pc. If you want a latest generation amd cpu and an nvidia gpu, that's going to drive up the cost, more so with the price hikes coming up soon. You're also going from a 1050 Ti to a 5060 Ti, but you want to reuse your PSU... Are you sure that's actually feasible? What is the wattage of your old PSU? What condition is it in? Unless you upgraded it recently, I doubt it'll work with those components you listed. It's not an irrelevant detail at all. Please find out before you commit to your build.  That CPU will also likely need better cooling than what you have right now, especially if you'll be doing LLM work on it. Also on that note, your RAM. 16 GB is not going to be enough for this kind of work. It feels like you've gone all out on the other components but bottlenecked yourself there.  Look, a 7800 would be around 100 euros cheaper and still do what you want it to. And all Nvidia gpus are way overpriced right now, but perhaps a 3060 or a 3080 (if you can find a cheap one, just go with the 3060 tbh) would still be within your budget while also being decent for what you want it for.  The motherboard is way, way overpriced as well. You can find excellent options at half the price. That ""minimal"" rig is not minimal at all and you've overlooked some pretty critical points.",buildapc,2026-01-03 02:28:50,1
Intel,nx86rp9,"Well it's not a Threadripper PRO with 32 cores. So, what would you call a rig with that? Compared with one with 64 cores or 96 cores?  I'm not sure what I've set can be considered as mid though. The diff between 8 core CPU and 32 is kind of...day and night.",buildapc,2026-01-02 09:21:53,0
Intel,nx86kzb,"Hey, thanks for the info about comfy UI! Have you tried training your own LoRA?   I am really interested now. If I might ask, if you have time, for science purposes.  Could you try:   1. Training your own LoRA on a 50 custom images dataset and write here how much it took?   2. Run ollama with \`qwen2.5-coder:14b\` model and record a short video how fast it outputs tokens? Ask it to generate let's say a \`golang project to check available hostnames from yml config\` or smth.  [https://techtactician.com/how-to-train-stable-diffusion-lora-models/](https://techtactician.com/how-to-train-stable-diffusion-lora-models/)  If it's working well I'll consider [ASROCK Intel Arc A770 Phantom Gaming 16GB OC Grafikkarte 16GB GDDR6 HDMI, 3x DP](https://www.computeruniverse.net/en/p/2E33-00V) for \~300 EUR.  Thanks for the comment!",buildapc,2026-01-02 09:20:06,1
Intel,nxg0gv2,Would you recommend me to go with:  A: [SPARKLE Intel Arc B580 Titan OC 12GB GDDR6](https://www.computeruniverse.net/en/p/2E22-14J)   B: [SPARKLE Intel Arc A770 Titan OC 16GB GDDR6](https://www.computeruniverse.net/en/p/2E22-004),buildapc,2026-01-03 14:39:09,1
Intel,nxdat8h,"EDIT: I've put together a starting list of components you can take a look at. Not super-researched as it is quite late, but hopefully enough to point you in the right direction.  [https://pcpartpicker.com/list/6zg6v4](https://pcpartpicker.com/list/6zg6v4)  Total with cpu, gpu, ram and motherboard at 1092 euros, if all bought at the cheapest option. That particular ram is what I have on the build I just completed for myself. Good price and no issues so far.  As for how to get the parts to Moldova... That is going to be tricky, but a third-party courier or forwarder may be of help, though I would be careful with those.",buildapc,2026-01-03 02:39:13,1
Intel,nxgs4nn,I can't speak for the A770 but the B580 works fine. If you need more vram you could also look into the Arc Pro B60,buildapc,2026-01-03 16:55:47,1
Intel,nxfw88d,"Thanks for the help!      1. I have 800 watts capacity right now, I thought it would be enough. (I bought it with reserve at the time)   2. A cheap \~60 EUR CPU cooler I thought I would just omit, it's like a small detail.   3. I honestly thought it's budget build, but I guess I just don't know the brackets, or maybe I think there isn't an universal gradation for it. I thought my current one is budget, better said ""outdated"".   4. I think motherboard is kinda ""non-negotiable"", I didn't want to cheap out on it, it's for many years to come, I might downgrade CPU/buy cheaper GPU or go with less RAM, but motherboard is there to stay for future upgrades, plus I kinda need a few features from there, USB-C, local network 10 Gb/s Ports (Marvell AQtion)     I don't know if I can change the title now to simply ""I need advice for my PC build"", but maybe I should just get a few more salaries and get something better.   I thought maybe I can run qwen3-coder:30b which in 19gb requirement for VRAM as [mstreurman](https://www.reddit.com/user/mstreurman/) said it works on XPUs, I think it should run on DirectML too.  I was thinking maybe something like:   [XFX SPEEDSTER MERC 310 AMD Radeon™ RX 7900 XTX Black Edition, 24GB GDDR6, 384-bit](https://www.emag.ro/placa-video-xfx-speedster-merc-310-amd-radeontm-rx-7900-xtx-black-edition-24gb-gddr6-384-bit-rx-79xmercb9/pd/D0NJ1PYBM/)  But maybe I genuinely need to just go for a cheaper option for now and after a few years stash some money and update again.",buildapc,2026-01-03 14:15:18,1
Intel,nxg5ogs,"It's crazy that the ram is already doubled in price. I guess it's being picked up fast.   The power supply should be good as long as it's in good condition. If you go with the rx7900 xtx, be aware that for optimal use you'll need 3 8-pin connectors. You may be able to get away with 2.   Definitely look into motherboards a bit more. As long as you have DDR5 and pcie 5 support, you're future-proof for current standards. Current graphics cards don't even saturate 16 lanes of pcie4, nevermind pcie5.    Honestly, I would say to pick up the price-critical components (ram, gpu, SSD if you want one) now and save up for the rest.",buildapc,2026-01-03 15:07:09,1
Intel,ny2ta54,I would move one fan to the rear so you have one exhast and one intake. You don't need to buy a third fan.,buildapc,2026-01-06 21:03:32,2
Intel,ny2u7jd,"Depends on what you're cooling. I put three P12 Pros on the front and moved the stock fans to the top (front intake, rear exhaust) to keep my North's contents cool, there's ~400 watts of GPU and a 5800X3D in there.",buildapc,2026-01-06 21:07:50,1
Intel,ny2wxqy,I have a 4060 TI Super and a 7950x3D in mine with a Noctua NH-D12L on the CPU. Other than that it’s the stock fans and it has never had any heating issues.   I do have the North with the mesh side instead of glass.,buildapc,2026-01-06 21:20:09,1
Intel,ny3xtyq,Buy a 5 pack of 120mm case fans and do 3 intake 2 exhaust. They're cheap and worth it. You can use the 2 case fans that come with it to side mount on the mesh panel but your cpu cooler and gpu will need to be low profile/sff for them to fit.,buildapc,2026-01-07 00:21:20,1
Intel,num2lc1,"Having same issues, having 40-60% decrease in fps across games. Along with stutters, and fps drops. Hard to run even easy to run games like roblox and LoL. Insane that a windows update caused this somehow.",buildapc,2025-12-18 01:14:38,1
Intel,numncz8,"I think I have an restoration point, but damn it sucks",buildapc,2025-12-18 03:26:55,1
Intel,numqe9o,It's working perfectly fine after I rolled back the latest security update.,buildapc,2025-12-18 03:47:40,1
Intel,nxtjdj4,"Why not just plug it in and see?  All the games you mentioned are CPU heavy, so i guess the effect will be quite noticeable. But it also depends on your resolution and game settings",buildapc,2026-01-05 14:17:14,20
Intel,nxtk9mm,"PCIe isn’t the issue. Even running an RTX 5070 Ti on PCIe 3.0 x16 costs basically nothing in real-world gaming (think 0–3% at most). That part is a non-concern.  CPU: The i7-10700 is still a capable 8c/16t chip. At 1440p and up, you’ll usually be GPU-limited. At 1080p / high refresh, you can see ~10–20% lower FPS in CPU-heavy games (Battlefield, big Anno simulations), mostly showing up in 1% lows, not average FPS.  RAM (biggest limiter): DDR4-2133 is slow and will hurt frame-time consistency more than outright FPS. If you can enable XMP and get it closer to 2933–3200, that alone recovers a noticeable chunk of performance.  Overall: You’re not “crippling” the build. Expect roughly: 	•	0–10% loss in GPU-bound scenarios 	•	10–20% in CPU-bound cases, mainly in lows  Totally reasonable to run the 10700 platform for now and wait instead of dropping €1K+ on a new CPU/RAM/mobo combo. If anything, a cheap DDR4-3200 kit would be the best interim upgrade.",buildapc,2026-01-05 14:22:14,9
Intel,nxtjc12,"resolution?  [https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/19.html](https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/19.html) at 1440p pretty bad. Probably inline with an 11400f I suppose, maybe worse. Sure a 4090 is faster than a 5070ti, but you have shit ram if it's truly 2133mhz.  Not really a 1080p GPU. at 4K shouldn't be horrendous.",buildapc,2026-01-05 14:17:00,3
Intel,nxtns99,Just try it out. It doesn't matter if you lose out on 50% of the possible performance if your experience is acceptable.,buildapc,2026-01-05 14:41:27,2
Intel,nxtjcvg,You won't get all the performance but if you crank the graphics and play on a high resolution it will be enjoyable,buildapc,2026-01-05 14:17:08,2
Intel,nxtm1fo,I have paired the 10700 with a 4070 and 3080 and the 10900k (factory clock) with an Rtx 5070.  Both configurations were pegged at 1440p.  I'd imagine you'd get that level of performance with a higher tier card as well.,buildapc,2026-01-05 14:31:58,1
Intel,nxtmad0,Not that bad in 4k. Anything under that you will bottleneck GPU.,buildapc,2026-01-05 14:33:19,1
Intel,nxtmeml,"Depends a lot on the game, resolution and graphics settings. I would just install the new GPU and enjoy it. If you after some time desire more performance then consider upgrading the rest.",buildapc,2026-01-05 14:33:58,1
Intel,nxtmjv9,"It will work fine, but not ideal, until one year ago I had paired my new 4080 with an 8700k, it worked fine at lower resolutions, but at 4k the cpu was bottlenecking a lot. Now that I got the 9800x3D it is a big improvement.",buildapc,2026-01-05 14:34:46,1
Intel,nxtmx03,You don’t have a new system yet so plug it in and enjoy the increase in performance from what you had before. Every system is “bottlenecked” somewhere. It may be small and not very noticeable but it’s a complex system with multiple parts so no system is perfectly tuned. Don’t get too bogged down with that line of thinking that you can’t enjoy the new card.,buildapc,2026-01-05 14:36:46,1
Intel,nxtnb1o,"i’ve got an i5 11600k and 48gb ddr4 3200mhz , had a 3070 and upgraded to the 9070xt.   it runs wonderfully at 1440p, no complaints.   worst case you don’t love it and you go buy a new mobo / cpu / ram combo.",buildapc,2026-01-05 14:38:53,1
Intel,nxto2b0,"You will be fine! If you see anybottle neck, just increase graphics quality and the gpu will have more work.",buildapc,2026-01-05 14:42:56,1
Intel,nxudulj,15%? Lol it’s like 100% from a 9800x3D and DDR5 ram at 1080p  It has to be a very high resolution and single player games to be only 15%,buildapc,2026-01-05 16:47:05,1
Intel,nxug83l,"My previous rig was 10900X paired with RTX 3080 Ti and DDR4-3200. It performed well for the most part, but there were quite a few micro stutters (frame drops) when I played Valheim, and the GPU never got to 80% or above in utilization for any games.  Micro stutters all disappeared and the GPU was actually hitting 99% when I switched to 14900K and DDR5-6000, so I’m pretty sure I was CPU bottlenecked before.  Your new GPU is 32% faster than mine according to techpowerup, so I’m sure it’s going to be CPU bottlenecked and under utilized as well if paired with your 10700.  p.s. might want to check out Facebook marketplace and eBay for RAM, unless you **have to** get a CPU/RAM/mobo combo deal. I recently got SK Hynix 2 x 8GB DDR5-5600 for $50 from Facebook marketplace, and I see a listing for the same sticks but double capacity for $250 on eBay. I think that’s a pretty reasonable price nowadays.",buildapc,2026-01-05 16:58:07,1
Intel,nxujq9b,Not ideal but also not as unusable as people make it out to be. I ran a 10850K + 9070XT for a while and it was fine for RT Ultra Cyberpunk at 1440p with FSR4Q+FG. You'll probably have slightly more frame drops with the slower ram and fewer cores but you can mitigate it slightly by lowering crowd density and render distance in games.,buildapc,2026-01-05 17:14:31,1
Intel,nxuld5t,"I had been using an i7-9700k, 16GB DDR4, and a 4060 Ti at 1440P   Last week I installed 32GB DDR4, and a 5070 Ti, and moved to 4k   I don't play the games you listed. However, BG3 is running better, and I've been playing Clair Obscure for a week at 4k between 60-75 FPS.     DDR4 is compatible on newer motherboards as well. Meaning, if you want to go from the 10700 to a 12 series or similar (buying used), you could probably reuse the RAM if you buy more. That's kind of my plan.      I do have my 9700k running overclocked at 4.6 (really easy and small overclock). You did not indicate if your chip is a K chip, so it might not be, but if it is, I recommend looking into what you can do with running higher clocks.",buildapc,2026-01-05 17:22:06,1
Intel,nxux26n,I ran a new 3060 Ti with an i5-4570 for a while lol. Sure i wasn’t getting the ideal gaming performance but the alternative of waiting for my new processor and RAM was having no performance at all. So yea just go for it cos it’s better to game now than not at all. It’s not like it takes that long to reinstall a GPU anyways.,buildapc,2026-01-05 18:15:12,1
Intel,nxv0j1g,you're good dude 100%,buildapc,2026-01-05 18:30:40,1
Intel,nxv6o85,"As long as you are not running it at low resolutions or settings you should experience very little bottleneck. I have a 11700K system and just replaced the 2080 Super with a 4070 Super, and the only benchmarks out of the ones I tried so far where I saw no improvement were the 720p ones or really old/low spec 1080p ones. Any higher-end 1080p or higher res benchmarks showed significant improvement. Don't forget to enable Resizable Bar on your BIOS if your motherboard supports it (You might need a BIOS update). Also do you have XMP enabled for your RAM?",buildapc,2026-01-05 18:57:59,1
Intel,nxvcf2f,"You'll get most of the performance, dropping down to a non ti 5070 you will lose more fps than you miss out on with an older cpu.  So yeah get the 5070ti",buildapc,2026-01-05 19:24:02,1
Intel,nxvqw2j,"OMG just awful! I'd be happy to take the 5070 Ti off your hands.  Seriously, though, you'll be losing out as much as you can see by looking up how a 10700 compares to modern CPUs. Probably worth it go higher for sims and esports, but otherwise the 10700 should have some life left in it. Maybe Anno 117 will get a little choppy, as you progress. Make it an excuse to upgrade your monitor, if you're still on 1080P.",buildapc,2026-01-05 20:31:17,1
Intel,nxtk6m6,"Well, it also depends on your desired resolution, higher resolutions will suffer more.  But, in all fairness, you can have as much fun with this pc than with a high end pc, so for now with the prices being what they are, I would recommend to build this pc, and see how it performs.   If it really doesn't run good enough, you can still upgrade. But in the meantime, you can already game. If needed lower the settings and always remember, high fps is nice, but not needed for most games. Consoles used to run 30 fps and people were having the time of their lives.",buildapc,2026-01-05 14:21:45,0
Intel,nxtku9t,"It will be a bottleneck but its hard to tell by how much, depends on the game and resolution. BF6 is quite CPU intensive and will be impacted the most i think.   What resolution do you play at? At 1080p it will be a massive bottleneck but at 4k it can basically be a non-issue. Your CPU should be capable enough to push 100 fps in BF6 so you are probably fine.   But nobody knows for how long this RAM price hike will keep going, although most people expect it will be for a while. You could search for an ""upgrade bundle"" which include mobo/ram and cpu. Those apparently have the best value at this moment.   I would assemble the parts you have and see how it runs and go from there.",buildapc,2026-01-05 14:25:24,0
Intel,nxtnaf5,"It will be a huge bottleneck. I had a setup similar to that but with an older Ti card. I finally upgraded to an i7 14th gen and DDR5, same graphics card. It was a massive improvement.",buildapc,2026-01-05 14:38:47,0
Intel,nxugb4d,That CPU should still be enough for 60 fps.,buildapc,2026-01-05 16:58:30,0
Intel,nxuihri,"It's not going to be great with ray tracing only games, as raytracing is CPU intensive too.",buildapc,2026-01-05 17:08:42,0
Intel,nxtlnyn,"Yeah man. Just plug it in and play. If you already bought the gpu and that’s your rig, the only thing you can do is plug it and see how it runs. Go do some benchmarks or something if you want to compare it or just play some games.",buildapc,2026-01-05 14:29:55,3
Intel,nxv9wtv,i went from a 10700f 2933 ddr4 to a 9800x3d 6000 and doubled or more my fps in 1080 games.,buildapc,2026-01-05 19:12:35,2
Intel,nxtlez8,3440x1440p  Thanks for the link. According to that (if I interpreted it correctly) I think I could expect about a 20% drop in performance from the slower CPU alone but I don’t know how much worse the RAM would perform as well. Looks like I may have to buy the expensive RAM afterall.,buildapc,2026-01-05 14:28:34,1
Intel,nxv4y4a,"I paired a 4070 Super with a 11700K and in the 1080p benchmarks I tried it far outperformed my old 2080 Super. I had to drop down to 720p or really low-end/old 1080p benchmarks for the bottleneck to be enough to not give me any performance improvements. Though that's also an advantage of the 4070 Super PCIe 4.0 and ReBar over the 2080 Super's PCIe 3.0 and lack of ReBar support. (The 10000 series CPUs only went up to PCIe 3.0, 11000 added PCIe 4.0).     And that's when XMP enabled for the 2080 Super but currently disabled for the 4070 Super.",buildapc,2026-01-05 18:50:24,1
Intel,nxv5ob1,"> it worked fine at lower resolutions, but at 4k the cpu was bottlenecking a lot  That's odd, generally the bottleneck is OPPOSITE. It's the lower resolutions/settings where a weaker CPU can bottleneck you, at higher settings/resolutions it's the GPU that is the bottleneck.     Reason being at lower settings the GPU is able to render a frame faster since it has less resolution/settings to apply, so it needs the CPU to be able to send data to it faster, which if your CPU or RAM cannot keep up will bottleneck the GPU as it's sitting around waiting for the CPU to send it data for the next frame. At higher resolutions/settings the GPU needs to take more time to render each frame because there is a lot more to do per frame, which gives the CPU/RAM more time until they need to send the GPU data for the next one.",buildapc,2026-01-05 18:53:36,1
Intel,nxtl7ri,Also are you sure you have XMP enabled in the bios to have your RAM run at the rated speeds? Check the sticker on the ram stick for its specs and set it to the correct profile.,buildapc,2026-01-05 14:27:28,1
Intel,nxv5zxx,"At 720p or low 1080p settings yes, not at higher-end 1080p or higher resolutions. And even then, the bottleneck would be something like only being able to run a 720p game at 250FPS instead of 300.",buildapc,2026-01-05 18:55:00,1
Intel,nxvbnsv,"Yeah, but how much did that cost you for that upgrade?",buildapc,2026-01-05 19:20:32,1
Intel,nxtlqol,just use the old ram and see what it's like. depending on your motherboard if it's a crappy one you may only be able to benefit from 2666mhz anyway.,buildapc,2026-01-05 14:30:20,2
Intel,nxvaago,I think people have it right but flip the reasoning in their head.,buildapc,2026-01-05 19:14:18,1
Intel,nxvkus4,"What you are saying is right, if the gpu and cpu are both strong, but when your cpu is weak, it can not handle high resolutions. My cpu would cap out at 100% playing in 4k and gpu would just sit there not being able to use it's power. Now that I have the 9800x3d, the cpu can handle anything and gpu is the one capping out at 100%.",buildapc,2026-01-05 20:02:58,1
Intel,nxtmu7n,Not sure. I’ll look it up and see if I can boost the RAM. I wanted to play in 1440p so it could be enough.,buildapc,2026-01-05 14:36:20,1
Intel,nxvcjb7,650,buildapc,2026-01-05 19:24:35,1
Intel,nxvnn5d,"> What you are saying is right, if the gpu and cpu are both strong, but when your cpu is weak, it can not handle high resolutions.  I am saying that's not how it works, rather, I said the exact opposite. The CPU has very little to do with higher resolutions, that's all the GPU's doing. When you are running at a higher resolution it's the GPU that has more work to do, not the CPU, you will be bottlenecked less by a weaker CPU, not more.     I have seen this countless times in my testing, whenever I paired a newer GPU with an older CPU, it still showed a noticeable improvement at higher resolutions/settings, but little to no difference at lower resolutions/settings. I literally just did that a week ago with the very system I am typing on, I saw no difference when I ran a 720P benchmark, and a massive near double performance improvement when I ran a 4K benchmark.     You have it backwards, a weak CPU means you will not see much of a performance increase at LOWER resolutions, not higher ones.     Here is an example of my own benchmarks showing this:    https://imgur.com/a/2667-v4-rtx-2060-super-vs-11700k-gtx-1070-8n5OtYs     This is a RTX 2060 Super paired with an older 5th-gen Intel Xeon CPU from 2016 vs a computer with a weaker GTX 1070 GPU but paired with a much newer and more powerful 1th gen 11700K CPU from 2021. In the higher-end 1080p/4K tests the system with the more powerful GPU performs significantly better despite the CPU being half a decade older, yet in the low-end 720P benchmarks the system with the more powerful CPU performed better. The exact opposite of what you are claiming.",buildapc,2026-01-05 20:16:04,1
Intel,nxtnl03,I read in your other comment that you play at 3440 x 1440. In your case thats a good thing as it narrows the difference between your GPU and CPU performance. More pixels means more GPU usage that is not directly tied to your CPU. But personally i would bite the bullet and spend some extra on the ram to finish the rest of your build,buildapc,2026-01-05 14:40:22,1
Intel,nxvea6q,"Wow, fair play you’ve clearly managed to get yourself a very good deal there.  Was that all new or second-hand? I’m having a look myself and even second-hand I’m seeing the CPU alone around 400, and once you add DDR5 memory and a motherboard, I just can’t get the total to come in under 650.  From what I’m seeing, it looks more like 700–800 minimum unless parts were reused or bought very cheaply.  Not doubting the gains at all — just struggling to see how that total works out for most people.",buildapc,2026-01-05 19:32:38,1
Intel,nxz02xl,"Alright, pair an 8700k with an RTX 4080, try Battlefield 5 at 4k ultra settings and you tell me what is causing the bottleneck.",buildapc,2026-01-06 07:52:44,1
Intel,nxvekot,microcenter,buildapc,2026-01-05 19:33:59,1
Intel,nxvgbte,"Ah, that explains it Micro Center is US-only.  Those bundle deals don’t exist in Europe, and once you factor in EU pricing, VAT, and availability, the numbers just don’t translate. Over here, the CPU alone is around 400+, and when you add DDR5 RAM and an AM5 motherboard, you’re realistically looking at 700–800 minimum, often more.  So yeah, great deal if you’re near a Micro Center in the US, but it’s not really a comparable or repeatable upgrade path for people buying parts in Europe.  Was the 650 plus tax?  I might have fly over and buy one myself",buildapc,2026-01-05 19:42:07,1
Intel,nxvll3e,"it was 700 with tax but the whole build was 1100 as i got a new case, psu, cooler which i didnt need to. May have gone up to 750 though as the under 700 deal no longer includes the ddr5 i got and has a bit cheaper one instead.",buildapc,2026-01-05 20:06:26,1
Intel,nxl5man,what's the CPU,buildapc,2026-01-04 07:11:50,1
Intel,nxl5nbi,What CPU are you running in the build?,buildapc,2026-01-04 07:12:05,1
Intel,nxl5p31,Ryzen 7 5800x,buildapc,2026-01-04 07:12:31,1
Intel,nxl5swk,ryzen 7 5800x,buildapc,2026-01-04 07:13:26,1
Intel,nxl60iz,are you using an NVME?,buildapc,2026-01-04 07:15:15,1
Intel,nxl648w,SSD,buildapc,2026-01-04 07:16:08,1
Intel,nxl6n8n,"did you reinstall the drivers completely, like you DDU'd and everything?",buildapc,2026-01-04 07:20:41,1
Intel,nxl6tdr,"im not super knowledgable so im not sure what DDU is, through the app i did CLEAN install which said it'd remove the old drivers and install the new",buildapc,2026-01-04 07:22:12,1
Intel,nxl7g4e,use DDU to uninstall the drivers then install fresh drivers from the Nvidia website,buildapc,2026-01-04 07:27:48,1
Intel,nxls0kq,"DDU = display driver uninstaller.  Download the latest drivers from ngreedia site, then download DDU and use to uninstall all display drivers in your PC, then reboot and install drivers again.",buildapc,2026-01-04 10:33:40,1
Intel,nwjwdn2,Your list is private,buildapc,2025-12-29 14:50:36,2
Intel,nwjvzrr,"It looks like you may have posted an incorrect PCPartPicker link. Consider changing it to one of the following:  * [Use the Permalink](https://i.imgur.com/IW0iaOm.png). note: to generate an anonymous permalink, first click [Edit this Part List](https://i.imgur.com/uqDIcdt.png).  Or, make a table :    * [new.reddit table guide](https://imgur.com/a/1vo0GHH)   * [old.reddit table guide](https://imgur.com/C86vdxB)         *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",buildapc,2025-12-29 14:48:34,1
Intel,nwjwfd7,Your list is Private,buildapc,2025-12-29 14:50:52,1
Intel,nwjwm63,Ur list is private,buildapc,2025-12-29 14:51:53,1
Intel,nwjxjpz,"Your list is set as private, so we can’t see it.",buildapc,2025-12-29 14:56:46,1
Intel,nwjxqdy,"No use getting a 9800X3D (high end) CPU with a B580 (low end) GPU.   If this is for gaming, I would move some money from the CPU and the motherboard to the GPU.   What is your total budget? Do you have a Micro Center nearby?",buildapc,2025-12-29 14:57:45,1
Intel,nwkrlth,"Everything but the gpu is fine.  I saw in a comment you are close to a micro center.  Consider dropping to the 7800x3d bundle or even the 7600x3d bundle and bumping the gpu up to the powercolor 9070xt IF you are hard stuck at $1500 budget   If you have a little wiggle room, you could go with all micro center parts:  7800x3d bundle is 580 (580) -7800x3d -ASUS Tuf gaming b650e -gskill flare 32gb ddr5-6000  Powercolor rx 9070XT is 580 (1160)  Inland QN450 1TB nvme ssd is 100 (1260)  Corsair RM850 psu is 110 (1370)  Peerless assassin is 50 (1420)  Leaves you 80 for a case.  I like the Montech XR. Just used it for my kids build (almost identical to this one) and it’s got good cable management and was easy to work in.  Only 70 bucks too.   Just make sure whatever case you get fits an ATX psu, an ATX board, and the longish GPU.",buildapc,2025-12-29 17:22:03,1
Intel,nwjxmiw,I fixed it,buildapc,2025-12-29 14:57:11,2
Intel,nwjxn95,fixed it,buildapc,2025-12-29 14:57:17,1
Intel,nwjxnsv,fixed it,buildapc,2025-12-29 14:57:22,1
Intel,nwk5f6j,I have a budget of 1.5k and yes I do have a micro center nearby. what gpu and cpu should I get instead then,buildapc,2025-12-29 15:36:29,1
Intel,nwk029g,"I think this is pretty unbalanced, in that you have a budget GPU but a really high end CPU. You should either spend less on the CPU and keep the B580 for a lower cost overall, or you should spend money from saving on the CPU to upgrade the GPU. Also the RAM you selected, it's nice that it is cheaper, but CL46 is quite slow, imo worth to get more expensive 6000CL36 (lower CL is better).  I'd perhaps do it like this, maybe switching out the GPU to a 5060 Ti 16GB: https://pcpartpicker.com/list/PpRqYd",buildapc,2025-12-29 15:09:50,3
Intel,nwkcnq6,"I would get a bundle from Micro Center (https://www.microcenter.com/site/content/bundle-and-save.aspx). Get the 7600X3D one for $400, which comes with a 16GB RAM stick, and you can get a ***matching*** stick on top of that if you want. That will be much cheaper. That CPU is very, very good for gaming thanks to its extra (X3D) cache, which also mitigates any RAM speed/latency concerns one could raise.   With the savings from that bundle, you can beef up your GPU ***significantly***: RTX 5070 Ti > RX 9070 XT > RX 9070 > RTX 5070 > RTX 5060 Ti 16GB > RX 9060 XT 16GB... You can use the relative performance table on this webpage to get a feel for which GPU is faster: https://www.techpowerup.com/gpu-specs/geforce-rtx-5070-ti.c4243  PS: Make sure your PSU supplies enough power considering your changes to the build. Cybenetics rates PSU quality on its website. You should be able to find a solid PSU for $100 or less.",buildapc,2025-12-29 16:11:21,1
Intel,nwk1l5x,"With the current RAM market, there’s no need to be picky about RAM speed and latency. Especially if sticking with an X3D CPU, whose extra cache will negate the speed/latency gap in gaming.",buildapc,2025-12-29 15:17:32,1
Intel,nwk4wzo,how good of a build is that?,buildapc,2025-12-29 15:34:01,0
Intel,nwkfdmd,ty for thr advice,buildapc,2025-12-29 16:24:18,1
Intel,nwkc05v,"If you're talking 6000CL30 to CL36, or like 5600CL36, sure. But 5600CL46 vs 6000CL36 is a very big difference. And in my comment I do also say not to spend for a X3D, because a 250 dollars cheaper regular is a much better combination with the GPU.",buildapc,2025-12-29 16:08:13,1
Intel,nwkecy4,"I get you. I just wanted to specify that, if OP sticks to X3D (e.g., a 7600X3D from Micro Center), the RAM speed and latency gap will have ~~zero~~ minimal (3–4%) impact on gaming performance on a high-end GPU. ~~Zero~~.   *Edit: Corrected my assertions.*",buildapc,2025-12-29 16:19:27,1
Intel,nwkj404,"Yes and no, there is a performance difference still of a few percent, but with a low end GPU probably not very noticeable.",buildapc,2025-12-29 16:41:55,1
Intel,nwkjz8o,Fair enough... about 3–4% on average when using a high-end GPU... I will correct my comments accordingly.,buildapc,2025-12-29 16:46:01,1
Intel,nxixgo8,1080p? Honestly anything 3060 12GB and up.   People overestimate their needs when it comes to GPUs.,buildapc,2026-01-03 23:05:42,61
Intel,nxh8n6m,9060XT 16GB,buildapc,2026-01-03 18:11:36,238
Intel,nxhjftz,Rx 9060 xt 16gb,buildapc,2026-01-03 19:00:23,38
Intel,nxj10o1,"RTX 5070, they dip to 489$ a lot",buildapc,2026-01-03 23:24:27,18
Intel,nxi2zdo,"You can get lucky and get a 9070xt for like 550 on sale. If you’re patient and can hunt sales, I would suggest that.  If you don’t want to wait you can get a 9060xt 16GB for 400 and it’ll crush everything at 1080p anyway. Just make sure it’s the 16GB model.",buildapc,2026-01-03 20:34:10,105
Intel,nximug1,A GPU for 1080p gaming for around 500? Hmm checking around  The easy answer   Rx 9060 xt 16 gig  Rtx 5060 ti 16 gig  Something used from a previous gen or two that was high end.   With some dexterity  Rtx 5070,buildapc,2026-01-03 22:11:46,26
Intel,nxhek10,You build it completely new or did you already have some components? I think a7800x3d and save money for gpu with tha game u plan to play make no sense! You should go for a 9600X and push all your money in the gpu. 9070XT or 5070ti. Wich Country and what is your max budget?,buildapc,2026-01-03 18:38:22,15
Intel,nxit06u,I'd honestly look on the secondhand market and pick up a 2070 Super and you'll be fine with those games. Usually can find them for <$200.,buildapc,2026-01-03 22:42:43,7
Intel,nxj9gns,"5070 is just a bit above this budget, but quite a bit better than lower options like 5060ti or 9060XT.",buildapc,2026-01-04 00:08:33,4
Intel,nxj6zwr,5070 and use DLDSR,buildapc,2026-01-03 23:55:35,4
Intel,nxjvdmt,I’m gonna echo the 9060 XT 16GB. I just got a prebuilt with it and it’s great for 1080p gaming at high settings and 120 FPS. It was $1200 for the whole machine and it came with 32GB DDR5-6000 RAM and 1TB SSD. I imagine prebuilt prices are gonna skyrocket soon too.,buildapc,2026-01-04 02:07:21,5
Intel,nxj4p0h,"RX 9070XT/non XT + RX 9060 16GB + RTX 5060 Ti 16GB are the best options, sadly the 9070 and non XT are pricier than they should be right now.",buildapc,2026-01-03 23:43:32,3
Intel,nxjfwmn,"Walmart has 5070s for $489, beats the 5060ti and 9060xt by a good margin",buildapc,2026-01-04 00:42:26,3
Intel,nxjrfu5,I just got 5060ti 16gb and its been bad ass at 1440.,buildapc,2026-01-04 01:45:39,3
Intel,nxjxvt1,9060xt is 389 on Best Buy rn,buildapc,2026-01-04 02:21:15,3
Intel,nxi9tlx,It's insane that in 2025 a GPU for freakin 1080p is 500 bucks,buildapc,2026-01-03 21:08:09,13
Intel,nxhby43,Go to the used market if you want the best performance for $500. You can get a 7900xt for around $400-$450. If you want brand new you can probably get a 5070 for $500 if you wait for when it goes on sale,buildapc,2026-01-03 18:26:31,8
Intel,nxk3r0c,"A lot of people are suggesting to downgrade the CPU and spend the money on a better GPU which is totally a valid option.   Personally, I’d go a different direction though. Keep the 7800X3D as it’s an excellent CPU. Go with a 9060XT (specifically the 16GB, not the cheaper 8GB) as your GPU. This combo will future proof you at 1080p for some time and allow you to jump to 1440p if you want. The 9060XT can utilise FSR4 and frame gen as well if you do decide to go to 1440p, and the extra VRAM will help future proof you somewhat.   I have 2x rigs at the moment. My main one is a 7900X3D (I wanted the extra cores for work stuff) and a 7900XTX which I couple with a 4k monitor.   My other machine is on my TV which has a Ryzen 7 3700X and a 9060XT. I can play newer games at 4k medium settings with FSR4, but do most of my gaming at 1080p and it runs brilliantly.",buildapc,2026-01-04 02:54:07,2
Intel,nxkb5q7,I have a 3060 and a HP omen monitor. It's 1440 and 165hz. I love it.,buildapc,2026-01-04 03:36:26,2
Intel,nxkwh4h,Maybe RTX 3080 10GB.,buildapc,2026-01-04 05:57:41,2
Intel,nxitf13,Used 1080ti if you can find it for less than 150 bucks  New arc b580 if you can find it for less than 350,buildapc,2026-01-03 22:44:47,0
Intel,nxjul5q,why get a 7800x3d for 1080p?? very odd choice,buildapc,2026-01-04 02:02:58,1
Intel,nxjz3bp,"9060xt is pretty good but if you want a little more expensive for certain slightly higher performances, 5060ti is excellent for budget as well",buildapc,2026-01-04 02:28:00,1
Intel,nxk1hqm,The 5070 MSI Shadow is $489 at Walmart if you have one close by. I Just upgraded from a 2060 super and  it is a huge improvement.,buildapc,2026-01-04 02:41:26,1
Intel,nxkc7on,"I game at 3440x1440 100hz, and my i7 7700 and 2070 were doing everything great, especially with DLSS at performance or ultra performance. I’ve upgraded since (285k and 5080), but I played through cyberpunk 2077 at max graphics (no RTX) at a good enough frame rate that I was just playing the game and having fun.  So, 2070 is ok used, not the best.  3060 ti would be better used.  5070 is sub $500 a lot now.  Even 5060 ti 16gb would work.  I’m not an AMD guy, but everyone is saying they have great options too, I think one with 16gb of vram?  A 5070 would blow your mind probably and is better than you might think. A 5060 ti 16gb would probably definitely serve you 1080p60 excellently.",buildapc,2026-01-04 03:42:29,1
Intel,nxko463,Arc B580,buildapc,2026-01-04 04:58:00,1
Intel,nxl713m,9060XT 16GB,buildapc,2026-01-04 07:24:06,1
Intel,nxl829m,1080p? 6650xt.. it can run 1440p too.   otherwise rx 9070 you should be able to find for around $550 and its brand new architecture,buildapc,2026-01-04 07:33:13,1
Intel,nxl829s,"Honestly for 1080p I would look at an older card but top tier for that gen. A 2080 super would run everything easily at 1080p for example, doesn't have to be the latest tech if you don't care for 1440 or 4k",buildapc,2026-01-04 07:33:14,1
Intel,nxlil9o,5070 would be perfect if you can get it for $500 .,buildapc,2026-01-04 09:08:11,1
Intel,nxlr7qi,"cheapest 5070 at micro center is exactly at $500, costing the same as the cheapest rx 9070.",buildapc,2026-01-04 10:26:24,1
Intel,nxn98t7,i got the 5070 for 1080p and its doin me great . get the best u can find bcs its a long investement,buildapc,2026-01-04 16:16:14,1
Intel,nxnjvt9,"Used 3080,3080ti",buildapc,2026-01-04 17:05:20,1
Intel,nxnnbr7,A 4070 or 4070 super of eBay,buildapc,2026-01-04 17:20:57,1
Intel,nxi7ets,"If you have a Microcenter near you, check their refurbished and open box section frequently. They may have really good deals.",buildapc,2026-01-03 20:56:14,1
Intel,nxiw4ns,Do you plan on staying at 1080p or moving to 1440p? Cause 1440p monitors have gone down in price as 4k becomes more popular. Is $500 your top top budget cause if you depending on your location a better future proofed 9070xt may be available. Or just not spend that much on a gpu for 1080p.,buildapc,2026-01-03 22:58:42,1
Intel,nxjhd36,Xbox series X,buildapc,2026-01-04 00:50:13,0
Intel,nxj0yhw,"Don't go this way.   Drop the 7800x3D for a 7700x or 9700x, which will allow you a bigger budget for a GPU, like the 9070 XT to play easily at 1440p.",buildapc,2026-01-03 23:24:08,-1
Intel,nxkrg3d,You need 1440p,buildapc,2026-01-04 05:20:53,0
Intel,nxjjdcb,"Shit man i still have a 1080ti i have overclocked to the tits that runs everything i throw at it in 2k, Newer stuff that relies on dlss and frame gen i may have to settle for 60 fps med-hig, but everything from the last few years with a 5700x3d is still chugging 100+ fps mostly on max",buildapc,2026-01-04 01:01:10,18
Intel,nxk4yey,What CPU around $200-$250 would you recommend to pair with it?,buildapc,2026-01-04 03:00:51,21
Intel,nxk81ug,Just bought it today. Loving it already. Big upgrade from my ancient RX 580 lol,buildapc,2026-01-04 03:18:30,6
Intel,nxrf6l2,"My friend, do you recommend that gpu for 1440p too?",buildapc,2026-01-05 04:26:04,1
Intel,nxhhdvc,Makes no sense literally 0 performance differences between the 7600x and 7800x3d with a 9060xt https://youtu.be/rM0B8sidI0Q,buildapc,2026-01-03 18:51:04,-157
Intel,nxj93t5,"I haven't seen a 9070XT below $600 in at least a month, and those were MC in-store exclusive. Normal price seems to be $630-660 now, and that's likely to keep climbing.  Even the 9070 is above $580 now.",buildapc,2026-01-04 00:06:43,25
Intel,nxkpsru,My PC Hellhound 9070xt was like $600 on Black Friday.  What were the ones that were going for $550?,buildapc,2026-01-04 05:09:27,4
Intel,nxlzve8,"From what i saw on early benchmarks, the 5060 8gb can run every new game pretty much maxed at 60fps 1080p native. 16gb is mostly overkill for 1080p",buildapc,2026-01-04 11:42:23,2
Intel,nxiuyip,Just get a 5070 they’re on sale for 449 to $500 everywhere,buildapc,2026-01-03 22:52:39,16
Intel,nxi3d22,This! The difference you’ll see going for a 9070xt over a 9060xt is farrrr greater than going from 9600X to 7800x3d,buildapc,2026-01-03 20:36:05,14
Intel,nxj5ki6,"It's not, you're just making it sound like it is. A €200 3060 Ti with DLSS turned on will run everything nicely at 1080p.",buildapc,2026-01-03 23:48:06,9
Intel,nxihbyr,"It's not. A sub 400 dollar 9060xt 16 gb is more than plenty for 1080p and can push 1440p. The Arc B580 is a 250 dollar card that will be great for 1080p. You can even find cheaper for solid 1080p 60 gameplay. 500 dollars is overkill for 1080p, OP is just giving us his budget for a gpu.",buildapc,2026-01-03 21:44:46,43
Intel,nxja384,The GTX 970 launched in 2014 for $349. That's $448 today.  People just don't seem to realize how much inflation has happened in recent years.,buildapc,2026-01-04 00:11:56,8
Intel,nxjfcz1,"Its not, but op is asking for 500, what's the best. A $100 gpu can run 1080p, but so can a $2500 gpu. Op limit is 500",buildapc,2026-01-04 00:39:35,2
Intel,nxkczk4,5090 can do 1080 as well buddy. Crazy that you gotta pay 3k for 1080 gaming in 2026,buildapc,2026-01-04 03:46:54,3
Intel,nxip55j,Maybe I’m crazy but I think a 1080p SYSTEM should be around 500-600,buildapc,2026-01-03 22:23:14,0
Intel,nxknfpx,Good choice actually. Higher the resolution the more GPU dependent you become.,buildapc,2026-01-04 04:53:29,2
Intel,nxkpmfb,this is the way. I just checked and they had a 5070ti for $675...  Also 5070 for $480,buildapc,2026-01-04 05:08:13,1
Intel,nxjtsau,My 1080ti still going and I'm guessing clinging on for dear life. I pray to the gamer gods it lasts until I can afford an upgrade.,buildapc,2026-01-04 01:58:31,8
Intel,nxkaibp,I know I’m not the original guy but the 7600x or 9600x seem pretty solid for their prices.,buildapc,2026-01-04 03:32:41,31
Intel,nxlw261,As other guys said BUT if u dont have ddr5 ram just get 5600x since they use ddr4   Obviously its a downgrade but u will save more money maybe 100-200$ ( 40$ for AM4 motherboard 70$-90$ for ram if 16gb probably a 20-30$ for cpu ),buildapc,2026-01-04 11:09:20,2
Intel,nxhir15,"I don't disagree, but they're not asking about their CPU, they're asking about their GPU. I think the 9060XT is the best 1080p card you can buy right now, regardless of their CPU choice.",buildapc,2026-01-03 18:57:16,96
Intel,nxicpk4,Written downvote for you sir,buildapc,2026-01-03 21:22:19,40
Intel,nxj8td1,Don't generalize performance like this. Different games have different hardware usage. Obviously AAA games at max settings will be GPU bound with a lower-end GPU.  You also linked a video running CS2 and Valorant at Very High settings... so the person obviously doesn't know what they're doing.,buildapc,2026-01-04 00:05:13,11
Intel,nxjhmwx,Here let me proofread this before you post it.   “Would it make more sense to go with less CPU and upgrade the GPU for better performance now and into the next upgrade?”  Copy/paste if you like.,buildapc,2026-01-04 00:51:42,4
Intel,nxkkvfn,It's not 2016 bro,buildapc,2026-01-04 04:36:25,7
Intel,nxlredz,are you a time traveler that both come from 2010 and is a boomer by any chance?,buildapc,2026-01-04 10:28:06,1
Intel,nxjmtbp,Cards already went up $50-70 since two weeks ago.,buildapc,2026-01-04 01:20:11,10
Intel,nxkk00n,Just got the 9070 XT 16gb for $580 at micro center in Fairfax,buildapc,2026-01-04 04:30:40,3
Intel,nxl3i63,There was a Power Color 9070xt for 550 at Newegg a couple days ago. But it was a one day deal.,buildapc,2026-01-04 06:53:48,2
Intel,nxmp4se,"Until you enable RT and then it’s not. Unless you’re budget constrained I would definitely avoid an 8 GB card. Even then, I would suggest getting an arc b580 with 12 GB.",buildapc,2026-01-04 14:34:39,1
Intel,nxke67n,At 1080p? Not a chance.,buildapc,2026-01-04 03:53:53,7
Intel,nxjn0ji,It will do just fine with dlss turned off lol 3060ti is pretty strong and 1080p is easy drive. My Nephew is still running my 1060 6gb lol,buildapc,2026-01-04 01:21:13,3
Intel,nxmnris,"*5k, they raised prices. Wanted to buy a 4090 when the 6090 comes out but it's gonna cost me an eye and a lung",buildapc,2026-01-04 14:26:54,1
Intel,nxl5szo,"the extra money spent on such a strong cpu would be much better put to use in a stronger gpu. the 7600x is cheaper and plenty unless he gets a very expensive gpu, which he's not doing. he is guaranteed to have a better computer if he balances this curve right; gpu is supposed to be the strongest component",buildapc,2026-01-04 07:13:27,0
Intel,nxk0yp1,"Same here,  except I'm still rocking a gtx 960.",buildapc,2026-01-04 02:38:27,4
Intel,nxkdmh2,"As someone with an R5 7600 and an RX 9060 XT 16GB, I can confirm it runs pretty much any game on 1080p max settings at very high FPS (most competitive games over 200/250), unless it's something with wonky performance like Tarkov/Arma or Expedition 33.",buildapc,2026-01-04 03:50:39,15
Intel,nxhjj88,Since op is building a complete new system I would invest less in a cpu and more in a gpu. For 1440p proof. But that's me.,buildapc,2026-01-03 19:00:48,-116
Intel,nxlmnew,Fuck.. I wanna upgrade but idk if should wait to find a deal or just bite the bullet? I hear AMD gpus will increase even more by the end of the month..,buildapc,2026-01-04 09:45:28,1
Intel,nxm9so3,Meanwhile the 5070Ti went up about 500$ already lol,buildapc,2026-01-04 13:00:34,1
Intel,nxl86nk,"Agreed, it's all overkill for 1080p",buildapc,2026-01-04 07:34:18,1
Intel,nxjvhzm,Yeah lol a 2070 super will do the business too at 1080p. Op definitely has a lot of flexibility.,buildapc,2026-01-04 02:08:01,2
Intel,nxl9tjk,"Wrong. This is completely subjective and depends on use case.  1080p doesn’t take much to drive. You’re going to hit your monitors refresh rate limit in most games with even a 3070… Aside from the AAA titles, of course. But 1080p im guessing OP plays a lot of FPS titles. A 7800x3D will give OP better 1% lows and more consistent frames.",buildapc,2026-01-04 07:48:57,0
Intel,nxkwtnf,You'd have no problem with 1440p as well.,buildapc,2026-01-04 06:00:21,7
Intel,nxhle66,"I would agree, but they're not asking that question. I am simply answering what they had asked and I'm not providing unsolicited advice.",buildapc,2026-01-03 19:09:18,99
Intel,nxo42ju,Prob wont find a deal. Demand just skyrocketed and stocks are going dry.,buildapc,2026-01-04 18:36:06,1
Intel,nxo3l58,Not seeing that yet. I am seeing base model stock going out on this panic buy though. The OC'd and special editions are still left but those are the +$150 versions.,buildapc,2026-01-04 18:34:01,1
Intel,nxmrwzi,"Except for Tarkov, where you’d notice a HUGE difference if you use an X3D chip.",buildapc,2026-01-04 14:50:11,1
Intel,nxmrniy,"Yeah, I don’t have a 1440p monitor but I’ve seen some benchmarks and it should run well.",buildapc,2026-01-04 14:48:45,1
Intel,nxhncbh,"Op is indeed asking for GPU advice to match with the CPU. However, if you're more experienced than op your advice could also be to downgrade the CPU and upgrade the GPU for better overall performance.  It's still reddit and not everyone is experienced enough",buildapc,2026-01-03 19:18:17,-109
Intel,nxo4gaf,"Damn.. that’s what I was thinking. I know MicroCenter has a deal if you sign up for their credit card, you’ll get 5% off which is a little over $30 for the card I want but idk if getting a card is worth it just for that.",buildapc,2026-01-04 18:37:44,1
Intel,nxrbpw7,"on newegg the ASUS Prime 5070Ti I bought was 1100$ CAD, 2 days later they refunded me for it being ""out of stock"" and then i check back the cards were never out of stock, instead the price jumped to 1900-2000$ CAD.  I decided to grab an MSI card out of spite. and it was 100$ cheaper than the ASUS one.",buildapc,2026-01-05 04:05:33,1
Intel,nxn2fr2,9060xt 16gb and Ryzen 7 7700x just got a new 1440p monitor and it handles it well.,buildapc,2026-01-04 15:44:08,2
Intel,nxi27c7,I assume he stopped responding because he realized it was pointless with you. You are more than welcome to go have your own convo with the OP about your CPU suggestions. But currently you are attacking someone who directly answered the OPs question with info outside of the scope of the original post.  It's not a good look!,buildapc,2026-01-03 20:30:12,65
Intel,nxj12n3,you clearly have 0 clue whay youre talking about lmfao,buildapc,2026-01-03 23:24:44,18
Intel,nxi69tp,I didn't try to attack him if it came across like that I'm sorry.   I just try to help Op getting the best price to performance.  Like more people in this thread funny how I get down voted for giving alternative suggestions.  While I'm not the only one suggesting downgrading the cpu and upgrading the gpu.,buildapc,2026-01-03 20:50:35,-32
Intel,nxjv8ge,Lol check the video and see it yourself,buildapc,2026-01-04 02:06:32,-1
Intel,nxjgtc4,"Hmm, so you acknowledge you aren’t the only one suggesting downgrading the cpu, and yet you’re the one getting downvoted. So maybe, that in itself is not the reason why people are downvoting you.",buildapc,2026-01-04 00:47:17,16
Intel,nxjw32k,"Dont you just love it when you've got great info to provide, but if you speak in a slightly rude or mean tone of voice/writing style. You get downvoted to oblivion by cowards, for being a big meaniehead..... The mob sure gets easily uncomfortable on this website.",buildapc,2026-01-04 02:11:11,-7
Intel,nxk2a8m,Bro the autism in this sub I swear,buildapc,2026-01-04 02:45:52,4
Intel,nxjwobl,I'm not even trying to be rude or mean. People probably not used to Dutch directness,buildapc,2026-01-04 02:14:27,1
Intel,nxll2qx,1. Depends on the situation. In a situation where 16gb of ram is insufficient: having 8+8+16 would be faster. In a situatuon where 16gb is plenty: having 8+8+16 would be slower.  2. How is your gpu and cpu usage when playing the game.,buildapc,2026-01-04 09:30:53,2
Intel,nxlmzdv,"Google is wrong about arc raiders. I think the 5070TI should see about 100-120ish FPS at 1440p high without raytracing, but it can vary based on your settings. Your CPU and RAM speeds do play a part as well.  With the 5070Ti, you can enable the DLSS Transformer model and set it to quality or balanced to b get extra frames at basically no visual cost. You can also use frame generation if you want, but I would personally not recommend it.  You can check things like making sure your RAM OC is on and that ray tracing is set to what you want.",buildapc,2026-01-04 09:48:28,2
Intel,nxlmboe,"Don't do it, get another kit of 2x 8gb.",buildapc,2026-01-04 09:42:26,1
Intel,nxo79fm,"So when it comes to not reaching performance goals, you mentioned that your GPU usage was 60-80%. That is not good. That means that your processor is holding you back from achieving a higher frame rate. If you turn down the graphics settings, or decrease resolution and the frame rate doesn't improve: you are CPU limited. This happens often in esports/open world titles when trying to achieve high frame rates. Make sure that PBO is turned on for your processor in the bios, get a proper cooler for said processor before you do that though (stock coolers suck), go with a thermalright peerless assassin if you want to save money. In your bios, turn on XMP or expo if your ram and motherboard support it. The default profile or configuration for XMP is fine, tuning it could give a little more performance but can be unstable and time consuming.   Ideally you want close to 99% GPU utilization for a smooth gameplay with consistent frame times.  You might find turning up some gfx settings actually helps in a CPU bound scenario.   Edit: YouTube is a very good source for benchmarks. I wouldn't trust Google ai. Look up an arc raiders benchmark with your specific specifications: 5600x +5070ti and compare settings and fps. If your within 5-10 fps it's likely variables with card model (OC or non OC versions), overclock or undervolt performance on CPU, or ram kit speed.   Also another thing I remembered, in certain ryzen x model CPUs they will boost to what's called TJ Max temperature (sometimes 85-95C) automatically with PBO turned on so that they can get the maximum clock speeds. Improving the cooling solution will therefore increase the clock speeds because it allows the CPU to run at a higher voltage.",buildapc,2026-01-04 18:49:49,1
Intel,nxllg3f,"Right, so basically most games it’s better to just stick with 2x8 then, because I usually use within the limit I guess.  To answer the question, my GPU usage is usually around 60-80% and CPU depends on the game, but usually quite high, I have gotten CCleaner to clear up background usage too, I can give you exact numbers within 10 minutes, I will go an boot my PC up now. Also my CPU runs at around 70-85°C which I don’t know if it is too high or not??",buildapc,2026-01-04 09:34:21,1
Intel,nxlms8a,To answer the question my CPU right now whilst in menu of ARC Raiders is between 30-40%,buildapc,2026-01-04 09:46:41,1
Intel,nxloqnl,"How do I check my RAM OC is on?? Also, I have an MSI 5070TI Gaming trio OC, would it hurt to overclock it or Undervolt it? Would that help a bit?",buildapc,2026-01-04 10:04:11,1
Intel,nxlomkc,"Okay, maybe I will sell this 1x16 as it’s worth £100 rn and get 2x8",buildapc,2026-01-04 10:03:12,1
Intel,nxlqptm,"For a 5600x, 70~85°C sounds little too hot. Do you happen to be using stock cooler?",buildapc,2026-01-04 10:21:54,1
Intel,nxlp1f1,Usually it will show in Task Manager on the RAM page. Do you know what speed your RAM is supposed to be?  I would personally not worry much about overclocking or undervolting the card. Performance is unlikely to change much.,buildapc,2026-01-04 10:06:50,1
Intel,nxlqwq2,"I am using the stock cooler, yes. The next investment needs to be a good cooler that looks good also, just hard to find an affordable one since I would ideally prefer liquid cooling.",buildapc,2026-01-04 10:23:38,0
Intel,nxlpind,Just went to my Amazon order history and the ram I bought was: Corsair VENGEANCE RGB PRO DDR4 RAM 16GB (2x8GB) 3200MHz,buildapc,2026-01-04 10:11:06,1
Intel,nxlpngg,"So, 3200MHz to answer your question, on Task Manager the RAM states to be running at 2133 MT/s (I don’t exactly know what this means, or whether it’s up to speed or not as it’s a different measurement to MHz)",buildapc,2026-01-04 10:12:17,1
Intel,nxmb88n,"If space allows it, grab a Phantom Spirit 120, not too expensive and I've found it does a great job cooling my 5700x3d",buildapc,2026-01-04 13:10:18,1
Intel,nxlq5ve,"2133 is the base speed of DDR4, which means that your RAM is not currently overclocked. I would suspect this is what is causing your performance issues.  If you go into your BIOS, there should be a setting somewhere for loading up a 'XMP' profile that says 3200 on it. Enable that and then boot your PC and check task manager again.",buildapc,2026-01-04 10:16:54,1
Intel,nxlqa97,"Okay, I shall do just this and get back to you ! Thank you for the time being!",buildapc,2026-01-04 10:18:00,1
Intel,nxlqfjk,If you need help you can probably find a YouTube video showing you where the setting is. Basically all Asus BIOS should look the same. I think it's on the left side of the main page in the 'EZ Mode'.,buildapc,2026-01-04 10:19:19,1
Intel,nxlssvm,"Okay, I have OCd my RAM and it’s at 3200 MT/s now. Thank you very much, let me test my performance now!",buildapc,2026-01-04 10:40:44,2
Intel,nw7r87c,B580.  VRAM matters.,buildapc,2025-12-27 16:51:43,9
Intel,nw7rzuo,"easily an rtx 3070. the 3070 destroys b580 in pure rasterisation to the point where vram doesn't make a difference.i would instead look at the 9060xt 8gb instead of the b580, which is newer than the 3070, performs slightly better, uses less power and has more features like fsr 4",buildapc,2025-12-27 16:55:36,4
Intel,nw7tqjq,B580 is hard to beat at its current discounted price.,buildapc,2025-12-27 17:04:19,2
Intel,nw7urnc,"B580 and if you’re into one of the 4 free games, that’s an added bonus",buildapc,2025-12-27 17:09:35,1
Intel,nw8bavu,If you plan to use a linux distro then B580.,buildapc,2025-12-27 18:33:00,1
Intel,nw7sbd8,B580. Do not support NVidia for their predatory market manipulation.....,buildapc,2025-12-27 16:57:10,-1
Intel,nw8es7m,"Get an NVIDIA GPU, probably the RTX 4060. The GeForce cards before the 4000 series don't support Frame Generation, while the RTX 4060 will support it.   NVIDIA DLSS + Frame Generation together is awesome.",buildapc,2025-12-27 18:50:23,0
Intel,nw7ufdn,"Where are you located (i only deal with locals)  If youre in canada, im going to be selling my rtx3070ti for $250 cad.  Never overclocked, was used in my gaming rig.",buildapc,2025-12-27 17:07:50,-2
Intel,nw88ryo,"True, true",buildapc,2025-12-27 18:20:29,1
Intel,nx0v4hv,"CPUs don't support resizable BAR, devices do. Arc cards don't require resizable BAR it's just expected to be used.",pcmasterrace,2026-01-01 03:06:37,2
Intel,nx0ze2r,"Rebar isn't needed for Arc gpus to function, but it is needed for them to function as intended. I believe Arc does need a motherboard with a UEFI bios, but could be wrong on that.",pcmasterrace,2026-01-01 03:35:19,2
Intel,nwywe0i,You can look up compatibility on the PC builder site.   I got a Z790 for 125$   I got the I5-14600Kf for 150$   It holds up really nice.  BE REALLY CAREFUL the Intel GPUs need REBAR so keep your eyes peeled for that.,pcmasterrace,2025-12-31 20:02:48,2
Intel,nwyvx1g,"Must be nice to have a MC, my closest is a few thousand miles away.",pcmasterrace,2025-12-31 20:00:16,1
Intel,nwz9s45,intel 13th and 14th gen have cpu failures so only buy if you are willing to risk that,pcmasterrace,2025-12-31 21:15:39,0
Intel,nwyxczb,Totally forgot about the rebar. Thanks for the reminder. My current cpu and mobo support rebar and I have it activated. Definitely something I’ll have to consider when I upgrade to ddr5,pcmasterrace,2025-12-31 20:08:03,1
Intel,nwyxm93,Would 12600kf for 100 be good?,pcmasterrace,2025-12-31 20:09:27,1
Intel,nwyxsvs,A few thousand miles? Sorry that sucks. Their bundles/combos on parts are better than sex sometimes.,pcmasterrace,2025-12-31 20:10:26,2
Intel,nwyxsdq,I mean.. CPUs are pretty far ahead of GPUs at least in terms of gaming.   Unless you planned on running 1440 with everything on ultra the B580 is great on most games. You’ll never feel like you are forced to run a game on low graphics.,pcmasterrace,2025-12-31 20:10:22,1
Intel,nwyy97j,If you plan on investing in Intel GPUs I would press you to get the newest gen.   Intel ARC B580 which is an entry Intel GPU didn’t play nice with older Intel Chips   12600 would hold up just fine.,pcmasterrace,2025-12-31 20:12:54,1
Intel,nwyyd72,Thanks there’s also the brand new core ultra 5 225f with bf6 for 150 on amazon. Would ultra core be a better fit?,pcmasterrace,2025-12-31 20:13:30,1
Intel,nwyyobw,If you don’t plan on upgrading it’s fine but if Intel releases a newer card that more powerful it might get bottlenecked.,pcmasterrace,2025-12-31 20:15:12,1
Intel,nwyyr3l,Dang maybe am5 cpu is better,pcmasterrace,2025-12-31 20:15:37,1
Intel,nwz119d,I’d say to go AMD.   Intel royally fucked up in that race but since they did have a major price drop because of it I scooped one up cheap.,pcmasterrace,2025-12-31 20:27:59,1
Intel,nv5otsy,Going to be a downgrade obviously. Intel has Been getting to the point those cards are actually good but it’s still going from high mid tier to low tier,pcmasterrace,2025-12-21 06:00:51,1
Intel,nv6e0b8,you got a free game from intel,pcmasterrace,2025-12-21 10:05:37,1
Intel,nv6e403,as a low tier card it's quite decent. i use it to play games at 4k,pcmasterrace,2025-12-21 10:06:36,1
Intel,nv6hh4w,Yep went in expecting a downgrade but I don't game too much lately haven't found the latest games all that good and mainly playing older games at the moment so the trade off for now isn't huge for me.,pcmasterrace,2025-12-21 10:39:59,1
Intel,nv6hlv9,Yep Battlefield 6 key.,pcmasterrace,2025-12-21 10:41:18,1
Intel,nw0ptay,How?,pcmasterrace,2025-12-26 13:17:50,1
Intel,nvaa10t,Very cool Exxazy,pcmasterrace,2025-12-22 00:00:40,1
Intel,nvacuz3,"![gif](giphy|Idg2rAVGS3xMZtBdhu|downsized)  It’s fine, plastic doesn’t conduct electricity 🫣",pcmasterrace,2025-12-22 00:16:49,1
Intel,nvag1q5,![gif](giphy|xT5LMtbEtZnbbCE08g),pcmasterrace,2025-12-22 00:35:00,1
Intel,nvavrl6,It lost its sparkle  ![gif](giphy|63VH5uoslatcA),pcmasterrace,2025-12-22 02:08:58,1
Intel,nvb01k1,[https://www.enostech.com/sparkle-intel-arc-b-series-graphics-cards/](https://www.enostech.com/sparkle-intel-arc-b-series-graphics-cards/)  Is this your card? So it's like a design / badge of the brand,pcmasterrace,2025-12-22 02:35:26,1
Intel,nvbewrp,Remove it before it falls in the fan and fucks it up.,pcmasterrace,2025-12-22 04:11:20,1
Intel,nxmjc0s,What exactly is the point of putting 2 different screenshots with different in game locations?,pcmasterrace,2026-01-04 14:00:52,3370
Intel,nxmf1kv,But can't you just adjust the monitor/ TV brightness etc,pcmasterrace,2026-01-04 13:35:00,929
Intel,nxmmzmk,"This literally changes nothing. It just disables the in game overlay and the ability to have preset filters. You can still go the Nvidia panel and change contrast, gamma, and digital vibrance to achieve the same effect. It applies it to the monitor rather than a specific program",pcmasterrace,2026-01-04 14:22:31,286
Intel,nxmdz77,"Always sad when the filters are disabled, people who abuse them just do it the other way and other people who just use them to make the game look lil better witthout really boosting the brightness up suffer.",pcmasterrace,2026-01-04 13:28:14,167
Intel,nxmgavr,Well I'm screwed my monitor sucks in the blue part of the color spectrum,pcmasterrace,2026-01-04 13:42:50,35
Intel,nxmd6jv,"Filters gave PC players a clear advantage, this evens things out.",pcmasterrace,2026-01-04 13:23:06,501
Intel,nxmhr6n,Oof this better not disable RTX HDR  Edit: I would rather have the intended picture but i wonder if you could get a similar effect with windows HDR then deliberately mis-calibrating your display to have a higher max nits.,pcmasterrace,2026-01-04 13:51:31,29
Intel,nxmgr7f,Yeah cause monitors don't have gamma settings xD,pcmasterrace,2026-01-04 13:45:36,17
Intel,nxmhaz4,Ok now disable aim assist on consoles and NOW we're fair,pcmasterrace,2026-01-04 13:48:55,245
Intel,nxmiwc3,Let's ignore monitors can do same thing...mine has some AI Night vision...,pcmasterrace,2026-01-04 13:58:14,22
Intel,nxmjci3,Should do it in escape from tarkov as well...,pcmasterrace,2026-01-04 14:00:56,5
Intel,nxmormb,"DayZ the game that dealt with this problem and no night vision besides the in-game night vision googles will let you see at night.  If you will try to fix this problem, you will see that game actually insanely grainy at night. You practically can barely see anything outside of things that you were already able to see. See someone in the bushes? Good luck with that. See someone 10 meters away? Yeah, good luck.",pcmasterrace,2026-01-04 14:32:35,6
Intel,nxmhw1o,Mom said it was my turn to post this now,pcmasterrace,2026-01-04 13:52:18,22
Intel,nxmp5rj,Do they get how folks don't have the same exact eyesight they want?   I know I don't with my thick glasses.,pcmasterrace,2026-01-04 14:34:48,5
Intel,nxmp5w1,Damn I guess that's some ugly graphics these sweaty players must suffer through to get an advantage? All the best to them I guess. Imagine if they still get killed 🤣,pcmasterrace,2026-01-04 14:34:49,5
Intel,nxs2pj5,I don't play Arc so I don't really care. But I hate when games where it's important to see shit make it hard to see shit depending on your settings and hardware setup.,pcmasterrace,2026-01-05 07:17:59,4
Intel,nxmke0u,"I play games for fun. You will never see me lowering the settings in a competitive games, unless I get low frame rate. I play for fun.",pcmasterrace,2026-01-04 14:07:10,12
Intel,nxmhf8e,Alienware monitors have alien vision and it’s completely broken. The night mode or sniper mode is so broken it’s like the original arc filters but on steroids.,pcmasterrace,2026-01-04 13:49:37,8
Intel,nxp5usc,Literally who cares in a game that's riddled with wallhackers lmao,pcmasterrace,2026-01-04 21:27:05,3
Intel,nxra75a,"I like playing in the dark, feels more immersive",pcmasterrace,2026-01-05 03:56:45,3
Intel,nxoxmih,One of the biggest reasons I dumped PvP titles all together. Min/Max culture has absolutely destroyed gaming as a whole.,pcmasterrace,2026-01-04 20:48:44,8
Intel,nxmh8mp,People who turn down their res and graphics to get advantages are so sad.   Way to sweaty.,pcmasterrace,2026-01-04 13:48:31,27
Intel,nxmnr4z,Sooooo… Tarkov next?,pcmasterrace,2026-01-04 14:26:51,2
Intel,nxmrd7n,Common cross play L,pcmasterrace,2026-01-04 14:47:09,2
Intel,nxpx9fw,I mean I can get this.  This is the same as using modded skins in old counterstrike to make players light up like leaking gaspipes.,pcmasterrace,2026-01-04 23:38:19,2
Intel,nxs405z,like people cannot adjust their brightness and saturation from nvidia control panel,pcmasterrace,2026-01-05 07:29:38,2
Intel,nxs8awp,"Here I am never intending to play the game and just watching the discourse, as is any multiplayer game 😆.  I'm all for a (more) balanced playing field though",pcmasterrace,2026-01-05 08:08:59,2
Intel,nxsey2p,Good. I'll never understand why people do this. I play games to be immersed not to use every trick in the book to get an advantage. Graphics settings are always turned up.,pcmasterrace,2026-01-05 09:12:06,2
Intel,nxmkljj,Let's just cut cross-play.,pcmasterrace,2026-01-04 14:08:24,4
Intel,nxmmrzk,I play on pc and I hate ppl who do this. Even when on console never upped the brightness for dark places. It's for the weak,pcmasterrace,2026-01-04 14:21:18,5
Intel,nxowmzf,"Cool, now please disable auto aim on consoles.",pcmasterrace,2026-01-04 20:44:10,3
Intel,nxovmoq,Let it be known only PC Gamers with micropenises (micropeni?) use Nvidia Filters to get an advantage. Real men play games all natural.,pcmasterrace,2026-01-04 20:39:26,3
Intel,nxmj80g,I mean I just don’t get why PC players like their shit looking terrible. Flex a 2k+ build but it looks like doo doo intentionally just for a way to win..,pcmasterrace,2026-01-04 14:00:11,5
Intel,nxpl486,"Pc player here, why can’t all of you pussies just leave your brightness alone and deal with it. The game is supposed to get dark, stop trying to adjust your settings to get an unfair advantage",pcmasterrace,2026-01-04 22:38:11,3
Intel,nxmtavx,so whiny console peasants got something nerfed on pc,pcmasterrace,2026-01-04 14:57:44,2
Intel,nxp3ihc,Good. Look at Tarkov. Streamers were playing that with their shit looking like fuckin Fortnite...,pcmasterrace,2026-01-04 21:16:13,2
Intel,nxrxiix,They forgot game is supposed to be fun,pcmasterrace,2026-01-05 06:33:50,2
Intel,nxmjpsp,What after this?! Are people going to complain that setting the graphics to low gives people advantage?,pcmasterrace,2026-01-04 14:03:06,2
Intel,nxnd13f,Good. If they want to cheat by having better vision and/or esp at least make them work for it.,pcmasterrace,2026-01-04 16:33:55,1
Intel,nxrg0bg,this fixes absolutely nothing you just move from ingame filter to nvidea settings and or monitor settings and guess what guys your tv settings can do the same,pcmasterrace,2026-01-05 04:30:43,1
Intel,nxmhutn,"I don't play arc raiders, but I hate it when games clamp down on stuff like this if they aren't actually taking concealment and camouflage seriously as a game mechanic. It's even worse when the maps aren't designed well around balancing hiding and exposure. It just feels so random.",pcmasterrace,2026-01-04 13:52:07,2
Intel,nxmk8gv,I think my monitor has a function that does the same as the filter did.,pcmasterrace,2026-01-04 14:06:15,1
Intel,nxmn3tj,Idk what to compare anything too since each image is a different location with presumably its own lighting,pcmasterrace,2026-01-04 14:23:12,1
Intel,nxmn7ym,It’s always the double braided hair.,pcmasterrace,2026-01-04 14:23:51,1
Intel,nxmo76y,What,pcmasterrace,2026-01-04 14:29:22,1
Intel,nxmqo7r,"Isn't every game like this? Turn down the settings, insane frames, you're basically just looking at pixels.",pcmasterrace,2026-01-04 14:43:18,1
Intel,nxmr9pg,Reminds me of how back in the day you could turn the graphics down to the minimum to not load bushes in War Thunder,pcmasterrace,2026-01-04 14:46:37,1
Intel,nxmrdak,Oh no! Game is r/literallyunplayable now,pcmasterrace,2026-01-04 14:47:09,1
Intel,nxmske4,Changes nothing you can still do it with other software and Arc raiders and Nvidia cant do anything about it,pcmasterrace,2026-01-04 14:53:44,1
Intel,nxmtny1,This is literally Harrison Bergeron. /s,pcmasterrace,2026-01-04 14:59:40,1
Intel,nxmvxua,God forbid they actually optimize anything.,pcmasterrace,2026-01-04 15:11:43,1
Intel,nxowbnn,"Same thing happened in Dayz, there's always someone using the filters or in DayZ's case - Reshade, to cheat. Sucks too, it's hard to look at dayz without Reshade.",pcmasterrace,2026-01-04 20:42:41,1
Intel,nxox6es,You can increase the brightness of your monitor in the nvidia control panel or in your monitors settings.,pcmasterrace,2026-01-04 20:46:38,1
Intel,nxp3wp9,Okay anyways. Darken the ingame settings and then go to Nvidia control panel and increase your settings there. Boombadabing.,pcmasterrace,2026-01-04 21:18:03,1
Intel,nxp5vt7,You can still easily do this with third party programs.,pcmasterrace,2026-01-04 21:27:13,1
Intel,nxp6sfl,"Im not sure which aspect of the lighting specifically causes it but Arc is definitely hard on the eyes for me, to the point where its literally the only game Ive ever used nvidia filters to alleviate it. Now only players with a monitor that has shadow settings can do what the filters were doing which isn't limited to pc players. Messing with nvidia control panel is not a good alternative.",pcmasterrace,2026-01-04 21:31:25,1
Intel,nxpgg4k,You know the only thing the filters ever get used for is the unfair advantage in pvp games idk why most even bother having it enabled at first only to disable it later because they realized “ah people are just sad” the filters are great but more so for singleplayer games,pcmasterrace,2026-01-04 22:16:11,1
Intel,nxpkoty,Me when i make my game look worse for no reason,pcmasterrace,2026-01-04 22:36:12,1
Intel,nxqg6kg,I have a genuine question. Regardless of whether or not you think it’s ok for people to utilise this feature.   Why would Nvidia go through the effort of disabling this for arc raiders? Just because Embark asked? Is it typical for them to work that closely with game studios?,pcmasterrace,2026-01-05 01:13:24,1
Intel,nxqi30b,"me monitor has night vision, gg",pcmasterrace,2026-01-05 01:23:40,1
Intel,nxqwxz3,This was a convoluted way to do it anyways that made your game look better than doing it the old way.  If you just type “calibrate” in ur search calibrate display will come up. Press next 3x and turn up ur gamma. And then just turn up saturation/digital vibrancy in control panel..or you can do it all thru control panel but I like the slider on the calibrate display windows app..once you exit the window ur gamma goes back to normal.. You can then see in the dark any game (I used this for Roblox rust like 6 years ago)  Everyone’s monitor can do the same anyways. I have an oled so I literally can barely see in the dark regardless of settings unless I do the black boost thing but that makes black looks gray and it’s ugly,pcmasterrace,2026-01-05 02:42:38,1
Intel,nxs3z5v,"Same thing happened in Hunt Showdown, people where using filters and reshade to abuse vision",pcmasterrace,2026-01-05 07:29:23,1
Intel,nxu6st8,"it may be an advantage, but its not an unfair advantage. Anyone can increase their tv/monitor brightness.",pcmasterrace,2026-01-05 16:14:21,1
Intel,nxus2te,Hey thanks for the reminder that gamers actually are as annoying as sports goons,pcmasterrace,2026-01-05 17:52:49,1
Intel,nxvfumz,That shiz is ugly regardless,pcmasterrace,2026-01-05 19:39:55,1
Intel,nxwy5l7,Meanwhile every cheap Gaming monitor: [https://youtu.be/rqvUeAx--Qk](https://youtu.be/rqvUeAx--Qk)  I have AOC screen for 100 Euro and this is video from my monitor actualy in very dark place the rathaus in buried city,pcmasterrace,2026-01-06 00:02:31,1
Intel,nxz3ntl,Doesn't really fix it unless they add a film grain filter during the night the way dayz/arma did it.,pcmasterrace,2026-01-06 08:26:09,1
Intel,ny0stxu,"NVCP, brightness 55%, contrast 72%, gamma however much you need. There are your filters that they can’t disable, enjoy.",pcmasterrace,2026-01-06 15:33:56,1
Intel,ny4svp5,I'd rather die than make my games look like ass.,pcmasterrace,2026-01-07 03:08:17,1
Intel,nxpymgn,"Who cares? You can increase gamma and raise black level on any modern monitor or tv and make the night raids look like day raids without any ""filters""",pcmasterrace,2026-01-04 23:45:09,1
Intel,nxmo5e1,I mean playing on consoles is dog shit anyway with a controller. You're competing with someone who can b-hop around you and point-click headshots vs someone in a tank: LEFT!!!!........ RIIIIGHT!!!....... UUUUP!!!,pcmasterrace,2026-01-04 14:29:05,1
Intel,nxmnouw,They also did similar for The Finals and it's caused issues with higher end cards running the game. I saw 3 or 4 posts about it on their sub the other day.,pcmasterrace,2026-01-04 14:26:29,1
Intel,nxmtygg,"Can still adjust the monitor and windows color management.  The only solution is a locked system with only one approved monitor as if you were on some irl tournament.  As long as we have the ability to manage the monitors and windowsthen this type of ""cheating"" will always exist.",pcmasterrace,2026-01-04 15:01:14,1
Intel,nxow0rr,reshade  https://preview.redd.it/6svd04w79ebg1.jpeg?width=588&format=pjpg&auto=webp&s=7b040ea81f23edc551b0f22927e4dec1a22f1490,pcmasterrace,2026-01-04 20:41:17,1
Intel,nxp3juy,Good. Look at Tarkov. Streamers were playing that with their shit looking like Fortnite...,pcmasterrace,2026-01-04 21:16:24,1
Intel,nxr799l,Not sure how I feel about this one,pcmasterrace,2026-01-05 03:39:39,1
Intel,nxrfych,I dont use filters but i want to cause i legit cant see shit sometimes on my HDR monitor and it fucking sucks,pcmasterrace,2026-01-05 04:30:22,1
Intel,nxmh6g8,At this point nvidia just hates us,pcmasterrace,2026-01-04 13:48:08,0
Intel,nxmpqbb,Lets lock fps to so console players have fair chance.,pcmasterrace,2026-01-04 14:38:04,0
Intel,nxoxa74,\*Laughs in AMD\*,pcmasterrace,2026-01-04 20:47:08,0
Intel,nxs08v9,"Amateur move, should've removed mouse and keyboard support.",pcmasterrace,2026-01-05 06:56:43,0
Intel,nxou3f0,I'd give a damn if these developers gave a damn about Voice Actors,pcmasterrace,2026-01-04 20:32:23,-2
Intel,nxq4tt0,Laughs in AMD,pcmasterrace,2026-01-05 00:15:39,-1
Intel,nxqv7su,That will not fix it at all. Ppl changing gamma in control panel... Embarc surprises me weekly with how little they do in terms of balancing. Even their filters change does fuck all...,pcmasterrace,2026-01-05 02:33:25,-1
Intel,nxrdjou,Lets keep it a buck arc raiders is garbage and more garbage then that is being restricted to make the console plebs happy,pcmasterrace,2026-01-05 04:16:27,-1
Intel,nxms2a7,Good thing I use Dolby vision and I can still make my game look like this at night. They fixed nothing,pcmasterrace,2026-01-04 14:51:00,0
Intel,nxmtxwu,Lmao what excactly the first filter actually do to gain that of an avantage? Remove shadows?,pcmasterrace,2026-01-04 15:01:09,0
Intel,nxmug8a,I have an oled. I don’t even have to use a flashlight in the dark areas.,pcmasterrace,2026-01-04 15:03:53,0
Intel,nxp1c97,huh...didnt even know there was such a thing like filters...  how is nvidia enforcing this? through game client or driver update?,pcmasterrace,2026-01-04 21:06:07,0
Intel,nxsaqd4,"With filters, you can see much better, but I don't think that this is huge difference",pcmasterrace,2026-01-05 08:32:00,0
Intel,nxqyl2l,"So we remove PC filters for “fairness”, but console aim assist stays borderline aimbot, Cronus is everywhere, and somehow it’s still the PC players’ fault. Crossplay didn’t expose PC advantages, it exposed hypocrisy. If you really care about fair play, start by nerfing consoles.",pcmasterrace,2026-01-05 02:51:19,-3
Intel,nxmslsx,Can someone tell me why Nvidia has control over customer hardware?,pcmasterrace,2026-01-04 14:53:57,-1
Intel,nxp9cio,"So funny thing: disabling filters seemed to make things more washed out, like the left image, while the right image was closer to what I remembered the game looked like before this happened.  Fun Fact: you can still apply monitor-wide filters via the Nvidia Control Panel and the game doesn’t stop it, it was how I got the game to look normal again after everything looked washed out. (This is the only game so far that has looked washed out)",pcmasterrace,2026-01-04 21:43:13,-1
Intel,nxms7hb,This post is so lazy it should just be removed.,pcmasterrace,2026-01-04 14:51:47,-2
Intel,nxmedpk,"I used to use filters to play CSGO, I could see through smokes and never got banned, people raged in every game",pcmasterrace,2026-01-04 13:30:48,-47
Intel,nxov8lw,"Because whoever threw that image together is lazy as shit.  The left is clearly screenshotted from a YouTube video, you can see the cursor in the bottom left. And the other is probably just some random screenshot they found.",pcmasterrace,2026-01-04 20:37:37,920
Intel,nxmojpr,Probably because it doesn't do them justice to do the same scene.,pcmasterrace,2026-01-04 14:31:20,522
Intel,nxovesw,Also the screenshot depicting NVIDIA filters isn’t using NVIDIA filters at all. It’s using gamma correction from NVIDIA control panel.,pcmasterrace,2026-01-04 20:38:25,48
Intel,nxmtwws,Different locations wouldn't be too bad. Different maps is stupid.,pcmasterrace,2026-01-04 15:01:01,44
Intel,nxsiqvx,"It's the same location, it just that different /j",pcmasterrace,2026-01-05 09:48:30,1
Intel,nxs9trc,"I mean regardless you can still see the differences plain as night.  The first one has effectively no shadows due to nvidia's version of curves and technicolor being edited, looks like they used some sort of modified levels as well to make the whole image a bit brighter and making dark spots also brighter.  Idk what they're called on nvidia game filters, I use reshade on all of my games that allow it personally but for cheating reasons. I just like better color shading \*shrug\*",pcmasterrace,2026-01-05 08:23:26,1
Intel,nxpumhj,"Illustrative purposes. It isn't showing apples to apples, but someone who hasn't played the game wouldn't know that. They *would* know, just by looking at the two photos and seeing one is brighter than the other, roughly what the problem and solution were.",pcmasterrace,2026-01-04 23:25:07,-10
Intel,nxmgo5c,That would be fair as you can do that with a console too.,pcmasterrace,2026-01-04 13:45:06,470
Intel,nxmo0pa,My monitor has a mode that lightens shadows for fps games,pcmasterrace,2026-01-04 14:28:20,66
Intel,nxmrbmk,"Yeah in nvcp put gamma 1.20, saturation 75%, contrast 52% and you’re good. Thats my Tarkovs night raid settings because im on oled (all black) , but this game doesnt need that lmao.",pcmasterrace,2026-01-04 14:46:54,15
Intel,nxp0c36,"I have an LG, Gamer 1 is my regular preset, Gamer 2 is my *I need to actually see shit* preset, Gamma 1, full black stabilizer. In EFT, notorious for super shitty color grading that melds people with objects borders, it's a godsend. I also activate it in ArmA Reforger during night if I don't have NVGs.",pcmasterrace,2026-01-04 21:01:23,1
Intel,nxpud8z,"The Nvidia filters are really easy to swap between, there used to be a filter setup you could do in Counter Strike that would severely limit the effect of smokes, so you can flip between 2 filters when a smoke lands in front of you for example.   I don’t think it was what most people were using the filters for, but definitely would have an impact",pcmasterrace,2026-01-04 23:23:51,1
Intel,nxt4k56,"Yes but that's fundamentally different than Nvidia settings. Nvidia settings can fundamentally alter how in-game lighting is rendered to you, Nvidia also openly allows games to decide if they want to enable it. More competitive games usually disable like Rust and Tarkov.   Arc raiders makes sense to disable it.   I have used it in DayZ before and it's hard to convey just how crazy it is except that night is as easy to see in as day. It makes it look like just an overcast day, no blurry vignette like you get with brightness and contrast.",pcmasterrace,2026-01-05 12:47:29,1
Intel,nxp93w2,Yeah filters can let you do all sorts of shit. But all you need is gamma/contrast. Pretty much the same shit.,pcmasterrace,2026-01-04 21:42:06,0
Intel,nxmp4jj,It’s not the same thing at all. It will still have the same ratio of light to dark between surfaces,pcmasterrace,2026-01-04 14:34:36,-11
Intel,nxrl1ql,If you say its the same you didnt use filters,pcmasterrace,2026-01-05 05:00:41,-33
Intel,nxmk0dl,"Did they only disable the color Filters or also the HDR one? With the ingame/normal Windows Auto HDR the Game Looks Like Shit one my end,even though i calibrated my Monitor. Would be a shame.",pcmasterrace,2026-01-04 14:04:52,27
Intel,nxov92n,It's pretty funny because Nvidia control panel can change brightness and gamma anyway and isn't blockable 😂,pcmasterrace,2026-01-04 20:37:40,16
Intel,nxmsc8t,Yeah this really only hurts more casual players. Hardcore players will just use their monitor settings but more casual players might not want to change their monitor settings solely for one game.,pcmasterrace,2026-01-04 14:52:30,11
Intel,nxt2flk,"I'm screwed either way, because my monitor sucks in every part of the color spectrum",pcmasterrace,2026-01-05 12:32:25,4
Intel,nxmeb3z,Pc players will just do it the old fashioned way to adjust thier monitor settings,pcmasterrace,2026-01-04 13:30:21,364
Intel,nxmiscc,And here I am with an OLED seeing absolutely nothing the moment a corner doesn't have a light,pcmasterrace,2026-01-04 13:57:33,39
Intel,nxmqw9c,> this evens things out.  ...and also shows that your HW isn't yours anymore.,pcmasterrace,2026-01-04 14:44:33,6
Intel,nxmjobv,"my monitor has a cross hair display. Can't take that away, can you, there Jensen Huang?",pcmasterrace,2026-01-04 14:02:52,13
Intel,nxmeqnl,Did you forget to switch accounts😭,pcmasterrace,2026-01-04 13:33:06,22
Intel,nxmf2ob,"Not really, everyone can still use their Nvidia CP to turn up gamma and brightness, AMD users can easily use windows gamma and adrenalin filters.   Still a good step in the right direction, the filters should never have been allowed.",pcmasterrace,2026-01-04 13:35:12,43
Intel,nxmryt8,"Damn We got Twitter-style OF bots now. At least, this is my first time spotting one esp. bc of that NSFW pfp.",pcmasterrace,2026-01-04 14:50:27,6
Intel,nxml5iu,PC players still have the other 9/10 advantages,pcmasterrace,2026-01-04 14:11:43,5
Intel,nxmnazm,Mouse and keyboard is already an advantage,pcmasterrace,2026-01-04 14:24:20,4
Intel,nxozs5k,I don't play arc but I've never seen anyone I know use any filter in any game despite a potential advantage. Some people surely do but I don't think that's a majority. Wanting your game to look like shit only to gain a little advantage isn't worth it in my opinion.,pcmasterrace,2026-01-04 20:58:47,2
Intel,nxq0elv,But console players have Aim Assist.,pcmasterrace,2026-01-04 23:54:07,2
Intel,nxmhzoc,It's called masterrace for a reason ;-),pcmasterrace,2026-01-04 13:52:53,6
Intel,nxmti8j,Mouse/kb already gives PC a clear advantage.,pcmasterrace,2026-01-04 14:58:50,0
Intel,nxmwapn,"PC players have been using 3rd party software like reshade for a long time now, this change hardly matters.",pcmasterrace,2026-01-04 15:13:35,0
Intel,nxmj0ls,"""Clear advantage"" like TVs don't have brightness/gamma settings too 😭  I promise you nothing has changed except that it's more inconvenient now, which is good, but it's 100% on Embark to solve this. I haven't played Rust in like a decade, but I remember at one point at least they would sample the darkest areas on the screen and make them pitch black no matter the brightness in order to fight this tactic.",pcmasterrace,2026-01-04 13:58:57,-5
Intel,nxmfmcs,They should cap PC players to 120FPS in Fortnite so they don’t get unfair advantage.,pcmasterrace,2026-01-04 13:38:39,-32
Intel,nxp8dyy,This did seem to disable RTX HDR. Everything looked extremely washed out after this update. I wasn’t sure what was going on.,pcmasterrace,2026-01-04 21:38:48,7
Intel,nxmi3af,"I don't really play shooters, but I was under the impression that Mouse/Keyboard is usually ""better"" than controllers with aim assist.    Is that not the case here?",pcmasterrace,2026-01-04 13:53:27,73
Intel,nxmidkv,"I get where you're coming from with this but let's be real, even ~~id~~ if you're not that good at shooters it's still way easier to aim with the mouse than even with aim assisted controller. And I'm not saying that in a ""PC players are playing an easier game"" way, on the contrary believe it raises the skill ceiling and it's why all of the serious shooter esports are on PC.",pcmasterrace,2026-01-04 13:55:09,-2
Intel,nxmkxg9,![gif](giphy|JjiieDMHZ6pEI),pcmasterrace,2026-01-04 14:10:21,1
Intel,nxouh6q,I don't think the aim assist in arc raiders is that strong compared to games like apex or cod.,pcmasterrace,2026-01-04 20:34:07,1
Intel,nxmnnzt,If you can't win against controller players without a night vision shader filter you might as well stop playing pvp games lmao,pcmasterrace,2026-01-04 14:26:21,1
Intel,nxmm2ln,"If you want to absolutely kill a console game then yeah, disable aim assist.",pcmasterrace,2026-01-04 14:17:10,-1
Intel,nxmtrbz,Aim assist is no where near as bad as you all keep making it out to be. I'm on pc btw,pcmasterrace,2026-01-04 15:00:11,-5
Intel,nxmo3xh,Same...it actually works pretty well in arma reforger too...,pcmasterrace,2026-01-04 14:28:51,0
Intel,nxmlj18,"That shit actually works?! I just never touched those settings cause they always looked too stupid to use. Definitely won't be using them now though, I'd rather have a skill issue and lose than be cheating and still lose.",pcmasterrace,2026-01-04 14:13:57,2
Intel,nxqvquv,"You could do this on consoles too, hdcp has been broken for years and they can't ensure your monitor gets what the console wants you to see.",pcmasterrace,2026-01-05 02:36:18,1
Intel,nxp1ha7,"Blame devs, realistic lighting/fog/etc does not make a shooter fun.",pcmasterrace,2026-01-04 21:06:45,-11
Intel,nxmlpgh,\>Buy a 5090 for the best performance possible   \> Turn the graphics to low,pcmasterrace,2026-01-04 14:15:01,15
Intel,nxmi76x,"I know people who are in the lowest Counter-Strike leagues that do that. They want the FPS for faster reaction and I'm like... dude, the weakest link in your setup is you.",pcmasterrace,2026-01-04 13:54:05,20
Intel,nxsig5x,Gama and saturation is even more op.,pcmasterrace,2026-01-05 09:45:43,1
Intel,nxuo8aj,"You answered it yourself: you like to be immersed, people who use filters like this just want every advantage they can get to win.. cause that is the only thing they care about, to Win. I mean i‘m on your side but yeah they can now just fiddle around with the contrast and brightness on the Monitor/Screen settings and will get the same result",pcmasterrace,2026-01-05 17:35:20,1
Intel,nxmi356,At this point? Nvidia has been steadily fucking everyone over atleast for the last decade. Only thing they don't hate is their wallet,pcmasterrace,2026-01-04 13:53:25,10
Intel,nxmkyik,For once they fucked with people who deserve it.,pcmasterrace,2026-01-04 14:10:32,-2
Intel,nxmelk8,"That's because they were adding filters that improved visibility at night, not the one to make the game ""prettier""",pcmasterrace,2026-01-04 13:32:11,34
Intel,nxmfbul,"Filters are used to degrade graphics quality for getting advantage in competitive games. Oldeest example, i guess, in quake 2/3 people were making location textures just a white material to make enemy model seen more easily",pcmasterrace,2026-01-04 13:36:49,8
Intel,nxmdqx0,They can also ban reshade. Reshade was super popular in Ark Survival Evolved as it pretty much gave you wallhacks until it eventually got banned,pcmasterrace,2026-01-04 13:26:45,3
Intel,nxme0x8,Isnt reshade already banned?,pcmasterrace,2026-01-04 13:28:33,1
Intel,nxrzvbz,Which does not have filters and so complains about not being fair,pcmasterrace,2026-01-05 06:53:31,1
Intel,nxpp393,Or why tf they care about how people play Arc Raiders?   Stay in your fucking lane.,pcmasterrace,2026-01-04 22:57:35,0
Intel,nxmf0vc,Thats why we can’t have nice things.,pcmasterrace,2026-01-04 13:34:52,32
Intel,nxmhoit,Except that you can't. There is no setting to do that.,pcmasterrace,2026-01-04 13:51:06,7
Intel,nxmpj22,Either this or they’re just lazy,pcmasterrace,2026-01-04 14:36:54,118
Intel,nxr2xey,Which can't be stopped can it?,pcmasterrace,2026-01-05 03:15:20,8
Intel,nxrmbv9,"It is using the filters, you can't take a screenshot with gamma correction from control panel since that only changes visually on your monitor and it's not really a ""filter"" just for that game.   Also because you can't really achieve that much color, visibility and sharpened image with just the NVIDIA control panel settings, game still looks kinda blurry after all.  https://preview.redd.it/5bd2uk8mrgbg1.png?width=2128&format=png&auto=webp&s=a06ff7a46677fe725f48a77d58be141b63dc1e43",pcmasterrace,2026-01-05 05:09:11,6
Intel,nxujeab,"Or just stop pairing pc with consoles, problem fixed",pcmasterrace,2026-01-05 17:12:57,2
Intel,nxprsd0,It just reduces contrast,pcmasterrace,2026-01-04 23:11:05,28
Intel,nxqsj5i,Yup i use that for arc its a game changer for dark areas forsure,pcmasterrace,2026-01-05 02:18:58,2
Intel,nxoxzqy,"Im on an oled playing tarkov as well. Thanks for this. You got good settings for day time? I was using 1.5 gamma and 60% contrast. It hurt my eyes how bright it was with the snow, hell the whole game was to bright. Now playing witj 1.3 gamma and 55% contrast but I wonder what you are using.",pcmasterrace,2026-01-04 20:50:26,3
Intel,nxpj6k5,do you play Battlefield 6? wondering how those settings impact the game. with my OLED some enemies can be right in front of me and I cant see em,pcmasterrace,2026-01-04 22:29:08,1
Intel,nxrmics,"true lol, it's not even close",pcmasterrace,2026-01-05 05:10:25,-15
Intel,nxslh9k,HDR filter is still available.,pcmasterrace,2026-01-05 10:13:43,2
Intel,nxuyrec,"Windows Auto HDR is your problem. Best to use alt+windows+B to switch to HDR for HDR content, and SDR for SDR content.",pcmasterrace,2026-01-05 18:22:47,1
Intel,nxpz14g,"Nah, making exploits less convenient is better than just letting shit happen.",pcmasterrace,2026-01-04 23:47:14,-2
Intel,nxqadan,If they were adjusting shaders then they aren't casuals. That's sweaty.,pcmasterrace,2026-01-05 00:42:44,-1
Intel,nxmj4nk,"Removing bloom, light shafts and turning down shadows, turn off foliage ,running lower graphics. We had that discussion in 2013 (DayZ), Example:  https://preview.redd.it/1lw3zxej9cbg1.jpeg?width=1910&format=pjpg&auto=webp&s=d5d41c0ae2f0792a7fe576849c230c441576d8ab",pcmasterrace,2026-01-04 13:59:38,375
Intel,nxmk02r,Monitor settings can't do nearly as much as those nvidia filters,pcmasterrace,2026-01-04 14:04:49,14
Intel,nxms1jh,"It creates a barrier of entry and an extra obstacle instead of hotkey enable. The filters also allow way more granular adjustments than you could ever get from changing monitor settings. That is before we even get into the ability to blow out certain colors so they are super noticeable, something you can't really replicate with monitor settings at all.",pcmasterrace,2026-01-04 14:50:53,2
Intel,nxn4zjz,"Check out BurntPeanuts brightness, its literally daylight for him when he is streaming.",pcmasterrace,2026-01-04 15:56:14,2
Intel,nxsaqrk,"It's not as good to adjust your monitor settings, a lot of it will just become super flat and will be hard to see anything (it's also a pain in the ass to revert if your monitor doesn't let you set profiles through onboard memory)",pcmasterrace,2026-01-05 08:32:07,1
Intel,nxmkdtr,"my monitor's HDR keeps messing with me, so i have to constantly switch on the flashlight and off again for it to recover...",pcmasterrace,2026-01-04 14:07:08,6
Intel,nxmo3y4,Mine has an AI night mode that I wont use 👀,pcmasterrace,2026-01-04 14:28:51,-1
Intel,nxmteac,Thats fine then because filters are software,pcmasterrace,2026-01-04 14:58:15,-1
Intel,nxmkind,same... he he he he....,pcmasterrace,2026-01-04 14:07:55,1
Intel,nxmstb3,This account appears to be a bot from just reading their comment history lol.,pcmasterrace,2026-01-04 14:55:05,7
Intel,nxmhc6s,"HAAAAANK, don't abbreviate ~~Cyberpunk~~ Control Panel",pcmasterrace,2026-01-04 13:49:07,104
Intel,nxmfoob,Nvidia has CP?! damn they are really trying to expand into some weird industriee,pcmasterrace,2026-01-04 13:39:04,59
Intel,nxmorss,Nvidia what now?,pcmasterrace,2026-01-04 14:32:36,4
Intel,nxmlmr0,Adrenaline isn't nearly as good as NVIDIA filters. Not by a longshot.,pcmasterrace,2026-01-04 14:14:34,3
Intel,nxmi1r0,A step in the right direction? This is like dumping water out of a sinking boat with a cup. You either fix the hole or you abandon the ship. This fix doesn't do a damn thing,pcmasterrace,2026-01-04 13:53:12,-6
Intel,nxp3qen,Not if you're like me and are trash,pcmasterrace,2026-01-04 21:17:14,7
Intel,nxmsbiw,"Higher fps and faster input too, and invtenrory management.",pcmasterrace,2026-01-04 14:52:24,2
Intel,nxru1xx,Yeah but I can circle strafe most console players in close quarters and they cant rotate fast enough to kill me.,pcmasterrace,2026-01-05 06:05:43,1
Intel,nxmkfhb,stoopid plebs...,pcmasterrace,2026-01-04 14:07:24,2
Intel,nxmhmfr,Anyone playing at a level where FPS matters has a PC or prefers console anyways.,pcmasterrace,2026-01-04 13:50:46,6
Intel,nxqu2hj,Still working for me,pcmasterrace,2026-01-05 02:27:13,1
Intel,nxmiqou,"Depends on the game, some games has insane aim assist like cod and apex, but in arc raiders the aim assist isn't as strong as those two titles. So MnK is better.",pcmasterrace,2026-01-04 13:57:17,135
Intel,nxn2qm1,"It depends on how strong the aim assist is.  There are plenty of videos online of games like COD where the overturned aim assist is accurately tracking targets with zero input from the controller. That really pisses off MKB players, because if we take our hand off the mouse the game doesn't magically aim for us. It's especially aggregious in close quarters situations where the controller player can't flick to peripheral targets, so the aim assist just flicks for them with or without input. As a MKB player you can understand that they can't flick, which isn't fair, but it still feels like you're getting beaten by a borderline aimbot.  There's also a big question here about the intentionality of devs in tuning how strong their aim assist is going to be. If 2/3 of your playerbase is using controllers, wouldn't you want to make aim assist stronger to make them happy?  Even outside of the context of cross-play, aim assist has become so ubiquitous in console shooters that I genuinely think that some console players have never played without it. So, they may not fully appreciate how much the aim assist is helping them. This definitely has an effect of flattening the aiming skill curve in a way that isn't present in PC shooters that don't have aim assist.  But at the end of the day it becomes yet another reason why I don't even know why I still bother playing competitive shooters. I can play co-op shooters and friend-slop and single-player games and never have to worry about hackers or normalized toxicity or over-tuned aim assist.",pcmasterrace,2026-01-04 15:45:35,13
Intel,nxmjvzf,"In competitive games with controller support and significant aim assist like COD, pros play on PC to take advantage of more indepth game settings but use controllers to take advantage of aim assist.   Then again there are many implementations of controller aim assist but COD aim assist is the most egregious of the lot.",pcmasterrace,2026-01-04 14:04:08,33
Intel,nxmindf,I think it also depends on the aggressiveness of the aim assist. But gamers will always find a way to attribute their own incompetence to an advantage on the enemies side in some way.,pcmasterrace,2026-01-04 13:56:45,23
Intel,nxml8zz,"Every time this comes up I like to remind people that if the game offers aim assist on controller and has a pro scene, nearly every pro competing is using a controller.  Modern high TTK shooters involve a lot of player movement tracking which mouse isn’t good at.",pcmasterrace,2026-01-04 14:12:18,9
Intel,nxmlyxh,"I'm going to add another opinion on top of what others already told you. While mouse and keyboard is undoubtedly the better control scheme, the aim assist that comes with controllers does a lot to smooth the curve. It obviously depends on how aggressive it is on a per game basis but a mediocre controller gamer (plus aim assist) is going to have a better time than a mediocre mouse and keyboard player, because the latter will have to (typically) play against players with an higher skill ceiling.",pcmasterrace,2026-01-04 14:16:34,5
Intel,nxmj144,It used to be until devs started putting weird mouse acceleration and curves in to “make it fair”. That plus making aim assist damn near magnetic for controllers makes playing on a gamepad at an advantage.,pcmasterrace,2026-01-04 13:59:02,8
Intel,nxmldu4,"Not really the case these days, depending on game. Devs crank that shit to 11 on mass appeal games.",pcmasterrace,2026-01-04 14:13:06,4
Intel,nxp3i6i,"Not having to spend so much brainpower aiming is a massive advantage.  If I didn't have to spend as much effort predicting where a player was going to be and moving my mouse there, I could instead focus more on movement, positioning, etc.  Plus aim assist has a massive advantage in regards to response time and when visual clarity is low.  It's why rapid ad movement doesn't work against controller players but doesn't again PC players.  It's why many pros in certain games like Apex have moved to controller where they can lock onto players behind smoke.",pcmasterrace,2026-01-04 21:16:11,2
Intel,nxmji10,"some games have stupidly op aim assist, for example in fortnite you can lock onto someone thats behind a wall and you can't see yet and basically can't miss with automatic weapons",pcmasterrace,2026-01-04 14:01:51,2
Intel,nxq9rbc,I would say its more precise than controller with aim assist but in my experience you can get just as fast and precise with a controller and no aim assist.,pcmasterrace,2026-01-05 00:39:40,1
Intel,nxmj0gs,"I was half kidding. I know that MBK has a huge advantage vs no aim assist.  But as for which one is better, it depends on the game and/or situation. Some games crank up the aim assist higher than others.  It even varies by season, when there is an exodus of console players leaving the game because they are getting clappped they will crank up aim assist, sometimes going too far and causing PC players to quit in frustration.",pcmasterrace,2026-01-04 13:58:55,0
Intel,nxmt1aj,"Aim assist is an advantage given only to console players. Even if it was small, it is not, it's still unfair. If they talk about fair then they should remove it as well.  Or do something like a game taking the competitive aspect seriously like valorant and stop cross play. Counter strike is not even on consoles. I'm sure there are more examples those I just know.   To your question. There are variables. Not all pc gamers are good with aim and movement. Not all games have the same aim assist. The impact is different...etc But it is still enough to at least try to level the playing field. That alone tells you how much it is unfair.",pcmasterrace,2026-01-04 14:56:17,1
Intel,nxovu7c,"It is, people in this sub just love to bitch about this topic.  Nobody on console with aim assist is running train on competent pc players.",pcmasterrace,2026-01-04 20:40:25,-2
Intel,nxmizx0,"Simply look at any tournament shooter game where mouse has to compete with auto aim controllers, it's dominated by controller players, I understand cod and arc are different but this has been widely established  for normal people to be happy playing shooter games on controllers, it just works out to an unfair amount of autoaim",pcmasterrace,2026-01-04 13:58:50,21
Intel,nxmv3rk,"Just no, AA on Apex/CoD is straight up equivalent to some soft aimlock on pc a cheaterd could use. I mean, when theres is more videos on yt on how to set up your aim assist than there is of actual gameplay.. Maybe there is a problem   And i'm champ on R6 PC/Iri since MW2 9PC KBM too)",pcmasterrace,2026-01-04 15:07:22,7
Intel,nxp5oea,">if you're not that good at shooters it's still way easier to aim with the mouse than even with aim assisted controller.  This is beyond false, I have to believe it's a joke.  No, it's very widely recognized that M&K has a lower floor and higher ceiling.  All the serious shooters are on PC sure but all the pros play with controller.  Why?  Having the aiming part done for you frees up your brain to do other things, like focusing on positioning, movement, etc.  Plus there are certain advantages to auto-aim a M&K player can never compete with.  Being able to auto-aim through visual clutter is insane, for a M&K player it increasing the processing time to identify and move over a target.  Auto-aim is much better at tracking as well.  An algorithm's reaction time is much better than a humans, it's simply unfair.",pcmasterrace,2026-01-04 21:26:16,6
Intel,nxp4dk8,"Oh, like Siege?",pcmasterrace,2026-01-04 21:20:13,3
Intel,nxn3b8i,"It's actually MUCH worse than you think. Some of us used to game back when aim assist didn't exist and with zero aim assist, controllers were not viable AT ALL. The difference was a joke.",pcmasterrace,2026-01-04 15:48:18,11
Intel,nxmlu8i,Yeah lol. On night maps you’re essentially granted god made as a centre rectangle on your screen has day brightness so you can pick headshots very accurately whilst everyone else is just spraying in the dark.,pcmasterrace,2026-01-04 14:15:47,5
Intel,nxp89uw,"I just finished Control and it absolutely does. I built my PC to enjoy games at their highest fidelity, not to lower everything to their minimum to get the maximum advantage.",pcmasterrace,2026-01-04 21:38:17,4
Intel,nxp5cb8,"Often lumen seems underbaked with all the artifacts it can cause, even in less photorealistic games.",pcmasterrace,2026-01-04 21:24:43,2
Intel,nxpjcmw,>>boot a multiplayer game to sit and stare at the roses   Turning graphics down to gain an advantage has always been a thing in games with pvp,pcmasterrace,2026-01-04 22:29:56,6
Intel,nxmms3i,especially in counter strike where low setting is making you harder to spot enemy since everything is blocky including the model and the game without anti aliasing is terrible even though it is competitive gaming,pcmasterrace,2026-01-04 14:21:19,3
Intel,ny0m3w2,True,pcmasterrace,2026-01-06 15:01:20,2
Intel,nxmj6ci,Yeah but now they dont even talk to you untill you spent your first billion on a datacenter,pcmasterrace,2026-01-04 13:59:54,0
Intel,nxmlxiw,A fiew years ago i would ask why you hate nvidia so much but.. yeah,pcmasterrace,2026-01-04 14:16:20,2
Intel,nxme46o,inis work to this day you can disabled pretty much everything,pcmasterrace,2026-01-04 13:29:07,2
Intel,nxsv7wg,"No no, it's fair. In fact very fun still. No complaints",pcmasterrace,2026-01-05 11:37:32,1
Intel,nxzopoz,Every Monitor has gamma settings.,pcmasterrace,2026-01-06 11:38:16,2
Intel,nxmnx1o,"Doesn't even remove it, just changes it a bit as using AMD/NVIDIA control panel still works to change this a good bit.   It's not as powerful as filters, but it does work. I had to do it going from a TN LCD to an OLED.   The blacks were so deep I couldn't see anything in even a regular room on Buried City with the OLED, when with the TN LCD it was much more manageable.",pcmasterrace,2026-01-04 14:27:46,26
Intel,nxmpzd1,"Yes they are. Filters are merely pre-configured settings for things like brightness, contrast, and gamma. Those settings are still there, by the way. Also, anyone with a 10-year-old gaming monitor has access to the same in a menu somewhere. You can mimic the same with manual settings on most newer televisions.  Ultimately, this isn't as big of a thing as console players seem to think it is, and this move stops nothing but the whining.",pcmasterrace,2026-01-04 14:39:29,4
Intel,nxmsjmc,"There is options like ""shadow balance"" or ""night vision"" in a lot of new gaming monitors, they basically brighten shadows without changing overall gamma much.  Also, I am pretty confident that it's still possible to disable filters using nvidia inspector.",pcmasterrace,2026-01-04 14:53:37,1
Intel,nxmmxar,Awwww poor plebs,pcmasterrace,2026-01-04 14:22:09,-43
Intel,nxp4ldp,"Ive got shadow boost on monitor - “level 3” little bit helps, gamma 1.17%, saturation 65%, contrast 52%, brightness default, in game shadows ultra and grass shadows on turns down that brightness exposure from snow.",pcmasterrace,2026-01-04 21:21:13,1
Intel,nxyzgcg,Decreasing the contrast is literally the same as lifting the shadows and reducing the highlights in nvidia filters lol,pcmasterrace,2026-01-06 07:46:59,0
Intel,nxqxs1m,Huh? It's just an NVIDIA feature that allows you to tune your graphics to your liking. I use it in a lot of games. It can really enhance the graphics of your game or tune the colors to something more to your liking.,pcmasterrace,2026-01-05 02:47:09,5
Intel,nxmjm43,Hated this in games like arms and squad.,pcmasterrace,2026-01-04 14:02:30,151
Intel,nxmkokg,"Yeah this has always been the way for competitive FPS, which frankly sux of course cause then we're playing a lesser, shittier looking game for more FPS, higher visibility, etc.  I stopped doing these kinds of things for better or worse I'd rather play the game in it's full graphics glory than to get an advantage. I used to min-max more, play lots of PvP games and take it all very seriously, now I play to have fun again.",pcmasterrace,2026-01-04 14:08:54,42
Intel,nxmo9ev,"Same thing with rust. The more potato your graphics are the better your advantage, screen brightness too as it’s pitch black at night",pcmasterrace,2026-01-04 14:29:43,6
Intel,nxmrcje,"Homie, that's HLL",pcmasterrace,2026-01-04 14:47:02,5
Intel,nxnkczi,They were literally deleting game files so that grass wouldn't render.,pcmasterrace,2026-01-04 17:07:32,3
Intel,nxmtyon,I remember doing similar things as early as Call of Duty 2 in 2005. You could force DX7 and see people through foliage.,pcmasterrace,2026-01-04 15:01:16,1
Intel,nxqhjab,Also pubg mobile,pcmasterrace,2026-01-05 01:20:44,1
Intel,nxt8xfp,This was also done in PUBG during it's peak. Most people would just turn down quality and borderline get rid of foliage altogether. The contrast between the environment and characters would just make the players just pop out even from a mile away.,pcmasterrace,2026-01-05 13:16:10,1
Intel,nxmpeej,"Yeah, filters were a problem in counter strike for a while until they disabled them there too. It’s unquestionably a significant advantage and the scope goes far beyond anything brightness/gamma settings can do. People claiming otherwise just don’t have first hand experience of the possibilities",pcmasterrace,2026-01-04 14:36:10,5
Intel,nxmubli,"My monitor does just as well as Nvidia filters and takes me about 5 seconds to toggle. Easily good enough for DayZ, and I don't even bother for Arc because it's so rarely an issue.",pcmasterrace,2026-01-04 15:03:13,2
Intel,nxmkuvq,Any kind of auto HDR is stupid and whoever invented it should have their fingernails pulled till they admit it was just for advertising.,pcmasterrace,2026-01-04 14:09:57,13
Intel,nxmu6sd,Seems to be good enough for most people.,pcmasterrace,2026-01-04 15:02:30,1
Intel,nxqjap2,Yep,pcmasterrace,2026-01-05 01:30:12,1
Intel,nxmid8q,"I mean they are moving towards AI, didnt realize they meant Grok specifically",pcmasterrace,2026-01-04 13:55:06,18
Intel,nxmtjl5,No one said they were.,pcmasterrace,2026-01-04 14:59:01,0
Intel,nxq02h3,"There is no way to ""fix the hole"" though. PCs inherently give far more control to the user about it's settings than consoles. Nothing the game can do will ever be able to ""fix the issue""",pcmasterrace,2026-01-04 23:52:26,2
Intel,nxmk6we,"Although controllers do have minor aim assistance, On pc you can do so much more faster since you have way more buttons that are easily accessible.  I don't think aim assist is as insane as people say and I'm on console.",pcmasterrace,2026-01-04 14:05:59,-32
Intel,nxmvi7o,Black Ops 7 aim assist on release was just a full blown aimbot in disguise. Saw some clips from console after they turned it down and it's just hilarious.,pcmasterrace,2026-01-04 15:09:27,11
Intel,nxmnizw,Console players play with literal aiming cheats and you say its the pc players who are wrong? Shooters with aim assist should absolutely not have crossplay enabled.,pcmasterrace,2026-01-04 14:25:35,8
Intel,nxp44im,"It's not that mouse isn't good at tracking, it's that human response time dictates that you aren't going to have perfect tracing if the target knows what they are doing.  AD spam will always be effective again mouse because of human reaction time unless the game incorporates inertia.  Controller is much much worse than mouse and keyboard without aim assist in terms of tracking.",pcmasterrace,2026-01-04 21:19:04,6
Intel,nxr38kw,"If you use a controller on PC, do you get the same aim assist as console folks?",pcmasterrace,2026-01-05 03:17:05,1
Intel,nxpocya,"> All the serious shooters are on PC sure but all the pros play with controller.  Lmao what. I'm talking about Counter-Strike and Valorant. Even Siege and Fortnite pros play with M+KB.  > An algorithm's reaction time is much better than a humans, it's simply unfair.   Maybe it depends on how the aim assist is tuned/calibrated, all I know is that every time I try to play a shooter game with controller on PC, even with aim assist I get too annoyed after like 10 minutes and go back to using mouse and keyboard.",pcmasterrace,2026-01-04 22:53:57,1
Intel,nxnck05,"No, it's really not. Maybe you're just getting old and slow. you'd be what, mid 40s to early 50s? I've played fps games that didn't have it, and ones with it, both were fine. Been a PC player most of my life too and was there for the switch over to cross-play becoming mainstream as well, I didn't start doing worse due to console players with aim assist.",pcmasterrace,2026-01-04 16:31:45,-6
Intel,nxn4az5,"Make sense now, same feeling when I found out about the door/out-of-bounds glitches, ""Eh I'll be throwing nades into locked rooms now.""",pcmasterrace,2026-01-04 15:53:01,1
Intel,nxu0p3r,Shit like this is why I hardly care about playing fps anymore. It seems that someone is always going to have some kind of shady advantage lol. Maybe I'd play CS2 but that's probably it.,pcmasterrace,2026-01-05 15:45:51,1
Intel,nxpsz2d,single player emersion is not the same as pvp.,pcmasterrace,2026-01-04 23:16:56,-1
Intel,nxtnobh,Always been lame too,pcmasterrace,2026-01-05 14:40:52,5
Intel,nxmecwo,ini my beloved. Cba to play ASA with all the garbage thats on the screen,pcmasterrace,2026-01-04 13:30:40,1
Intel,nxmrdhq,Then you should make sure youre not running the oled in some weird profile. Because properly setup blacks carry so much detail on oled.,pcmasterrace,2026-01-04 14:47:11,17
Intel,nxmriga,"This relates to me so much, OLED looks great but fuck man. When sneaking around and can't use any torch or light source makes it really hard to see shit. But them blacks tho",pcmasterrace,2026-01-04 14:47:57,4
Intel,nxmp3vj,Does profile inspector work?,pcmasterrace,2026-01-04 14:34:30,2
Intel,nxnfbz3,It’s definitely for more things than just brightness/gamma/color sliders.   You can use it for all sorts of funky effects like comic book filters etc. Things that monitors don’t have built in settings for.,pcmasterrace,2026-01-04 16:44:29,-1
Intel,nxmniwi,Are you lost?,pcmasterrace,2026-01-04 14:25:34,11
Intel,nxp5d3a,Thanks!! Will try this out tomorrow! Cheers,pcmasterrace,2026-01-04 21:24:49,1
Intel,nxmkryu,"Squad was so frustrating for a bit. ""This squad won't see me, I'm fully inside this big bush""  Immediately domed from 100 meters because their SL was playing on low settings and my bush looked like a singular stick",pcmasterrace,2026-01-04 14:09:27,166
Intel,nxmk7e6,War Thunder....,pcmasterrace,2026-01-04 14:06:04,26
Intel,nxmmvgd,My friend always did this in Rust. He would run everything as low as possible. I get it gives a advantage but id rather have the graphics...,pcmasterrace,2026-01-04 14:21:51,28
Intel,nxrowbv,Pubg had this real bad too in the early days,pcmasterrace,2026-01-05 05:27:02,1
Intel,nxmw9na,"The main reason I don't play online multiplayer games is for this type of thing.   I like to utilise my hardware by running my games at the highest detail I can.  When that is a disadvantage online,  there's no point in playing that game for me.",pcmasterrace,2026-01-04 15:13:26,9
Intel,nxsazjq,I only turn it down in games that look about the same on low vs ultra/epic  BF6 I have on all low except textures and aa because it looks about the same as ultra but ultra halves your performance.,pcmasterrace,2026-01-05 08:34:24,1
Intel,nxmo91i,"nah, them fingernails should turn into fish hooks... with an unyielding itch to develop around the anal area...",pcmasterrace,2026-01-04 14:29:39,3
Intel,nxnd6h6,"Saying *""AMD users can easily use gamma & adrenalin filters""* easily implies that the same can still be achieved by AMD users. Which obviously is not the case.",pcmasterrace,2026-01-04 16:34:36,0
Intel,nxt6060,That's my point. Why are they wasting time fixing it then they didn't do anything.,pcmasterrace,2026-01-05 12:57:14,1
Intel,nxmlma4,It has almost fuck all to do with more buttons in most fps games that are crossplay. It has to do with the potential for faster more precise aiming/turning of a mouse.   But that doesn’t matter for the vast majority of players because the skill level for MKB to match controller players with high aim assist games is ridiculous. When you’re playing against top players there is no room for error while aim assist is incredibly forgiving.,pcmasterrace,2026-01-04 14:14:29,32
Intel,nxmoazx,"It truly depends on the game. There's games where aim assist is incredibly powerful on a controller, and very clearly is an advantage as a result. If it's done right, it shouldn't be an advantage.",pcmasterrace,2026-01-04 14:29:58,4
Intel,nxmntom,"Nope, in games like CoD after MW2019 it's a chore to play with MnK... Controller is actually much better as if you know how to abuse their aim assist and have the right settings, it's superior to MnK. The only FPS I'll use a controller on matter of fact. Tried it on Arc and it's negligible, MnK is still much better.",pcmasterrace,2026-01-04 14:27:14,5
Intel,nxmlg92,Try Apex out.,pcmasterrace,2026-01-04 14:13:30,8
Intel,nxmm72s,"Depends on the game, as they’re all gonna be at different levels. But it’s one of those things that once you’re used to it, you would probably be surprised how much of a difference it can actually make.",pcmasterrace,2026-01-04 14:17:53,2
Intel,nxzjey4,"I mean the easiest way to tell is to look at the pro scene for a game, if they're using controllers the aim assist is too strong. They'll always gravitate to what is strongest, and given controller is a harder input to use then they won't be using it unless it gives sizeable cheats.  Apex is the usual example as aim assist in that game is so strong it's borderline cheating. I'm pretty sure most cod pro players are on controller for the same reason, etc.  In cases where there is only ""minor aim assist"" yeah, pc has massive advantage. Undeniably there are games where the aim assist is too cheaty though, making this advantage pointless.",pcmasterrace,2026-01-06 10:53:49,1
Intel,nxmm7zo,"I played console for +20 years and switched to PC roughly 3 years ago. Aiming is soooo much easier and faster, even compared to a fully optimized Xbox elite 2 controller setup (that thing was nuts).  Left hand on keyboard is still struggling but it's fairly easy to compensate with a mouse with a few more buttons.",pcmasterrace,2026-01-04 14:18:02,1
Intel,nxr7azd,Wouldn't a pc player using controller have the same aim assist as console players?,pcmasterrace,2026-01-05 03:39:55,1
Intel,nxmo4b9,"If your aim with a mouse is worse than someones aim with a controller, maybe you‘re just not good enough. You have a massive accuracy advantage with a mouse if you know how to use it.",pcmasterrace,2026-01-04 14:28:55,-12
Intel,nxrwvay,"It depends on the game and other factors.  Apex for example will lower your assist if you are on PC.  They give controller gyro on PC 0 assist and a lower than default assist on console as well.  Other games will provide the same aim assist on console and PC with controller, it just varies.  Of course it's also possible to spoof inputs as well (like using M&K but appearing as a controller user) but that's a separate issue.",pcmasterrace,2026-01-05 06:28:27,1
Intel,nxp712g,"Considering that the entire COD and Apex pro scenes use controller, I think he has a point.  Not having to think about aiming frees your brain up to focus on other things.  Aim assist doesn't have issues with visual clutter, when manually aiming though, it directly increasing your target acquisition time.  It's also impossible to compete with the reaction time of aim assist in regard to tracking.  It can just lock onto a target far better than any human can.  It's why M&K players on Halo Infinite have a much much smaller perfect rate than controller players.",pcmasterrace,2026-01-04 21:32:32,5
Intel,nxp7j89,"That doesn't really track modern reality where developers have been making aim assist stronger and stronger.  When games are actively nerfing aim assist and it still manages to remain the most popular input method at the highest level of play (with very clear statistical advantage), then something has gone wrong.",pcmasterrace,2026-01-04 21:34:52,2
Intel,nxpiz4c,"FPS like cod have crazy aim assist with rotational input INCREASING your aim assist bubble so as you take gunfights you control yourself with your LS and let aim assist do its thing. I know I’ve gotten old (34) but I’ve played majority of my life on MnK, swapping to controller increased my KDA and made aiming a joke with how strong aim assist is",pcmasterrace,2026-01-04 22:28:09,2
Intel,nxmfjo6,also works in ASA you just paste it into the console we have multiple copy&paste in our discord prepared + start up options,pcmasterrace,2026-01-04 13:38:11,1
Intel,nxq9acr,Yeah def sounds like a crushed blacks calibration issue,pcmasterrace,2026-01-05 00:37:17,3
Intel,nxnwcg4,"It's a ROG Swift 32"" 4k OLED (PG32UCDM), and I am just running Gaming HDR with 78 brightness and 63 contrast, any higher and everything looks super washed out.  I tried with both HDR on and off, and didn't seem much of a change TBH, but if you see anything I am doing thats weird or I can test to change, let me know.  I've spent a lot of time already sitting in the extracts trains on Buried City night raids messing with settings that keep it decent enough to see like I used to on my TN LCD, but if I push it any further Cold Snap and Day Time look like ass.",pcmasterrace,2026-01-04 18:02:33,1
Intel,nxnvner,"100% a love-hate relationship, but I do have to admit I love it more than I hate it.   Really only a bigger problem in Arc Raiders for me since pretty much every other game lets you do brightness.",pcmasterrace,2026-01-04 17:59:24,2
Intel,nxnwefs,I don't know what profile inspector is?,pcmasterrace,2026-01-04 18:02:48,1
Intel,nxng5by,"The thing people are complaining about is lightening dark shadows. That's done through standard sliders and yes, monitors and more modern televisions can do EXACTLY the same thing. I know because I hit that ""FPS Mode"" button on my monitor every time I log in and laugh at these misinformed posts thinking 1) They've stumbled upon a new problem, and 2) Thinking there's a ""fix.""",pcmasterrace,2026-01-04 16:48:14,0
Intel,nxmnmkb,"No, the plebs crying made this happen",pcmasterrace,2026-01-04 14:26:08,-27
Intel,nxp5hcl,On interchange I turn back gamma 1.22%-1.25%,pcmasterrace,2026-01-04 21:25:22,0
Intel,nxmoulf,All small bushes disappeared from enough distance. This was a problem in dayz,pcmasterrace,2026-01-04 14:33:03,43
Intel,nxmy7bj,"I remember something like this in battlefield, can't even remember which one. Had to play that game at bare minimum settings so I'd stop laying down in an ""open field""",pcmasterrace,2026-01-04 15:23:18,15
Intel,nxpqa34,"Even when playing at max foliage settings, tall grass would completely vanish after 100m in that game for me",pcmasterrace,2026-01-04 23:03:34,1
Intel,nxmqudm,Everyone does this. Especially in a game like rust. Where graphics are the least concern.,pcmasterrace,2026-01-04 14:44:16,9
Intel,nxpoqnz,"That works both ways. You're functionally saying - if I don't have an advantage, there's no point in playing. You want your 'better' system to give you more than others.",pcmasterrace,2026-01-04 22:55:51,-6
Intel,nxtilh9,"Yeah there are some cases where it is basically mandatory to be able to play at decent enough frames, or even a person's rig dictating hey they have to play with everything on low/off cause their on an old rig or laptop, i've been there, most of us have been there.",pcmasterrace,2026-01-05 14:12:53,2
Intel,nxn33be,"Windows HDR being outright garbage and the lack of monitor/computer being able to talk to each other is diabolical. There is no reason any of this isn't controlled by the OS over HDMI/DP and it's all possible with current hardware or minimal changes to it, but no HDCP is more important.  My AVR turns on when the TV turns on, the volume is synced, so is the audio stream, so is just about everything else, but on PC we get video, and audio.",pcmasterrace,2026-01-04 15:47:15,3
Intel,nxmnb0h,It was during the Christmas train event in Apex that I truly realised how fast these console players could nail me with laser weapons. That is when I uninstalled.,pcmasterrace,2026-01-04 14:24:20,11
Intel,nxu6qn9,you should look into the azeron cyborg or gaming keyboard/controllers like it. i also came from console to pc and hated the transition to keyboard (i loved the mouse) but the azeron has a joystick with WASD for movement and the controller is shaped in the shape of a hand with 4 inputs per finger. not great for typing but definitely one of the best keyboard alternatives ive used.,pcmasterrace,2026-01-05 16:14:05,1
Intel,nxmrxy0,"I played FPS games at a top level and I still think that aim assist in some games is ridiculous and unfair. Like in CoD in close combat you sometimes genuinely can not tell the difference between aim assist and humanized aim bot.  So I guess that my aim is bad now too despite being paid to play shooters and being known specifically for my aim? It's not just me, Shroud has said the same thing about some shooters. I suppose that he is bad too?",pcmasterrace,2026-01-04 14:50:20,12
Intel,nxmohuo,"In apex controllers on close range are unbeatable. On long range mnk has the advantage but I dont give a shit, i dont want to play vs someone who aim locks on my head its stupid.",pcmasterrace,2026-01-04 14:31:03,5
Intel,nxraxnq,"In spellbreak, players with controllers had homing projectiles, while mouse and keyboard people had to predict where people would be 5s in advance to hit the same ability. Crossplay killed that game.",pcmasterrace,2026-01-05 04:00:52,1
Intel,nxqa5zf,It’s a third party nvidia driver plugin. It can sometimes give extra graphic options for games.,pcmasterrace,2026-01-05 00:41:42,1
Intel,nxnhiya,"I understand what people are complaining about.   I’m just pointing out that your statement of filters merely being preconfigured settings for brightness/color/etc, isn’t entirely correct in the context of being able to do more with filters than built in monitor settings.   The two sharing some settings doesn’t make them 1:1 comparable when one has a whole swath of additional settings.   For people who are just learning about Nvidia filters for the first time here, they should at least get a proper idea of what the system actually does.",pcmasterrace,2026-01-04 16:54:33,0
Intel,nxmo3qc,"""pleb"" being used a decade late isn't something i was expecting to see",pcmasterrace,2026-01-04 14:28:49,9
Intel,nxmnqf9,why are you stuck in 2010,pcmasterrace,2026-01-04 14:26:44,6
Intel,nxmrwia,bruh,pcmasterrace,2026-01-04 14:50:07,1
Intel,nxp84m7,Battlefield Vietnam had this.,pcmasterrace,2026-01-04 21:37:38,3
Intel,nxnjmgw,"This is what I like to call the cheater's fallacy:  ""I have to cheat because eVeRuOnE eLsE is cheating, and even if they aren't it's their own fault.""",pcmasterrace,2026-01-04 17:04:08,10
Intel,nxprjnd,"Not really, I want everyone to have the same advantage.    If one person runs a game at max settings and can't see somebody hiding in a bush across the map, the person playing on low settings also shouldn't be able to see that player either.    Having the player on low settings able to see the character because the bush doesn't render is an unfair advantage.    They should implement some sort of visibility thing for those types of situations,  if a character should be obstructed from view, but the thing obstructing them doesn't render due to graphic settings, maybe render the character with a lower opacity so it is also harder to see for those people without the actual visible obstruction?",pcmasterrace,2026-01-04 23:09:55,10
Intel,nxtmetp,"Ironically I have a overkill build for most games, but most games just aren't well optimized unfortunately.",pcmasterrace,2026-01-05 14:34:00,2
Intel,nxpf6iv,In my experience windows hdr is better than on playstation or switch. It's just more complicated to setup.,pcmasterrace,2026-01-04 22:10:15,2
Intel,nxmv0k4,"Controller players don't like being reminded the game is doing the heavy lifting for the hardest part of an FPS for them. I forget the exact statistic, but at one point it was something like 99% of all CoD pros were running controller even if they were also top ranked in other games running mnk. That should tell you something.",pcmasterrace,2026-01-04 15:06:53,18
Intel,nxmtylf,"There are ridiculously good players on k/m as well who make it seem as though they have aimbot, just because their aim is so good. I would say I‘m a top 30% FPS player at max regarding skill and I hardly ever complain about cheaters and never about aim assist because it‘s never been a big enough issue to complain about. Sure, if a pro player on k/m is playing against a pro player on controller in a game with significant aim assist, I understand that it can be an unfair advantage. 90% of players are not professional though and just like to whine about others being better.",pcmasterrace,2026-01-04 15:01:15,-4
Intel,nxr0ypx,"No idea then, I have a 9070XT so I am on AMD.",pcmasterrace,2026-01-05 03:04:19,1
Intel,nxqfze5,I don't think turning down the graphics is considered cheating. Just a choice between aesthetics and function... A lot of people choose function over aesthetics in competitive multiplayer games.,pcmasterrace,2026-01-05 01:12:19,10
Intel,nxoxpu2,That’s why games have to ban people and have rules. Glad NVDA did this honestly,pcmasterrace,2026-01-04 20:49:10,-1
Intel,nxpw9a6,"I understand that. But if you have a better FOV, or further view/less fog, a faster refresh rate etc. you have a distinct advantage over 'poorer' setups. Functionally you're saying you want everyone locked to minimum specs.  There will never be a 'fair' playing field unless you bump everyone down to Doom graphics.",pcmasterrace,2026-01-04 23:33:15,-4
Intel,nxs9bsr,"Im sorry to hear that, sounds like a common struggle for all of us.",pcmasterrace,2026-01-05 08:18:43,2
Intel,nxmw4se,"Yeah, a lot of them complaint for years and ultimately just switched to controller. KBM is by far the superior control mechanic, but the aim assist in that game so heavy that it no longer is. It is just a legal aimbot at that point.",pcmasterrace,2026-01-04 15:12:44,11
Intel,nxmxgxk,"I helped CB and ESL with demos of cheaters back in the day. It can be very difficult to spot an aimbotter, but that is mostly because cheats have become so good at emulating human aim.  However, when an aimbot becomes too good it becomes too mechanical and you can spot it. That's what the aim assist in CoD sometimes looks like. It is just a legal aimbot.  It's not about losing to an equally skilled player, it's about losing to a worse player who simply has the computer do all the work. KBM is by far the superior control mechanic. When you put top controllet players without aim assist against top KBM players, the KBM players are going to win 99% of the time. So I don't mind them leveling the playing field a tiny bit, but in CoD it has gotten so had that pretty much every top KBM player switched to controller. So the best players stopped using the best controls because aim assist on controller is just too broken.",pcmasterrace,2026-01-04 15:19:36,4
Intel,nxtk06t,"If you're altering something like that specifically to give yourself an advantage, it's cheating bro. You can try to justify it all you want.    Same thing with these people altering their gamma settings and shit to see better at night. It's poor man's cheating.    Gray Zone devs had to step in and limit the in-game gamma adjustment because so many people were abusing it to give themselves night vision. Of course, the players were on the message boards afterwards talking about how they were just going to do it on their monitors or in their graphics card control panel.    Kind of funny people don't think trying to give themselves an unintended/unfair advantage isn't cheating though.",pcmasterrace,2026-01-05 14:20:46,-3
Intel,nxtkdqr,Dumbest take I've ever heard,pcmasterrace,2026-01-05 14:22:52,1
Intel,nxyq6qz,I get exactly what you mean and agree.,pcmasterrace,2026-01-06 06:25:23,2
Intel,nxtl2vr,"You know how ABSOLUTELY ridiculous you sound, right?  In this comment thread we were talking about Rust and DayZ... Where you can change the graphics to a lower preset which lowers the amount of foliage and LoD. A choice the developers gives. Something some people have to use because they have a potato as a pc...",pcmasterrace,2026-01-05 14:26:43,4
Intel,nxv7373,Dumbest comment I've ever read,pcmasterrace,2026-01-05 18:59:51,0
Intel,nxdmcsx,I mean it depends on what budget you have,pcmasterrace,2026-01-03 03:49:10,1
Intel,nxdnt4u,Nvidia Geforce RTX 5070 is the most popular card at the moment.  Great quality/price.,pcmasterrace,2026-01-03 03:58:14,1
Intel,nxdtki8,I don't know overly much about prices but probably $400-$600 AUD,pcmasterrace,2026-01-03 04:35:40,1
Intel,nxdqsi8,"Yup, got a 16gb 5060ti today and I'm very impressed. In real world gaming scenarios, it's seemingly superior to the 3080 I had while using half the power",pcmasterrace,2026-01-03 04:17:13,1
Intel,ntzzijx,"Each GPU chip is unique. Some are better at overclocking or running cooler, causing stock performance differences.  It's called the Silicon Lottery",pcmasterrace,2025-12-14 16:23:04,130
Intel,nu0fe6w,What did you need 6 B580s for? AI farm?,pcmasterrace,2025-12-14 17:43:21,25
Intel,nu2dgac,"The best card here is about 2.3% better than the worst. I'd chalk that up to run-to-run variance especially with only 3 runs each. I'd be willing to bet some of these individual cards had a similar spread in results.  This is entirely normal. No two cards are entirely identical. One might have a tiny bit better cooler mounting or need a tiny bit less voltage to hit top clocks, and that one will be a tiny bit faster.  If you were to overclock or undervolt and chase a record, that top performer is the first one I would try first, but they are all so close that any one of these could be the technically best one.",pcmasterrace,2025-12-14 23:32:02,7
Intel,nu3fwgv,Any idea on how the temps vary with those scores?,pcmasterrace,2025-12-15 03:18:01,2
Intel,nu29vqq,"That doesn't look like too much of a variance to me, I bet nvidia and amd cards would act the same. Not all chips are equal, the silicon lottery determines what you get",pcmasterrace,2025-12-14 23:12:14,1
Intel,nu314vs,Seems within margin of error,pcmasterrace,2025-12-15 01:47:02,1
Intel,nu3403p,pretty close. was thinking of a sparkle b580 as my upgrade as well.,pcmasterrace,2025-12-15 02:04:20,1
Intel,nu3unzs,"Neet data. Not many people test a bunch of the same card against itself.  I know with cpus they talk about the ""silicone lottery"" - extreeme overclockers would look for the best CPUs and get a few percent extra performance. I'd guess thats whats happening here.",pcmasterrace,2025-12-15 04:56:59,1
Intel,nu4bp7a,"Furmark is known for a *lot* of run to run variance. You'd need to do something like ten runs, discard the outliers, and average them.  It's not very useful as a benchmark, doesn't run long enough, which is why you won't see many folk using it as a benchmark: It's a stress test.",pcmasterrace,2025-12-15 07:18:04,1
Intel,nu55567,"So we're talking about about Standard deviation of 78.2 and mean of 9187.6, which means less than 1% deviation from the mean for about 84% percentile of the cards. While sample size isn't to big to draw any conclusions and tests should be more thorough and controlled, this is not high variance.",pcmasterrace,2025-12-15 12:00:51,1
Intel,nu72hkh,Average variation +/- 66 with maximum of -113  Max is a 1.22%,pcmasterrace,2025-12-15 18:18:55,1
Intel,nu2qn6m,thanks. now i can tell if mine is good or bad. wiat wait wait... no ya didn't benchamrk it on a core ultra 9 100series. thats a laptop only chip!,pcmasterrace,2025-12-15 00:43:53,0
Intel,nu2qri9,"NO ONE NOTICED THE CORE ULTRA 9 ""100"" SERIES?!?! I think he meant 200.",pcmasterrace,2025-12-15 00:44:34,-1
Intel,nu29ydv,Fucking ai people and their retarded local llm bs,pcmasterrace,2025-12-14 23:12:38,-12
Intel,nu1j49c,"And not just GPUs but CPUs, vehicle engines, electric heaters, gas boilers, light bulbs, batteries etc  Basically anything will have a “tolerance” where the “output” will vary between x and y and is not a constant.",pcmasterrace,2025-12-14 20:55:37,36
Intel,nu2h4v3,"and also if you get a higher tier gpu then youre guaranteed to get a really good chip out of this lottery, such as gigabyte aorus xtreme cards",pcmasterrace,2025-12-14 23:52:10,2
Intel,nu0gi0o,Yeah I was planning doing some local llm inference,pcmasterrace,2025-12-14 17:48:54,25
Intel,nu3u4ps,Good question.  I wonder if they are being thermally restrained or power limited.,pcmasterrace,2025-12-15 04:53:04,1
Intel,nu3inr7,Very good GPU IMO the sparkle one looks better and clocks higher than stock,pcmasterrace,2025-12-15 03:36:00,1
Intel,nu69g0s,Hmm ok while I still have the cards not deployed in anything what benchmarks should I do like what exact testing are y'all looking for,pcmasterrace,2025-12-15 15:58:05,1
Intel,nu2ughs,Yes I benchmarked it on my mini PC it is a mobile chip,pcmasterrace,2025-12-15 01:06:12,2
Intel,nu2d053,"What I do with my money and my hardware should not upset you. When people say ""AI people,"" we are referring to large corporations and data centers buying up all the GPUs and RAM and using the AI to ruin our lives even more. What **local LLMs** are doing is to bring that power back into our hands, the consumers. The everyday person. Would you like it so that all things to do with AI belong to Google and OpenAI, or would you not want a world where us, the consumers, can fight back with our own models?  if you dont know what to say about a matter its best to enlighten yourself before shitting on something",pcmasterrace,2025-12-14 23:29:30,13
Intel,nu4a1zc,"And also humans, it's called genetic lottery.... Usually pcmr members don't fare too well on that. 😥",pcmasterrace,2025-12-15 07:03:04,4
Intel,nu3qgkg,"Nvidia bins their chips into very tight bins. Board partners aren't even allowed to sell a chip as overclocked when Nvidia deems it to not be, so there's less of a lottery going on than there is in other places.  That being said, there will still be sample variance.",pcmasterrace,2025-12-15 04:27:29,1
Intel,nu2v9r3,"Why is this guy getting downvoted? Local LLM inference is really cool and you get full control over every part of it. Maybe people are thinking ""AI bad"" but the random hobbyist buying $2k of hardware is not the same as the large corporations buying the entire world's DRAM supply.",pcmasterrace,2025-12-15 01:11:11,39
Intel,nu1jruw,Why go for Intel and not AMD or the one who shall not be named? Is Intel best?,pcmasterrace,2025-12-14 20:58:53,-12
Intel,nu2zhyf,what?!? how did you fit the gpu?,pcmasterrace,2025-12-15 01:37:06,0
Intel,nu4z9h0,"Yep, they have 2 chip models with one permitting factory of and one not. And the ones permitting also tend to get you higher speeds  But for example if you buy an aorus card, chances are you can't go any higher because the chip couldn't make it to be an xtreme model. On the other hand, an xtreme card probably will go up a bit extra",pcmasterrace,2025-12-15 11:10:15,0
Intel,nu3z1yj,Nuance is too hard for most people.,pcmasterrace,2025-12-15 05:30:00,6
Intel,nu7sluk,Most pple dont care they are like robots them selves they hear ai and immediately down vote some pple even believe that someone buying a few GPUs directly means they are taking away from gamers people dont care as long they have something to hate on,pcmasterrace,2025-12-15 20:26:14,3
Intel,nu1pdng,"From what I saw when I was looking, you can get decent amounts of vram for not too bad, I grabbed an intel b580 for my home server cause it was mega cheap and it can rip transcodes pretty ez",pcmasterrace,2025-12-14 21:26:04,19
Intel,nu2d97v,as others have mentioned currently the best bang for buck is intel for vram and also i got all of these cards for super cheap around 200-240 each also i dont want to support nvidia at all they can go fuck themselves and as for amd there is very little llm support on their cards,pcmasterrace,2025-12-14 23:30:55,15
Intel,nu25qj2,"that is 6 b580 carts at 12GB each right now they are selling for like 260 USD each at some retailers so that is 1,560 USD all before tax that is.  And you get 72GB vram to use as long as you have a system for using this.  And 3060 12gb are going for about 300-350 and the 5060 ti 16gb is going for about  400-450  then the 9060xt 16gb is going for about 380-450.  so the 3060 12gb is out, it is way to much  Then lets say he got 5 9060 XT 16gb at 380 that's 1900 and that would be 80 GB of vram or 4 cards at 1520 with 64GB of vram.  Then their is the 5060 ti 16gb it would be even more.  With Vulkan API working with different LLM stuff it is easy to use just about all cards now, aside from the compile process if you want to get maximum performance. Other wise LMStudio will get you going with Vulkan fairly decently.  EDIT: My mistake you can get the B580 right now for 239.99 from B&H photo.",pcmasterrace,2025-12-14 22:49:18,9
Intel,nu26t3l,Nvidia tends to be the most performative for AI but also most expensive. Intel is the best bang for buck if you're going for high vram counts. It'll be slower but at least the model fits. Amd isn't really a player in the AI space yet,pcmasterrace,2025-12-14 22:55:08,7
Intel,nuge5qp,How about an EU chip since you declared people should be independent of american ones hmm,pcmasterrace,2025-12-17 03:54:47,1
Intel,nu349na,beelink gti 14ultra look it up it has a full pcie slot,pcmasterrace,2025-12-15 02:05:56,2
Intel,nu698w6,"It is hard to predict how chips will overclock. We've had generations where CPUs were apparently binned very conservatively, so even way down the range chips would overclock like crazy (Intel Core 2nd gen) or allow you to unlock whole extra fully functional cores (AMD Phenom triple cores), but we've also seen generations where most headroom was already 'priced in' (Intel 13th/14th gen) or arguably even exceeded. We've seen generations with more and less headroom on the GPU side as well.  That being said, now that consumer GPUs are competing with AI chips, they'll want to squeeze every inch out of each wafer, so I suspect they'll bin things as tightly as possible.",pcmasterrace,2025-12-15 15:57:08,2
Intel,nu6d5hz,"Yeah thats a fair point, some generations can be liklier to overclock than others",pcmasterrace,2025-12-15 16:16:03,1
Intel,nxydhar,"EA Skate runs EA Javelin Anti-Cheat.  That shit is not only always active in the background, but also very annoying to delete as you need to remove it with an additional software before deleting the Game that installed it.  www.google.com",pcmasterrace,2026-01-06 04:50:18,1
Intel,nxxm6z9,"This really looks like a corrupted Windows install, not hardware.  Key red flags:  Only anti-cheat games fail (EAC window flashes & dies) Windows Reset made things worse Reset/update errors = broken component store Non-anti-cheat games run fine  Important: “Reset this PC” is NOT a clean install and often reuses corrupted files. Anti-cheat drivers are very sensitive to this.  What I’d do next:  In BIOS: load defaults, enable Secure Boot + TPM, disable CSM/Legacy.  Create a Windows 11 USB (Media Creation Tool). Boot from USB → delete all partitions on the NVMe → install to unallocated space. After install: Windows Update → AMD chipset → NVIDIA driver.  Test Apex only before installing RGB/LCD/overlay tools (SignalRGB, TRCC, etc.). If it still fails after a true clean install, then start looking at TPM/firmware or rare board issues — but right now this screams OS-level corruption, not GPU/CPU/RAM.",pcmasterrace,2026-01-06 02:10:16,0
Intel,ny37rs2,https://preview.redd.it/53rextvsysbg1.jpeg?width=4284&format=pjpg&auto=webp&s=22a2d9b93560b2cd450ef232532c4ada7686b9fc  I did everything you said and when I finally went to boot from the disk image I got this. I did install it to disk 0 partition 3 which I think was the largest option. Is that wrong?,pcmasterrace,2026-01-06 22:10:08,1
Intel,ny3ae4c,I just tried again and now I got this  https://preview.redd.it/imgwsyx31tbg1.jpeg?width=4284&format=pjpg&auto=webp&s=cfcaa7136a94251bbdb33ce843d5cf2ed000d7aa,pcmasterrace,2026-01-06 22:22:36,1
Intel,nxx43m8,nice upgrade 👍🏾 got the same cpu cooler,pcmasterrace,2026-01-06 00:33:28,2
Intel,nxx4ila,"It’s really nice, my cpu stays very cool",pcmasterrace,2026-01-06 00:35:35,3
Intel,nw0g76t,"> TLDR in CPU intensive games, if your CPU is maxed, it holds the GPU back with it, which leads to some pretty significant drops, but don't sweat it. A 5600/5600x is a suitable CPU upgrade and will very well balance the system  i'm using intel 225f to pair with the b580 and they work well",pcmasterrace,2025-12-26 11:58:13,2
Intel,nvxl4zm,so ur tldr is that if you get bottlenecked from ur cpu the gpu doesnt perform well? sorry for the sarcasm but who wouldv thought! merry christmas,pcmasterrace,2025-12-25 22:14:41,1
Intel,nw3o9mi,"Nice, the 12400f is definitely a solid pairing with the B580! Way better than my old 8400 for sure. How's it handling CPU heavy stuff like open world games? Been thinking about upgrading myself but trying to squeeze more life out of this setup first lol",pcmasterrace,2025-12-26 23:12:45,1
Intel,nw0j3gr,"Lmao ur right it's stupid But I was doing research for this specific combo and couldn't find much, so I wanted to post my findings. Also, this is a problem specifically with arc GPUs, where the card requires a lot of CPU overhead to work well. Officially intel says not to pair this card with less than 10th gen, but it works so oh well 🤷",pcmasterrace,2025-12-26 12:24:08,1
Intel,nw3puq8,>How's it handling CPU heavy stuff like open world games?  does assassin's creed shadows count as open world? i think it's not so cpu intensive though. 225f+b580 can produce 4x fps at 4k medium quality  https://preview.redd.it/3ilcoxnptm9g1.jpeg?width=3840&format=pjpg&auto=webp&s=61369f28fca9c392917690bc631e2c0ecb667faf,pcmasterrace,2025-12-26 23:22:13,2
Intel,nwbto0a,"Tengo el mismo micro el i5 8400 y me interesa la b580 lo recomendas? Tengo un core 225f nuevo, esperando a que pueda comprar las ddr5 para poder usarlo xd , pero me llama mucho la atención la b580",pcmasterrace,2025-12-28 07:13:50,2
Intel,nwt5h99,Ja es un scheda grafica muy potente. En mi ciudad costa €300 por el version steel legend. Lo siento mi español es una mierda 😅,pcmasterrace,2025-12-30 22:09:51,1
Intel,nx97l0z,"The way DayZ combatted this was putting a noise filter in low light, that way if you turned up gamma it would look like a sand storm.",pcmasterrace,2026-01-02 14:11:52,581
Intel,nx8pisn,">Using the Nvidia App filters, players could tweak the look of the game to make night scenes look significantly brighter. By changing contrast, brightness, colour and saturation values, PC players with Nvidia hardware were essentially making the darkness of night a non-factor, allowing them to see rival players much easier than the game naturally allows.  Pretty sure this is possible regardless of Nvidia settings. I'm not sure if it's still the case, but back in the day Rust would determine the dark parts of the screen and make them pitch black to account for people merely cranking their gamma.",pcmasterrace,2026-01-02 12:10:52,335
Intel,nx92y6i,"Anyone whose moderately determined will figure out how to get similar results by tweaking monitor and driver settings. Also, how does this game not have basic options for brightness/gamma calibration? Is that stuff not essential for OLED/HDR users?",pcmasterrace,2026-01-02 13:44:30,97
Intel,nx8x08i,Next they gonna ask monitor manufacturers to remove those filters?,pcmasterrace,2026-01-02 13:06:49,59
Intel,nx9r8f1,Too bad these gaming monitors like the ASUS rog ones have this exact filter built in and is undetectable. Just saw 10 vids of it on TikTok,pcmasterrace,2026-01-02 15:54:54,14
Intel,nx9fphu,"This is ignorance every single monitor and tv you can fix this setting the only good using the AMD and NVIDIA is you can do it per game, since all games have different color mapping, plus fix color settings is a most every monitor, tv doesn’t have the same screen for example oled, led, etc.",pcmasterrace,2026-01-02 14:57:15,14
Intel,nx9bwue,I just use it for rtx hdr. Still working as of 2 hours ago so not sure if there's an update for the app that stops it working I dont have yet?,pcmasterrace,2026-01-02 14:36:32,5
Intel,nx9jenv,Hello tarkov my old friend. Crank the gamma up again.,pcmasterrace,2026-01-02 15:16:37,11
Intel,nxakzj0,HDR and brightness up already do a decent job,pcmasterrace,2026-01-02 18:13:48,3
Intel,nxaimfx,They can't do anything about the big low light advantage you get from just having OLED.,pcmasterrace,2026-01-02 18:02:59,2
Intel,nx9ktix,"If it isn't glitching behind locked doors for better loot, ziplining to areas deemed out of bounds, lowering graphics to see through bushes and trees, backstabbing or extraction camping, it's something else. Cringey little incels will always do anything they can  (except just getting better at a game) to gain that sweet advantage over everyone else. Nothing new.  Edit: Being downvoted by said incels.",pcmasterrace,2026-01-02 15:23:43,2
Intel,nx9w43s,Wonder if known nvidia inspector exploits are patched? New games tend to forget it exists.,pcmasterrace,2026-01-02 16:17:54,1
Intel,nxaelyi,Did no one learn anything from Tarkov having and then disabling these?,pcmasterrace,2026-01-02 17:44:27,1
Intel,nxafhte,Dead by Daylight players sweating right now,pcmasterrace,2026-01-02 17:48:33,1
Intel,nxctinj,"Coming from someone who used them because I have vision loss issues, this sucks.",pcmasterrace,2026-01-03 00:58:54,1
Intel,nxf83wd,"They deactivated it 100%, if you try to mess with the Nvidia profile inspector while the game is running you are gonna get a message like “profile inspector detected” and the only option is to exit the game",pcmasterrace,2026-01-03 11:29:29,1
Intel,nxg0r89,"I just want my RTX Dynamic Vibrance back, RTX HDR works tho..games looks so much better with these on imo. I used to use Reshade in games.",pcmasterrace,2026-01-03 14:40:44,1
Intel,nxg1azw,When I see stuff like this I'm so glad that I primarily play single player games. There's so much less to worry about,pcmasterrace,2026-01-03 14:43:42,1
Intel,nxgu68g,"Dark should be dark. I could not imagine playing game like EFT with the most garbage graphics possible where closed off rooms was still perfectly lit on by a magic global illumination BS! All because PvP junkies cried that they don't see s##t. So silly nikitka made his game to look LITERALLY like s##t with all the washed out lighting on maps! Zero shadows, night have luminated objects like walls, some vehicles etc. Literally I never played worst game in lighting regards. PvP games are infested by sad people that want to have some magic eye sight and cry when there is dark shadow and nights are realistically dark!   I love SP games because I can mod stuff to be dark like in real world and immerse myself into this world and not feel like some mutated owl with magic eye sight. PvP junkies ruin games.",pcmasterrace,2026-01-03 17:05:22,1
Intel,nxji4y9,This is why I don’t play games like this.,pcmasterrace,2026-01-04 00:54:26,1
Intel,nxool67,that's why I launched the game now and I was like where the fuck are my filters?,pcmasterrace,2026-01-04 20:07:01,1
Intel,ny2muz4,"Y'know it would be interesting to see a game that completely locks the video settings (apart from resolution) across every platform so that everyone is seeing almost exactly the same thing (aspect ratio and widescreens with bigger field of view aside).  Just to see if it levels the playing field.  I played a lot of DayZ mod and you had a massive advantage putting everything except render/view distance on low.  Which sucked because you could have the hardware to make it look stunning, but would be at a massive disadvantage if you used it.",pcmasterrace,2026-01-06 20:34:02,1
Intel,nx8pcwc,Cheaters going to cheat,pcmasterrace,2026-01-02 12:09:34,-2
Intel,nx9ne27,"This is pointless, you can still install Reshade into overlay software such as Lossless Scaling or Magpie and use them to scale the game window, and you'll be able to make the Reshade effect work on any window, unless you disable all the capture APIs such as DXGI and WGC, which also means that video recording software such as OBS will not work either",pcmasterrace,2026-01-02 15:36:29,0
Intel,nxakb7d,Try hards will do anything they can for an edge. It’s so sad.,pcmasterrace,2026-01-02 18:10:43,0
Intel,nx9jqzd,What's the work around ?,pcmasterrace,2026-01-02 15:18:20,-4
Intel,nxbvlpy,"Honestly if you have a good set up and OLED, the dark isn't bad anyway",pcmasterrace,2026-01-02 21:56:53,0
Intel,nxcstbt,So peanut is gonna struggle in dark places??? I must say im.excited to see the result,pcmasterrace,2026-01-03 00:54:56,0
Intel,nxctx3t,With the amount of cheaters in the game currently they should allow it if you turn off cross play so you don't have an advantage over the console kids.,pcmasterrace,2026-01-03 01:01:11,0
Intel,nx99doi,Just use nvidia control panel?,pcmasterrace,2026-01-02 14:22:06,-9
Intel,nxaak63,PC players gonna PC player,pcmasterrace,2026-01-02 17:25:25,-8
Intel,nx9ytqs,They did something similar with RUST too I think. People used gamma to avoid the pitch black night's darkness,pcmasterrace,2026-01-02 16:30:39,54
Intel,nx9dh0w,I remember the days of easily seeing at night   But yea. Definitely glad they added this later on,pcmasterrace,2026-01-02 14:45:11,142
Intel,nx9y5jc,"Yup, they have the best fix.",pcmasterrace,2026-01-02 16:27:31,14
Intel,nxaoglb,"As an original DayZ mod player (and part of the group who started the first US servers) I keep feeling like I'm in that ""First time?"" meme whenever I see a headline about Arc Raiders.",pcmasterrace,2026-01-02 18:29:34,7
Intel,nxdhubr,Suoer cool,pcmasterrace,2026-01-03 03:21:11,1
Intel,nx8x1zs,Also why using Nvidia Profile Inspector can trigger bans.,pcmasterrace,2026-01-02 13:07:09,98
Intel,nx8z8zn,"It's similar to how turning the foliage to low on PUBG worked, at least it did when I used to play it like 8 years ago. By having it set on medium or above you was at a disadvantage.",pcmasterrace,2026-01-02 13:21:35,24
Intel,nx9hjn8,Shit my monitor can do this,pcmasterrace,2026-01-02 15:06:58,8
Intel,nxao8t3,Yeah typically you can darken the in games settings and then go to Nvidia control panel and edit the brightness there. That's always been a thing in any game tbf.,pcmasterrace,2026-01-02 18:28:34,2
Intel,nxb2wqy,"I doubt it's because of gamma and brightness, in counter strike it was breaking the game because people with certain filters could see through smoke, I think similar thing was happening here",pcmasterrace,2026-01-02 19:37:13,1
Intel,nx9mneb,You can still turn up gamma with the Nvidia app and Nvidia control panel. They just disabled being able to do it with overlay,pcmasterrace,2026-01-02 15:32:50,23
Intel,nx9o5t1,I just use the settings on my monitor to mess with that. It usually looks better than messing with in game options anyway,pcmasterrace,2026-01-02 15:40:14,5
Intel,nxa5w9f,"I believe it doesn't support HDR and some areas are pitch black on OLED. I suppose the flashlight is meant to help with that, but give away your position.",pcmasterrace,2026-01-02 17:03:33,4
Intel,nx973ia,Hopefully it’s enough to discourage a good percentage of people.,pcmasterrace,2026-01-02 14:09:01,11
Intel,nx9wuhh,A simple profile switch on my monitor does the trick.,pcmasterrace,2026-01-02 16:21:20,2
Intel,nxa8eqk,i use monitor then vibrance gui to put colour back in,pcmasterrace,2026-01-02 17:15:21,2
Intel,nx9zhao,The difference is opening an overlay and toggling on a preset takes a lot less time than mashing the buttons on your monitor when the scene changes to night.,pcmasterrace,2026-01-02 16:33:43,2
Intel,nxdvmba,Because arc raiders is trash,pcmasterrace,2026-01-03 04:49:24,0
Intel,ny5b0ma,"as long as you have a nvidia gpu (not sure how it works on AMD), its not detectable, as long as you use nvidia control panel or NVIDIA app.",pcmasterrace,2026-01-07 04:59:00,1
Intel,nx9mie8,RTX HDR won't be blocked,pcmasterrace,2026-01-02 15:32:08,-1
Intel,nxbc5yu,Backstabbing and extraction camping are not like the others. Those are actual gameplay options.,pcmasterrace,2026-01-02 20:22:08,2
Intel,nx9ux7e,"Yep. Absolutely disgusting what people will do in order to ""prove"" they are good at something.  Stopped playing competitive shooters 15 years ago. Just got tired of the bullshit, and that was back then.",pcmasterrace,2026-01-02 16:12:19,-1
Intel,nxa4h16,Bruh I'm on OLED and just want to be able to who is shooting me from the corner of the extract.,pcmasterrace,2026-01-02 16:56:52,2
Intel,nxbzfj4,Who hurt you?,pcmasterrace,2026-01-02 22:16:06,0
Intel,nx9v1oj,Thats why theres no point try harding any game. People will just cheat their way.,pcmasterrace,2026-01-02 16:12:54,-3
Intel,nx9xkk0,Adjusting my monitor color settings makes me a cheater?,pcmasterrace,2026-01-02 16:24:47,9
Intel,nxa6hx4,"Man I don't even play AR, but I hate this type of argument so much. We shouldn't do anything to make it harder to do something bad, because people will still be able to do bad thing anyways. Will it solve the problem? No. But a minor improvement for a negligible cost is still worth it.",pcmasterrace,2026-01-02 17:06:24,9
Intel,nxe039x,"'it's pointless to stop pedophillia because other countries have legal child marriages' type of arguement, young people are so fucking nihilist they would rather starve to death than grab food literally next to them.  THIS IS JUST A STEP into either implementing wide anti-tampering techniques like cleverly hidden noise ala DayZ, or standardizing properly implemented calibration tools for stuff like HDR.  You take action to make change happen.   Being a nihilist and DOING NOTHING solves gues what... NOTHING.   We have shitty implementation for shit like HDRT because you gamers rather would chrip about 'woke games' than making noise about stuff like proper HDR implementation.",pcmasterrace,2026-01-03 05:20:48,1
Intel,nxrb5lt,"The game needs to be fixed. It is absolutely terrible for visuals and lighting. Either they implement it properly, let us all do it with in-game settings, or we will find a way to fix it ourselves outside of the game. Nobody should have to buy a specific monitor or panel type just to be able to see what is on the screen in this game…",pcmasterrace,2026-01-05 04:02:11,1
Intel,nx9sbts,Use your flashlight in game,pcmasterrace,2026-01-02 16:00:04,5
Intel,nx9asdg,"What you can do in control panel is very limited compared to the filters. Same with monitor settings.  Tarkov banned these for the same reason, as well as some other games.",pcmasterrace,2026-01-02 14:30:08,2
Intel,nxajg4f,"The thing is there's hardware solution, your TV. Also, don't act like xim and cronus aren't the 2 most bought PS5 accessories. Nearly all known console Apex Legends predators use the zen to my knowledge.",pcmasterrace,2026-01-02 18:06:46,5
Intel,nx979we,"I would imagine this as I do recall this exploit being used, aleast back in Battlefield V and forcing the LOD to the lowest, allowing you to see who was the enemy very well.",pcmasterrace,2026-01-02 14:10:03,36
Intel,nx9xt98,"Same in Hunt Showdown, people would derender trees and buildings to see across the map.",pcmasterrace,2026-01-02 16:25:55,12
Intel,nxaf1m5,It can trigger bans? I use it for FPS caps in certain games thats kinda mental,pcmasterrace,2026-01-02 17:46:29,2
Intel,nxbp2du,"How so? Nvidia Profile Inspector allows you to tweak various driver-level settings that aren't exposed in the Nvidia App or Nvidia Control Panel, either globally for all programs, or only for specific ones.  Once you change a setting, that's it. It's done. You can close the app now. It doesn't have to remain running while you're playing a game. So how would the game detect NPI and ban you for using it?  Or are you saying that having that simply having that app on your PC will trigger the ban, even if you've not used it at all, let alone for the particular game? Given that NPI is a portable program (i.e. it doesn't get 'installed' anywhere, it's just an EXE), that would mean that the game would have to scan the entire PC? That's a very time-intensive process.",pcmasterrace,2026-01-02 21:25:08,1
Intel,nxas1pd,"How would someone even detect usage of Nvidia profile inspector? It doesn't touch the game files in any way. It just tells your GPU how to handle things.   Like it's a tool that allows you to change the profiles of a game... I wouldn't think that would breach pretty much any EULA.  Sure you can say, ""gain unfair advantage"" but... It's not an exploit, vulnerability, cheat/hack, etc. It's just using the Nvidia driver to its fullest.",pcmasterrace,2026-01-02 18:46:01,1
Intel,nxah58c,Any game that bans people for stuff like this is a game I don't care about being banned from.,pcmasterrace,2026-01-02 17:56:09,-11
Intel,nx9aedi,I still do that today.    Single player campaign runs at ultra high ray tracing DLSS everything settings.   Multiplayer runs on low settings except fog is disabled or view distance increased.    Been a habit since quake 2,pcmasterrace,2026-01-02 14:27:54,16
Intel,nxarzk0,"I'm used to doing this just cuz my monitor is naturally pretty dark, even when I tweak the settings on The panel I still have to turn up the gamma some... About 2 weeks ago I definitely noticed that things look different in Arc raiders. I figured it was something that embarked it on their side, but it must have been this Nvidia change?  . It is a welcomed one. Now nobody can try and get around that, while I still am able to keep the brightness for other areas. It worked out in the end for me and my use case but crazy it took this long lol",pcmasterrace,2026-01-02 18:45:45,2
Intel,nxahbcm,Why? Who cares? People can do whatever they want.,pcmasterrace,2026-01-02 17:56:57,-17
Intel,nxa1zdp,Honestly I thought everyone just did It via Nvidia control panel by just wacking up the gamma that's the way peanut showed how he did it,pcmasterrace,2026-01-02 16:45:22,-9
Intel,nxajbrz,Why don't you go back to consoles if you hate pc customization,pcmasterrace,2026-01-02 18:06:13,-18
Intel,nxagtxg,"You can save display settings profiles and have them on a key shortcut or use the companion application to do it, at least that's how my Gigabyte ultrawide works.",pcmasterrace,2026-01-02 17:54:43,6
Intel,nxa280n,Tbh if your upping your gamma you might as well keep it up at all times playing arc so many times people will be hard to see in the shadows just in buildings on daytime raids,pcmasterrace,2026-01-02 16:46:29,4
Intel,nx9mq6l,Thats a filter though?,pcmasterrace,2026-01-02 15:33:12,3
Intel,nxaktjj,"fr, anybody who’s seen how dark this game is on an OLED knows",pcmasterrace,2026-01-02 18:13:03,5
Intel,nxakqf3,Those corners etc are intentionally dark.,pcmasterrace,2026-01-02 18:12:39,5
Intel,nxr7sfs,can't even play on night map with an oled screen.,pcmasterrace,2026-01-05 03:42:44,1
Intel,nxcfpp9,"Certainly not you, with your unbelievable wit.",pcmasterrace,2026-01-02 23:42:49,0
Intel,nxa1mdj,There are plenty of games without mass cheaters,pcmasterrace,2026-01-02 16:43:41,2
Intel,nxa46jq,Right?! I'm on an OLED.  I'm severely nerfed fighting on most maps because the moment anyone is in any slightly shadowed area they are absolutely pitch black and invisible on my screen.,pcmasterrace,2026-01-02 16:55:31,10
Intel,nxb77co,Does it help you gain an advantage?   Clearly the devs think its cheating if they're updating the game to make it harder to do.,pcmasterrace,2026-01-02 19:57:54,1
Intel,nxauukk,"If you are doing it to gain an advantage, yeah.  I think intent matters a whole lot for these discussions.  Being able to see at night because you have a shitty monitor is different than adjusting your monitor to get an advantage",pcmasterrace,2026-01-02 18:58:55,0
Intel,nxa8n1a,"Nah the devs need to do something about how dark the game is in some situations: I've got 2 PC's right next to each other, and on my main PC even *dark* areas are visible to me (on a pricy HDR monitor), but on the other computers monitors those same dark areas are literally pure black areas you cannot see in. For example, in Stella Montis the ventilation shaft with the ladder? On my main PC I can see in there (*barely) without my flashlight, but enough to navigate through it (though I may not see someone quickly or easily), but on the other computer its literally *pitch black*, meaning just by having a different monitor I don't *100% need the flashlight*, and on another I'm literally blind.  That in and of itself is an unfair advantage, just based purely on hardware, and I set an Nvidia filter to make it look *similar* to the main PC. In neither situation was I attempting to make things unfair, simply *as visible* as they should be (neither by getting my expensive monitor, nor using the filters), nor was I completely eliminating shadows and dark areas.  Don't get me wrong, I get people were using the same filters to completely eliminate the dark areas (that's unfair), but it's INSANE that the game doesn't have even BASIC gamma settings to make the game more playable on certain hardware combos. Because without it the literal hardware itself can completely nerf or cheat for you.",pcmasterrace,2026-01-02 17:16:26,8
Intel,nx9cih2,Tarkov added their own filters to the game.,pcmasterrace,2026-01-02 14:39:54,3
Intel,nx9hlck,Sure I guess having all those options are kinda extra but raising the gamma/contrast is really all needed to see again. Not everyone likes to play the game not being able to see anything. I get that’s how the game was made but I’d rather not lol. Arc raiders can have spots so dark even the flash light doesn’t reach much,pcmasterrace,2026-01-02 15:07:12,1
Intel,nxayd6i,It's disingenuous to act like it's not way more prevalent on PC.,pcmasterrace,2026-01-02 19:15:26,-3
Intel,nx9bsdt,"Yup. You could also use hidden filters that gave you a competitive advantage.      It's why I always say if you play online games but use NIS in other apps, to make sure it's closed or you revert to the stock config before booting the game because some ACs can tell you've modified the Nvidia defaults.",pcmasterrace,2026-01-02 14:35:50,9
Intel,nxa4bjv,It used to be so buggy I had missing textures without any cheats.,pcmasterrace,2026-01-02 16:56:10,7
Intel,nxbzsum,"Because you've modified the GPU driver outside of what it is meant to be doing by default. NPI changes the driver database itself for the specific game or the default global profile that the Nvidia drivers use in games if a game doesn't have a specific profile within the Nvidia app.  There are driver checks that anticheat systems perform. Using NPI can invalidate the driver check, especially when you enable flags that are not exposed within the Nvidia app or GeForce drivers and doubly so when you use NPI to selectively not render certain textures like buildings or foliage.  NPI has been used in games like Battlefield or Hunt Showdown for competitive advantages by telling the GPU to not render walls so you can see through them or to not render grass or environmental hazards like smoke and fog.",pcmasterrace,2026-01-02 22:18:01,1
Intel,nxb808i,"i know that arc raiders anti cheat will trigger anytime you have nvidia profile inspector open. It will not ban you, but it will close the game.",pcmasterrace,2026-01-02 20:01:46,5
Intel,nxb3a63,"It is an exploit. You can use the profile inspector to make the GPU driver not render certain textures letting you see through objects or through walls and foliage.      It's quite easy to detect because you're modifying the GPU driver itself, which then interacts with the game engine and the game's anti cheat.",pcmasterrace,2026-01-02 19:39:02,1
Intel,nxaw6cf,"I meant all the settings there too sorry. Yeah. I darken the in game settings even lower in game gamma sometimes and then use the Nvidia settings to bring them all up one. I don't play Arc Raiders not a fan of it, so I can't say.   I don't even press Apply in the control panel. I just hit cancel once I'm done needing it and boom back to my normal settings.",pcmasterrace,2026-01-02 19:05:08,1
Intel,nxasl4y,It’s a PvP game,pcmasterrace,2026-01-02 18:48:30,11
Intel,nxauagg,So people don't cheat and ruin other people's fun,pcmasterrace,2026-01-02 18:56:20,2
Intel,nxb5xcx,Peanut mentioned      # Peanut is a hack,pcmasterrace,2026-01-02 19:51:43,6
Intel,nx9mwxi,"It's not technically a filter as such, since it gets turned on for applications from the driver level, works differently from normal filters AFAIK",pcmasterrace,2026-01-02 15:34:08,0
Intel,nxbpg6o,Yes and on LED it's still possible to at least distinguish figures.  it's near impossible on OLED without a small gamma bump.  luckily that can still just be done in Control Panel just at the monitor level instead of game level.,pcmasterrace,2026-01-02 21:26:58,1
Intel,nxr9h15,I highly doubt it…this generation is completely socialized to a pay to win attitude. They are not going to “earn” their way through like the rest of us who put in the work to succeed…,pcmasterrace,2026-01-05 03:52:29,2
Intel,nxa4pq6,"Everyone has different monitors, there will never be a totally fair playing field. It’s very possible that some people bought their monitors secondhand and never noticed that the settings were already adjusted. My point is that you don’t know the situation.",pcmasterrace,2026-01-02 16:57:59,8
Intel,nxahc6r,Welcome to the linux gaming experience.,pcmasterrace,2026-01-02 17:57:03,-5
Intel,nxeg8l9,"Embark didn’t update the game to change that, so no lol",pcmasterrace,2026-01-03 07:30:34,0
Intel,nxbb97z,You realize that’s the exact reason they removed it right? In the same sentence you said it’s okay to have a monitor so you can see at night and then proceeded to say you shouldn’t use it to gain an advantage. What the fuck are you talking about bro,pcmasterrace,2026-01-02 20:17:38,1
Intel,nxrbz7x,"Exactly, every one of my monitors is different. Even the two that are exactly the same. One I can see fine in the dark, the other is pure black. My cheap upper monitor is clear as day on night raids with default settings.  So, there is a hardware advantage for sure. They need to fix the lighting and give us in-game settings and presets.",pcmasterrace,2026-01-05 04:07:07,2
Intel,nxarpbl,And they are also very limited on what you can do compared to Nvidia  filters. Nothing you can do in Post FX can make nighttime look like daytime like you could with Filters. Some of the settings  also hit performance fairly hard,pcmasterrace,2026-01-02 18:44:26,0
Intel,nxdac0j,I leave a global setting on for all my games. Never been kicked off. Maybe it's because I only use it to globally enable DLSS4?,pcmasterrace,2026-01-03 02:36:26,1
Intel,nxb4i45,"That's not an exploit. An exploit in the sense you're using (cheating) means you're taking advantage of a flaw or bypassing protection systems. That's not what is happening.   ""Modifying the GPU driver itself"" you know that simply updating the drivers on your GPU modify the drivers right?  You're not exploiting anything. You're using a legitimate feature of the GPU. It's not like someone did this and broke Nvidia'e TOS. The driver is literally designed to do this.",pcmasterrace,2026-01-02 19:44:53,-5
Intel,nxeducr,Peanut is fucking great     Fantastic entertainer,pcmasterrace,2026-01-03 07:09:56,1
Intel,nx9n0ti,"Interesting, does require the overlay and filters enabled to work however.",pcmasterrace,2026-01-02 15:34:41,6
Intel,nxe6403,"You can enable it via the driver or as an overlay via the Nvidia app. The filter version gives you control over saturation/contrast/etc, the driver version doesn't.",pcmasterrace,2026-01-03 06:06:23,1
Intel,nxr7wri,my oled is cranked and it's still pitch black lol,pcmasterrace,2026-01-05 03:43:25,1
Intel,nxgaj08,I don't think you understand OLED monitors honestly lol,pcmasterrace,2026-01-03 15:31:47,1
Intel,nxdslqb,????,pcmasterrace,2026-01-03 04:29:10,0
Intel,nxdr4el,"There's a difference between a slight adjustment and making things super clear though.  Anyone who does the latter is absolute filth, but if you're on an OLED or something in the day sure give it a little boost.",pcmasterrace,2026-01-03 04:19:21,2
Intel,nxbnlok,I'm saying intent matters a lot,pcmasterrace,2026-01-02 21:18:09,0
Intel,nxawgmh,I mean you could use the in game filters to get much more clarity at night and in general. I’m not saying nvidia isn’t more powerful. Was more just making a statement they (battlestate) wanted control over it.,pcmasterrace,2026-01-02 19:06:28,1
Intel,nxbf4hg,"No. Modifying the driver to force it to not render certain things in a game is an exploit.  The filter agreement I could see as not an exploit.  But changing the driving to see through walls, etc is certainly an exploit.",pcmasterrace,2026-01-02 20:36:48,9
Intel,nxe57yi,"Bruh.  It’s using an external program to circumvent a gameplay mechanic to gain an advantage and it’s explicitly forbidden by the developer.  Idk how much more cut and dry you can get.   You can argue the semantics all you want, at the end of the day, the developer gets to determine what they determine an exploit is and it’s clear where they drew the line in this case.",pcmasterrace,2026-01-03 05:59:24,2
Intel,nxjdvdj,"You don't think that ""not rendering cover"" is an exploit that should be prevented? Might as well just fess up and admit you're a cheater.   Also, this isn't a court of law. Just because ""Nvidia let's me do do it"" is _zero_ reason why a game publisher needs to allow it. It's totally unrelated.",pcmasterrace,2026-01-04 00:31:48,1
Intel,nxgek4m,![gif](giphy|DFNd1yVyRjmF2),pcmasterrace,2026-01-03 15:51:32,1
Intel,nx9nufj,"RTX HDR does not require nvidia overlay to be enabled, you can turn on RTX HDR without installing the nvidia app via Nvidia Profile Inspector",pcmasterrace,2026-01-02 15:38:42,1
Intel,nx9n7dk,"You'll find that all the filters will disappear, but RTX HDR & RTX Vibrance will still be available.  Same as for EFT and a number of other games that have the normal filters blocked.  The overlay doesn't stop working, since that again works from a driver level.",pcmasterrace,2026-01-02 15:35:35,-1
Intel,nxekhik,"Playing some multiplayer games on linux results in bans just for using linux, because game devs think that only cheaters use linux.",pcmasterrace,2026-01-03 08:06:53,0
Intel,nxdr8ly,You people are intolerably stupid,pcmasterrace,2026-01-03 04:20:06,1
Intel,nxc624x,"You can't see through walls with Nvidia Profile Inspector. That's not how that works. Unless you're specifically telling the walls not to render, which isn't a native option in the application.   Nearly all the things you can modify deal with effects and how your GPU handles them. Walls are not effects. So if it's modifying walls... Then that's not the intended application of it. Hardly a NPI issue.",pcmasterrace,2026-01-02 22:50:21,-1
Intel,nxe5t8m,"Nah you don't get to call something a pineapple because you don't know what to call it lol.   Call it the proper term, a violation of the TOS or EULA or whatever it's listed under. It's not an exploit, pretty simple.",pcmasterrace,2026-01-03 06:04:02,1
Intel,nx9nytz,I never said it did but I assume you can enable filters via profile inspector too right?,pcmasterrace,2026-01-02 15:39:18,2
Intel,nxel8r8,"brother, as a fellow Linux guy, that was a completely insensible reach.",pcmasterrace,2026-01-03 08:13:24,2
Intel,nxdrdtf,So you're just pro cheating?,pcmasterrace,2026-01-03 04:21:03,1
Intel,nxcnpo2,"If you can change how your gpu renders things, and it gives you an advantage in the game and it is not approved by the developer, then it’s an exploit.",pcmasterrace,2026-01-03 00:26:55,2
Intel,nxf6jvl,Ok buddy.,pcmasterrace,2026-01-03 11:16:23,1
Intel,nxds8rt,If you think that changing the color settings of your monitor constitutes cheats there is absolutely no helping you understand,pcmasterrace,2026-01-03 04:26:46,0
Intel,nxcq6ey,"...by that logic, having a 5090 gives you an advantage over someone that has a 1080 because a 5090 renders things differently than a 1080, so having a 5090 is an exploit.  Now as for ""not approved by the developer"", there are PLENTY of things not explicitly approved by a developer. MMO mice, for example, that enable you to have 10 buttons on the side of your mouse may give you an advantage over someone that does not. I don't see that explicitly approved by nearly any developer, so that must be cheating, right?",pcmasterrace,2026-01-03 00:40:22,-2
Intel,nxdtk07,"I said the degree to which you change them is important.  If it's meant to be difficult to see enemies at night, and you change some setting that makes it easy to see enemies at night, I'd call that cheating.  If it's meant to be difficult to see enemies at night but due to your environment or something you can't see at all and you make it so that it's difficult to see them but at least possible, that's of course fine.  Isn't necessarily a hard line or anything, but if you're there in the settings going 'heh heh heh this'll make it so I can see enemies really easily at night', and maybe making profiles to switch while in game...",pcmasterrace,2026-01-03 04:35:35,0
Intel,nxcy7wl,"Swap the word approved for permitted.  Ultimately it’s up to the developer to determine what is an exploit and what isn’t. It’s their game, their terms of service. The make the rules.  If they see it as an exploit, then it is one.  Good example is overwatch, if you have an ultrawide monitor it’s cropped on the sides because they see the additional screen space as an advantage. So if you mod your game somehow to get an uncropped ultrawide view, then having a nicer ultrawide monitor is an exploit.",pcmasterrace,2026-01-03 01:25:55,1
Intel,nxra8pn,"I change certain settings for almost every program I open. There is no universal setting that works for everything. The good programs have built in settings with presets. The garbage ones like arc raiders require significant outside changes just to see anything on screen. Everything I do is manual though. Through windows, Nvidea, and monitor settings…",pcmasterrace,2026-01-05 03:56:59,1
Intel,nx0z44d,"Rx 6800, not sure i would rely on intel driver support being long lasting",pcmasterrace,2026-01-01 03:33:25,5
Intel,nx17qcu,"The RX 6800 XT for sure. That gpu is equivalent to a 3080 ti out of the box and if you overclock it I have seen people hit RTX 3090 scores with it! If you can manage to find one for a good price, try to snipe an RX 7800 XT. The 7800 XT is even more powerful with better driver support for longer.",pcmasterrace,2026-01-01 04:34:23,3
Intel,nx10wfz,"I have two custom builds, one with a 7900xtx and another with an ARC B570. I like the RX 6800 because of its 16GB of VRAM; its performance is good, and you can play anything you want on it. Intel's option is fantastic, the best blue GPU, but times are tough, and I don't like 12GB of VRAM if you can get something better.",pcmasterrace,2026-01-01 03:45:35,2
Intel,nx154vl,6800 is better  b580 doesnt like old hardware and drivers are still early,pcmasterrace,2026-01-01 04:14:53,2
Intel,nx1gqen,"Currently using a 7700xt for this game and it's holding up but wow, it's a demanding title",pcmasterrace,2026-01-01 05:48:56,1
Intel,nx108dj,why?,pcmasterrace,2026-01-01 03:41:05,1
Intel,nx158ir,intel doesnt have decades of gpu driver development like amd and nvidia.  intel doesnt play well with old hardware  intel needs resizeable bar in bios settings to work properly  intel might fail and abandon gpu support. intel isnt doing good right now. 13th and 14th gen cpu had failures company lost 50% of its stock value in 1 year.,pcmasterrace,2026-01-01 04:15:37,5
Intel,nx1vemn,"Intel has been supporting and maintaining GPU drivers since the early 2000s.  In most years Intel was the largest GPU vendor by shipping video products in the world.  Intel's not doing great at the moment, but it is now part-owned by the US government and Nvidia, like how the Chinese do things.",pcmasterrace,2026-01-01 08:10:15,1
Intel,nxtvzb1,Are there any scorch marks on the CPU pins?,pcmasterrace,2026-01-05 15:23:20,714
Intel,nxtxcin,cat grieving the death of its heater,pcmasterrace,2026-01-05 15:29:53,688
Intel,nxuil8u,Did the pc overheat after sucking all the hair off the cat?,pcmasterrace,2026-01-05 17:09:09,281
Intel,nxtuqj4,I blame the bakeneko.,pcmasterrace,2026-01-05 15:17:17,546
Intel,nxu56tm,I blame Beerus.,pcmasterrace,2026-01-05 16:06:50,162
Intel,nxtyqha,What BIOS version where you on? Im on same Mobo and Chip,pcmasterrace,2026-01-05 15:36:33,52
Intel,nxtxsz7,So it’s not just AsRock,pcmasterrace,2026-01-05 15:32:04,173
Intel,nxu68cq,"You lucked out, imagine if it was the ram",pcmasterrace,2026-01-05 16:11:43,70
Intel,nxuoucm,Bruh I am seeing one 9800x3d dying post like everyday more than I see 5090 melting cables post. Is this the 13th-14th gen intel moment for AMD?,pcmasterrace,2026-01-05 17:38:07,48
Intel,nxtycpt,I bet its because of all the cat hairs....,pcmasterrace,2026-01-05 15:34:44,114
Intel,nxua9w6,Is it just the matter of the 9800x3d being the most popular cpu by far and therefore failure rates are usual for the proportion?   I just ordered this chip and this board… I’m Slightly concerned but suspect it’s the above.,pcmasterrace,2026-01-05 16:30:34,28
Intel,nxtwueg,Surprisingly not asrock,pcmasterrace,2026-01-05 15:27:28,83
Intel,nxu4jxw,Make sure you put a warranty claim in. AMD replaced my 3600XT when it died randomly.,pcmasterrace,2026-01-05 16:03:52,22
Intel,nxu0nn6,Cat don't care it would seem.,pcmasterrace,2026-01-05 15:45:39,6
Intel,nxu5kmo,What settings were used on the Mobo? Were you using that AI overclock feature? What BIOS version did you have on the mobo was it an older version? Surge protector used or was pc plugged straight into the wall socket?,pcmasterrace,2026-01-05 16:08:38,7
Intel,nxu394f,That’s a weird name for a cat. Should probably also bury him.,pcmasterrace,2026-01-05 15:57:48,24
Intel,nxunza3,"I'll never get why people allow cats to rest on their PC. Sure it's cute etc, but all it takes is for the cat to be sick one time and that's a PC full of cat pee that you'll never get out.   When cats are ill, it's not rare for them to urinate in place of comfort rather than their litter box.  It's just not worth it",pcmasterrace,2026-01-05 17:34:09,27
Intel,nxtz26l,It looks like your cat is grieving the death of your CPU as well 🤣,pcmasterrace,2026-01-05 15:38:07,5
Intel,nxtxa29,*A hairless cat sits atop a tower...*,pcmasterrace,2026-01-05 15:29:33,7
Intel,nxu3gfx,so is RMA an option ? or done for gud?,pcmasterrace,2026-01-05 15:58:45,5
Intel,nxuf3sr,i guess you didn't update bios when you build the pc because asus and asrock were famous for cooking out the cpu especially am5 sockets so yeah could be that or just bad luck btw asus did fix that issue so if you have asus mobo but not updated bios i recommend doing so and asrock to this day issues persist,pcmasterrace,2026-01-05 16:52:53,4
Intel,nxu74q9,I specifically bought a 9800x3d when my i9 13900k died because I didn't want to deal with something like this again and now I'm seeing them dying left and right. It's very concerning.,pcmasterrace,2026-01-05 16:15:55,13
Intel,nxu8717,"What overclock settings, of any, were you using?",pcmasterrace,2026-01-05 16:20:54,6
Intel,nxuf13s,its that asus board. its going to happen again,pcmasterrace,2026-01-05 16:52:33,3
Intel,nxug804,it might the god of destruction on top of ur pc lol joking aside its just not asrock anymore hmmm,pcmasterrace,2026-01-05 16:58:06,3
Intel,nxura8b,Curious if you were using default BIOS settings or if you had overclocked?   And I ask that in an entirely non-critical way...you should be able to do reasonable overclock without fear of anything more than a crash. I guess if you're using a custom water loop you then you almost certainly were...otherwise what would be the point?,pcmasterrace,2026-01-05 17:49:15,3
Intel,nxuywny,Probably all the cat hair,pcmasterrace,2026-01-05 18:23:26,3
Intel,nxvbpy6,My sphinx lives on top of my Xbox for the heat haha,pcmasterrace,2026-01-05 19:20:48,3
Intel,nxzq5by,weird name to call your cat,pcmasterrace,2026-01-06 11:49:25,3
Intel,nxu1rg2,Is this is an issue with 9950 x3d also?,pcmasterrace,2026-01-05 15:50:51,4
Intel,nxua263,"I thought ASRock was shit but now I'm starting to get a bit suspicious at how willing AMD is to RMA CPUs if these failures are happening through no fault of their own. Surely they'd put up more of a fight if they knew they were in the clear, right?",pcmasterrace,2026-01-05 16:29:34,6
Intel,nxudb5r,4th time I’ve seen arc raiders kill a machine,pcmasterrace,2026-01-05 16:44:35,5
Intel,nxu94x4,I think amd was pushing out bad batches of chips and didn't want to ruin their image so they probably paid ASRock to take the hit because business hasn't slowed down for either of them if you check their subreddit.,pcmasterrace,2026-01-05 16:25:18,5
Intel,nxu6fjf,I hope the 7700x i have stays fine. Is that a common problem with AMD cpus?,pcmasterrace,2026-01-05 16:12:38,2
Intel,nxu6s45,What BIOS version??,pcmasterrace,2026-01-05 16:14:16,2
Intel,nxub1xz,Upvoting for bingus.,pcmasterrace,2026-01-05 16:34:10,2
Intel,nxug25m,"Damn, this is alarming to see on an ASUS board. Went through this myself a few months back, good luck man.",pcmasterrace,2026-01-05 16:57:21,2
Intel,nxuj56p,"Well, at least you know it wasn't the cat hair.",pcmasterrace,2026-01-05 17:11:45,2
Intel,nxull1j,Bastet demands sacrifices!,pcmasterrace,2026-01-05 17:23:08,2
Intel,nxumxpf,weird name for a cat but RIP anyways,pcmasterrace,2026-01-05 17:29:21,2
Intel,nxuok37,"Dang, instead of focusing on power. I think company's need to focus on reliability.",pcmasterrace,2026-01-05 17:36:50,2
Intel,nxv8j1i,i love the cat casually staying on top of the pc,pcmasterrace,2026-01-05 19:06:20,2
Intel,nxvpo2z,UPDATE YOUR BIOS WITH THE NEW CPU.   if your motherboard has a problem where it burns the CPU the bios should fix it,pcmasterrace,2026-01-05 20:25:33,2
Intel,nxx1qsc,Cat looks dead,pcmasterrace,2026-01-06 00:21:15,2
Intel,nxxfh7t,I'm relieved that your cat isn't named 9800x3D,pcmasterrace,2026-01-06 01:34:05,2
Intel,nxy1tpp,I blame the cat,pcmasterrace,2026-01-06 03:37:21,2
Intel,nxyq1sm,I've got an i7-6700 bought in 2015 and its still going strong,pcmasterrace,2026-01-06 06:24:15,2
Intel,nxzqkie,"Be honest - what kind of ""off default"" settings did you have?",pcmasterrace,2026-01-06 11:52:38,2
Intel,ny02036,"Of course it died, it sucked all the hair of your cat...",pcmasterrace,2026-01-06 13:11:35,2
Intel,ny2xdc6,That’s wild man. Sounds like you did everything correctly too. Sorry to hear about your misfortune,pcmasterrace,2026-01-06 21:22:08,2
Intel,nxu8k78,Cat hair is probably what got it. This one's shed a lot.,pcmasterrace,2026-01-05 16:22:37,3
Intel,nxujayz,As long as the cat is fine.,pcmasterrace,2026-01-05 17:12:31,2
Intel,nxvywki,![gif](giphy|IARsaTPpY5IiY),pcmasterrace,2026-01-05 21:08:45,2
Intel,nxu1wi1,Beautiful cat.,pcmasterrace,2026-01-05 15:51:31,2
Intel,nxu3i0a,"What PSU do you have, how old is it?",pcmasterrace,2026-01-05 15:58:57,1
Intel,nxu6rbf,Weird name to give your cat...,pcmasterrace,2026-01-05 16:14:10,1
Intel,nxu7ozz,I see the grim reaper was collecting.  R.I.P.,pcmasterrace,2026-01-05 16:18:33,1
Intel,nxu81r8,Such an odd name for a cat…,pcmasterrace,2026-01-05 16:20:13,1
Intel,nxuak57,Sorry about your cat.,pcmasterrace,2026-01-05 16:31:54,1
Intel,nxubmb7,It was beerus,pcmasterrace,2026-01-05 16:36:47,1
Intel,nxuc177,Thanks for including the cat,pcmasterrace,2026-01-05 16:38:41,1
Intel,nxuckvo,Latest bios?,pcmasterrace,2026-01-05 16:41:12,1
Intel,nxucngk,At least it wasn't the ram that died,pcmasterrace,2026-01-05 16:41:32,1
Intel,nxud7tk,"I'm sorry, mostly because as a fellow distro plate enjoyer, it really sucks having to drain it",pcmasterrace,2026-01-05 16:44:09,1
Intel,nxudwak,Have you tried to reinsert the old 9800x3d to see if the PC would post?,pcmasterrace,2026-01-05 16:47:18,1
Intel,nxue7ok,Have you overclocked it?,pcmasterrace,2026-01-05 16:48:44,1
Intel,nxuev9o,I'm sorry for your loss.,pcmasterrace,2026-01-05 16:51:47,1
Intel,nxuf1o1,At least we know it's not from fur.,pcmasterrace,2026-01-05 16:52:37,1
Intel,nxufmag,How long did you have the CPU before it died?,pcmasterrace,2026-01-05 16:55:18,1
Intel,nxufng2,Is the heat exchanger for your custom water loop mounted to the top perchance? It wouldn't make sense for just the CPU to overheat but the cat probably isn't helping with your airflow,pcmasterrace,2026-01-05 16:55:27,1
Intel,nxug66s,"The water loop is the issue, just get a 360 AIO.",pcmasterrace,2026-01-05 16:57:52,1
Intel,nxugj2f,No airflow through watercooler due to cat? Jokes aside sorry to hear that.,pcmasterrace,2026-01-05 16:59:31,1
Intel,nxugpyf,Didn't Asus Boards have an issue with setting SOC voltage too high when Expo was on? My understanding was that you had to set it manually to under 1.3 [preferably 1.2] to be safe.,pcmasterrace,2026-01-05 17:00:24,1
Intel,nxuh2ch,Strange name for a cat,pcmasterrace,2026-01-05 17:02:01,1
Intel,nxuhfc8,At first I thought the cat was named 9800x3D....,pcmasterrace,2026-01-05 17:03:43,1
Intel,nxuhroj,This cat is probably worth more than the PC itself. It deserves to be kept warm. 😹,pcmasterrace,2026-01-05 17:05:20,1
Intel,nxuhvbe,"This sucks. Sorry you have to deal with it.    Also, CPUs can die from overtightening the bolts. Be careful not to tighten the bolts too much just in case anyone on here is thinking about screwing their cpu down with the grip of a thousand wrenches",pcmasterrace,2026-01-05 17:05:47,1
Intel,nxujy1j,"I was expecting to see Asrock somewhere in this post, shocked to be wrong.",pcmasterrace,2026-01-05 17:15:30,1
Intel,nxukjo0,Op give this one a read.    https://www.techpowerup.com/forums/threads/silicon-lottery-the-winner-gets-a-new-cpu.344586/,pcmasterrace,2026-01-05 17:18:17,1
Intel,nxukvp6,Thought this was just an Intel thing ?,pcmasterrace,2026-01-05 17:19:51,1
Intel,nxul8f9,That's a weird name for a cat 😢,pcmasterrace,2026-01-05 17:21:29,1
Intel,nxulpck,I just build it a new rig with a 9800x3D I hope I’m safe with my Gigabyte mobo 😭,pcmasterrace,2026-01-05 17:23:40,1
Intel,nxun7ln,That's a weird name for your cat.,pcmasterrace,2026-01-05 17:30:37,1
Intel,nxunlcw,Gigabyte board safe? I didn’t update bios for g757,pcmasterrace,2026-01-05 17:32:22,1
Intel,nxup7kv,I thought it was the cat's name...,pcmasterrace,2026-01-05 17:39:47,1
Intel,nxusvjh,"I’m so sorry for your loss, this is devastating. Is there a warranty?",pcmasterrace,2026-01-05 17:56:23,1
Intel,nxusxa9,"Processor can die like that. ,* new fear unlocked",pcmasterrace,2026-01-05 17:56:36,1
Intel,nxusykj,My cat also likes to step on top of the computer.,pcmasterrace,2026-01-05 17:56:46,1
Intel,nxutzf2,"I’m horrified, it’s like impending doom for me",pcmasterrace,2026-01-05 18:01:19,1
Intel,nxuu2co,"Okay is there something super wrong with the X870e chipset? These problems seem universal, mine freezes all the time.",pcmasterrace,2026-01-05 18:01:41,1
Intel,nxuv8qu,how did it die? i have the same Chip i use a B650e motherboard though. its not when mine dies that i can buy a new Chip or anything thats not an option,pcmasterrace,2026-01-05 18:07:01,1
Intel,nxuvq7p,What PSU?,pcmasterrace,2026-01-05 18:09:13,1
Intel,nxux8ki,As long as the cat isn't named 9800x3D then i don't care,pcmasterrace,2026-01-05 18:15:59,1
Intel,nxuz157,That's reason 9800x3d is biggest problem and I have see many news about 9800x3d issue,pcmasterrace,2026-01-05 18:23:58,1
Intel,nxv009c,"I am seeing that AS rock thing, but asus 870 as well. And always 870 versions",pcmasterrace,2026-01-05 18:28:18,1
Intel,nxv1udr,Rip.,pcmasterrace,2026-01-05 18:36:33,1
Intel,nxv2fcq,I’d be more upset all the work that would go into removing that mobo,pcmasterrace,2026-01-05 18:39:09,1
Intel,nxv2fjp,RIP,pcmasterrace,2026-01-05 18:39:10,1
Intel,nxv3whm,My 7800X3D still rocking and rolling 3 years later! Though my 7900XTX barely made it a year. AMD was a fun test run but once this one dies I’m going back intel with my 5080.,pcmasterrace,2026-01-05 18:45:44,1
Intel,nxv4870,"9800x3D, what a weird name for a cat.  Why did you leave the corpse there?",pcmasterrace,2026-01-05 18:47:11,1
Intel,nxv4zdu,"Sorry for your loss.     But PLEASE for the love of almighty, take that damn plastic off the MOBO lol",pcmasterrace,2026-01-05 18:50:33,1
Intel,nxv6hrm,I blame the cat,pcmasterrace,2026-01-05 18:57:11,1
Intel,nxv6nbv,Strange name for your cat but my condolences nevertheless.,pcmasterrace,2026-01-05 18:57:52,1
Intel,nxv7e7c,Even my Ryzen 5 7600 on a MSI B650 tomahawk WiFi just randomly died (no visual damage) within 1 year and 4 months of use since day 1  What's happening with these dead Ryzen chips from 7000/9000 series? What causes them to die and how do we prevent such a thing,pcmasterrace,2026-01-05 19:01:13,1
Intel,nxv7f15,Bro can't cook his chicken anymore. But seriously RIP.,pcmasterrace,2026-01-05 19:01:19,1
Intel,nxv7iut,Dope PC BTW,pcmasterrace,2026-01-05 19:01:48,1
Intel,nxv7k7i,My condolences.   But that was a terrible name for that cat.,pcmasterrace,2026-01-05 19:01:57,1
Intel,nxv7p6d,What was your average gaming max temperature? Did you use PBO? 200mhz max override?   Negative curve optimizer?  If there is a voltage to manually dial in lower I'm considering it. Maybe SOC?,pcmasterrace,2026-01-05 19:02:35,1
Intel,nxv8d8x,I have 9800x3d and asrock b650m-c board w/7800xt. Not any problems yet after about a year. I run tarkov around 200fps on 1440. Easily 10 hour sessions sometimes. Fingers crossed I guess,pcmasterrace,2026-01-05 19:05:36,1
Intel,nxv8i49,".... you didn't call your cat 9800x3D, right?",pcmasterrace,2026-01-05 19:06:13,1
Intel,nxv9gqx,Weird name for a cat but sorry for your loss 🥀,pcmasterrace,2026-01-05 19:10:34,1
Intel,nxvaho2,"Was thinking about upgrading from my 7800x3d. Thanks for posting this, I'll hold out until the next generation.. hopefully they figure out what's going on. Sounds like a manufacturing issue though.",pcmasterrace,2026-01-05 19:15:13,1
Intel,nxvd5lj,"Got a Ryzen 9 5950x still going strong, almost coming up to 5 years now.",pcmasterrace,2026-01-05 19:27:25,1
Intel,nxvdm2u,What settings were you running on BIOS? Were you undervolting? Did you touch the PBO settings? What about Scalar?,pcmasterrace,2026-01-05 19:29:30,1
Intel,nxvdnru,I have the same mobo and cpu combo. Now I’m worried 👀,pcmasterrace,2026-01-05 19:29:44,1
Intel,nxvdu4u,Hareless cat to prevent fur getting in the PC - smart,pcmasterrace,2026-01-05 19:30:32,1
Intel,nxvfjt3,What's up with all the 3D CPU's dying?  Makes me nervous having one.,pcmasterrace,2026-01-05 19:38:31,1
Intel,nxvfyh5,"Was it from the early release batches? I bought mine day 1 and after 9months of usage it died in October I think.  Edit: I had an ASUS board back then, always up to date BIOS. Never messed with CPU related OC.",pcmasterrace,2026-01-05 19:40:24,1
Intel,nxvijbm,Who the fuck names their cat after a CPU?  /s  Sorry for your loss!,pcmasterrace,2026-01-05 19:52:17,1
Intel,nxvjhmt,How can we prevent this situation from happening?,pcmasterrace,2026-01-05 19:56:39,1
Intel,nxvjm2o,How many accounts do you have…?,pcmasterrace,2026-01-05 19:57:12,1
Intel,nxvjnzw,https://preview.redd.it/e4bkb74b6lbg1.jpeg?width=1320&format=pjpg&auto=webp&s=9fe1fe062997aefb62d154987c2e8d1e0594244b,pcmasterrace,2026-01-05 19:57:26,1
Intel,nxvk60p,"Shit.  Glad I stopped at the 7950X3D then.  I thought having shitty GPU drivers was bad after I bought my 7800XT, but damn, an actual hardware failure really blows.",pcmasterrace,2026-01-05 19:59:45,1
Intel,nxvk8kl,Just my tiny 2 cents. I have :  9950x Asus x870e-e  64gb 6000mts 7900xtx 360aio for CPU  Updated bios every time a new one came out. Ive had it for almost 18months and haven't had anything kill it's self.,pcmasterrace,2026-01-05 20:00:05,1
Intel,nxvl7p9,"Odd question, did you ever monitor your vrm temps?",pcmasterrace,2026-01-05 20:04:40,1
Intel,nxvm5x4,"I’m about to upgrade from a 7800x3d to a 9800x3d but now I’m scared, is this common? I have an MSI mobo",pcmasterrace,2026-01-05 20:09:08,1
Intel,nxvp505,Worst case scenario email nexxus games and they might want to buy your cpu to i vestigate the flaw,pcmasterrace,2026-01-05 20:23:03,1
Intel,nxvq8xf,I wonder why 😂😂,pcmasterrace,2026-01-05 20:28:16,1
Intel,nxvqqjs,What bios version did you have?,pcmasterrace,2026-01-05 20:30:34,1
Intel,nxvucft,On an ASUS board? Shiiiiiit. I'm on MSI but this does not bode well at all...,pcmasterrace,2026-01-05 20:47:28,1
Intel,nxvukuw,sorry to hear this. Unrelated: But what an absolutely beautiful creature your cat is,pcmasterrace,2026-01-05 20:48:35,1
Intel,nxvumw5,Did you manually set the VSOC in bios?,pcmasterrace,2026-01-05 20:48:51,1
Intel,nxvuxoj,https://preview.redd.it/m1wfz3aqflbg1.jpeg?width=1080&format=pjpg&auto=webp&s=3258e23547871950a03ebd87d06ca90f89104f30,pcmasterrace,2026-01-05 20:50:14,1
Intel,nxvxskl,What was the bios version?,pcmasterrace,2026-01-05 21:03:33,1
Intel,nxvzynj,Was it oced? Voltages?,pcmasterrace,2026-01-05 21:13:39,1
Intel,nxw1pya,"Everyone’s talking about asrock. My gpu is by them, is that bad? It had the best review for a 9070XT I found",pcmasterrace,2026-01-05 21:21:46,1
Intel,nxw23jy,Yeah this over-volting thing has been an issue with AMD CPUs for many years. I remember even with Zen 2 launch there were the same mitigations at the beginning.,pcmasterrace,2026-01-05 21:23:31,1
Intel,nxw55cv,"Opened this post expecting to see Asrock, saw that it was NOT Asrock. I got scared because it might be chip related too,  fortunately it looks like Gigabyte Mobos are unaffected for now..",pcmasterrace,2026-01-05 21:37:32,1
Intel,nxwgq9t,"Did you do any OC or UV?  The ""better"" the board the higher the risk, especially if you set mobo limits in the OC - it is going to fail because mobo has no limits basically.",pcmasterrace,2026-01-05 22:32:34,1
Intel,nxwiu3h,"As the owner of a ASRock B650M Pro RS etc etc mobo with a 9800X3D, I live in daily expectation of this happening to me too.  :(  All the ASRock hate didn't happen until after I built this rig last spring - if I could go back I'd pick a different mobo. Going forward I'll stick with MSI or Gigabyte I guess.  I don't overclock my CPU and I have a Corsair PSU, so I'm keeping my fingers crossed that it survives.  It could be that it's an issue with the 9800X3Ds and ASRock boards are juuuust enough off/different to tip the balance into failure, maybe.",pcmasterrace,2026-01-05 22:43:03,1
Intel,nxwlpn6,look at it from the bright side.     at least you have RAM,pcmasterrace,2026-01-05 22:57:29,1
Intel,nxx24fi,I saw another Asus board kill the X3D CPU the other day as well. Guess it's Asus time for killing the CPUs.,pcmasterrace,2026-01-06 00:23:13,1
Intel,nxx3sd2,"I got mine on release day and it's still solid, ROG mobo.",pcmasterrace,2026-01-06 00:31:53,1
Intel,nxx4pzv,Damn I got the same set up but just air cooled on a astral kinda of shocking,pcmasterrace,2026-01-06 00:36:40,1
Intel,nxx4ud7,God I love sphinx cats,pcmasterrace,2026-01-06 00:37:17,1
Intel,nxx55yw,"mine too just recently, wtf is going on",pcmasterrace,2026-01-06 00:38:57,1
Intel,nxx58f8,Mine died last month after 3 months of use. It was easy to RMA but I'm not confident the replacement will last very long.,pcmasterrace,2026-01-06 00:39:18,1
Intel,nxx5l9k,Your cat blocks the AIOs exhaust fans. That can potentially increase heat and fry the CPU.,pcmasterrace,2026-01-06 00:41:09,1
Intel,nxx81oa,Beerus on the rig,pcmasterrace,2026-01-06 00:54:08,1
Intel,nxx8tlt,"Savage motherboard too. I went with the cheapest I could get in the summer. B850 aorus elite for 279$ cad, open box 48GB(2×24GB) 6000 c30 for 169$ CAD and reused my velocity waterblock from my am4 build. Everything is working great but I still worry about going with the cheapest best options I could get at the time.  I was on the fence between b850. Had buyers remorse I didnt get the x870 or x870e chipset. Fingers crossed.",pcmasterrace,2026-01-06 00:58:16,1
Intel,nxxalpc,I've always been weary of mobos. Started out with Asus on my first build and decided to stick with MSI on a 5600x then 9800x3d,pcmasterrace,2026-01-06 01:07:49,1
Intel,nxxbkvu,Cat hair got into it. /S,pcmasterrace,2026-01-06 01:13:03,1
Intel,nxxc08s,Maybe your puter' sucked all of the kitty's hair off?,pcmasterrace,2026-01-06 01:15:21,1
Intel,nxxcp9c,Been there.,pcmasterrace,2026-01-06 01:19:04,1
Intel,nxxkfmr,"Mine also did this, RMA worked and they sent me a new one.",pcmasterrace,2026-01-06 02:00:45,1
Intel,nxxszlc,What was your vSOC?,pcmasterrace,2026-01-06 02:47:25,1
Intel,nxxujp2,You forgot to remove the plastic on some of the heat syncs.,pcmasterrace,2026-01-06 02:55:57,1
Intel,nxxvdba,Cool cat,pcmasterrace,2026-01-06 03:00:29,1
Intel,nxxwdei,I see your cat blocking all the airvents. That might be part of the issue.,pcmasterrace,2026-01-06 03:06:06,1
Intel,nxxwxcd,That looks like a beauty man,pcmasterrace,2026-01-06 03:09:13,1
Intel,nxxyu5n,"That sucks man. Also, that pc setup looks dope as hell. Got any more pics that show it better?",pcmasterrace,2026-01-06 03:20:12,1
Intel,nxymxks,"$700 USD motherboard killing a X3D chip, what the destiny beholds for me?",pcmasterrace,2026-01-06 05:59:06,1
Intel,nxynv2l,All the lights are still on though,pcmasterrace,2026-01-06 06:06:31,1
Intel,nxyonko,Did you down volt the CPU?,pcmasterrace,2026-01-06 06:12:53,1
Intel,nxyrlyt,"So, this is like one of those cats that sits on old people in the nursing home just before they die? Put your platter drives out and see if it picks one!",pcmasterrace,2026-01-06 06:37:21,1
Intel,nxytsps,This is a very old issue with x3D.  They require very specific mounting pressure or there are going to be issues.  They also like very specific tunings.,pcmasterrace,2026-01-06 06:55:59,1
Intel,nxyvrv5,Weird name for a cat. My condolences.,pcmasterrace,2026-01-06 07:13:16,1
Intel,nxyzjev,RIP   Hope you can get a new one through warranty,pcmasterrace,2026-01-06 07:47:46,1
Intel,nxz3ojw,What kind of case is this?,pcmasterrace,2026-01-06 08:26:21,1
Intel,nxz3qsi,Would you like to smash motherboard with a hammer as a revenge ?,pcmasterrace,2026-01-06 08:26:55,1
Intel,nxz3rd2,ARC Raiders has something completely fucked about it since December. it causes non-stop transient spikes to the CPU.,pcmasterrace,2026-01-06 08:27:05,1
Intel,nxz53b7,"ok, that's rare. Don't normally hear people having issues like that unless they bought a second hand/damaged one.",pcmasterrace,2026-01-06 08:39:57,1
Intel,nxza4tc,I expected it to be an Asrock motherboard because it’s usually there fault,pcmasterrace,2026-01-06 09:28:40,1
Intel,nxzgr37,I am scared now,pcmasterrace,2026-01-06 10:30:18,1
Intel,nxzlrlv,I’m seeing quite a few people say the same. I wonder what it is?  I have the 7800x3d and luckily for me it has lasted 18months so far.,pcmasterrace,2026-01-06 11:14:02,1
Intel,nxzyf64,"I'm curious; what drivers are you running on your gpu, OP?  I'm still on 566.36 and was wondering if their drivers are finally safe stable enough to pair with 4090's.",pcmasterrace,2026-01-06 12:48:51,1
Intel,ny0kwe5,I just wanted to stop by and say that I think your tower is very beautiful.,pcmasterrace,2026-01-06 14:55:17,1
Intel,ny170kg,This is the 10th post about the ryzen 7 9800x3d dying,pcmasterrace,2026-01-06 16:39:05,1
Intel,ny1sbg9,The cat sleeping on your CPU radiator exhaust prb doesn’t help keep it cool.,pcmasterrace,2026-01-06 18:15:04,1
Intel,ny2er8u,bro. my cause was overheat i guess. these are exactly the symptoms of my i7 6700k. which it showed for 2 weeks before finally not working again.  and i guess the screen is completelt frozen and u can only force shut down via case button right.  in my case it was 8 years of no thermal paste change,pcmasterrace,2026-01-06 19:56:20,1
Intel,ny2mc40,"It got wet... 🤦🏻‍♂️  For way less than the price of the circuit, I have about 10 fans cooling and without noise 🤣",pcmasterrace,2026-01-06 20:31:35,1
Intel,ny2wp4l,We have bigger problems than a dead CPU.... there's some kind of ball sack with ears on top of the PC people! Wake up!,pcmasterrace,2026-01-06 21:19:03,1
Intel,nxu2j8l,"Seems this is happening alot now, well ""alot"" from what is posted",pcmasterrace,2026-01-05 15:54:27,1
Intel,nxu0gyk,Daaaamm sexy pc :0  I hope you get your coy RMA’d and that your replacement cpu can be returned for full refund haha :P   Or that AMD fully refunds your fault cpu 👀,pcmasterrace,2026-01-05 15:44:47,1
Intel,nxu89ua,Poor Kitty has to freeze now,pcmasterrace,2026-01-05 16:21:16,1
Intel,nxutxy5,"I'm sorry for your loss. However, it does somehow make me feel less bad about still rocking my 7700k on a daily basis.",pcmasterrace,2026-01-05 18:01:08,1
Intel,nxv2kjb,"It’s because AMD CPUs are trash.  High specs, but low quality control.",pcmasterrace,2026-01-05 18:39:48,1
Intel,nxvy9tt,"Why is it socially acceptable for people to allow their pets to lay all over their electronics? Before anyone gets butthurt, we have 2 dogs and I dust my consoles and PC every so often with a rechargeable air duster.",pcmasterrace,2026-01-05 21:05:48,1
Intel,nxwdl8o,Cat hair got it,pcmasterrace,2026-01-05 22:17:05,1
Intel,nxx4zwo,Was regretting getting the 14900k but now I’m kind of thankful,pcmasterrace,2026-01-06 00:38:05,1
Intel,nxubtr8,"....9800x3D isn't the cat's name, right?",pcmasterrace,2026-01-05 16:37:45,1
Intel,nxvg4qb,But you still have the ugly cat right? As long as the cat's ok...,pcmasterrace,2026-01-05 19:41:12,0
Intel,nxvueob,https://preview.redd.it/55z7l5s9flbg1.jpeg?width=3024&format=pjpg&auto=webp&s=1efcaff5fc0d7b502b5c0bb01690661f981e3611,pcmasterrace,2026-01-05 20:47:45,0
Intel,nxu67te,"Sorry for your loss, bro. On related thoughts I am so glad I went with a non X3D cpu with my build last year.",pcmasterrace,2026-01-05 16:11:38,0
Intel,nxu76un,"Were you overclocking it too far that eventually killed itself? That's the only thing I can think of since that the only issue is cpu, unless you didn't do a bios update if there a voltage issue for your board?",pcmasterrace,2026-01-05 16:16:11,0
Intel,nxu9nid,Were you rawdogging it with default speeds/voltage in the BIOS or did you have an over or undervolt with PBO or something?,pcmasterrace,2026-01-05 16:27:42,0
Intel,nxulez9,"These issues was happening before x3d chips was a thing, main board was asus 7000 series. SOC voltage was the main culprit i bet it’s the same again. Either bios updates sorted this or you can change yourself in the bios. Sad to see its still happening",pcmasterrace,2026-01-05 17:22:21,0
Intel,nxw63lj,I assume that photo documents the CAT scan you did on your system?,pcmasterrace,2026-01-05 21:41:53,0
Intel,nxxsgsm,"My high ass thought that ""rest room"" meant you have a completely separate room just to rest in and take a break from stuff in.",pcmasterrace,2026-01-06 02:44:34,0
Intel,nxyq0w4,Seem like user error,pcmasterrace,2026-01-06 06:24:02,0
Intel,nxyq3bs,Asus mobo too yeah no wonder it died,pcmasterrace,2026-01-06 06:24:36,0
Intel,nxyxzyb,Kekw. PC Mustard Race.,pcmasterrace,2026-01-06 07:33:33,0
Intel,nxzxmyk,F in chat boys  ![gif](giphy|fxsAcheaMi1PnxPJaf|downsized),pcmasterrace,2026-01-06 12:43:40,0
Intel,nxu3113,Asus is the new ASROCK.,pcmasterrace,2026-01-05 15:56:45,-7
Intel,nxu9z0z,Isn’t asus=asrock?,pcmasterrace,2026-01-05 16:29:11,-3
Intel,nxuau45,My 9950X3D died this way around a month ago. Scorch marks in the middle of the cpu.,pcmasterrace,2026-01-05 16:33:11,349
Intel,nxu1nez,"I didn’t notice any, but I only looked at it for like three seconds. Was too busy dealing with water and trying to swap the new one in the laziest/ fastest way possible.",pcmasterrace,2026-01-05 15:50:20,76
Intel,nxw0mnc,https://preview.redd.it/myjpso8gklbg1.jpeg?width=3024&format=pjpg&auto=webp&s=952f97f6ed595c62a772c8473c48bfdab0f4fd9d  Nope,pcmasterrace,2026-01-05 21:16:44,9
Intel,nxtwsog,"This would have been my question. How does the CPU look?   Also, I wonder what finally made OP decide it is the CPU? Not questioning, just curious.",pcmasterrace,2026-01-05 15:27:15,22
Intel,nxveq9t,I’m so glad I didn’t buy into the Asrock nova hype lol,pcmasterrace,2026-01-05 19:34:42,1
Intel,ny3jmpj,Cats are heaters,pcmasterrace,2026-01-06 23:07:52,1
Intel,nxuzq9l,"Don't worry, the replacement oven element will bring the internal temp to 165°F.",pcmasterrace,2026-01-05 18:27:04,10
Intel,nxvodty,hmm warm chimken,pcmasterrace,2026-01-05 20:19:31,1
Intel,nxuo9gl,Dying over here lol,pcmasterrace,2026-01-05 17:35:29,36
Intel,nxyh0z8,One of those rare actual lols.,pcmasterrace,2026-01-06 05:14:47,2
Intel,ny2nnv7,Now that was funny.,pcmasterrace,2026-01-06 20:37:50,1
Intel,nxu1t6o,I read that it in Takemura's voice,pcmasterrace,2026-01-05 15:51:05,133
Intel,nxtvidq,Honestly so do I,pcmasterrace,2026-01-05 15:21:02,57
Intel,nxwawez,I'm playing this again since 2020 (highly recommend it now 9/10 game) and I feel like it's following me now lol,pcmasterrace,2026-01-05 22:04:11,5
Intel,nxukxy3,Thought you were talking about the keyboard and was wondering where OP posted it lol,pcmasterrace,2026-01-05 17:20:08,1
Intel,nxuq3fi,They don’t call him the God of Destruction for nothing,pcmasterrace,2026-01-05 17:43:49,34
Intel,nxy5cz6,Hakaizen 9800x3D,pcmasterrace,2026-01-06 03:58:11,1
Intel,nxu23wo,"I forget the number, but it was the before the latest one to release. Also the poly lighting thing on the side of the motherboard that reads “ROG” on mine is messed up. The “G” flickers red and green, and the “O” hardly ever lights up now.",pcmasterrace,2026-01-05 15:52:27,29
Intel,nxu4nmd,Never was. There have been plenty reports on other boards too,pcmasterrace,2026-01-05 16:04:21,136
Intel,nxubbd2,I've RMA'd two different cpu's from two different Asus X870 TUF's.,pcmasterrace,2026-01-05 16:35:23,4
Intel,nxu504m,"Yeah its happening to every brand, asrock is just more common",pcmasterrace,2026-01-05 16:05:58,5
Intel,nxubus9,Asus owns AsRock so it's always been Asus lol.,pcmasterrace,2026-01-05 16:37:52,6
Intel,nxwggq3,Just yesterday I saw some table or spreadsheet here where it was in % chance your and chip is going to die. In brand top 2 were asrock and asus and then it was the chipset and it was the x870 x850 and b650. The higher the chipset the higher the chance of failure. Brand was like 75% asrock 13% asus 5% msi 2% GB.,pcmasterrace,2026-01-05 22:31:14,1
Intel,nxx0ame,never was you monkey,pcmasterrace,2026-01-06 00:13:41,1
Intel,nxx8gjd,"It wasn't, but somehow they became the prime target of the news whenever a chip burned up.",pcmasterrace,2026-01-06 00:56:19,1
Intel,ny0dunr,"When I've learned about the failures it was mostly pointing at ASUS as they also did some shady business practices like stopped accepting warranty claims because of the CPU failures and allegedly CPU's killing their motherboards according to them and other issues, basically ASUS became the least trustworthy brand when it comes to motherboards at that time, seems like with the 9800X3D's it's ASRock's turn now.",pcmasterrace,2026-01-06 14:18:16,1
Intel,nxusvlr,"He doesn't live in our world,  he ""just went and bought a new CPU"".",pcmasterrace,2026-01-05 17:56:24,45
Intel,nxvy0r3,G.Skill is goated in my book for RMAs. I had to RMA my RAM in December and it took 3 week from approval to receiving my new set.,pcmasterrace,2026-01-05 21:04:38,1
Intel,nxvigu2,At least these failures seem pretty definitive. Unlike random instability that frustrates the shit out of you while trying to diagnose it for a month. Still sucks either way.,pcmasterrace,2026-01-05 19:51:58,16
Intel,nxw2pjc,"This isn't new.......it was reported for 7800X3D too during launch, when AM5 first launched. And similar thing happened during Zen 2 era as well.  Looks like it just keeps happening every time.",pcmasterrace,2026-01-05 21:26:21,7
Intel,nxu21yj,*A hairless cat sits atop a tower...*,pcmasterrace,2026-01-05 15:52:12,36
Intel,nxvhzom,"I got 2 cats 1 newfounland I dust once a week fml... Lol I get quite a bit but I have a 149ks a nice 6.2 boost at 28c,  At 35c I dust like Danny tanner...",pcmasterrace,2026-01-05 19:49:47,2
Intel,nxuevvy,"Yes, it's because it's the most popular gaming CPU, [Mindfactory.de](http://Mindfactory.de) has an RMA rate of 0.65% on 38.100 units sold.   247 dead 9800X3Ds sounds like a lot when you don't take into consideration the amount of CPUs sold.  But I would also avoid Asrock and Asus as a plague with the 9800X3D, most of the dead CPU reports come from their boards, try getting something from Gigabyte/MSI.  At least in Asrock's case they tried to fix it and everything is as transparent as it can be. Asus? Crickets from them and their sub-reddit is littered with dead CPUs. Shit-tier handling from them, as expected.",pcmasterrace,2026-01-05 16:51:52,24
Intel,nxty8cy,"I fully expected Asrock too  Honestly Im getting increasingly concerned about these 9800x3d chips, in my completely uneducated and irrelevant opinion, I bet they have a design flaw leading to them being incredibly sensitive to power surges  Asrocks motherboards may be crappy and show this issue sooner than others but I bet all motherboards CAN cause this, just depends how long it takes for them to age  I think as time goes on we will see a lot more and eventually uncover something similar to those Intel chips that oxidised over time",pcmasterrace,2026-01-05 15:34:09,39
Intel,nxvj1u5,Asrock did start from ASUS  *taps head*,pcmasterrace,2026-01-05 19:54:38,0
Intel,nxu0npc,Asrock is asus,pcmasterrace,2026-01-05 15:45:40,-14
Intel,nxuip76,Same with my 9700x.,pcmasterrace,2026-01-05 17:09:40,5
Intel,nxwyci4,I will say it was strange how quietly and nicely AMD approved my 7950X3D claim under warranty after dieing. It's almost like they knew....,pcmasterrace,2026-01-06 00:03:31,1
Intel,nxwn227,This is the real question,pcmasterrace,2026-01-05 23:04:22,1
Intel,nxvt6vv,"I get that's a concern but that has to be so rare as to be hardly worth considering.  Like ""tripped and spilled some shitty liquid"" has got to happen like 10000 times for each cat that can't make it to the litter box.",pcmasterrace,2026-01-05 20:42:04,3
Intel,nxuve2i,"He’s not up there too often, and if he’s sick the door to the room gets closed so I don’t have to worry about accidents",pcmasterrace,2026-01-05 18:07:41,5
Intel,nxv0hot,"Cats may expel grossness from some hole if they are sick, but they are fairly less likely to do it where they sleep.",pcmasterrace,2026-01-05 18:30:30,2
Intel,nxu4bqh,"Sent the request to AMD, just got an email back this morning asking for photos of the CPU. So hopefully",pcmasterrace,2026-01-05 16:02:48,12
Intel,nxvmeuf,"Lol, if you look up just about any component you have you will see a flood of defects and problems.   At the end of the day, it's a drop in the bucket for the amount of 9800X3Ds produced.  First thing people do when they have a problem is complain and troubleshoot online.  They don't post that everything is working as intended.",pcmasterrace,2026-01-05 20:10:18,4
Intel,nxudqgh,"No it's not, you see a very small percentage of every 9800x3d on the planet that dies on reddit.  No where close to the amount of processors that are out there in the wild.",pcmasterrace,2026-01-05 16:46:33,9
Intel,ny0a4n1,"Curious about the same, I assume undervolt aka Curve Optimizer only would be pretty safe meanwhile Stock and PBO +200Mhz puts the 9800X3D at risk.",pcmasterrace,2026-01-06 13:57:52,1
Intel,nxvji17,U ain't wrong about that .... Wheres elitegroup when u need them...🕵,pcmasterrace,2026-01-05 19:56:42,1
Intel,nxu25y3,I had a 9950x3d get cooked with a 00 code on an asrock mobo. My new CPU has been rock solid since the RMA back in March 2025. Also switched boards to a MSI carbon,pcmasterrace,2026-01-05 15:52:43,12
Intel,nxxhclr,"pretty much with 9xxxX3D, the PBO boost sometimes push the SOC voltage too aggressively that sometimes fried the cpu.",pcmasterrace,2026-01-06 01:44:05,1
Intel,nxujj4g,"I had a 7950x, and now a 9950x3d without a problem.",pcmasterrace,2026-01-05 17:13:35,1
Intel,nxubsta,Its all boards. But yea i have no idea. Im sticking with my 7900x for now. Though i have noticed a weird issue.  If pc been off for a week. It has trouble turning on. But eventually posts and works fine.,pcmasterrace,2026-01-05 16:37:37,2
Intel,nxvr7js,"I will say, arc raiders is the only game that makes my 9800x3d hit 95c (thermal throttle).  It’s not a paste issue, have pulled it off and confirmed there’s full coverage and even wiped and reapplied paste with the same outcome.  I play arma games very frequently (typically Arma runs your computer hard and hot and is mostly CPU bound) and I never get above 60c.  Pop arc open for an hour or two and see my max temp has hit 95c multiple times throughout the play session.  Google claims it’s normal for these CPUs to operate right at the 95c limit if you have PBO on, but it’s pretty odd to me no other game does that and in fact they act just as you’d expect every other cpu to act.",pcmasterrace,2026-01-05 20:32:46,3
Intel,nxudqzn,"Coulda been karma, ratted 5 people doing matriarch on blue gate right before the cpu died  https://preview.redd.it/3jtg0zi98kbg1.png?width=2560&format=png&auto=webp&s=80b845f49eb65ae306434f9923441840d9c048bf",pcmasterrace,2026-01-05 16:46:37,4
Intel,nxu8x4y,"Mostly with the X3D ones and on the newer AM5 motherboards, as far as I know. It also seems to happen more on ASRock boards, but other manufacturers' products also have the problem to some extent, as in this case.",pcmasterrace,2026-01-05 16:24:17,2
Intel,nxvu4ng,"I did, the chipset drivers also.",pcmasterrace,2026-01-05 20:46:27,2
Intel,nxu1ds2,The cpu temps were good. Motherboard read out of the temp hardly ever went over 51c. And HwInfo temp on a single core maxed out around 70. The build has 2 360 radiators so it was fine.,pcmasterrace,2026-01-05 15:49:05,10
Intel,nxysvio,Idiot,pcmasterrace,2026-01-06 06:48:10,0
Intel,nxu4gdi,1000w Corsair. Little over two years old.,pcmasterrace,2026-01-05 16:03:24,1
Intel,nxw24y8,The cat was actually gifted to us lol.,pcmasterrace,2026-01-05 21:23:42,1
Intel,nxvr512,Gigabyte is 1% of deaths claims on the web. ASUS (mine) is 15% circa.... You can sleep well.,pcmasterrace,2026-01-05 20:32:27,1
Intel,nxv56uu,"Yeah, that’s why I was hoping it was the cpu rather than the mobo. More expensive and time consuming to replace.",pcmasterrace,2026-01-05 18:51:27,2
Intel,nxvj3l4,"I believe so, I got it within the week of release.",pcmasterrace,2026-01-05 19:54:52,1
Intel,nxvmcn7,?,pcmasterrace,2026-01-05 20:10:01,2
Intel,nxvm9w6,Good thing a $600 motherboard is killing things lol,pcmasterrace,2026-01-05 20:09:40,2
Intel,nxvn497,"Any chance you can link this post, the description at the bottom sounds like the same situation.",pcmasterrace,2026-01-05 20:13:36,2
Intel,nxvmr7d,Someone else just commented a photo of the same mobo and cpu combo I have with the same error code. Maybe MSI will be fine.,pcmasterrace,2026-01-05 20:11:55,3
Intel,nxvxryt,"No, only cause I don’t know what that is at all",pcmasterrace,2026-01-05 21:03:28,2
Intel,nxyqyrx,"AMD ""quality""",pcmasterrace,2026-01-06 06:31:57,1
Intel,nxyqsip,"Nonsense, the CPU will throttle if it overheats.",pcmasterrace,2026-01-06 06:30:28,2
Intel,nxxfu4z,I have two 360 rads. Temps were fine.,pcmasterrace,2026-01-06 01:35:59,2
Intel,nxx9kyc,https://preview.redd.it/tlj27ojpombg1.jpeg?width=1816&format=pjpg&auto=webp&s=92f25fceec72daff2d8469f68c672ea555d21a81  My sphinx Mowgli,pcmasterrace,2026-01-06 01:02:21,0
Intel,nxxwuhv,"Motherboards, especially expensive ones like his would shutdown the pc before it overheats to that point.",pcmasterrace,2026-01-06 03:08:46,1
Intel,ny0i081,The picture is before it died lol. The lights and fans power up with no cpu activity any way. As long as the controller is receiving power they come on.,pcmasterrace,2026-01-06 14:40:29,1
Intel,ny1uo42,Build has two 360 rads. Wasn’t an issue,pcmasterrace,2026-01-06 18:25:26,1
Intel,ny2n1ff,What are you going on about?,pcmasterrace,2026-01-06 20:34:52,1
Intel,nxu2bze,"I would love if they refunded it, I do not feel like draining and taking off the cpu block again lol.",pcmasterrace,2026-01-05 15:53:30,1
Intel,nxuxk70,I got a 6700k myself 😂,pcmasterrace,2026-01-05 18:17:26,2
Intel,nxxgqjt,"I guess you missed the part where the entire Intel 13 and 14th Gen lineup are capable of melting themselves regardless of motherboard brand, and then Intel proceeded to release a new generation that was literally slower and worse value than the previous gen.  Take off the blue sunglasses",pcmasterrace,2026-01-06 01:40:49,1
Intel,nxw0h18,I wouldn’t let my dog lay on my pc….,pcmasterrace,2026-01-05 21:16:01,2
Intel,nxxvtzy,"Lol, a CPU with known faults...nice joke",pcmasterrace,2026-01-06 03:03:05,1
Intel,nxudbh4,"The board has some ai Overclocking thing. Idk lol. The cpu would sit at 5,252 while under gaming load according to HwInfo. The new one actually sits a bit higher at 5,320.",pcmasterrace,2026-01-05 16:44:38,1
Intel,nxz7zq3,Is it bad to use the default settings?,pcmasterrace,2026-01-06 09:07:47,1
Intel,nxu38ty,Why are you commenting on anything other than tummy aches.,pcmasterrace,2026-01-05 15:57:46,13
Intel,nxub0yi,no,pcmasterrace,2026-01-05 16:34:03,4
Intel,nxubb50,motherboard?,pcmasterrace,2026-01-05 16:35:21,104
Intel,nxv48sv,This shit happens with the new generation too? Bios problem again?,pcmasterrace,2026-01-05 18:47:16,1
Intel,nxwcd6r,"That sucks, sorry to hear. I wonder why that happens? And how long have you been using it?  I have a 9950X3D for around a month now and love it so far. Using an AiO cooler and it runs super cool at all times.",pcmasterrace,2026-01-05 22:11:10,1
Intel,nxwdewc,What motherboard?,pcmasterrace,2026-01-05 22:16:14,1
Intel,nxuep35,But I thought this was only an intel problem?,pcmasterrace,2026-01-05 16:50:59,-4
Intel,nxuf9qb,"You should have inspected it properly, scorched marks on the CPU = scorched pins on the motherboard = another dead CPU.  If the failure was internal, then the Mobo should be fine to be used with another CPU, but I wouldn't risk it when I could just RMA it.",pcmasterrace,2026-01-05 16:53:40,104
Intel,nxw1tvy,Pretty clean to me which is surprising. The CPUs that have failed recently had scorch marks on them. CPU failures are generally pretty rare too.,pcmasterrace,2026-01-05 21:22:17,10
Intel,ny4euwk,wtf bro ☠️,pcmasterrace,2026-01-07 01:52:11,1
Intel,nxtzxjt,Probably the 00 code on the motherboard …  And the fact that swapping the cpu fixed the issue after attempting other troubleshooting steps …  I don’t see how that leads to anything besides cpu issues.   Unless you didn’t read the post 😜 heh,pcmasterrace,2026-01-05 15:42:15,80
Intel,nxy3i6i,Are you OP's CPU?,pcmasterrace,2026-01-06 03:47:07,12
Intel,nxuz1pi,![gif](giphy|3R30XMeokyyLzZcu3l|downsized),pcmasterrace,2026-01-05 18:24:02,18
Intel,nxuq8fk,Hey another mech keyboard enthusiast in the wild!,pcmasterrace,2026-01-05 17:44:27,3
Intel,nxueich,"Is this after replacing the CPU? If multiple components have died or degraded, it could be the power supply doing something funky.",pcmasterrace,2026-01-05 16:50:07,15
Intel,nxu7vkf,"They represent an outsized percentage. It's not JUST them, but it's more them than others. Level1tech on youtube has been covering it for quite a while now, he talks about how they're over represented in the failures. Not by a huge amount, but enough to make you pause on using their boards.",pcmasterrace,2026-01-05 16:19:25,72
Intel,nxua3w8,"Asrock is a known issue.  Other boards are within the margin of error, whereas the asrock boards are well above margin of error.    All products can have issues, it’s when they start exceeding that margin that it’s a problem.",pcmasterrace,2026-01-05 16:29:48,11
Intel,nxu7nmd,"So, how do some AsRock boards kill 2 or even 3 AMD CPUs from different batches?",pcmasterrace,2026-01-05 16:18:22,12
Intel,nxu4w5s,I mean when does it become an AMD problem then. At first the blame seemed too one sided on AsRock now this? Isn’t it a bit sus?,pcmasterrace,2026-01-05 16:05:26,10
Intel,nxuxwv2,"fr thats what i was thinking, something like this would force me to downgrade or wait for a replacement.",pcmasterrace,2026-01-05 18:19:00,19
Intel,nxu39y8,Yes because all Hair got sucked into the PC,pcmasterrace,2026-01-05 15:57:55,58
Intel,nxu3ckw,r/whoosh,pcmasterrace,2026-01-05 15:58:15,12
Intel,nxvj4mx,Hey my 6.2 14900ks is a beast at 28c with 300 fans and an industrial A\C unit blowing on demand cold air next gen ill have to just use the pure ice from the caves in the Arctic....,pcmasterrace,2026-01-05 19:55:00,2
Intel,nxukpao,"I've been buying ASUS boards for almost 20 years, but the way they've handled this plus the Armoury Crate dogshit: It's safe to safe to say Im absolutely done with their boards. Never again.",pcmasterrace,2026-01-05 17:19:01,8
Intel,nxw3395,"Here I am with an ASUS board, fuck...",pcmasterrace,2026-01-05 21:28:06,4
Intel,nxwq6zf,I'm coming back to desktop after years of only using laptops and ended up getting the MSI Tomahawk X870E w/ my 9800X3D. Would you consider that a decent motherboard?,pcmasterrace,2026-01-05 23:20:33,1
Intel,nxu287g,I read somewhere that 3D V-cache by design is extremely weak to power surges (or even voltage surges? I'm not smart enough nor is my memory good enough to remember). I'm not sure if it'll ever be fixed or if motherboard manufacturers simply have to be maniacally careful with how much power the CPU gets at every instant,pcmasterrace,2026-01-05 15:53:01,19
Intel,nxtznhf,Looks like 7800x3d it is,pcmasterrace,2026-01-05 15:40:55,9
Intel,nxukvv0,Margin of error is fine for most boards except asrock. With intel we had above margin of error across the board. Some will always fail the question is if its normal failure rate vs ford 6.4 failure rates.,pcmasterrace,2026-01-05 17:19:52,2
Intel,nxutl6s,Would be very curious if undervolting would help address this at all for these cpus.,pcmasterrace,2026-01-05 17:59:33,1
Intel,nxvdmtr,How would you go about preventing this?,pcmasterrace,2026-01-05 19:29:36,1
Intel,nxu1nol,no it is NOT,pcmasterrace,2026-01-05 15:50:22,9
Intel,nxw25li,"That's good at least, you're already more attentive than most cat/PC owners lol",pcmasterrace,2026-01-05 21:23:48,3
Intel,nxu6a50,May need to RMA your mobo too. But take lots of pics before you send it in if you do.,pcmasterrace,2026-01-05 16:11:57,6
Intel,nxu4z6w,Best of luck bro 🤞,pcmasterrace,2026-01-05 16:05:50,1
Intel,nxvz21n,"If it helps, I was able to RMA a 7900x a few months back that I was having significant stability issues with. Just had to tell them everything I tried and they didn't give me any trouble.",pcmasterrace,2026-01-05 21:09:28,1
Intel,nxya603,Thats true. But this also looks very concerning.  https://videocardz.com/newz/cafe-owner-reports-15-dead-ryzen-7-9800x3d-cpus-out-of-150  Also on ASUS boards.,pcmasterrace,2026-01-06 04:28:20,0
Intel,nxucz36,"In a weird way, I think I lucked out by being a cheapskate and getting a 7800X3D tray CPU instead of a 9800X3D lol.",pcmasterrace,2026-01-05 16:43:01,1
Intel,nxvocef,"Fans and RGB come on but no screen or mouse and keyboard? That's just memory training more than likely, and isn't an issue.",pcmasterrace,2026-01-05 20:19:20,1
Intel,nxxx7w0,"95C? Never seen mine go higher than 60C in game, and usually less.",pcmasterrace,2026-01-06 03:10:54,1
Intel,nxuedqb,"Not sure if they all had similar cases, but the same game has been recently popping up frying systems. Glad it wasn’t your GPU",pcmasterrace,2026-01-05 16:49:31,1
Intel,nxu9hen,I got a asus b650-a prime ax 2 in my machine. Had the pc since late June and its been fine. Hopefully that stays the case,pcmasterrace,2026-01-05 16:26:54,3
Intel,nxu1rf3,And even then those CPUs can run at max temp no problem for hours.,pcmasterrace,2026-01-05 15:50:51,9
Intel,nxvukcl,"Then it's likely some early manufacturing fault that AMD never admitted publicly. I asked them multiple times what was the fault, they never disclosed it. Received my replacement in matter of days, no questions asked.",pcmasterrace,2026-01-05 20:48:31,2
Intel,nxvnep4,Yeah I gotchu,pcmasterrace,2026-01-05 20:14:58,1
Intel,nxvo1yz,The Link was removed bro.. I’ll DM you,pcmasterrace,2026-01-05 20:17:59,1
Intel,nxvogj1,Nvm I can’t dm you  https://preview.redd.it/4ev53u7balbg1.jpeg?width=1320&format=pjpg&auto=webp&s=2584a4698ec85220dc1fc12a83b2f9960cced99a,pcmasterrace,2026-01-05 20:19:53,1
Intel,nxvmx41,Sorry for your loss OP. Hope you manage to get it RMA’d,pcmasterrace,2026-01-05 20:12:41,1
Intel,ny0w24g,well i had 2 intels die before that,pcmasterrace,2026-01-06 15:48:58,1
Intel,ny0svd1,Agree but throttles to where? The barrier can still be there.,pcmasterrace,2026-01-06 15:34:08,1
Intel,nxu2r7e,Mood lol.   I am also hardlined and I had to troubleshoot why my 4TB nvme drive wasn’t working and man what giant PITA… lol (my GPU block covers my m.2 bay XD)  Hoping for the best for you! :D,pcmasterrace,2026-01-05 15:55:28,2
Intel,nxxh7ld,Capable of and doing it are entirely different things.  There are vastly more reports/posts/etc of AMD chips nuking themselves than intel.  Take your own advice and take the rose colored shades off,pcmasterrace,2026-01-06 01:43:21,1
Intel,ny0nqzr,Not a joke. But okay fanboy.,pcmasterrace,2026-01-06 15:09:31,1
Intel,nxuyver,"Can you post the ss of those ai OC settings in bios, i wonder what voltages where you running and did it use scalar and aggressive tuning.",pcmasterrace,2026-01-05 18:23:17,1
Intel,nxvo3uz,So you used the AI overclocking feature?,pcmasterrace,2026-01-05 20:18:14,1
Intel,nxu6xgd,Lol ☠️,pcmasterrace,2026-01-05 16:14:58,1
Intel,nxudga8,ASRock x870e nova WiFi.  https://preview.redd.it/j0x2ksqz7kbg1.jpeg?width=3024&format=pjpg&auto=webp&s=ea44f8e21972c7ef92ebac54a35f86608bb38a74,pcmasterrace,2026-01-05 16:45:14,235
Intel,nxxqwqz,I knew the answer b4 I scrolled down,pcmasterrace,2026-01-06 02:36:03,3
Intel,nxug439,"ASRock and Asus have a propensity to crank the VSOC too high, leading to CPUs going poof.",pcmasterrace,2026-01-05 16:57:36,40
Intel,nxvgkm3,You are referring to something completely different… intel had chips actively oxidizing ruining the cpu. This is a motherboard over volting. If you were joking it’s really not even a good attempt either,pcmasterrace,2026-01-05 19:43:15,1
Intel,nxuka9w,what does rma mean,pcmasterrace,2026-01-05 17:17:04,4
Intel,ny12f6b,Same with my 7950X3D that suddenly died. Appears completely pristine on the outside but multiple motherboards report the CPU as being the issue. Swapping to a new CPU solved the problem.,pcmasterrace,2026-01-06 16:18:05,1
Intel,nxu1vzg,"Oh I read the post. It seems to have gone in one eye and out the other. I'm going to be completely honest. I was on the phone with my mother while reading, and she had called right after I had taken a rip of the weed pen, so I wasn't expecting her. And she rambles. And my mind got a little scrambled up. Pobody's perfect. I'm leaving the post up because I'm not afraid to own my dumb moments. Lol",pcmasterrace,2026-01-05 15:51:27,7
Intel,nxyws46,"Even with a funky PSU it couldn't fry a CPU, the VRMS would have failed way before any electrical damage from a faulty PSU could have reached the actual Processor. It's still a good idea to check the PSU regardless since it might be causing the other issues.",pcmasterrace,2026-01-06 07:22:21,2
Intel,nxuakm1,"They do because the only good source for any documentation on that is the Asrock subreddit that only accepts failures on Asrock boards in their statistics.  Yes, numbers are higher but truth is that we simply have no data on any other board manufacturer.   There are 337 well documented cases with bios versions, bios settings, model etc etc. which is more than all dead intel CPUs and burned 12VHPWR reports in the usual hardware subreddits combined. And still, no media really cares about it or does deeper investigations.",pcmasterrace,2026-01-05 16:31:58,29
Intel,nxu8mov,No one knows. But no one is really investigating this either.,pcmasterrace,2026-01-05 16:22:56,4
Intel,nxw8fa7,"Because it's probably the chip that has issues. I'm not anti AMD, I just bought an ASRock 9070 xt. But I think we should call the baby ugly on this one. Better than Nvidia burning up $3000 gpus.",pcmasterrace,2026-01-05 21:52:35,1
Intel,nxu6irj,"Media says it’s an Asrock issue because the statistics in the Asrock subreddit are lots of Asrock boards.  They just leave out that they only accept Asrock Boards in their statistics and there are now 337 reports of people who spent like 10-20 minutes filling out a form (with bios version, settings etc).   AMD said it’s not their fault, so most media sees no to investigate further. AMD blames Asrock and Asrock doesn‘t seem to know what the problem is.   There are no questions on why AMD is still providing chipsets or why they haven’t solved that yet. Because says it’s not their fault.",pcmasterrace,2026-01-05 16:13:03,7
Intel,nxu7nf3,An AMD problem? Acknowledged on this sub? You have got to be smoking weed!,pcmasterrace,2026-01-05 16:18:21,5
Intel,nxull0j,"Me too, Asus user for 15ish years...right now I'm selling my Flow Z13 and won't get anything from them again, unless there is no other vendor around for a product type I'm interested in (like a special handheld or a specific type of monitor).",pcmasterrace,2026-01-05 17:23:07,6
Intel,nxx588f,> I've been buying ASUS boards for almost 20 years  Same. Went Gigabyte this time and don't regret it based on the way Asus CS has been handling folks,pcmasterrace,2026-01-06 00:39:16,1
Intel,nxw95zs,"Make sure you're on the latest bios version and check your Vsoc in HWinfo64, since ASUS likes to raises voltages on their motherboards. You should check this under different loads/benchmarks/stresstests.  In HWinfo you should check:  * CPU VDDCR\_SOC Voltage (SVI3 TFN) - this is the value reported by the CPU sensors, should be 1.2 V by default or close to it -- mine is 1.202V * CPU VCORE SoC - is the voltage reported by the Mobo, on mine is at most 1.26V -- this may be innacurate, that's why it's best to look at the SVI3 TFN data  If it's 1.3V and over it... that's bad.  You're most likely be fine and even if something happens, you're under warranty. For example, here in EU you get 3 years on the 9800X3D... If nothing happens in the first year and you don't fuck with the settins after and you fry it, 99.99% that CPU life will outlast it's usefulness.",pcmasterrace,2026-01-05 21:55:59,4
Intel,nxwsqen,"Looks solid, it's one of MSI's premium motherboards...Good chipset + good VRMs + good cooling on them + good ports and connectivity + debug screen & easy GPU release (won't buy a GPU without these 2 ever again). Haven't seen a lot of reports of MSI mobos with fried 9800X3Ds...  Just be sure to be on the latest bios version and doublecheck your core voltage to be safe.",pcmasterrace,2026-01-05 23:34:00,1
Intel,nxu84pb,"Power surges aren't the issue or shouldn't be an issue. As long as there's a PSU that's handling current overflow, this should be excluded (or at least as long as there's a decent PSU handling it). I have read of at least one case where the person also had an UPS between their PC and the power outlet. This is a design flaw that's above or beyond this design aspect. Or it should be!",pcmasterrace,2026-01-05 16:20:36,21
Intel,nxyhs6f,Wow if that’s the case then to maybe add a layer of protection is to undervolt the cpu?,pcmasterrace,2026-01-06 05:20:13,1
Intel,nxu5wel,Had a 7800X3D die after two years a couple of weeks ago (MSI motherboard - brother’s PC). Luckily it has a 3 year warranty. AMD sent me a brand new one.,pcmasterrace,2026-01-05 16:10:09,11
Intel,nxulg5c,King of cpus,pcmasterrace,2026-01-05 17:22:30,1
Intel,nxusloj,"With Intel, it wasn't a motherboard design flaw but rather a CPU flaw coupled with idiotic motherboard manufacturers who overclocked, overvolted and removed power limits on their boards. Usually, a flaw is something that escapes QC and in this case, Intel had QC issues while motherboard vendors were simply retarded.",pcmasterrace,2026-01-05 17:55:10,5
Intel,nxu47qj,"It is, they are sister companies",pcmasterrace,2026-01-05 16:02:17,-7
Intel,nxvf2b5,Yeah I wouldn't stick any other components I cared about in that board..,pcmasterrace,2026-01-05 19:36:14,1
Intel,nxuj93f,Yea. I wanted a 9950x3d but so many failure rates,pcmasterrace,2026-01-05 17:12:17,2
Intel,nxvq7c9,Yea correct. It randomly does that once in awhile.,pcmasterrace,2026-01-05 20:28:04,1
Intel,nxyl1de,"Same here, 59 on cpu, 60 on gpu. Never ever higher. All my settings are at epic/high except for two.",pcmasterrace,2026-01-06 05:44:28,2
Intel,nxvcyl4,"not even hours, years. Running your 9800X3D at 90c while not optimal is perfectly within spec and its expected to not degrade/break running at those temps for a LONG time, otherwise they would lower the max operating temp to keep people from frying their CPUs within 2-5 years because thats bad for your business if your products are known to have a short lifespan in reasonable scenarios.  You'd see major outrage if simply doing something like 3D renders, video encoding, or code/game compilation over a year or two broke your CPU because the temps got high but stayed within spec.",pcmasterrace,2026-01-05 19:26:31,2
Intel,nxvubh7,Yes,pcmasterrace,2026-01-05 20:47:21,2
Intel,nxuepl8,You'll get that on them big jobs (assrock MB),pcmasterrace,2026-01-05 16:51:03,218
Intel,nxumoeb,asrocks were reported to have such issues.,pcmasterrace,2026-01-05 17:28:08,15
Intel,nxun6yr,"Ahh, good old ASRock. Yep, this is a known issue. Glad I avoided them when I built my rig earlier this year. How long did it take from the time you built your system to finally see this problem?",pcmasterrace,2026-01-05 17:30:32,10
Intel,nxw1ddu,"would be intresting to see a ct scan of these dead cpu-s what died inside, internal vrm or what",pcmasterrace,2026-01-05 21:20:10,1
Intel,nxwgmx3,![gif](giphy|11FiDF2fuOujPG),pcmasterrace,2026-01-05 22:32:05,1
Intel,nxx7tvw,"\*nervously glances at his ASRock X870E Nova WiFi\*  I've been keeping BIOS up to date, which is not something I normally worry about, hoping it's something that's dealt with in an update. Might be futile; IDK.",pcmasterrace,2026-01-06 00:52:59,1
Intel,nxyt3ft,"That’s a real smoking gun type picture, wonder what those melted pads are for.",pcmasterrace,2026-01-06 06:50:01,1
Intel,ny25sna,Wow the cpu became so hot it turned blue💀,pcmasterrace,2026-01-06 19:15:22,1
Intel,ny2oycb,My asus crosshair hero 670e also fried my cpu,pcmasterrace,2026-01-06 20:43:48,1
Intel,nxup9sb,\>asrock,pcmasterrace,2026-01-05 17:40:03,-3
Intel,nxujqfa,"The failures aren’t vsoc related. High vsoc can lead to accelerated memory controller degradation, which did actually happen before it was capped at 1.3v in BIOS when people were running insane 1.4-1.5v vsoc, but won’t straight up scorch the cpu like we’ve seen. My speculation is it’s probably related to some kind of VRM flaw in some affected boards but no one (publicly) knows root cause still.",pcmasterrace,2026-01-05 17:14:32,16
Intel,nxugdbx,That is not good at all and sounds like a warranty nightmare.,pcmasterrace,2026-01-05 16:58:46,6
Intel,nxuibhf,Wasn’t the high VSOC issue resolved with the newest bios updates? I haven’t touched my VSOC settings and it stays locked at 1.25v,pcmasterrace,2026-01-05 17:07:53,3
Intel,nxukx18,"RMA = Return Merchandise Authorisation, mostly means send it to the merchant for the warranty, but can also mean send it to the merchant if you are within the return period, so you'd get your money back.",pcmasterrace,2026-01-05 17:20:01,22
Intel,nxul27l,Sending it back to the manufacturer (Return Merchandise),pcmasterrace,2026-01-05 17:20:41,3
Intel,ny16at8,unlucky,pcmasterrace,2026-01-06 16:35:52,2
Intel,nxukiqk,"I never delete my weed pen comments, I just apologize when I’m wrong. 🤣",pcmasterrace,2026-01-05 17:18:10,4
Intel,nxuq2h9,"Because generally there has been a huge AMD boots licking among the community, so saying negative things about AMD chips don't really generate clicks. Spamming ""OMG intel bad"", on the other hand...  People need to wake up, before AMD also turn into shit. (They kind of already are, with how overpriced their X3D chips are, and how they are randomly being fried for no reason whatsoever).",pcmasterrace,2026-01-05 17:43:42,18
Intel,nxudn9c,"Isn't Gamers Nexus conducting an investigation with some confirmed killer boards. A part of me hopes it turns out to not be ASRock's fault as they have by far the best value boards, but we can't tell anything until further evidence is obtained.",pcmasterrace,2026-01-05 16:46:08,7
Intel,nxuc44k,"Gamers Nexus conducted a thorough investigation on AsRock boards but their search was inconclusive. Because they said that the one this is for sure the chips are being fried but there isn’t any trace as to what fry’s them.   Further discussion should be made about the extent of other MBs burning these chips, and maybe it can come down to a AMD forced statement, it cannot all be coincidence, the only one underlying similarity are X3D chips.",pcmasterrace,2026-01-05 16:39:03,5
Intel,nxwulcz,"That sounds reassuring. Yea, I'm using the latest BIOS w/ AGESA PI 1.2.0.3g and so far so good. Here is my CineR23 run w/ the stats. Would also like to know if my bin is good? I'm using only PBO -20 w/ no clock override.  https://preview.redd.it/ihn3bh22ambg1.png?width=1134&format=png&auto=webp&s=7fca085a908f1aabfdd11de642d028c7984aab84  I also got a Corsair Vengeance 6000MHz CL36-36-36-76 @ 1.35v (AXMP1) that I'm thinking of UV'ing to 1.25v or would that be too much and do 1.30v instead? Thanks for the input",pcmasterrace,2026-01-05 23:43:58,1
Intel,nxuygbv,Technically it was a motherboard problem.,pcmasterrace,2026-01-05 18:21:24,1
Intel,nxwa00o,"I don’t know why you’re getting downvoted. They literally make their motherboards with the same manufacturer (Pegatron) in the same factory. They might not be run by the same management anymore, but they definitely operate like sister companies.",pcmasterrace,2026-01-05 21:59:53,0
Intel,nxvsyyz,"Yep that's completely normal. AM5 with DDR5 is very sensitive to memory timings, and has to retrain every now and then.    Only something to worry about if it's doing that every time you boot.  If it's every couple weeks, or after a prolonged shutdown -- it's normal behavior, unfortunately.",pcmasterrace,2026-01-05 20:41:03,1
Intel,nxzydtx,Yeah something is wrong if you hit 95C just playing a game.,pcmasterrace,2026-01-06 12:48:36,1
Intel,nxufbqn,Different issue.  Asrock has had a lot of their MOBOs killing 9800X3Ds.  GamersNexus did a video on it.,pcmasterrace,2026-01-05 16:53:55,163
Intel,nxulfa3,"ASRock is typically good, it's just that their AM5 boards are not. I've seen only 2 ASRock boards fail and both were physical damage that wasn't the board's fault. My last 3 rigs have been ASRock and they've all been rock solid.",pcmasterrace,2026-01-05 17:22:23,5
Intel,nxulbhc,"A guy at my job says that all the damn time, we work in emergency healthcare....",pcmasterrace,2026-01-05 17:21:54,1
Intel,nxushtq,"I rebuilt my PC in April. I started having some random issues in November that caused me to upgrade my PSU just to make sure it wasn’t that. It randomly shutoff during a game of Master and Command and would not reboot the week before my Xmas break. My X870E was running BIOS 3.50. Unfortunately, I ended up replacing with the same motherboard and cpu. I really should have swapped out do a different motherboard, but I had no idea about the past history with ASRock mobo’s. The reviews on microcenter seemed very positive for the motherboard. I really hope it doesn’t happen again. I started the RMA process with ASRock around Xmas time. I hope to see an update from them this week.",pcmasterrace,2026-01-05 17:54:41,4
Intel,nxv8amv,"Well, I've enjoyed the Nova Wifi.. but I'm not running XMP/Auto voltages. Fully tuned, been running well since 9800X3D release day.",pcmasterrace,2026-01-05 19:05:16,1
Intel,nxuiq3p,That's what happens when motherboard manufacturers need to get that 1% extra point in synthetic benchmarks.,pcmasterrace,2026-01-05 17:09:47,17
Intel,nxuv288,Hasnt been for many. Amd amd the mobo manufactures have been good about it from the posts ive seen.,pcmasterrace,2026-01-05 18:06:12,1
Intel,nxux3oz,"Yeah, I remember ASRock trying to pull some shit like asking their customers to contact AMD and claim the CPU was faulty to get a replacement. Not sure what happened to that though. It's 100% on them to cover the damage imo.",pcmasterrace,2026-01-05 18:15:23,1
Intel,nxuj0yn,"1.25 is actually quite high, but not terribly high... try to manually reduce it. I run my 9950X3D with 6000MHz/CL28 memory and 1.1V VSoC without a problem.",pcmasterrace,2026-01-05 17:11:13,11
Intel,nxul1kt,thanks,pcmasterrace,2026-01-05 17:20:37,11
Intel,nxv4jjw,"Yep. I've been saying this for a while. People are overhyping AMD, and it's driving the prices to insane levels",pcmasterrace,2026-01-05 18:48:35,3
Intel,nxwqjce,I lean more towards your point of view for sure. It's getting bad out there.,pcmasterrace,2026-01-05 23:22:22,1
Intel,nxvw0xb,"Bro, just stop.   The X3D chips are better and cheaper than every single Intel chip on the market, the 9800x3D slaps the 295k and 14900k in performance and uses less power while doing it. Literally massively cheaper to buy an AMD chip lol, always has been.  https://youtu.be/x59AsHBKx7A?si=Lo7Q-r9bIiR27SgH  Intel had to re-release the high end 14700k and 14900k as the 275k and 295k with updated microcode that didn't attempt to draw INFINITE WATTAGE from the socket which is what was leading to CPU deaths.   This situation is large in part due to motherboard manufacturers, since the vast majority of chips are affected by Asrock motherboards first.  Intels problems were deep routed and affected the CPU's on every board with that socket because it was Intel's mistake from the ground up.",pcmasterrace,2026-01-05 20:55:16,-1
Intel,ny06egh,"Cope harder.   >AMD held around 80-84% of the DIY CPU market share on Amazon in October 2025, according to various reports.   AMD's Ryzen 7 7800X3D and Ryzen 9 9800X3D frequently claim the #1 and #2 spots, sometimes selling more units individually than all Intel CPUs combined, says [TechPowerUp](https://www.techpowerup.com/342716/amd-ryzen-x3d-cpus-outsell-intels-entire-cpu-lineup-on-amazon-in-october).",pcmasterrace,2026-01-06 13:37:10,0
Intel,nxuf1ex,"Well, I’d argue that it’s at least not only Asrocks fault.  AMD says it’s not their fault and Asrock doesn’t seem to know what it wrong.  Thing is, Asrock gets the chipsets, requirements and settings from AMD. So, Asrock either doesn’t meet some requirements or there is something wrong in the configuration.  Asrock said they’ve worked with AMD on this.  So, AMD is either partially responsible for the problem or is still willingly selling providing chipsets to Asrock despite knowing that they kill their CPUs.",pcmasterrace,2026-01-05 16:52:35,6
Intel,nxun02s,"Yea I think steve said on their end of year video they have even recently bought more CPUs from viewers that have failed. They have started to even buy their complete rig as is, so they can try to recreate exactly what happened without any added variables and they are still scratching their heads about it. From what it sounded like they still haven’t been able to recreate it and they even brought back a dead chip.",pcmasterrace,2026-01-05 17:29:39,3
Intel,nxug7bl,"GN can only do so much as they are a tech youtuber. They know a lot, more than average, but not as much as an engineer from Asrock or Asus themselves.",pcmasterrace,2026-01-05 16:58:01,2
Intel,nxvwi9l,Why would you want a budget board for an ultra high end chip?  Sorry but that's just stupidity.  Just get a higher end B650 Board if you want to save money so much.,pcmasterrace,2026-01-05 20:57:30,1
Intel,ny40e79,"Yeah, their conclusion was ..they don’t know. they couldn’t reproduce any failures..",pcmasterrace,2026-01-07 00:34:33,1
Intel,nxuddti,"And never asked AMD or questioned them.  All they said is ""AMD says it’s not their fault, so believe them"".",pcmasterrace,2026-01-05 16:44:56,2
Intel,nxx0aci,"You core voltage (seen as CPU VDDCR\_SOC Voltage (SVI3 TFN)) is 1.202V looks good, same as mine (I run -30 in PBO).  You can try to uV the ram, but IDK how stable they will be or if you can even uV them at all and boot. Mine are CL30 but run at 1.4V, never tried to mess with them, too lazy...",pcmasterrace,2026-01-06 00:13:39,1
Intel,nxzholb,"You call it a ""problem"" or ""flaw"" when it's something that has escaped QC, generally. When a certain integrated component isn't working the way it's supposed to (think of VRMs not doing their jobs or the pcie slots being 8x instead of 16x). In Intel's case, every motherboard vendor KNEW exactly what they were doing because they all chased that idiotic advertising prowess.",pcmasterrace,2026-01-06 10:38:39,1
Intel,nxvzu9h,Good to know! That's what i thought but i never knew for sure.,pcmasterrace,2026-01-05 21:13:06,2
Intel,nxuk1vp,"They kill all X3D chips, its not JUST the 9800X3D.",pcmasterrace,2026-01-05 17:16:00,145
Intel,nxuuhn5,Go to r/asrock for carnage,pcmasterrace,2026-01-05 18:03:36,5
Intel,nxysh1f,"Which is really a shame, because asrock based on specs has the most diverse line of mobos and probably the best mobos. ECC, ram and m.2 fans, connectors, you name it they have it. I think even all or almost all their mobos have ecc. But it's not worth turning your CPU into scrap.",pcmasterrace,2026-01-06 06:44:44,2
Intel,ny08fp7,"They did so a video on it, and they couldn't find a cause. As did Wendell from Level 1 Techs. He also couldn't find a cause.  It's looking like the issue is massively exaggerated.",pcmasterrace,2026-01-06 13:48:30,1
Intel,nxvdldz,ASRock is the only brand I've had issues with. Besides Colorful they were the only ones to make an mATX X570 motherboard. It already felt cheap when I got it and then eventually the m.2 fan crapped out. Then when I heard their boards were killing X3D CPU's I decided to stay away,pcmasterrace,2026-01-05 19:29:25,6
Intel,nxw67gm,My experience with asrock was great. This sucks for them and especially their customers.,pcmasterrace,2026-01-05 21:42:22,2
Intel,nxv0h9o,"So it took several months before it finally went kaput. Were there any other signs or performance issues, like fans kicking on super loud? One thing I dislike about AMD CPUs is I can't easily view individual core temperatures while booted into the BIOS or Windows (you can with Intel CPUs). I know you can view a global temp in the BIOS and software like Core Temps, but I've always wondered if the average reported temperature could be masking a hot zone that may go undetected.",pcmasterrace,2026-01-05 18:30:26,1
Intel,nxvlrrv,"Yeah it doesn't affect everybody, just a percentage of consumers. If you're still running fine, then it sounds like you won the lottery.",pcmasterrace,2026-01-05 20:07:18,1
Intel,nxukmak,Is it? I thought it just needs to be below 1.3v? I’m going to set it to 1.1v if you say you’re stable. I have the same CPU and ram as you.  The VSOC surge is my greatest fear.,pcmasterrace,2026-01-05 17:18:38,3
Intel,nxvwacf,"AMD is cheaper than Intel at every single level, better in performance and then better in temps and power draw to boot.   No one is overhyping AMD  Intel is just that bad.",pcmasterrace,2026-01-05 20:56:29,4
Intel,nxy4rsj,"The new chips are absolutely not just repackaged 14700Ks and 14900Ks. ""Infinite wattage"" also isn't what was killing them, transient spikes in core voltage were killing them. You're allowed to like AMD and prefer them over Intel, but making things up to slander Intel just screams ignorance",pcmasterrace,2026-01-06 03:54:37,2
Intel,ny0b9g8,Exhibit E.,pcmasterrace,2026-01-06 14:04:09,1
Intel,nxuh8ko,I'm guessing it's some sort of funky transient behaviour from the voltage regulators similar to the power draw problems observed with RTX 3000 series GPUs. It would explain why data logging doesn't reveal it as the duration of such a spike may be smaller than what the sampling rate accounts for. This would also explain why ASRock may be following AMD's guidelines but still be facing problems.,pcmasterrace,2026-01-05 17:02:50,1
Intel,nxvwwlt,"You just re posted misinformation that Asrock spread online to shirl responsibility.  Everyone got the same numbers, 80%+ of deaths only happen on Asrock boards.  You cannot blame AMD's numbers which are safe default values, because then EVERYONE would have the same issues equally.",pcmasterrace,2026-01-05 20:59:21,-2
Intel,nxx16sz,Funny you say that cause I’m currently about to post a question in this sub regarding UV’ing my RAM. I happen to snag a Corsair Vengeance in FB (6000MHz CL36-36-36-76 @ 1.35v). It only has AXMP1 profile available but everything seems to be stable so far (AIDA64 memtest and some gaming sesh). I’m wondering if I could go to BIOS and just manually adjust VDD and VDDQ for UV?,pcmasterrace,2026-01-06 00:18:21,1
Intel,nxup6jj,"They kill all newer AMD chips, not just the X3D ones. Go to r/ASRock and watch the horror.",pcmasterrace,2026-01-05 17:39:38,77
Intel,nxukhik,Oh didn't know about the others.  Majority of the posts I've seen were just 9800X3D but there's probably more of those then the other chips so that'd make sense in terms of being owned by people.,pcmasterrace,2026-01-05 17:18:01,26
Intel,nxw1mib,my 7800x3d died on a b650m pg lightning,pcmasterrace,2026-01-05 21:21:20,2
Intel,nxus4aw,"ASRock put out BIOS updates in May/June to fix those problems.  Of course, you still have to install it.  And to install it, you have to know about it.  And fixing a problem doesn't get nearly as much media coverage as there being a problem.",pcmasterrace,2026-01-05 17:52:59,8
Intel,nxxd585,"Almost entirely just the 9000's though. Out of everything reported I think there were fewer than a handful of 7000X3D, and they weren't even sure if it was related.  I've been fine with a 7800X3D on ASRock B650E PG Riptide for about 2 years.  The issues 9000X3D on primarily B850/870 with a few cases from B650/670  A shame, because I've enjoyed the experience with my board so far compared to MSI and Gigabyte in my other systems.",pcmasterrace,2026-01-06 01:21:29,0
Intel,nxzu6cv,Yeah I've had a couple of their boards for previous builds in the past.,pcmasterrace,2026-01-06 12:19:38,1
Intel,ny0d83z,Hard to tell since supposedly it was only Asrock boards doing it.,pcmasterrace,2026-01-06 14:14:55,1
Intel,nxvboqa,"I would say there were no “performance issues” per se. The random issues I started having was my computer monitor/keyboard/mouse suddenly turning off and my case fans ramping up to 100%. I would have to shut my computer down and it would start like nothing happened. No BSOD or error reported. I started using Claude/AI to help me diagnose issues since I feel like electrical issues are out of my wheelhouse. It had me run memtest86 which reported a PASS status. It also had me install HWiNFO and start logging reports when I was gaming then feeding it the data. It noticed that the 12v to my GPU was out of spec, being below the threshold, which prompted me to purchase and install a new PSU. I did another logging report after the PSU was installed and the 12v was in spec now. I actually had zero issues with my computer for around a month after the new PSU installation until it suddenly failed when my CPU got cooked. Before and after the PSU installation, my CPU never got over 80C with my Noctua NH-D15, so it seemed like an electrical issue rather than a high temps over time issue. Im not sure if the issues I encountered before the new PSU were related to the CPU failing or not, or if it was actually the 12v being out of spec to my 4090. I called microcenter and they said to start a RMA with ASRock but to also mention the CPU damage to see if they would cover that as well. Reading some of the comments here… it sounds like I may need to start the RMA process with AMD directly for the CPU. It’s been quite the ride. I have been building and rebuilding my PC’s for around 20 years now and this is first catastrophic failure I have seen.",pcmasterrace,2026-01-05 19:20:39,2
Intel,nxw1n3d,Most I've heard of were just running XMP and all auto,pcmasterrace,2026-01-05 21:21:24,1
Intel,nxun3z9,"1.25 V is only 0.05 V below the upper limit, so it's quite high. If the system is stable, lowering VSoC makes sense and has no downside. I generally prefer lower voltages when stability allows. Less heat and less strain on the hardware. :)",pcmasterrace,2026-01-05 17:30:09,2
Intel,nxwq300,"Nah man, I upgraded from a 10600k processor. I am partial to I gel and went with a 265kf, but I tried to go with an x3d chip first because it was advertised as such a jump in performance. But it was going to be $450 alone for that processor, plus ddr5 plus a cooler. With Intel I got 2 free games, BF6 and Star wars outlaws, a free 240mm aio MSI MAG core liquid a15. And I only paid around $230 for the processor. All on black Friday, but still AMD wasn't offering anything close to that. And I'm glad I went with it anyway, because after keeping up with, and going more in depth on performance info, if you go 1440p or 4k then the performance gain falls to around 3 to 5 % at 1440p and even lesser in 4k. Intel is destroying productivity tasks too. I personally don't have any use for that, but it's there! So, no Intel doesn't suck at all. I'm very happy with my 265kf and plan to keep it for at least 5 years. With a new chip layout it will take some time for them to work out the kinks and really get into performance boost. I'm tired of seeing the negativity thrown at Intel, it's undeserved.",pcmasterrace,2026-01-05 23:19:59,-4
Intel,ny55czr,"Exactly, dudes getting into cult territory.",pcmasterrace,2026-01-07 04:22:04,1
Intel,nxui4ef,"Possible. Or maybe similar to Intels issues with Raptor lake.  I just think this should be investigated. But unfortunately, both media and the communities are pretty quiet about this topic. I mean they have nothing to lose and in the best case they might get a final fix for this problem.",pcmasterrace,2026-01-05 17:06:58,1
Intel,nxw0yno,">You just re posted misinformation that Asrock spread online to shirl responsibility.  Source for that?  >Everyone got the same numbers, 80%+ of deaths only happen on Asrock boards.  what numbers exactly?",pcmasterrace,2026-01-05 21:18:17,2
Intel,nxx3qf9,"You should be able to disable the xmp and manually set the frequency, timings and voltages from the bios  Succesfully uV-ing them...that's another question...",pcmasterrace,2026-01-06 00:31:36,1
Intel,nxwo585,"Lol literally the first post on there when I opened it up. Jeez, glad I went with MSI instead…",pcmasterrace,2026-01-05 23:09:56,23
Intel,nxww0fy,do 7600xt resist?,pcmasterrace,2026-01-05 23:51:22,1
Intel,nxupdkk,You see the 9800X3D pop up more because thats the most common X3D chip to be paired with an 800-series ASRock board.  Also the 9950X3D came out a few months after the 9800X3D.,pcmasterrace,2026-01-05 17:40:32,22
Intel,nxx1oc7,"And then if you're like me you'll try 15 different usb sticks to try to flash the bios and none of them work, and then give up and leave it out of date and pray your cpu doesnt explode.",pcmasterrace,2026-01-06 00:20:53,2
Intel,ny0e2ti,"Supposedly being the operative word. It's definitely not just ASRock boards and never has been. As per the OP's situation. X3D CPUs are dying in similar ways on all boards, but ASRock were supposedly overrepresented.",pcmasterrace,2026-01-06 14:19:28,2
Intel,nxvrkdt,"Yeah, sounds like you went through a very logical troubleshooting procedure. I have about the same level of experience building rigs (15 years) and that would have thrown me off too. Certainly the case fans ramping up to 100% could have been an early sign, but if it's sporadic or only happened once, it could have been hard to pin down as solely a CPU problem (and no BSOD as you mentioned, which can sometimes indicate CPU issues). Very strange the GPU 12V PSU change seemed to coincide with CPU failing, but I highly doubt the new PSU is the culprit.   When setting up the BIOS, did you check the PBO settings? From a Google search, ""Many ASRock AM5 motherboards (specifically mid-range and high-end models like the B650E, X670E, and B850 Steel Legend) had aggressive ""Precision Boost Overdrive"" (PBO) settings. These pushed excessive amperage (EDC and TDC) to AMD CPUs, which has been linked to premature failures and visible burn marks.""  I think ASRock sent out some BIOS updates the past year to address this. I only learned about this ASRock/AMD issue from here and my Google News feed. Had I built my rig from earlier this year blind, I would have never known this issue to exist at all. I guess I need to give thanks to Chrome for spying on my shopping habits and updating my newsfeed; it probably saved me some hassle.",pcmasterrace,2026-01-05 20:34:27,1
Intel,nxwhh0a,XMP? Are you running an Intel build?,pcmasterrace,2026-01-05 22:36:16,1
Intel,nxusveh,"I mean upper limit in what AMD has deemed safe. It was when it spiked to 1.4-1.5 that it was an issue. New BIOS versions capped it at 1.3 under AMDs guidance.  Nevertheless lowering SoC is still good practice, but I don’t want people thinking they’re going to fry their CPU because it was running at 1.3.  1.25 is thrown around a lot because that’s what Buildzoid had in his ram OC set at.",pcmasterrace,2026-01-05 17:56:22,1
Intel,nxux64l,That’s how you get a system that is stable until strained,pcmasterrace,2026-01-05 18:15:41,1
Intel,nxwt3rw,"Are you dumb?  Comparing a MID RANGE chip to the highest end Gaming CPU's on the market?   Intel sucks, the 265 can't compete with the 7600x3D for price or performance.  That's the actual comparison you should have made  And on top, they are changing the socket for the next gen of CPU's so you won't ever be able to upgrade from that same motherboard.   And new chip layout? The have used the same chip layout since the 12000 series. It's all on the same socket, same chip design.",pcmasterrace,2026-01-05 23:35:59,4
Intel,nxw4lby,"Source for that? Asrock themselves.  They blamed AMD in their press release following the issues.   Every board manufacturer got the same safe parameters for operation, the same voltages and so on.  Are you bot or just playing stupid?",pcmasterrace,2026-01-05 21:34:59,0
Intel,nxx08xd,I had an X870E Taichi Lite sitting on my desk the day the 9800X3D launched.  Thank fuck I couldnt find a case I liked in my budget that could fit an eATX board.  Sent that shit back and got an overpriced Aorus Master instead.,pcmasterrace,2026-01-06 00:13:27,15
Intel,nxx53an,"Been rocking msi since the 1700 came out, I’ll upgrade next month and was already planning on getting a msi mobo.  Can’t go wrong with their stuff, msi is legit one of the best brands out there.",pcmasterrace,2026-01-06 00:38:34,13
Intel,nxx1eod,"Yeah I went with the MSI B850 Gaming Plus myself. I was really hesitant to spend $200+ on a damn motherboard considering that ASRock has options that are $50-$75 cheaper, but I also didn't want to end up with a burned 9800x3D. I was lucky enough to find that MSI B850 on sale for $150 during Black Friday, putting it much closer in price to ASRock's crap.  Most seem to have their x3D cards within two months of using them in an ASRock board, but I'm at almost 3 months with no issues.",pcmasterrace,2026-01-06 00:19:30,5
Intel,ny11zwi,"Unfortunately no mobo seems fully safe from this cause my MSI X870 randomly killed both my 7950X3D and its own cpu slot. I was working on a simple Word document of all things, suddenly my PC just shut down and wouldn't turn back on after a year of working with no issues whatsoever. Had to get a new mobo + cpu, hopefully this Gigabyte board and 9800X3D play nicer together",pcmasterrace,2026-01-06 16:16:08,1
Intel,nxxneza,Is it just the 800 series boards? I've been running a b650e tachi lite for 2 years with a 7800x3d with no issues,pcmasterrace,2026-01-06 02:16:55,1
Intel,nxxh96z,That sounds like hell. Feeling lucky that my MSI mobo came with its own USB stick for this very purpose.,pcmasterrace,2026-01-06 01:43:34,4
Intel,nxxjbm5,Don't they have a Windows App you can update the BIOS from?,pcmasterrace,2026-01-06 01:54:44,2
Intel,nxygxs1,Glad my gigabyte mobo can just get the bios from a folder in my hdd instead of,pcmasterrace,2026-01-06 05:14:09,1
Intel,ny0lun3,Glad to hear then... usually like their boards.,pcmasterrace,2026-01-06 15:00:03,1
Intel,nxxu0uu,"Sorry, the AMD equiv being EXPO but the term can be used interchangeably as AMD supports and reads some of the same profiles",pcmasterrace,2026-01-06 02:53:03,2
Intel,nxveh6l,"True, but 1.25/1.3V might be unnecessary high. It's a conservative value for bad IMCs when enabling EXPO or XMP.",pcmasterrace,2026-01-05 19:33:32,1
Intel,nxvdhyw,Huh? It is stable. I ran memory tests for hours. No problems.,pcmasterrace,2026-01-05 19:28:59,1
Intel,nxx11gy,"No, I'm just more informed than you apparently. A 265kf is not comparable to a 6 core chip lmao. You have nice passion, it's just a bit misdirected. What I said stands.",pcmasterrace,2026-01-06 00:17:35,0
Intel,nxw8q5l,"So, no source. Just an AMD fanboy based on your post history.",pcmasterrace,2026-01-05 21:53:58,1
Intel,ny2ityj,"That's the classic G-hub being garbage issue - try switching to onboard memory mode and set your DPI there using the software first, then disconnect from G-hub completely     The onboard mode definitely lets you change DPI, you just gotta configure it through G-hub first then switch the physical button on the bottom of the mouse",pcmasterrace,2026-01-06 20:15:13,1
Intel,nx5elyy,Already seen card prices here in Canada jump 10-15% since last night. It's insane.   Now is not the time to build.,AMD,2026-01-01 22:05:04,4
Intel,nxlg7un,"Hi, I was wondering if there's any reason to worry if my 7800X3D sometimes spikes for 1-2 seconds to 100°C while gaming and then goes back to the usual temp. I have noticed the highest temp recorded by HWiNFO at one point was 104°, though I never noticed it on the OSD while in a game and never noticed a performance drop. Is there a problem with the cooling or something that could damage my CPU or is it just a sensor bug/issue?",AMD,2026-01-04 08:46:39,3
Intel,nx583ui,"If you're looking to do a PC build...just don't.  If you NEED to do one, do it right now. It's not getting any cheaper this year.",AMD,2026-01-01 21:31:55,5
Intel,nx5jal3,"Here's a dumb question that would be absolutely ridiculed if I dared to create a whole thread around it.  Is there any truth to my hypothesis that Play Station PC ports are likely to be relatively well-optimized for AMD GPUs, given that the Play Station 5 itself is indeed some variant of RDNA? I recently got a 9070xt and have been overall very impressed, but its achilles heel seems to be ray tracing. This isn't exactly surprising to me, as I researched my GPU options to death before buying one, and the general consensus is that Nvidia is stronger in the ray tracing department. But if I were to boot up, say, Ratchet and Clank Rift Apart, a game that supports ray tracing at 60 fps on the base Play Station 5, could I expect it to perform better than a similarly demanding game that wasn't particularly optimized for AMD hardware?  It's largely hypothetical question, as I already own the GPU, am satisfied with the GPU, and of course did my due diligence before buying the GPU so I would know exactly what to expect. But I just haven't really heard much discussion of what, if any, overlap we get optimization-wise for games that were optimized first and foremost for the AMD-based Play Station 5.",AMD,2026-01-01 22:29:53,2
Intel,nx6hz7y,"Thinking about doing a platform upgrade from a 5800X3D to a 9800X3D, how much of an improvement will I see with my RX 7900 XTX?   Obviously I know that DDR5 is priced high now but I think it's only going to get worse if I wait. I live near a Microcenter as well so I'll be doing one of their combos with the CPU, Mobo, and RAM.",AMD,2026-01-02 01:47:36,2
Intel,nx8k7tp,"Early 2025 I was thinking about upgrading to AM5 but there's no way that's happening, I only got a sapphire nitro+ 9060 XT 16gb on Black Friday.  Current setup is Ryzen 3600 on Gigabyte b450 Elite v1, 16gb ram 3200, 9060 XT. My question is, would an upgrade to 5800X make sense? It costs 165 euros where I am and it's the only upgrade I can make that I see. I play games like Helldivers 2, BF6 nowadays. Also I play on 1080p.   Thank you.",AMD,2026-01-02 11:26:25,2
Intel,nxim4kb,"Hi all  I'm about to give my water-cooled 6950xt to my brother as I picked up a 9070xt.  As I've got to.out the og heatsink back on I'd like to.replace the pads ofc. Does anyone know the sizes needed.  I'd also throw a kryonaut grizzly bear pad on the GPU, would this be a 1mm pad?  I'd like to get this right as he's on a 5700XT so it will be a good upgrade for him.  Many thanks.",AMD,2026-01-03 22:08:10,2
Intel,nxaeni9,"I could use some suggestions on upgrading a desktop box my son built for me in 2013. It was used for my graphic arts business (Adobe Suite) and has performed admirably for the last 12 years. It's running Windows 10 and most of the patches will not install. It can't be upgraded to Windows 11, and while I realize that every MS upgrade I ever did in the past caused major mayhem, I probably should go ahead and do it before it quits running altogether.  Below is a list of what he ordered and put in it.   What should I order that will swap out and last me another 5-10 years? I just used this for work and internet. No games.  • MB Gigabyte|GA-970A-UD3P AM3+R   • VGA Sapphire|100365BF4L R9 270 2GD5   • PSU Roswell|RX850-S-B 850W RT (has been replaced)   • CPU AMD|8-Core FX-8350 4.0G 8M R   • SSD 256G|Samsung MZ-7PD256BW R   • MEM 8Gx2|Corsair CMZ16GX3M2A1600C9  It also has a DVD RW Drive and I added a 12TB WD Hard drive   I'm sure most of you folks can look at that list and quickly see what I need to change. I'm thinking CPU, Motherboard and RAM? Thanks for your expertise.",AMD,2026-01-02 17:44:40,1
Intel,nxc2q2i,"I typically wouldn't do a pre-built but considering I can get my hands on this right now if I wanted and the prices of things going up, would this be worth grabbing?  $1,649.99 AMD Ryzen 7 9800X3D, AMD Radeon RX 9070XT 16GB, 32GB DDR5 RGB,2TB NVMe SSD  [https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5](https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5)  Thank for the input in advance!",AMD,2026-01-02 22:33:02,1
Intel,nxeipp1,"Quick sanity check: Am I right to say that there are no new production of X570 boards at the moment, and therefore I should just sit tight with my Asus X470 Stix-F board until the RAMmegeddon eases before moving up to AM5/AM6?",AMD,2026-01-03 07:51:43,1
Intel,nxfttws,"Bonjour, j'ai un vieux pc qui a malheureusement commencé à rendre l'âme fin 2025 et je dois donc me dépêcher d'en racheter un avant que les prix deviennent exorbitants. Je recherche un Pc fixe (si possible prémonté étant donné que je suis peu doué là dessus) pouvant faire tourner les jeux d'aujourd'hui (E33, Dlc Baldur's Gate etc...) et si possible ceux de demain.   J'ai un budget correct (1200 euros max) et je risque pas de faire grand chose à part jouer dessus.    Merci d'avance pour vos avis !",AMD,2026-01-03 14:01:31,1
Intel,nxqoxma,"I just installed my new RX 9070 XT today, replacing my RTX 3060 Ti, and after getting the new drivers set up and the old ones gotten rid of, i'm having an issue of intermittent audio crackling. Is there a know simple fix for this?",AMD,2026-01-05 01:59:57,1
Intel,nxu74dg,what are the best settings for my rx 9070 xt steel legend on adrenalin? should I prioritize lower temps or higher performance? and will the performance between settings be negligible playing in 3440x1440p? I'm currently running the default option under Performance>Tuning,AMD,2026-01-05 16:15:52,1
Intel,nxurns3,"When I'm playing a game, my screen suddenly goes black and I have no way to shut down my PC; I have to restart the power supply. Does anyone have any solutions, please?",AMD,2026-01-05 17:50:56,1
Intel,nxv130z,"Are there plans for chipset refresh for Zen 6 or 7 or there will be only firmware and BIOS  updates  for existing ones? I heard Zen 6 should have better memory controller , with higher 1:1 RAM speed support (perhaps 8000MT/s + ) etc. , but of course still same AM5 socket.",AMD,2026-01-05 18:33:10,1
Intel,nxv5q21,"Hey guys.  Whats the best way to get a smooth 60fps lock on a 120hz display?  I use MSI Afterburner and the adrenaline app, neither felt as smooth as native 60hz.  On nvidia i used the half vsync feature and that worked for me but AMD doesn't have an equivalent option.",AMD,2026-01-05 18:53:49,1
Intel,nxzp3f0,"weird issue as off 2 days ago: RX 7700 XT with 25.12.1 driver on W10 - when powering on the system, the secondary screen (HDMI) is not receiving any signal until the HDMI cable is unplugged and plugged back in. No recent updates installed.",AMD,2026-01-06 11:41:16,1
Intel,ny1f10g,"7800x3d SUSPICIOUSLY LOW TEMPERATURES   I just finished building my computer and tested it in two games, at 2k resolution and the highest settings: The Last of Us Part Two and Battlefield 6. My 7800x3d is showing temperatures below 50 degrees Celsius, even though I'd read on forums that it can get hot. I checked it on the cooling display, HWMonitor and in MSI Afterburner. Is it possible for air cooling to be this efficient, or do I need to configure something in the BIOS to get the processor to run at full performance? Bf6 runs with 180fps and TLOU have around 100fps.  I have rtx 5070 and 32gb ddr5. Cooler: Phantom spirit Evo vision with stock paste.",AMD,2026-01-06 17:15:25,1
Intel,ny4j9i2,"New to AMD and plan to keep the same cpu cooler, I have a NH-D15. I bought this cooler back in 2021-22. Would I need a new mounting bracket to accommodate this change?   I have upgraded to 7 9800X3D, Mobo is a Tuf gaming B650E-E if this information is needed. Any help appreciated!",AMD,2026-01-07 02:15:55,1
Intel,nxdpvtk,"https://i.redd.it/n3zqy5xj72bg1.gif  Apparently my driver stopped updating nearly 2 years ago, and I was never concerned about it. Do I need to do some ridiculous workaround here?",AMD,2026-01-03 04:11:23,0
Intel,nxmzstu,"Hi. HWiNFO is a very reliable monitoring tool, so unless there is a known open issue regarding sensors for your CPU SKU, I'd trust these temp readings.  I don't use a Ryzen 7 7800X3D, but the maximum operating temperature (Tjmax) for the 7800X3D is 89°C. If you're seeing temperatures over 100°C, that's likely a cooling problem that could damage your chip over time. I'd probably check my cooling system and setup if I were you. That said, another 7800X3D user might think differently, so maybe there is nothing to worry about.  Based on my experience, CPU temps over 100 °C usually indicate poor thermal management or inadequate cooling.",AMD,2026-01-04 15:31:19,1
Intel,nxtun1i,"check if memory and/or fabric clocks spike at the same time (max values basically), if they do it is a sensor bug.",AMD,2026-01-05 15:16:47,1
Intel,nx5b3qi,You still can build a PC as long as you know where to get the parts you need at a price you can afford despite the crappy RAM and GPU prices.,AMD,2026-01-01 21:47:15,2
Intel,nx8r8gw,"China stolen Samsung DRAM tech, this year we may have influx of chinese cheap RAM from CXMT to save us",AMD,2026-01-02 12:24:29,1
Intel,nx92ypu,"> how much of an improvement will I see with my RX 7900 XTX?  Up to 50% but this is assuming heavily CPU bottlenecked games (stuff like Battlefield 6, Factorio, Stellaris etc). Less than 15% in a standard AAA grade single player title if you play at 1440p. 0% if you play at 4k.   There's no single metric here as it really depends on a game. If you love 4X games like Stellaris I would upgrade. If you prefer Silent Hill or Alan Wake 2 I wouldn't.",AMD,2026-01-02 13:44:35,2
Intel,nx92d32,"It actually might make sense considering you are playing CPU heavy games at a relatively low res. I would also check if 5700X is available since it's pretty much the same thing as 5800X, except often a bit cheaper.   I see techspot actually tested BF6:  https://www.techspot.com/review/3043-battlefield-6-cpu-benchmark/#2025-10-15-image-png  3600 got 62 fps 1% lows and 86 averages whereas 5800X reached 80 fps 1% lows and 108 averages. So theoretically up to 30% better. Still, in both cases it's playable, fps dropping to 62 probably won't kill you.",AMD,2026-01-02 13:40:59,2
Intel,nxq9rvb,It can vary model to model. I watch search for the sku you purchased and if you can't find it try contacting the manufacture to see if they can tell you. EVGA used to be good about providing this info but it really depends.,AMD,2026-01-05 00:39:44,1
Intel,nxav8dz,"CPU, motherboard and RAM yes. AM5 and DDR5 are the newest.  I don't like windows 11 and I will keep using windows 10 for as long as I can. Unless you absolutely need to upgrade I wouldn't bother.",AMD,2026-01-02 19:00:42,0
Intel,nxev4pq,"According to the review on that site it comes with 5200 MT/s RAM which is not ideal. 6000 matches the memory controller's speed so that's what I would recommend. It's not a big issue, only a small performance difference. Other than that it looks like a solid setup.",AMD,2026-01-03 09:40:03,1
Intel,nxn91rb,"it's not constantly hitting 100 °C so idk what to think of it, hasn't happened today yet and I've been monitoring it so maybe it's nothing",AMD,2026-01-04 16:15:19,1
Intel,nxaknxe,"Thank you for your reply!  Price difference for me between the 5700x and the 5800x is 10 euros so cost isn't something to consider in my case. I'll look up thermals to see if it makes a difference. The benchmark you linked is so helpful for my purposes, kudos!",AMD,2026-01-02 18:12:20,1
Intel,nxrzsmj,"Thanks bud. It's the actual AMD card branded 6950xt, some people mis name it as a founders edition. I'll.try reach out to AMD today.",AMD,2026-01-05 06:52:53,1
Intel,nxngkvz,"It's good that it doesn't happen constantly, but even if those readings occur occasionally or intermittently, it's generally not a good sign.  However, if it hasn't happened again today, and you're under similar or identical workloads to when you had those readings, you probably have nothing to worry about. It could just be a few inaccurate readings.  Continue to monitor your CPU temperatures and, if you notice occasional readings over TJmax again, it's worth checking your current thermal management (thermal paste, contacts, etc.) and cooling setup (fans, AIOs, and/or liquid cooling). Prevention is better than cure.",AMD,2026-01-04 16:50:14,1
Intel,nvc9b2o,"The Radeon RX 9060 XT offers the highest raw frame rates at 1080p, outperforming the competition by roughly 4-5% on average.  The RTX 5060 provides nearly identical performance but adds the advantage of DLSS 4 for superior upscaling and image quality.  While the Intel Arc B580 is the slowest card, its 12 GB of VRAM allows it to handle Ultra settings that cause the 8 GB cards to stutter.  Ultimately, the video recommends the 16 GB version of the RX 9060 XT as the best long-term choice for modern gaming.",AMD,2025-12-22 08:32:19,147
Intel,nvcj3xg,Had to sell the 6600 XT and went for the 9060 XT 16GB to play at 1440p. I’m loving it,AMD,2025-12-22 10:10:46,38
Intel,nvim4sd,I got my 8GB 5060Ti open box excellent BestBuy for $309. It was brand new.,AMD,2025-12-23 09:07:15,4
Intel,nvotif9,Personally out of the 3 I'd pick the 5060. Transformer model is but better than FSR4 at 1080p,AMD,2025-12-24 08:41:46,3
Intel,nvgivw5,"Bought a 9060XT 8GB for 247e (renewed on Amazon, Black Friday stuff) and sold the temporary 4060 non-TI 8GB for 220e on marketplace. Good deal...",AMD,2025-12-23 00:02:30,2
Intel,nvahjmg,Only compares 8GB cards from teams red and green since it’s only considering <$300.,AMD,2025-12-22 00:43:27,8
Intel,nw2aruf,😮🫳🍿,AMD,2025-12-26 18:41:22,1
Intel,nwnskte,"I found an openbox 9060 XT 16GB at Microcenter for $305 and jumped on it. Very impressed so far, especially with undervolting.       I have the Powercolor Reaper model and it is legitimately impressive that they were able to make it that small.",AMD,2025-12-30 02:33:44,1
Intel,nvbruur,"I feel like the HUB guys are going too hard with their VRAM crusade. Why recommend a GPU that's slower now just because it might be faster in the future?   A slight downgrade in render resolution or texture quality is hardly even noticeable, and with looming shortages I feel like most studios are going to start optimizing for lower VRAM further reducing the long term disadvantage of 8GB GPUs.",AMD,2025-12-22 05:48:05,-14
Intel,nvcdgdm,"The real answer, buy a used 2080ti. Usable VRAM, DLSS4, it still is 250W so it can run on most PSUs.  It is the most balanced option if you can't afford a 9060XT 16GB.",AMD,2025-12-22 09:14:12,-15
Intel,nve7fwf,"all of them are power hungry junk, where are good cards?",AMD,2025-12-22 16:44:49,-8
Intel,nvm6i3z,real hero here,AMD,2025-12-23 21:49:44,4
Intel,nwe5oim,"Intel is on the right path, but they need to start using 384-bit memory interfaces on 12GB cards instead of the 192-bit memory interface they used on this card.",AMD,2025-12-28 17:23:33,1
Intel,nvm9ob0,"The 5060 will crush, without less than a 40 percent difference, from dlls alone. Then add frame gen. WOW I can't believe you can get away with this.",AMD,2025-12-23 22:06:17,-11
Intel,nvftrl1,5060 then cuz way better in AI  5% performance cut to gain 2x-3x AI speed,AMD,2025-12-22 21:41:30,-32
Intel,nvfjawi,What processor are you using with 9060 xt?  Is it the same as you were using with 6600 xt?,AMD,2025-12-22 20:46:22,9
Intel,nvjicw9,"Can you tell me how well it runs games at 1440p? Have you played some of the demanding ones like Black Myth Wukong, Stalker 2, etc? Do you play at medium? high? I assume FSR is always on.   And also what's your target FPS? Would really appreciate the feedback, because I have the same card and I'm thinking on switching to 1440p but I don't know what monitor would be good refreshrate-wise",AMD,2025-12-23 13:36:03,1
Intel,nvb3bfo,"Well yeah, the cheapest RX 9060 XT 16GB is [$380](https://pcpartpicker.com/products/video-card/#c=596&sort=price&page=1&P=11811160064,51539607552) and the cheapest RTX 5060 TI 16GB is [$430](https://pcpartpicker.com/products/video-card/#sort=price&P=11811160064,51539607552&c=593). When you're comparing $300 GPUs, you're not going to bring up a GPU that's nearly another hundred dollar.",AMD,2025-12-22 02:55:15,48
Intel,nvcb05n,"That would be a completely new phenomenon if you look at the past. Sure, some (probably indie) studio will optimize their games, but they would have done so already because they care about their customers.  Nothing will change with the current devs or tech, it's just a temporary issue that memory is that expensive. The prices will be lower in 2027, or we'll get used to it and buy more expensive stuff.",AMD,2025-12-22 08:49:20,10
Intel,nvda1uj,"6 ish year old product that is out of warranty from some rando, is not exactly comparable here and definitely not a ""real answer""",AMD,2025-12-22 13:46:25,19
Intel,nvd5m8s,"Dunno why you're being downvoted, the 2080 Ti is still very good value for the price and often has good OC headroom. Beats 5060 in most cases and you're right about 11GB being decent",AMD,2025-12-22 13:18:38,-1
Intel,nvckhpi,The real answer is to stop being cheap and spend money on your hobbies.,AMD,2025-12-22 10:24:05,-27
Intel,nvgc2fa,you tell us,AMD,2025-12-22 23:21:59,1
Intel,nvhwm4g,Why do power requirements matter?   Electricity costs pennies,AMD,2025-12-23 05:17:09,0
Intel,nwno1my,So many think memory bandwidth matters more than it does. The 5060 ti has less bandwidth than the B580. Architecture matters a lot.  More bandwidth would do next to nothing for it.,AMD,2025-12-30 02:09:09,1
Intel,nvr8r0s,AMD cards have upscaling and framegen as well...,AMD,2025-12-24 18:35:16,5
Intel,nvgtg9q,"Can you elaborate what do you mean by ""AI speed""?",AMD,2025-12-23 01:04:59,26
Intel,nvlqflt,No. Dlss and frame gen is much less impactful in terms of performance boost at the low end and the latency is more noticeable. It’s also half the vram.,AMD,2025-12-23 20:25:01,9
Intel,nvm9h9y,WOW 25 so far for the TRUTH. HUB and fooling now.,AMD,2025-12-23 22:05:16,-3
Intel,nvfl8ph,"Yes, same processor, 5600x",AMD,2025-12-22 20:56:36,8
Intel,nvjk7kj,"yeah seriously, here b580 is noticably cheaper for example.",AMD,2025-12-23 13:47:03,4
Intel,nvcwed4,"Yeah, I just wanted to point it out because there’s people like me to whom prices in dollars means nothing (or who don’t read the title) and then waste time watching an irrelevant video (though I skipped to the conclusion so not that much time).🙂",AMD,2025-12-22 12:11:20,-7
Intel,nvdtbxt,"Point being? If a cap blows because it's old any repair shop can fix it, If it's a fan dying you can fix that yourself.  On the other side there's not much the warranty can do for running out of VRAM.",AMD,2025-12-22 15:34:11,-7
Intel,nvcpxpj,"In this economy? It doesn't make any sense to not keep perfectly usable hardware that does the job just fine out of a landfill.   A 2080ti or a 3070 or AMD equal is more than enough performance for most people. Easily, and is way better bang for your buck.",AMD,2025-12-22 11:15:29,10
Intel,nvgxp18,"In a time of global economic uncertainty, it's a horrendous time to overspend on hobbies.",AMD,2025-12-23 01:31:13,4
Intel,nvhzp95,"heat, noise, size, messy cables",AMD,2025-12-23 05:41:18,1
Intel,nvuuu5m,"Correct, but they do not have commercial dlss support. How many games do you not have the ability and ww do?   Thanks for the dowmvotes nvidia. Amd brainwashed.   Just to let you know: you have all been played. Look closer.",AMD,2025-12-25 11:21:47,-2
Intel,nw6d88g,"They're seemingly referring to the speed of running LLMs locally using that GPU, unless I'm also out of the loop. A good sub to look into that stuff would be /r/LocalLLM   I wouldn't recommend doing that with a 5060 but the 16gb version must be the best choice in that price range and would handle the very small models easily and the small ones with a little slowness.",AMD,2025-12-27 11:35:46,1
Intel,nvcxw7p,"News flash: We're always in ""this economy"". I know someone who works at a fucking McDonalds, has a kid, and spends more money on his hobbies than you do.",AMD,2025-12-22 12:23:17,-11
Intel,nvlzofj,nothing global about it,AMD,2025-12-23 21:14:23,-2
Intel,nvipuzv,"So you prefer low power for lower heat and smaller size.   I'm not space conscious, so those things don't matter.   What's with messy cables? The PC sits under the table, so it also doesn't matter how ugly it is.",AMD,2025-12-23 09:44:14,2
Intel,nvw2mmm,"Well actually people have been modding games to put FSR where neither AMD or NVIDIA added official support.   Pretty much every game had amd nvidia and even intel upscaling these days.   In fact, when i still had my 3080ti, i was able to use AMD’s framegen in many games (cyberpunk, dying light, talos principle) because NVIDIA didnt provide any option for 3000 series.   I still bought nvidia because amd doesnt offer any cards at 5080 level, so no brainwashing here. You’re completely uneducated blinded by consumerism",AMD,2025-12-25 16:47:51,3
Intel,nvimsbd,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-23 09:13:38,1
Intel,nveay56,They sound irresponsible.,AMD,2025-12-22 17:02:16,11
Intel,nvfpbcp,"If you spend more on your hobbies than the things you actually need, you might be financially irresponsible",AMD,2025-12-22 21:18:17,6
Intel,nvj2hzu,"the unhinged extra power cables attached to the card itself, it makes everything harder to handle  and heat isn't just about the size of the case, it's noise and comfort of the room  and no, AC doesn't solve that as it's another source of noise that is even worse than the PC itself, only to be used when the weather is too bad to live otherwise",AMD,2025-12-23 11:42:04,0
Intel,nvwscdp,"Oh yeah? And who do you think had went through hundreds of accounts taking about that mod?   Came out DEC 22 2023 I remember the day I went from 22fps in Alan wake 2 to 50 (3070). I posted in this site non stop ban after ban just to try and get you this information. Go on the forums you will pick me out if you look back.   I, in all seriousness, would not be surprised if you know about that mod from ME.   Therefore, I am not blinded. I simply understand that an entire company propped up by manipulation on social media (and GPU) should not exist, and a real competitor would be in their place. There I have just demonstrated that not only am I not blinded, it might be you. Good on you for knowing about that mod (serious).   I will tell you another secret, maybe not meant for you. If you don't mind using dlss and mfg? 5060ti 16gb all day for 1440p or lower. Not only is it hundreds of fps, it has valuable vram on it that will see the card rise in price since it's discontinued.   Hows that for an uneducated prediction?",AMD,2025-12-25 19:18:39,-2
Intel,nvjm540,Gotcha.  What's your preferred card?,AMD,2025-12-23 13:58:18,1
Intel,nvkhap4,from this generation that'd be RTX Pro 4000 Blackwell generation SFF with replaced cooler  personally I own a passive A2000 SFF that replaced my modded 1650 (KalmX was released too late),AMD,2025-12-23 16:39:40,2
Intel,nvkshx0,"Hopefully I am wrong but there is no aftermarket cooler for the RTX Pro 4000 SFF, right ?  https://n3rdware.com/gpu-coolers",AMD,2025-12-23 17:35:10,1
Intel,nvkw08y,"unfortunately no, nothing ready to use that I know of  if you have access to measuring equipment machining a shim isn't even that expensive, haven't seen any publicly available projects for it yet",AMD,2025-12-23 17:52:24,1
Intel,nvkx5mu,"Hmm that sounds tricky.  I’m thinking about getting PCI express extensor and a GPU holder to be able to use it with my MS-A2, keeping the GPU externally til the n3rdware cooler is available.",AMD,2025-12-23 17:57:58,1
Intel,ntamglk,"From r/radeon   * Ray Caching: Only available in Warhammer40K today, more games next year. * Ray Reconstruction: Only available in Black Ops 7 today with more games next year. * AI Frame Gen: Available in Black Ops 7 today with 40 games by end of 2025.",AMD,2025-12-10 14:35:10,104
Intel,ntak2ov,It's almost 2026 and AMD keeps reinstalling the AMD Install Manager that I do not want and have to keep manually uninstalling. Stop this AMD.,AMD,2025-12-10 14:21:39,304
Intel,ntam2kl,What is fsr redstone? and which games use it?,AMD,2025-12-10 14:32:55,85
Intel,ntak8pe,"I got a notification for the update in AMD Adrenalin Edition, but it does not appear in the actual install manager lol",AMD,2025-12-10 14:22:35,46
Intel,ntbp2n9,I just tested the release on four machines (76X&78XT/78X3D&79XTX/97X&9070XT/75F&76XT). Every system still suffers from crashing drivers when hardware-accelerated apps are used (Chrome/Discord/etc.).  Please fix. :),AMD,2025-12-10 17:49:10,23
Intel,ntanibq,so can I open adrenalin on this one with a rdna 2 igpu and rdna3 gpu or is it still broken like the last version,AMD,2025-12-10 14:41:05,20
Intel,ntalx8c,<--- Int8 rdna2 enjoyer,AMD,2025-12-10 14:32:04,84
Intel,ntaottt,did they fix enhanced sync and noise suppression yet,AMD,2025-12-10 14:48:21,35
Intel,ntayll4,Did this driver fix purple visual glitches with the RX 7700 XT? It's a known bug that appeared after the driver 25.4.1,AMD,2025-12-10 15:39:11,14
Intel,ntav0ai,"The ignorance by amd of Rx 7000 users is astounding tbh, but this is 2025 AMD not prior AMD where they would try to appease a larger user base.  It's going to make me rethink my loyalty for future gfx purchases",AMD,2025-12-10 15:20:57,48
Intel,ntam4ms,So we cant test path tracing performance yet on Cyberpunk? Lol,AMD,2025-12-10 14:33:14,31
Intel,ntbddly,"This is a very underwhelming update for RDNA 4 users I get that this technology needs to mature, but they should already be at a point where the implementation is across more wide array of games. My fallen RDNA 2 and RDNA 3 brothers will be remembered. The only reason AMD gpus are still relevant rn is price, nvidia tax is crazy. GG",AMD,2025-12-10 16:51:29,11
Intel,ntbtbc6,"Thanks for nothing again, AMD.  Signed, 7900 XTX user.",AMD,2025-12-10 18:09:31,27
Intel,ntanq1w,So is there any point to installing this if I'm on RDNA2 and don't have any of the issues that they fixed?,AMD,2025-12-10 14:42:16,18
Intel,ntboygm,This is the worst driver amd made 9060xt for me. 2 games instantly crashes. Indiana jones and silent hill 2. With this driver if you enable ray tracing game hangs and give error.i already report bugs in 25.12.1 and same with 25.11.1 and amd does nothing. every ray tracing titles works ok with 25.10.1 driver and this is bad. amd does not listen users anymore. anyone has any crashes happen like me?thanks...,AMD,2025-12-10 17:48:37,8
Intel,ntan1w5,Nothing on Oblivion Remastered crashing? Intermittent application crash or driver timeout on 9000 series when playing Battlefield 6?,AMD,2025-12-10 14:38:30,14
Intel,ntcc5fr,AMD Software still crashes randomly,AMD,2025-12-10 19:40:52,7
Intel,nti2mdh,#AmdNeverAgain Give Fsr4 on rdna3,AMD,2025-12-11 17:48:52,8
Intel,ntba2eq,New update new problems,AMD,2025-12-10 16:35:17,6
Intel,ntayron,"The adrenalin app just auto updated my 9070xt mid game, now my screen is black with no signal output to my monitor but my music is still playing lol. I waited for 10mins then I had to hard restart my computer for it to say the update failed",AMD,2025-12-10 15:40:02,12
Intel,ntamhuj,Pretty dissapointing ngl,AMD,2025-12-10 14:35:22,22
Intel,ntb58wr,Should I get the RTX 5070 ti or 5080 at msrp? I am currently selling my XTX after radio silent news about FSR 4 int 8 on it.,AMD,2025-12-10 16:11:45,22
Intel,ntak0ko,Everything is RDNA 4 exclusive? awesome /s  RIP finewine.,AMD,2025-12-10 14:21:19,58
Intel,ntb9myj,Please add the broken noise suppression to “Known Issues”.,AMD,2025-12-10 16:33:11,5
Intel,ntbbh4c,"If  this driver update keeps crashing my gpu im not leaving 25.9.2 for a while, im also starting to think about selling my gpu and get nvidea, and really black ops 7 why not a real game like cyberpunk i dont want to waste 70 euro for fifa with guns",AMD,2025-12-10 16:42:08,4
Intel,ntbih3y,"Can confirm on my 9060XT that Silent Hill 2 is still crashing and Avatar Frontiers of Pandora currently has a bug when FSR4 is enabled where the entire screen starts flashing like a strobe light, shadowy areas seem to trigger it. This is with both games fully patched & up to date.",AMD,2025-12-10 17:16:44,5
Intel,ntbq2n7,"Let me see - all the new ""Features"" will be available for Cyberpunk 2077 in at least 1 year time and ONLY with RDNA4 ??",AMD,2025-12-10 17:53:59,6
Intel,ntcf8iq,AMD NoiseSuppresion still broken. Since September!,AMD,2025-12-10 19:56:49,5
Intel,ntedkus,"Are pink artifacts fixed on RX 7700 xt, anyone ? It was bugged in 25.11.1 driver last month.",AMD,2025-12-11 02:20:15,5
Intel,ntb4cu0,Where‘s support for 7000 series? Wtf is this dead meat,AMD,2025-12-10 16:07:26,13
Intel,ntaofpr,I’m on a 6000 card is there literally no reason for me to download this,AMD,2025-12-10 14:46:12,20
Intel,ntasl1x,"all i want is to be able to capture clips in my games but for whatever reason amd either doesnt understand im in the game, recognizes the game wrong (battlefield 6 shows as elder scrolls online which i dont even have).",AMD,2025-12-10 15:08:16,3
Intel,ntdy3yt,It's december and still no FSR4 for vulkan.,AMD,2025-12-11 00:46:14,4
Intel,ntf1v9l,25.11.1 was dog water driver timeout city for me I'm just gonna assume this new one will also be the same.,AMD,2025-12-11 04:59:50,4
Intel,ntaql28,Is this worth updating to from 25.11.1  Is it more stable?,AMD,2025-12-10 14:57:42,7
Intel,ntc0jb7,"I had to downgrade to 25.9.1 to have some level of stability, can somebody confirm that the new driver is safe to upgrade to without it messing stuff up?",AMD,2025-12-10 18:44:11,7
Intel,ntawnis,Still no fsr 4 support for rdna3 🙄,AMD,2025-12-10 15:29:24,10
Intel,ntb91tu,"Guys calm down. RDNA3 being moved to maintenance mode is part of their new strategy, no longer ""Fine Wine"", the new approach is Stale Ale. That way their products remain DOA after launch and people won't keep them very long.",AMD,2025-12-10 16:30:16,14
Intel,ntatk4y,idk why I find it so funny that a specific Roblox game got called out in the patch notes,AMD,2025-12-10 15:13:24,3
Intel,ntavzqu,Did they fixed the amd noise supression not turning on?,AMD,2025-12-10 15:26:00,3
Intel,ntbglg2,"Anyone know why Cronos: The New Dawn has been showing [""FSR 4""](https://i.ibb.co/nqW2VMng/Cronos-The-New-Dawn-2025-12-04-02-28.png) for me on a 7900 XT for a few weeks? At first it was 3.1.  I know it can be modded in but this is on a new Windows 11 install and I haven't done any modding.",AMD,2025-12-10 17:07:23,3
Intel,ntbm1c9,"Looks like new chipset drivers, too.",AMD,2025-12-10 17:34:16,3
Intel,ntc41oy,"I thought the application freeze fix might have stopped monster hunter wilds from crashing on me but nope still does it (DXGI_ERROR_DEVICE_REMOVED,)",AMD,2025-12-10 19:00:46,3
Intel,ntcb4ur,/u/amd_vik are you aware of assetq corsa evo vr not working on AMD cards since 25.9.1 ? It displays the left and right eyes out of alignment and therefore fails to show a cohesive single image.,AMD,2025-12-10 19:35:42,3
Intel,ntcgpo5,so no fsr4 support on Vulcan still? this is getting ridiculous,AMD,2025-12-10 20:04:11,3
Intel,ntdhmve,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Thank fucking god.,AMD,2025-12-10 23:10:51,3
Intel,ntdxxbg,Still experiencing 100% gpu usage almost constantly as soon as you boot up BF6 on newer drivers after 25.10.1 and higher temps in general  I'm locking my FPS to 144 but the older drivers is showing better overall temps and less gpu usage for me 🤔  [https://imgur.com/a/ctbMCx7](https://imgur.com/a/ctbMCx7),AMD,2025-12-11 00:45:09,3
Intel,ntf7tyk,BF6 crashing after a few minutes in game with that driver on a 6800xt,AMD,2025-12-11 05:46:22,3
Intel,nth0425,"ever since 25.9.2 still same bug is present even now and now it causes even more problems because ML based FSR and FG fails when it happens: Adrenalin app just shuts down randomly even when idle, no errors, no driver timeout, no dx12 trimeout, just adrenalin itself gets shut down in random times. why wont you guys do something about it finally? Seriously its been so long now... im on 9070XT Steel Legend Dark Edition from ASRock, 80% of your users or more report the same issue FIX IT for the love of GOD. I tried everything hoping its on my side but windows reinstall, DDU and AMD cleanup app and fresh driver install nothing helped its still here",AMD,2025-12-11 14:36:29,3
Intel,ntsip7g,"Both 25.12.1 and 25.11.1 drivers have the same bug on RX 9060 XT. When my screen goes blank and later i wake up screen, i have two mouse cursors on the screen, until i launch some app and then will second, fake cursor disappear.",AMD,2025-12-13 09:49:20,3
Intel,ntaw89a,I hope this fixes the many crashes I've had since the last update...,AMD,2025-12-10 15:27:13,4
Intel,ntc9i8c,"Still enjoying the piss out of the 7900XTX on 25.9.2. It chews through everything I throw at it at the settings I choose, don't care about new driver releases unless a new game I want to play doesn't play well on whatever driver I currently have installed.",AMD,2025-12-10 19:27:35,5
Intel,ntb1a2q,"Even tho I have a 9070xt this is still so underwhelming… We waited 6 months and got basically nothing yet. Sorry for all rdna2, 3 users.  Fun fact: Its been years now that the adrenaline software cant be opened, the only fix ist to press win+p and select only main monitor. Than start it, than swap monitor profile again…   Definetly buying nvidia next time, not supporting this big company anymore, which is behind in every aspect. Image you just want to play alan wake 2 (looks beautiful).",AMD,2025-12-10 15:52:19,9
Intel,ntaqa2o,"ass. no support for rdna2/3, no new features for rdna2/3, rdna4 have only one game that support all of that, redstone framegen almost identical to 3.1 framegen, frame pacing still there.",AMD,2025-12-10 14:56:05,13
Intel,ntazsry,hardware unboxed tested it and frame facing is broken when amd frame gen is on sadly,AMD,2025-12-10 15:45:05,4
Intel,ntaqj5o,So the HDMI crashing issues should be fixed in this version yes?,AMD,2025-12-10 14:57:25,2
Intel,ntayomq,Any news on fixing the gpu vram leak issue on bf6? Sorry I’m lazing not reading the patch notes,AMD,2025-12-10 15:39:36,2
Intel,ntb8vq9,25.12.1 does not even install on my Minipc (780M) + 6650XT eGPU Setup.   I thought I might fix 25.11.X not opening in an eGPU Setup.   Guess I will be running 25.9.2 for another few Months.  God why something always break? I thought it would be better going all AMD for the eGPU setup.,AMD,2025-12-10 16:29:25,2
Intel,ntbdrmk,"Yeah I'll still be with 25.9.1 until the texture flickering is fixed in BF6, also instant replay just didn't work in 25.11.1 for me.",AMD,2025-12-10 16:53:24,2
Intel,ntbt5fb,Will this help Warzone not look so blurry on 7900xt? Game is unplayable,AMD,2025-12-10 18:08:44,2
Intel,ntc1hhd,So there seem to be two links - going through support>picking GPU(9070XT in this case) downloads the 25.21.1 win 11-b.exe file meanwhile going from this release note article it downloads the win11-c.exe . Any difference?,AMD,2025-12-10 18:48:42,2
Intel,ntc4frb,"im using 6800xt the driver page has the win11-a version and article have win11-c version. which one should i choose i really dont know and this ""different builds"" confusing a lot of people",AMD,2025-12-10 19:02:40,2
Intel,ntcb9km,"Genuine question, why all the hype and rush to release this today when it has just two games to showcase the benefits?",AMD,2025-12-10 19:36:22,2
Intel,ntcl3bs,Jesus how long has that Cyberpunk Pathtracing crash been in the known issues. It feels like it's been more then half a year.,AMD,2025-12-10 20:26:14,2
Intel,ntcrk7m,Installed with no issues,AMD,2025-12-10 20:58:08,2
Intel,ntdoxgx,"I can’t play Warzone because I can’t update my bios, there doesn’t seem to be a recent bios update available for my Acer Nitro 5, using Adrenaline. Anyone know if this will help?",AMD,2025-12-10 23:53:00,2
Intel,ntfskqf,Doesn't look like they fixed the bug with Enhanced Sync not working properly with Freesync.,AMD,2025-12-11 09:00:35,2
Intel,ntgkfzg,Any fix planned for 9070 users who cant enable Hardware Lumen on Oblivion Remastered? Game crashes as soon as we turn on the option.,AMD,2025-12-11 13:04:42,2
Intel,nthjjtu,Still no fix for Battlefield 6 for those with AMD 6800M GPU. I swear my next setup is going away from AMD if this is not resolved anytime soon.,AMD,2025-12-11 16:15:50,2
Intel,ntkinfb,u/AMD_Vik      In 2022 AMD made changes to OpenGL Driver. So since 2022 the extension gl\_ati\_fragment\_shader is missing in the driver. It cause problems in older games like Call of Duty 1 from year 2003. Stutter on some maps and broken water rendering because the games can't use the extension anymore.     Our Community is waiting since 3 years for a fix.,AMD,2025-12-12 01:38:24,2
Intel,ntn9tly,in black ops 7 only 25.9.2 driver work better even new 25.12.1 much worse fps drops,AMD,2025-12-12 14:01:19,2
Intel,ntp4ou0,Very unstable for me (7900XTX). Driver keeps crashing even when I'm just watching videos. Reverting to 25.11.1,AMD,2025-12-12 19:39:09,2
Intel,ntq9ysh,i just had to roll back to 25.9.2 because 25.12.1 kept crashing my system with poe2   even GGG straight up said don't use 25.10-25.12,AMD,2025-12-12 23:26:01,2
Intel,ntsszh2,"After observing you guys for a few days xD, 25.12.1 was installed along with new chipset driver on my system.  To my surprise, unlike previous 25.11.1, Adrenalin interface now runs properly with igpu enabled.  I need to test it out with real games, but for now, I've dodged instant roll back.  FYI, If you're using two or more GPUs, including igpu, on a single system with muti-monitor. Download the C package(1.65GB one including rdna1&2+3&4).",AMD,2025-12-13 11:33:00,2
Intel,nttlrcj,"NoUnfortunately, they don't work (( Random crashes remained + In some games, the inability to use frame generation through drivers was added (( Sad ( R5 3600 32gb ram Rx 7700 xt ) Rolled back to 25.9.1 everything works with it",AMD,2025-12-13 15:00:42,2
Intel,ntyj52n,"I had a very weird issue:     My PC would just crash when i did an Windows Defender Scan (only Full Scan, it worked fine with QuickScan or other programms like Malewarebytes) like the power was cut. I did a number of things even rollback the chipset driver but that didn't help. Then i rolled back to 25.9.1 + the newest chipset driver and everything worked fine again.   In case somebody had a similiar issue",AMD,2025-12-14 10:21:51,2
Intel,ntz35tj,"Anyone having problem with AMD overlay with this update? Somehow not showing at any game even if enabled, if I click to different monitor, it shows up. But when I click back to the game it disappear again.",AMD,2025-12-14 13:16:47,2
Intel,ntzh5jv,AMD Wattman settings don't apply for the first time they're set. They have to be changed and applied to a different setting and then to the desired one back and forth to get them to work. I use wattman to set my custom fan curve and it's been glitchy since 25.11.1.,AMD,2025-12-14 14:45:37,2
Intel,nu4w43f,Should I download the new driver version if I have 6800XT? There is nothing in the patch notes about this series... And if yes - why?,AMD,2025-12-15 10:40:37,2
Intel,nu8xc58,"Error code 182 for my AMD Radeon™ 780M integrated GPU on my Ryzen 7 8854HS CPU.  All other driver updates before 25.12.1 worked fine on my Lenovo Legion Slim 5 Gen 9, but this one says my GPU is incompatible, even though AMD's driver download page is providing [this download link](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe) to the installer:  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-8000-series/amd-ryzen-7-8845hs.html",AMD,2025-12-16 00:02:21,2
Intel,nuazy8e,7000 Series web browser glitch? and sound glitch? huh,AMD,2025-12-16 09:03:04,2
Intel,nwzds1t,"So I upgraded on a 6800xt and lost a lot of features like video recording, screenshotting, custom game profiles, and hotkeys. Is that intended?  Crazy to be missing hardware supported features",AMD,2025-12-31 21:37:22,2
Intel,ntb2bag,"Is it safe to update, 25.9.1 is stable for my 9070XT and causes zero crashes with the timeout bullshit from clock speeds going to 3300+ MHz",AMD,2025-12-10 15:57:21,4
Intel,ntapbf5,What does fsr Redstone means ?,AMD,2025-12-10 14:50:59,3
Intel,ntb3ek3,"Is this driver more stable than 25.11.1 it was causing driver time outs and i even got a blue screen. I rolled back to 25.9,2 and now im scared to update to this one lol",AMD,2025-12-10 16:02:45,3
Intel,ntc136m,These comments are all over the place is it better than 25.11.1 or not? 😂😂,AMD,2025-12-10 18:46:48,3
Intel,ntc1b9d,"So in short, still no support for 7000/6000 series, yipee",AMD,2025-12-10 18:47:54,3
Intel,ntfncpt,"Idk what happened but after this update my game crashed and then my PC crashed and when I turned it back on AMD Adrenaline disappeared from my PC, completely gone. What did you do lol.",AMD,2025-12-11 08:07:39,3
Intel,ntavbyt,For Sale: 7900 XTX - $50 OBO  I know these are no longer desirable due to being left in the dust by AMD after only a few months of real support but hopefully it will be at least a good paper weight for someone.,AMD,2025-12-10 15:22:38,3
Intel,ntb0knq,So now driver frame gen is gone? Unless the game specifically supports it? And the overlay as well? Both are completely gone now after the update...,AMD,2025-12-10 15:48:53,2
Intel,ntb1r9y,What about the fixes for the 7900xtx crashing all the time?,AMD,2025-12-10 15:54:38,2
Intel,ntim4nj,"«#AmdNeverAgain” Where’s the Christmas gift in the form of FSR 4 for RDNA 3? In the new 2026 year, it might be time to think about switching to Nvidia.",AMD,2025-12-11 19:23:42,2
Intel,ntbtv4u,"Toujours pas de FSR4 pour les séries 7000 ? C’est mort. Pour ma part, je n’achèterai plus de cartes AMD. Si Nvidia continue à proposer son DLSS pour les anciennes cartes, alors mon choix est fait.",AMD,2025-12-10 18:12:11,2
Intel,ntaitkk,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 14:14:29,1
Intel,ntaqe06,"Downloads ""whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe"" for 9070XT, ""whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe"" for 5700 XT  What does it mean?",AMD,2025-12-10 14:56:40,1
Intel,ntatgqs,It took a while for DLSS 4 to get implemented in a good way on 40 series cards too but a version made it there. Give it time. Now if they can just start prodding developers to incorporate that as well it’ll be worth it. Not enough games yet but here’s hoping!,AMD,2025-12-10 15:12:54,1
Intel,ntau6wp,Any update the in fact that and adrenaline software is not working when second monitor is connected? Especially using iGPU for second monitor ?,AMD,2025-12-10 15:16:42,1
Intel,ntauct1,Omg I think they fixed the LG oled tv reboot bug,AMD,2025-12-10 15:17:33,1
Intel,ntb6asm,Should i install it directly or should I use AMD cleanup utility first?,AMD,2025-12-10 16:16:51,1
Intel,ntb80pv,some one have problem with instaling?,AMD,2025-12-10 16:25:13,1
Intel,ntb8lei,Does this fix the driver timeouts that were happening with Edge? I had to revert the November update because of that problem,AMD,2025-12-10 16:28:00,1
Intel,ntb9m3i,Any fix or still need iGPU disabled for 7000 and 9000 cards?,AMD,2025-12-10 16:33:04,1
Intel,ntb9mdl,The update is still not showing up in install manager,AMD,2025-12-10 16:33:06,1
Intel,ntbb7t2,Honestly this software was the bane of my card for the longest time. Not having it anymore stopped so many weird bugs and crashes.,AMD,2025-12-10 16:40:52,1
Intel,ntbbs61,Does AMD's Instant Replay record still bug out?,AMD,2025-12-10 16:43:39,1
Intel,ntbd79c,Anyone tried the new fsr redstone yet? I am hoping for a big improvement over the old fsr,AMD,2025-12-10 16:50:36,1
Intel,ntbif1z,do you guys remove the old drivers before you install new ones? or just install ontop,AMD,2025-12-10 17:16:28,1
Intel,ntbp8r8,The path tracing crash STILL on Cyberpunk is absolutely wild to me. Finally AMD has a card capable of playable raytracing but we can't use it on the 'Crysis' of modern times to even test it out.,AMD,2025-12-10 17:49:59,1
Intel,ntbtqi7,Adrenalin doesn't show this update for me yet lol,AMD,2025-12-10 18:11:33,1
Intel,ntbw2oz,"> Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Glad for this, it was annoying that we were stuck in 25.9.2",AMD,2025-12-10 18:22:46,1
Intel,ntc4icg,Are the issues with SecondLife fixed? Last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:03:00,1
Intel,ntc4u1h,9060 non XT 8GB can do the math 7900 XTX Nintendont,AMD,2025-12-10 19:04:36,1
Intel,ntcbbg1,"I’m at work, so I cannot check for myself: does this fix the constant crashing in Oblivion when hardware lumen is turned on?",AMD,2025-12-10 19:36:37,1
Intel,ntcdlil,Has anyone tested Marvel Rivals on 25.12.1 version of the driver? The only stable driver that works without crashing on that game is the 25.8.1 version.,AMD,2025-12-10 19:48:14,1
Intel,ntchv47,Adrenalin Panel not showing bug still present… :-((((,AMD,2025-12-10 20:10:02,1
Intel,ntcyy65,Am I the only one who doesn't have the new option in the drivers for frame generation with a 9070 XT?  https://i.redd.it/7r86hslx3g6g1.gif,AMD,2025-12-10 21:34:34,1
Intel,ntd63ea,"why are there 2 versions, b and c, 1.65Gb and 991Mb, release notes and through the support page, and is it stable or shall i just keep 25.8.1 as any other seems to crash call of duty, regular, other games seem fine,  ryzen 9 7950x3d/rx7900xtx",AMD,2025-12-10 22:09:16,1
Intel,ntdfkjs,Did it fix the god of war 2018 checkered shadows?,AMD,2025-12-10 22:59:20,1
Intel,ntdjg2j,"I spent all this time with 25.9.2 on my 9060xt because the following ones were disgusting to me, I will give this new update a chance and let's hope everything improves a little!!",AMD,2025-12-10 23:21:02,1
Intel,nteannh,Arc raiders crashes are fixed or not?,AMD,2025-12-11 02:02:44,1
Intel,ntek6ze,"Makes my 9070 XT to constant run on 100% load in bf6 no matter if i play or sit in the menu. Cause device hung, graphic glitches and high temps.   Same with all drivers above 25.9.   25.9.1 works flawless with no errors and the load varies depending on the scenery as it should.",AMD,2025-12-11 03:00:09,1
Intel,nteshht,Noticing in Hogwarts Legacy with the new FSR and FG enabled over a period of like 30 seconds my 9070XT will go from \~250W used and 200 FPS and then drop down to say 120W used and 90 FPS and then after a short period go back up again. With FG disable it stays consistent 140 FPS-ish,AMD,2025-12-11 03:54:05,1
Intel,ntfd9ta,"FYI for ""Driver Only"" guys, 25.12.1 still have an issue to install this option.  l've open ticket to support team for last 2 versions. but I can't follow their request to observe the issue.  Don't know how long to keep using extracted file method. lol  Will see how 25.12.1 ""driver only"" perform.",AMD,2025-12-11 06:33:01,1
Intel,ntfu45p,oh nice! they fixed the FSR4 Quality Presets artifact issue,AMD,2025-12-11 09:16:24,1
Intel,ntg07nz,"When AMD finished Orange, Yellow Green, PurpleStone, can we unlock FSR Infinity?",AMD,2025-12-11 10:18:18,1
Intel,ntgsz5x,"Is it worth updating to this latest driver? I am not planning to use frame gen, is the image quality better or are there any fps improvements in games?",AMD,2025-12-11 13:56:05,1
Intel,nti5u4n,"Updated to 25.12.1 now, before I was on 25.8.1, have a Rx 6800 XT and a Ryzen 7 7700X. Also updated my Chipset-Driver today. Haven't testet much yet, played now for like 1 hour Space Marines 2, watched some Youtube vids since I updated. So far looks ok. Only thing that worried me first was that I found in my Reliability History, 2 critical entries of LiveKernelEvents of code 1a8. But these were written down by Windows on the time, while I was updating my driver. We will see, if anything happens I will keep you updated.",AMD,2025-12-11 18:04:23,1
Intel,ntigzns,"Despite the device ID-based driver update blocking set in August, it has worked until now. The windows tried to install some driver on the 6700XT just now, and unfortunately, it also replaced the software itself somehow. threw an error message too.  Manual update would not go through unless i removed the driver update block.   What a sad situation.",AMD,2025-12-11 18:58:17,1
Intel,ntnwqsn,"Anyone else has problems with CS2/Fortnite? Started happening after i updated drivers to 25.11 My whole PC would randomly freeze for like a minute with the ""AMD software detected that a driver timeout has occurred"" error. Once the PC unfreezes i must kill the game from task manager.",AMD,2025-12-12 16:00:37,1
Intel,nto9zy8,Does it fix the arc raiders dxgi crash of the previous driver?,AMD,2025-12-12 17:05:14,1
Intel,ntpp1wj,"How do I downgrade from this driver?   I’ve tried four different older drivers and all of them give me error 182 – GPU is not supported (RX 9070 XT) during install.   I’ve already used DDU and the AMD Cleanup Utility, but the only driver I can install successfully is 25.12.1.",AMD,2025-12-12 21:27:18,1
Intel,ntqd768,pc started to crash 7900xtx... reverted to 25.11.1,AMD,2025-12-12 23:45:59,1
Intel,ntw41n8,Hi me and other people I know. Also forums and Facebook pages . Have had an issue with the frame gen after 25.9.2 . When they released new features we have all had issues where its drops fps and is completely unplayable. Has this been fixed in 25.12.1 I have 7900 xtx 7800x3d. Friend has 9070xt 9800x3d Both have issues. And im on windows 10 he's on windows 11. I used ddu and tried all settings on frame gen and other settings to fix it. Not to mention the drivers where stutters and lower fps without frame gen. Thanks,AMD,2025-12-13 23:14:13,1
Intel,ntwqp19,"When I enable V-Sync in the game, I experience lag; it only runs smoothly with V-Sync enabled when I also activate the performance overlay. This problem has existed since driver version 25.11.1.",AMD,2025-12-14 01:37:28,1
Intel,ntytwdj,"I have a second card from amd. And both cards have driver problems. Now I have an rx 9070 xt oc. I don't do any undervolting. Everything is at factory settings including the bios. I had 4 driver failures in 7 hours. What good is FSR if the driver doesn't work? It would be good if you finally solved this problem. I can stand it for a while, but if it continues like this, I'm leaving AMD.",AMD,2025-12-14 12:03:18,1
Intel,nu0egj3,"Wish they would acknowledge the bug where turning on GPU scaling and integer scaling adds more input delay, so for example the mouse movement will feel sluggish.  Been having this issue for 3 months now since nya bought a a 9070 XT",AMD,2025-12-14 17:38:40,1
Intel,nu0h263,"On the RX 7600S graphics card, Adrenalin does not launch at all, and during installation it removed the driver PCIVEN_1022&DEV_15E2&SUBSYS_15131043&REV_60.",AMD,2025-12-14 17:51:39,1
Intel,nu2b8e5,"How are those with a Cezanne CPU supposed to install this?  Selecting the 5750G from the drivers download page offers 25.21.1, yet none of the 3 variants of the installer support it.  * whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe (Vega, supposedly?) - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe (combined? ""Systems with RDNA series graphics products"") - nope  Each of them return Error 182.  Even the minimal web installer, amd-software-adrenalin-edition-25.12.1-minimalsetup-251207_web.exe, only offers 25.8.1.  VEN_1002&DEV_1638 is nowhere to be found in the .inf for any of the 25.21.1 variants.",AMD,2025-12-14 23:19:40,1
Intel,nu3psfe,I'm still hesistant to upgrade on this driver until they resolve these driver timeouts hell I'm even on 25.9.1 still experiences time to time TDR's.,AMD,2025-12-15 04:22:56,1
Intel,nu4kg83,"Is it worth for my 9060XT to go from 25.11 to the latest, Im having some problems where ghost of tsushima crashes.",AMD,2025-12-15 08:42:46,1
Intel,nu51zrq,Driver is making valorant run like crap for me idk why .,AMD,2025-12-15 11:34:40,1
Intel,nv057ke,im using an rx6600 and up until today i was fine avg 200fps on r6 today the game says its at 22 usaeg when it avgs 1-4 and now it has major fps drops/tears,AMD,2025-12-20 08:25:06,1
Intel,nv2wvfi,This driver constantly crashes call of duty for me. Whatever windows update installs(which seems to be 25.10. something) is the most stable there is. 9070XT.,AMD,2025-12-20 19:41:18,1
Intel,nvf8wml,"Parabéns, fiz a atualização para 25.12.1 e agora não consigo jogar nada sem travamentos, além do google estar lento",AMD,2025-12-22 19:52:06,1
Intel,nvtvg24,Anyone see if this fixes the issue of the graphics sliders not working at all and being stuck?,AMD,2025-12-25 05:17:11,1
Intel,nw7hqmm,Still crashes. They will never fix it. Just buy nvidia or intel.,AMD,2025-12-27 16:03:55,1
Intel,nw7j7uy,"u/AMD_Vik It says ""Intermittent application freeze when using the in-game Radeon™ Overlay."" in fixed issues but I've actually had my whole system lock up because of what seemed to be adrenalin having issues with the performance overlay....  I noticed one thing that pointed towards the overlay specifically: I was going through Adrenalin and when I was on the recording tab I switched to performance; it seemed like Adrenalin froze so I clicked Smart Tech. to see if it would respond.  Initially it didn't, before eventually swapping to the smart technology screen. I then went back to the record tab and tried again: same results.  That's about all I've got for specific steps. I closed Adrenalin and went back to doing whatever and I noticed my fans turned on and like two minutes later when I went to close my browser my cursor stopped before I got to the corner of my screen and I needed to hard power down my system.  Not sure if this is at all related to that issue. But i had it happen on the last driver as well, and came here trying to see if there was a known issue...",AMD,2025-12-27 16:11:24,1
Intel,nxehl3e,Any chance to support VR HP reverb G2 (WMR) 60hz mode with Oasis driver? I'm locked in win10.,AMD,2026-01-03 07:42:05,1
Intel,nxhczy3,"Indiana Jones crashing every 5 minutes, cant complete the game. Its just freezes and the PC barely responsive with these Timeout messages.  9070 with 5800x3d",AMD,2026-01-03 18:31:17,1
Intel,ny008s3,"AMD sucks: FSR 4 is locked to RDNA 4, while NVIDIA’s DLSS 4.5 runs even on RTX 20-series GPUs. My next GPU will be NVIDIA only, and I advise everyone against buying AMD. It’s a greedy company with no respect for customers — you buy a graphics card last year, and the next year it’s already outdated",AMD,2026-01-06 13:00:32,1
Intel,ntakvhf,FSR Redstone support? Will my minecraft machine run faster now?,AMD,2025-12-10 14:26:09,0
Intel,ntap40e,Gonna be able to play modern titles on my HD5750 thanks to redstone !,AMD,2025-12-10 14:49:52,1
Intel,ntdi8fj,All that build up for dog water. Built my first PC in March and went with a 9070xt full of hope. I'm beginning to understand why AMD is so widely despised.,AMD,2025-12-10 23:14:14,1
Intel,ntclu0p,Windows just installed the June   update from AMD. The fck is this,AMD,2025-12-10 20:29:51,1
Intel,ntcrpsp,I'm not seeing the new update in AMD Install Manager,AMD,2025-12-10 20:58:55,1
Intel,ntd2bv7,so in 2028 10 games will have it like FSR4 XD,AMD,2025-12-10 21:50:50,1
Intel,nte3gul,Anyone knows if it fixed the crashes with Oblivion Remastered and Silent Hill?,AMD,2025-12-11 01:18:47,1
Intel,ntf7sz1,"""Intermittent application crash or driver timeout may be observed""  This is not an issue tied to a few games.... is a wide issue for me with a 9060 XT whenever i rise my monitor refresh rate.",AMD,2025-12-11 05:46:09,1
Intel,ntaqgkm,"Ray Caching in 40K?  Not sure how they got this to work on the tabletop in real life but sounds awesome  In all seriousness there are a large number of games in the Warhammer 40K universe, any chance they are saying which one?  Space Marine 2 Darktide Battlesector   Etc",AMD,2025-12-10 14:57:02,47
Intel,ntavwoz,"so pretty much nothing for today, shrug...",AMD,2025-12-10 15:25:34,12
Intel,ntapnnp,Is there a partial list of the 40 games with the new frame gen? Is it something different from the fg we already have?,AMD,2025-12-10 14:52:46,8
Intel,ntbyc9u,There are well over 100 warhammer 40k games. Did they not specify?,AMD,2025-12-10 18:33:41,1
Intel,ntam71a,they wont,AMD,2025-12-10 14:33:37,53
Intel,ntf1fzq,"It's so annoying.  I would keep it if it didn't constantly pop up trying to get me to install ""AMD Chat"" and ""AMD Privacy View"".  I don't want your shovelware AMD, take a hint.",AMD,2025-12-11 04:56:41,20
Intel,ntbob9s,"There should be an option during install to exclude it, it can't be that hard to do. Same as you, u/MihawkBeatsRoger , I also uninstall it afterwards.       Notifying u/AMD_Vik",AMD,2025-12-10 17:45:29,18
Intel,ntasnol,"This.   Why I took it out are my own reasons and quite frankly, irrelevant. It's my PC and I don't want it. So please AMD, listen to me and keep it off.",AMD,2025-12-10 15:08:39,22
Intel,ntb2rjk,"Focus on serious matters, this is a joke. If you do not want it feel free to install the driver only version, and be happy u have that choice. If you want the full features of adrenalin, well install manager is one of them.",AMD,2025-12-10 15:59:33,-2
Intel,ntapviv,It's a rebranding of the entire FSR ecosystem. What's new today is machine learning enhanced frame generation for RDNA4 cards. You can enable it in the driver for any game with FSR 3.1.4 or newer.,AMD,2025-12-10 14:53:56,135
Intel,ntb2cv7,It adds denoising for Path tracing. In theory it should look way better now,AMD,2025-12-10 15:57:34,8
Intel,ntap7sr,All the games that don't use bluestone,AMD,2025-12-10 14:50:26,26
Intel,ntbfl85,"Only one , the new call of duty ATM. So if you enjoy shitty games , have at it",AMD,2025-12-10 17:02:21,3
Intel,ntalgrc,"Same, and I'm still on the October drivers",AMD,2025-12-10 14:29:29,17
Intel,ntaosq5,You can download it from the website. The app release notification always lags behind the site. This is nothing new.,AMD,2025-12-10 14:48:11,9
Intel,ntcmrfi,9070xt i see brave or discord freezing and lagging when watching a YouTube video still. I dont understand how hardware acceleration bug hasn't been fixed yet. Wtf are they doing.,AMD,2025-12-10 20:34:27,12
Intel,ntcuvcq,Yup same here. Had to roll back to October to fix again,AMD,2025-12-10 21:14:37,7
Intel,ntem0sw,25.9.1 works on my 9070 XT. Everything after that is a mess for me,AMD,2025-12-11 03:11:43,7
Intel,ntfe4wd,Tagging u/AMD_Vik  so they are aware of the issues.       I encountered the same problems on my 6800xt. Figma on chrome is causing random BOSD. The system will just restart without notice. Every single web app seems unstable on my system and memory usage is all over the place. Rolling back to 25.9.1 doesn't fix everything but it eliminates 70% of the issues..,AMD,2025-12-11 06:40:43,5
Intel,ntf2h1v,Oh well. :/  Funny thing is I rebooted my PC again for a Windows update. The first thing that greeted me after opening a web browser was the driver giving up the ghost.  On 25.11.1.,AMD,2025-12-11 05:04:26,2
Intel,nvesl6s,Ive been wondering what this seemingly random crashing has been. Thanks for this comment!,AMD,2025-12-22 18:30:18,2
Intel,ntgomie,"9800x3d, 6950xt, no issue with either chrome or discord or firefox with hardware accelerated set",AMD,2025-12-11 13:30:30,1
Intel,ntkqfzv,"Me too.  Installed 25.12.1, whenever I use YouTube in Full Screen, the whole system freezes, while the sound is still audible, then I have to hard-restart my PC. Happened three times, decided to downgrade to 25.11.1 again.",AMD,2025-12-12 02:24:44,1
Intel,ntaon6n,"This should be fixed, I'm not sure why it was omitted from the release notes",AMD,2025-12-10 14:47:21,29
Intel,ntbfx3u,<--- inte 8 rdna3 enjoyer,AMD,2025-12-10 17:04:01,33
Intel,ntbh75k,"How do I set this up, can't find any info",AMD,2025-12-10 17:10:24,1
Intel,ntaukfz,"I can't speak on enhanced sync, but noise suppression is still busted and not working =/",AMD,2025-12-10 15:18:39,17
Intel,ntarxtz,"I'm piggybacking, because I need that info too",AMD,2025-12-10 15:04:52,4
Intel,nwscpi6,"I can't seem to keep framerates under control in a lot of games, generally smaller simpler games, with the new 9070xt. Enhanced sync, vsync, chill, boost, whatever I do I'm still wondering why my pc is at 100% gpu, 600fps, and 300w power draw playing something like minecraft or geometry dash.  Even with a 144hz display. I'd be happy locked at 60 even.",AMD,2025-12-30 19:52:48,1
Intel,ntb6txh,Ok I thought I was the only one having the enhanced sync issue because no one replied to any of my posts about it. I use it because then I can lock my fps to 120 (on a 4K OLED TV) and use enhanced sync instead of Vsync because of the screen tearing when locking to 120. Now I have to do the frame lock to 117 which is fine but just annoying me I'm missing out on 3 fps lol it was causing issues in a few games where it would stutter like crazy and it all came down to enhanced sync. I don't use noise suppression so I don't know what's up with that.,AMD,2025-12-10 16:19:26,1
Intel,ntbjznn,"Been using it for a few hours with the 7900XTX, so far so good.   Hopefully it's 100% fixed.",AMD,2025-12-10 17:24:13,7
Intel,ntbqeln,I hope they fixed it. I will test it now,AMD,2025-12-10 17:55:35,3
Intel,nteci65,"Did the typical test that I usually do and it didn't show up for me and I'm on the RX 7700XT as well. So hopefully, it's fixed.",AMD,2025-12-11 02:13:47,2
Intel,ntb5cx9,"AMD stopped giving a shit about it's fans once the company was saved and they started raking in the money. The change in tone was clear as day. That said, I'll still buy their GPUs because I hate Nvidia far more and I don't see that changing.",AMD,2025-12-10 16:12:17,25
Intel,ntch9q3,"yeah my next one will be Nvidia, better features, better performance espacialy with RT/PT   And apperently longer support... and AMD cards in a simmilar performacne bracket don't even cost THAT much less sooo.... jeah I am mad aswell",AMD,2025-12-10 20:07:01,16
Intel,ntk9244,"I agree. AMD has shown poor support for 7000 series owners. If there was a FSR4 int8 leak, AMD should officially release FSR4 for 7000 series owners.  I bought my 7800xt only 2 years ago before RDNA4 cards came out.  Nvidia provides DLSS4 upscaling to their older generations like rtx3000 series",AMD,2025-12-12 00:39:32,3
Intel,nw3y7cj,"Your system is almost exactly like mine, did you also have crashing problems while having the Xbox Gamebar DVR feature turned on? I would have constant driver timeouts until I turned it off.",AMD,2025-12-27 00:12:39,1
Intel,ntd2msi,looking back rn i think it wasnt worth the 100 dollars gain i gotwhen my 7900xt does consume more than rtx 4070ti and i do have shit features even the antilag+ scam that was one of the main reason i bought the GPU isnt here anymore .,AMD,2025-12-10 21:52:20,1
Intel,ntaoqmx,"If you're referring to the app crashes with RTPT reflections enabled, we're working with CDPR on a fix",AMD,2025-12-10 14:47:52,57
Intel,ntcukk4,Signed /another 7900xtx user,AMD,2025-12-10 21:13:09,15
Intel,nu3780v,"I came over from NVDA last March, bought a 7900xtx, RMAd it a few weeks ago due to pink/purple pixelation that would randomly happen. Now it's non stop driver timeouts and random performance issues every time I boot my PC or games. I am never buying another AMD card. I'd rather get ripped off by NVDA and not have constant headaches.",AMD,2025-12-15 02:23:41,1
Intel,nugitp7,"Which driver are you currently on? I'm just curious; personally, I'm on 25.9.2, and surprisingly, I have 0 problems, unlike with previous versions. Should I try 25.12.1?",AMD,2025-12-17 04:25:49,1
Intel,ntapar1,"Nope. Generally if the driver does not massively increases performance in some game, or you don't have any issues or the issue you have isn't fixed, then it's not worth updating, unless there is some new feature you want.    I reverted back to 25.9.1 (from the top of my head) because with any newer driver BF6 crashes randomly, and neither DICE nor AMD seem to give a damn about it.    And before someone asks, I tried any other fix on the internet for Battlefield and nothing else worked.",AMD,2025-12-10 14:50:52,18
Intel,nte7fbw,Same here. Anything above 25.9.2 crashes ray tracing games like Silent Hill  2 and Oblivion Remastered.   Ihr never had a more crash prone GPU than the 9070XT.,AMD,2025-12-11 01:43:09,3
Intel,ntc8k09,"Try this, taken from another comment branch https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 19:22:53,1
Intel,ntissbw,Might potentially be fixed by a recent Windows update?  24H2 (and an earlier mini-patch that included this) apparently resolved a lot of crashing for folks.  See [here](https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/),AMD,2025-12-11 19:57:19,2
Intel,ntaohu1,"Yeah I was really hoping they'd have got buy in from a decent number of devs with updates to big RT showcase games like Indiana Jones, Alan Wake 2, Cyberpunk, etc. But Black Ops 7 and Warhammer 40K... and that's it (for the RT features)?",AMD,2025-12-10 14:46:32,10
Intel,ntb73f1,5070ti fs  basically a better 9070xt,AMD,2025-12-10 16:20:42,13
Intel,ntcro7s,"I’d wait on 5080ti with more VRAM but these are going to be obscenely expensive knowing nvidia + current RAM prices. Both 5070ti or 5080 are more of a sidegrade than upgrade, not worth the hassle IMO.",AMD,2025-12-10 20:58:42,2
Intel,nted84w,Get a 5070ti. I never thought I would say that. But this is what is is.. 9 months after release and the drivers are still D.S.,AMD,2025-12-11 02:18:08,2
Intel,ntb6h5d,What about a secondhand 5070ti?,AMD,2025-12-10 16:17:43,3
Intel,ntbx8qa,"I mean, I wouldn't get either. 5070 TI is a sidegrade from the XTX, and 5080 is only slightly better. DLSS and RT would be the only reason.",AMD,2025-12-10 18:28:25,2
Intel,ntbzcmw,Sidegrading for an upscaler sounds like a joke.,AMD,2025-12-10 18:38:32,3
Intel,ntam4ba,"I think Linux developers are doing some experiments As of now, FSR 4 (FidelityFX Super Resolution 4) does not officially support RDNA 2 or RDNA 3 GPUs, even on Linux. However, thanks to Develer’s work on VKD3D-Proton 3.0, there is partial and unofficial support for RDNA 3 under specific conditions.  RDNA 3: Partial Support via Develer’s VKD3D-Proton  - Develer’s VKD3D-Proton 3.0 includes support for FP8 (8-bit floating point), which is required for FSR 4. - This means RDNA 3 GPUs (like RX 7600, 7900 XT/XTX) can run FSR 4 in some games via Proton, even though AMD doesn’t officially enable it. - Global override toggles in AMD’s 25.9.1 driver can bypass the FSR 4 whitelist, allowing it to run in FSR 3.1-compatible games.  I hope they succed it will be a slap in the face.",AMD,2025-12-10 14:33:11,27
Intel,ntakqc7,This has been announced for months.,AMD,2025-12-10 14:25:21,24
Intel,ntazxem,Yeah AMD refusing to port features to any card released before the 9 series makes supporting them really hard.,AMD,2025-12-10 15:45:42,8
Intel,ntakt3q,Say thanks they haven't demoted 7000 series to only game drivers,AMD,2025-12-10 14:25:47,8
Intel,ntamwc6,"Your best case is your RX 7900 turning into Balsamico, whatever that means.",AMD,2025-12-10 14:37:39,1
Intel,nte0i5m,"Its because RDNA 4 added hardware that 3 and 2 don't have. Now before I get kicked to death by angry people, there is a version of FSR Redstone that uses and INT8 path that is compatible and will work on 2 and 3, however that has not been launched today and AMD have not confirmed it will be.   That isn't to say they won't do it, but right now it's not been announced. Perhaps there will be enough noise to get AMD to change their mind or it might be that they want to get it out on their latest cards first before complicating matters with older RDNA support.  Only time will tell",AMD,2025-12-11 01:00:33,1
Intel,ntaoydb,"Bro the AI accelerators completely got revamped, upscaling technique isn't usually the indicator for 'fine wine', it is when non-upscaling raw performance numbers improve.",AMD,2025-12-10 14:49:01,2
Intel,ntf8us3,Same boat here. Tired of trying.,AMD,2025-12-11 05:54:50,1
Intel,nte6mdt,Thanks for testing. Have you perhaps tested Oblivion Remastered?,AMD,2025-12-11 01:38:14,1
Intel,ntf8lpl,Finally fixed! It's a christmas miracle!!!,AMD,2025-12-11 05:52:44,6
Intel,ntf2wry,I regret getting this 7800xt,AMD,2025-12-11 05:07:41,2
Intel,ntazn6o,Any card released prior to the 9 series.  Amd could give 2 shits as they chase the AI bubble (jokes on them if I was an exec I'd double down on the consumer market to insulate from the impending bubble burst),AMD,2025-12-10 15:44:19,16
Intel,ntaw82d,sadly,AMD,2025-12-10 15:27:12,6
Intel,ntedpok,"Yep, I go back between 23.9.1 and 25.9.2. I couldn't be happier.",AMD,2025-12-11 02:21:03,2
Intel,ntnjpo8,"If it's any consolation, I was on an NVidia card for 2+ years where I wasn't getting the DLSS updates. Then they actively removed features when they went to the NVIDIA app.  Looking at AMD's roadmap, RDNA4 looks like a stopgap anyway until RDNA5 (prob will be called UDNA?) comes out. So in another year and a half I'll be in the same situation with my 9060XT.",AMD,2025-12-12 14:55:34,2
Intel,ntbhjqe,"Use OBS, replay buffer",AMD,2025-12-10 17:12:09,3
Intel,ntg1daf,Was just thinking of giving a shot for Indiana Jones and the Great Circle - I guess not anymore since FSR4 doesn't work with it..,AMD,2025-12-11 10:29:39,2
Intel,nuji470,"That was a terrible driver for me also. New one has been night and day improvement, give it a shot.",AMD,2025-12-17 17:12:44,1
Intel,ntasabf,"Microsoft had bugs also causing hanging crashes. Everyone loves to blame GPU drivers immediately, but check this out:  https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 15:06:41,23
Intel,ntars3b,I also want to know this.,AMD,2025-12-10 15:04:00,3
Intel,ntasr85,"I'm wondering the same thing, 25.11.1 is still the most stable for me!",AMD,2025-12-10 15:09:10,3
Intel,ntbzovt,Wondering too. I bumped back down from 25.11.1 because it was unstable on my machine.,AMD,2025-12-10 18:40:09,2
Intel,ntf7xwz,Stay on 25.11.1 if you are on RDNA 1 or 2,AMD,2025-12-11 05:47:16,2
Intel,ntgynxw,squash hard-to-find sharp reach memorize fade husky divide subsequent plough   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:28:31,1
Intel,ntlwtqy,"~~It messed up my audio, now everything sounds 8-bit. If you're on RDNA4, avoid this update.~~  EDIT: it's not the drivers, after much tinkering I was about to deduce that it was my monitor. So it should be ok to update",AMD,2025-12-12 07:15:44,1
Intel,ntelprh,I can't use anything above 25.9.1 on my 9070 XT,AMD,2025-12-11 03:09:46,2
Intel,nth7dkb,forget it they gave u the middle finger move on fuck both amd and nvidia,AMD,2025-12-11 15:15:41,3
Intel,ntb6lhi,won’t happen,AMD,2025-12-10 16:18:18,3
Intel,ntbt1wv,"Fine wine is only a thing for very few and specifics types of wine, typical wine still goes bad over time.",AMD,2025-12-10 18:08:16,5
Intel,ntbeh2e,What is the source for this or is it trust me bro?,AMD,2025-12-10 16:56:49,5
Intel,ntecwqr,They should just remove this feature. It never worked from day 1..,AMD,2025-12-11 02:16:12,1
Intel,ntbm91h,what is the difference,AMD,2025-12-10 17:35:20,1
Intel,ntcnmrm,"Can you tell me if this is also applicable to 25.12.1? There are several (frustratingly unlisted) VR-specific fixes aligned, one of them closely relates to what you've just described",AMD,2025-12-10 20:38:48,2
Intel,ntekn1e,Same here. 25.9.1 makes my problems go away,AMD,2025-12-11 03:02:56,1
Intel,ntfr6xt,Same for my 9070 XT. Device hung error,AMD,2025-12-11 08:46:36,3
Intel,nu0gqvz,"Thanks for reporting, had that once with 25.11.1 + 9070XT (W10) before reverting to 25.9.1 (since then, it never reappeared).",AMD,2025-12-14 17:50:07,1
Intel,ntb6g84,i think your sorry should extend to people with rdna4 cards because this is pretty underwhelming,AMD,2025-12-10 16:17:35,1
Intel,ntoma7o,Do you get a firmware update pop up? Is this one?  https://i.redd.it/w46j86mnct6g1.gif,AMD,2025-12-12 18:06:40,1
Intel,ntmvi3j,"I'm familiar with this impacting United Offensive, I don't believe we're reintroducing this old vendor specific extension, however. I do have a ticket for the performance issues though; I don't believe this is related to the missing extension.",AMD,2025-12-12 12:32:54,1
Intel,nu65nki,"Tested for 2 days(1day and 22hrs uptime)  No crash, No BSOD for me so far. Nothing strange.  MS Edge, Google Chrome video playback, youtube...etc all play nice while gaming on main monitor.  Diablo 4, MSFS 2024, Doom dark age, Forever winter(UE5), Witchfire(UE4)...etc All run fine.  Lossless scaling runs fine on spicy vids to all of the above games xD  HWinfo64 and MSI Afterburner, RTSS all run as they should.  (Win11 25H2 uptodate, X670E, igpu(98x3d)+7900xtx+6400 3gpus, 2 monitors, hybrid mode)  Edit) rx 6800 + r7 7700x on win11 25H2, X670E, Single monitor, igpu-disabled -> runs fine.  rx 6700xt + i7 8700k on win11 25H2, Z370, Single monitor, igpu-disabled -> seems good.",AMD,2025-12-15 15:39:46,1
Intel,nuizppc,"25.11.1 had pink artifacts glitch on chromium browsers with 7700 xt but i installed 25.12.1 yesterday and no issue so far, i did not see artifact pink glitches or sound issue so far ?",AMD,2025-12-17 15:42:58,3
Intel,ntemk82,My 9070 XT hate every driver above 25.9.1,AMD,2025-12-11 03:15:11,1
Intel,ntcw96j,I updated to this driver and immediately got a BSOD. Rolled back to October 25.10.2 again,AMD,2025-12-10 21:21:24,4
Intel,ntgzi2n,offer steep theory scale straight obtainable physical ad hoc selective test   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:33:06,1
Intel,ntc7ylk,thats what I wonder too! Is it more stable??,AMD,2025-12-10 19:19:58,3
Intel,ntb2i9s,haha r u fr,AMD,2025-12-10 15:58:18,7
Intel,nteeogf,la même. C'est scandaleux,AMD,2025-12-11 02:26:49,2
Intel,ntbcqw3,"Means that they've created separate driver packages tailored for the specific gens (A rdna1/2, B for RDNA3/4, C - combined fat package that contains both drivers for systems that might have both gens on the same machine (igpu + dgpu) )",AMD,2025-12-10 16:48:24,5
Intel,ntb2cwp,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 15:57:35,1
Intel,ntcubg5,"Cleanup utility first, always!",AMD,2025-12-10 21:11:55,3
Intel,ntbx46o,I also wanna know best way updating drivers. DDU kinda annoying but maybe must be done i don't know,AMD,2025-12-10 18:27:48,1
Intel,nte7te5,It's doing it there too.,AMD,2025-12-11 01:45:34,1
Intel,ntcm462,Never seen any crashes on it with latest driver prior to today 9070xt w11,AMD,2025-12-10 20:31:13,1
Intel,ntctang,"Just tested it tonight, and for me it's working fine, 9060xt here, windows 11 with the latest update, although i play with the ""medium"" preset which disables ""lumen"", can't say it might work for you but you can try it if it still crashes constantly",AMD,2025-12-10 21:06:50,1
Intel,ntdql6b,Might want to check [https://www.reddit.com/r/radeon/comments/1pjeonb/fyi\_fsr\_ml\_framegen\_requires\_windows\_11/](https://www.reddit.com/r/radeon/comments/1pjeonb/fyi_fsr_ml_framegen_requires_windows_11/) :|,AMD,2025-12-11 00:02:35,2
Intel,ntltbvr,nope. I still crash,AMD,2025-12-12 06:44:56,1
Intel,nujivd3,"I fixed my arc raider crashes (mostly in blue gate map load) by running DDU, installing 25.10.1 (down from 25.11.1), and deleting shader caches (dont know if the shader cache delete helped or not). I upgraded to the newest drivers the day after they released and haven't had a single crash since in arc raiders, including w overlay.",AMD,2025-12-17 17:16:27,1
Intel,nts13sv,"for me, DDU in safe mode, disconnect internet, install 25.9.1 fine for me(9060XT).  I've tried 25.10/ 25.11 and revert back to 25.9.1 with this way. Now observing 25.12",AMD,2025-12-13 06:52:17,2
Intel,nujhigl,Yes. I downgraded from 25.11.1 because of the crashing. Now been on 25.12.1 all week and havent had any issues come up. You also get proper fsr4 upscaling now.,AMD,2025-12-17 17:09:46,1
Intel,nv4w9xy,"I just want to say I think I found the culprit. It also happens on the winupdate one too, because it started crashing all the time.  Core clock boosts itself WAY past what it is declared on the card(I got a Sapphire 9070XT Nitro, supposed to be 3060MHz). Here's the moment before it crashes to a black screen:  [afterburner screenshot](https://i.ibb.co/3YpJFtzM/Screenshot-2025-12-21-030537.png)  The dip in clocks is the moment it crashes. As you can see, it is running well above boost clocks. Hence, freezing in a few minutes, proceeded by a black screen, and a crash. The ups and downs are from me alt tabbing in the graphs, by  the way.   This is with core clock -200mhz applied in Afterburner and no crashes, boosts to just above declared boost clocks. Here the dips in up and down on power are probably me toying around how much exactly -mhz is needed.  [afterburner -200mhz](https://i.ibb.co/YTQfGJtc/11111.png)  All of the crashing behavior so far is replicable in COD, CS2, Cronos New Dawn.  u/AMD_Vik",AMD,2025-12-21 02:43:00,3
Intel,nw7st51,thanks for reaching out - funny timing; I noted that on the internal ticket for this issue yesterday having seen other accounts of end users noting this issue persists even with 25.12.1. Perhaps the fix aligned to that point release somehow slipped.,AMD,2025-12-27 16:59:39,1
Intel,ntbhx4e,I still have my 5670,AMD,2025-12-10 17:14:01,2
Intel,ntfsndh,"No problems with RX 9070 xt in ARC raiders, i have win10",AMD,2025-12-11 09:01:20,1
Intel,ntazsxe,It’s for Darktide apparently,AMD,2025-12-10 15:45:06,23
Intel,ntauw3f,any game with fsr 3.1 fg also has the new fg since drivers override it. it’s also why they stopped versioning fsr. any game with fsr 3.1 should just automatically have any new version of fsr when the drivers update,AMD,2025-12-10 15:20:21,11
Intel,ntfnlow,"It's for Darktide. But it's not even ready for launch there, either.",AMD,2025-12-11 08:10:07,1
Intel,nthk5bz,I forgoed any amd software entirely  Use more clock tool  10x better with 0% of the bloat   ^^ helped me get my 4th in world furmark score (7900xtx user),AMD,2025-12-11 16:18:42,2
Intel,ntbig0n,If you want to be in control of what’s on your computer then Windows is not the OS for you,AMD,2025-12-10 17:16:36,19
Intel,ntbjons,"Dumbest take one can have, since installing only the driver won't let you manage the settings at all.  Which has nothing to do with this useless launcher no one wants or needs.",AMD,2025-12-10 17:22:43,16
Intel,nteh1sb,Found the install manager dev lol,AMD,2025-12-11 02:41:05,3
Intel,ntaqzke,Thanks.,AMD,2025-12-10 14:59:49,15
Intel,ntb7o1t,Unfortunately Redstone FG is bugged with poor frame pacing,AMD,2025-12-10 16:23:28,20
Intel,ntaqis1,Nice to see the innovation continuing on,AMD,2025-12-10 14:57:21,18
Intel,ntbic15,But only on the 9060 and 9070 right?,AMD,2025-12-10 17:16:03,1
Intel,ntaoqu8,Yeah same,AMD,2025-12-10 14:47:53,2
Intel,ntapwco,"Remember when you could click ""Check for Update"" inside the AMD Software and if there was an update, it would download and install it for you?  Glad they fixed that awful experience, and we have the Installation Manager now.",AMD,2025-12-10 14:54:03,26
Intel,nte60vn,I remember this mentioned since the  GCN 1.0 days. Lol,AMD,2025-12-11 01:34:30,7
Intel,ntfp000,"On my end, the driver crashes. Most of the time it manages to recover (sometimes it will crash a few more times before stabilising). Sometimes it doesn't recover (leaving only 1 of my monitors working), so I had to reboot. Then after rebooting, strong chance it'll crash again the moment I open my browser.",AMD,2025-12-11 08:24:09,5
Intel,ntwnl8a,"\+1 on this. Most games crashed drivers with any newer drivers except 25.9.1, but poe2 i cant play with vulkan or Directx 12 only with Dx11",AMD,2025-12-14 01:17:11,2
Intel,nuur9u6,"My experience with switching to amd was so smooth and perfect until 25.9.1. Everything after that just caused stutter issues in games, programs randomly crashing, drivers crashing completely causing my pc to reboot, this is so sad i hope they fix this soon and bring back a stable version asap. Rolling back to 25.9.1 now aswell until that happens.",AMD,2025-12-19 12:46:06,2
Intel,nv8ptlv,přesně zustávám na 25.9.1 všechno jiné crash,AMD,2025-12-21 18:59:03,1
Intel,nw3xh0a,"I had been having the absolute worst time with drivers when I first bought my 7600XT, but finally found stability with 25.8.1 (and turning the Xbox Gamebar DVR off...) but I'm so paranoid now to update my drivers again. The only reason I decided to check on updates now though is a sudden appearance of my screen flashing black at random times.",AMD,2025-12-27 00:08:20,1
Intel,nth6kuu,Are you able to tell us what the error code is on the BSOD? I don't suppose you have a kernel memory dmp pertaining to one of these failures over at      C:\Windows\MEMORY.DMP,AMD,2025-12-11 15:11:30,4
Intel,ntap5oq,Thanks will give it a try after I finish work,AMD,2025-12-10 14:50:07,9
Intel,ntchncg,"Wait, AMD Customer Support told me that 2 monitors connected to iGPU and dGPU has never been officially supported and that this configurations breaks performance… so they told me bullshit?",AMD,2025-12-10 20:08:56,1
Intel,nte3vcl,Any update on three Oblivion Remastered and Silent Hill  2 Remake crashes? A lot of us are still with the September drivers because of them.,AMD,2025-12-11 01:21:16,1
Intel,ntcb9cq,<--- Ditto,AMD,2025-12-10 19:36:20,6
Intel,ntbpv70,Optiscaler lets you inject it. Do not use in multiplayer games though.,AMD,2025-12-10 17:53:00,3
Intel,ntauof3,it cannot possibly be this difficult to fix when there’s already community workarounds,AMD,2025-12-10 15:19:15,9
Intel,ntb6tpy,both are still broken somehow,AMD,2025-12-10 16:19:24,1
Intel,nwsjipr,running at 600 fps with vsync on means that something’s terribly wrong with something in your software that’s breaking vsync. that’s definitely not normal,AMD,2025-12-30 20:25:38,1
Intel,ntb77ho,both have been broken since 25.10.1. enhanced sync just makes your display run at an extremely low framerate when freesync is on and then noise suppression just doesn't even turn on. I don't understand how they haven't fixed either of these yet. they haven't even acknowledged it,AMD,2025-12-10 16:21:15,3
Intel,nte5171,"I have to do the same. My monitor is  240Hz and the TV  120Hz and I have to use Chill, which sometimes will cause stuttering, because enhanced sync always causes stuttering.   Man I'm starting to miss the NVidia setting of just putting vsync on in the driver and everything just working.",AMD,2025-12-11 01:28:24,1
Intel,ntcnloa,I did some testing AND as far as I can tell I do think it's actually fixed finally,AMD,2025-12-10 20:38:39,4
Intel,ntbd1ml,I would continue buying their GPUs if they gave me something to buy.  The XTX has no upgrade path on RDNA4.,AMD,2025-12-10 16:49:51,18
Intel,nteixfg,"I had Nvidia for years, the main reason I switched was that the drivers went to shit last year. I'm just sick of them in general, too. The 7800 XT I bought has been one of the most trouble free cards I ever had, aside from Adrenalin randomly closing in certain versions.",AMD,2025-12-11 02:52:24,2
Intel,ntdc84n,"If I could get my hands on a 5070 Ti I’d happily switch. AMD likes to take advantage of the underdog, for-the-people image whenever it’s convenient but they’ll just as quickly throw us under the bus and fuck us raw once they’ve got the bag.  Is Nvidia a gang of greedy fucks? Sure. But at least the bullshit’s right out front where you can get a good strong whiff of it. You know what you’re in for.",AMD,2025-12-10 22:41:15,3
Intel,ntm5vgi,"I purchased a 7700 XT and a 7600 8gb I'm March and while I'm satisfied with performance, it would definitely be awesome to have FSR 4 on both cards as FSR 3 and 2.2 (overwatch )leave alot to be desired",AMD,2025-12-12 08:42:08,2
Intel,ntapkhc,It's been so long bro :( Hopefully the fix comes with ray regeneration support?,AMD,2025-12-10 14:52:19,24
Intel,ntbfm3b,"Hey Vik, is there any info for FSR4 Vulkan support?  It's quite sad to see that there still isn't support for it as it has been 9 months by now since the release of the 90 series  Also is there any info about the EAC issue with Star Citizen and the latest drivers?",AMD,2025-12-10 17:02:29,15
Intel,ntc52w2,"Amd Noise Supression doesn't work, when I try to turn it on, nothing happens, but in 25.9.1 it works",AMD,2025-12-10 19:05:49,7
Intel,ntcc596,"Hey amd\_vik is amd Aware of the 1 year on going Darktide issues with amd  ( GPU , and specially X3D cpus ? ), and that even the Dev of Darktide ( Fatshark ) seemingly gets ghosted by amd ?  heres some more info specially first links includes a few Dev comments  [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)  [https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f](https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f)  [https://forums.fatsharkgames.com/c/darktide/performance-feedback/97](https://forums.fatsharkgames.com/c/darktide/performance-feedback/97)",AMD,2025-12-10 19:40:50,7
Intel,ntdbffr,"Vik, weren't you on holiday leave? xd",AMD,2025-12-10 22:37:02,4
Intel,ntc7hrz,Any fixes for the SecondLife issues we've had the last few months? last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:17:39,2
Intel,ntbwr0w,Will this update fix some of the artifacting I’m seeing in cyberpunk with fsr enabled?,AMD,2025-12-10 18:26:01,1
Intel,ntcji37,Also getting driver timeouts in Cyberpunk with RDNA3 with raster or RT. I did not have these problems with my RDNA2 card.,AMD,2025-12-10 20:18:15,1
Intel,ntcuy8r,"The AMD FSR ML-based Frame Generation option in the Radeon panel disappears in Windows 10.  So I have a question: Is ML-based Frame Generation no longer usable in Windows 10? This option is available in Windows 11, but not in Windows 10.",AMD,2025-12-10 21:15:00,1
Intel,ntdcmj2,Can I join if mine's just an XT?,AMD,2025-12-10 22:43:23,1
Intel,ntawh67,What GPU are you using?,AMD,2025-12-10 15:28:30,2
Intel,ntfkwf8,Try reinstalling Windows. That fixed it for me.,AMD,2025-12-11 07:43:52,1
Intel,nte7o94,"This doesn't work. We are talking about games that crash with or without it, the only difference being the older AMD driver working.",AMD,2025-12-11 01:44:41,4
Intel,ntc9ed0,I already install the latest update before update drivers its not update related. Vulkan driver is the problem in indina jones and silent hill 2 after windows update 25.11.1 not crashing ray tracing enabled but in 25.12.1 its broken again. So driver is the problem...,AMD,2025-12-10 19:27:04,2
Intel,ntamwm4,"They said earlier in 2025 they were working on FSR 4 support for RDNA 3, and then it leaked in September with the INT8 version...",AMD,2025-12-10 14:37:41,9
Intel,ntal44u,"They might as well have lol, they aint getting no new features",AMD,2025-12-10 14:27:29,14
Intel,ntbssdw,They also promised features to the few of us who bought 7900 XTX. Good luck defending them when it's your turn to be disappointed.,AMD,2025-12-10 18:06:58,3
Intel,ntbim9d,I expected them not to abandon their king card lmfao. Who does that,AMD,2025-12-10 17:17:26,2
Intel,ntar1pe,"Not really, they teased the possibility of including other architectures.",AMD,2025-12-10 15:00:06,1
Intel,ntimm5h,Maybe next time you should read the whole thread before replying.,AMD,2025-12-11 19:26:09,1
Intel,ntaqnxp,"It's also related to getting new features in generations other than just the latest one, ""bro"".",AMD,2025-12-10 14:58:07,1
Intel,nthyzs0,"I have the 7800 xt hellhound i F love it, tbh i care less about this redstone thing but its frustrating why a 2 year old lineup is abandoned all of a sudden",AMD,2025-12-11 17:30:59,2
Intel,ntbkahh,"> I'd double down on the consumer market to insulate from the impending bubble burst  If that bubble bursts nobody is going to have much money to spare for consumer goods. That bubble bursting will tank the entire economy along with it.  *Long* term that might work out better, though.",AMD,2025-12-10 17:25:41,3
Intel,ntbdi9g,further reminder amd is not your friend sadly,AMD,2025-12-10 16:52:07,9
Intel,nth472g,Same for me but Doom Eternal. I play at 4k and it needs upscaling at that res.,AMD,2025-12-11 14:58:42,1
Intel,ntfl1sl,What if I'm on RDNA 4?,AMD,2025-12-11 07:45:19,1
Intel,ntheoob,Yeah there are no good choices,AMD,2025-12-11 15:52:11,1
Intel,ntbmlpj,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html),AMD,2025-12-10 17:37:04,1
Intel,ntcu71s,Adrenalin is for GPUs.   Chipset is for CPU & mobo.,AMD,2025-12-10 21:11:19,1
Intel,ntb6pmb,"Yeah im sorry for all of us, already shopping for a 5080 rn…",AMD,2025-12-10 16:18:51,1
Intel,ntpptsg,Yes that’s the one. I have no idea where to turn lol,AMD,2025-12-12 21:31:29,2
Intel,ntnglvb,Sad news. Nvidia still supporting old extensions.,AMD,2025-12-12 14:38:54,2
Intel,ntbuxj0,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-12-10 18:17:20,1
Intel,ntdymrv,oh wow i haven't had any issues yet but that doesnt mean much. 25.11.1 i didnt have issues for a week or so.,AMD,2025-12-11 00:49:22,1
Intel,ntcmz77,"Interesting, I’ll test it today. I was crashing non stop on 25.11.1 so hopefully this update fixes it",AMD,2025-12-10 20:35:33,1
Intel,ntlyzyv,Same,AMD,2025-12-12 07:35:41,1
Intel,nv973go,"I have the same model GPU inconsequentially boosting well above the advertised clocks (nearly 3.4GHz) in both windows 10, 11 and fedora 43 with no issues.  This has been discussed several times on this community; whilst the clock behavior may surface other issues or instabilities on the system, it's not in itself the cause of problems.",AMD,2025-12-21 20:28:00,2
Intel,nw7vn10,"I actually have one more potentially related thing for you!   During the game I tried to turn the overlay on using my hotkey. Noticed it didn't. Since I've seen this before (we can call this a ""soft lock"") I tried to open the full screen experience with the hotkey. Which brought up my mouse (was using a controller in game before pressing the keys) but I could not move it...  My workaround has been: ctrl+alt+esc to task manager, tab to the search bar, type ""radeon"" and force kill the host service.  The instance I reported before this was a ""hard lock"" that I've noticed while trying to use my browser over a borderless game running, before this time where it was when the gpu wasn't under any actual load as far as I knew.  Glad to hear it's a known issue and not my hardware though... Thanks for getting back to me!",AMD,2025-12-27 17:14:03,1
Intel,nxaf1nm,Which driver version DOESNT have this issue?   I've tried going back all the way to .10 and it's all having the issue...,AMD,2026-01-02 17:46:29,1
Intel,ntg8mgs,6970 here,AMD,2025-12-11 11:36:12,1
Intel,ntc5yzi,Literally the one game I don't play lol,AMD,2025-12-10 19:10:11,9
Intel,ntb8cnz,"Yay, I own that one",AMD,2025-12-10 16:26:50,3
Intel,ntestwe,"It's not even out for Darktide yet either. Fatshark clarified that it's experimental and needs more work, so it's not in the live build",AMD,2025-12-11 03:56:23,3
Intel,ntazo47,So it's under the umbrella of the fsr4 override if I understood this correctly. For the fsr2 and 3.0 games I can use optiscaler right? Sorry I just bought a 9070xt coming from nvidia so I need to get used to these things.,AMD,2025-12-10 15:44:27,5
Intel,ntifqj7,I used to do that but a few games can use the FSR4 in driver upgrade.  The enhanced sync was nice too when it worked.,AMD,2025-12-11 18:52:16,2
Intel,ntbjqio,"Unfortunately I play games and run software that require Windows so I have it on a separate drive. When I do switch to it (and I update the driver to take advantage of new features), this shit typically happens along with a slew of forced updates.  You are right though, I do primarily run CachyOS.",AMD,2025-12-10 17:22:58,16
Intel,nte7ly0,found the linux user,AMD,2025-12-11 01:44:17,5
Intel,ntd5qr8,You're talking nonsense.  Engineer managing 2k endpoints and several hundred servers.,AMD,2025-12-10 22:07:29,4
Intel,ntcmjlp,Wasn't the dude's claim it has been always bugged with AMD,AMD,2025-12-10 20:33:21,6
Intel,ntctlcs,🌍👨‍🚀🔫👨‍🚀,AMD,2025-12-10 21:08:21,1
Intel,ntasjqd,It's barely an improvement.,AMD,2025-12-10 15:08:04,13
Intel,ntcmoxo,It's branding,AMD,2025-12-10 20:34:05,1
Intel,ntbkqjy,"Yes, RDNA4 refers to the RX9000 series.",AMD,2025-12-10 17:27:52,3
Intel,ntb0ccn,"Uninstalling the install manager brings back the ""check for updates"" functionality until you update again (and have to re-uninstall the install manager)",AMD,2025-12-10 15:47:46,14
Intel,ntp6j29,I have one of these captures if you want it (error code 0x00000119). I've been having a TON of driver timeouts and BSOD for the past couple of driver versions and I've had to roll back to October to resolve them. Seems like any app that has hardware acceleration enabled causes it and exasperated when viewing the system via RDP.,AMD,2025-12-12 19:48:47,1
Intel,ntap8zv,Let us know how it goes!,AMD,2025-12-10 14:50:37,9
Intel,ntci6s3,"I don't know how much of an impact this could have on perf since it's not something I've measured. I personally wouldn't do this, though. With a dGPU installed I keep iGFX off.",AMD,2025-12-10 20:11:40,4
Intel,nted5dt,"performance wise it should only be a couple frames of latency, when doing rendering on dgpu and going out through igpu it'll just copy over the frame buffers.   Main impact is on pcie bandwidth as it'll use up quite a lot there, and to a smaller degree RAM load, so you definitely don't want to run some other dynamic load on the igpu when gaming to overwhelm its pcie link. I think on 7000/9000 it's x8 so it may be fine? But I'm really not sure could be x4 too",AMD,2025-12-11 02:17:41,1
Intel,nth79az,"We're tracking a failure in silent hill 2 remake, I believe a fix is aligned to a future release. I'll need to check in with oblivion remastered",AMD,2025-12-11 15:15:04,2
Intel,ntdvql1,"Do you have to do that convoluted setup and download the drivers from Limewire, or has Optiscaler wrapped it in to their application?",AMD,2025-12-11 00:32:26,2
Intel,ntbvuyt,"So, no official release... ;(",AMD,2025-12-10 18:21:45,1
Intel,nte1rh2,Any tutorial for a noob on RDNA2?,AMD,2025-12-11 01:08:16,1
Intel,ntbfsb9,what workaround?,AMD,2025-12-10 17:03:21,4
Intel,ntbgebv,"Same issue with fsr4 on rdna1-3.   It shouldn't be this difficult, it's in a perfectly working state made possible by like one guy's few days worth of work.   And yet AMD just doesn't do it...",AMD,2025-12-10 17:06:24,2
Intel,ntbmhrj,FUG,AMD,2025-12-10 17:36:32,1
Intel,nwtucuk,"Oh, definitely not normal for sure... but I have this issue on multiple games and I did not have this issue on the 6080 it replaced. This seems to only be impacting my 9070.",AMD,2025-12-31 00:22:00,1
Intel,nte56dv,Enhanced sync makes games super stuttery even in  25.9.2.,AMD,2025-12-11 01:29:17,2
Intel,nteewr6,So I tried out enhanced sync and the game that I first noticed the issue on is no longer an issue. I've checked like 5 other games and no issues so far. Maybe it's fixed?,AMD,2025-12-11 02:28:10,1
Intel,nteepyk,I have my 5070ti build hooked up to my 240hz Ultrawide Oled because of Multi-Frame Generation. I was hoping that would be part of Redstone but it's not.,AMD,2025-12-11 02:27:04,1
Intel,ntczm93,"Such a relief, but i am also annoyed because they are ignoring 7000 series... I can literally use FSR 4.0.2 on my 7700XT and it is WAY better than FSR 3.1....",AMD,2025-12-10 21:37:51,2
Intel,ntcztos,I hope it is fixed for me as well 😭🙏. Thanks for the info.,AMD,2025-12-10 21:38:50,2
Intel,ntchg2c,yep would have upgraded but with an XTX.... you can cut your vram in 2/3 and have less Raster performance for a good upscaler and better RT performance it's such a stupid fucking problem....,AMD,2025-12-10 20:07:57,5
Intel,ntaq6sy,"That's not something I'm privy to, but it could be worth reaching out to them to request looking into if they're not already.",AMD,2025-12-10 14:55:36,31
Intel,ntbho9v,"I'm not privy to any of the FSR stuff - that's a different team to mine. I can pass on the feedback.  The Star Citizen EAC issue should be addressed, please let me know how it is.",AMD,2025-12-10 17:12:47,23
Intel,nte0zy9,i still am!   so many fixed issues out of the release notes that I felt the need to stick around and help clear things up in the communities I frequent. I'll go back into hiding again soon,AMD,2025-12-11 01:03:33,5
Intel,ntciqi1,"I've seen something like this over at OCUK Forums but weren't given enough data to work with. We've attempted to reproduce a corruption issue but apparently we've not been successful.  Can you give me a step by step breakdown on how to hit this, as well as a clear depiction of the issue?",AMD,2025-12-10 20:14:24,3
Intel,ntqc750,"No, XT peasants needs to form their own group.",AMD,2025-12-12 23:39:51,2
Intel,ntcxhw3,6800XT.,AMD,2025-12-10 21:27:27,6
Intel,ntanrvb,Some of their marketing said they would like to get it working if possible.,AMD,2025-12-10 14:42:33,10
Intel,ntarc4r,"There are already third party options, but it would be nice to see if Steam Machine drives INT8 FS4 support since it runs on RDNA 3 tech. Let's see what happens in 2026.",AMD,2025-12-10 15:01:37,3
Intel,ntbkv5b,Yeah there are going to be serious consequences as major retirement funds have invested in all these AI stocks because they have made so much money.,AMD,2025-12-10 17:28:30,3
Intel,ntflk2b,"Give it a try, for my 6800xt it's crashing in almost all games...  ![gif](giphy|QMHoU66sBXqqLqYvGO)",AMD,2025-12-11 07:50:12,1
Intel,ntsqrgy,"Sorry, out of curiosity, if you close it, it won't let you play? What do you get? Could you send me a photo so I can understand?",AMD,2025-12-13 11:11:24,1
Intel,ntnluq2,"I agree. Please can you raise a ticket requesting support for this over at our GPUOpen and ask other end users and developers to upvote it and leave a comment registering their interest? (please share a link to it here if you do) https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues  As far as I'm aware, the impacted titles are: IL-2 Sturmovik: 1946, Neverwinter Nights Diamond Edition and Call of Duty. If there are any others, I would really appreciate you letting us know.  E: I believe it's posted here: https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues/80",AMD,2025-12-12 15:06:46,2
Intel,nthltl0,"Just an update - I ended up running DDU and re-installing the latest update and now things are pretty stable, no driver timeouts from hardware accelerated apps either. Could be something to do with the architecture change between driver packages - but doing a complete removal between updates seems required now.",AMD,2025-12-11 16:26:46,1
Intel,ntcop7s,I never seen 1 crash on 25.11.1 although I did use the preview update for windows 11 last week which fixed some amd gpu related crashing and that solved my arc raiders random crashing,AMD,2025-12-10 20:44:09,1
Intel,nv9cafd,"My apologies then - it seems latest driver on Windows seems to be the source of issues then, seems more people have issues posting on /r/AMDHelp , also with 9070XT's. Seems all device hung errors and timeouts recently posted are with 25.12.1. I had no issues on cachyOS (Hyprland) running CS2 too, latest amdgpu.",AMD,2025-12-21 20:55:29,1
Intel,nxbs3rv,I believe this was introduced with the 25.20 driver branch. it shouldn't be present in 25.9.1/2,AMD,2026-01-02 21:39:50,1
Intel,ntb0k7p,"yes, 3.1 is where AMD adopted the same modular approach as nvidia so any game at fsr 3.1 or above just runs at whatever latest fsr version your driver supports, which is currently 4 although now the versions aren't numbered anymore",AMD,2025-12-10 15:48:50,6
Intel,ntcauqa,Hell yeah 👍🏻   Impressive you can run that on a 5x86,AMD,2025-12-10 19:34:18,4
Intel,ntbryby,"Since you're already an advanced user, perhaps you could block it from installing by selectively blocking AMD in your hosts or pi-hole? It's not a dumb solution, but it's better than having to deal with push-installs.",AMD,2025-12-10 18:02:57,3
Intel,nthi3lk,I might be an ass but I’m not wrong,AMD,2025-12-11 16:08:50,2
Intel,nthi8dc,Sorry  If you’re a **consumer** and want to be in control of what’s on your computer then Windows is not the OS for you,AMD,2025-12-11 16:09:28,2
Intel,ntfql24,"Yes, If you mean the bad frame pacing when fps is lower.  I still opt to spent 1-200 hrs of my gaming session with FSR 3 frame gen, 7900xtx.  It's not that bad when the output is close enough to monitor max hz, similar to what hardware unboxed did in thier test.  The generated frame still comes out too early but it has to wait for the monitor's nest refresh which is consistent.",AMD,2025-12-11 08:40:22,1
Intel,ntc2hr1,ty,AMD,2025-12-10 18:53:26,1
Intel,ntb1b5l,"u/amd_vik it sounds like this person doesnt want the manager to install again, but I am pretty sure you can do custom option to uncheck it. If you do express of course it will put it back sschuler.",AMD,2025-12-10 15:52:28,9
Intel,ntpa4lm,can you run analyze -v in windbg or fire it over to me via your preferred file sharing method?  I personally like to use https://send.vis.ee,AMD,2025-12-12 20:07:35,1
Intel,ntcew16,Can confirm this issue is fixed for me on 9800x3d + 9070xt (I had this issue on 25.11.1 and reverted to 25.10.2 until today) 👍,AMD,2025-12-10 19:55:01,8
Intel,ntb65up,"Seems to be working fine, though when I was installing the driver my igpu showed up separately from the dgpu in the installer with a download link. But when re-running it they both show under 25.12.1  Should I be installing some separate older driver for it to keep things like hw accel working or was that just some hiccup?",AMD,2025-12-10 16:16:11,6
Intel,ntaufrk,Oh great will also test after work it’s been headache since last driver update,AMD,2025-12-10 15:17:58,5
Intel,nthzjga,Thank you for taking the time to respond. This has been very frustrating.,AMD,2025-12-11 17:33:43,2
Intel,ntlgdax,"I'm sorry to comment directly to you here. Do you have any report about monster hunter wilds performance drops in recent GPU drivers ?    I'm using 9070xt.    I have to use version 25.3.1 to play wilds with no stutters, anything newer gives a lot of stutters in many places.",AMD,2025-12-12 05:02:06,1
Intel,ntjjshb,"Yeah you still have to download it on your own, the creator of Optiscaler already said they aren't going to bundle it probably due to the whole legality around it.",AMD,2025-12-11 22:14:22,1
Intel,ntbpcf0,"i saw a post that detailed how to essentially replace noise suppresion with the working version in newer drivers, you can probably find it here somewhere",AMD,2025-12-10 17:50:29,2
Intel,nwtxl54,yeah something’s definitely wrong. i’m assuming you’ve already tried ddu?,AMD,2025-12-31 00:39:33,1
Intel,nter7t7,worked fine for me idk,AMD,2025-12-11 03:45:44,1
Intel,nter4u6,still busted for me,AMD,2025-12-11 03:45:12,1
Intel,ntef1nl,"Also from what I can tell, enhanced sync is fixed at least on my end.",AMD,2025-12-11 02:29:00,1
Intel,ntbytau,Thank you for this! been waiting for a fix with Star citizen.,AMD,2025-12-10 18:35:58,6
Intel,ntcdxlc,Yeah SC seems to be working for now.,AMD,2025-12-10 19:49:59,7
Intel,ntcib7n,"Bonjour, pour le moment sur Star citizen le problème avec EAC fonctionne pour la 7900XT. Merci d avoir réglé le problème. Bonne fêtes de fin d'année.",AMD,2025-12-10 20:12:16,2
Intel,ntcro8s,That's good to hear. What about Noise Suppression not working since 25.9.2?,AMD,2025-12-10 20:58:42,1
Intel,ntcnbes,Hmm let me try. So pretty much having installed the latest driver (25.12.1) I just open SecondLife. I look closely at my avatar/character and my skin looks like this  [https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4](https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4) (excuse my outfit but just easier to show)  this is how it's supposed to look and also does on 25.9.1 [https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4](https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4)  I've heard that this doesn't occur on linux but only windows (But I don't have linux so can't say for sure)  I think you need PBR / Materials or some reflection on your skin to see the issue.   If you fly up to around 2000+ meters above ground it becomes easier to see  These are my settings [https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png](https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png)  I have an rx 7900XTX,AMD,2025-12-10 20:37:14,3
Intel,ntcnscv,"Hello! I am actually one of the developers on the client team for Second Life, and I have been trying to figure out how to get in touch - we have found at least one nasty bug on some of the Strix Halo chips with the current drivers.  Can you send me a message here so we can exchange emails?",AMD,2025-12-10 20:39:36,3
Intel,nu85qao,😭😭😭😭,AMD,2025-12-15 21:31:35,1
Intel,ntdy4eq,"I had a similar issue with my 6800xt and the other thing that helped was to sit it to fullscreen or borderless and swap back and forth. Now I'm only playing in fullscreen (which is annoying), but it doesnt crash anymore.",AMD,2025-12-11 00:46:19,1
Intel,ntf5uuk,I have the same card and exactly the same problem. Can't install newer drivers or BF6 just constantly crashes.  I'm on 25.10.2 tho,AMD,2025-12-11 05:30:42,1
Intel,ntapogt,"And it is, and they did, we have the leaked int8 version from September... Just needs official driver implementation now.",AMD,2025-12-10 14:52:54,6
Intel,ntu98tq,"Before the Black Ops 7 (which I don’t own) integration to Warzone, I could click off it & carry on. But since the integration it just closes the game.",AMD,2025-12-13 17:06:00,2
Intel,ntudqmy,"Yes, i have created this github issue.",AMD,2025-12-13 17:29:47,3
Intel,nva2mbl,"If those failures are avoided by clock limiting the board, the problem area could be a different domain entirely (CPU, memory, power, etc.).  The linux remark is interesting, it kind of calls back to similar failures with NV31 in certain apps like Helldivers 2; we had a little internal discussuon about how the amdgpu kernel driver managed to mostly avoid such issues, though I dont recall the outcome.  If you get the opportunity, I'd recommend a suite of system integrity routines as a sanity check; please take a look at [one of my older posts](https://old.reddit.com/r/Amd/comments/1l9ox9r/amd_software_adrenalin_edition_2562_optional/nn3yuay/) for some background.",AMD,2025-12-21 23:17:59,1
Intel,ntb3jrw,"They aren't numbered in the sense of like 4.0.2 or like there won't be an ""fsr 5""? Thank you very much btw, very helpful info!",AMD,2025-12-10 16:03:28,3
Intel,ntdazyo,Like a charm. :D,AMD,2025-12-10 22:34:45,1
Intel,ntbte7r,"I probably could, but AMD (and any other company, really) should be following the users preference anyways. It is a band aid fix and doesn't solve the problem.  Not a bad idea though.",AMD,2025-12-10 18:09:54,4
Intel,ntwpskf,I've been using computers since dos 3.  You're a spanner.  I'm sure MacOS is soooooo much more open.,AMD,2025-12-14 01:31:33,3
Intel,ntbtmtr,"Thank you for the idea, I just tried a custom install during an update, was given 2 choices (update/dont update driver and install/dont install privacy view). After installing drivers, step 2/2 was installing the install manager.    After updating through adrenaline using the custom option, I attempted reinstalling again using the auto-detect, custom install. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.   Installing via the WHQL package, custom install follows the same steps as above. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.",AMD,2025-12-10 18:11:03,1
Intel,ntpczrx,Here you go: [https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw](https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw)  I did run in windbg but I have no idea how to save the output unless you just want a copy + paste of it here haha,AMD,2025-12-12 20:22:52,1
Intel,ntci8tp,Appreciate the feedback,AMD,2025-12-10 20:11:57,6
Intel,ntb8e8t,Thank you for confirming.  That interesting though. I think the most seamless way to support products from both branches is to use the AMD auto detect tool. Can you tell me how the iGPU is represented in Windows' Device Manager?,AMD,2025-12-10 16:27:02,5
Intel,ntbkinx,"[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html)  https://i.redd.it/3vxsa8yave6g1.gif  If you suspect the installation is incorrect, download the package that includes the IGPU driver using the link provided above. The basic version does not include the IGPU driver, but provides a separate download option during installation.  Anyway, it seems like a lot of bugs have been fixed in this version.",AMD,2025-12-10 17:26:48,1
Intel,ntbxdgl,"if you can find it, you will be the goat",AMD,2025-12-10 18:29:03,3
Intel,nwv35gt,"This isn't every game, this is only some games. Not all games have a native vsync option either. That being said, from what I can find, this is a known issue.  https://steamcommunity.com/discussions/forum/1/601900047372731730/  https://www.wumeicn.com/screen-tearing-fix-for-rx-9070xt-and-freesync/",AMD,2025-12-31 04:49:26,1
Intel,nteuykg,"Yeah everything was fine until I tried to play Resident Evil 2 Remake, then the issue was back but not in the other games. This is a very odd issue.",AMD,2025-12-11 04:10:49,2
Intel,ntehipf,Nvm the issue I was having just happened in Resident Evil 2 Remake.,AMD,2025-12-11 02:43:56,1
Intel,ntciae3,Thank you for letting us know 👍,AMD,2025-12-10 20:12:10,7
Intel,ntcoi7s,appreciate the info. I'll ask our technicians to check in with the settings you've provided,AMD,2025-12-10 20:43:12,3
Intel,ntfvp58,I can confirm there is no issue in linux. A windows version running under proton in linux has no issues as well.   In the video there is flickering on head and body. I see only flickering on the head (when running it on the windows pc)  But my body has no layers attached - the body in the video usually comes with layers. But all heads have multiple transparent layers. The problem occurs even when that layers are not in use and are fully transparent.   Probably related.,AMD,2025-12-11 09:32:57,1
Intel,nte0wcf,"Hey there, thank you for reaching out!  I don't suppose it would be possible for one of our devrel folks to contact you via a linden lab email address like business@lindenlab.com?",AMD,2025-12-11 01:02:56,3
Intel,ntuvq16,"So if you click dismiss, the game closes, did I understand correctly? It doesn't let you enter the COD HQ ? I'm telling you this because I too should update the bios, in fact it happens to me too, but I click dismiss and it lets me play anyway.",AMD,2025-12-13 19:03:40,1
Intel,nva7np2,"for CS2, it was the newest driver that caused crashes exclusively, but on that driver I also got stronger boosts off the bat, hence it crashed faster. Now on 25.10.1(from windows update), COD still crashes with a black screen then tab to desktop with a driver timeout detected. Looking at afterburner(just using it to monitor clocks, no OC/UV applied or anything) the moment the GPU touches 3300+ I get thrown to the desktop. Can't even finish the training course even with ""speedrun strats"" before it crashes. It boosts [momentarily to 3300+](https://i.ibb.co/bgLFC0dp/coreclockcrash.png) and I get a screen freeze, crash, and sent to desktop with a driver timeout.   [These](https://send.vis.ee/download/103635cf66bdb907/#t2lRq409eeNwv6AaafhKJA) are both my crash report submissions. I'd go tomorrow over the stress tests, but I have managed to complete Time Spy/Steel Nomad without issues. And like I said, my system has has 0 issues before on a 2080ti.",AMD,2025-12-21 23:47:10,1
Intel,nvccr7w,"FYI, I passed [everything.](https://imgur.com/a/WyB9FeE)  This leaves the driver only. I made sure windows update didn't download its own driver this time, installed 25.12.1, still getting driver timeouts and crashes in games. I don't know what to tell you. Memtest86 also passed without any issues.",AMD,2025-12-22 09:07:04,1
Intel,ntb5lb9,"there won't be an ""fsr 5"" because any game with fsr implemented from here on out should, in theory, be compatible with every future version of fsr made, so numbering them isn't as meaningful. they're probably just going to stick with unofficial codenames like redstone for diffrentiation. Nvidia still uses versioning for DLSS despite it using the same system because it's good for marketing and diffrentiation so I'm not sure that dumping the version numbers is a wise decision but it also makes sense",AMD,2025-12-10 16:13:25,4
Intel,ntbuj2m,"I agree with you wholeheartedly, but super users do what they do best - sudo that shit. x)",AMD,2025-12-10 18:15:24,2
Intel,ntgknre,"I've never had AMD Chat or Privacy View force install, I hate they show up in the available software to install when updating, but I just dont click to install them lol, just update the gpu/chipset drivers",AMD,2025-12-11 13:06:05,1
Intel,ntpzp27,I guess a snippet of the faulting component from the output would work.  This is a minidump. Do you have a kernel memory dump>?,AMD,2025-12-12 22:25:23,1
Intel,ntqn26t,"sorry i missed this, seems it had expired. maybe someone downloaded it before i did?",AMD,2025-12-13 00:46:30,1
Intel,ntbdwdm,"Right now in devmgr with re-running the driver installer from the site things look like this https://u.numerlor.me/2faMBA . I also remembered adrenalin has full driver details and everything looks fine there https://u.numerlor.me/w1Snxw https://u.numerlor.me/EOclpA so I think it was just the installer being a bit confused.  Compared to the installer on the first screenshot, when doing the actual update (from inside adrenalin) the Radeon Graphics was a separate item, and had a ""Download driver"" or something along those with the link I mentioned",AMD,2025-12-10 16:54:02,4
Intel,ntbapq8,What about the combined exe? It's still available? That will install both gen but was bugged with control panel disappearing on previous driver.  The combined exe is around 1.6GB.,AMD,2025-12-10 16:38:26,3
Intel,ntc59vr,This might be it? Worth a shot I suppose.  Edit: This worked for me on the latest driver  [https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2025-12-10 19:06:45,3
Intel,nwvgoe4,"i don’t have this issue in any of the same games, but i have no idea what could be causing it in your setup and not mine though",AMD,2025-12-31 06:30:39,1
Intel,ntgiwq2,geenz@ but yes,AMD,2025-12-11 12:54:38,2
Intel,nvf2a9r,any news? SL are not updating their customers with anything constructive and it is affecting most of us.,AMD,2025-12-22 19:18:19,1
Intel,ntvblbb,"Hmm, when I can, I’ll have another look! Thanks!",AMD,2025-12-13 20:31:34,2
Intel,nvd9inn,"I see. Is this specific to CS2 or does it occur with other apps on your end?  We're presently tracking and working on TDRs in that game specifically, though I'm kind of worried in a way that clock limiting works around this failure.",AMD,2025-12-22 13:43:12,1
Intel,ntr6yhj,"I do not, only the minidump but I've uploaded it again here [https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN\_IxLREw](https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN_IxLREw)",AMD,2025-12-13 02:56:08,1
Intel,ntbd7hc,"Yes it should be fixed under that scenario, and the combined package is linked on the release notes:  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html  https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe  Kind of guessing here but I believe the '-c' towards the end of the file name denotes a combined package spanning RDNA support.",AMD,2025-12-10 16:50:38,5
Intel,nthusz8,This worked for me btw - did it a few days ago before these drivers dropped. When I update I'll be using the same method.,AMD,2025-12-11 17:10:36,2
Intel,nwwqpp5,"Are you running 4k in freesync on a 9000 series card?  I'm going by the radeon performance metric overlay saying minecraft/etc is using 300w power.  UE5 games are fine, games with an internal frame cap don't have an issue (well, they have their own frame pacing issues but that's not this).  I can always tell when framerate is going nuts because I can hear the squealing in my speakers when the gpu is at 100%. It's especially bad in menu's. If I turn off features/settings that improve quality or try a lower in game resolution, it gets much worse.",AMD,2025-12-31 13:15:38,1
Intel,nth5y4y,thanks a bunch. I'll pass this on to my ISV contact and see where we get with that.,AMD,2025-12-11 15:08:11,2
Intel,nvfir9a,You can find it here [https://github.com/secondlife/viewer/issues/5048](https://github.com/secondlife/viewer/issues/5048),AMD,2025-12-22 20:43:28,1
Intel,nub8ufp,news ?,AMD,2025-12-16 10:31:04,1
Intel,nve66vp,"COD is the greatest offender - I can't even get through the training course for Zombies without a black screen>driver timeout message, even if I try to speedrun it in a way (because I've attempted it so many times) it is inevitable it's going to crash, that one crashes with this [error](https://i.ibb.co/KjxynXH5/image.png).  Again, NO OC is applied. Other than the ram running at 2666, which as stated with both mem tests successful and went through both by Karhu's test and Memtest, have no issues. Including no issues with my previous GPU,2080ti, again. CS, I can't even start a match with friends because it'll inevitably crash randomly, sometimes it is within 5-10 mins, sometimes it is near instant in a couple of minutes. Tried everything from 25.12.2 to 25.9.1. PSU is a RM1000e, using the 12pin cable natively from the PSU. It is all the way in, this PSU I specifically even got for this GPU as I didn't want to use an adapter to power the card from all the experiences I've read with the 12pin + adapters.  Here is also a [video](https://www.youtube.com/watch?v=cSkaI6WSfJY) of it happening.",AMD,2025-12-22 16:38:34,1
Intel,ntvi492,"huh, that's odd. Do you have any larger files over at       C:\Windows\LiveKernelReports\WATCHDOG\",AMD,2025-12-13 21:08:32,1
Intel,ntbt0lr,"Installed the c one. And seems to be working fine. 780M and 6800 here. Still when selecting a specific GPU for a specific app, both energy saver and performance show 6800. This bug has been forever. And it's probably just a registry key when the driver install. Win11.",AMD,2025-12-10 18:08:05,5
Intel,nx0meck,"i’m using a 1440p freesync monitor, i basically always have fps counter on in all of my games so i can verify that vsync always works. even works without the freesync monitor. frame rate only ever goes uncapped when i disable vsync. is it only an issue at 4k?",AMD,2026-01-01 02:07:41,1
Intel,nwkq3wh,No updates there,AMD,2025-12-29 17:14:59,1
Intel,nuyojnk,I do actually have one in there that's 17MB from a BSOD yesterday caused by the AMD driver,AMD,2025-12-20 01:45:48,1
Intel,nwyqsn1,it's tagged as a milestone for feb,AMD,2025-12-31 19:33:08,1
Intel,nonhqm9,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",AMD,2025-11-13 16:12:15,78
Intel,nonf76t,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",AMD,2025-11-13 15:59:52,125
Intel,nonf5bq,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",AMD,2025-11-13 15:59:37,73
Intel,nooaz8h,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,AMD,2025-11-13 18:34:35,18
Intel,nonkrkq,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,AMD,2025-11-13 16:27:06,12
Intel,nonnn81,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too 😿.",AMD,2025-11-13 16:41:13,10
Intel,nonfuov,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,AMD,2025-11-13 16:03:01,18
Intel,noob2qb,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? I’ve spent 1 entire afternoon try every solutions given by Google but today the problem is still there…,AMD,2025-11-13 18:35:02,8
Intel,nopg6ma,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",AMD,2025-11-13 21:59:54,9
Intel,nonp8tv,So does this mean Arc Raiders will stop randomly crashing in Windows?,AMD,2025-11-13 16:49:00,11
Intel,nonw7rh,Just installed these zero issues so far!,AMD,2025-11-13 17:23:17,5
Intel,nondz23,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",AMD,2025-11-13 15:53:59,26
Intel,nonlldq,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,AMD,2025-11-13 16:31:10,5
Intel,noofqtg,There was a long delay with the blank screen. Made me a bit nervous,AMD,2025-11-13 18:57:20,4
Intel,noo2zob,At this point i'm sure that cyberpunk will never be fixed.,AMD,2025-11-13 17:56:32,10
Intel,noolxx3,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,AMD,2025-11-13 19:27:59,6
Intel,nonj6l9,No fix for being unable to enable Noise Suppression...,AMD,2025-11-13 16:19:20,8
Intel,nooktgl,When does Linux get this,AMD,2025-11-13 19:22:26,3
Intel,nop2o04,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",AMD,2025-11-13 20:52:28,3
Intel,noqem0g,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",AMD,2025-11-14 01:15:45,3
Intel,noqnucr,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",AMD,2025-11-14 02:10:59,3
Intel,nou4y1d,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,AMD,2025-11-14 17:01:23,3
Intel,nouw9o1,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",AMD,2025-11-14 19:17:52,3
Intel,novj51b,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",AMD,2025-11-14 21:17:52,3
Intel,npexfdr,Windows update keeps trying to update my driver.,AMD,2025-11-18 00:54:41,3
Intel,noo4qjo,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,AMD,2025-11-13 18:05:01,4
Intel,noniqz3,No FSR4 on RDNA3 no care,AMD,2025-11-13 16:17:12,18
Intel,noo25hd,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",AMD,2025-11-13 17:52:27,5
Intel,nonthc8,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,AMD,2025-11-13 17:09:50,2
Intel,nooud97,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,AMD,2025-11-13 20:10:18,2
Intel,nortjvj,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,AMD,2025-11-14 07:15:55,2
Intel,nos3s8s,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",AMD,2025-11-14 08:57:09,2
Intel,not49x9,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",AMD,2025-11-14 13:52:51,2
Intel,noux5p5,This driver was way better than the version before it(for me at least).,AMD,2025-11-14 19:22:23,2
Intel,novpivg,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",AMD,2025-11-14 21:50:55,2
Intel,np4ombz,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",AMD,2025-11-16 11:19:30,2
Intel,npp1qov,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,AMD,2025-11-19 16:55:57,2
Intel,nqawzsb,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",AMD,2025-11-23 03:42:39,2
Intel,npaw51d,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",AMD,2025-11-17 11:47:34,4
Intel,nondc4t,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",AMD,2025-11-13 15:50:55,7
Intel,nonmrak,"Brooooo, they didn‘t fix the flickering in BF6 when recording…",AMD,2025-11-13 16:36:52,2
Intel,nonvub9,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.  * Fucking LOL.,AMD,2025-11-13 17:21:27,2
Intel,nonmi72,25.10.2 completely broke vsync... not even a mention about this in the notes?,AMD,2025-11-13 16:35:38,2
Intel,noncnxo,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-13 15:47:39,1
Intel,nonguv3,There is new AFMF features too.,AMD,2025-11-13 16:07:56,1
Intel,nonmglo,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,AMD,2025-11-13 16:35:26,1
Intel,nonn4xw,bf6 fps drop fixed?,AMD,2025-11-13 16:38:44,1
Intel,nonvhb6,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,AMD,2025-11-13 17:19:40,1
Intel,noo456j,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",AMD,2025-11-13 18:02:08,1
Intel,noo651n,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",AMD,2025-11-13 18:11:49,1
Intel,nooad23,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",AMD,2025-11-13 18:31:40,1
Intel,nooeeia,How is the driver ? 7700 XT here.,AMD,2025-11-13 18:50:53,1
Intel,noojnun,Finally a potential fix for CPU metrics? Look forward to seeing if it’s true!,AMD,2025-11-13 19:16:39,1
Intel,noovdps,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,AMD,2025-11-13 20:15:26,1
Intel,noozjd6,do yall use ddu for every driver or do yall just update it with the app?,AMD,2025-11-13 20:36:35,1
Intel,noozq5o,"New AMD update 👏👏👏👏, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",AMD,2025-11-13 20:37:32,1
Intel,nop06vu,I just can’t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video I’m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,AMD,2025-11-13 20:39:55,1
Intel,nop4b7m,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",AMD,2025-11-13 21:00:46,1
Intel,nopfrqo,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,AMD,2025-11-13 21:57:50,1
Intel,nopilp6,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,AMD,2025-11-13 22:12:30,1
Intel,noplmto,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,AMD,2025-11-13 22:28:42,1
Intel,nopnmjz,Think this broke Vulkan in POE2,AMD,2025-11-13 22:39:24,1
Intel,nopyn4b,"Hi [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/), other players and myself are having problems using DXVK with latest drivers. From driver timeouts to black screen. I'm particularly having problems with Fallout New Vegas, but there is reports in other games. How can I help in fixing these issues? Examples: [https://github.com/doitsujin/dxvk/issues/4999](https://github.com/doitsujin/dxvk/issues/4999), [https://github.com/doitsujin/dxvk/issues/5204](https://github.com/doitsujin/dxvk/issues/5204), [https://github.com/doitsujin/dxvk/issues/4851](https://github.com/doitsujin/dxvk/issues/4851)",AMD,2025-11-13 23:41:55,1
Intel,noqjzdo,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",AMD,2025-11-14 01:48:12,1
Intel,nor6g8r,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,AMD,2025-11-14 04:06:42,1
Intel,nor7il2,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,AMD,2025-11-14 04:14:01,1
Intel,nor9p0f,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,AMD,2025-11-14 04:29:34,1
Intel,nord0sz,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,AMD,2025-11-14 04:54:17,1
Intel,norxf8j,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",AMD,2025-11-14 07:53:31,1
Intel,nos6z6k,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",AMD,2025-11-14 09:29:57,1
Intel,nos7i23,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:35:25,1
Intel,nos7vbg,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",AMD,2025-11-14 09:39:11,1
Intel,nosa7uh,subtract strong cats brave outgoing husky coordinated important rustic juggle   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-11-14 10:02:49,1
Intel,nosrlfs,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,AMD,2025-11-14 12:33:14,1
Intel,nosysjm,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",AMD,2025-11-14 13:20:27,1
Intel,not149u,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",AMD,2025-11-14 13:34:35,1
Intel,notb7lg,I'm glad the CPU metrics are showing again,AMD,2025-11-14 14:31:49,1
Intel,notcd57,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",AMD,2025-11-14 14:38:07,1
Intel,notlcun,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",AMD,2025-11-14 15:24:40,1
Intel,notm5ep,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),AMD,2025-11-14 15:28:36,1
Intel,notufou,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,AMD,2025-11-14 16:09:14,1
Intel,noue3ki,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",AMD,2025-11-14 17:47:54,1
Intel,nouooi9,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",AMD,2025-11-14 18:39:53,1
Intel,noutw0a,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 😅   What a fucking joke",AMD,2025-11-14 19:05:43,1
Intel,nov8foi,Shits been crashing my system since the update :( sapphire 7900xt,AMD,2025-11-14 20:21:15,1
Intel,novg42t,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",AMD,2025-11-14 21:02:02,1
Intel,nowdvrw,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,AMD,2025-11-15 00:14:02,1
Intel,noydj17,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",AMD,2025-11-15 09:37:48,1
Intel,noypu29,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",AMD,2025-11-15 11:42:58,1
Intel,noyv323,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",AMD,2025-11-15 12:27:57,1
Intel,nozb3zp,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",AMD,2025-11-15 14:16:00,1
Intel,noze8xv,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,AMD,2025-11-15 14:34:56,1
Intel,nozoxq5,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",AMD,2025-11-15 15:35:11,1
Intel,nozv077,У меня Мультимедиа контроллер выдает ошибку. Для этого устройства отсутствуют совместимые драйверы. (Код 28),AMD,2025-11-15 16:06:54,1
Intel,np0n0ro,Noise Suppression still broken. 3rd release without that functionality in a row.,AMD,2025-11-15 18:33:00,1
Intel,np0qihb,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalación del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",AMD,2025-11-15 18:50:19,1
Intel,np0sz88,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",AMD,2025-11-15 19:02:53,1
Intel,np2gy28,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",AMD,2025-11-16 00:44:12,1
Intel,np2igku,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,AMD,2025-11-16 00:53:10,1
Intel,np2iy25,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",AMD,2025-11-16 00:56:07,1
Intel,np2n7ns,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",AMD,2025-11-16 01:22:23,1
Intel,np2rc23,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,AMD,2025-11-16 01:46:41,1
Intel,np3zqgd,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me 🙏",AMD,2025-11-16 07:09:06,1
Intel,np4btup,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",AMD,2025-11-16 09:09:18,1
Intel,np4c4bj,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",AMD,2025-11-16 09:12:17,1
Intel,np59xsp,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,AMD,2025-11-16 14:05:46,1
Intel,np5tc80,Still not working AMD NOISE S,AMD,2025-11-16 15:57:27,1
Intel,np5w51d,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,AMD,2025-11-16 16:12:01,1
Intel,np6sb4d,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",AMD,2025-11-16 18:55:53,1
Intel,np75mw5,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,AMD,2025-11-16 20:02:08,1
Intel,np7fiy7,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,AMD,2025-11-16 20:52:53,1
Intel,np9tmrb,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",AMD,2025-11-17 05:29:10,1
Intel,npa497n,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",AMD,2025-11-17 07:05:30,1
Intel,npbc7th,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",AMD,2025-11-17 13:41:36,1
Intel,npbdww5,"Unfortunately, version 25.11.1 does not start with Windows.",AMD,2025-11-17 13:51:35,1
Intel,npcr8ua,Is AMD going to come up with another driver soon?,AMD,2025-11-17 18:04:51,1
Intel,npd465l,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",AMD,2025-11-17 19:07:02,1
Intel,npeqls5,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,AMD,2025-11-18 00:15:16,1
Intel,npgfe4k,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly 😞.",AMD,2025-11-18 07:12:00,1
Intel,npgq8pe,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",AMD,2025-11-18 09:03:29,1
Intel,npgujea,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",AMD,2025-11-18 09:49:30,1
Intel,nph1gio,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,AMD,2025-11-18 10:58:38,1
Intel,nphl085,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,AMD,2025-11-18 13:22:24,1
Intel,npikkr4,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",AMD,2025-11-18 16:26:09,1
Intel,npnxcnt,getting bsod randomly since 25.9.1 sad..,AMD,2025-11-19 13:20:00,1
Intel,npowfg1,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",AMD,2025-11-19 16:29:43,1
Intel,npwkypv,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",AMD,2025-11-20 20:27:52,1
Intel,nq842b2,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",AMD,2025-11-22 17:50:50,1
Intel,nq9u9z3,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",AMD,2025-11-22 23:33:52,1
Intel,nqwbryc,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,AMD,2025-11-26 15:56:06,1
Intel,ns8k1w2,"Sorry but for me the drive give me crash pop up message every time i boot up my pc. Also just right now i got a freeze, black screen to all my monitors.",AMD,2025-12-04 12:42:18,1
Intel,ns9soky,The worst driver this year so far,AMD,2025-12-04 16:45:18,1
Intel,nscxupo,"Still havent fixed the noise cancellation lmao, guess its another month+ of old version :) Thanks amd, truly doing wonders.",AMD,2025-12-05 02:44:53,1
Intel,nsgsekn,CS2 crashing with driver timeout after tabbing out or watching streams on 2nd screen 7900xtx,AMD,2025-12-05 18:29:05,1
Intel,nsqr7j8,"When is 25.12.1 coming out? I have read only bad things about 25.11.1 here, so I wanted to skip this one.",AMD,2025-12-07 10:40:19,1
Intel,nonf78x,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",AMD,2025-11-13 15:59:52,0
Intel,nond6d4,So no redstone yet,AMD,2025-11-13 15:50:09,2
Intel,nonqjy0,FSR AI frame gen??? Didn’t they say that’d it would also have a driver toggle?,AMD,2025-11-13 16:55:25,1
Intel,nonv0vm,Did AMD ever add support for Cronos?,AMD,2025-11-13 17:17:25,1
Intel,nonxx39,Well Star Citizen will load now!  Now some longer term testing....,AMD,2025-11-13 17:31:41,1
Intel,nonw8zf,Anybody tried this with Anno 117 yet? I’m hoping it helps performance,AMD,2025-11-13 17:23:27,0
Intel,nooyqhv,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,AMD,2025-11-13 20:32:28,0
Intel,nooyuwp,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,AMD,2025-11-13 20:33:06,0
Intel,noqrxh3,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,AMD,2025-11-14 02:35:08,0
Intel,nozwu6t,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,AMD,2025-11-15 16:16:36,0
Intel,np07ekg,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,AMD,2025-11-15 17:12:12,0
Intel,noni2qa,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,AMD,2025-11-13 16:13:55,-4
Intel,noqc54j,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,AMD,2025-11-14 01:00:45,-1
Intel,np4dff7,"Here we go again, jetzt stürzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen außer XMP war aktiviert, dann stürzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das übernehmen müsst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team Grün nicht.",AMD,2025-11-16 09:25:48,-1
Intel,nonpv4u,Yeah same here LG c5 42inch 😰,AMD,2025-11-13 16:52:03,21
Intel,noockre,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesn’t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",AMD,2025-11-13 18:42:09,18
Intel,noprnhq,"I have this but on display port, HDMI works fine",AMD,2025-11-13 23:01:25,5
Intel,nonyety,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",AMD,2025-11-13 17:34:06,10
Intel,nopqt8d,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",AMD,2025-11-13 22:56:47,2
Intel,nonu691,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,AMD,2025-11-13 17:13:13,2
Intel,nop2vm5,I have the same issue with display port but it’s okay with hdmi :/,AMD,2025-11-13 20:53:31,1
Intel,nq0dwdl,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",AMD,2025-11-21 12:22:58,1
Intel,nonpu8n,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",AMD,2025-11-13 16:51:56,83
Intel,nonjytd,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,AMD,2025-11-13 16:23:11,28
Intel,noo9nj4,"V25.10.2  here… I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",AMD,2025-11-13 18:28:20,4
Intel,not85q8,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",AMD,2025-11-14 14:14:59,3
Intel,nonkdfa,combined again it looks like 🤷‍♂️,AMD,2025-11-13 16:25:10,3
Intel,nongchq,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,AMD,2025-11-13 16:05:27,100
Intel,nono7wt,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",AMD,2025-11-13 16:44:00,3
Intel,nonhdck,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",AMD,2025-11-13 16:10:27,5
Intel,np0qz7g,You try install last chipset driver ?,AMD,2025-11-15 18:52:41,1
Intel,nongxu7,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",AMD,2025-11-13 16:08:21,1
Intel,nonrxcq,So it's the driver that's why that happens 😡 and it's not fixed?,AMD,2025-11-13 17:02:09,0
Intel,noogyei,Thank you for your service,AMD,2025-11-13 19:03:14,8
Intel,nopxjjg,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",AMD,2025-11-13 23:35:26,8
Intel,nov7gjn,Any update mate?,AMD,2025-11-14 20:16:03,1
Intel,nosfu5h,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",AMD,2025-11-14 10:57:06,0
Intel,nonw38z,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",AMD,2025-11-13 17:22:40,5
Intel,npdh2mf,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",AMD,2025-11-17 20:11:40,3
Intel,np42etk,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",AMD,2025-11-16 07:35:21,2
Intel,noroh5d,"I'm the opposite, I just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",AMD,2025-11-14 06:29:26,1
Intel,nonifp9,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDU’d it again to go back to 25.9.2 since games were stuttering.",AMD,2025-11-13 16:15:40,13
Intel,noozgtx,Same.,AMD,2025-11-13 20:36:13,3
Intel,nop6flo,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",AMD,2025-11-13 21:11:26,3
Intel,noxqsoq,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",AMD,2025-11-15 05:51:40,1
Intel,nosbqvm,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,AMD,2025-11-14 10:17:54,1
Intel,nozhfiv,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,AMD,2025-11-15 14:53:42,1
Intel,nopl6z7,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",AMD,2025-11-13 22:26:21,1
Intel,noppntf,If it still crashes set RTX Global Illumination to Static.,AMD,2025-11-13 22:50:26,5
Intel,nor7jw2,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",AMD,2025-11-14 04:14:16,2
Intel,nonlw78,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",AMD,2025-11-13 16:32:40,11
Intel,notyc45,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as they’ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since they’re much deeper in engine code/inputs.",AMD,2025-11-14 16:28:32,2
Intel,noocnzc,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it 🤓",AMD,2025-11-13 18:42:35,1
Intel,noo416z,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",AMD,2025-11-13 18:01:35,1
Intel,not2qjr,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",AMD,2025-11-14 13:44:02,2
Intel,nopbmoh,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,AMD,2025-11-13 21:37:14,2
Intel,nosw536,Ugh,AMD,2025-11-14 13:03:59,2
Intel,nooumki,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",AMD,2025-11-13 20:11:38,3
Intel,noqgvkg,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",AMD,2025-11-14 01:29:31,1
Intel,nos3g9h,"Linux doesn’t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, that’s when you get driver updates, and they’re completely different from windows branch.",AMD,2025-11-14 08:53:47,1
Intel,noqn7n2,just uninstall it I prefer manual check myself.,AMD,2025-11-14 02:07:15,2
Intel,nor7u07,So AMDs default driver overclocks and doesn’t reflect that in the values?,AMD,2025-11-14 04:16:14,1
Intel,nqsncxf,Same issues here i underclocked it but this new update just made it worse,AMD,2025-11-26 00:03:57,1
Intel,np5tu2z,ok it is still crashing ... complete reboot :(,AMD,2025-11-16 16:00:03,1
Intel,nq4e73q,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",AMD,2025-11-22 01:28:20,1
Intel,nope0rx,Okay.,AMD,2025-11-13 21:49:03,1
Intel,nonl1up,I’m hoping Valve’s new steam machine will push them on that since it’s RDNA3 based.,AMD,2025-11-13 16:28:30,19
Intel,noukbhw,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,AMD,2025-11-14 18:18:29,3
Intel,nooggfu,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",AMD,2025-11-13 19:00:46,4
Intel,noozx8g,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,AMD,2025-11-13 20:38:33,1
Intel,noptibm,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",AMD,2025-11-13 23:12:01,1
Intel,noo53y9,welcome to amd,AMD,2025-11-13 18:06:50,0
Intel,nonvvaf,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,AMD,2025-11-13 17:21:35,1
Intel,nooca2m,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",AMD,2025-11-13 18:40:45,1
Intel,np1vdc1,Same. Never even had Ryzen master installed.,AMD,2025-11-15 22:35:21,2
Intel,npiam42,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",AMD,2025-11-18 15:37:33,1
Intel,nowsbia,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,AMD,2025-11-15 01:45:44,1
Intel,nonegtb,What is redstone?,AMD,2025-11-13 15:56:21,4
Intel,nonnq47,What's weird is Black Ops 7 has ray regeneration.,AMD,2025-11-13 16:41:37,5
Intel,none418,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",AMD,2025-11-13 15:54:38,2
Intel,nontlx6,vsync issue fixed with win 11 KB5068861 update.,AMD,2025-11-13 17:10:27,12
Intel,nonxa48,had no issues with vsync on 25.10.2,AMD,2025-11-13 17:28:33,4
Intel,nons4sz,works fine for me,AMD,2025-11-13 17:03:10,7
Intel,noorn1m,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,AMD,2025-11-13 19:56:34,1
Intel,nopcb8w,"That it did, lol. My only complaint.",AMD,2025-11-13 21:40:35,1
Intel,noqh6ym,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",AMD,2025-11-14 01:31:26,0
Intel,nonl36f,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,AMD,2025-11-13 16:28:41,3
Intel,noovbth,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,AMD,2025-11-13 20:15:10,2
Intel,nonpyiq,"Fps drop over time? That's a game issue, it's got a memory leak",AMD,2025-11-13 16:52:31,6
Intel,nopz2ou,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",AMD,2025-11-13 23:44:30,1
Intel,nov7ewn,I’d settle for bf6 going one entire game without drivers crashing the game and freezing pc,AMD,2025-11-14 20:15:48,1
Intel,noorxgl,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",AMD,2025-11-13 19:58:00,1
Intel,nov7k59,Crashes?,AMD,2025-11-14 20:16:34,1
Intel,nowyxe0,I have this problem in all games.,AMD,2025-11-15 02:28:26,1
Intel,nprco16,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",AMD,2025-11-20 00:04:27,1
Intel,not2cbd,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",AMD,2025-11-14 13:41:44,1
Intel,np3fwq0,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",AMD,2025-11-16 04:25:12,1
Intel,norugaj,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",AMD,2025-11-14 07:24:32,3
Intel,nopmfkv,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,AMD,2025-11-13 22:33:01,2
Intel,noscuea,Epic version runs just fine.,AMD,2025-11-14 10:28:45,3
Intel,not9drm,Cyberpunk GOG last version patch runs fine on this driver.,AMD,2025-11-14 14:21:42,1
Intel,nosnl0p,"Hey there, can you give an example of how this looks now versus how it's supposed to?",AMD,2025-11-14 12:03:38,2
Intel,noso7o5,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,AMD,2025-11-14 12:08:26,1
Intel,nou0ebb,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,AMD,2025-11-14 16:38:48,1
Intel,nox9yy0,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,AMD,2025-11-15 03:41:47,2
Intel,nou7nae,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,AMD,2025-11-14 17:15:04,2
Intel,noypui8,"The game is booting, this message was for the 25.10 they just didn't removed it",AMD,2025-11-15 11:43:04,2
Intel,noza0c5,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,AMD,2025-11-15 14:09:14,1
Intel,np31dy5,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,AMD,2025-11-16 02:48:35,1
Intel,npgrqyr,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",AMD,2025-11-18 09:19:37,1
Intel,nqit1yt,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",AMD,2025-11-24 12:56:50,1
Intel,nsv6cts,"Good call, it caused nothing but problems for me and pretty severe. Were talking driver timeouts with black screens and even a couple bluescreens.",AMD,2025-12-08 01:50:11,1
Intel,nonny1j,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",AMD,2025-11-13 16:42:40,5
Intel,nopggve,My 9070 xt crushes while I try to use fsr 4 on new drivers,AMD,2025-11-13 22:01:20,1
Intel,noo04cb,Why don't you try it and let us know if you can. Would be helpful for lots of us,AMD,2025-11-13 17:42:31,1
Intel,nont8g8,It's in Redstone. Still not out yet,AMD,2025-11-13 17:08:37,4
Intel,nopd6c2,Didn't work for me...,AMD,2025-11-13 21:44:51,1
Intel,not23h8,Wait until you see how much your browser's cache is churning...,AMD,2025-11-14 13:40:17,2
Intel,notlyfp,Why cant you use Adrenalin? I'm using it on 25.9.1,AMD,2025-11-14 15:27:40,1
Intel,nq0kohy,I just received a windows extension update for my LG monitor. If you can boot up go check.,AMD,2025-11-21 13:08:34,1
Intel,nopw101,The last time I had this problem it was a RAM issue.,AMD,2025-11-13 23:26:32,4
Intel,npd560g,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,AMD,2025-11-17 19:11:57,1
Intel,norotfv,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",AMD,2025-11-14 06:32:31,1
Intel,nood411,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,AMD,2025-11-13 18:44:41,10
Intel,noo4uio,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",AMD,2025-11-13 18:05:33,5
Intel,nooyzy7,Do u reintall already up to date chipset drivers?,AMD,2025-11-13 20:33:49,1
Intel,norplxi,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,AMD,2025-11-14 06:39:36,1
Intel,nonuzmx,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",AMD,2025-11-13 17:17:15,3
Intel,nonxvx2,doing so (separation) will create a freak out shitstorm part 2.,AMD,2025-11-13 17:31:31,13
Intel,nonzgmu,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,AMD,2025-11-13 17:39:17,15
Intel,nonz6zk,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),AMD,2025-11-13 17:37:57,10
Intel,nons9ct,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,AMD,2025-11-13 17:03:49,19
Intel,nooofaj,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",AMD,2025-11-13 19:40:29,2
Intel,nony71m,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,AMD,2025-11-13 17:33:02,16
Intel,nonscqs,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,AMD,2025-11-13 17:04:17,1
Intel,nonuj7z,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,AMD,2025-11-13 17:15:00,22
Intel,nonzcwc,Thank you for communicating,AMD,2025-11-13 17:38:47,12
Intel,nononki,Unfortunately happens to me too. So for me it’s a big issue as I can’t update to this driver until it is fixed 😰,AMD,2025-11-13 16:46:06,5
Intel,nooyj1v,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.  Issue goes away using a non 4k 240hz display.     I believe this system crash is deeply related to DSC on Windows.  I only got these two PC bsods when I bought a 4k 240hz display.  Returned a monitor (bad oled) and the issue went away.  Got a new oled a few weeks ago and now I have these bsods again.     Never had a bsod before I got these 4k 240hz displays.  Fresh Windows 11 installs too between both PCs and between my first and second oled.  Systems are both solid and stable.     Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.  Hopefully someone else had experience with them on 4k 240hz.",AMD,2025-11-13 20:31:24,6
Intel,nons1mi,Thank you AMD my bad for getting upset,AMD,2025-11-13 17:02:44,5
Intel,nongngq,Thank you.,AMD,2025-11-13 16:06:56,2
Intel,noobm6s,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why it’s failing. Would be cool to see the technical details if that’s possible. (I’m actually more interested now on why it’s not working vs just getting it fixed).,AMD,2025-11-13 18:37:36,2
Intel,nop1khf,Thank you!,AMD,2025-11-13 20:46:53,2
Intel,nop0wol,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,AMD,2025-11-13 20:43:34,1
Intel,nonlavb,Redstone when?,AMD,2025-11-13 16:29:44,0
Intel,nonhqde,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",AMD,2025-11-13 16:12:13,15
Intel,nonlm2a,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,AMD,2025-11-13 16:31:16,12
Intel,nonz8d7,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",AMD,2025-11-13 17:38:09,6
Intel,nonjrum,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",AMD,2025-11-13 16:22:14,4
Intel,nonovmq,Non pc monitor tvs are sometimes cheaper especially for larger sizes. I’m on lg c5 oled 42inch and it only has hdmi…,AMD,2025-11-13 16:47:11,4
Intel,noo0hf6,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",AMD,2025-11-13 17:44:18,3
Intel,nonq4py,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",AMD,2025-11-13 16:53:21,2
Intel,noolj45,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",AMD,2025-11-13 19:25:57,1
Intel,noqqvuj,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",AMD,2025-11-14 02:28:55,1
Intel,nosnlnp,> Are y'all playing on televisions?  Do you guys not have phones?,AMD,2025-11-14 12:03:46,0
Intel,nonhcwn,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,AMD,2025-11-13 16:10:23,2
Intel,noqf5pn,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,AMD,2025-11-14 01:19:04,6
Intel,noqno3l,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",AMD,2025-11-14 02:09:58,1
Intel,nov6ye9,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",AMD,2025-11-14 20:13:22,1
Intel,noyds7c,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",AMD,2025-11-15 09:40:31,1
Intel,nsoev4p,Why does it seem like driver quality/support has gotten substantially worse this past decade? Are we running out of skilled software engineers or is hardware just getting too out of hand?,AMD,2025-12-06 23:58:22,1
Intel,noshsep,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,AMD,2025-11-14 11:15:06,5
Intel,nonx5ls,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",AMD,2025-11-13 17:27:56,1
Intel,np52n5a,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",AMD,2025-11-16 13:17:38,3
Intel,np1d4kt,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",AMD,2025-11-15 20:53:05,2
Intel,nonis5q,OK thought I was the only one. 25.10 is bad bad,AMD,2025-11-13 16:17:22,5
Intel,nood354,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",AMD,2025-11-13 18:44:34,1
Intel,nos072w,Thanks for testing it,AMD,2025-11-14 08:20:41,1
Intel,np22kzb,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",AMD,2025-11-15 23:18:12,1
Intel,nosh56b,I thought FSR 4 was only on RDNA 4? 🤔,AMD,2025-11-14 11:09:11,1
Intel,nozuikm,My thoughts exactly. Thanks.,AMD,2025-11-15 16:04:18,1
Intel,nopzlun,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,AMD,2025-11-13 23:47:39,2
Intel,noq7kwh,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,AMD,2025-11-14 00:33:58,2
Intel,not1lyv,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,AMD,2025-11-14 13:37:28,1
Intel,noojjne,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",AMD,2025-11-13 19:16:05,1
Intel,nooj67g,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",AMD,2025-11-13 19:14:14,2
Intel,nopbvvl,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,AMD,2025-11-13 21:38:30,5
Intel,nouxgnr,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,AMD,2025-11-14 19:23:57,1
Intel,norm1yc,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",AMD,2025-11-14 06:08:00,3
Intel,npawrxf,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",AMD,2025-11-17 11:52:53,1
Intel,noo4anu,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,AMD,2025-11-13 18:02:52,6
Intel,nosbylr,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",AMD,2025-11-14 10:20:00,0
Intel,nphuo0h,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game 👀?,AMD,2025-11-18 14:14:35,1
Intel,noojmmw,Fun fact - i am dual booting and on Linux this bug is not existent...:)),AMD,2025-11-13 19:16:29,2
Intel,noockb2,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",AMD,2025-11-13 18:42:05,2
Intel,nqw6fd5,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,AMD,2025-11-26 15:29:31,1
Intel,noznq2r,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",AMD,2025-11-15 15:28:41,1
Intel,nongh5i,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,AMD,2025-11-13 16:06:05,7
Intel,noozl1i,It's a thing you can search for on Google,AMD,2025-11-13 20:36:49,1
Intel,nonsdkw,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,AMD,2025-11-13 17:04:24,3
Intel,nonm0k8,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",AMD,2025-11-13 16:33:16,0
Intel,nonxcza,ahh i'm on Win 10 so probably why I didn't see it.,AMD,2025-11-13 17:28:57,2
Intel,noorjtx,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,AMD,2025-11-13 19:56:07,2
Intel,nonn11b,"Yes, but was it in the previous WHQL driver ? I'm not sure.",AMD,2025-11-13 16:38:12,1
Intel,nons5g1,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),AMD,2025-11-13 17:03:16,2
Intel,nos7pyk,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:37:40,1
Intel,nqrxf23,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",AMD,2025-11-25 21:39:49,1
Intel,nov8osy,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",AMD,2025-11-14 20:22:35,1
Intel,np3gi86,Either launch with curseforge or rollback,AMD,2025-11-16 04:29:38,1
Intel,nopmt4r,"Damn, didn’t work for me last driver either. I can get FSR4 to work in other games just not BF6",AMD,2025-11-13 22:35:02,1
Intel,nr5w3fb,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,AMD,2025-11-28 03:55:10,1
Intel,noxy5g3,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",AMD,2025-11-15 07:00:01,1
Intel,noz0zh9,You 100 procent sure on this?,AMD,2025-11-15 13:12:06,1
Intel,np59d5s,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,AMD,2025-11-16 14:02:08,1
Intel,np32vom,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",AMD,2025-11-16 02:57:54,2
Intel,npgs1he,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic card🤡,AMD,2025-11-18 09:22:45,1
Intel,nswxbvi,"I had randomly black screens with 24.2.1, this was annoying as hell. Had to DDU the Driver and went back to 23.11.1, after this everything was fine.",AMD,2025-12-08 10:15:23,1
Intel,nonplo5,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,AMD,2025-11-13 16:50:46,1
Intel,noolo6b,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",AMD,2025-11-13 19:26:39,2
Intel,nov5qbd,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,AMD,2025-11-14 20:06:52,1
Intel,nq588dp,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,AMD,2025-11-22 04:54:10,1
Intel,noo85c3,They do not.,AMD,2025-11-13 18:21:19,5
Intel,np5srze,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",AMD,2025-11-16 15:54:29,1
Intel,noo8tps,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,AMD,2025-11-13 18:24:30,11
Intel,noo2nnu,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,AMD,2025-11-13 17:54:54,8
Intel,noolzkz,"AND is taking away one additional driver feature per day, you say?",AMD,2025-11-13 19:28:13,1
Intel,nooy45h,"Yes, I’m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select “GPU” you get a file that has a different dimension from the one you download if you choose “CPU”. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with “minimal_install), but Adrenalin App does not open.",AMD,2025-11-13 20:29:17,5
Intel,noo1i55,Thank you for explaining it before the rage baiters go nuts.,AMD,2025-11-13 17:49:17,1
Intel,noo3cx3,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,AMD,2025-11-13 17:58:18,46
Intel,nooncln,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",AMD,2025-11-13 19:35:04,3
Intel,noo0xcy,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",AMD,2025-11-13 17:46:27,13
Intel,nopu61n,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",AMD,2025-11-13 23:15:46,6
Intel,nopc45s,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",AMD,2025-11-13 21:39:38,2
Intel,noo3fsu,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,AMD,2025-11-13 17:58:41,25
Intel,npp1edb,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,AMD,2025-11-19 16:54:16,1
Intel,nopvrx5,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,AMD,2025-11-13 23:25:04,3
Intel,noo53xx,Already launched in COD 7,AMD,2025-11-13 18:06:50,3
Intel,nonp7d7,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so you’re saying i shoulf switch to hdmi?",AMD,2025-11-13 16:48:48,3
Intel,nonwqs3,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,AMD,2025-11-13 17:25:53,1
Intel,norbib0,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,AMD,2025-11-14 04:42:59,1
Intel,noni0s3,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",AMD,2025-11-13 16:13:39,1
Intel,nor1k1x,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,AMD,2025-11-14 03:34:28,1
Intel,nonzc4h,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",AMD,2025-11-13 17:38:40,2
Intel,noshb1m,With the compiled leaked DLL you can use it on RDNA3 as well.,AMD,2025-11-14 11:10:42,1
Intel,nosbtoj,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",AMD,2025-11-14 10:18:39,2
Intel,notnotg,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,AMD,2025-11-14 15:36:20,1
Intel,noqg8tt,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),AMD,2025-11-14 01:25:43,4
Intel,noonewp,Thank you! Exciting keen to see what it’s like,AMD,2025-11-13 19:35:23,1
Intel,noosgem,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",AMD,2025-11-13 20:00:38,2
Intel,nopjngc,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,AMD,2025-11-13 22:18:03,2
Intel,nphu5po,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",AMD,2025-11-18 14:11:53,1
Intel,npb27so,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,AMD,2025-11-17 12:35:58,1
Intel,nopnm90,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,AMD,2025-11-13 22:39:21,1
Intel,nqw82e8,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",AMD,2025-11-26 15:37:49,2
Intel,nozpqvb,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",AMD,2025-11-15 15:39:28,1
Intel,nonmi38,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",AMD,2025-11-13 16:35:37,5
Intel,nony81v,lmao chill out dude go touch some grass,AMD,2025-11-13 17:33:11,6
Intel,nonozwd,Could be grounds for lawsuit… That’s funny!,AMD,2025-11-13 16:47:47,4
Intel,norvwn6,Because of MPO.,AMD,2025-11-14 07:38:45,4
Intel,noq77oq,yeah same with 25.11.1 25.9.2 works for me,AMD,2025-11-14 00:31:52,1
Intel,nonv3ns,"25.10.2 was the previous WHQL, so also yes :P",AMD,2025-11-13 17:17:48,2
Intel,noo30h0,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",AMD,2025-11-13 17:56:38,1
Intel,noxixmd,Which driver version and does it still crashing?,AMD,2025-11-15 04:47:44,1
Intel,nopn1gw,OK I will install it now and test it and get back to you. Give me 10 mins.,AMD,2025-11-13 22:36:16,2
Intel,noppoge,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,AMD,2025-11-13 22:50:32,2
Intel,nr7hjjv,I'll work with the engineer from that ticket check if that issue has somehow regressed.,AMD,2025-11-28 12:30:29,2
Intel,nrptkfo,We've not been able to reproduce this internally so far. Can you remind me which GPU (was this a 7900XTX?) + connectivity method you're using?,AMD,2025-12-01 14:48:30,1
Intel,noztt6w,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,AMD,2025-11-15 16:00:37,2
Intel,np0adh6,"Yup just need to say ""No""",AMD,2025-11-15 17:28:00,2
Intel,npgto9y,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",AMD,2025-11-18 09:40:23,1
Intel,nonps7q,I don't see how it would work on 23.9.1 lol,AMD,2025-11-13 16:51:40,-1
Intel,nov73co,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",AMD,2025-11-14 20:14:06,1
Intel,noprr9f,I did it this morning before the new driver and confirm chipset drivers were untouched,AMD,2025-11-13 23:02:01,3
Intel,noooxx5,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and don’t use the latest drivers. At least AMD owned up to it so I can’t be too upset but hopefully they really do fix this soon as new users may not understand what’s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly it’s stable for them and they don’t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs don’t always have a DP connector at all.,AMD,2025-11-13 19:43:06,4
Intel,noo4q8p,"ah, that explains it. Thanks. :)",AMD,2025-11-13 18:04:59,1
Intel,nooab1c,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named “minimal install”). Obviously I’m referring to AMD driver download page.",AMD,2025-11-13 18:31:24,1
Intel,nop73kl,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",AMD,2025-11-13 21:14:50,1
Intel,novl7li,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",AMD,2025-11-14 21:28:30,1
Intel,noo4i0q,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",AMD,2025-11-13 18:03:52,22
Intel,noo7r27,What about Noise Suppression not working since 25.9.2?,AMD,2025-11-13 18:19:27,6
Intel,np8f5i6,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,AMD,2025-11-17 00:06:17,2
Intel,nopub91,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",AMD,2025-11-13 23:16:35,1
Intel,noo3ufw,Hell yeah 🙂 amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,AMD,2025-11-13 18:00:40,7
Intel,norjsvf,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",AMD,2025-11-14 05:48:36,2
Intel,nonrmjz,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",AMD,2025-11-13 17:00:38,10
Intel,noo81ru,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,AMD,2025-11-13 18:20:51,6
Intel,nosmcf6,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",AMD,2025-11-14 11:53:57,3
Intel,noniq65,That was my very first actual driver issue I experienced with AMD.,AMD,2025-11-13 16:17:06,3
Intel,noshl11,Oh that's nice! I'll look into it when I get the chance.,AMD,2025-11-14 11:13:15,1
Intel,nosh6j0,Cool. Thank you,AMD,2025-11-14 11:09:32,1
Intel,notszvs,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience 😖,AMD,2025-11-14 16:02:13,1
Intel,not1h8l,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,AMD,2025-11-14 13:36:42,2
Intel,npb8iqb,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",AMD,2025-11-17 13:18:49,1
Intel,npbldor,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",AMD,2025-11-17 14:34:50,1
Intel,noprwpb,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,AMD,2025-11-13 23:02:52,2
Intel,nonmz2f,"Fair enough, and yeah sooner the better for all of us",AMD,2025-11-13 16:37:55,0
Intel,noofit0,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,AMD,2025-11-13 18:56:16,-1
Intel,nozg3tu,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",AMD,2025-11-15 14:45:59,1
Intel,nopq3va,Fingers crossed,AMD,2025-11-13 22:52:53,1
Intel,nsxlbh0,"Thanks for attempting to retest.  It's a 7900XTX with an Index connected via DisplayPort. I am on the latest 25.11.1 driver.  I run a monitor at 4k 120Hz 10bpc with HDR Off, which uses DSC, as my main and only display. I tried disabling DSC in the monitor settings which runs at 4k 120Hz 8bpc with HDR Off but I don't think I noticed a change in latency. I thought that DSC on and off on two different devices might contribute to the problem but I'm not sure.   I have also tried running the Index under a RX480 on another PC and I fairly certain the latency looks different under 90Hz and looks similar under 120Hz. Can't play much to test though as an RX480 runs the Index at a very blurry setting. Getting around to doing this test is what took me so long to reply.",AMD,2025-12-08 13:31:32,2
Intel,ntkkg7z,Were you able to find the issue?,AMD,2025-12-12 01:49:15,1
Intel,np0lx2u,"Allright ty, will Install new, any differences in performance?",AMD,2025-11-15 18:27:26,1
Intel,nphlmf1,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",AMD,2025-11-18 13:25:51,1
Intel,nonq3uo,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,AMD,2025-11-13 16:53:14,2
Intel,npb8jiw,Thank you for this. This was very helpful. Got adrenaline working fine now.,AMD,2025-11-17 13:18:58,2
Intel,noot79m,"I wish my LG C4 42"" had a display port. Its my primary monitor.",AMD,2025-11-13 20:04:24,3
Intel,nop8j9i,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man it’s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for me… and have zero time to reinstall Windows.",AMD,2025-11-13 21:21:55,1
Intel,nqeioib,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",AMD,2025-11-23 19:07:08,1
Intel,npaqybw,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",AMD,2025-11-17 11:00:13,4
Intel,nopyh74,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",AMD,2025-11-13 23:40:57,5
Intel,notchza,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,AMD,2025-11-14 14:38:51,2
Intel,nosoenw,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",AMD,2025-11-14 12:09:54,1
Intel,notd4le,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",AMD,2025-11-14 14:42:17,1
Intel,notu48n,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,AMD,2025-11-14 16:07:41,1
Intel,npblkwc,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,AMD,2025-11-17 14:35:55,1
Intel,nphlnml,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,AMD,2025-11-18 13:26:02,1
Intel,noru29k,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,AMD,2025-11-14 07:20:44,1
Intel,np08w4v,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",AMD,2025-11-15 17:20:05,1
Intel,ntmuect,We've still not been able to reproduce this unfortunately. I'll need to check in when I'm back at work next year,AMD,2025-12-12 12:24:51,2
Intel,np0tp7f,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,AMD,2025-11-15 19:06:34,2
Intel,nphr5q3,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",AMD,2025-11-18 13:55:53,1
Intel,nopc4t4,No you can't.,AMD,2025-11-13 21:39:44,1
Intel,nonrg54,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",AMD,2025-11-13 16:59:46,0
Intel,npbfbpp,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",AMD,2025-11-17 13:59:52,1
Intel,noq4fcn,"They are TV's, not pc monitors. Buy the right tool for the job",AMD,2025-11-14 00:15:47,-2
Intel,noxv18g,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",AMD,2025-11-15 06:30:19,1
Intel,np729v3,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",AMD,2025-11-16 19:45:29,1
Intel,npiownv,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",AMD,2025-11-18 16:47:14,1
Intel,nrkoujc,since last BF6 Update i had zero crashes also on 25.11.1,AMD,2025-11-30 18:13:02,2
Intel,noruco5,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",AMD,2025-11-14 07:23:31,2
Intel,np08z2a,What about 25.11.1?,AMD,2025-11-15 17:20:29,1
Intel,npkeuqy,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,AMD,2025-11-18 21:52:44,1
Intel,nopdsez,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,AMD,2025-11-13 21:47:54,3
Intel,noo8n6z,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",AMD,2025-11-13 18:23:39,2
Intel,nonsm12,"Look online for fsr 4 on 6000 and 5000 series, you will understand,    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",AMD,2025-11-13 17:05:33,2
Intel,np73g8a,Did you reboot after setting that key? Is the display with chrome still only partially updating?,AMD,2025-11-16 19:51:24,1
Intel,norvx55,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",AMD,2025-11-14 07:38:54,1
Intel,npkhv83,thank you,AMD,2025-11-18 22:08:07,1
Intel,nopey1i,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",AMD,2025-11-13 21:53:39,1
Intel,noozt1l,"Not a typo, I was asking about something else and he missed my point...",AMD,2025-11-13 20:37:57,2
Intel,norw6su,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",AMD,2025-11-14 07:41:30,2
Intel,nopm704,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",AMD,2025-11-13 22:31:45,2
Intel,nopq646,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",AMD,2025-11-13 22:53:13,0
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,157
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,83
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,47
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,17
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,4
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,4
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,5
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,4
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,4
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,You’re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asf😭,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,ng0v4qd,absolute cinema,AMD,2025-09-24 21:52:34,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,44
Intel,mz2vfon,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",AMD,2025-06-22 01:09:53,19
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,14
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,17
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,5
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,14
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,8
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,5
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,4
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,3
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,8
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,18
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,6
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,4
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,4
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,4
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"👍   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,nlb3nwr,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-10-25 13:40:37,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,21
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,15
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,5
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,34
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-8
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,7
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,6
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,79
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,10
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,22
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,15
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-15
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-15
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,5
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,5
Intel,m84dadg,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,10
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,4
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,58
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,20
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-13
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,46
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-3
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,23
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,2
Intel,lgze3vw,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-8
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-10
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,13
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,17
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,5
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,6
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,5
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-4
Intel,lfkw8g2,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,6
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,11
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,11
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,9
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,5
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,9
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,4
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,32
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,19
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,8
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,4
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,8
Intel,lf385p0,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,4
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-2
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,9
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,4
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,21
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,10
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,5
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,10
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,3
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,2
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,10
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,222
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,24
Intel,kxiush3,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,108
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,16
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,29
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,119
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,67
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,7
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,36
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,34
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,59
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,13
Intel,kxiah6c,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",AMD,2024-04-01 05:59:50,24
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,20
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,3
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,4
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,26
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,6
Intel,kxp7mvs,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,5
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,5
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,4
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,11
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,13
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,4
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,8
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,14
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,3
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,3
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,4
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,3
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,6
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,0
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-6
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-23
Intel,kxk9iir,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-4
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-3
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-5
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,44
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,AMD,2024-04-04 17:12:00,3
Intel,kxjwsde,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",AMD,2024-04-01 15:08:22,25
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,16
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,4
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,32
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,13
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,12
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-3
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,19
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,9
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,4
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,9
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,4
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,5
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,28
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,8
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,6
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-6
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,12
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,3
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-6
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,25
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,8
Intel,kxjbu8k,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,5
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,2
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,3
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,13
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,6
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,4
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,3
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-17
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,9
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,14
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,1
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-2
Intel,kyy38w2,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,30
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,4
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,7
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,6
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,44
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,31
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,4
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,16
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,4
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,8
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,9
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,6
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,9
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,2
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,1
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,2
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,13
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,2
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,9
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,23
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,22
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,4
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,6
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,5
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,4
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-12
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,-1
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,31
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,26
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,1
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,5
Intel,kyhsjnw,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-3
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,9
Intel,kxj49ms,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,7
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,5
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,15
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,11
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,1
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,13
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,11
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,5
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting 😂🤣,AMD,2024-04-01 14:23:58,3
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,9
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,9
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,6
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,8
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-3
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,2
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,8
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,0
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,2
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,2
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,1
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-6
Intel,kxih401,Oh then just ignore my comment 😅,AMD,2024-04-01 07:20:10,-2
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,8
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-13
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-12
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,4
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,0
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,6
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,12
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,4
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,28
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,6
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,5
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,38
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,11
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,24
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,17
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,19
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,6
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,11
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,6
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,15
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,6
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,4
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,6
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,-1
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,17
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,1
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,6
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-6
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-8
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-2
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-3
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-5
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,3
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,8
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,19
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,5
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,6
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,8
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,7
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,-1
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,8
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,5
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-2
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,-4
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,3
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,4
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,kpp4kwl,Really love how the 6000 series radeons look.,AMD,2024-02-09 21:57:31,11
Intel,kpqv9od,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",AMD,2024-02-10 05:25:10,5
Intel,kpougfk,That's a good looking line up,AMD,2024-02-09 20:58:04,1
Intel,kps7pkq,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",AMD,2024-02-10 14:18:43,2
Intel,kpr86tx,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",AMD,2024-02-10 07:45:28,4
Intel,kpq3r57,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",AMD,2024-02-10 01:49:13,3
Intel,kptibdx,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,AMD,2024-02-10 19:15:04,-1
Intel,kptwmeu,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",AMD,2024-02-10 20:44:28,3
Intel,kpv2g8f,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,AMD,2024-02-11 01:23:45,1
Intel,kpv5euk,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",AMD,2024-02-11 01:44:32,3
Intel,kpvwyyr,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",AMD,2024-02-11 05:16:13,2
Intel,kcvx2pq,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,AMD,2023-12-11 10:20:41,10
Intel,kcvsq1w,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",AMD,2023-12-11 09:20:24,15
Intel,kcvzwca,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,AMD,2023-12-11 10:58:26,2
Intel,kcyc7u2,That 7900xtx sale number is insane,AMD,2023-12-11 21:59:22,2
Intel,kcytq9l,That just shows that most people that buy GPU's don't know a thing about them.,AMD,2023-12-11 23:54:41,1
Intel,kcwedyi,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",AMD,2023-12-11 13:30:14,14
Intel,kcvzjgq,best discounts were 6750xt 6800 and 7800xt,AMD,2023-12-11 10:53:41,1
Intel,kdazjvv,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,AMD,2023-12-14 10:36:15,1
Intel,kcvv71l,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",AMD,2023-12-11 09:54:52,6
Intel,kcwe3k6,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",AMD,2023-12-11 13:27:46,5
Intel,kcw3vwl,"They're not out of stock there, duh",AMD,2023-12-11 11:47:20,7
Intel,kcyhmsr,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,AMD,2023-12-11 22:33:46,3
Intel,kd0h0lm,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2023-12-12 07:54:01,1
Intel,kcxlwiu,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",AMD,2023-12-11 19:18:18,5
Intel,kcxu0yw,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",AMD,2023-12-11 20:09:16,1
Intel,kcx65jb,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",AMD,2023-12-11 16:46:01,2
Intel,kcw55l4,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",AMD,2023-12-11 12:01:31,1
Intel,kcy5bwz,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",AMD,2023-12-11 21:17:17,1
Intel,keln136,the card is pretty bad if you missed that somehow,AMD,2023-12-23 12:50:02,1
Intel,kcw5qf2,AMD probably ships leftover to countries in which they know it will sell,AMD,2023-12-11 12:07:48,3
Intel,kdhrs0l,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",AMD,2023-12-15 17:44:32,1
Intel,kemomla,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",AMD,2023-12-23 17:01:05,1
Intel,nxy85c1,That article claims it on par with the 4050 laptop. Jesus christ,Intel,2026-01-06 04:15:26,31
Intel,nxygoos,this is nice but the handheld market could use less ultra 9 and 7s and more ultra 3s.  the closer they can get to the nintendo 2DS XL in size while being under $400 the better.,Intel,2026-01-06 05:12:20,25
Intel,nxy9s8q,"Hopefully they deliver. Amd needs a shake up in the APU market, mostly the GPU side of it.",Intel,2026-01-06 04:25:51,22
Intel,nxywtjt,I am definitely looking to get a gaming handheld PC with PTL in it. Gonna cost a fortune probably but it's my first and intend to stick with it for a long time  The only thing that would stop me is if they skimp on ram... Which might be a very real problem,Intel,2026-01-06 07:22:44,2
Intel,nxz3n59,77% faster while using 80% more power.  I rather see power matched benchmarks.,Intel,2026-01-06 08:25:58,6
Intel,nxzqm0j,"happy about more handheld focus, genuinely have put in more hours on my steam deck than my pc setup this year. i have my eyes on ARM going forward as well",Intel,2026-01-06 11:52:57,1
Intel,nxzumfv,Really excited to see these chips on handhelds.,Intel,2026-01-06 12:22:50,1
Intel,ny14mg9,fun to see them tout XeSS MFG on mobile gpus while the B580 still doesnt have it....,Intel,2026-01-06 16:28:10,1
Intel,ny3a7m6,"I’ll always want a really good handheld besides my PC. Currently own a Legion Go S and the Switch 2 so this is good for everyone. AMD stays on their toes and if intel is good and gives us a SteamOS native device, I’ll definitely try them next upgrade. The",Intel,2026-01-06 22:21:45,1
Intel,ny52cw8,"I'm looking forward to this, especially for a gaming handheld/mini pc device, having a gpu that is nearly capable of a RTX 4050 with that power profile could be game changing.  Plus all the existing Intel XESS features are icing on the cake, although support for that scaler is flakey. I'm just hopeful that more games will support XESS.",Intel,2026-01-07 04:03:23,1
Intel,nxyuq77,"Who cares, gives cheaper powerful GPUs for 2k, 4k gaming",Intel,2026-01-06 07:04:00,-19
Intel,nxyagzo,"I looked at the benchmark scores they put out and it looks pretty promising, apparently the 12XE core variant can score somewhere around 6300 on Time Spy graphics (https://www.notebookcheck.net/Early-Intel-Panther-Lake-iGPU-benchmark-impresses-with-50-faster-performance-vs-Lunar-Lake.1138923.0.html).  Intel is comparing a 4050 with low wattage (60W system TDP IIRC) so it's not as good as the full powered 4050 which scores around 8000 on Time Spy. On low powered 4050s though like the one in the XPS and other thin and lights it will compare pretty evenly. It also outscores basically any 3050 on the market since the highest powered ones get around 4500-5000 on Time Spy (which was already matched pretty decently by the old 8XE core GPUs)",Intel,2026-01-06 04:30:18,19
Intel,nxzdq9g,"It will depend on game to game basis. Some perform well on iGPUs, some tank hard due to memory bandwidth or whatever issue they have with it.",Intel,2026-01-06 10:02:48,3
Intel,nxznnuh,"Yeah, RDNA 3.5 lasted way too long. Admittedly, RDNA4 was a special case where they gave up on a mobile version in favour of getting UDNA ready but that's their own fault. Hopefully, this pushes them to make UDNA a mobile focused architecture as well and perhaps push more cores in igpus to take back the integrated graphics crown. Competition is very good for the consumer.",Intel,2026-01-06 11:29:47,9
Intel,ny16bso,agreed. these rehashed mobile chips with bad upscaling are well beyond their lifespan.,Intel,2026-01-06 16:36:00,3
Intel,ny086nv,They did against the 285h and it's a similar margin. Lunar lake has a power burst max wattage below panther lakes max sustain power here so they can't compare 1:1 properly,Intel,2026-01-06 13:47:07,4
Intel,ny0hpkm,82% of 890M with 30% more power draw with native resolutions,Intel,2026-01-06 14:38:55,3
Intel,nxyitdu,Panther Lake with an iGPU being able to play the newest games on medium/high settings in a thin notebook is pretty crazy,Intel,2026-01-06 05:27:44,13
Intel,ny2qsag,"And then UDNA has been nowhere to be found, probably coming next year. AMD completely blew their lead in the APU space.",Intel,2026-01-06 20:52:10,2
Intel,ny0v0jb,rdna? dude their vega lasted too long they got very complacent in their igpu department,Intel,2026-01-06 15:44:10,3
Intel,ny2t2go,"At this point, we'll be lucky if they even care about consumer cards at all.  It's AI all the way these days.",Intel,2026-01-06 21:02:33,1
Intel,ny55lgh,"Low to medium , not high",Intel,2026-01-07 04:23:33,1
Intel,ny2rjyw,"Eh, they will still have the best igpus on the market for a while. If they price the 388 well there is hope for them. But it's never going to sell the volumes Intel will.  UDNA is a major architecture overhaul on par with the the introduction of Ryzen and RDNA. A year is a long time but AMD only really needs a single gen to recover this gap if they so wish. But UDNA will need to be made with versatility and low power application in mind.",Intel,2026-01-06 20:55:40,1
Intel,ny2tvmr,"Unlike Nvidia they actually can make a lot of money relative to what they do right now if they get consumer marketshare. Iirc, Nvidias gaming revenue still beats AMDs enterprise earnings.",Intel,2026-01-06 21:06:18,1
Intel,nxxytpz,"despite me using amd, please let intel succeed",Intel,2026-01-06 03:20:08,20
Intel,nxxv1o4,And Intel steps back into the game. The next generation of handhelds is upon us.,Intel,2026-01-06 02:58:42,24
Intel,nxxlswe,This is insane for a ultrabook without a dedicated dGPU. Intel cooked! AMD brought us the same thing refreshed.,Intel,2026-01-06 02:08:08,25
Intel,nxy0xsg,"7200 DDR5 support out of the box slaps. the IMC should be bonkers, like the RAM prices(I know I know.,..)",Intel,2026-01-06 03:32:13,4
Intel,nxxk3t3,Knocked it OUT of the park,Intel,2026-01-06 01:58:59,7
Intel,nxxbn9v,"intel cooked, amd socked. I cant wait to replace my old aging i7-9750h laptop",Intel,2026-01-06 01:13:24,15
Intel,nxxy441,can't wait to get my hand on this in handheld PC.,Intel,2026-01-06 03:16:02,3
Intel,nxy6trb,Wonder why they haven't announced Wildcat Lake yet,Intel,2026-01-06 04:07:06,3
Intel,nxxbuuu,Is this meant for the Strix Point or Strix Halo price segment ?,Intel,2026-01-06 01:14:32,4
Intel,nxxslar,"I'm curious to see if framework will have Panther Lake options, but I'm not sure if these are compatible with SODIMM or not.",Intel,2026-01-06 02:45:15,2
Intel,nxzl8ju,I want this in my nuc.,Intel,2026-01-06 11:09:30,2
Intel,nxz6yui,"So it's available at Jan 27th, but is that only for laptops or is it for desktops as well?",Intel,2026-01-06 08:57:51,1
Intel,ny3n1s5,ok but can we talk about how the ultra 5 332 is worse than the ultra 5 325? i mean wtf did they do to the names?,Intel,2026-01-06 23:25:18,1
Intel,nxys920,"I’m confused, is cooked good or bad? ELI5",Intel,2026-01-06 06:42:48,7
Intel,nxxce1j,"Strix Point, but we'll see what happens with the RAM situation.",Intel,2026-01-06 01:17:25,8
Intel,nxxf85s,"Isn’t much point buying strix point if you can get this instead. Better chips and a faster iGPU, and better battery. Issue is whether you want to buy any laptop this year given the price of components…",Intel,2026-01-06 01:32:44,13
Intel,nxz1cdn,The Arc B390 parts are LP-DDR5x only.,Intel,2026-01-06 08:04:24,1
Intel,ny3gnxb,"No Sodimm on the existing X series parts, LPCAMM2 for DDR5 seems dead still so I only expect to see good use of it with DDR6.",Intel,2026-01-06 22:52:59,1
Intel,nxzf4k0,Its a BGA mobile part - you might see some desktops using it (particularly in SFF NUC type designs or All-in-Ones) but its not a socketed desktop part.,Intel,2026-01-06 10:15:33,3
Intel,nxzmu72,Desktop version drops next year like usual with intel. First laptop then pc.,Intel,2026-01-06 11:23:01,1
Intel,nxytdvk,"Usually when someone cooks its good, but if they are cooked its bad.  Intel/AMD cooked/ is cooking = good Intel/AMD is cooked/ they are cooked = bad  At least that's how i differentiate it.",Intel,2026-01-06 06:52:29,19
Intel,nxytih9,"Cooking is good.  Getting cooked is bad.  One has you as the victim, the other as the victor",Intel,2026-01-06 06:53:35,13
Intel,nxxfmz4,"There could be if I can get it far cheaper.  This kind of performance should melt good part of premiums off Strix Halo, too.",Intel,2026-01-06 01:34:55,3
Intel,nxzmr6n,Its probably LPCAMM memory.,Intel,2026-01-06 11:22:20,1
Intel,ny3iqio,"Yeah I certainly don't expect a smaller company like FW to adopt a niche standard like LPCAMM2 for DDR5, probably DDR6 given I doubt SODIMM can support the speeds that DDR6 will have.",Intel,2026-01-06 23:03:18,1
Intel,nxzyhhz,"desktop version will be novo lake, later this year or early next year.  It will a significant upgrade.",Intel,2026-01-06 12:49:16,3
Intel,nxzn7ws,So in 2027? It takes them a whole year to release the desktop versions?,Intel,2026-01-06 11:26:10,3
Intel,nxylinl,Strix halo is irrelevant because that shit isn’t and won’t be available in mainstream laptops regardless of its pricing. Even in amd marketing material just now it’s the same asus tablet and hp Zbook nothing else.,Intel,2026-01-06 05:48:06,2
Intel,ny3g4rp,"Nah the love of Strix Halo is it's 256 bit bus and 128GB of memory, which this doesn't address at all. It's good groundwork though, you could imagine an update that grows the current design with two Xe4 12 core chiplets connected to the I/O die that then goes to DDR6 192 bit bus being good enough to compete with an RTX5060.",Intel,2026-01-06 22:50:22,1
Intel,nxzez6p,"I get downvoted, but it literally shows on the chart that the 388H, 368H, 358H and 338H are LP-DDR5x only: [INTEL-PANTHER-LAKE-1.jpg (2629×1341)](https://cdn.videocardz.com/1/2026/01/INTEL-PANTHER-LAKE-1.jpg)  You won't see Intel supporting consumer designs which use DDR5 with these parts, and if you do make a product running it you'll lose the Arc branding - same as if you do an Arrow Lake-H design with single channel memory, or a Lunar Lake with a sub-17W PL1.",Intel,2026-01-06 10:14:13,1
Intel,nxznayq,LPCAMM2 is still LP-DDR5x.,Intel,2026-01-06 11:26:51,1
Intel,ny3jgvd,"I mean they tried for Strix Halo, but AMD did not validation or design work in mind to support it sadly so framework couldn't risk it. Intel tends to be better, but still.",Intel,2026-01-06 23:07:01,1
Intel,ny2jemc,"No, I have no idea what u/kazuviking is talking about.   Panther Lake won't have desktop chips. The mobile chips might get used in some mini PCs or whatever, but PTL is a mobile architecture.   What is coming late 2026 though, near the end of the year, is Nova Lake, a completely different arch than Panther Lake. And then we will likely see Nova Lake for laptops at CES next year.",Intel,2026-01-06 20:17:53,1
Intel,nxznspb,"It was always like this, release new gen on laptop then a year later on pc.",Intel,2026-01-06 11:30:53,1
Intel,ny33fhy,It is going to be in the Asus Tuff A14 this year.,Intel,2026-01-06 21:49:56,1
Intel,nxznqea,Yeah but completely different standard.,Intel,2026-01-06 11:30:22,1
Intel,ny3kun0,"That's a bit of a different case, because they are designing a new product either way they might as well look into supporting something like LPCAMM2. They already have a mainboard for the FW13 that can support these newer CPUs, so there has to be a very compelling reason to spend money to redesign that, which I think a single Intel SKU is not unfortunately.",Intel,2026-01-06 23:14:05,1
Intel,ny3gxyu,Do we know if we are getting real Nova Lake mobile chips or just a BGA version of desktop parts with a panther lake refresh?,Intel,2026-01-06 22:54:23,1
Intel,nxzp8ts,"The DRAM standard between LPCAMM2 and LP-DDR5x is the same - that's the point. LPCAMM2 is a replaceable implementation of LP-DDR5x memory.  But LPCAMM2 is completely different from SO-DIMM. SO-DIMM is DDR5. LPCAMM2 is LP-DDR5x.  You will see Arc B390 designs with LP-DDR5x memory down on the mainboard, and you'll see (some) LPCAMM2 designs, but they'll be far less common.  But you won't see DDR5 SO-DIMM designs - if they claim to be DDR5 SO-DIMM and Arc B390 then its wrong.",Intel,2026-01-06 11:42:26,1
Intel,ny4seb7,"It certainly sounds like we will get dedicated NVL-H, and that's what the rumors claim as well. Here's an excerpt from an Intel executive at an Intel BoA conference:   >Yeah, so maybe just baseline everybody on Panther Lake, so Panther Lake is a product that’s going to launch in the second half of this year, and it is all built on Intel 18A. Really, Panther Lake is an all mobile stack. When you get to the next generation Nova Lake it is both a mobile stack and a desktop stack",Intel,2026-01-07 03:05:35,1
Intel,ny4w3p0,"Looks like some folks are saying Nova Lake replaces panther lake stack exactly while also having a really big H chip at top, missed that cause I thought it was just the big 8+16+4 design",Intel,2026-01-07 03:26:25,1
Intel,nx9rf7h,How is the AI running on the B50?,Intel,2026-01-02 15:55:47,3
Intel,nxaevc4,Where did you purchase the B50?,Intel,2026-01-02 17:45:39,1
Intel,nxc8zfu,nice case,Intel,2026-01-02 23:05:56,1
Intel,nxlmpl5,"love the size of it. love the psu, are there any psus in this formfactor that are more powerful?",Intel,2026-01-04 09:46:01,1
Intel,nxsj613,100°C,Intel,2026-01-05 09:52:28,1
Intel,nx9s5yu,"cute fan lol  like other user, how is the B50 performance?",Intel,2026-01-02 15:59:17,1
Intel,nxeazfq,"weird comparison. the mac mini is the real beast and its in part thanks to not having to cater to OEMs, but the 48gb mini pro is $1800 vs this $350 drop in card so that's a strange comparison.   m5 in the ipad has only about 150gb/s of bandwidth. good for light inference but I really doubt its practical for actual scale production.",Intel,2026-01-03 06:46:02,7
Intel,nxahmts,"As an ardent and lifelong Apple hater, I must admit that they will probably come out much stronger and on the very top of the current chaotic situation if they manage to keep the current price/perf ratio of their offerings. Even with the Apple tax, they are unmatched right now.",Intel,2026-01-02 17:58:24,5
Intel,nxlml7c,"apple stuff is hard to get used to for many pc nerds and mechanical engineers and engineers in this field. When pro software like catia/nx nativly will work nativly on arm then maybe the big car/air/motorcycle/""every day crap all around us"", then product developers will adopt arm/apple. but right now x86 is the king for these guys/this sector that design all stuff u see around u.",Intel,2026-01-04 09:44:53,1
Intel,nxb0egt,"its not hard, you can literally just buy them on newegg",Intel,2026-01-02 19:25:10,4
Intel,nxbf6k9,"[https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007](https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007)  They're finally back in stock as of this reply, though likely not for long.",Intel,2026-01-02 20:37:05,3
Intel,nxvcb13,"It's the Flex ATX form factor. I think the most powerful one that is also reputable is the Enhance ENP-7660L, which is 600w.",Intel,2026-01-05 19:23:31,1
Intel,nxs9peb,"a lot of commercial software also just does not give a shit about improving, and I don't mean that as a defense for apple.  after effects is just ass for a 2025 product. basic filters are still using legacy code and memory management is horrible. like you're not going to clean 64gb of memory until I hit 96gb utilization, then you're going to slow to a crawl and maybe crash because of threadlock? why even bring back MT rendering? 3rd party scripts people wrote in their basements outperform this stupid thing. spoofing multiple instances and then stitching the results works better than just running the software, its baffling.  anyway yeah, there's a lot of good to x86 and not having to reinvent the wheel, but god damn if so many companies are using it as an excuse to resell garbage.",Intel,2026-01-05 08:22:17,1
Intel,nxgupsq,"It's not exactly weird. They went PPC before Intel because powerpc was more effective for workloads most people used macs for. The switch to Intel was just because Intel had node leadership and performance leadership. Instead stick to A-chips for low power mobile where intel gave up on servicing. Intel loses node leadership to TSMC for a long time, Apple moves on to everything in-house is a pretty logical progression.  Apple having bespoke solutions isn't new either. they've been doing it since their G workstation days. Their current situation is pretty much on brand for apple, but the difference is the huge mistakes intel made (particularly firing so many top engineers) that led to staff fleeing to other companies, including leadership at Apple processor design.  basically apple did their own thing as usual and did a great job don't get me wrong, not taking anything away from apple. the biggest difference however was intel's CEO and board destroying the company.",Intel,2026-01-03 17:07:56,1
Intel,nv0zs3r,"If only Intel had stayed in the memory business!   They'd be enjoying Micron valuations and wild profits and performance from copackaged CPU+GPU+LPDDR of their own design and manufacture...     But no, they'd rather invest billions in buying donuts as a service, or whatever their crazy investements went into.",Intel,2025-12-20 13:16:05,43
Intel,nv0mnlo,"damn an iGPU using 32GB of vRAM, I wonder if they're testing a Panther Lake laptop with 48GB RAM or even more (since X7 & X9 Panther Lake only accepts soldered memory)",Intel,2025-12-20 11:24:26,12
Intel,nv2mj8t,"If Intel is really about to release a B770, honestly the **only thing that could make it competitive is the price**. (FOR ME, competitive in 2026 means <400€) From a performance standpoint, it would need to undercut existing GPUs quite aggressively to make sense, especially given how crowded the mid-range already is.  That said, I’m pretty skeptical about how realistic that is. **With the recent RAM shortages and rising memory costs**, pricing a new card competitively while still keeping margins doesn’t sound easy at all. Memory is a huge part of the BOM, and we’ve already seen how shortages can push prices up across the board.  So unless Intel is willing to take a serious hit on margins (which seems unlikely), I’m not convinced the B770 will land at a price point that truly shakes up the market. Happy to be proven wrong, but for now the pricing question is the big unknown for me.",Intel,2025-12-20 18:47:27,5
Intel,nv30mtq,So there's a 20GB variant. A 28GB variant and a 32 GB variant?,Intel,2025-12-20 20:01:42,2
Intel,nv40zo0,Optane was practically **built** for the type of AI workloads that they're shoveling money at.  If Intel didn't give up literally only a matter of months before GPT released and the bubble began in earnest lol,Intel,2025-12-20 23:28:07,18
Intel,nv29blj,"If Intel stayed in memory business, it would be long dead in the 80s and killed by Japanese memory companies. CPU remains the top niche area with less competition and deeper moat. See how China has quickly come up with their GPU designs? Well it will take at least another decade for them to make 2nm CPUs",Intel,2025-12-20 17:39:17,13
Intel,nv2qfnf,"are people high or something? intel was losing money on optane and their SSD business became irrelevant the minute regular memory manufacturers slammed the market. don't get me wrong, they were some of the most durable on the market, but they were no where near printing money on the memory business.  optane may have survived if their nodes were on schedule, keeping CXL support on schedule, but not because it was profitable.",Intel,2025-12-20 19:07:24,8
Intel,nw63y3k,CXL killed octane it’s that simple. No one wanted to be locked to just Intel. CXL was and is just better,Intel,2025-12-27 10:05:29,1
Intel,nvsy1nn,"I feel like the price has to be more than competitive. If they can undercut competition cards of the same performance by 100 or so (or maybe offer rebates or freebies) they could potentially steal the market in that category. With Nvidia and amd cards being tried and true for many many years, I feel like their marketing needs to grab the attention of consumers in a somewhat drastic way.",Intel,2025-12-25 00:56:50,3
Intel,nvbtvc9,If the b770 is 5060ti levels even €500 is competitive,Intel,2025-12-22 06:05:31,1
Intel,nw63tp3,Yeah sure it would’ve been perfect but CXL killed octane and offers pretty much everything it did while not being loved to just Intel lol,Intel,2025-12-27 10:04:18,1
Intel,nv4153e,"it's not like any of this AI garbage right now is profitable for anyone except nvidia and the hardware companies anyway, it's not stopping everyone from shoveling money into it",Intel,2025-12-20 23:29:02,5
Intel,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
Intel,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
Intel,nxyngsv,"Hello Intel\_support - I have just one important Question ( do you really work at company Intel , or you are just being knowledgable )  Why does this matter? - because I would like to work with Intel - I can teach your senior engineers to make Quantum Processors  PLUS I LOVE INTEL! ( that's the only Processor I swear in )  Currently I am using some i-5 from year 2018 - I have no issues with it.  I am not AMD hater - but AMD cannot compare to Intel. Class difference is BIG.  Intel is like Mercedez Benz- and AMD is like Fiat.",Intel,2026-01-06 06:03:18,1
Intel,ny2u31x,"With the crazy RAM prices, I'm looking to move to a 13600K or 14600K to keep using the 64GB of DDR4 from my ancient 7700K build. Do we users generally consider Vmin Shift Instability to be fixed at this point through the series of BIOS and microcode updates?  Related: Should I expect something in the range of a 10% performance drop from any of the reputable reviews, due to the fixes? Also, are efficiency-cores pretty much working as intended at this point, or is thread scheduling still a concern on them where your high performance thread ends up on an e-core?  Thanks all!  Note: This question is not for Intel\_Support. The answer from your side would obviously be ""Yes!"". :)",Intel,2026-01-06 21:07:15,1
Intel,nxwkozf,"u/Chelostyles Thank you for your inquiry regarding the CPU and RAM upgrade for your Dell Vostro 3670. As much as I'd like to provide my technical insights on this upgrade path, I'm not in a position to provide specific suggestions since this involves hardware modifications to an OEM system.  For the best compatibility outcome and to ensure optimal system performance, I strongly recommend reaching out to your system manufacturer directly. They can provide definitive guidance on supported CPU upgrades (i5-8400 to i7-9700) and maximum RAM configurations for your specific model. We don't want to inadvertently bypass any warranty terms and conditions on your system by providing modification recommendations that might affect your coverage.  Your system manufacturer's technical support team will have access to the exact specifications, BIOS compatibility matrices, and supported hardware configurations for your Vostro 3670 model. They can confirm whether the motherboard supports the i7-9700, the maximum RAM capacity (64GB vs 128GB), and any potential limitations or requirements for these upgrades.  This approach ensures you get accurate, manufacturer-validated information while maintaining your system's warranty protection.",Intel,2026-01-05 22:52:24,1
Intel,nxwjdkt,"u/I_like_carsyay  XPS 15 9530 hardware does support Resizable BAR, which is why Intel's system detection shows it as available for both your Arc A370M and Iris Xe graphics. However, the system manufacturer has designed their BIOS interface to prioritize stability and user-friendliness, often managing advanced PCIe features like ReBAR automatically in the background rather than exposing manual configuration options. This approach ensures optimal system performance while reducing complexity for users. I recommend checking for the latest BIOS updates from your OEM's support site and contacting their technical support team, as they would have the most current information about how ReBAR is implemented on your specific model and whether any additional configuration steps are needed to fully utilize this feature.     I've posted an article below in case you haven't yet come across it:  **Helpful Resources:**  *  [What Is Resizable BAR and How Do I Enable It?](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html)",Intel,2026-01-05 22:45:46,1
Intel,ny3upu3,"u/QunatumLeader Hi, thanks for your interest!  You can find and apply for all of our jobs online at [http://](http://jobs.intel.com/)[j](http://jobs.intel.com/)[obs.intel.com](http://jobs.intel.com/). We don’t currently accept submissions via social.  Good luck!",Intel,2026-01-07 00:05:20,2
Intel,ntkfg69,"> With up to 192GB of VRAM across eight GPUs in a single system, Battlematrix positions itself as a relatively cost-effective alternative to other professional GPU ecosystems for AI inference workloads.",Intel,2025-12-12 01:18:37,21
Intel,ntmjjev,Hope they do some image and video generation benchmarking as well. Nice to see someone testing AI rigs out there.,Intel,2025-12-12 10:55:28,4
Intel,ntlssa2,Wish they’d give prompt processing speeds. AI coding generates very few tokens compared to input. Nvidia seem to dominate here.,Intel,2025-12-12 06:40:14,3
Intel,ntxw9ob,"How many concurrent users will this serve, 30 devs would be nice",Intel,2025-12-14 06:36:27,1
Intel,ntsaqxn,:),Intel,2025-12-13 08:27:35,2
Intel,nvplqob,"These 12Xe3 cores are pretty neat, and because it fits in a normal socket it isn't ludicrously expensive to make.  I suspect we'll see a ton of these different form factors for this chipset.",Intel,2025-12-24 13:00:31,4
Intel,nvpsi8z,Mac Pro Trashcan 2.0 is crazy,Intel,2025-12-24 13:45:33,6
Intel,nvt0j8h,"> These 12Xe3 cores are pretty neat  have there been any leaked benchmarks or gaming FPS?  on paper they look good, but... some synthetic benchmarks suck",Intel,2025-12-25 01:15:41,3
Intel,nvte7q0,No one knows. Synthetics seem to put it roughly at a 3050m.,Intel,2025-12-25 02:59:28,2
Intel,nwfd6hr,"3050 to 3050ti mobile if leaks are to be believed. Could get a bit better than that if software is still not mature, so I'm calling a max of 3060M performance.",Intel,2025-12-28 20:51:38,1
Intel,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,54
Intel,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
Intel,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
Intel,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
Intel,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,9
Intel,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,9
Intel,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,5
Intel,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,5
Intel,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,4
Intel,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
Intel,nspzeik,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,6
Intel,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
Intel,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
Intel,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
Intel,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,2
Intel,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,5
Intel,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
Intel,nsv64t7,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,2
Intel,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
Intel,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
Intel,nsyv727,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
Intel,nv5fgk8,"should have a 3y warranty on it. submit an RMA ticket  regarding the actual query though, the silicon is the same the 14900ks is just a slightly better bin. you wouldn't notice the difference at stock let alone normalized for energy consumption",Intel,2025-12-21 04:48:32,8
Intel,nv5nfm2,"I have i9 14900ks, what I did is that I reset bios settings to optimized defaults and then I limit pl1 and pl2 to 150w and enabled XMP, these are the only two settings i changed, the rest is default, and temperatures are in check, i still get the same performance, and it’s very efficient in gaming that way, the extra heat and power consumption of 253 or 320 are not worth it, I recommend just get the ks and make these two changes and forget it.",Intel,2025-12-21 05:49:09,3
Intel,nv5h74y,The performance difference will be tiny and definitely not noticeable with a 3090. Go for the cheaper chip.,Intel,2025-12-21 05:00:55,3
Intel,nv5hr1e,"Get the KS for better silicon quality only limit power , set pl1/2 253w and set it to 350 or 325A, definitely want the better 14900ks silicon quality it’s overall better and better IMC as well. It’s a better bin and typically only the best 14900k will run stable 6.2ghz and at lower voltages even if you limit your chip to 6ghz",Intel,2025-12-21 05:04:58,2
Intel,nv5yk6v,If it doesn’t cost you extra then get the 14900ks and lock all the cores to 5.6 and power limit 256w,Intel,2025-12-21 07:30:16,2
Intel,nv70zt0,"A 14900KS is nothing more then a binned 14900K. Running a 320W/400A extreme setting is not advisable with a AIO. I run my 14900KS on custom loop with 320W/307A performance setting and it does not thermal throttle at all. If you get lucky, you could get a 14900K that can run KS settings. Performance in that case is( should be) identical. Without benchmarks i can't really tell the difference between 125/253/307 and 320/320/400 except the heating of my room.",Intel,2025-12-21 13:28:34,2
Intel,nvehvk5,"As an update - I went ahead with the 14900ks and also changed my cooler to a 420mm AIO.   Ensured latest bios update then set Intel presets (performance) but also went ahead and reduced PL to 150w, set temp limits to 70c, system agent voltage to 1.12, 307A, and I was blown away by the temps!! I am getting basically identical performance (+ few fps) to my previous 13900ks, but a whole whopping 30°c cooler in game!!! I would average 80-85, now it’s sitting super chill with same in-game settings on BF6 & ARC at 50-60c.   Thank you everyone for your inputs, I sincerely appreciate it and I’m extremely happy with the outcome!",Intel,2025-12-22 17:37:19,2
Intel,nv6um6s,I also PL1/2 at 150. My temps stay under 60c when gaming.,Intel,2025-12-21 12:41:07,1
Intel,nv7icoh,"if the cooling wasn't sufficient disable HT(useless for gaming) and undervolt it this lower CPU temperature by 20c, in games the CPU temperature should be around 65c.",Intel,2025-12-21 15:16:34,1
Intel,nvb3hok,Im using a duel air tower for my 14900k game temps are at 60 to 70,Intel,2025-12-22 02:56:21,1
Intel,nvkr6z0,"14900KS is just a better binned 14900K. All things equal, you should have lower temps/voltage/power draw for the same exact workload/clocks on a 14900ks vs a 14900k. How big of a delta between the two comes down to how well you struck the silicon lottery with the KS.",Intel,2025-12-23 17:28:35,1
Intel,nv62yz6,"I have the K version only because of the onboard gpu. In case my GPU gives issues and I'll still be able to boot. But otherwise there is almost no performance gain. I ran my i9-14900k pl on 320 watt and did a cinebench benchmark, temps were ok: average 94c, max 98c with a 360 aio.  That said, go for the cheaper version if you don't need onboard gpu.  Edit: I have my pl on 253w now. No need to go any higher.",Intel,2025-12-21 08:13:54,-3
Intel,nv6s3tx,5 years warranty on 13 and 14gens now.    I have the 13900ks. Run it at 253w. Clock locked at 5.5ghz. Temp 80c and cinebench 23 39k,Intel,2025-12-21 12:20:08,6
Intel,nvx118g,Why not keep pl2 at 253 and 1 at 150/185 ? Did you try undervolting? Most of them can take 50mv offset with 75 /85 needing a bit more stability testing. Can also cap the vr limit and iccmax. I feel like going 150 pl2 makes you miss some performance in games unless you had thermal issues and doing it to keep it from thermal throttle.,Intel,2025-12-25 20:10:57,1
Intel,nv6848y,"Thank you for the feedback. Forgive me for the dumb question; if I ran either a 14900ks or a 14900k at these settings, would they both have the same temps? Or would the KS still run hotter?",Intel,2025-12-21 09:06:11,0
Intel,nwbwttq,"Dropped the voltage further down by -0.10000 and now I’m getting 58°c core temp and max 65°c package temp under load. Really happy with this, and with some tweaks to my in-game video settings I’m able to still maintain a framerate that matches my screen refresh rate.",Intel,2025-12-28 07:43:31,1
Intel,nv7zbxm,This post is about the K and KS. Both have the same iGPU,Intel,2025-12-21 16:46:01,3
Intel,nv7imu3,even better. i take it they extended tbe warranty period for those products?,Intel,2025-12-21 15:18:10,1
Intel,nvgy84t,For 5.5 39k in CB23 is a little low,Intel,2025-12-23 01:34:28,0
Intel,nvx2yr5,"I have tried and tested all my games, i saw absolutely no difference between 253w, 150w, 125w or even 100w, the fps were exactly the same, the only difference was in temperatures, performance wise i saw no difference between any of them, i was using 100w before but then I switched to 150w because I thought it was too low, even though the performance is still the same as 100w, just higher temperatures, my cooler is pretty good kraken elite 360, it never goes above 80 even on 253w but I just like to keep temperatures between 50-70 while gaming.",Intel,2025-12-25 20:22:49,1
Intel,nv7bdbq,The ks should run cooler because of the better bin. Less voltage being required to hit certain frequency points.,Intel,2025-12-21 14:35:05,2
Intel,nv84jg4,"Oh shit, I thought only the K had an igpu! Should have gone for the ks version lmao",Intel,2025-12-21 17:12:59,1
Intel,nv7jm5d,Yes because of the degrading issue.,Intel,2025-12-21 15:23:39,2
Intel,nvhfrt3,Lol stock is 5.8ghz lol and most stock after the update get 35k,Intel,2025-12-23 03:22:04,1
Intel,nvhi35n,All core cinebench is not 5.8... I get 39k stock what are you talking about lol,Intel,2025-12-23 03:36:52,1
Intel,nxyvuyn,"That is a little on the lower end, my 12900K gets 30K. Also in single core a little below 285K, multicore just below 9900X, top of 13700K. Back to it :)",Intel,2026-01-06 07:14:02,1
Intel,nvhitar,Search on reddit on 13900-14900k.  After the code update stock most 13-14k can barely do 35k. Dont like to your ego brother.   So millions on reddit are getting those score and you are the special bin whose getting a higher score.   Mr 1 post and 7 comment history lollllllllllllllllllll,Intel,2025-12-23 03:41:34,2
Intel,nvhk1pd,LOLOLOLOLOLOL HAHAHAAHAHAHAH ARE YOU DUMB? This really shows you don't have a 13900k or 14900k,Intel,2025-12-23 03:49:33,1
Intel,nvkpd9m,"My guy what are you talking about? 5.5 ghz for 39k is a good score on 13900k. My 14900KS completely stock does 41.5k and downclocks to about 5.5-5.6 ghz with hyperthreading on. If he's got HT off, his score is even better.   You have to be rage baiting.",Intel,2025-12-23 17:19:30,2
Intel,nvkq3xk,Talk to him not me... The guy said 35k is the score 😂😂😂😂😂,Intel,2025-12-23 17:23:13,2
Intel,nsktadm,Will be a interresting CES,Intel,2025-12-06 11:12:47,21
Intel,nso11hn,I'm half-expecting this to show up as a server-only AI-focused SKU with video outputs removed.,Intel,2025-12-06 22:37:18,8
Intel,nswpbo1,Merry Christmas everyone,Intel,2025-12-08 08:52:05,2
Intel,nswyceh,"4070 performance for $400, I'm calling it. Would have been great if this had come out right after the wave of negative press that the 5070 received for only being 10-15% better than the 4070 with a mediocre 12 GB of VRAM, but I feel like Intel missed the boat again if the Steam Hardware survey is anything to go by, the 5070 has really made a comeback with recent sales.",Intel,2025-12-08 10:25:38,2
Intel,nsofcmj,They can't even ship B60's.,Intel,2025-12-07 00:01:12,3
Intel,nsp4pld,"What is taking Intel so long?      It's already been almost a year after Battlemage's initial launch. And for what? RTX 5060 performance at the same price with some extra VRAM?  I had really hoped Intel would be able to gain ground on their competitors. At this rate, we'll get the ARC C770 to compete with the RTX 6060 in another 3 years.",Intel,2025-12-07 02:38:20,1
Intel,nsm5mtm,Aren't they always,Intel,2025-12-06 16:34:23,3
Intel,nt8d8jl,give it some time...,Intel,2025-12-10 03:42:11,1
Intel,nt8d10w,Merry Bitmas,Intel,2025-12-10 03:40:49,1
Intel,nt9tjuy,"4070 performance for... used 4070 price, now with driver issues and an objectively worse upscaler!  intel greatest hits",Intel,2025-12-10 11:20:47,0
Intel,nsol1s5,Sure they can if you search for it   B60  https://www.idealo.at/preisvergleich/OffersOfProduct/207972918_-arc-pro-b60-sparkle.html   Or b50 https://geizhals.at/intel-arc-pro-b50-a3584363.html,Intel,2025-12-07 00:34:27,6
Intel,nsq0noc,"Intel's GPU division has been operating at a loss never mind Intel as a whole and ARC series cards aren't as popular as the enthusiast circles would have you believe. Coupled with how expensive R&D is for things like GPUs, it's hard for them to pump out a competitive product while remaining just profitable enough to undercut AMD and Nvidia.",Intel,2025-12-07 06:21:43,6
Intel,nswuidx,What is taking Nividia so long with the super cards?,Intel,2025-12-08 09:46:34,2
Intel,nt0mfun,"Battlemage gpu chips are made through TSMC and Intel is getting screwed on supply, this is why even if the B770 comes it will only be a small amount. Hopefully Intel can put together enough rare earth to pump out discrete Celestial Gpus but it takes time to ramp everything up. In addition Intel has their chiplet design, EMIB that could take off soon. They may be able to bring Apple back into the fold, but let us hope Discrete Arc lives on.  I have learned to not have expectations for anything that is outside my direct control, I do the best I can to just go with the flow. Whatever will be, will be.",Intel,2025-12-08 22:47:54,1
Intel,nsn1l8j,There's definitely been lame ones.,Intel,2025-12-06 19:20:53,3
Intel,ntco711,"I'm fully aware it's not a great deal, but that's my expectation when it comes to Arc.",Intel,2025-12-10 20:41:40,0
Intel,nt0k5mq,"These are European links and will be out of stock. I found a mom and pop place back in my old stomping grounds in San Francisco, and they normally only sell B60s in prebuilt systems but a special order is possible.   Intel can't rely on TSMC for Battlemage supply, so let us pray that Discrete Celestial GPUs are made (entirely) at IFS and release in 2026 / 2027.   May your Bits Byte Hard, long live the Arc.",Intel,2025-12-08 22:35:24,2
Intel,nt0ni97,"If CES 2026 comes and goes without any details for Discrete Arc GPUs then it could be awhile. The main thing being promoted is Panther Lake which should be made entirely at IFS, a step in the right direction. The TSMC monopoly is destroying the industry and it hurts companies here in the US.",Intel,2025-12-08 22:53:46,1
Intel,ntafqck,Not sure how the link being European matters. They are European shops and the B60 is in stock.,Intel,2025-12-10 13:56:29,1
Intel,nqdrca0,"We kind of know where it will land. It will be a 3050M level chip, maybe a bit better, but will have improve scaling and frame gen.   8060S is 40 RDNA 3.5 units. One Xe3 unit is about 2.1 RDNA 3.5 units. That put it at about 65% of Strix Halo, though it will have a worse memory bus and no MALL cache. Somewhere in that range.   So that 60% is almost bang on the 3050M. Maybe a bit better. It won’t be like the 4050 but 3050M isn’t bad for an iGPU that fits in a normal socket",Intel,2025-11-23 16:52:41,13
Intel,nqe5eoy,"> and no MALL cache  PTL does have 8MB of memory side cache, fwiw.",Intel,2025-11-23 18:04:37,13
Intel,nr20ozq,"I don’t know how this score compares to the 3050M. I only know that this score is about 55% of the B580. And the B580, at 2K and 4K, is about 1.7–2 times the performance of the desktop 3050",Intel,2025-11-27 14:11:52,1
Intel,nu0mh30,and also an iGPU won't be stuck with 6GB of vRAM 😅,Intel,2025-12-14 18:17:52,1
Intel,nqga537,Really don't understand why they don't go with a larger cache.  Pretty sure they still have a bunch of cache chunks spread all around the SoC.,Intel,2025-11-24 00:43:14,6
Intel,nqg407n,"in configs without die-to-die memory performance in general should be worse if bandwidth limited. despite not being specifically dedicated to onboard memory like Lunar, people are still planning configs with local LPDDR5x, though peak bandwidth is limited by a 128bit bus.",Intel,2025-11-24 00:08:08,3
Intel,nqfqmb2,I see it personally as not really being at 20W if you’re asking much from the GPU. It’ll increase the juice dynamically if it gets demanding enough. So it’ll be hard to say unless you force the power limits way down manually.,Intel,2025-11-23 22:49:52,2
Intel,nnbivzp,"I actually like the idea of discrete GPU naming scheme for the new iGPU, 300 series for integrated graphics is really makes sense but they should add 'M' suffix to make it clear.",Intel,2025-11-05 22:28:54,36
Intel,nnbkl4o,"I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  Also looks like the 3\_8 and 3\_6 versions are differentiating maximum boost clocks, though I wonder if instead those may reflect that configurable upper TDP bound. Might make sense for 65W and 80W to be differentiated if that will coincide with anything about the ""experience-based"" PL1 behavior.",Intel,2025-11-05 22:37:53,14
Intel,nnbr1vs,There are 2 dies.  One for professional workload which will be mass produced.  Second is for gaming. Even a 10-12 core xe3 will be barely enough for modern 1080p. Lunarlake can only run alanwake at 1080p at low settings getting only 25 frames so even if this is 50-100% better this is the minimum for a 2026 product.   I see no point of a 8 xe3 core system when all people will do is just complain.,Intel,2025-11-05 23:13:11,7
Intel,nncz7bz,"quite confusing, Xe3 should start from C (celestial), if using name like B390  we think this  is a battlemage Card (Xe2)",Intel,2025-11-06 03:36:19,5
Intel,nnftti4,"I wonder if rumors about Zen 6 clocking way higher than current cpus turn to be true, and the 5.1ghz max on mobile PT mean Amd might have an edge in next generation   Only time will tell",Intel,2025-11-06 16:14:24,2
Intel,nndm2ga,please add M for Mobile or i for iGPU  * Arc B390M Xe3 Graphics * Arc B390i iXe3 Graphics,Intel,2025-11-06 06:31:30,4
Intel,nnc0ph2,"A clock speed regression vs the prior gen on N3B, with a remark that it's difficult to cool, really isn't a good look for the process side. 18A branding with more like N4 performance...",Intel,2025-11-06 00:08:27,-6
Intel,nnjsaw2,Will the 10 core Xe be better than radeon 890m or worse?,Intel,2025-11-07 05:14:22,0
Intel,nnbhb9s,"i mean i get this is a laptop part but man 16 threads is not much to phone home about when it comes to horsepower, isnt next gen desktop aiming for something like 48 threads?",Intel,2025-11-05 22:20:34,-5
Intel,nnbj579,Still weaker than x3d,Intel,2025-11-05 22:30:15,-16
Intel,nnm29u0,Yes indeed we need that M&M. Mobile platforms are not a priority for me and are dedicated mobile gpus really comparable to Big Boy Discrete GPUs? It is very confusing.  Lunar Lake laptops should fall in price. Has anyone used Lunar Lake and if so which models? Buying latest gen is for guinea pigs and the rich!,Intel,2025-11-07 15:46:06,3
Intel,npu5d43,Doesn't the B already serve that purpose?,Intel,2025-11-20 13:03:06,1
Intel,nnbmjlf,">I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  The 10 Xe3 core model is a binned down 12 Xe3 N3 die, and I doubt yields are so bad that they would even be able to find more dies where they have to disable more cores.   The other die is the 4 Xe3 Intel 3 die, so you can't go up from there.",Intel,2025-11-05 22:48:16,14
Intel,nnd5hpv,To be fair Alan Wake 2 low settings look great. This was covered by DF awhile back they said in some ways Alan Wake 2s low settings look better than some modern games high.,Intel,2025-11-06 04:19:26,8
Intel,nnclctl,"I mean, but that logic, most of Intel's historical bigger iGPUs don't make sense. There are use cases other than AAA gaming. Media creation is another big one.",Intel,2025-11-06 02:10:09,6
Intel,nnde8rz,"Xe3 is not GPU family name but GPU core architecture, it's like Nvidia Ampere, Ada Lovelace. But Alchemist, Battlemage, Celestial is GPU family name.   Panther Lake 12Xe3 being B series GPU makes sense because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture. Intel confirmed Celestial will have Xe3P.",Intel,2025-11-06 05:25:14,9
Intel,nngpesf,"Think of it like AMD Zen X+ nodes. Ryzen 8000 is more or less a laptop only APU series on Zen 4+.   Xe3 is a half-generation, it doesn't get the letter upgrade to C, but it gets the 3, signifying a new architecture, but not a new generation.  Zen 4+ is a half-generation, it doesn't get the number upgrade to 5, but it gets to be 8000-series, signifying a new architecture, but not a new generation.",Intel,2025-11-06 18:45:02,0
Intel,nnklixi,"Mobile Zen 6 is likely to come around the same time NVL does. Both should use N2 and will presumably have similar frequencies, well above any 18A parts.",Intel,2025-11-07 09:57:31,2
Intel,nnlg7cm,"Considering the timing of 2nm, zen 6 would be around late 2026, and mobile zen 6 late 2027 wide availability. for whatever reason amd takes forever despite high mobile demand, but this quarter it looks like it worked out for them (maybe a big bump up from x3d sales).",Intel,2025-11-07 13:50:18,1
Intel,nnbigvq,What're you doing on a laptop?,Intel,2025-11-05 22:26:37,19
Intel,nnbiwg3,"These are for thin and light office notebooks and light gaming. Think Lunar Lake. For CPU power, Nova Lake H will exist.",Intel,2025-11-05 22:28:59,8
Intel,nnbowvu,It's for handhelds and office laptops not hyper enthusiast shit.,Intel,2025-11-05 23:01:09,2
Intel,nncdk44,Nobody buys AMD laptops,Intel,2025-11-06 01:23:02,11
Intel,nnbk5wm,">Still weaker than x3d   Source?? Also Panther Lake is H series only, HX will be based on Nova Lake.",Intel,2025-11-05 22:35:40,6
Intel,nnco8fw,Yeah so it is weaker for gaming with a dGPU than the 0.2% of laptops currently sold that have either a 7945hx3d or 9955hx3d that makes up for less than 0.1% of all laptop users. What's your point?,Intel,2025-11-06 02:27:36,1
Intel,nnz3axo,"While I haven't used it daily or anything, and I've only done initial setup on the Lunar Lake, the feedback we've gotten both on Arrow Lake and for Lunar Lake (e.g. 268V and 265H) Dell models is that it's a big increase in battery life and performance. The integrated graphics (e.g. 140V and 140T) are very capable compared to a **workstation** grade NVIDIA Ada 500 GPU, but they are not even comparable to a gaming GPU like the GeForce 4060 or even a 5050.  The integrated graphics do however get used for 99% of all workloads unless explicitly specified because they are vastly more battery efficient and draw less power compared to a dedicated NVIDIA chip, meaning you can have a much smaller external power supply, and your graphics performance in those basic desktop workloads with one of these chipsets will be **much** better than previous generation Intel chips. Exceptions are obviously something like gaming or AutoCAD that specify to use the high performance dedicated graphics chip.  140V/140T are barely functional for modern AAA gaming, but if you stay 5-10 years back for AAA titles you might be okay. It will smoke most Indie games. Just look at the per title benchmarks for a 140V/140T and you can see if your game benches. You could probably get away with a lot of functional mobile gaming without a dGPU, but I wouldn't expect to be able to play a recent Call of Duty or Black Myth or anything with anything like an acceptable framerate at a decent resolution. This integrated graphics chip compares very favorably to its more common Ryzen 7 equivalent, I believe the 780M, and it's a very good APU for handhelds overall due Lunar Lake's power efficiency compared to other X86 chips.  You have to understand that for these next two generations Intel seems to be making big strides in terms of both power efficiency and integrated graphics for mobile, it's a very attractive option and the first time I've seriously considered a laptop without a dGPU. I think Panther Lake is going to be a very nice kit next year for both laptops and handhelds and give AMD a run for its money.   I suspect AMD genuinely needs a new APU graphics architecture implemented next year to keep up, which I expect them to. Not a bad problem to have.",Intel,2025-11-09 18:46:41,2
Intel,nnbn2as,"Yeah for sure. It's a small die and should be yielding pretty high. See also the number of 4+8+4 SKUs. Looks like the larger CPU tile is also yielding decently, so not a ton to cut down.  I'm partly saying that because a larger Intel3 die was certainly possible. Even if it was 6 Xe3 cores and built as half of the larger die (just one of the two render slices)  it would fill the void a bit more.",Intel,2025-11-05 22:51:02,9
Intel,nndnorc,Which is rather silly. They should've named celestial Xe3 and the current Xe3 as Xe2P,Intel,2025-11-06 06:46:30,8
Intel,nndrn8v,"> But Alchemist, Battlemage, Celestial is GPU family name.  Specifically, *discrete* GPU family name.   > because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture  That is simply not true. Xe3 brings much bigger changes over Xe2 than Xe3p does over Xe3. That's why they were named that way.   > Intel confirmed Celestial will have Xe3P.  No, they actually haven't said anything about Celestial (again, as a dGPU) at all. They said that C-series naming (i.e. NVL iGPU) will start with Xe3p.",Intel,2025-11-06 07:23:49,3
Intel,nnbivk1,Highly immersive porn on the go,Intel,2025-11-05 22:28:50,13
Intel,nnc0bgd,These are H series chips. Even the U series chips don't go down to LNL min power levels.,Intel,2025-11-06 00:06:14,3
Intel,nnbkp8h,"NVL-H is 400 series, *replacing* this, next year. Not supplementing this lineup.   Adding more cores won't do anything for gaming.",Intel,2025-11-05 22:38:29,7
Intel,nnbkxvp,PTL extends up to the -H series too.,Intel,2025-11-05 22:39:46,4
Intel,nnbkts2,"HX this year will still be Arrow Lake.   Nova Lake will be a full line up, with S, U, H, and HX, but end of 2026 / early 2027",Intel,2025-11-05 22:39:10,3
Intel,nnbl91n,">Source??  You can't seriously be asking for a source for whether or not this part will be able to power dGPU gaming laptops better than X3D chips.   >Also Panther Lake is H series only, HX will be based on Nova Lake.  Not till late next year or early 2027. It's all arrow lake till then.",Intel,2025-11-05 22:41:25,4
Intel,nnc0woc,> Source?   Common sense suffices. It's a tick core with a clock speed regression at that.,Intel,2025-11-06 00:09:35,3
Intel,nnc2335,"Surely a cost decision. The 4Xe die, including the choice of Intel 3, is supposed to be the cheapest thing to deliver an acceptable mainstream PC experience. They need PTL to be a proper volume runner and start displacing the RPL that's still a large chunk of sales. WLC should hopefully finish the job.",Intel,2025-11-06 00:16:15,9
Intel,nnblgop,Just get a Vision Pro?,Intel,2025-11-05 22:42:32,5
Intel,nnbyre8,"Tbh, more cores would just make that go faster, but 16 would already be plenty. Especially for something like that where it's probably going to be a linear analysis and ram constrained if they actually modelled the gas (which would not necessarily be required).",Intel,2025-11-05 23:57:18,1
Intel,nnbmp20,Lmao this is funny we both responded to the same comments with the same things within like 2 minutes of each other.,Intel,2025-11-05 22:49:05,4
Intel,nnbrc9f,I don't see anything wrong with asking for actual benchmark information especially when there isn't anything official. X3D is nice but it isn't the end all be all. I would be curious to see if Intel can manage to compete.,Intel,2025-11-05 23:14:48,0
Intel,nnc5j69,"Oh I totally agree, but it would've been nice you know? Jumping to 6 Xe3 is a  significant area increase for a tiny tile. I understand exactly why the 4-10 gap exists, but I can't say I don't wish there was something to fill that gap if only because it looks weird.  I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.",Intel,2025-11-06 00:35:47,5
Intel,nnd2wbt,"PTL's main changes are fixing MTL/ARL's terrible SoC design, which should net a few % performance. It'll see a mild IPC increase, getting a few more % performance. And it'll lose a bit of clockspeed, erasing most of those gains.  Expect PTL to be very similar performance to ARL, but with lower power consumption, a much better iGPU, and most importantly to Intel: Using their own fabs instead of TSMC.  It absolutely won't be X3D in gaming.  Edit: Actually shocked that people think this would compete with X3D.  9955HX3D is \~16% faster than a 275HX in gaming...and a 275HX itself is easily 10%+ faster than a 285H in gaming.  Not even Intel themselves are claiming this. Their own marketing refers to PTL as ""ARL performance with LNL efficiency"". Nobody realistically expects PTL-H to see a 25%+ gaming improvement over ARL-H. The fact that IPC increase is less than 10% and clockspeed is slightly lower than ARL-H should make this obvious",Intel,2025-11-06 04:01:09,4
Intel,nnc1auf,CGC is a LNC tick. This is well known at this point. And we see it's even a clock speed regression.    Even entertaining the notion it will close the gap to AMD's X3D chips is just delusional.,Intel,2025-11-06 00:11:48,1
Intel,nncm4mg,"Oh, yeah, I get you. Wish they could give more granularity. Just personally think some sacrifices are worthwhile if it can condense Intel's mobile lineup back down to something sane again.   > I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.  Yeah, should be a good fit. Shame they don't have anything with a bit more CPU umph, though. 4+8+4 and only up to 5.1GHz is *fine*, but not great. Especially without an HX replacement.",Intel,2025-11-06 02:14:47,7
Intel,nnj3sul,"Two options seems right, either you care about it or you don't.",Intel,2025-11-07 02:28:49,1
Intel,nnh5cgt,"Its not ""delusional"" to want to see actual numbers instead of speculation. I have been in this game long enough to see plenty of speculation even with accurate information not give the actual numbers.",Intel,2025-11-06 20:02:16,1
Intel,nncnlt8,"Given how well ARL HX was received in gaming laptops, I think they may wait to have something from NVL take that top spot. 5.1ghz does seem low though. ARL-H will happily do 5.4 and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP.  I suspect these may not be totally final clocks though they do seem reasonable.",Intel,2025-11-06 02:23:44,3
Intel,nncrc61,"Honestly, surprised ARL-HX is doing as ok as it is. The deficits of the architecture in gaming are well known. If it could hit the same clocks and core counts, PTL should look a lot better still. And all that besides, ARL's cost structure is horrible. For Intel's own sake, the sooner they move on, the better.   > and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP  From the same leaker, these chips are at 65W or even 80W TDPs, so they're not merely power limited. It seems that 18A just significantly underperform some expectations, though in line with some rumors and the gist of the revisions Intel's been making to its projections over the last year or two.  > I suspect these may not be totally final clocks though  If they're defining SKUs and such, these clocks need to be finalized for all practical purposes.",Intel,2025-11-06 02:46:29,0
Intel,nnd4ak4,"If I'm reading correctly those are max power limits, not the TDP,  though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  As for why ARL HX is doing well in gaming laptops, I think a good bit of that is also part of what made it lackluster on desktop. It doesn't really scale up that well with higher TDPs and power limits, but it does seem to scale down. The 285HX with its 55W TDP and 160W max limit doesn't perform far off the 125W TDP and 250W max of the 285K.  It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system. The 9955HX3D is very impressive, but quite a lot of laptop buyers seem to value the ability to do more laptop-like things with their gaming laptops than the extra frame rate. I'm hoping this gets shaken up as AMD adopts new packaging tech as seen in Strix Halo.",Intel,2025-11-06 04:10:51,2
Intel,nndr8zp,"> If I'm reading correctly those are max power limits, not the TDP, though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  Not guaranteed given it's just a twitter leak, but I'm assuming the leaker is using the term TDP consistently with Intel's historical usage, i.e. PL1. For ARL, 115W is PL2. I would also assume there wouldn't be the disclaimer about it being hard to cool if they cut the PL2 so much, though PowerVia does create some interesting complications there, so maybe not quite apples to apples.  Either way though, don't think it should have much impact on ST boost. You're talking a good 70%-ish of power going to compute, so even at 65W PL2, that's still 40-50W available for one core. Should be *easily* sufficient to hit whatever the silicon is capable of.  > It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system.  Yes, and this is something I've very much looking forward to with NVL-HX. At this point, the biggest demerit of the -HX platform vs -H is the use of standard DDR5 vs LPDDR. That's because it's still based on the desktop silicon with the different SoC/hub tile. But with NVL using a shared SoC die, they should be able to offer an -HX platform with the core counts people expect (though probably limited to single die 8+16), but the power/battery life advantages of -U/-P/-H. In general, should help make the -HX more of a straight-up upgrade than the tradeoffs one faces today.  AMD has this situation even worse today, because there's a much bigger gap between their desktop SoC architecture and the mobile one. Though as you say, they may also bring them closer together in the future.",Intel,2025-11-06 07:19:59,4
Intel,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,11
Intel,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,5
Intel,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
Intel,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
Intel,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,2
Intel,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
Intel,np6680l,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
Intel,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
Intel,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
Intel,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
Intel,nn1h3l3,Great work dude! Only 200MHz to go 😉,Intel,2025-11-04 11:15:21,2
Intel,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
Intel,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
Intel,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,3
Intel,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
Intel,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
Intel,np7d3w5,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Intel,2025-11-16 20:40:17,1
Intel,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
Intel,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
Intel,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
Intel,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
Intel,nmoztyk,But Core Ultra 255Hx is almost $150 more than Zen 4 8945HX on lenovo Legion. That price gap is enough to upgrade 5060m to 5070m.,Intel,2025-11-02 12:36:20,6
Intel,nmzufnv,"Can I redeem the codes to my accounts on a different Intel system? I bought a B860 motherboard and an Ultra 5 245k, but I won't be building that system till Christmas. I'm currently running an 8700K on a Z370.",Intel,2025-11-04 02:52:11,2
Intel,nmquye0,Idk if I did it wrong but redeemed my cpu but not my arc card on the website. Couldn’t contact support because it kept throwing invalid captcha at me.,Intel,2025-11-02 18:34:43,1
Intel,nqarc70,just purchased a laptop from micro center with a 275hx but don't know how I would redeem this specific offer as I only get the one that lets you pick 1 of 4 games. Does mine not qualify/is micro center not participating?,Intel,2025-11-23 03:04:10,1
Intel,noa52e1,"nope, you need to have installed Ultra processor to get promo game, because Intel used software to check it",Intel,2025-11-11 13:57:33,1
Intel,nlbgoss,I really would like to know how it will compare to AMD Ryzen AI MAX+ 395 (Strix Halo) APU?,Intel,2025-10-25 14:51:22,12
Intel,nlawueh,That naming scheme really is complete and utter dogshit,Intel,2025-10-25 12:59:00,6
Intel,nlp3wrh,Sounds like fooz will be getting a taste soon? Christmas or Q1 2026?,Intel,2025-10-27 19:03:55,1
Intel,nlrilyu,Can we just toss random darts at tech words and numbers to assign a different naming scheme to each and every different Intel Product?,Intel,2025-10-28 02:55:34,1
Intel,nlz6ywu,I'm looking forward to check how those series will perform!!,Intel,2025-10-29 09:04:56,1
Intel,nlbj26i,"This isn't a ""halo"" product. An upgrade to Arrow Lake/Lunar Lake when it comes to iGPU, but not Strix Halo. It won't cost as much as well.",Intel,2025-10-25 15:03:33,19
Intel,nlbihha,look forward to new APUs,Intel,2025-10-25 15:00:35,2
Intel,nlhde6w,Wait for Nova Lake AX. It will have 48 GPU cores instead of 4/8/12.,Intel,2025-10-26 14:36:10,2
Intel,nlpqfuu,Likely will be worse since it has only 1/3 the power usage and significantly smaller chip size.,Intel,2025-10-27 20:58:10,1
Intel,nmbs4xl,"It's meant to confuse you in purpose, so you ignore it and go by the 3/5/7/9 scheme. Marketing success is dependent on the company leading the customers to the way they want it. So it needs to be complex and confusing.",Intel,2025-10-31 06:11:07,1
Intel,nldt8zb,"also why is everything in a lake. like really that's the last place you want your chips to be.  and why is it 255k, 285k and so on instead of just 250k and 280k.  and why is it arc xe3 and not just xe3 ahh  so many weird conventions really. no consistency at all. just pure confusion.  apple and nvidia get it right too, it's really not that hard.",Intel,2025-10-25 22:15:21,1
Intel,nlclyxd,"'Strix Halo' is lees about being a halo product, because it’s just an AMD internal naming scheme for the set of specific APU products. APUs that will follow 'Strix Halo' will be called 'Gorgon Point'. Since 'Strix Halo' is already out and the Panther Lake in question is about to come out it might be better to compared it to the next AMD 'Gorgon Point' APU line? But I still think it can be compared to 'Strix Halo'. At least I’m looking forward to see the comparison.  Edit: changed 'Strix Point' into 'Gorgon Point' since that’s what I meant.",Intel,2025-10-25 18:23:40,-7
Intel,nlguu28,You’ll be waiting till 2027 on the amd side.,Intel,2025-10-26 12:44:06,3
Intel,nmbs0u8,All halo iGPUs are way too overpriced. Even regular iGPUs are overpriced going into $1K laptops.   And if you want to spend that money you can do it today with Strix Halo.,Intel,2025-10-31 06:09:58,1
Intel,nlq5vco,"Lesser power usage doesn’t always equals to performance losses.  See the Apple Axx SOCs and the Qualcomm snapdragon SOCs.  Even with the x86-CPUs over time they had more performance gains while maintaining almost the same power consumption.  Not anymore, but they could catch up with better chip design?",Intel,2025-10-27 22:20:54,1
Intel,nlg1l4u,>also why is everything in a lake. like really that's the last place you want your chips to be.  Easier to cool them,Intel,2025-10-26 08:16:14,7
Intel,nlp4s9i,"Stop trying to find meaning in the naming scheme. Patience grasshopper, all will be revealed in time.",Intel,2025-10-27 19:08:23,2
Intel,nlpqrfz,Like how is every chip company so bad in naming stuff?  Intel names things from geographical stuff to avoid politics etc. So I get the lakes  The numbers I dont.,Intel,2025-10-27 20:59:49,1
Intel,nlcna8j,"> APUs that will follow 'Strix Halo' will be called 'Strix Point'.  What? No, that's not what any of these terms mean. The Strix lineup is already out. ""Strix Halo"" is literally their halo, big compute/iGPU product.  PTL will compete with normal Strix Point and its refresh next year.",Intel,2025-10-25 18:30:27,10
Intel,nlp4f4m,Really? This should be good for Intel in 2026.,Intel,2025-10-27 19:06:31,1
Intel,nmbt7jh,These Halo iGPUs are meant for LLM first and foremost. If you just want to game just get a normal RTX laptop.,Intel,2025-10-31 06:22:00,1
Intel,nlqexwa,They are like 20% more efficient.  You are talking about 200% more efficient if it hits the same performance.    They might as well make it a mobile chip if that is how good it is. Would be great on phones.  Would be better than most pcs right now on 5W envolope.,Intel,2025-10-27 23:11:50,1
Intel,nmbsirj,"It's intentional, and non specific. So most go by the 3/5/7/9 naming. In order for them to have max profit, they need to lead you to the chips they want you to buy.",Intel,2025-10-31 06:15:03,1
Intel,nld2j3h,'Strix Halo' is a product line as 'Gorgon Point' is also/another product line. Both have different variants.  In the case of 'Strix Halo'  [Check](https://hothardware.com/news/amd-strix-halo-cpu-rumors)  Find the official lineup here  [AMD](https://www.amd.com/content/dam/amd/en/documents/partner-hub/ryzen/ryzen-consumer-master-quick-reference-competitive.pdf)  Edit: changed 'Strix Point' to 'Gorgon Point' since that what I meant.,Intel,2025-10-25 19:51:15,-3
Intel,nlqjpxg,👍,Intel,2025-10-27 23:38:19,1
Intel,nld2x4w,"Yes, Strix Point lives *alongside* Strix Halo. Strix Point is already out, and came out *before* Strix Halo at that. It is not the followup to Strix Halo.",Intel,2025-10-25 19:53:17,9
Intel,nld43st,You are right. I meant 'Gorgon Point'.,Intel,2025-10-25 19:59:34,1
Intel,nln841i,"Gorgon point is just a refresh of strix point. These terms mean less in the next gen anyway as the follow on products have quite different names. Medusa point isn't really the successor to gorgon point, for example.",Intel,2025-10-27 13:21:07,2
Intel,nlp48zs,My brain hurts and I’m still confused,Intel,2025-10-27 19:05:38,1
Intel,nmbrwqs,That's why you have guys that are completely ChatGPT-levels of confident when they are completely out of whack with the info lol. No worries.,Intel,2025-10-31 06:08:49,2
AMD,nxxyy8i,Does this pair well with Nvidia’s newly release of the 3060?,hardware,2026-01-06 03:20:50,25
AMD,nxxmhi8,"So exactly the same specs as the 9800X3D but clocked slightly higher? Seems incredibly pointless, but I suppose it's a convenient way for them to make extra money off the chips that win the silicon lottery.",hardware,2026-01-06 02:11:50,73
AMD,nxxkel3,Would be killer value if this replaces 9800X3D at the same msrp. But it's AMD so they will price it at $549 and sell it alongside 9800X3D.,hardware,2026-01-06 02:00:35,47
AMD,nxyyl7u,"So, no 9950X3D2 announcement? I wonder if it was just a rumor after all.",hardware,2026-01-06 07:39:03,10
AMD,nxxnb08,"it just exists to keep the prices high and higher when it replaces 9800x3d which is already too expensive frankly but what you gonna do, buy intel?",hardware,2026-01-06 02:16:19,23
AMD,nxzdpug,"I'd rather it have a better memory controller so you could do 6400 6600 1:1, not 200 extra mhz but we get what we get",hardware,2026-01-06 10:02:41,4
AMD,nxxp3jo,Release 👏 more 👏 AM4 👏 X3D 👏 chips 👏,hardware,2026-01-06 02:26:02,21
AMD,ny0hjfh,"Intel Nova Lake-S with bLLC can't come fast enough, AMD just slapping X3D to everything and not doing anything interesting at all. At least Intel's APO+ will be special sauce along with their bLLC technology.",hardware,2026-01-06 14:38:00,3
AMD,nxywuqr,Hopefully the 9950x3d2 too,hardware,2026-01-06 07:23:00,2
AMD,ny03cn2,I was just about to get 9800x3d. Should I,hardware,2026-01-06 13:19:36,1
AMD,ny08w0s,cant wait to push this baby to 5.9 ghz or higher <3,hardware,2026-01-06 13:51:01,1
AMD,ny2l8d6,I just got the 9950x3d and had no clue about this until today😂😂 not mad but I might’ve waited,hardware,2026-01-06 20:26:26,1
AMD,ny2w378,"""4% more performance, 20% more cost."" All jokes aside, halo products do be like that. I'm assuming it'll come in at an MSRP of $549.99, but I hope it's more like $529.99 so at least it makes sense for the price to performance gain if you just want the absolute best CPU for gaming.",hardware,2026-01-06 21:16:16,1
AMD,ny3gx28,"I was not expecting amd to release a new cpu, am so closing to finishing my pc too just need cpu I might wait a little longer to save up and buy this cpu instead of the r7 9800x3d, I read it’s going to have a tdp of 120w I hope my psu is enough for it considering I have an rx 9070 xt.",hardware,2026-01-06 22:54:15,1
AMD,nxyagl1,"No, need more CPU for that if you want to play CS2 at 640x480p with below minimum graphics settings.",hardware,2026-01-06 04:30:14,7
AMD,ny00uek,if you plat MMOs then yes.,hardware,2026-01-06 13:04:22,8
AMD,ny397o2,"It doesn't have AI in the name, so clearly no. Zero future proofing, dead on arrival, no place in the market with zero useful performance metrics.  Once again, AMD never misses an opportunity to miss a opportunity.   /s",hardware,2026-01-06 22:17:00,2
AMD,nxxo2dm,If it translates to 7% better peak performance it could be worth it. The 9800X3D is already pretty awesomely fast.,hardware,2026-01-06 02:20:25,46
AMD,nxyire2,"Yes, pretty common for a CPU sku. This should at least be more manageable than the Intel version (KZ CPUs).",hardware,2026-01-06 05:27:19,3
AMD,nxyeem4,it has a new step which allows memory OC to 9800,hardware,2026-01-06 04:56:30,2
AMD,ny1hdpn,Everything always pointed to it just being the equivalent to a -KS version of a 9800X3D.,hardware,2026-01-06 17:26:07,1
AMD,nxz6guz,"Can't tell without the pricing, but if it'll be a price bump as expected, why exactly would one get this over a 9950x3d that already has a higher clocking cache ccd while not being an overpriced 8 core cpu?",hardware,2026-01-06 08:53:08,10
AMD,ny07opx,I'm betting this will be $499 and the 9800X3D will go to $449. AMD confirmed they will continue selling the 9800X3D.,hardware,2026-01-06 13:44:21,1
AMD,ny1fihl,"CES rumors has the MSRP at $499, slight bump from the 9800X3D at $479 I believe. That chip is still at $469 right now.",hardware,2026-01-06 17:17:38,1
AMD,nxxqv30,"hey, 12/13/14 with DDR4 is gona be the way to go for a ton of people rofl  i would laugh if they started to make 5850X3D soon",hardware,2026-01-06 02:35:48,22
AMD,nxypfeh,"It's not replacing the 9800x3d, both will exist side by side.",hardware,2026-01-06 06:19:09,3
AMD,nxxnx1n,Intel with ram,hardware,2026-01-06 02:19:37,4
AMD,nxyu9ss,delusional,hardware,2026-01-06 07:00:04,-1
AMD,ny11elk,"With that much cache, does the additional memory bandwidth even improve performance? I'm genuinely asking.",hardware,2026-01-06 16:13:26,3
AMD,nxy9rjp,Ryzen 5500X3D with PCIe3.0 speeds /s,hardware,2026-01-06 04:25:43,6
AMD,ny00xl8,never going to happen.,hardware,2026-01-06 13:04:56,2
AMD,nxy1dz6,AMD is saying a 2-3% uplift over the 9800X3D.  https://www.digitalfoundry.net/news/2026/01/amd-unveils-ryzen-7-9850x3d-fast-incremental,hardware,2026-01-06 03:34:49,44
AMD,nxxrngk,"7% higher boost at stock, but the 9800X3D can already hit 5.6-5.7 via ECLK and I doubt the 9850X3D will reliably do 6.0-6.1.  Serious overclockers will probably still want it for the better binning. Hard to say it will be a better value for anyone else yet.",hardware,2026-01-06 02:40:07,32
AMD,nxymq6e,Is this confirmed? Or is it still just that one screenshot and MLID's source?,hardware,2026-01-06 05:57:30,8
AMD,nxz68tb,a new strap means absolutely nothing except to 5 OC people when the iod is the same,hardware,2026-01-06 08:51:00,6
AMD,nxyiu8v,Woah,hardware,2026-01-06 05:27:54,0
AMD,nxz92km,"9950x3d are more usefull for multi-threading while 9800x3d bench show better perf at single-thread gaming  Which was my hope for 9850x3d, but 2-3% for probably 150-200$ more isn't worth it, better to just OC the 9800",hardware,2026-01-06 09:18:24,6
AMD,nxz7od0,Same reason people get 9800X3D over 9950X3D to be honest. Gaming vs AI,hardware,2026-01-06 09:04:41,3
AMD,nxy8w8y,"If they made a 5850X3D, I would be first in line to buy one.  I doubt they would since it seems chip packaging is a big bottleneck at the moment and it has to be more profitable to make 9000 series CPUs.",hardware,2026-01-06 04:20:08,11
AMD,ny1r5tm,[It seems to at 4K.](https://www.youtube.com/watch?v=EBF6B-f5me0),hardware,2026-01-06 18:09:59,2
AMD,nxyadpc,PCIE3.0 speeds and cut down lanes.,hardware,2026-01-06 04:29:43,2
AMD,ny1ehnc,You must be fun at parties,hardware,2026-01-06 17:12:59,-2
AMD,nxyt3ba,atleast they are honest,hardware,2026-01-06 06:49:59,15
AMD,nxyq5cx,Then I'd say that would warrant maybe a 5-10% price hike at most!,hardware,2026-01-06 06:25:04,5
AMD,ny07azf,AMD said last night that gaming uplift will be 7%. Which just about the same uplift as the 9800 was over the 7800.,hardware,2026-01-06 13:42:13,6
AMD,nxznkht,"Which you'd need an ECLK board for in the first place.  Meanwhile with 9850X3D 5.8 will just be enabling PBO, seems pretty sweet to me.",hardware,2026-01-06 11:29:01,6
AMD,nxzp2sm,Was seen with a valid benchmark.,hardware,2026-01-06 11:41:08,-1
AMD,ny3hp8u,"My understanding is that there’s virtually no difference for gaming. Digital Foundry had the 9950x3d performing within 1% of the 9800x3d in most games.  If you’re only gaming obviously the 9800x3d has much better price:performance, but you aren’t losing gaming performance by getting a 9950x3d, just gaining productivity performance.",hardware,2026-01-06 22:58:09,1
AMD,ny15xzk,"AI? The 9950X3D is more useful for classic ""HPC"" tasks, at least for the subset that sees the benefits of the additional cache",hardware,2026-01-06 16:34:12,5
AMD,ny39k7o,"Or for all the countless stuff that actually uses more cores, rather than AI.  Plenty of current popular ""AI"" stuff are heavily GPU bound and notoriously don't care at all what cpu you use, 4 core or 40 core processor. 9950X3D is probably one of the worst choices you should select when balancing out building a machine dedicated to ""AI.""",hardware,2026-01-06 22:18:40,2
AMD,nxy9vmv,"yeah, I mean, it would be a value play and it would make no sense unless 78 and 98 demand craters hard or something  it was a joke that I think will never happen, or rather, I pray never happens because it means that DDR5 got ratfucked",hardware,2026-01-06 04:26:28,2
AMD,ny0otr1,I know you're trying to make a joke but it's a dumb one.  The 5500 has 20 PCIe 3.0 lanes available.  It's up to the motherboard to determine how those lanes are allocated.,hardware,2026-01-06 15:14:47,2
AMD,ny028ex,"Yes, I hope +200 at least is still nearly guaranteed. That 2-3% figure in the DF article worries me a bit.",hardware,2026-01-06 13:13:01,3
AMD,ny1p6b2,"""AI"" is the new know-it-all buzzword.",hardware,2026-01-06 18:01:09,3
AMD,ny3m9fu,"Ddr5 is ratfucked. By the time prices come back down to even half of what they are, drr6 will be moving in. And you know ddr6 is going to start at the price ddr5 is at.    It's GG for home computing unless the americans wake up and do something about their technocracy problem. So it's GG for home computing.",hardware,2026-01-06 23:21:17,2
AMD,ny03vxc,For games I'd imagine it could hit that quiet well since they aren't really heavy workloads but I'm really interested to see aswell how good these chips manage.,hardware,2026-01-06 13:22:47,2
AMD,ny3a68x,"Haha so true.  I'll buy a fancy 5000 dollar computer case made with exotic materials and has too much RGB fans!  Why?  Because AI needs it!  It also needs this 3000 dollar HDMI cable, I can practically hear the AI singing in high fidelity from the digital bits!",hardware,2026-01-06 22:21:34,1
AMD,ny4yjkq,Seems like it would make one heck of a NAS CPU.,hardware,2026-01-07 03:40:31,2
AMD,ny4ql8t,just in time for OpenAI to announce they're out of stock until 2030,hardware,2026-01-07 02:55:40,-1
AMD,ny54o6l,Synology will use it in their NAS in 2035,hardware,2026-01-07 04:17:45,3
AMD,nxxeocb,Great.  Does this mean AMD will finaly stop pricing Strix Point as if it was made out of gold ?,hardware,2026-01-06 01:29:45,290
AMD,nxxrlux,"One of the biggest things the current AMD driven handhelds lack is a decent upscaling option, so getting native XeSS support on a fast GPU would be a HUGE performance boost.",hardware,2026-01-06 02:39:52,94
AMD,nxy2sll,"I think the LPE cores and them going at chiplets a second time after Meteor lake is paying off. This chip is more efficient than lunar lake, a chip that could do 0.62W idle lol.",hardware,2026-01-06 03:42:57,33
AMD,nxy9wxm,"This is exciting. Hope some decently priced handhelds can drop, RAM prices notwithstanding.",hardware,2026-01-06 04:26:42,10
AMD,nxxr090,am confused. this is battlemage too right? because its a B series. but its supposed to be all new. and the old gen was battlemage too on the 200V series. so what is going on here?. is this just a bigger GPU or is this Xe3 so that would be Celestial.,hardware,2026-01-06 02:36:35,21
AMD,nxxhg0r,brah they straight up claiming it's equivalent to a 4050 on stream >!(a 60W RTX 4050)!<,hardware,2026-01-06 01:44:36,52
AMD,nxyc1w5,Xps is a huge seller for Dell and they are straight up using Panther Lake and XE3. They are exclusively going intel. Intel is 100% securing up there dominance in Labtops. In the process also taking business away from Nvidia.,hardware,2026-01-06 04:40:47,15
AMD,nxykdxb,"Even if that claim were overstated by 2x, would still be a colossal L for amd.",hardware,2026-01-06 05:39:32,16
AMD,ny1ev6y,XESS and native frame gen is going to make handhelds monsters with Panther Lake in them.,hardware,2026-01-06 17:14:42,4
AMD,nxxgruz,I hesitate to trust Intel's charts. But I am interested if Intel will actually get companies to adopt panther lake for their handheld pc. They did not have much luck with lunar lake.,hardware,2026-01-06 01:41:01,25
AMD,ny1ifg6,"Assuming intel also keeps those mobile CPUs a good price, this could be really good. Hopefully as well they add the B390 in their high power desktop CPUs, seeing a core ultra 5 with an iGPU like this would really mitigate the need for a dedicated GPU right away Mostly because iGPUs on other generations were bad, and only a select few Ryzen CPUs had the 890M. Budget systems could become much better for gaming on the low side for graphical intensive games",hardware,2026-01-06 17:30:54,3
AMD,nxz4ji6,>Intel reference platform; Memory: 32GB LPDDR5 9600;  I wonder how much difference that makes and if we'll even see laptops with such RAM in this economy...,hardware,2026-01-06 08:34:38,5
AMD,ny34ie7,"I'd love to see benchmarks comparing it to lower end discrete GPUs (like 5050, B570, etc). Could be a boon for ultra low cost builds depending on what price point it lands at.",hardware,2026-01-06 21:54:52,2
AMD,nxxyz1h,How many compute units does it have?,hardware,2026-01-06 03:20:58,3
AMD,ny0ibg4,I think people need to be ready for the fact that OEMs aren't going to use lpddr5x-9000,hardware,2026-01-06 14:42:06,1
AMD,nxxd70q,"We'll see. Every year they claim they're faster, and every year they have been proven not to be",hardware,2026-01-06 01:21:44,-25
AMD,nxxgaz9,NOTE: this might be because it has MFG (Multi-Frame-Generation).  We have to see reports to see if its true or not.,hardware,2026-01-06 01:38:30,-11
AMD,nxxoge8,"This ain't gonna matter. It's the sku with 50% more igpu cores compared to lunarlake, which already has an igpu that's larger than the hx 370, it's real expensive. Imagine a hx370 with 26cu instead of 16, that's the price range you're lookin at  Any system built with this is gonna need to run at extremely high mem speed to feed the really large igpu which in the current market with insane ram prices is gonna be priced out of most people's budget. Are ya prepared for a gaming handheld that costs north of $1500?  And since this is gonna compete against nvidia's entry level mobile gpus oems are gonna have to choose between nvidia and intel for the gaming brand on laptops. Amd learned this through the hard way that most oems would choose nvidia over a large igpu.",hardware,2026-01-06 02:22:32,-11
AMD,nxykt3q,Haven't they been making similar claims for all their failed GPU's?,hardware,2026-01-06 05:42:44,-11
AMD,nxyax11,"I mean Intel has never fudged the numbers before when they were behind, or do something crazy like literally bribe people.... Oh wait.... Uh.... Oh.....   Jokes aside, with what the current and future state of the market looks like, people might have to get used to iGPU graphics.",hardware,2026-01-06 04:33:13,-16
AMD,nxyljzq,"To be fair it kind of is, the die size is huge, larger than an RTX 5070 die",hardware,2026-01-06 05:48:23,51
AMD,nxxotcs,"Intel laptops were already better tbh, AMD had nothing to compete with Lunar Lake, and Arrow lake pretty much was better at high perf efficiency. Zen 6 better not be delayed or AMD will be buried under intel, qualcomm and apple all launching a real next gen shortly",hardware,2026-01-06 02:24:30,82
AMD,nxz1mq8,"Nah, because it's an ""AI chip"" and AMD will market it as so. AI equals fancy even though the AI capability can't match a regular desktop computer for far less.  Intel is probably gonna strategically match AMD in price.",hardware,2026-01-06 08:07:06,5
AMD,nxxvwrz,"With how wide the memory bus is, how much RAM it requires, nah the price is going up.",hardware,2026-01-06 03:03:31,22
AMD,nxzxy26,it has soldered ram ... so it's better then gold!,hardware,2026-01-06 12:45:44,2
AMD,ny04wij,I guess that depends on Intel pricing too. Considering it's using both the latest TSMC and Intel foundries in one chip package. Not to mention the LPDDR5 9600.,hardware,2026-01-06 13:28:38,2
AMD,nxxhzsm,You mean OEMs.,hardware,2026-01-06 01:47:35,-11
AMD,nxyn46u,No?  People will still value AMD more ue to brand so Intel will have to rely on volume for revenue  For reference only yesterday on this sub we had people talking about Intel lacking efficiency in comparison in mobile space,hardware,2026-01-06 06:00:32,-11
AMD,nxy8gkb,Crazy AMD haven't updated their iGPU to RDNA 4. I know they're probably waiting for UDNA but it would have been almost 3 years on the same architecture by the time we get the UDNA refresh next year (if they even bring it to their APUs right away). Sort of disappointing.,hardware,2026-01-06 04:17:21,56
AMD,nxxun0v,"tbf the most important issue is, few games implemented XeSS, just like AMD FSR.  And I think XeSS 3 being implemented in more games is a net positive for AMD GPU too.",hardware,2026-01-06 02:56:27,21
AMD,nxz85mh,"With everything happening around NVIDIA´s price increases and AMD´s lack of providing updates where it hurts, it **feels like** AI-Datacenters are more important right now for them (like the last 2 years).  But who can resent them as Intel had products that where not so much competitive that time.    Arrow lake (Desktop) at least closed on efficiency, but lacks a bit of gaming performance still, hopefully Nova Lake will be the step required to push more competition.   On GPU the same, AMD does not compete with NVIDIA in higher segments while NVIDIA is fairly comfortable with their setup and increases prices because they want to milk customers to increase their ridiculious margins (up to 70%) that they are used to from AI-Chips.   And now Intel also provides Multi-Frame generation, while a niche for me still, starting to compete with AMD and closes up to NVIDIA in terms of Software support, which they lacked the most and fixes a lot of problems.   Now let them release a B770 that is rumoured to be fairly mid/high range and we can hope for competition that actually learned from bad products recently and tries to make it better.",hardware,2026-01-06 09:09:24,9
AMD,nxz92l2,"Handhelds is a tiny tiny market, basing your product stack around them would be monumentally stupid.",hardware,2026-01-06 09:18:25,2
AMD,ny0x7rg,"It may not beat LNL in very low power envelopes (LNL was designed for ~10W, PTL for 15W+), but it's a much, much better baseline than what Intel's historically had in client. Even just extending vaguely LNL-tier efficiency across the stack is a very big deal. Looks like Intel finally has a respectable SoC architecture. Now just need to get the cores and such in shape.",hardware,2026-01-06 15:54:14,6
AMD,ny4y069,"I mean the Xbox Ally X handheld is considered a $999 ""console"" so it sets the floor for what the Steam Deck and other handhelds would be priced at.",hardware,2026-01-07 03:37:21,1
AMD,nxxwdna,"It's branded as a Battlemage for some reason, but the architecture is Xe3. It's much closer to Celestial than it is to Battlemage.",hardware,2026-01-06 03:06:08,50
AMD,nxxt4ce,"Battlemage is the brand name. The actual architecture of Lunar Lake is Xe2, same as desktop Battlemage, but they never explicitly called it Battlemage, only “Arc Graphics”.  What is meant to be desktop Celestial is Xe3P, but desktop Celestial is likely cancelled or significantly scaled back. Alchemist was a massive flop, and by the time the B580 came out to salvage Arc’s reputation the axe had probably already swung.",hardware,2026-01-06 02:48:07,9
AMD,nxxpiop,"They claimed ""10% faster"" than 4050   https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Famd-is-done-v0-8op4m6l6bmbg1.jpeg%3Fwidth%3D1851%26format%3Dpjpg%26auto%3Dwebp%26s%3Df229e1ff0e364a6db90715de23ba799261ffe9e3",hardware,2026-01-06 02:28:19,56
AMD,nxxqhlr,APUs are always way worse at gaming than synthetics when compared to a DGPU due to memory bandwidth limitation and power sharing with the CPU among other things like cache set up etc.  when they compare them to GPUs its always synthetics unless you get benchmarks of games,hardware,2026-01-06 02:33:43,19
AMD,nxxl8hj,"Where's the bandwidth coming from?   Reviewers were saying that the 890m was bandwidth starved, so how can this chip be neck and neck with a recent dgpu with multiple memory channels",hardware,2026-01-06 02:05:05,6
AMD,nxy0hn7,60W is the laptop power draw. It looks like 30W for the 4050  this is the laptop they used for the comparison https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor,hardware,2026-01-06 03:29:36,5
AMD,nxy6cgs,"At best, it’s a 16% difference between a 100 watt and 60 watt RTX 4050 I believe, based on synthetic performance  Edit: Intel used a 30 watt 4050, this comment is incorrect",hardware,2026-01-06 04:04:08,0
AMD,ny0j8e4,"I got downvoted everytime I brought this up, but this is precisely why Nvidia wanted a deal to have an Nvidia iGPU tile on an Intel APU: Large iGPUs in thin and lights are going to get good enough over the next few years to make them the new entry-level graphics option for people. This directly threatens Nvidia's consumer laptop volume in the entry segment. Intel is claiming close to 4050 performance at this lower TDP, and that's certainly good enough for many to not have the tradeoffs of having a dGPU in their laptop.  If the new market is moving towards putting GPUs on the CPU package instead of discretely on the board, Nvidia doesn't want to place all of their hopes on WoA becoming better, and are hedging by doing both their own SoC *and* an x86 APU with Intel.  The XPS line dropping Nvidia discrete all together is proof of this. In these sub 70W total laptop power markets, a discrete GPU is just eats into the power budget too much.",hardware,2026-01-06 14:46:50,8
AMD,nxzgk6d,"In the ultraportables market (like XPS), integrated graphics just make so much sense (energy envelope; cooling system required; battery life; etc); and that's already substantial and before considering the cost of a NVIDIA mobile dGPU itself.  I don't understand why AMD decided to price Strix Point and Strix Halo so ridiculously -- it's their market for the taking.",hardware,2026-01-06 10:28:32,6
AMD,nxzf0cx,I think they’re trying to take away business from Qualcomm/arm on windows before it takes off,hardware,2026-01-06 10:14:30,5
AMD,ny0ce80,What do you mean that a refreshed Strix Point can’t compete with an updated architecture?,hardware,2026-01-06 14:10:24,4
AMD,nxxj6sm,Intel charts for their gpus have been pretty on point   Msi claw with lunar lake is one of the best handhelds,hardware,2026-01-06 01:54:01,62
AMD,nxyetge,"Lunar Lake was a expensive product which didn't make sense in handhelds, Intel just didn't have anything else so they slapped that on the MSI Claw. Now the options should be much better considering they are selling a lower core Xe3 version for cheap too",hardware,2026-01-06 04:59:20,8
AMD,ny2yll4,"I'm just curious how they handle the need for such high speed RAM on desktop though? I guess this is an application where CAMM2 will be required, I don't think DDR5-9600+ is possible without it and this is presumably pretty key to the performance.",hardware,2026-01-06 21:27:43,4
AMD,nxzgr0g,"It certainly would make a huge difference because iGPUs are very memory bandwidth bound; and as the name suggests, LPDDR5 9600 has literally twice the bandwidth of the JEDEC standard 4800.  Unfortunately I doubt we'll see reasonably priced laptops with LPDDR5 9600 -- even as an add-on option. I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800; and many SKUs that were 2x16GB are now 1x16GB; yes **single channel**.... they charge you extra if you want 2x8GB.",hardware,2026-01-06 10:30:16,14
AMD,nxyivuh,"It has 12 Xe3 cores. Intel doesn't use the term Compute Units, AMD does.",hardware,2026-01-06 05:28:14,12
AMD,nxzfdoo,X9 and X7 have 12 Xe cores and the best Ultra 5 has 10 Xe cores,hardware,2026-01-06 10:17:51,3
AMD,nxxf8he,??? lunar lake has already shown to be faster than the 890m.  73 percent though seems like a bit much since panther lake was claimed to be around a 50 percent increase over lunar lake,hardware,2026-01-06 01:32:47,66
AMD,nxxh29b,Example?,hardware,2026-01-06 01:42:33,15
AMD,nxxhysy,"They did make a graph specifically to compare the performance of HX 370 and this Arc B390 while they were both using 2x upscaling, which is where this 73% number comes from. In another graph featuring supposedly ""native"" 1080p, they claimed Arc B390 was 82% faster than the HX 370 (why don't they just call it Radeon 890M though...)",hardware,2026-01-06 01:47:26,26
AMD,nxxhhap,"No, intel claims 73% with upscaling (both) and 82% native",hardware,2026-01-06 01:44:47,19
AMD,nxy83by,"If it was only a 73% gain *including* MFG, then that would be a serious performance regression. If they were using MFG in their graphs, it would easily be 200% - 300% ""faster"" at the same ""real"" performance",hardware,2026-01-06 04:15:06,5
AMD,nxxhnbn,"The graphs all listed games and I didn't see any synthetic benchmark scores were listed, so yeah.",hardware,2026-01-06 01:45:42,23
AMD,nxxsvz8,The relative proportion of the die isn't as important as the die size itself and the node ofc.   Lunar lake for example has an estimated die size smaller than the hx370 so even if they did make the die bigger I don't think that is going to massively raise the price. Not to mention intel owns the foundry unlike AMD who are outsourcing to TSMC. This isn't in the realm of a strix halo competitor with a 300 mm\^2 + die size.   Dell for example has already refreshed the XPS line with intel panther lake and cut out the option for a dedicated gpu.,hardware,2026-01-06 02:46:52,19
AMD,nxxhf0d,"There are 50% more GPU cores here than on Lunar or Arrow Lake. CPU is still 16 cores as well compared to Arrow Lake, just shifted from 6+8+2 to 4+8+4.",hardware,2026-01-06 01:44:27,27
AMD,ny08rcj,"Do you mean Strix Halo?  Halo is made up of THREE dies. Two are regular CCD and one is a ~300mm2 graphics die. Total die area is around 440mm2 IIRC.  It's expensive, but not THAT expensive.",hardware,2026-01-06 13:50:17,26
AMD,nxxs32m,Doesn't this depend on use case? AMD laptops are more capable for gaming and the iGPU can also use the lesser version of FSR. Intel is obv better for productivity.,hardware,2026-01-06 02:42:30,15
AMD,nxzgbtp,"I disagree, Arrow lake HX seems to be more expensive than AMD HX as least on Lenovo Legion Pro setup.   I would have buy Arrow lake for the same price but AMD is cheaper by $200.",hardware,2026-01-06 10:26:24,4
AMD,nxy0jsm,Eh? It's a standard 128 bit memory bus.​,hardware,2026-01-06 03:29:57,33
AMD,nxz8yd7,"You can't price it higher than people are willing to pay, how high that is I have no idea, people bonkers buying CPU only laptops at these high prices if gaming is something they really want to do.",hardware,2026-01-06 09:17:16,1
AMD,nxyjv70,Oems magically dont price intel variants as if they were made out of gold?,hardware,2026-01-06 05:35:35,4
AMD,nxz7zsg,"Reddit isn’t indicative of anything really, most casual laptop buyers don’t even know what AMD is.",hardware,2026-01-06 09:07:48,7
AMD,ny2zhn1,"Lmao check the data, Intel has 79% of the laptop market share currently",hardware,2026-01-06 21:31:48,2
AMD,nxzevm6,"I wouldn't be surprised if this is because the team has chosen to focus efforts on UDNA because that's the architecture next-gen consoles would use. They only have so much talent and headcount on their graphics division after all, and consoles have much higher volume (even tho low margins) and thus take priority.",hardware,2026-01-06 10:13:17,31
AMD,nxy8549,Not an ideal solution but Optiscaler exists,hardware,2026-01-06 04:15:24,15
AMD,nxzf3cy,"NVIDIA has increased margins but they haven't been that terrible. Part of the compounding issue at play is limited TSMC capacity; with both gaming and DC on the same TSMC node.  Ampere (crypto bubble ignoring) was priced well and many excellent cards in there since it was on Samsung, a cheap fab; while DC/workstation chips got TSMC.",hardware,2026-01-06 10:15:15,5
AMD,ny0vvvv,Who said anything about basing the entire product stack around handhelds?,hardware,2026-01-06 15:48:11,4
AMD,ny13z79,"That's not quite right. Power levels are determined by the frequency of a given CPU core. The LPE cores in Lunar and Panther lake both clock up to 3.7 GHz, so given the added IPC of the new Panther lake e-cores and better process node, it is more efficient. Base power levels tell you nothing really.",hardware,2026-01-06 16:25:13,3
AMD,nxz3zvx,"[It's actually closer to Battlemage than Celestial. Straight from Tom Petersen](https://youtu.be/P2AsCkKi-vs?t=1576)  >""Unfortunately that Xe3 name got decided years ago, it's actually spread around the Linux stack. Changing the name of that would have been very, very painful. So, that's why you're seeing this disjointedness abut Intel Arc ""B"" series. **Well, [Panther Lake] is B series because it's similar to Xe2** and we want to be transparent with our customers. Panther Lake has a new and improved GPU, that GPU is bigger and **it's very similar to B series.**""",hardware,2026-01-06 08:29:22,16
AMD,nxyzwrf,"Xe3 isn't Celestial, only Xe3P will be. See [https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake](https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake)",hardware,2026-01-06 07:51:09,9
AMD,ny0y47o,"Yeah, it's a proper generational jump. Intel marketing is just dumb, and the comments from Peterson claiming Xe3 is somehow a smaller jump than Xe3p are just laughable.",hardware,2026-01-06 15:58:20,3
AMD,nxyhb1s,The reason is marketing (the Battlemage brand is hot and filled with good will ATM so resetting to celestial so soon is not ideal regardless of panther Lake being xe³) Peterson addressed this a bit ago.,hardware,2026-01-06 05:16:46,1
AMD,nxysdu1,Xe3p was alr confirmed coming im sure Celestial happens,hardware,2026-01-06 06:43:57,2
AMD,nxzg5fp,"We will see, while Intel's PR and marketing is extremely confusing, Intel did confirm Xe3P will come to desktop; and at least from driver updates (as a very happy B580 owner) driver support has been constant and lively.  I had some issues with an older Civ game, I reported an issue in [their app](https://www.intel.com/content/www/us/en/support/articles/000057021/graphics/other-graphics.html) with screenshots/etc, and while I never got any notification, the game works perfectly now a few months later. Dunno if they read those reports, but my card keeps getting better.  I actually think a MSRP B580 is another card that will age like fined wine -- YMMV depends on games you play, but in Australia they have been regularly sold slightly below international MSRP and represent phenomenal value in the price class.",hardware,2026-01-06 10:24:47,0
AMD,nxzdyw4,That's bloody good for an iGPU. It's been nice to see them finally get to respectable performance over the last few years. Intel in particular has really upped their iGPU game & it shows.,hardware,2026-01-06 10:05:03,19
AMD,nxxpg2g,not to mention it is a little skewed as they threw in a title which pushed the vram limit on the 4050 making the b390 over 800 percent faster in that title which obviously messes with the average.,hardware,2026-01-06 02:27:55,18
AMD,nxxsozj,It's 10% faster geomean across 45 games,hardware,2026-01-06 02:45:47,25
AMD,nxzfaqu,That can be resolved if either Intel or AMD decides to unlock quad-channel on consumer chips and mobos. It's artificial market segmentation; the die area needed to deliver more (LP)DDR5 channels is absolutely minuscule; for a huge boost in iGPU performance.,hardware,2026-01-06 10:17:06,3
AMD,nxy0rol,Cache. Lots of it.,hardware,2026-01-06 03:31:13,23
AMD,nxxpegk,"They are using 9600mt/s lpddr5x, could also have a lot of cache, (iirc 890m configs were nerfed in cache because they wanted to put a npu instead), and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.",hardware,2026-01-06 02:27:40,13
AMD,nxxumwa,Panther Lake still has a 128 bit memory bus so only models with 9600 mt/s will get slightly faster shared memory bandwidth than Lunar Lake.   I wonder how this will manifest in games as the only performance leaks have been from Geekbench and 3DMark which may not be as bandwidth intensive as real games and applications.,hardware,2026-01-06 02:56:26,5
AMD,nxy8e9t,"This seems to be correct, since checking NotebookCheck for the 30 watt 4050 shows that it’s around 70% faster than the HX370 in games, which is roughly where Intel places their iGPU.  The performance difference between a 30 watt 4050 and full 140 watt 4050 is around 41 percent performance based on Time Spy",hardware,2026-01-06 04:16:57,14
AMD,nxy2xyi,"basically cheating tho, rtx cards in dell laptops are barely getting enough watts to even turbo",hardware,2026-01-06 03:43:51,3
AMD,nxzpm42,"That's the only way to do a fair comparison, really.   Because the 45 watts that Intel chip uses is shared for the entire chip.   So it's still 45w Intel + igpu vs 60w Intel+gpu",hardware,2026-01-06 11:45:18,1
AMD,ny2zoh2,The thing about that... what sort of tile are we expecting them to package up? As you say if we can get 4050ish performance from an Intel iGPU then they really can't be far off 5050M... and maybe even 5060M performance in future.  Do you think they'll offer something like a 5070 tile? that almost seems excessive (and difficult to actually package from a thermal point of view in a laptop) but it seems like the 5050/5060 sort of tier is going to be pretty well covered as a traditional iGPU soon.,hardware,2026-01-06 21:32:40,1
AMD,nxzk1m0,AMD actually introduced lower tier Strix Halos in this CES; and the first budget laptop thats gonna use it is [the Asus TUF A14](https://youtu.be/h27w0PXFBgk?si=Pa7UQhinywF-uFMj&t=306),hardware,2026-01-06 10:59:13,5
AMD,ny0na4n,"It’s a super strong generational gain though, it’s like the jump from Vega 8CU to rdna2 12CU. The kind of single gen gain you see once in 5 years at most",hardware,2026-01-06 15:07:11,9
AMD,nxxklk7,They're a lot more accurate than whatever the fuck Nvidia has been doing where you have to decode their bar graphs for proper scaling lmao,hardware,2026-01-06 02:01:38,47
AMD,nxzfsf4,"I'm pretty sure Intel threw lots of ""marketing money"" for the MSI Claw too. There were heaps of MSI Claw promotional booths / draws at shopping malls / public places in Australia and it was heavily discounted.  I picked one up for about $550 AUD (after rebates; tax included), which is like $369 USD inclusive of tax.",hardware,2026-01-06 10:21:31,2
AMD,ny13km9,> Lunar Lake was a expensive product which didn't make sense in handhelds   What do you mean? All the tradeoffs LNL made were pretty good fits for a handheld.,hardware,2026-01-06 16:23:22,1
AMD,ny13z0b,"> I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800   You're looking at normal DDR, not LPDDR. LPDDR5-9600 *is* a JEDEC spec, and already available in mobile.",hardware,2026-01-06 16:25:12,6
AMD,ny020mx,"The majority of the lineup still only has 4. Will be interesting to see what the pricing and performance is on those since these will likely be quite limited. What's also a bit crazy is there's three different nodes being used for the various GPUs, and the full 12 unit one is probably on N3.",hardware,2026-01-06 13:11:40,2
AMD,nxxlxrw,"I doubt it's exactly 73% outside of cherrypicked games, but it should not be shocking that it's significantly faster than rehashed rdna3.",hardware,2026-01-06 02:08:52,-1
AMD,nxxjlfd,"Every Intel marketing benchmark for like a decade or so, but especially their GPUs seem to do far better in their benchmarks than they do in reality.",hardware,2026-01-06 01:56:13,-19
AMD,nxxhxjk,"Ice lake, Alder lake, Metor Lake",hardware,2026-01-06 01:47:15,-19
AMD,ny0aepy,"Different poster than OP.  Compute tile on Lunar Lake is 140mm2 on N3E with a small 46mm2 controller N6 tile. Strix Point (HX 370) as a whole is 233 mm2 on N4P. Lunar Lake is clearly cheaper, but given the newer node and packaging not massively so, likely by around 20-25%.  Panther Lake, with the B390, is going to be significantly more expensive than Lunar Lake. The B390 GPU has 50% more CUs, and that is very likely still on N3 or some variant (Intel only labeled this as external on their deck). CPU size 4xP+12xE as opposed to Lunar's 4+4, which should still be significantly more area with the core upgrades despite being on A18.  The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.  Intel Foundry in general isn't any cheaper than TSMC. With Intel being practically the only user and development expenses it's likely more expensive than TSMC despite TMSC's margins. For all purposes it's an accounting trick to hide CCG's and DCAI's 5-15% operating margins if you divide the foundry losses per group revenue.",hardware,2026-01-06 13:59:25,1
AMD,nxxpvi7,"Yes, although the actual low power 8 core successor to Lunar Lake is the 335/365 with half GPU cores and slower RAM.",hardware,2026-01-06 02:30:19,-8
AMD,ny0b9nu,"even if you exclude the CPU CCDs the graphics die alone is bigger than a RTX 5070Ti mobile GPU, which also retails for \~$2000-$3000, same as Strix Halo laptop",hardware,2026-01-06 14:04:11,10
AMD,ny393aj,"This is slightly splitting hairs but the 8c CCDs in Strix Halo is actually NOT the same chiplet as the ones in desktop zen 5 parts. It iirc is produced on a smaller node, slimmed down, and has different ( or no) TSVs.  It is similar to design and cache sizes to desktop however, but the changes to the CCDs were done to improve low power performance characteristics. They are likely a bit more expensive than Desktop CCDs.  I believe it is discussed in a chips&cheese deep dive.",hardware,2026-01-06 22:16:26,5
AMD,nxycvko,"AMD have largely been ""winning by doing nothing"" due to their better driver support stack for gaming on iGPUs, rather than actually throwing superior hardware at it.  It's almost ironic how AMD's mobile chipsets are now the ""Intel 14nm+++++"" of this generation.  Constant minor refreshes or even straight-up re-badges of old chips.  Now that Intel Arc has been around a while now and is getting quite capable.  I suspect Intel have a real opportunity to overtake AMD this generation in the iGPU space (ie. handheld and mini-PCs), especially since the new AMD APUs are just **another** refresh with a clock boost and Strix Halo is not scaled or priced to be actually affordable by normal people in that market.  XeSS can also act as a massive force-multiplier in power-constrained scenarios like handhelds.  AMD really shot themselves in the foot by either not building or not allowing FSR4 to function on RDNA3/3.5, which all current and now next gen AMD handhelds are stuck on.  Given how effective DLSS is on the Switch2, one could only imagine how kickass a Nvidia chip in a handheld PC could be with the far more ubiquitous DLSS support.",hardware,2026-01-06 04:46:15,74
AMD,nxyz1xa,Now they are not. The panther lake igpus are undisputed winners (excluding the 395+ from amd since it's just not gonna be mainstream). You can get a 358H or 368H and you'll have solid laptop for igpu gaming far cheaper than the 395+,hardware,2026-01-06 07:43:17,14
AMD,nxz8aui,"For business apps laptops have been good enough for 10 years now, iGPU and battery life is really the only differentiator.",hardware,2026-01-06 09:10:50,5
AMD,nxxvlpa,"Intel is plenty competent for gaming, and has XeSS which is way better than FSR3.",hardware,2026-01-06 03:01:47,38
AMD,nxxx4hv,Lol? No 6 or 8 core 3dvcache laptops and no 5080 or 5090 laptops. Strix Halo is a joke for gaming as well,hardware,2026-01-06 03:10:21,-9
AMD,ny1dwcs,The 9955HX + 5070Ti is $2240 and the 275HX + 5080 is $2540.   When both 5070Ti configurations are on sale they should be the same price.,hardware,2026-01-06 17:10:18,2
AMD,nxy23yz,My mistake I was thinking of Strix Halo,hardware,2026-01-06 03:39:00,30
AMD,ny0f479,"It's about compromise. I don't *want* a 4lb laptop. I don't want a laptop that runs hot when web browsing. Or a laptop that has loud fans, or gets poor battery.  I have a desktop for gaming and other demanding tasks. For a laptop, I, and most of the market, want it focused on portability. Light weight. Cool running. Long battery. These big iGPU PTL laptops are really interesting because they provide *good enough* gaming without sacrifice to the non-gaming livability of the device.",hardware,2026-01-06 14:25:02,1
AMD,ny02zjc,"It took me way too long to convince my sister the AMD laptop I bought her isn’t going to blow up in her face and lose all her data, the Intel(and now Apple) CPU brands are very strong.",hardware,2026-01-06 13:17:30,2
AMD,ny0wejw,"> most casual laptop buyers don’t even know what AMD is  We're past that point now. Even ""normies"" have heard of AMD from news.",hardware,2026-01-06 15:50:32,1
AMD,ny006yw,"its always ""fix it next generation"" with AMD.",hardware,2026-01-06 13:00:12,17
AMD,nxyndbc,"I mean yeah it's not ideal, but you could argue it's the same with XeSS or FSR 4 on RDNA 3. Since the OP said ""there's no decent upscaling on AMD handheld"", therefore I assume Opsticaler is out of the question too.",hardware,2026-01-06 06:02:32,8
AMD,nxzib6q,"Well you said it, it's TSMC capacity, meaning also a priority issue. They prioritize AI over consumers and then increase the price by reducing availability, meaning the same chip costs more, meaning more margin.  Seeing they increase the 5090 to roughly 5k (USD) is just the beginning and as I know all companies will use the increasing memory prices to say they must increase the product price, just not proportional to the memory costs.  next step: then they will use this to move more to streaming instead of owning",hardware,2026-01-06 10:44:14,-2
AMD,ny15kho,"> Power levels are determined by the frequency of a given CPU core   There are SoC and platform level targets that depend on a lot more than just clock speed for the same cores. Consider how LNL's PMICs scale vs FIVR/DLVR. Or what operating point benefits the most from the on package memory.   Especially at really low power, the cores are not your big concern. Consider the difference at 10W between 50% of your budget available for compute and 80%.   > so given the added IPC of the new Panther lake e-cores   We're talking a couple percent. DKT is a tick.    > and better process node   Very much unproven.    If you want to give credit somewhere, pretty much all of it should go to the SoC and GPU teams.",hardware,2026-01-06 16:32:30,0
AMD,ny0idmq,"Xe2, Xe3, etc. are the ""real"", more accurate names. Battlemage, Celestial are the marketing names.  Intel's decision to label the new Xe3 iGPUs as ""Battlemage"" is certainly an interesting (odd) choice - my best guess for this decision is that next year, Xe3P discrete will launch alongside Xe3P iGPU in NVL, and they're saving the new Celestial naming for that launch event.  Xe2 -> Xe3 is the bigger change.",hardware,2026-01-06 14:42:24,5
AMD,ny1hu62,"Peterson states explicitly it's to take advantage of good Battlemage branding, around 1:30 of this video. [Intel Talks Xe3 Improvements For Gaming - YouTube](https://www.youtube.com/watch?v=Bjdd_ywfEkI)",hardware,2026-01-06 17:28:12,1
AMD,ny0xhzi,"> Xe3p was alr confirmed coming  Not for client dGPUs, which are what get the Battlemage/Celestial brand.",hardware,2026-01-06 15:55:31,2
AMD,ny0xj91,> Intel did confirm Xe3P will come to desktop  They have not.,hardware,2026-01-06 15:55:41,1
AMD,nxy8fjz,"They also might be getting better value out of the ""2x scaling"" choice for benchmarking. Notice how they are behind Nvidia in all the none scaled titles except Dota2 that I saw.  Still very good results for a iGPU, but they are not entirely honest numbers either.",hardware,2026-01-06 04:17:11,12
AMD,nxxshuz,It's 1 game out of 45 in geomean which devalues outliers. ~~9.9% faster instead of 10% faster if you take it out.~~  Edit: Oh no it's actually 6 FPS on the 4050. Yeah that's way too big for geomean to smooth out.,hardware,2026-01-06 02:44:44,21
AMD,nxy7k6w,And the fact they showed 45 games shows how confident they are in this product.  I remember the Intel slides with 5 hand picked titles we used to get just a few years ago.,hardware,2026-01-06 04:11:44,26
AMD,ny17afz,You mean in desktop? Or do you want mainstream mobile to go quad channel?,hardware,2026-01-06 16:40:20,2
AMD,nxzgpgy,I wonder how 96MB cache would do had Intel put that much on it.,hardware,2026-01-06 10:29:53,2
AMD,nxyig6s,They have 16 MB of L2 just for the GPU alone lmfao,hardware,2026-01-06 05:25:03,9
AMD,nxyiiu1,"Also there's like 45 games on display here, it's not just 3dmark",hardware,2026-01-06 05:25:35,5
AMD,ny08xk6,> and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.  They're benching 45 games dude.,hardware,2026-01-06 13:51:15,5
AMD,nxy8oam,41 percent difference in performance compared to a full 140 watt in Time Spy. Honestly a bit surprised it isn’t more performance difference.,hardware,2026-01-06 04:18:43,3
AMD,ny55i5l,"No, it's disingenous. Because everyone would think 60w 4050 = 60w on gpu alone",hardware,2026-01-07 04:22:58,1
AMD,ny384o8,"Not really sure. I believe it's Hammer Lake that's debuting the Nvidia tile, and that's rumored for a 2029 launch, so still quite a ways off, and 2 generations ahead of Blackwell.  The only rumors I'm aware of that it's going to be a pretty big iGPU",hardware,2026-01-06 22:11:51,4
AMD,nxzkp7q,Fantastic -- but at least six months too late ;),hardware,2026-01-06 11:04:52,5
AMD,nxz5md2,You don't like graphs with zero scale claiming their latest 100W GPU is somehow a gazillion percent better than a 4090 or something?,hardware,2026-01-06 08:45:06,10
AMD,nxy8v6h,Wattage limited 4050 to 30 watts is the only slide that’s suspect.  It’s around a 41 percent performance loss based on Time Spy from the 140 watt 4050.,hardware,2026-01-06 04:19:56,9
AMD,ny0l0pv,The standard 4Xe models use the extra die space they save to have more PCIe lanes. that large iGPU adds cost and doesn't make much sense to use that chip if you're gonna add an Nvidia dGPU,hardware,2026-01-06 14:55:55,3
AMD,nxxp4au,"yeah just looking at the game sample I can see a few that really don't perform well on RDNA architecture at least relative to nvidia(idk what really constitutes an ""intel favoured"" title)   Like stalker, csgo 2, civ vii, dying light the beast, and delta force ik run a lot better on nvidia relative to amd so im guessing the same holds true for intel vs amd.   A couple titles amd does well in were thrown in there too though like God of war and Cod but im guessing the real performance difference is more like 40-60 rather than the claimed 70-80.   Pretty large sample though which is nice so the numbers can't be that off.",hardware,2026-01-06 02:26:09,10
AMD,nxxtyhj,Why not?   It's 50% more cores + architectural improvements + clock  speeds,hardware,2026-01-06 02:52:42,9
AMD,nxzgxn6,"Please provide a **single** example in the past ~5 years of an Intel marketing benchmark that is materially inaccurate or untruthful.  NVIDIA is the one playing it loose with BS charts, AMD generally has a good track record (with some exceptions), and Intel on the GPU side has been pretty accurate. For example, these benchmarks have 45 games (!!) and use geomean to reduce outliers.  While I disagree with their choice of LPDDR5 9600MHz (hah, imagine a single consumer product shipping with that in this DRAM market), it is not untruthful.",hardware,2026-01-06 10:31:54,8
AMD,nxxkps6,All were pretty accurate.,hardware,2026-01-06 02:02:15,18
AMD,nxxkqio,But lunar lake igpu actually perform better than 890M.Like comparison of core ultra 7 and z2 extreme in handheld like msi claw.,hardware,2026-01-06 02:02:23,15
AMD,ny0lzbq,">The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.     The mainstream unit that's more directly comparable to LNL is the same core count (4+0+4) with a smaller iGPU tile. It'll be cheaper.  The 4+8+4 w/ 4Xe is the direct replacement to ARL-H, and that should also be cheaper than ARL-H.",hardware,2026-01-06 15:00:42,1
AMD,nxxvizf,"True, but then still, that's not a removal of CPU cores like they said it was.",hardware,2026-01-06 03:01:22,10
AMD,ny3ft4h,That is due to the Nvidia tax and AI bubble rather than the production cost of the chip. Even Apple ships cheaper silicon than that.,hardware,2026-01-06 22:48:46,2
AMD,nxyibby,"This is very topical and cyclical of Intel/AMD. Intel did really poorly for like a half a decade which was unusual but usually they go back and forth. One gets lazy and incompetent, the other curated a masterful product that becomes dominant for a while and then they get lazy and it flips around.  Intel is planning on socketing a ton of cache on their next breed of chips which will massively boost their gaming perfomance and they have pretty darn efficient chips now too.",hardware,2026-01-06 05:24:04,13
AMD,nxygca9,Thank you for the thorough explanation! Very excited for the future of miniPCs and handhelds since there's so many games I'd like to play on the go.,hardware,2026-01-06 05:09:52,2
AMD,nxz2wtv,"Yup, I am very happy to learn how wrong I was thanks to other people in this thread as well.",hardware,2026-01-06 08:19:04,8
AMD,nxzn2om,"For business apps 10 years ago yes, now even Office has bloated itself up so much it's genuinely taxing even on the Apple chips  And well, the better the chip, the more outrageous the user workload gets. I appreciate the modern laptop chip's ability to import a CSV the size of Excel's row count limit and make a pivot table out of that, but now that it *can* do that I'm *expecting* that to be possible as quickly and as efficiently as possible.",hardware,2026-01-06 11:25:00,6
AMD,nxxy7wk,I was under the impression that XeSS needed a dedicated GPU? If it can run on iGPU that's a whole different story.,hardware,2026-01-06 03:16:38,-13
AMD,nxz8pk3,"Their GPU's only look good when compared to 1 generation old bottom tier GPU's of their competitors. Its wild the praise they get.  Same thing will happen here, AMD will release a new iGPU architecture and Intel will be left comparing to out of date CPU's no one buys anymore.",hardware,2026-01-06 09:14:52,-7
AMD,nxxynsf,Sorry I should have specified that I'm talking about budget laptops with iGPUs.   I would sooner build a pc than even think about a 5080 laptop with 3dvcache options.,hardware,2026-01-06 03:19:11,15
AMD,ny01arz,"Yeah; meanwhile NVIDIA just released DLSS4.5 for **every single RTX GPU**... yes all the way back to Turing. It runs a lot better on more recent cards, but it's available on every single RTX GPU if you want to.",hardware,2026-01-06 13:07:13,13
AMD,ny0x2bs,"XeSS and FSR 4 on RDNA 3 both use downgraded versions of those upscalers, that either look worse, perform worse, or both. In the case of FSR 4, it's a leaked one-off model that people got their hands on. All I really meant by ""decent"" was having an officially supported modern upscaler without all the downsides.  An Intel GPU running XeSS would presumably get the full version of XeSS without the performance hit and with good visuals.",hardware,2026-01-06 15:53:33,2
AMD,nxzipnz,I can currently buy a brand new 5090 in Australia for $2841 USD with express postage included; I'm not sure why it's 5k USD in your region; but there's no reason you should be paying 5k USD. Which country are you in?,hardware,2026-01-06 10:47:45,4
AMD,ny4uh4q,"The ultra X9 388H has a base TDP of 25W and minimal assured power draw of 15W. Meanwhile the ultra 7 155U has base TDP of 15W and minimal assured power draw of 12W. Both these numbers are lower for the meteor lake chip, yet the Panther lake chip is waaay more efficient (+2x). The base power level doesn't mean anything. It might be the point where the chip had the most perf/watt, but that doesn't mean that the performance at lower wattages is the same.",hardware,2026-01-07 03:17:13,0
AMD,ny0xx01,"Battlemage, Celestial, etc are named they (usually) use only for the dGPUs, even if that does correlate with the B/C-series naming. I think at some point this is just reading the tea leaves. The name's misleading for the tech difference.",hardware,2026-01-06 15:57:25,3
AMD,nxyibw6,XeSS FG has lower overhead than Nvidia IIRC,hardware,2026-01-06 05:24:11,2
AMD,nxyaxye,If you actually do the maths it'd go down to (1.1^(45)/9)^(1/44) = 1.049 = 4.9% faster,hardware,2026-01-06 04:33:24,9
AMD,nxysgiw,Mobile rtx 4050-70 maxes out at roughly 90-100 watts due to voltage limitation,hardware,2026-01-06 06:44:37,5
AMD,ny3bk9e,"Oh wow that's a lot later than I expected, I was thinking this year or next.  Yeah no clue in that case.",hardware,2026-01-06 22:28:12,3
AMD,nxzfk6p,Infinity percent better at a feature the older GPU used for comparison does not support!,hardware,2026-01-06 10:19:28,1
AMD,ny0kqkf,"I don't really think that's ""suspect"". They said they're limiting the total laptop power on the 4050 to match the total laptop power of the PTL chip. If you want stronger performance out of a 4050, you're gonna need to have much higher power draw than the PTL laptop",hardware,2026-01-06 14:54:29,2
AMD,nxyj1tk,"CSGO is known for running like utter shit on Intel Arc, you can check r/IntelArc for details LOL. The game selection looks pretty reasonable to me.",hardware,2026-01-06 05:29:27,9
AMD,nxy8v14,"Yeah all depends on pricing, 6 core ultra 5 model is however technically downgrade from last generation and the same core config as the i3 1315U.",hardware,2026-01-06 04:19:54,-2
AMD,nxyk9c8,Honestly not that unusual. It takes an average of around 4-5 years to develop a processing unit from the ground up. If we assume each one does this when they get mushroom stamped by the other for being lazy it accounts for the 5 years gaps till they show back up with something to sell.,hardware,2026-01-06 05:38:33,22
AMD,ny0dmdk,"I think AMD is getting a bit lazy when it comes to consumer graphics. I think their attempts at laptop have been really half-assed given just how good their IP portfolio is.  But when it comes to their core businesses, they're definitely been keeping the heat on and have been quite aggressive. They're datacenter first and foremost, and that trickles down to amazing desktop CPUs too. They're heavily focused on building out their Mi series too...but they're just dropping the ball in laptop and consumer GPU",hardware,2026-01-06 14:17:03,5
AMD,ny39k1v,Pantherlake also has an oddity in that it has MUCH higher L2 cache than even desktop zen 5 parts. I'm curious to see its CPU performance in low resolution scenarios.,hardware,2026-01-06 22:18:39,1
AMD,nxxyz2v,"The good version of XeSS runs on any chip with XMX units (Intel's version of tensor cores). Lunar Lake, Arrow Lake mobile, and now Panther Lake have GPU tiles with XMX units, so they get the same XeSS as discrete Arc cards.",hardware,2026-01-06 03:20:59,26
AMD,nxyty97,"Dedicated hardware, not dedicated GPU. The new Intel CPUs have iGPUs with the necessary hardware.",hardware,2026-01-06 06:57:17,5
AMD,nxy622y,"It needs dedicated GPU hardware to run faster, but they’ve started incorporating it on Lunar Lake and Panther Lake",hardware,2026-01-06 04:02:21,5
AMD,ny0e8rw,Intel has been very aggressive in the iGPU space. AMD isn't going to have any real updates to their iGPUs until 2027 the earliest.,hardware,2026-01-06 14:20:20,6
AMD,ny2bpjc,"> Same thing will happen here, AMD will release a new iGPU architecture   ... Based on what history? AMD's iGPU has not significantly changed in years. It's still hugely memory bottlenecked and no matter how many times they add an extra 2 CU's, it will still be memory bottlenecked.  IIRC someone disabled 2 CU's on their 7000 series APU and their in-game FPS almost didn't change because the bottleneck was actually memory access.  Intel ARC is actually very good on this metric. Intel doesn't exactly need to sling anything better than ""slightly more Battlemage on a better transistor"" to completely swamp out AMD iGPU in this space.",hardware,2026-01-06 19:42:26,1
AMD,ny2y1c0,"Intel Panther lake base tdp is 25w, around the same as AMD Strix Point/ Gorgon Point. Why will they compare it to a 55w tdp Strix Halo?",hardware,2026-01-06 21:25:10,1
AMD,nxy5y1b,"Even on the budget laptops category the new Ryzen 7s suck compared to the Intel Lunar Lake options, they seem to be priced closer with Lunar Lake getting stuff like nice displays. In the really budget category I feel like they are tied on value and I don't know how sales affect that. This is partially cause AMD went cheap on the mid-range kraken point chips and also had to fit in the still dead weight 40 tops NPU for Microsoft. So it only has 8 GPU cores.",hardware,2026-01-06 04:01:39,11
AMD,nxzj5en,"First custom design OEM are fast, here 4400€ on Amazon https://amzn.eu/d/idxVW9M  And you know how this goes, one starts the other follow.  Here in the US for a normal founders edition for 4.2k USD + TAX… one article from the first of January quoted ot that time being at 3.7, like 5 days ago.  https://www.newegg.com/nvidia-founder-edition-900-1g144-2530-000-geforce-rtx-5090-32gb-graphics-card-double-fans/p/1FT-0004-008V4?source=f",hardware,2026-01-06 10:51:32,-1
AMD,ny0ypvn,">The name's misleading for the tech difference.  Yeah, that's my point. People are reading too much into the ""B series"" naming scheme for B390.  As you said, ""(usually) use only for the dGPUs"". So if Xe3P is launching as a discrete Celestial Card, then it would make sense to have Xe3P tile be part of the ""Celestial"" launch, rather than Celestial Discrete being ""one year later than Celestial integrated""",hardware,2026-01-06 16:01:03,3
AMD,nxyf825,Oh dang you're right lmao.  The 4050 has SIX (6) FPS at 540p high. I thought OP was exaggerating with 800%.,hardware,2026-01-06 05:02:05,4
AMD,nxyahr2,Yeah those kinda suck. Should be Ultra 3s given they're basically WCL spec.,hardware,2026-01-06 04:30:27,2
AMD,ny2mxg2,"More like 10 years, 5 to realized that they are getting stomped in the face, and another 5 to actually make something of it.",hardware,2026-01-06 20:34:21,1
AMD,nxyak62,"Man, there are so many older and less demanding titles I'd love to play through on the go, but knowing that Lunar Lake laptops have better displays for the price is really good. Thanks for the info!",hardware,2026-01-06 04:30:54,5
AMD,nxzkmr7,"That's a marketplace listing, it's basically eBay, because Newegg is out of 5090FEs directly.  You can get it on the overpriced StockX for far cheaper: https://stockx.com/nvidia-geforce-rtx-5090-32gb-graphics-card-900-1g144-2530-000",hardware,2026-01-06 11:04:15,6
AMD,ny12fxe,"That would make some sense if they *did* plan a Celestial launch, but that's a big ""if"" and is just creating confusion for now. And it'll be even worse when NVL mixes Xe3 and Xe3p.    You also have Intel marketing actively making the situation worse like that Peterson interview people keep quoting to justify this nonsense. As if Xe3p isn't much more incremental than Xe3.    It's a particular shame when the product itself is actually good.",hardware,2026-01-06 16:18:11,2
AMD,nxyb20u,"Yeah, the first 6 core i/u5 series since 11th gen. :/",hardware,2026-01-06 04:34:08,-1
AMD,nxyoap3,"Yeah idk why but they typically got OLEDs exclusively, though could be a US market thing. I would also note I was mostly looking at decently built midrange to high-end laptops. I think AMD is more common in the plastic crap box design and may be a better value there, but those also typically seem to have a ton of older rebadged processors instead of the newer Kraken Point unless something changed.",hardware,2026-01-06 06:10:02,2
AMD,nxyh7a2,Didn’t everybody expect this to be a 2 year holding pattern until Zen 6 regardless? Isn’t this what usually happens?,hardware,2026-01-06 05:16:02,88
AMD,nxys8b4,I had the fastest gaming CPU in the world for 2 weeks,hardware,2026-01-06 06:42:37,27
AMD,nxyg77q,"Could be worse, you could be a threadripper customer.",hardware,2026-01-06 05:08:52,39
AMD,nxyibmy,"I don’t quite understand HUB’s hubbub here.  AMD stumbled upon a handful of higher-binned chips and decided to spin them off into a slightly higher-tier SKU.  That’s not exactly… unorthodox.  The original Ryzen 7 1700 not only had the 1700X, but also an even more “premium” 1800X variant, the latter of which (seemingly) had a very short production run (I’ve yet to see one in the wild).  But sure, let’s write a whole song and dance about it.   That’s what modern tech journalism is about, I imagine.",hardware,2026-01-06 05:24:08,56
AMD,nxz9oh5,"back in the days intel and amd released the same cpus but the higher ends usually had simply higher frequency so for avg joes it was easy peasy, but for enthusiasts it meant they could buy the lowest sku of that u-arch and just oc it.  So it is basically a return to the old, just oc your old 9800x3d and u are there.  mine is ticking at 5.6 all cores locked all the time in all gaming worklaods.",hardware,2026-01-06 09:24:16,4
AMD,nxyo3y8,I don't see how it's milking unless it's more expensive **and** they stop selling the original. As long as they keep selling it alongside then who cares,hardware,2026-01-06 06:08:30,16
AMD,nxykroo,HUB continues to milk AMD.,hardware,2026-01-06 05:42:26,17
AMD,nxyn5gh,If they would have simultaneously launched a 9750x3D for the lower binned parts I wouldn't be this disappointed.,hardware,2026-01-06 06:00:49,2
AMD,ny0g0qc,"I don't understand how this is milking - at least compared to the amount of milking that goes on across the industry, where paying a premium for largely irrelevant performance gains is the standard, not the exception.  What I really wonder is what amount of overhead these chips will have - I'm guessing they won't be able to keep up with the PBO / CO capabilities of the average binned 9800X3D, making them even less appealing to enthusiasts.  Looking forward to seeing what they are capable of.",hardware,2026-01-06 14:29:53,2
AMD,ny187xs,What does this equate to… +3 fps @4k max settings?,hardware,2026-01-06 16:44:33,1
AMD,nxyit9q,Plenty of people will pay for +7%.,hardware,2026-01-06 05:27:42,1
AMD,ny13cu1,"I just bought a 9800x3D it's still in the box, should I return it? Is this even going to be a noticeable difference?",hardware,2026-01-06 16:22:22,1
AMD,nxyhlu7,"JSH said in his presentation that Moore's law has stopped delivering the goods on speed so Nvidia has to redesign every part of their whole system stack to get performance increases.  That maps onto AMD's dilemma with these CPUs perfectly.  They have run out of ways to make a single generalist part give big performance increases at the same price point.  Nvidia's trick was to get you to buy a $400 GPU on year 1, a $800 GPU on year 3, a $1600 GPU on year 5, etc",hardware,2026-01-06 05:18:55,-5
AMD,ny1vaqd,It's a little refresh within the same gen and it's not like it's targeting 9800x3d users anyways so... Milking??,hardware,2026-01-06 18:28:14,0
AMD,nxzg285,Interested to see if it has the random combustion issues as the last one.,hardware,2026-01-06 10:23:59,-1
AMD,nxzqecd,"I thought the 9850X3D is rumored to be 5-10% faster than 9800X3D in games though? That seems more than ""slightly"" faster, especially for what will be the fastest X86 CPU on earth",hardware,2026-01-06 11:51:20,-1
AMD,nxykpaj,"Yea they did, this “milking gamers” thing is so odd. It’s like saying Nvidia is milking when releasing the super series.   It’s replacing the 9800X3D and making it better.",hardware,2026-01-06 05:41:56,56
AMD,nxykm5q,The internet is full of impatient people who don't understand how things work,hardware,2026-01-06 05:41:16,8
AMD,ny1v5cs,Also like it's normal to have multiple of the technically same configuration but at different clock speed for CPU. Intel and amd have done this for decades,hardware,2026-01-06 18:27:34,1
AMD,nxzyc9d,"There's a weird round of negative sentiment about AMD right now.   On the GPU end, people are pointing to the new DLSS 4.5 and saying, ""Look at this! Nvidia is supporting all of their GPUs with this while FSR4 only works on RDNA4!""   Mind you, RTX3000 GPUs that were new in 2022 have never seen any sort of frame-gen support from Nvidia. You had to rely on AMD's FSR3 or Lossless Scaling if you wanted that.   And multi-framegen (including the DLSS 4.5 flavor) are for the newest Nvidia cards only.",hardware,2026-01-06 12:48:19,-9
AMD,nxzlzvu,Never again.,hardware,2026-01-06 11:15:58,8
AMD,nxysnln,"Honestly, I haven’t been able to watch alot of the hardware YouTubers rage bait the last few weeks as all their videos are ‘is gaming dead’ ‘is PC hardware over’ ‘RAM destroying PC’ etc. I feel like it’s the same content over and over from a hand-full of YouTubers saying the same thing.",hardware,2026-01-06 06:46:17,22
AMD,nxyt7aq,"Was the 1800X rare? I had one from around 2017-2023 in my main pc, replaced it with a 3950X. I did ball on basically an all in day 1 AM4 build because I was upgrading from like a 2nd or 3rd gen i5.",hardware,2026-01-06 06:50:55,7
AMD,nxyrhi4,"> The original Ryzen 7 1700 not only had the 1700X, but also an even more “premium” 1800X variant, the latter of which (seemingly) had a very short production run (I’ve yet to see one in the wild).  I don't recall them being launched after a year. This seems more like milking with the 9800X3D likely getting discontinued and AMD retaining the same price bracket.",hardware,2026-01-06 06:36:18,6
AMD,ny1vcx7,2600k 2700k 4700k 4790k. 10900k 10850k. Etc,hardware,2026-01-06 18:28:30,2
AMD,nxylozh,"> I don’t quite understand HUB’s hubbub here  You see, they're afraid of people calling them the amd channel so they sometimes do the ""we have to shit on amd for consumers"" kinda thing. It's just a show to present their ""fairness"".",hardware,2026-01-06 05:49:26,16
AMD,nxyn4tl,"To put it as mildly as possible, HUB isn't that great. The 321URX burn-in test is helpful, though.",hardware,2026-01-06 06:00:40,6
AMD,nxytq8z,">AMD stumbled upon a handful of higher-binned chips and decided to spin them off into a slightly higher-tier SKU.  It's not even a new bin, it's just the VCache die bin that goes into the 9950X3D. You could technically already make a ""9850X3D"" by just disabling the other CCD.",hardware,2026-01-06 06:55:24,3
AMD,ny30toh,If this video was about Intel KS chips no one would have any problems with it.,hardware,2026-01-06 21:37:59,1
AMD,nxzvabn,"It’s a fan channel for fan bases, like many others",hardware,2026-01-06 12:27:32,0
AMD,nxytydo,"No money in just a normal preview, HUB since the VRAM outrage discovered that rage videos get more views than normal videos.",hardware,2026-01-06 06:57:19,-6
AMD,nxyqx8g,"They are gonna be more expensive. As to whether or not they’ll stop production on the 9800X3D remains to be seen, but I’m thinking they’ll slow down the production some.",hardware,2026-01-06 06:31:36,10
AMD,nxyuxh9,"> I don't see how it's milking unless it's more expensive and they stop selling the original.  Yes, it's more expensive.  Why wouldn't they stop producing the original? Or at least slow down the deliveries? Seems like a no brainer, you can sell the same CPU but slightly faster for higher MSRP again if you make the older cheaper model scarce.",hardware,2026-01-06 07:05:46,1
AMD,nxynrkv,9849X2D,hardware,2026-01-06 06:05:43,2
AMD,ny1h9to,At 4k max you probably will gain nothing,hardware,2026-01-06 17:25:38,1
AMD,nxyoblz,"It ain't 7%, more like 3-5.",hardware,2026-01-06 06:10:14,19
AMD,ny1h77z,Wouldn’t even be worth the effort to return the one you have now.,hardware,2026-01-06 17:25:19,3
AMD,ny1yo8a,"I decided not to do this two weeks ago. Like yeah it will be better, but also probably hotter... and they will release AMD Zen 6 in Q4 2026 or Q1 2027... and to deal with return system (i ordered it from another country with a small discount). Nah.",hardware,2026-01-06 18:43:31,2
AMD,ny24urt,When it's 2-4% faster in certain cherrypicked scenarios there is probably a lot of good 9800x3ds that are better than bad 9850x3ds.  These are margin of error type percentages,hardware,2026-01-06 19:11:05,2
AMD,ny2uts2,Let the milking begin!,hardware,2026-01-06 21:10:36,2
AMD,nxykire,"What? They didn’t run out of ways.  No it’s because CPUs for AMD have ~2year release cadences and that AMD needed something to “keep the crown” from Intels 2026 offerings until they can get Zen6 parts available.   Same with laptop, refresh the lineup with better clocks, improved firmware etc until the next gen CPU Engine is ready.   For Nvidia, it’s because they do yearly GPU releases, but their other components typically are kept and used for multiple years.  This just happens to be the year that “everything” in their system is going new.  Next year Nvidia will keep everything but the GPUs.   AMD too is making all new chips for their Helios platform using MI450s, Venice Zen6, and Vulcano AI NICs along with new Scale-Up and Scale-Out switches from a switch vendor.",hardware,2026-01-06 05:40:32,12
AMD,nxyko09,"This has nothing to do with AMD (or perhaps TSMC) hitting a silicon barrier.  It’s simply silicon binning, where higher quality dies are allocated to higher-tier SKUs, a concept HUB seemingly isn’t familiar with.  Besides, N2 is a major leap over the ongoing N4, and while it’s true that Moore’s Law is no longer in effect, N2’s transistor density advantage over N4 is seemingly sufficient for AMD to not only improve IPC on the upcoming Zen 6 (which almost always costs transistors) but also increase the core count by four per CCD.",hardware,2026-01-06 05:41:39,12
AMD,nxykjow,Moores law was never about performance...,hardware,2026-01-06 05:40:44,-3
AMD,ny0r7ip,"The absolute highest outlier is 6% Even in games they cherrypicked to show benefit most are less than 3%. In some cases like battlefield 6 literally 0%  This is AMD’s testing which I wouldn’t call rigged, but it’s going to be the absolute best results they can put forward.  I don’t really care other than AMD constantly makes the naming scheme awful for uninformed consumers.",hardware,2026-01-06 15:26:14,2
AMD,nxyol8e,"The most positive reaction to a Super refresh was that ""this is closer to what they should have released in the first place"".  There has been calls of milking customers with Super releases before too.",hardware,2026-01-06 06:12:22,55
AMD,nxzk4di,"Super series *usually* gives us more than just a minor clock boost, at least.  And tend to provide a general value boost by slotting in at previous price points, rather than stacking on top.   Also, people never had any problem criticizing Intel for milking people with similar sorts of 'refresh' releases.  Dont see why AMD should be off-limits.    The pricing will be the main thing.  If this thing comes with a somewhat tall premium and costs $550+, then yes, it's pretty much the definition of milking consumers.",hardware,2026-01-06 10:59:52,24
AMD,nxys3t7,"Well super series have historically given meaningful improvements in value.   Like the 4070S was around 20% faster than a 4070 for the same MSRP. The 4070 TiS was 10-15% faster than a 4070 Ti at the same MSRP, and the 4080S was around 3% faster for $200 lower MSRP.   That's different than releasing an overclocked CPU with insignificant performance improvements at a higher MSRP.",hardware,2026-01-06 06:41:32,24
AMD,nxyqoec,They’re also making it more expensive.,hardware,2026-01-06 06:29:31,14
AMD,nxyvqpg,"Audience loves the, ""greedy corpos are cheating you,"" angle though.",hardware,2026-01-06 07:12:59,28
AMD,nxzc81l,"Its replacing 9800X3D and it's resetting the price for minor clock improvements (maybe 2-3% faster), which means 9800X3D will be discontinued, soar in price - so either 499 for 9850x3D or around the same (maybe more for 9800X3D new/second hand.  It's milking at its finest.",hardware,2026-01-06 09:48:41,13
AMD,ny0z5ol,they slightly typoed the headline. Youtubers Milking gamers slightly faster.   Its like you said nothing its fluff but they need that ad revenue,hardware,2026-01-06 16:03:05,3
AMD,ny05n9u,>  It’s like saying Nvidia is milking when releasing the super series.   thats what this sub said when the 4000 series supers released...,hardware,2026-01-06 13:32:54,2
AMD,ny1j28j,The sad thing is now is that every 9800X3D will be obsolete /S,hardware,2026-01-06 17:33:51,2
AMD,nxz35kb,Replacing it outright at a higher price point would qualify.,hardware,2026-01-06 08:21:22,2
AMD,ny1da7h,"Is it replacing the 9800X3D? will it be the same price? According to Tom's hardware it's not:   [https://www.tomshardware.com/pc-components/cpus/amds-ryzen-7-9850x3d-promises-7-percent-uplift-over-ryzen-7-9800x3d-amd-fights-itself-with-new-fastest-gaming-processor](https://www.tomshardware.com/pc-components/cpus/amds-ryzen-7-9850x3d-promises-7-percent-uplift-over-ryzen-7-9800x3d-amd-fights-itself-with-new-fastest-gaming-processor)  I'm Not saying anything on the merit of the product, as it's not out yet for reviews and I don't know the price, but if it will be considerably more expensive (like the KS models from intel) then I can see why it will be ""milking gamers""  for the extra few % of performance.",hardware,2026-01-06 17:07:29,1
AMD,ny4panv,If the super series was 2% faster then yes?  What would you call milking a consumer?  When intel spent a decade releasing 4 core cpus with 5% increases was that not milking their consumers?,hardware,2026-01-07 02:48:40,1
AMD,ny00ic5,The upscaler part is what everyone is interested in.,hardware,2026-01-06 13:02:13,6
AMD,ny1buwn,"> There's a weird round of negative sentiment about AMD right now.   There are pretty global round of negative sentiment going on right now. It's a general economy thing. Consumer are feeling prices rising and ready to lash out at everyone. Just about every upvoted youtube video in this subreddit can be summed up by ""[company] is screwing you so hard"".   ""Milking"" gamers by releasing a new product is ridiculous. Don't want it? Don't buy it. It's not like they are taking away your old CPU if you don't agree to pay the new price.",hardware,2026-01-06 17:00:57,2
AMD,ny1omor,"Naturally you were downvoted for this. How depressing.  For what it's worth, this has been going on for many years now. It's just *constant* excuses.",hardware,2026-01-06 17:58:45,-2
AMD,nxz4thi,"Plenty of reasons to not have the highest hopes for consumer PCs at the moment, but reddit and youtube somehow still manage to take it to a comical degree. Avid techtube watchers must be living in the end times right now",hardware,2026-01-06 08:37:20,13
AMD,ny316pk,"People are eating it up. It’s rage bait. Just look at the comments on GeForce Now threads, which actually launched 10 years ago. People are mad about a decade old service they don’t even use.",hardware,2026-01-06 21:39:39,3
AMD,ny23pf9,Do you disagree? DIY PC gaming as a hobby has been a mess for like 8 of the last 10 years.  Do you want them to say this is good?  Or to benchmark GPUs that came out a year ago? Or do custom build videos and just ignore that the RAM kit cost as much as the GPU?,hardware,2026-01-06 19:05:47,2
AMD,nxz51sm,"Yeah it was pretty rare, you were just way better off getting a 1700X or later the 1700 for way less money and like 95-98% of the same performance. The only area where the 1800X shined was clock speed and even then it basically capped at like 4.0-4.1 GHz on golden samples. You were just better off buying a 1700X and OC'ing it to 1800X clock speed and pocketing the difference or spending it on more RAM or something else.  Notice how AMD basically doesn't allow for a slower speed variant with the same cores anymore on the high end like they used to? If you want a 9950X or 9950X3D you have to buy it. Only lower end SKUs like the 9600X will ever see a 9600 or 9500X with similar specs other than clock speeds. AMD might do a refresh of say the 5950X and make a '5950XT' but that's basically just a new stepping and it releases pretty much late into the lifecycle where it makes no sense to wait for it. You may as well wait another 3-6 months for the next generation stuff as your money goes further and it's a more substantial performance upgrade or has new platform features like PCI-E upgrades, DDR5/DDR6 etc.",hardware,2026-01-06 08:39:32,6
AMD,ny0bei2,No it wasn't that rare it was just more expensive,hardware,2026-01-06 14:04:55,3
AMD,ny4e9bm,"The 10850k was actually the reverse of this, it came after the 10900k. The 10900k was so aggressively binned the yields were terrible",hardware,2026-01-07 01:48:57,1
AMD,nxyo2l4,"They are at least consistent on this topic, having also reacted the same to AMD XT refresh CPUs that this is based on",hardware,2026-01-06 06:08:12,28
AMD,nxz96n8,I really hope more people start waking up to HUB’s bias. Techyescity called them out for questionable benchmarking not long ago and it’s always HUB with these crazy fine wine videos that nobody else can replicate.,hardware,2026-01-06 09:19:28,-10
AMD,ny1m0pm,Where did you see this? I'd love to learn more about it.,hardware,2026-01-06 17:47:21,1
AMD,nxz16wn,"Arrow Lake refresh? I think Intel said it's not a competitor to the x3d, meaning the 9850x3d is a CPU you don't need.",hardware,2026-01-06 08:02:59,1
AMD,nxylika,">It’s simply silicon binning, where higher quality dies are allocated to higher-tier SKUs, a concept HUB seemingly isn’t familiar with.   What are you talking about?  They called this AMD's version of the KS weeks ago.  Looks like they were right.",hardware,2026-01-06 05:48:05,7
AMD,nxz9fs0,HUB isn’t familiar as they are deathly afraid of overclocking.,hardware,2026-01-06 09:21:57,1
AMD,nxym0si,Performance and density go hand in hand ...,hardware,2026-01-06 05:51:56,11
AMD,nxzmauf,Exactly why should AMD get a free pass just because their AMD if any we should be harder on AMD than on Intel. They are absolutely milking us for higher performance we should much better pricing. Those should go in tandem not one or the other. Its not like AMD has very little time between generations if Apple can release a good chip every year. I dont see why AMD cannot give comparable performance uplift at decent prices.,hardware,2026-01-06 11:18:32,4
AMD,nxz4gd7,You forgot the VRAM increase on the 4070 Ti Super as well. That's a huge boon for a performance tier like that in terms of longevity and also just overall performance.,hardware,2026-01-06 08:33:48,11
AMD,nxz1mex,Some people also love gaslighting the audience.,hardware,2026-01-06 08:07:01,11
AMD,ny1v7fe,"Glad to see more people calling it out. It seems like doomposting and ""let's see how we can spin this into something negative that make people feel like victims"" is the norm these days and it sucks.",hardware,2026-01-06 18:27:50,1
AMD,nxzmehl,Its milking there is no question about it. If Apple can release CPUs every year with meaningful improvements I dont see why AMD can't.,hardware,2026-01-06 11:19:22,4
AMD,ny36j66,Yeah it’s just content strategy for the most part. People are loving it. Nothing like sticking it to the man….!,hardware,2026-01-06 22:04:14,1
AMD,ny367tk,I agree but no need for 80% of content to be the same doom and gloom thing over and over. It’s rage bait and clearly working for them.,hardware,2026-01-06 22:02:47,1
AMD,nxzb2cf,"I still watch hub, but I have accepted that they test hw like it was an prebuild system from hp, dell, and so on.   When I started building it was just to build a pc cheaper than an prebuilt, then the more I dug into the hobby then I discovered that one can tune the hw, u dont need to oc it to the brink of destruction but having a system that run optimally for your workload was a given.  I am surprised how hubs and gns crowd/audience defend running systems which are not optimally setup. It is like they dont build their own systems but buy systems built by the stores.  so many issues, with these hw techoutlets, like skewed results, obviously faulty settings in the bios, 5.1ghz 14900k in gaming? gear 2 ddr4 vs ddr5?  But what can they do, they are locked into this now, as this is their audience.",hardware,2026-01-06 09:37:42,-1
AMD,ny31pa7,Nova lake,hardware,2026-01-06 21:42:02,1
AMD,nxytrlw,"Regardless of what they labeled it as, this is *still* just silicon binning, which is hardly a new practice.",hardware,2026-01-06 06:55:43,8
AMD,ny31v2q,And everyone complaining about this video would shit on the KS chips,hardware,2026-01-06 21:42:46,3
AMD,ny06ib8,Density does improve performance all else being equal but its not the only contributor.,hardware,2026-01-06 13:37:47,1
AMD,nxyminy,Kinda   Some performance improvements are from efficiency gains from design   Some performance gains come from density   You can cheat density to some regard by just making it bigger but that adds cost and lowers yields,hardware,2026-01-06 05:55:49,-1
AMD,ny36i60,"It's because they know your only real choice is between Intel and AMD, the two pillars of the x86 duopoly.   If you choose something else you may lose compatibility with your x86 software, which many people aren't willing to do.",hardware,2026-01-06 22:04:07,1
AMD,ny3a8ne,"> if any we should be harder on AMD than on Intel     Objectively yes, kinda.  The reason we tend to be more forgiving to AMD is that it's been in 2nd place for so long, and without 'em we end up on monopoly land where everything is so much worse than it is now.     AMD's been kicking this shit out of Intel for a while now and you figure that means the script should flip and now *they're* the ones that get the extra scrutiny.  I dunno if we're at that point yet...just the other day we saw posts that 'games leave intel in droves', leaving them at a piddly [55%](https://www.reddit.com/r/technology/comments/1q3uw7g/gamers_desert_intel_in_droves_as_steam_share/) steam share, aka *still* a freaking majority.     But given how shit things are atm, we should probably just be suspicious of everything...",hardware,2026-01-06 22:21:53,1
AMD,nxzcz4b,"On a side note amount of gamers that have no knowledge about PCs is staggering. I was doing for ""tech support"" in my old WoW guild way to often.  If a reviewer has an OEM system and tests it as such it's fine. HUB rather has test systems rather than OEM boxes. And you can see when GamersNexus reviews an OEM system that fails to work properly :) such systems would quickly skew the results tables.",hardware,2026-01-06 09:55:45,4
AMD,nxzbg7u,"Yeah, I completely agree with you.  There is a big difference between dangerous overclocking and tuning your system. I thought the hardware enthusiasts would understand this.   I bought up how much faster raptor lake is with tuned fast ram the other day and people countered by showing benchmarks at stock XMP.",hardware,2026-01-06 09:41:21,2
AMD,ny4lsvw,Did they imply it was new?,hardware,2026-01-07 02:29:41,1
AMD,nxyqx3u,Duh. Tick tock was all about either getting your performance gains from using the same design at a new node or gains from a new architecture.,hardware,2026-01-06 06:31:34,5
AMD,ny3tfls,Well compatibility layers exist and Microsoft is working hard on making WoA a first class solution. This X86 nonsense has to stop then only we will get progress in other architectures which would benefit consumers greatly of more companies can contribute.,hardware,2026-01-06 23:58:42,2
AMD,ny3tp9d,Then X86 is the issue. I dont see why ARM could potentially be cheaper and yet better than X86. Then these companies would be irrelevant to the needed scrutiny.,hardware,2026-01-07 00:00:05,1
AMD,nxzf00t,"I had a few low end mobos, and all of them were way overbuilt compared to oem/branded prebuilts. But in many countries butique prebuilts is the most common. Oem prebuilts were very popular during the crypto crisis though, remember it was basically cheaper to get one complete system instead of just the gpu :P",hardware,2026-01-06 10:14:25,2
AMD,ny4dunj,"C&C made some measurements of Venice's Zen6c 32 core CCDs at \~165mm^(2)on TSMC N2. That is huge, Zen 5 classic is \~70mm^(2) on TSMC N4P and Zen 5c 86mm^(2) on TSMC N3E.  There are two IODs each \~353mm^(2) while the single IOD on Turin is \~400mm^(2) with each IOD having 4 chiplets that could either be for structural or eDTCs.  Also, what I found really interesting was that they snuck in 'Venice-X' 3D v-cache making a comeback since Genoa-X, missing since Turin. That said there is no more information given, whether or not we can see the v-cache on Zen6c dense, or details on v-cache itself if there is more capacity, If it is featured on Venice Dense that would be 384MB of L3$ per CCD, 3gb of L3$ total.",hardware,2026-01-07 01:46:45,9
AMD,ny4w3yl,All that compute just to push kernels to as many GPUs that can be connected per CPU.,hardware,2026-01-07 03:26:28,5
AMD,ny4rhqt,">Calculating the die sizes of the base dies and the IO dies, the die size of the base die is approximately 747mm^(2) for each of the two base dies with the off-package IO dies each being approximately 220mm^(2).   So AMD is going to have at worst marginally less area spent on their compute tiles as Nvidia will spend with Rubin, but they get a node advantage of using N2 vs Nvidia using N3.   And then they also get the advantage of having the base IO tiles (not the IO tiles flanking the base tiles) containing a bunch of extra cache.   It's hard to imagine a world where, for a single GPU, AMD shouldn't be ahead of Nvidia for compute.",hardware,2026-01-07 03:00:35,1
AMD,ny4h2m4,">C&C made some measurements of Venice's Zen6c 32 core CCDs at \~165mm^(2)on TSMC N2. That is huge, Zen 5 classic is \~70mm^(2) on TSMC N4P and Zen 5c 86mm^(2) on TSMC N3E.  It's a lil crazy how much they are managing to shove into that \~165mm2 N2 die. Yes it's double the cores for double the area vs the N3E Zen 5C dense variant, but even the dense variants of Zen 6 are now rumored to get the full 4MB L3 per core as the standard variants, and that by itself should eat up a good amount of area.",hardware,2026-01-07 02:04:07,6
AMD,ny4pro5,">That is huge  It ain't huge. You're comparing 8core ccds to 32core ccds. Venice dense ccd is 2.3x the area but packs 4x the cores, all of this without reducing the per core l3 (unlike bergamo and turin dense).  >There are two IODs each ~353mm2  They went with 2 large instead of cutting it further for simplicity and greater on die fabric bandwidth at the cost of iod yields, great for perf.  >what I found really interesting was that they snuck in 'Venice-X' 3D v-cache making a comeback since Genoa-X, missing since Turin  That's because it was replaced by zen4 hbm which brought 50% increase in perf over genoa x. Needless to say nobody is gonna waste precious hbm capacity on that now, more sram on older nodes will do",hardware,2026-01-07 02:51:13,2
AMD,ny59qcn,"I mean yes, a lot of Venice deployments will be to drive GPU compute. But it's not like the demand for general purpose compute will evaporate, and this will do quite well there.  It's a beautiful design. They're finally evolving server chiplet architecture which has been the same 1 IOD, multiple CCDs since Rome. They're also using more advanced packaging, which should finally resolve some long-standing inefficiences present since the beginning of their Zen server line.",hardware,2026-01-07 04:50:23,1
AMD,ny4vxey,You can do the exact same comparison of die sizes and nodes and base tiles with lots of cache for MI350 vs B200 and come to the same conclusion. And yet...,hardware,2026-01-07 03:25:26,1
AMD,ny4uocn,It's the shift to gaa. n2 sram density increased by ~20% over n4/n3e. It started scaling again after stagnating for almost 2 nodes.,hardware,2026-01-07 03:18:22,3
AMD,ny4yowa,I'm surprised there is still isn't much detail on Venice. I wonder if any outlet got info on the show floor or some Q&A if it is 4MB L3$.,hardware,2026-01-07 03:41:22,2
AMD,ny4urtl,"I mean obviously I meant it relatively, Z5c is 16cores while Z6c is 32cores, while we have a difference in node, the area almost increases linearly.  As for Zen4 HBM I assume you're talking about Azure's 'MI300C'? I mean, I'd say that's on a different platform entirely with Zen 4 CCDs on AIDs (even though they still brand it as Epyc). It was even held back, till people thought it was cancelled since they were focused on MI300X for AI and MI300A for supercomputers, until it was picked up by Microsoft",hardware,2026-01-07 03:18:54,1
AMD,ny4zoxc,"It is ahead in terms of compute, is it not? Even or ahead in most categories.   Whether it be software or rack level scale out differences that allow Nvidia to sell more GPUs (or the fact that they were ahead of AMD to this market), is the single GPU vs GPU hardware capabilities for the MI350 vs B200 the reason they are behind in market share?",hardware,2026-01-07 03:47:17,2
AMD,ny4vqb0,"Bitcell size is the same as N5. Only macro density shows some gains, but those are design specific and heavily dependent on DTCO.",hardware,2026-01-07 03:24:22,3
AMD,ny4vvh4,">Z5c is 16cores while Z6c is 32cores  Yea but turin dense cut the l3 in half  >As for Zen4 HBM  Hbv5 is the successor to v4 (genoa x), they served the same workloads",hardware,2026-01-07 03:25:09,1
AMD,ny53hti,"It's about even in terms of paper TFLOPS but given you mentioned cache I assumed you meant real world performance, where it's significantly behind even on just single GPU matrix multiply kernels.  Not considering high precision here as both basically abandon it, but AMD to a lesser extent",hardware,2026-01-07 04:10:22,1
AMD,ny4xyku,>Bitcell size is the same as N5  And what are the numbers for n5 and n2?,hardware,2026-01-07 03:37:05,1
AMD,ny4zj4d,Does Venice Dense not share l3 like previous? Is it confirmed each Zen6c core gets it's own 4mb l3 instead of 2?,hardware,2026-01-07 03:46:20,1
AMD,ny56fmj,">It's about even in terms of paper TFLOPS but given you mentioned cache I assumed you meant real world performance, where it's significantly behind even on just single GPU matrix multiply kernels.  I was thinking moving more of the cache off die gives the XCDs more room for logic, but is AMD's cache hierarchy buns or something?",hardware,2026-01-07 04:28:53,1
AMD,ny4y3mp,0.021 micrometer-squared.,hardware,2026-01-07 03:37:55,2
AMD,ny4zd08,And where do ya see that n2 is 0.021?,hardware,2026-01-07 03:45:18,1
AMD,ny50hct,TSMC papers at IEEE conferences.,hardware,2026-01-07 03:52:00,2
AMD,ny50sbg,"Show it to help educate us, thx. Would be good to have some official docs for reference.  *Never mind i found it, looks like it's macro improvements but n3p is getting a bump over n3b and n3e too*",hardware,2026-01-07 03:53:50,1
AMD,ny54irh,Most of it is paywalled but here is Ian Cutress [discussing one of the papers](https://www.youtube.com/live/KpZAhQvQMGk?si=ftDGq_qXEDQG72zl&t=1365) last year,hardware,2026-01-07 04:16:49,2
AMD,nx8qncw,"As the comments on the site says, these are not new, they are new revisions of old boards.",hardware,2026-01-02 12:19:54,86
AMD,nx8nudr,>DDR4 memory that's available at more affordable prices  doubt.gif,hardware,2026-01-02 11:57:08,54
AMD,nx8k6wk,AM4 is the gift that keeps on giving.,hardware,2026-01-02 11:26:11,38
AMD,nx9tn8c,"Would be nice if they specified what the differences are, were there bugs in the old revisions? Are they cheapening out on the VRM? Do they support something new?",hardware,2026-01-02 16:06:19,7
AMD,nx99vvl,"We're not very far from them releasing new DDR3 boards. Maybe after a year or so, after a kit of DDR4 costs as much as a 5080.",hardware,2026-01-02 14:24:59,2
AMD,nxafwsh,I'd be curious the changes but personally I find this exciting. I have a AM4 rig that needs a new motherboard and I've been holding off because the options I could find were very limited. If this improves availability I'm all for it.  To be frank motherboard failures have been by far my most common failure point in builds over the years. And when a socket becomes out dated it can be very challenging to find something equivalent to replace them with. The used market can be a gamble. Breathing new life into a still useful system with a new motherboard is a good option to have.,hardware,2026-01-02 17:50:28,1
AMD,nxqh17i,"My new system:  9700X 64GB DDR5-6000 RTX 5070Ti   Old system 5900X 64GB DDR4-3600 Same GPU   My old system was a better performer. And get this, I cannot use the full ram speed on my new system, it freezes. My old system used up to 3600 on XMP with no issues, my new system IS 3600 and I'm told I should leave it there or issues will occur.",hardware,2026-01-05 01:18:01,1
AMD,nx91pyo,32 GB of DDR4 costs the same i paid for 64 GB of DDR5 lol.,hardware,2026-01-02 13:37:06,32
AMD,nx8qsb5,"Easy enough to find used...people will presumably realise the demand and raise the used price as well though.  Still, if you're happy with standard speeds then there's plenty of it out there for not much.",hardware,2026-01-02 12:21:00,11
AMD,nxav5nm,"It is possible to save like $100-150 on a 32GB set compared to DDR5.  Not enough to be worth it in my opinion, though.  Yea, DDR5 prices are even more painful, but if you HAVE to build a PC at the moment, AM5 still makes more sense in the long term.  There's more to value than just what things are like today.",hardware,2026-01-02 19:00:21,3
AMD,nxao7p2,Looking at Amazon right now: * 32GB of G.skill DDR4-3200 CL16 is $240 * The cheapest 32GB DDR5 kit shipped by Amazon (a G.skill DDR5-6000 CL36 kit) is $365  * 64GB of G.skill DDR4-3200 CL16 is $420 * The cheapest 64GB DDR5 kit shipped by Amazon (a G.skill DDR5-5600 CL36 kit) is $710,hardware,2026-01-02 18:28:26,1
AMD,nx9xeur,"It's tech stagnation *(beyond the simple fact these are revisions, not new products)* - not something to celebrate.",hardware,2026-01-02 16:24:02,14
AMD,nxbsrrz,>To be frank motherboard failures have been by far my most common failure point in builds over the years.   I would guess that is most people's experience as well.  Motherboards are easily the most component-complex part of a computer.  Just so many different things that can go wrong with them.,hardware,2026-01-02 21:43:03,3
AMD,nxal5j1,I didn’t last rebuild because my x570 board finally dies enough it wasn’t usually. It was a slow death spiral that finally came to a head when I had no more sata ports and only a partially functioning x16 slot.,hardware,2026-01-02 18:14:34,1
AMD,nxbsf49,Why’s that. I’m new the pc world and thought they were up there,hardware,2026-01-02 21:41:20,5
AMD,nx9eonz,I actually was tempted to upgrade to 32GB (2x16GB) but I looked at the prices.  No thanks.,hardware,2026-01-02 14:51:46,7
AMD,nxa3x89,RAM prices are so shit that the next system I buy might use SO-DIMM slots so I can cannibalize the 96GB RAM I have in my mini-PC/server/NAS that I paid $170 for a few months back. Then I'd pay $170 for something like 16GB to temporarily live in the NAS (which has 280GB optane for extra caching anyway).,hardware,2026-01-02 16:54:20,5
AMD,nxamx2j,8GB DDR4 costs the same I paid for 32GB of DDR4 lol.,hardware,2026-01-02 18:22:35,1
AMD,nx8tpoy,">Easy enough to find used  Ye, even used high performance B-die is still considerably cheaper than a C30 6000 D5 kit for the most part. If you settle for something like 3200C14 b-die you can score some deals.  If you go outside of b-die, there's droves of memory if you start looking.",hardware,2026-01-02 12:43:35,8
AMD,nx9ytsf,Bad news. The used market sucks too. I just ebayed 64gb of DDR4 that had been sitting in my drawer for about a year. I paid less than $100 for it new and sold it for $240.,hardware,2026-01-02 16:30:39,2
AMD,nxa757w,New tech is still coming out though... They are just supporting older products,hardware,2026-01-02 17:09:26,11
AMD,nxa4jt2,They are objectively worth celebrating. It's socket longevity regardless of where you try to plant the goalposts.,hardware,2026-01-02 16:57:13,15
AMD,nxaglwl,"It is for me.   My daughters Asus motherboard went from Ryzen3, Ryzen5 and finally a Ryzen7.   Bumped the ram from 16 to 32gb.  She plays very few games and would not notice the difference if I upgraded to an AM5 motherboard.",hardware,2026-01-02 17:53:41,2
AMD,nxsc08a,"Can't use the LGA1200 platform to build anything worth using nowadays, despite all the stagnation and being newer than AM4. So the latter does have merits.",hardware,2026-01-05 08:44:07,1
AMD,nxlhbl9,"Not ""shit"". Well they had some really problematic PSUs, hot stuff. And with the RTX50/RX90 series launch they f'ed up their thermal conductive material but continued to deny the existence of any problems, which was disappointing.",hardware,2026-01-04 08:56:43,1
AMD,nxdy1i5,Might be easier to get a few so-dimm -> u-dimm adapters.,hardware,2026-01-03 05:06:04,3
AMD,nx9ywt9,I'm not picky I'm not going to spend $100 more for like 3% better performance so I'm just going to deal with losing a few FPS,hardware,2026-01-02 16:31:03,6
AMD,nxac8j7,"Sure, but if you need 64GB right now, DDR4 is a lot cheaper than DDR5.",hardware,2026-01-02 17:33:17,1
AMD,nxb84c5,Because this has been in the pipeline for a while.  You can pretty much guarantee that we're going to see product launches delayed and hardware specs scaled back because of these RAM prices.,hardware,2026-01-02 20:02:20,3
AMD,nxzg5tb,"Not, they are objectively worth deriding for forcing stagnation in CPU pinouts.",hardware,2026-01-06 10:24:53,1
AMD,nxakda3,"What does that have to do with a ""new"" AM4 motherboard release?",hardware,2026-01-02 18:10:59,0
AMD,nxbka6j,"Nothing, its just AM4 is a solid product since I was able to upgrade my kids PC multiple times without having to buy a new motherboard.",hardware,2026-01-02 21:02:00,2
AMD,nxaw0og,"I think they're saying that AM4 is still good enough for plenty of people, and having more options on the market is a good thing.  Not that AM4 motherboards disappeared, but certainly not quite the huge selection and availability as before.",hardware,2026-01-02 19:04:25,2
AMD,nx8kekz,I'm keeping my 5800X until DDR6.,hardware,2026-01-02 11:28:05,134
AMD,nx8frrc,TLDW:    14 game average (geomean):     A. 1080P (medium) using RTX 5090 GPU:          - 5600X 4% slower than 5800XT    - 5600X 20% faster than the 12400F using the same DDR4-3600 memory      - 5600X 9% slower than Intel Core Ultra 5 225F that uses faster DDR5 CUDIMM    - 5600X 17% slower than 7500F    - 5700X matched the performance of the 12400F using DDR5 memory     B. 1080P (ultra) using RTX 5090 GPU:       - 5600X 16% slower than 7500F     - 5700X matched the performance of the 12400F using DDR5 memory,hardware,2026-01-02 10:46:39,114
AMD,nx8imcq,AM4 really is a gift that keeps on giving.   I'll mention I've thing that doesn't get mention much in context of the videos - e-waste saved by allowing people to keep the same motherboard while getting meaningful CPU upgrade choices.   It's good for your pockets and for sustainability,hardware,2026-01-02 11:12:13,103
AMD,nx8h72c,I saw 5800xt and thought it was about GPUS... wtf,hardware,2026-01-02 10:59:29,22
AMD,nx8sc8w,"I literally just sold my 5700X, B450 , 32gb 3200mhz yesterday.  Did I make a mistake?  I bought a Ryzen 7 7700, B650 and 2×8GB 5600mhz today   Will be overclocking the ram to 6000mhz cl36  edit : the upgrade cost me 51 USD by the way, bought the upgrade on the used market. GPU is 5070Ti",hardware,2026-01-02 12:33:06,15
AMD,nx8gav7,u/hardwareunboxed  Thanks for clarifying why the Intel i5-12400F performs significantly below the Ryzen 5 5600X.,hardware,2026-01-02 10:51:28,44
AMD,nx8hub4,Except ddr4 memory has also increased in price. Just not by as much. Just double not 5x.,hardware,2026-01-02 11:05:16,16
AMD,nx95dks,"It's depressing that the 5800X3D has not deprecated one bit. If you bought your 5800X3D 3.5 years ago at $450 you could sell it used for $450 or more today, because AMD won't make any more of them.",hardware,2026-01-02 13:58:54,17
AMD,nxa9sn5,The 5800X I got 5 years ago is still trucking along just fine thanks.  It's getting passed on to my nephew next year. more because he wants a PC than because I feel a pressing need to upgrade.  We forget what a solid generation Zen3 was even before the X3D glamour.,hardware,2026-01-02 17:21:52,4
AMD,nx8fm2a,5700x3d upgrade path available as well for $355,hardware,2026-01-02 10:45:14,13
AMD,nxcske4,"I've been torn on whether or not I upgrade to 5xxx series cpus, 7xxx, or 9xxx... I'm currently using a 3600 still and the 5xxx would be a quick and easy upgrade but I'm not sure it'd be totally worth.  Jumping to 7xxx like a 7600x3d would mean a new mobo/ram, but the next gen 10xxx units are supposed to be am5 still so I could skip the 9xxx cpus.  I live a couple hours from a microcenter soooo bundles near me exist but idk",hardware,2026-01-03 00:53:33,2
AMD,nx9svcl,People are still believing the windows update boost!?,hardware,2026-01-02 16:02:39,5
AMD,nx9hr0w,"I upgraded my AM4 AB350 system from first gen Ryzen 1600 to 5700X3D and the gaming results lined up with the reviews. But these chips falter with high RT usage games that have even higher demands on the CPU and RAM.      For example, in Stalker2 CPU-limited scenario, 9800X3D was almost 2.5X of its performance. I doubt that non X3D would be more than 20-30% slower, so it'd be better to get AM5 instead.",hardware,2026-01-02 15:08:02,2
AMD,nx8lzup,In the UK the 14600kf is the same price as a 5700x and cheaper than a 5800x. Why do they always compare to a 12400f.   I’ve also never seen a 12400f perform this poorly other than when it’s these guys testing. They always try to make AMD look as good as they possibly can. If they compared to a 14600k the results would look very different indeed.,hardware,2026-01-02 11:41:52,8
AMD,nx9lyjz,So what would be the best choice of GPU for a 5700X on 1440p without bottlenecking it too much? RTX 5070 or RX 9070?,hardware,2026-01-02 15:29:24,1
AMD,nxc0owc,The 5600x is amazing. I kick myself in the head for selling mine when I upgraded to 9800x3d.. ended up building pc for my kid months later and had to use an i5-8500 that got recycled at work.,hardware,2026-01-02 22:22:35,1
AMD,nxf6nlm,"I'm so glad that I got a 5700X3D upgrade back in July, runs everything i throw at it without breaking a sweat. Managed to bag it for £200 with a discount.",hardware,2026-01-03 11:17:15,1
AMD,nxf9kcd,More proof that 6 cores is all you need for gaming,hardware,2026-01-03 11:41:29,1
AMD,nxlho64,"And we see also that the Intel Core Ultra 5 225F performs very well in game, at least they tested it",hardware,2026-01-04 08:59:51,1
AMD,nxlo5ia,I went from a 2600x to a 5600x. Nice upgrade. Same motherboard.  I'm happy for the foreseeable future.,hardware,2026-01-04 09:58:59,1
AMD,nxloqt6,what is fine wine with this test? runs as expected. alderlake espeically the locked ones with small cashe always performed on par with zen3 with slower ram.  am I slow(yes I am :P) but what is the specs of the ddr4/ddr5 ram used? remember quality ddr4 stick is extremely hard to get and very expensive too if u want b-die. is ths 3200c14 or 3600c14 or what is available today which is basically a step above jedec like 18-22-22-whatever..,hardware,2026-01-04 10:04:14,1
AMD,nxzeuhh,I see they are still testing CPUs on a GPU bottlenecked games. Only one game in thier test suite even utilizes the CPU properly.,hardware,2026-01-06 10:13:01,1
AMD,nxa9e7t,"I came very close to getting a 5700X or a 5800X. If I had known that the 5800XT would be *$200* right now, I would have done that instead of going full AM5. Not that I regret going AM5 exactly, especially with all the nonsense around RAM going on.  If you're on AM4 right now, get a 5800XT or something, they're crazy cheap.",hardware,2026-01-02 17:19:59,2
AMD,nxaeljd,"Thanks for the test. Imma stay on my 5800X3D till AM6 lol.  TBH, that upgrade from the 2700X to the 5800X3D was the best in-socket upgrade for me since Slot 1 (Pentium II 266 -> Celeron 950 with a PGA370-to-Slot 1 adapter).",hardware,2026-01-02 17:44:25,1
AMD,nxbkxzn,"Daily reminder that at the settings you actually game at, 1440p high, that you will be GPU limited on your 60 series Nvidia GPU on a 5600X.   If you have a 3600X it will be better to upgrade your GPU than going to a 5600X.  We are all probably GPU limited still.",hardware,2026-01-02 21:05:14,0
AMD,nx94ol7,Another garbage video from Hardware Unboxed 🤢🤢🤢  They gonna do everything to lower Intel cpus performance.,hardware,2026-01-02 13:54:47,-12
AMD,nx8r36h,What about 5950X to 9800X3D for high refresh gaming?,hardware,2026-01-02 12:23:21,-9
AMD,nx8rkqk,annoying they only did 1080p comparison,hardware,2026-01-02 12:27:08,-26
AMD,nx92l56,"Same, which was always the plan.  Still regret not switching to a 5700X3D when that was at its low, but 5800X is doing just fine for now.",hardware,2026-01-02 13:42:20,43
AMD,nx8mhum,"same, i want to upgrade but my old bastard ryzen 5 2600x still handle new games pretty well. so i guess waiting for DDR6 is better choice.",hardware,2026-01-02 11:46:03,12
AMD,nx9op07,"Last year I did a drop in upgrade, replacing my 3600X with a 5700X3D, and my 5700XT with a used 7800XT.  It plays 99.9% of titles at more than good enough settings.",hardware,2026-01-02 15:42:46,6
AMD,nx8t9b5,"Or until the cyclical DRAM market implodes again, and you can pick up cheap DDR5 sticks. It sucks how you have to time PC builds based on the DRAM cycle.",hardware,2026-01-02 12:40:11,13
AMD,nxjddu6,"i have a 5800x and just got a 5070ti and can play kingdom come 2 in 4k, everything maxed out with dlss turned off and get 80 fps.  the 3d chip would be nice to have as my performance in marvel rivals is basically the same as it was with a 3080.  but i’m not paying $500 for a used 5800x3d",hardware,2026-01-04 00:29:14,3
AMD,nxt943y,I just upgraded from i7 9700 to 5900X in October. Feels good to be an obsolete adopter lol,hardware,2026-01-05 13:17:20,2
AMD,nxagjy2,"I also have the 5800X. Was really tempted to get the 5800X3D for a few times since then, but it's feels like a slight upgrade at most.  Though this is the first time I heard of an 5800XT I believe.",hardware,2026-01-02 17:53:26,1
AMD,nxawt2b,Same. My 5800X3D currently scales to GPUs I can't afford. By the time I can afford a 5090-class GPU that would benefit from a CPU upgrade at 4K I expect AM6 will be on its second generation.,hardware,2026-01-02 19:08:07,1
AMD,nxcey7i,2030 is a long time     ive heard many people say they're waiting for ddr6 but we'll see if you can wait for another 4 years,hardware,2026-01-02 23:38:33,1
AMD,nxaeb3i,This is the way.,hardware,2026-01-02 17:43:03,0
AMD,nxk8qw2,"When did they say they were using CUDIMMs? I didn't think anybody was using CUDIMMs outside of a few overclockers and Intel-sponsored experiments.  **Edit:** ah, 11:05. Thanks, AI.",hardware,2026-01-04 03:22:31,0
AMD,nxad72i,Yes! I'm still running my AX370 since 2017.,hardware,2026-01-02 17:37:50,6
AMD,nx8kdt1,Offset by anti-repair policies. The economy needs you to consoom,hardware,2026-01-02 11:27:54,23
AMD,nx8ycqn,Because people are too lazy to write Radeon or Ryzen.,hardware,2026-01-02 13:15:51,2
AMD,nx8vruk,"That’s an insanely good price, you also got AM5 so if prices calm down you can easily upgrade to an X3D chip.   If you held out on a larger upgrade, you probably would have lost access to a good upgrade path like you have now.",hardware,2026-01-02 12:58:19,28
AMD,nx9513g,"It's a solid upgrade, the only drawback is that just 16 GB of RAM might become a problem in the future.",hardware,2026-01-02 13:56:51,17
AMD,nx8teb4,"Its obviously a big advantage to be on the new platform, especially for a good price before things deteriorate even further.",hardware,2026-01-02 12:41:14,6
AMD,nx8u4xi,Do you feel like you made a mistake? Is the increased performance worth 51 USD in your eyes? That's the real question you should be asking.,hardware,2026-01-02 12:46:44,5
AMD,nx92dzb,"No, you didn't make a mistake even with the ram downgrade, as long as your games don't suffer from 16gb.   You'll be able to offload the ram and buy 32gb when prices stabilize and you'll be in current gen.  AM4 is great for those who don't want to move. Nothing wrong with doing that as well",hardware,2026-01-02 13:41:07,5
AMD,nx9raxp,"8GB sticks are a performance downgrade from 16GB because they have half the amount of banks so will have less throughput, I wish someone would benchmark that.",hardware,2026-01-02 15:55:13,2
AMD,nx8h3ux,5600x must have left the 11600k in the dust as well,hardware,2026-01-02 10:58:40,27
AMD,nxavtms,"There was a big dip around 2023 when you could get a new 5800X3D for $290, though back then you could get 64GB DDR5 for like $150 as well during the DRAM oversupply, so it made no sense to go AM4 for a new build. And as recently as August 2025 you could get a 5700X3D for $200. Now, with sky-high DDR5 prices and end-of-production it makes sense that prices have gone way up.",hardware,2026-01-02 19:03:29,4
AMD,nx9c0hn,"It makes sense they stopped production as it essentially cut production capacity away from more profitable current gen X3D CPUs. They are rarely found on the second market because who would do away with an X3D AM4 CPU without also getting rid of the rest?  Perhaps if AM5 CPU sales go down due to prolonged RAM pricing they might actually put it back into production, though I doubt it'd be significantly cheaper - if at all - than the original MSRP.",hardware,2026-01-02 14:37:07,7
AMD,nxbzn3y,"I think what's really saved Zen 3 especially is that most games today get built to run at 60fps on consoles with their slightly hobbled Zen 2 CPU's.  Even taking into account console optimizations, it still generally gives Zen 3 enough headroom to play all the same games at 60fps+ on PC as well.  In a world where devs were utilizing the CPU's in the consoles to push more in other areas than just framerate and 30fps was standard again, I think Zen 3 would have struggled a lot more.  I'd honestly found it a bit of a shame that this didn't happen for a long time cuz I wanted to see devs be more ambitious, but obviously it's a saving grace for many these days.",hardware,2026-01-02 22:17:12,3
AMD,nx8u25g,Bought a 5800x3d a few years back.. rocking a really old crosshair 6 motherboard (2017)… Just upgraded my 1080ti to a 5080 over Xmas… and now I cannot see a single reason why I would need to change either the CPU or the motherboard in the next five years.,hardware,2026-01-02 12:46:09,7
AMD,nx8tbtw,355 is crazy you can get a 14700k and still use DDR4 for that much.,hardware,2026-01-02 12:40:43,25
AMD,nx8vp63,"if you manage to find one, that is",hardware,2026-01-02 12:57:47,4
AMD,nxbxup2,Seems a very steep upgrade just to have last generation performance and no further upgrade potential.,hardware,2026-01-02 22:08:05,2
AMD,nxzf19z,5700x3D has never been available to 95%+ of world population.,hardware,2026-01-06 10:14:44,1
AMD,nx8mo0w,That's the path I went down for my final AM4 upgrade. I went from a 3800X to a 5700X to a 5700X 3d.,hardware,2026-01-02 11:47:29,1
AMD,nxa1lw5,It's not boosting   This is basically performance locked by admin control which is now unlocked  I can provide you different benchmark links as proof,hardware,2026-01-02 16:43:37,15
AMD,nxa2pnw,When it comes to the CPU I definitely care more about non gaming than gaming.  It isn't a console.,hardware,2026-01-02 16:48:44,7
AMD,nxa6au1,That is user specific. Some people will keep a browser open with 50 tabs while playing a game. I myself close everything when I game other than maybe discord if I need to chat while playing but I know not everyone does it how I do.,hardware,2026-01-02 17:05:28,8
AMD,nxa6fej,Yeah at least 5700x will be an ideal pick,hardware,2026-01-02 17:06:04,3
AMD,nxbyi2u,">The dumb reviewers never take into account that while gaming you will have other apps open and in use.  Nah, I pretty much never do.  At most I'll have like a single Chrome page open for a guide or something, but even then I usually just use my phone instead.    Also, if you're using Exclusive Fullscreen, Windows does a pretty good job of making background processes irrelevant to performance.",hardware,2026-01-02 22:11:22,2
AMD,nx9npyn,Maybe that chipset is too old it doesn't even support pci gen 4,hardware,2026-01-02 15:38:06,3
AMD,nxa5x09,The 5700X3D is a good chip but its reduced clock speeds even compared to the 5800X3D does hurt its performance.,hardware,2026-01-02 17:03:39,2
AMD,nxc1dz2,"They aren't the same price once you consider that somebody would need to buy DDR5 memory and an LGA1700 motherboard to actually get the most out of it.  They aren't trying to make AMD look as good as they can, ffs.  It's embarrassing seeing this constant conspiracy crap from people who just hate when AMD products genuinely have good light to put on them.  The main point of the video is really just ""Hey, if you're on Zen 3 still, you should probably just keep cool and be happy what you have is still good"".  The point wasn't necessarily some Intel vs AMD battle or telling people what they should be buying right now.  It's literally the main text in the thumbnail telling you this! lol",hardware,2026-01-02 22:26:09,8
AMD,nx8rnxk,You havent seen 12400 perform like this because most reviews were using DDR5 and also latest windows updates as stated on the video fixed ryzen performance   Also here is video comparing 12400 with Ryzen 5500 both using DDR4 . Mind you  5500 is a decent bit slower than 5600  [https://www.youtube.com/watch?v=1i8cs74UC\_4](https://www.youtube.com/watch?v=1i8cs74UC_4)  Also where exactly is 14600k the same price. The cheapest i have seen on UK is 185 pounds at currys where as 5700x is 165 pounds. Granted thats not much of a difference but 5600x which performs almost as good as 5700x is 140,hardware,2026-01-02 12:27:50,17
AMD,nxb23ff,">Why do they always compare to a 12400f.   Yea stock 12400F is kinda meh, If they had overclocked results it would make some sense, cause the reason why 12400F was great buy cause you could ECLK overclock it to 5Ghz+ easily on an asrock b760m pg riptide(some other boards, if you could find them for cheap) so you essentially almost had a 12600k(which curiously missing from the test list..) for cheaper as the locked SA voltage wasn't that big of deal for an alder lake cpu with ddr5.  That being said I'm very surprised that stock DDR4 12400F loses that badly, like how is that gap so big. E: oh apparently it can't do 3600 Gear1 cause of the locked SA voltage so either 3600 G2 or 3200 G1, lol, even more of reason to have -K alder/raptor lake in the list then.",hardware,2026-01-02 19:33:20,2
AMD,nx8qvpv,"It's clearly weird that you only see this result with AMDunboxed. Search everywhere else and you'll find 5600x = 12400F   And yep skipping the regularly sub 200$, sometimes even 150$ (with BF6 code) 14600K in every comparison to make intel look worse is their intention",hardware,2026-01-02 12:21:45,-10
AMD,nxzf6x4,Depends entirely what you play. I can bottlenect that CPU easy with a 1060.,hardware,2026-01-06 10:16:09,1
AMD,nx9o6cb,"What kind of price differences are you seeing?  In addition, it comes down to this:  RTX 5070:  +DLSS and other aspects of Nvidia software dominance. +More robust performance with ray tracing turned on -12GB VRAM is already starting to show difficulties in some AAA now, let alone 2-3 years from now.  9070:  +16GB VRAM with performance that means when games really want more than that, you’ll want a new GPU +FSR4 is finally competitive -And yet is still more a promise than a reality, with few games supporting it -Drivers. AMD still has problematic drivers way more. -Ray tracing is now possible, still not nearly to the extent of Nvidia cards.  In the end, only you can decide which sounds better to you based on price and the facts above.",hardware,2026-01-02 15:40:18,1
AMD,ny02tqp,Is 5800xt really worth 60eur over 5700x,hardware,2026-01-06 13:16:32,1
AMD,nx95rnz,"But are they still making the X3D CCDs and 1st gen cache dies?   There's alot of extra steps that go into that  Die sanding, hybrid bonding, etc.",hardware,2026-01-02 14:01:12,11
AMD,nx93doc,AMD seems to have a compulsion to continue releasing AM4 CPus so they just might. :P  It truly is the GOAT socket.,hardware,2026-01-02 13:47:04,1
AMD,nx934fc,"When you switch from a 2 gen old workstation CPU to a current gen top of the line X3D gaming CPU, you may indeed find that your framerates increase...",hardware,2026-01-02 13:45:32,14
AMD,nx8voy0,Its to stress cpu properly,hardware,2026-01-02 12:57:45,27
AMD,nx95hz3,"720P would have been nice to see, but 1080P is low enough with modern high end cards to show a difference between CPUs.",hardware,2026-01-02 13:59:37,10
AMD,nx96m96,"somebody [Copponex] needs to have a deep investigation why cpu tests are done at the lowest possible resolutions [at 1080p or lower, which is the correct way] ...   edit: added the subtext.",hardware,2026-01-02 14:06:12,-8
AMD,nx9vpip,"I went from a 5600 to a 5700x3d last year, and it's a change that doesn't SEEM like it would be big just based on the average, but it feels a lot nicer in some titles. Some titles really aren't much faster, but when a game is faster it's often a LOT faster. RDR2 I noticed a huge difference, same with Cyberpunk.",hardware,2026-01-02 16:16:00,17
AMD,nxadgc9,"FR, thought it would be in the market for a good while. 5600 is gonna be here for a few years still.",hardware,2026-01-02 17:39:03,2
AMD,nxddhgl,"5700x3d is still super cheap on ebay, because Chinese price is insanely low.",hardware,2026-01-03 02:54:46,1
AMD,nxkcz59,"Switched my 5900x out for a 5700x3d a year ago when they were cheap, glad I did with all this craziness now.",hardware,2026-01-04 03:46:50,1
AMD,nxuk9ut,I upgraded to a 5700x like... a month before the 5700x3d came out and decided to roll with it. I'm going to ride that particular donkey right to AM6 if I can.,hardware,2026-01-05 17:17:01,1
AMD,nxbk12h,At high resolutions and settings the X3D chips don't really add anything as you are back to being GPU limited. For the handful of people who play CS at 520p the X3D chips are great.,hardware,2026-01-02 21:00:46,-8
AMD,nx92u8j,You can upgrade to a used 5x00,hardware,2026-01-02 13:43:51,16
AMD,nx9rbcc,I went 3600 to 5800X. Which I got a month before the X3D reveal.  Boo.,hardware,2026-01-02 15:55:17,2
AMD,nxbtc2i,"Eh, this is not part of any 'cycle'.  This is a very unique and pretty much unprecedented(at least in a very long time) kind of DRAM pricing explosion.  One that isn't likely to get corrected anytime soon.",hardware,2026-01-02 21:45:49,9
AMD,nxjimt6,I considered used a 5700X3D but I probably missed the boat and couldn't find out cheap enough.  This was pre-DRAM apocalypse too.  But I have a 7800 XT so it's less of a meaningful upgrade.,hardware,2026-01-04 00:57:12,1
AMD,nxtqswn,At least you can keep the RAM!,hardware,2026-01-05 14:57:15,1
AMD,nxie789,Looks like Crucial's damage control bots came for me.,hardware,2026-01-03 21:29:34,1
AMD,nx8kqrl,"I know and you're correct. There's s definite push to force obsolescence in PC, but also in other industries",hardware,2026-01-02 11:31:03,14
AMD,nx9me28,AMD never should have had the same naming scheme for both CPUs and GPUs to begin with.,hardware,2026-01-02 15:31:31,41
AMD,nx95v4v,"Thanks, my upgrade feels justified now. Can't wait for the RAM to come so I can see the CPU fps gains!!",hardware,2026-01-02 14:01:47,5
AMD,nx95qgb,"Thanks, I am not a streamer or anything. At most I have discord running in the background when playing competitive games which are well optimized and wont be needing 32gb ram anytime soon.   When I play singleplayer games I play on Fullscreen and have nothing in the background and 16GB has never been an issue for me. Maybe next gen consoles will make 32gb mandatory but that's a ways off and hopefully by then ram prices will be normal again then I can upgrade to 32gb",hardware,2026-01-02 14:01:00,4
AMD,nx960ov,Thanks! I dont regret the upgrade. Looking forward to seeing the cpu gains,hardware,2026-01-02 14:02:41,1
AMD,nxekss8,"Waiting for the parts to be delivered. I don't think it'll be a mistake. I was CPU limited in the 3 games I have been playing. GTA 4 path traced, CP2077 path traced DLSS B, Lords of the fallen, even at DLAA 1440p if I remember correctly.",hardware,2026-01-03 08:09:33,2
AMD,nx9s50e,"Hardware unboxed did a 2×4gb/8gb/16gb/32GB video recently on Steve's own pc with a 3 year old install of windows 11, many chrome tabs open and discord open in the background.  Performance between 2×8gb vs 2×16gb is exactly the same in all but 1 game tested but if they closed the background tasks 16gb would be exactly the same at 32gb.  https://youtu.be/Bj5v52R4qnk?si=-UkOP5n9myB6Poxz",hardware,2026-01-02 15:59:10,6
AMD,nx8hj4c,Since it's matching 12400f ddr5  It should be faster than 12600k as well,hardware,2026-01-02 11:02:30,3
AMD,nxo239y,iirc it was fairly competitive between the two back in the day.,hardware,2026-01-04 18:27:31,1
AMD,nxnzopz,I'm kicking myself for not upgrading mine and my partner's computers to 5700x3d when Microcenter had them for $175 in late 2024.,hardware,2026-01-04 18:17:06,1
AMD,nxgqe93,zen 3 x3d will still be performant even if zen 3 cpu is struggling provided dev really take advantage of the cpu or being ambitious in game design correct?,hardware,2026-01-03 16:47:47,2
AMD,nx8wik5,"you're missing the point a bit. you have to take into account two factors:  * it's one of the best upgrade paths if you want to stick to AM4.  * AMD isn't producing 5700x3D anymore so price keeps rising as it's progressively harder to find  totally agree current prices are absurd and heavily scalped but only because some are willing to pay. I got mine one year ago to upgrade my previous CPU (5600x), and paid less than 200€ for it. in hindsight it was a great purchase.",hardware,2026-01-02 13:03:27,15
AMD,nx8u6nz,I'm not saying I would do it. But its there. Some people on 3700x want to keep their mobos and maybe save a bit,hardware,2026-01-02 12:47:06,3
AMD,nx9jir3,I think 14700K with DDR4 is actually slower than 5700X3D in gaming though.,hardware,2026-01-02 15:17:11,2
AMD,nxbv85t,Used on eBay is where to find it now,hardware,2026-01-02 21:55:03,1
AMD,nxc7umx,If it isn’t that then don’t compare them to a crappy 12400f when a 14600k is in the same price range.,hardware,2026-01-02 22:59:47,1
AMD,nxb2dh1,>Also where exactly is 14600k the same price  There was a period in the summer/late summer where they were 150€/$ for month or two,hardware,2026-01-02 19:34:40,7
AMD,nxce63r,"> Also where exactly is 14600k the same price   idk about other places, but in canada, the 14600k has had sales several times over the past few months at $230-240CAD https://ca.pcpartpicker.com/product/jXFmP6/intel-core-i5-14600k-35-ghz-14-core-processor-bx8071514600k  5700X's around $220CAD for the same period https://ca.pcpartpicker.com/product/JmhFf7/amd-ryzen-7-5700x-34-ghz-8-core-processor-100-100000926wof",hardware,2026-01-02 23:34:11,3
AMD,nx8shgv,"14600k performs way better than a 5600x tho and is worth the extra 40.   It has gone up a bit since a few months ago, but it’s still worth it over a 5600x or 5700x.",hardware,2026-01-02 12:34:15,4
AMD,nx8vh4c,"Personally I'm never buying a 13/14th gen intel chip because of the degradation. Yes, the 14600's are less likely to be effected compared to the higher end chips, but there's still been many reports of it happening. I don't really care how they compare to CPUs I'm never going to buy, as a 12400f comparison is a lot more useful.",hardware,2026-01-02 12:56:14,7
AMD,nx9vcw1,Well I'm assuming those two are the best GPUs to use with this CPU regardless of the price. anything above a 9070 or maybe 9070XT would be wasted GPU power. no?,hardware,2026-01-02 16:14:22,1
AMD,ny058fn,"If you already have a 5700X, no. But it sounds like you're choosing between them. I assume you have a 2600X or something like that?  If you play conventional games that are mostly GPU-based, no.  But if you use software or play games that appreciate a strong CPU, such as huge strategy games (especially something a little older like Civilization V/VI or a somewhat older Paradox game) or especially emulators, absolutely yes.  For what it's worth, I would also recommend the 5600X over the 5700X unless they are somehow the same price. The 5700X exists in this strange middle ground that's really only worth it if you get a 5700X3D.",hardware,2026-01-06 13:30:33,1
AMD,nxzfaa0,If they wanted to stress the CPU properly they should have tested with CPU intensive games instead of GPU bottleneck ones.,hardware,2026-01-06 10:17:00,1
AMD,nx9jql6,i understand the technical reason for it. It just doesn't say much about how my 5600x will fair in 1440p.,hardware,2026-01-02 15:18:17,-9
AMD,nxa766x,why is a deep investigation required when it's common sense?,hardware,2026-01-02 17:09:33,9
AMD,nxc2q3z,You're gonna make Aussie Steve's head explode. lol  I dont think people are gonna realize you are being sarcastic.,hardware,2026-01-02 22:33:02,0
AMD,nxzfeay,"The reason is because the testers are lazy and reuse the same testing suite they use got GPUs, so they test GPU bound games and need to lower resolution rather than increasing CPU load.",hardware,2026-01-06 10:18:00,0
AMD,nxa5w8x,"I built my current rig in april of 2018. Picked up a 5800xt to replace a very dated r7 1800x because I wanted higher fps in MH wilds. its on a b350 motherboard. Given the current hardware situation, I may be using an almost 10 year old computer at some point and its still relatively ""high end"". AM4 was a great platform.",hardware,2026-01-02 17:03:32,8
AMD,nxa9v3f,Did your 1% lows get any better across the board? I'm really thinking about doing the same thing,hardware,2026-01-02 17:22:10,3
AMD,nxq9ptz,"I play a whole heap of wow and I wish I bought a 5700x3d, wow loves the x3d chips",hardware,2026-01-05 00:39:27,1
AMD,nxbk7o7,It probably didn't change anything as you are still GPU limited.,hardware,2026-01-02 21:01:40,-3
AMD,nxe90fl,"Nah, 5700X3D has been out of stock on Aliexpress for a while now, almost a year (last time they were on sale was November 2024 for Single's Day, ~140 USD). If you're stateside, that's probably because your market has the 5600X3D so there's less of a demand overall.  There's been a few sellers with stocks recently, but they're 300+ USD. Over double of what they were selling for a year ago.",hardware,2026-01-03 06:29:42,2
AMD,nxbkhsi,"In my region used CPU's sell for more than just buying a new one off of AliExpress, used prices are stupid.",hardware,2026-01-02 21:03:03,3
AMD,nx9uxdc,i use to think for an upgrade but i'm holding for now cause i just dont think its worth current situation. thats why i'm just gonna wait for DDR6.,hardware,2026-01-02 16:12:20,0
AMD,nxe2b7q,Stocks like Oracle which are proxies for OpenAI are already crashing hard. Investors are being heavily skeptical of the AI DC buildout.   I give it six months tops.,hardware,2026-01-03 05:37:16,5
AMD,ny07iv2,I bought 128GB of DDR4 in 2021 because it was cheap. My RAM has appreciated to more than what I paid for in 2021 since November,hardware,2026-01-06 13:43:27,2
AMD,nxa0tn5,Also true.,hardware,2026-01-02 16:39:59,7
AMD,nxd726d,"Well you can't blame them, intel didn't have gpu:s when they made zen, so they couldn't copy their naming to follow up rdna, like they did for cpu:s and motherboards. The cpu isn't too bad sure at least they started at 1000, but the mortherboard naming was so blatant to have X370 and X399 when intel is on Z270 and X299...",hardware,2026-01-03 02:17:17,1
AMD,nxbvbmt,"There ARE games that can still be a problem with just 16GB of RAM, single player or whatever.  Not a ton, but they absolutely exist.  And the crappy thing about it is that there's usually not a lot of ways to reduce RAM usage via options or whatever.  Often it's just a case of you either have the required RAM or you dont.  Maybe you can try and play with the Page File to help with stability issues or whatever, but in terms of performance issues, you're usually out of luck.    I also considered going with 16GB for now to save some money and then upgrade 2-3 years down the line, but I got lucky with a very good(relative) deal on a high performance 32GB kit the other day so figured I'd just spend the extra now and not worry about it again.  Plus I do play games like city builders and whatnot that can be memory hogs.",hardware,2026-01-02 21:55:31,2
AMD,nxa9htg,My main recommendation would be to debloat Windows down to the bone.,hardware,2026-01-02 17:20:27,1
AMD,nxbs01y,This does not test single rank vs dual rank. If you lock down a dual rank kit to half the size in software you still get the dual rank benefits.,hardware,2026-01-02 21:39:20,9
AMD,nxbwlzr,"To be fair, HUB does tends to have a fairly limited suite of games they test that dont tend to be that memory hungry.  It's a decent enough indication if you're mainly a 'mainstream' gamer, but I dont feel it was quite a comprehensive enough test to show where it can get people into trouble.  In fact, I think a video specifically talking about examples where 16GB isn't enough could be useful, not to push everybody into buying 32GB, but purely so people are informed on what types of games people might need to think about this more with.  Also, aren't DDR5 modules kind of 'dual rank' by themselves, in effective terms, compared to before?  Like, the whole single rank vs dual rank thing just doesn't apply as much to DDR5 like it did with DDR3 and DDR4?",hardware,2026-01-02 22:01:53,3
AMD,nx9v533,"Oh thanks haven’t seen that, I’m surprised it’s not noticeably worse performance.  Must be a big difference between server workloads and gaming because I’ve seen over 10% difference with x16 chips vs x8.",hardware,2026-01-02 16:13:21,2
AMD,nx8m7tk,"Yeah, but we all know that it isn’t faster than a 12600k",hardware,2026-01-02 11:43:44,22
AMD,nx90ggy,"I remember my 12700k ddr4 being better at gaming than a 5800x, I wonder how they compare to each other nowadays",hardware,2026-01-02 13:29:15,5
AMD,nx8wutr,"Yeah, below 200 it’s a good deal still.",hardware,2026-01-02 13:05:49,3
AMD,nx8udiw,"Yeah, it’s just not a sensible decision. I got 2 for 140ish last year from AliExpress and that was a banger deal.",hardware,2026-01-02 12:48:29,0
AMD,nx9jm88,It isn’t.,hardware,2026-01-02 15:17:40,2
AMD,nxbykqk,No it really doesn't.,hardware,2026-01-02 22:11:44,3
AMD,nxbdjy8,"You have some benchmarks for that? The only ""background"" app I've seen that actually reduces performance is watching a video on second screen and even that just reduces gpu bound performance, not cpu bound.  Just because something ""uses resources"" doesn't mean a game will care about it.",hardware,2026-01-02 20:29:00,2
AMD,nxb884s,Discord memory usage isn't to bad on my system about 800mb and I only use it if in game voice isn't available. I also have a dedicated sound card I don't use on board audio never have so no issue with high cpu usage.,hardware,2026-01-02 20:02:50,1
AMD,nxci6gf,"The 7500f, 225f and 12400f are some of the cheapest entry points to their respective platforms. The point of the video is to gage AM4 is still a good enough platform vs the entry point of more modern (ddr5) platforms.",hardware,2026-01-02 23:56:34,5
AMD,nxa6tpg,"> 14600k performs way better than a 5600x tho     less so when limited to DDR4, yeah it's gonna be a bit better, but it really needs the DDR5 to achieve anything of much significance.    you're not going to see anything besides like a 5-10% improvement at most in games when you're stuck on ddr4",hardware,2026-01-02 17:07:56,3
AMD,nx988n5,That's amazing that Intel was able to release a 5600x killer three years later.,hardware,2026-01-02 14:15:37,3
AMD,nx9uxa9,"I have seen way more 9800x3ds fail than 14600k  If anything, the 12600k exist and went for as low as 100 dollars  But lets be real, you wouldnt have buy any intel cpu so why does the shitty 12400f need to be compared at all",hardware,2026-01-02 16:12:20,0
AMD,nx8yd1b,Just set a power and voltage limit and they won’t degrade faster than any other cpu.,hardware,2026-01-02 13:15:54,-4
AMD,nxb4iba,"The real frustrating answer is “depends.”  The processor is your ceiling. If 80+ frames is what your processor can crank out for a game on settings you choose, then that’s all she wrote.  But wanna crank up GPU killing settings that don’t affect CPU performance? Then the more GPU the better.  The big wrinkle is that ray tracing tends to be heavy and both CPU and GPU due to aspects of how it’s rendered.  But to go back to a counter example, Counterstrike 2 would likely shrug at cranking up to 4K and heavy graphics pairing that processor with a GTX 5080 or similar.",hardware,2026-01-02 19:44:54,4
AMD,ny076vs,"Yeah, im upgrading from 3600 and 6600xt to 5700x or 5800xt and 5060ti, would like to stay on AM4 for a little longer lol. Cant make up my mind tho since 5700x is 140eur vs 5800xt 210eur",hardware,2026-01-06 13:41:35,1
AMD,ny0b30j,"Also 5600x is basically same price as 5700x, so its a no brainer to go 5700x instead. Like 6 eur difference in pricing",hardware,2026-01-06 14:03:10,1
AMD,nx9pu1z,"WTF are you talking about? Resolution has zero effect on CPU performance, your 5600X will perform at 1440p the exact same it will perform at 1080p.  CPU tests are done at low resolutions only to ensure GPU performance isn't a factor that can interfere with the test.",hardware,2026-01-02 15:48:14,12
AMD,nx9kgqo,"Depends on your gpu, settings, games you play. You're the only one who can answer your real world experience.",hardware,2026-01-02 15:21:56,8
AMD,nxaklvc,"obviously it's not as common as its name implies, or we wouldn't have geniuses demanding useless benchmarks where all numbers are the same (why not go 4k with a gt730, when we're already making dumb requests?) and wayyyyy below the cpu's capabilites - which is the point of these tests; figuring out what they can deliver when they are not limited by other components.",hardware,2026-01-02 18:12:05,1
AMD,nxcrzh3,"maybe it's an ESL issue, but what do you mean by me being sarcastic? i'm saying anything above 1080p is useless for a cpu comparisson, which is exactly what steve is in agreement with and which contrasts the previous posters implied wish for 1440p/4k benchmarks.",hardware,2026-01-03 00:50:19,0
AMD,nxaglxf,"I'm also rocking a b350 board haha, it's getting up there in the years but hey it's still trucking. 5800xt is a great pick, hard to go wrong now that the X3D prices are sky high.",hardware,2026-01-02 17:53:41,2
AMD,nxapcht,"Yes, frame time consistency is the X3D's special sauce. FPS averages don't tell the whole story. IMO there's no going back, I'm a believer and my next CPU will be another X3D for sure.",hardware,2026-01-02 18:33:35,6
AMD,nxagwy4,"yes they did improve in most cases! I'd imagine a lot of that is just from the averages being higher too, but I'll take what I can get. At this point you probably won't be able to get an X3D 5000 series chip for cheap so it might not be worth it, but if you're on a 3600 or earlier or something, you could grab a 5700x or a 5800XT for a smaller but still fairly nice boost.",hardware,2026-01-02 17:55:06,3
AMD,nxbpjuf,It's a good thing I'm not talking about cases where I was GPU then.,hardware,2026-01-02 21:27:28,6
AMD,nx9xs1p,Zen 3 single-CCD chips (anything at/below 8-cores) removed the micro-stutter that affects all Zen & Zen 2 chips.,hardware,2026-01-02 16:25:45,9
AMD,nxaaria,"The 2600x is slow as shit. Just get something 5th gen, it'll be way faster and won't have the stutter.",hardware,2026-01-02 17:26:21,2
AMD,nxeh7ad,"Thanks, do you know which games are unplayable with 16gb? Probably Real Time Strategy games and city builders and Microsoft Flight Sim. I don't play any of those, not my type.   Unfortunately I 100% couldn't afford 32GB. I kind of upgraded just for the sake of upgrading.  5700X, B450, 2×16gb 3200mhz  to Ryzen 7 7700, B650, 2×8GB 5600mhz. The upgrade cost me 51 USD. My budget for upgrading was 50 USD😅  I bought the ram for 90 USD which in my country is a good deal. From what I've researched as long as you don't have anything unnecessary in the background 16GB is still enough for AAA gaming.",hardware,2026-01-03 07:38:48,1
AMD,nxaabjj,I don't think that'll be necessary. I mean I've been gaming with 16GB since 2019. I've only been on 32GB (ddr4) for the last few months.   At the moment 16GB is sufficient for gaming.,hardware,2026-01-02 17:24:18,2
AMD,nxb9y3m,Not even a little necessary with 16gb of ram,hardware,2026-01-02 20:11:13,2
AMD,nxeg8vl,"Yeah, I was originally planning on getting 1×16gb and getting another stick later on but I'm not willing to take the performance hit.  2×8gb is better, I'll have to just resell it for cheap once 2×16gb is affordable.",hardware,2026-01-03 07:30:38,1
AMD,nxehpto,"Real time strategy games, city builders and microsoft flight sim type games need 32gb I'm sure but from what I researched 16gb is enough for AAA gaming. I'm not a streamer and when gaming I don't have anything in the background unless I'm playing competitive multi-player games which are very well optimized and dont need 32gb even with discord running in the background.  I waa considering 1×16gb now then another later but later is potentially months to 2 years and I saw that single channel still leaves performance on the table so I went with 2×8gb.   Hardware unboxed tested 1×16gb here :  https://youtu.be/_nMu1KFkOC4?si=ohvJoY9Kq2_2GgTH",hardware,2026-01-03 07:43:12,1
AMD,nx9xl36,https://youtu.be/_nMu1KFkOC4?si=hYbSdY51rfnrJBc3  I was originally planning on getting 1×16gb then getting another down the line but here performance is actually worse unlike the 2×8gb configuration.,hardware,2026-01-02 16:24:51,1
AMD,nx91kvy,As per HUB conclusion because of windows latest update amd cpu performance improved significantly while intel remains the same   It should be the same as 12400f ddr5,hardware,2026-01-02 13:36:14,9
AMD,nx9slu5,Still faster,hardware,2026-01-02 16:01:25,1
AMD,nx8ym9y,indeed. but you won't find one at that price.,hardware,2026-01-02 13:17:34,11
AMD,nx9keze,Can you find newer benchmarks after the Windows updates that boosted AMD performance? If we take the hit 12400 took from DDR4 and apply that to the 14700k it seems like it would end up slower but I don't know how much 14000 series is affected.,hardware,2026-01-02 15:21:41,6
AMD,nx9add0,Are you stupid? The 12600k released a year later.,hardware,2026-01-02 14:27:45,2
AMD,nx8zd8p,"But that'll reduce performance, and at that point I'd be better off with the comparable AMD chip.  Also, it's just not worth the headache. I've had power limits disable themselves seemingly at random before, and I'd rather not have to re-do them after finding out the limits haven't been enabled for the past 6 months.",hardware,2026-01-02 13:22:21,9
AMD,nxby348,"I don't like RT nor care for it, I'm trying to play games at 1440p 100FPS or more and I'm willing to use FSR/DLSS to reach that. I mostly play single player games and rarely ever Esport or PVP games(other than CS2) and some PVE games. Here's a list of games I want to play and ONLY because of these games I'm upgrading:  Stalker 2   Space Marine 2   Dark Tide   Doom the Dark Ages   Resident Evil 8, 4 Remake, Requiem   Dead Space 1 Remake   Dead Island 2   Dying Light 2   Silent Hill 2 and F   Cronos the New Dawn   Atomic Heart   Metro Exodus  Other than these specially Space Marine 2, I have no reason to upgrade...I would have been perfectly fine with my 1080ti because I would be playing indie 2D games or boomer shooters 90% of the time.",hardware,2026-01-02 22:09:16,1
AMD,ny0f7zq,"Yeah, at that point, the 5700X is free performance. When I bought my 5600X, it was not so close, sadly.",hardware,2026-01-06 14:25:37,1
AMD,nxh5xks,"Thinks he wants to know how much slower 5600x vs xxx including gpu bound. But it depends on the gpu, games   For example: [https://imgur.com/a/2yfATKu](https://imgur.com/a/2yfATKu)",hardware,2026-01-03 17:59:31,-1
AMD,nxdh6tj,You:  > somebody needs to have a deep investigation why cpu tests are done at the lowest possible resolutions...  Also you:  > which is the point of these tests; figuring out what they can deliver when they are not limited by other components.  So apparently you figured it out after all.  Or am I overestimating your ability to understand you yourself wrote?,hardware,2026-01-03 03:17:11,2
AMD,nxzfg0z,If you tested with correct games you could test in 8k and still be CPU bottlenecked.,hardware,2026-01-06 10:18:26,0
AMD,nxgg5oe,">Thanks, do you know which games are unplayable with 16gb? Probably Real Time Strategy games and city builders and Microsoft Flight Sim. I don't play any of those, not my type.  Yea, mostly those types of games, exactly.  Plus some of the big factory/simulator type games and whatnot.   And even if you do like those sorts of games, there are plenty of good ones where 16GB will still be fine anyways!",hardware,2026-01-03 15:59:16,2
AMD,nxehhb3,"That would've been even worse because it's single channel, not just single rank, which also halves bandwidth.",hardware,2026-01-03 07:41:10,1
AMD,nxb5wyy,">It should be the same as 12400f ddr5  Huh?  I mean same as an overclocked 12400F, sure, a stock 12700K is probably the same, couple of extra cores and slower speed.   But a stock 12400F is 4Ghz not 5Ghz+, it's slooow.",hardware,2026-01-02 19:51:40,3
AMD,nxbq93g,Delusional,hardware,2026-01-02 21:30:49,1
AMD,nxawdva,Got extremely downvoted for [suggesting that we need a new DDR4 shootout revisit](https://www.reddit.com/r/hardware/comments/1pwosq3/comment/nw5h0qr/). It would be really helpful to know what the fastest DDR4 CPU in 2026 actually is.,hardware,2026-01-02 19:06:06,10
AMD,nxd5yy0,"All I really found was [this comment on a reddit thread](https://www.reddit.com/r/discordapp/comments/1dqf5h7/does_discord_actually_drop_performance_in_games/nxcu0fp/), which talks about gpu, not cpu impact, also it's recording... so not really ""background"" and you can just disable it.   Some other random forum stufff ppl saying, not showing, that on a potato laptop it might affect things, which sure maybe some old low power dual/quad core might have issues, but tha's not the topic it's 5600 vs 5900x. Some talk about the overlay being bad, with no data, just disable it then whatever. Hence why I asked benchmarks cause ppl be saying all kinds of things, but no1 is showing anything about how having more cores fixes things let alone more cores of the same architecture.  Also i don't remember when it was, but discord did have brief bug where it chopped off -200 effective memory clock from gpu:s, but that was fixed real quick, idk if that affected anything else back then.  However there apparently is hub benchmark about stuff like this:https://www.youtube.com/watch?v=Nd9-OtzzFxs  Sure it's 3.5 years old and doesn't have a dual ccd cpu, only 8 vs 6 cores, but the performance hit on both is the roughly same and miniscule at that. I wouldn't be too surprised if the youtube video is responsible for nearly all of that small perf drop, even when cpu bound, which sure i thought cpu bound there would be basically 0 impact. The one 4k youtube video test results lowering the 5700x result more at 1080p is a bit weird and not lowering at 1440p so seemingly not gpu bottlenecked, idk what's going on in there.  E: noo they deleted their comments and the one after this one, god dammit i had nice one lined up  Well for any1 curious I did manage to drop factorio performance a bit with two 4k youtube videos up, [by whopping 5%](https://imgur.com/a/KO9Uev8) on 10k spm, and nothing on [50k spm one](https://imgur.com/a/sm9RgLl)(the final 1000 updates is higher cause i wiggled an 8khz mouse for ~half the duration) but I doubt ""more cores"" would make the drop less.  Which was their original point btw: ""I have went from 5600 to 5900x and the difference gaming with other apps open is night and day.""  I guess they realized that it's bullshit, good for them!",hardware,2026-01-03 02:10:54,3
AMD,nx9bwps,Oh I thought you said 14600k in the post I responded to. Maybe you haven't edited it yet.,hardware,2026-01-02 14:36:31,10
AMD,nx90wxi,"Raptor Lake doesn’t really consume that much power in gaming. It’s in productivity that the power consumption is high. It’s the same as overclocking a Ryzen to use 150w in gaming, it performs the same as at 65w.",hardware,2026-01-02 13:32:07,11
AMD,nx8zzx1,"No, it doesn’t reduce anything. 14600k can easily run at 5.5ghz Pcores under 1.3V.  Sounds like a skill issue on your part, if you set it in the bios it won’t ever reset.",hardware,2026-01-02 13:26:21,4
AMD,nxh9t1z,"That's not how it works. If you're GPU limited, what CPU you have doesn't matter, they will all perform within margin of error of each other. It would also not answer his question of ""how would my 5600X fair at 1440p"", because if you're GPU limited then you can't really see the capabilities of your CPU to begin with.  The only way to benchmark CPUs that gives you useful information is by removing GPU limitations entirely.",hardware,2026-01-03 18:16:51,4
AMD,nxdj91e,explain to me how you read what i wrote,hardware,2026-01-03 03:29:49,0
AMD,nxb8l19,"Take the DDR4 vs DDR5 performance difference on the i5-12400F and compare it with the i5-12400F vs i7-12700K (both on DDR4) benchmarks — you’ll notice the percentage uplift is very similar.  This clearly shows that performance scaling is not just about core count or clock speed.  If raw GHz or more cores were the only factors, then many older-generation CPUs—with higher clocks and more cores—should still perform on par today. But they don’t.",hardware,2026-01-02 20:04:34,7
AMD,nxazu2k,"I just checked your comment... No, you didn't suggest it, you demanded it while dismissing the work he does to do that sort of reporting as useless content just because you didn't enjoy it. I imagine that's why you got downvoted, not because people didn't want to see DDR4 retesting, I think many people want to see it.",hardware,2026-01-02 19:22:26,-1
AMD,nxkalie,"If you set a voltage limit that limits the peak boost clock, which will affect performance.  If you set a power limit only which doesn't get hit in gaming, you're not doing anything to reduce degradation (unless your CPU otherwise spends a lot of time under high-power multithreaded loads).",hardware,2026-01-04 03:33:11,1
AMD,nx95ckg,It's needless hoops to jump through in an attempt to justify buying faulty hardware.  Raptorlake should be avoided like the plague.,hardware,2026-01-02 13:58:44,6
AMD,nxhbvfq,"He's not interested in the capabilities of his cpu. He simply wants to know currently with my setup and near future, if I game at 1440p how much performance will I lose out with 5600x compared to xyz.",hardware,2026-01-03 18:26:10,-1
AMD,nxgebqf,In plain English.  Apparently you thought you were saying something different from what you actually said.  That's on you.,hardware,2026-01-03 15:50:24,1
AMD,nx95uvm,"I disagree, I find it funny how most people here look down on console users, but then can’t even navigate a simple bios menu and spend 5 mins setting it up.",hardware,2026-01-02 14:01:44,5
AMD,nxhgysh,"> He's not interested in the capabilities of his cpu.  He literally wrote ""how will my 5600X fair (sic) at 1440p"".  The answer is, it will fare exactly the same as it will at 1080p, because resolution doesn't affect CPU performance.  >currently with my setup and near future, if I game at 1440p how much performance will I lose out with 5600x compared to xyz.  This question doesn't make sense. There's nothing a tech reviewer can do that answers it for him. If reviewers turn up the resolution in the CPU tests, they are either going to get the exact same results as 1080p (if it's still not GPU-limited), or completely fail to measure CPU performance because the test became GPU-limited.  The only useful information you can obtain out of reviews is how good each CPU is when not GPU-bound (in which case resolution doesn't matter because it doesn't affect CPU performance), and how good each GPU is when not CPU-bound.",hardware,2026-01-03 18:49:11,4
AMD,nxhgics,"We dont even know the refresh rate, the games & future plans, settings dlss/fg, rt, low-med-high? How much is he willing to stomach for lets say 20% uplift in these specific cpu bound games?",hardware,2026-01-03 18:47:10,1
AMD,nxgmy83,"> somebody [Copponex] needs to have a deep investigation why cpu tests are done at the lowest possible resolutions [at 1080p or lower, which is the correct way] ...  subtext is hard",hardware,2026-01-03 16:31:39,1
AMD,nx9vepf,"People here look down on console and prebuild buyers but refuses to tune anything and drinks the HUB koolaid, jUst wOrk eXtra hOurs to buy the faster cpu",hardware,2026-01-02 16:14:36,6
AMD,nx9761o,"Shouldn't need to, should perform as expected out of the box without having to prevent it from killing itself.   I have no issues fucking about in the bios, but the chip shouldn't be configured out the box to degrade over time.",hardware,2026-01-02 14:09:25,2
AMD,nxhindo,It will not fare exactly the same as it will at 1080p and 1440p.  That depends on the gpu & the game selection & settings  i.e Theres a massive difference between 5600x and 9800x3d with 5090 at 1440p and 4k.   Yes he wont get his use case from reviews. He should outline his perimeters and look around from different outlets. Techpowerup is pretty good,hardware,2026-01-03 18:56:48,-1
AMD,nxhfl9c,Your comment read as you agreeing with the complaint about testing at low resolutions.  What you thought you were writing is not a reasonable reading of what you actually wrote.,hardware,2026-01-03 18:43:04,1
AMD,nxa5any,"Yeah, it’s a joke. They look down on people who want to get the most out of their hardware.",hardware,2026-01-02 17:00:44,0
AMD,nx9c05s,"I never said it should, it’s still an easy fix that takes 5 mins if that.",hardware,2026-01-02 14:37:04,6
AMD,nxhktho,"> It will not fare exactly the same as it will at 1080p and 1440p.  It will. Resolution has zero effect on CPU performance.  >That depends on the gpu & the game selection & settings  The GPU is selected to be one that will eliminate GPU-bound scenarios, hence this test was done with a RTX 5090. So no, it will not depend on the GPU, because the test is designed not to depend on the GPU.  >i.e Theres a massive difference between 5600x and 9800x3d with 5090 at 1440p and 4k.  Yes. You can only see that difference because the GPU used for the test was the 5090. Repeat the test with a 5060 instead, and suddenly the 5600X and 9800X3D appear to perform exactly the same, how curious!",hardware,2026-01-03 19:06:39,3
AMD,nxidgyj,"mind also explaining why it's not self-evident that ""somebody"" refers to coppex, when there's literally in the second half of the sentence the phrase  >investigation why cpu tests are done at the **lowest possible resolutions**  in my native language that's basically a given that it refers to whoever one responded like that.",hardware,2026-01-03 21:26:02,1
AMD,nxyqa20,"Meanwhile where did all the PSU companies that were shouting ""AI AI AI"" in 2025 gone? Now no one in the consumer market is buying their products since GPUs, SSDs, and RAM has been bought out by real AI customers.",hardware,2026-01-06 06:26:09,265
AMD,nxyzjm2,Should just rename it to B2BES at this point.,hardware,2026-01-06 07:47:49,77
AMD,nxybis1,"AMD's primary customer for AI were the people they brought on stage. AMD doesn't make money from consumers using AI. They make money from the companies presented tonight buying their hardware, and they'll only continue buying that hardware if you use their software.  I don't really see it as much different than AMD/Intel bringing Dell, HP, MS, Acer, etc. on stage when talking about their new laptop CPUs",hardware,2026-01-06 04:37:12,275
AMD,nxya3l3,"Yep, CES 2026 is the grand AI circle jerk.",hardware,2026-01-06 04:27:53,112
AMD,nxyi64r,"CES stands for ""Consumer Electronics Show"". Someone needs to remind the presenters of this.",hardware,2026-01-06 05:23:01,136
AMD,ny0hnc9,"Amd literally had nothing to show though, 300 series refresh, 9850xd nothingburger. Redstone doesn't matter for most people. The only interesting thing was the new max 300 CPUs with 40 CUs, but that price decrease will equal out with current ram prices",hardware,2026-01-06 14:38:35,11
AMD,nxyb0ux,drink every time you hear ai quickest ER Trip of your life.... can't wait til all this crap goes away like NFTs,hardware,2026-01-06 04:33:55,79
AMD,nxybhzf,anyone fucking suprised? you think Amd gives a shit? they are like any company they will go where what will fill their pockets,hardware,2026-01-06 04:37:03,25
AMD,nxya68d,Yeah it's been hijacked by AI. Very annoying.,hardware,2026-01-06 04:28:23,30
AMD,nxzk3ro,"You said you've never seen anything like this, which is weird. Lisa always features random people when she doesn't have anything big to announce. Here's an excerpt from an AI summary of their 2023 keynote:  >  > CES 2023 partners (video sRXVRgMF2lc)  > Microsoft (Panos Panay): Discussed AMD–Microsoft collaboration across Windows, security (Pluton), Xbox, and Azure, and positioned AI as a defining shift; he used Windows “Studio Effects” as a concrete example of on-device AI features that can run efficiently using AMD’s dedicated AI engine instead of taxing CPU/GPU/battery. > ​ > ​ >  > Intuitive Surgical (Bob DeSantis): Explained how Intuitive’s da Vinci and Ion robotic surgery platforms work and their scale/impact, then detailed how AMD adaptive computing (Xilinx-class devices) supports real-time, low-latency functions like motion control, visualization processing/augmentation, and safety mechanisms, plus how robotic approaches can speed diagnosis (example: lung cancer workflows). > ​ >  > Magic Leap (Peggy Johnson): Positioned AR as distinct from VR and emphasized real enterprise/healthcare value; she highlighted Magic Leap 2 features (optics, dynamic dimming) and said AMD helped define a custom processor and AI/computer-vision engine, then described healthcare use cases like surgical planning and real-time 3D guidance (including a partner solution, Senti AR), and noted steps toward operating-room certification and ecosystem growth.  It's unfortunately always been like this. You're probably just more sensitized to hearing about AI today. How was lung cancer diagnosis relevant at a Consumer Electronics Show? Or Magic Leap, which targets enterprise & healthcare users?",hardware,2026-01-06 10:59:44,13
AMD,ny071kd,"Tbh both AMD and Nvidia didn't have anything remotely interesting for the consumer market this year, so they just presented the things they had for companies",hardware,2026-01-06 13:40:46,6
AMD,ny092ei,"Yep.  I’ve been saying to people to look at recent investments from AI and tech companies, it’s all circular right now.  In my eyes, they are doing it to create a life raft big enough for them all, so when it all collapses they can claim it affects us all, so they have to be bailed out.  They are going to collapse the world economy, mark my words.",hardware,2026-01-06 13:51:59,3
AMD,ny0hajf,The AMD keynote was from hell,hardware,2026-01-06 14:36:41,3
AMD,ny2qu4q,This year is the first year I’m actively avoiding CES because every presentation starts with AI 2s in… it’s exhausting… no one wants these “AI” features…. Even the bloody Microwaves have AI now…,hardware,2026-01-06 20:52:24,3
AMD,nxybqo9,Did I somehow miss amd announcing the 9850x3d?,hardware,2026-01-06 04:38:41,5
AMD,ny1gckb,>I never seen anything like this.  Hasn't it been like this for multiple years now?,hardware,2026-01-06 17:21:25,5
AMD,nxyj884,"The C in CES stands for Corporate, don't you know?",hardware,2026-01-06 05:30:48,7
AMD,nxyb49t,You got Ryzen AI 400 what more could you want? /s At least Intel showed Panter Lake and Qualcomm probably X2.,hardware,2026-01-06 04:34:32,4
AMD,ny58qxf,"Nah, it's a case of you seeing what you want to see. CES has been B2B heavy for a long time.",hardware,2026-01-07 04:43:57,1
AMD,nxyc47f,"It’s just the Gardner Hype Cycle — don’t forget there was a dot com bubble and bust too, but the web did end up taking over the world.  I’m annoyed by how people jump on em dashes as evidence of AI so I’ve resolved to start using at least one in every post or comment I make. There’s nothing wrong with using AI to clean up your grammar or spelling, it’s not a whole lot different than wearing glasses.",hardware,2026-01-06 04:41:12,-9
AMD,nxyr281,What does this have to do with hardware? Go cry in a substack for nobody to read.,hardware,2026-01-06 06:32:45,-3
AMD,nxyc00d,"There are two types of people in this world. Those who are prepared for the AI bubble to pop and those who aren't,",hardware,2026-01-06 04:40:26,-12
AMD,nxyrcwe,The crash out from Reddit/gamers over AI has been epic,hardware,2026-01-06 06:35:14,-21
AMD,ny36qtt,"People keep pointing to this 'AI circular economy' as if they've discovered some new, false economic scheme.   Guys, that's just how every economy works.   Money generates value by flowing and exchanging hands, and devalues when sitting stationary. The unwaivable core tenet to economy and currency is transaction. Without transaction, you have neither.  All economic activity is circular, and not all economic activity is Business to Consumer. Business to Business is a real and valid thing.  Business to Consumer is valuable via scale. You produce something, and have millions of customers, this allows for the accumulation and concentration of capital by facilitating millions of smaller transactions for the profit of 1 business.  Business to Business has less scale. You might have 5 customers instead of millions, but you have the benefit of exchanging money with less but larger transactions. And yes, when 2 business cut massive deals together, that synergy can unlock economic value in some shape or form. These aren't paper trades, there's some functional components happening as a result of those deals.   Go ahead and question the economic value of AI. Go ahead and question its true utility for consumers. Go ahead and criticize presentations for losing sight of their consumer-oriented purpose.  But the 'circular AI economy' circlejerks are just exposing lack of economic understanding and education......Almost as painful to read as reddits new favorite word Private Equity.",hardware,2026-01-06 22:05:15,-4
AMD,ny0sabb,"I actually enjoyed listening to some of the presenters, there were influential people talking about developments we should expect to see in the near future",hardware,2026-01-06 15:31:21,-5
AMD,nxyvuur,Probably figuring out how to make a rgb server power supply,hardware,2026-01-06 07:14:01,189
AMD,nxz8571,"The AI, AI, AI marketing for consumer products has completely failed.  Most of it wasn't even AI and the branding itself seems to have negative connotations.  Even the actual AI consumer products don't do much. NPUs anyone? Copilot+? Literally useless.  If consumers want to use AI, they will do so using cloud services.",hardware,2026-01-06 09:09:18,122
AMD,nxzl95o,I missed the AI PSU commercials. What did they peddle?,hardware,2026-01-06 11:09:39,19
AMD,nxz8mqm,"AI can be useful for the end user, but that's not where the money is.  the money is fucking over normal people.  that's great if you sell vram or AI silicon, but tough to market now that everyone with a functioning brain knows AI is mostly a surveillance/data collection tool for the billionaires.",hardware,2026-01-06 09:14:07,28
AMD,ny2hcfz,> ai ai ai  Maybe they accidentally summoned an Eldritch horror?,hardware,2026-01-06 20:08:20,3
AMD,nxza0f2,"they could make some money, may be by doing load balance for that 12pin cable on their end. Those high end Nvidia user might want those ""features""",hardware,2026-01-06 09:27:28,1
AMD,nxyd8ll,"Regardless, the CONSUMER ELECTRONICS SHOW is not the place for a B2B circle jerk.",hardware,2026-01-06 04:48:41,332
AMD,nxycz6a,"Imagine if Dell was buying Intel chips with money they got from Intel ""investing"" in Dell.",hardware,2026-01-06 04:46:55,24
AMD,ny1x204,AI Enterprise Show.  they were jerking each other off the whole time.,hardware,2026-01-06 18:36:15,4
AMD,ny1etrs,It too.,hardware,2026-01-06 17:14:31,1
AMD,nxympzt,The AI companies are the consumers they care about.,hardware,2026-01-06 05:57:28,70
AMD,ny4g28n,Enterprises are consumers too?,hardware,2026-01-07 01:58:40,2
AMD,ny0stjm,"Venice was interesting to me from a PC enthusiast perspective. It showed the Sea of Wires design with two I/O dies. This has implications for Zen6 consumer products, but they are obviously not ready to launch those yet.",hardware,2026-01-06 15:33:53,2
AMD,nxydsqr,It ain't gonna go away. Nfts are speculative digital investments designed to scam the last guy holding the bag. Unlike nfts ai has actual wide industrial uses.,hardware,2026-01-06 04:52:26,19
AMD,ny0xtrp,"Stack Overflow is a social media site where programmers would ask questions and usually get answers and solutions to those questions. Basically if you got stuck on a problem on an issue you would generally be able to find a solution or a workaround on this site. It was pretty popular.  This is the graph of the traffic the site receives in the passed years: https://i.redd.it/5au29srq8jbg1.jpg?width=1290&format=pjpg&auto=webp&s=c2ea25b9a07457a0f7cec6028be6cf5980d701e1  Yeah, we're not going back.",hardware,2026-01-06 15:57:01,3
AMD,ny02npl,"mods please delete this, its advocating suicide.",hardware,2026-01-06 13:15:33,-5
AMD,ny2kq35,"> like any company they will go where what will fill their pockets  Exactly, and that's great!  It's how they've been able to whip Intel in the desktop processor market, while also competing in the GPU space.  Good work AMD!",hardware,2026-01-06 20:24:04,-1
AMD,nxyfrsa,Enterprise AI Show doesn't quite have the same ring to it :-/,hardware,2026-01-06 05:05:50,6
AMD,nxybzi1,a 3% if that boost per Hardware Unboxed,hardware,2026-01-06 04:40:20,20
AMD,nxydfz5,I skimmed the presentation and I too was wondering if I missed it.,hardware,2026-01-06 04:50:03,1
AMD,nxydxsn,Extremely limited use case since there's still cross ccm latency. Really needs two parallel workloads that want lots of cores and benefit from the cache.  In that use case though it's amazing and likely a significant productivity uplift.   Outside that use case though it's pretty niche.,hardware,2026-01-06 04:53:23,-5
AMD,nxzfxj7,"I've seen people start to use -- now, just because.",hardware,2026-01-06 10:22:48,1
AMD,nxyi7gw,Not really. It's more like using the Dark Souls hint system. Soon all books look the same.,hardware,2026-01-06 05:23:17,-2
AMD,nxyi6s0,What are we supposed to do to prepare for the AI bubble popping?,hardware,2026-01-06 05:23:09,4
AMD,nxywa5y,Who wouldn't be pissed off that ai is making everything more expensive,hardware,2026-01-06 07:17:49,17
AMD,ny58ve0,"> People keep pointing to this 'AI circular economy' as if they've discovered some new, false economic scheme.  > Guys, that's just how every economy works.  The difference is whether you have actual customers paying for the product/service because they actually want to use said product/service, or whether all you have is a bunch of people throwing money at each other hoping they will all make a return at some point despite a lack of real demand for the thing they're throwing money into.",hardware,2026-01-07 04:44:45,1
AMD,nxzbfz8,Ubiquity has entered the chat.,hardware,2026-01-06 09:41:17,56
AMD,nxzk0c3,"I hated reading that sentence, thanks   (just imagine the visual diarrhoea from a good couple of racks filled with those - not to mention an entire server park)",hardware,2026-01-06 10:58:54,17
AMD,ny3zsob,"Instead of making 750W PSUs for peasants, it'll be 7.5KW and 75KW units for datacenters.  Used market after an AI crash? High probability that the 7.5-75KW units are incompatible with consumer motherboards, assuming the user is willing to run those units external to the standard ATX cases.",hardware,2026-01-07 00:31:27,3
AMD,ny185z1,"> NPUs anyone?  NPUs could have been great. Could have been with every new product release where they had decent specs for the last year and a half.  But no. They're not for developers or end users. They're for OS AI bloat, and the vendors *will not* ship real drivers or devkits for them.",hardware,2026-01-06 16:44:18,40
AMD,ny0l8gc,"Back when this MS AI NPU requirement was pushed I speculated the whole point of local ""AI"" is to capture/analyse people reaction to advertising and social/political forums so they could sell that emotional feedback. That would allow MS to really compete against google's ads platform.",hardware,2026-01-06 14:56:58,30
AMD,ny17mt5,Remember the Rabbit R1? Or that stupid pin?,hardware,2026-01-06 16:41:53,8
AMD,ny3f530,"> NPUs anyone?   NPUs are here to stay. It's just that so far they are weak and have bandwidth limitations. By the time DDR6 comes in, NPU will see a sizable gain in performance.",hardware,2026-01-06 22:45:29,-1
AMD,ny0csmq,"I don’t know, all I know is the PSU I bought from MSI said “AI PC Ready”",hardware,2026-01-06 14:12:36,32
AMD,nxzl43n,The money is nowhere. There are zero profitable AI installations.,hardware,2026-01-06 11:08:27,36
AMD,ny012pn,"> everyone with a functioning brain knows AI is mostly a surveillance/data collection tool for the billionaires.  Unfortunately, functioning brains are nowhere to be found these days. Humans are raw garbage and they'll get what they deserve.",hardware,2026-01-06 13:05:50,5
AMD,ny20xof,EVGA Already had this with the 3090Ti and their PowerLink,hardware,2026-01-06 18:53:25,2
AMD,nxytjk6,"I feel like the CES has always had a rather heavy B2B side to it.  Beyond the keynotes, you have a lot of products that may be looking for investors, or are trying to get other businesses to buy in. You may remember things like Novec and variants (the evaporative cooling liquid) - things like that were never meant to be deployed for your average consumers.  Big tech keynotes have always had a very heavy b2b side to it, just that this year it has a much heavier tone thus far. Even then, the AMD one isnt even as bad as the nvidia one where ALL of it is B2B.",hardware,2026-01-06 06:53:50,61
AMD,nxyf2oq,"Indeed, sounds as out-of-touch as when MS unveiled the Xbox One to a gamers audience with presentations from cable companies etc about watching the NFL on a gaming console, and pretty much nothing about games 🤦  Hah it’s even more cringe than I remembered: “Xbox is about to become the next water cooler” https://youtu.be/KbWgUO-Rqcw",hardware,2026-01-06 05:01:04,74
AMD,nxyn0dt,Consumers barely have a place in the economic equation rn. They've been replaced by AI,hardware,2026-01-06 05:59:43,19
AMD,nxygkvx,"Sure, but besides for Gorgon Point, which is a boring Strix Point refresh with a mild clockspeed bump and a bigger NPU, they have nothing consumer focused to unveil until Zen 6 launch (although I guess they could've announced the rumored 9850X3D, but that also is barely much different than the 9800X3D)",hardware,2026-01-06 05:11:34,13
AMD,nxymbcq,"FYI. From CES themselves:   CES is a trade-only event that primarily targets **professionals, businesses, and innovators within the consumer technology industry**, rather than the general public. The event brings together a diverse global audience for networking, deal-making, and exploring future technology trends.",hardware,2026-01-06 05:54:13,38
AMD,ny0a9vt,"That is exclusively what CES is for...  It's a trade show, for the people who work in the trade. It is not for consumers (you literally can't even buy consumer products there), it is for companies in the consumer electronics industry and the companies that serve those companies.",hardware,2026-01-06 13:58:40,7
AMD,ny0m5xc,"This was their own private presentation in the same location and time as CES but not CES. All the people you want to present to are already here so why not do your own presentation? Its a win win, press dudes do only one trip and you are garmented to get loads of the right people attending the presentation.   Why people are watching presentations not geared to them is the real fuck up here. I doubt AMD care that some idiot nerd turned up to the wrong event.",hardware,2026-01-06 15:01:37,3
AMD,nxziz5j,"What? The B2B clients are the ones who sell these products to consumers.  It's a trade show, that is and was always the point of it.",hardware,2026-01-06 10:50:01,5
AMD,ny0dogq,"I've used to be in the industry, let me tell you: consumer electronics is and Industry is and the exhibitors there are, by far, suppliers for this industry.    The brands that sell directly to consumers are the tip of a very big iceberg. And traditionally they're only exhibiting there because the retailers are there, after all, that's what a trade show is about.    Reporters are not interested in most of CES, and consumers are not even allowed in most of it.",hardware,2026-01-06 14:17:21,2
AMD,nxzjbni,It's always been a b2b circlejerk. With a side for consumers.,hardware,2026-01-06 10:53:01,3
AMD,nxyhicd,Facts,hardware,2026-01-06 05:18:13,2
AMD,nxyggse,That's may end up being what Nvidia is doing with their Intel partnership.,hardware,2026-01-06 05:10:45,19
AMD,nxyjad3,"Haha, Intel would never offer financial incentives to customers to purchase its chips.  ... Right?  https://imgur.com/a/KXhciwE",hardware,2026-01-06 05:31:14,10
AMD,nxzjt0i,"Dell was buying Intel chips with the money they got from Intel for “marketing”, which just means “ not using AMD”",hardware,2026-01-06 10:57:10,3
AMD,nxz492k,They will just rename it to AES next year.,hardware,2026-01-06 08:31:49,19
AMD,ny0w586,"I guess kinda, but I don't really care until it's released. Intel's new CPUs are kinda hit and miss too but Im more curious about them",hardware,2026-01-06 15:49:21,2
AMD,nxypv7r,"> Unlike nfts ai has actual wide industrial uses.   Yeah, it's been a few years and the models aren't getting that much better. This one's not happening.",hardware,2026-01-06 06:22:44,36
AMD,nxyeqoj,He meant like VR and 3D.,hardware,2026-01-06 04:58:48,3
AMD,nxye816,"bubbles gonna burst soon, it's a fad. and blockchain has uses outside nfts too but that doesn't mean it's the best thing since sliced toast, this ""movement"" was forced by capitalists",hardware,2026-01-06 04:55:16,-5
AMD,ny0cz0b,It doesn’t need to go away for the size of hardware usage to go drastically down,hardware,2026-01-06 14:13:33,1
AMD,ny26tjj,They did that to themselves. Stack Overflow was a horrible place to find answers even before AI.,hardware,2026-01-06 19:20:05,2
AMD,ny03ict,https://en.wikipedia.org/wiki/Sarcasm,hardware,2026-01-06 13:20:32,4
AMD,nxyduio,"I swear I watched like 95% of it, I didn't see it...",hardware,2026-01-06 04:52:45,1
AMD,nxyg9ev,Wha.. why are you talking about cross ccm stuff and productivity? 9850X3D is just an overclocked 9800X3D...,hardware,2026-01-06 05:09:18,6
AMD,nxyg0b5,> Extremely limited use case since there's still cross ccm latency  In an 8 core part? Unless something very weird is going on it's a single chiplet.,hardware,2026-01-06 05:07:29,4
AMD,nxyuvt0,"Take financial and stock market advice from people, who aren't particularly good at either, obviously.",hardware,2026-01-06 07:05:22,15
AMD,nxyj48g,Cash out,hardware,2026-01-06 05:29:58,7
AMD,ny3zo9g,Honestly I've been wondering about this too. VXUS? It seems like most of the madness is stateside.,hardware,2026-01-07 00:30:48,2
AMD,ny04gd5,"preppers always exist, before internet even.",hardware,2026-01-06 13:26:05,1
AMD,nxyv2gi,"Sounds like he's ""preparing"" for a while since he has no money to put in an index... I guess that's a far as that goes",hardware,2026-01-06 07:06:59,1
AMD,ny2a4jx,"yeah, imagine if that NPU can be used as a universal GPU thing to add DLSS / XeSS / FSR to any gpu as part of direct x or some rendering pipeline with fuck it DX13 or vulcan feature  but nope, its just for marketing at this point and big players to farther process your local data for likely their own use.",hardware,2026-01-06 19:35:09,14
AMD,ny3f9vy,"That would be fairly inefficient.  Most data companies care about is already on their servers (social media, email, shopping history, etc.).  NPUs were mostly a hopeful aspiration of people being able to run personal AI assistants but paying for the power themselves.  Thing is, companies would rather have the raw data itself, process it on their end, and send the user back the results.  ""Field AI"" data collection is unreliable (because models are unreliable) and can't be used as effectively to iterate on models.",hardware,2026-01-06 22:46:08,2
AMD,ny2i2at,"At least the Rabbit R1's CEO has some self-awareness and acknowledged that the product is kind of a quirky toy. The Humane Pin's team thought they were Steve Jobs reincarnate and went full on Apple-esque marketing, like they were freaking changing the world. What a joke.",hardware,2026-01-06 20:11:41,6
AMD,ny3nkva,">By the time DDR6 comes in, NPU will see a sizable gain in performance.  DDR6 isn't going to be *that* much faster, and it's still going to be severely limited by the architecture itself.  It's still going to be lower performing than even an entry-level Nvidia GPU, and still won't be useful for much of anything practical.",hardware,2026-01-06 23:28:04,5
AMD,ny1gcwo,What more do you need? IT'S READY,hardware,2026-01-06 17:21:28,16
AMD,ny3nxfs,All that means is that it has a 12V-2x6 connector for a GeForce RTX 5070/5080/5090 GPU.,hardware,2026-01-06 23:29:54,3
AMD,ny12ol8,"maybe not profit, but I'm sure companies like Palantir are getting what they want out of the AI bubble.  All the facial recognition, government contracts, and tracking people.",hardware,2026-01-06 16:19:17,18
AMD,ny0ljsn,The real market is the government/MIC.,hardware,2026-01-06 14:58:34,17
AMD,nxzphsr,"Hey, Topaz probably makes some money on their software even though its most of it is a shitty nightmare fuel generator",hardware,2026-01-06 11:44:22,-1
AMD,ny2aonh,it is being used as a way to possibly replace workforce.  there are lots of companies that are cutting workforce and hoping that the rest with ai have the same productivity   same with the AI chatbots vs support agents.  and well self driving and all that replacement of logistics drivers  and one thing we see that is 100% for sure is using AI to replace voice actors in certain games.  all of the AI so far that is profitable is to save labour costs.,hardware,2026-01-06 19:37:42,-2
AMD,nxzerkg,"It’s literally a trade show, it was always designed around B2B. The hope is you see something there and place an order for XYZ amount for your business or store.   The only difference is that it went from B2B dealings on consumer electronics, like distributors and such, to data centre hardware.   Regardless, the overall goal of the show has always been to push B2B sales. It was only “consumer” solely in name and has been for a while now, even before the AI boom.",hardware,2026-01-06 10:12:16,35
AMD,ny3o9ib,">I feel like the CES has always had a rather heavy B2B side to it.  Ever since they killed COMDEX.  COMDEX folded into CES, so now CES is about 50% B2B stuff.  Probably over 50% by this point, since consumers can't afford this shit anymore.",hardware,2026-01-06 23:31:39,6
AMD,nxyljbb,"Funny enough, most of the reasons people trashed the original Xbox One, like a focus on multimedia, the focus on digital games with no trading, voice commands and voice activation a whole year+ before Amazon launched the Alexa - were in many ways too *early*. The market eventually ended up going the direction Xbox One tried to take it in a lot of ways",hardware,2026-01-06 05:48:14,50
AMD,ny01wqh,but wasnt Xbox main purpose iso be a very ugly blueray player anyway?,hardware,2026-01-06 13:11:01,1
AMD,nxyk98t,"to be fair, more people use consoles as a media box than a game player",hardware,2026-01-06 05:38:32,-7
AMD,ny16co2,Everything is targeted to investors.  They have more money than consumers.,hardware,2026-01-06 16:36:07,3
AMD,ny0u7ni,the uber wealthy are risking everything to try and replace (remove) the majority of humans on the planet. no need for billions if subservient AI replaces them. no risk of being usurped if you don't have human competition.,hardware,2026-01-06 15:40:26,2
AMD,ny0dedu,We are the consumer. The AI companies are trying to get us addicted to AI chatbots and other stupid AI tools and then stuffing ads into them.,hardware,2026-01-06 14:15:51,1
AMD,nxyozwd,". It’s always been a trade show and until the last two years the products they talked about were “consumer technology”. If I sold my house, I could get 4 or 5 top level GPU cards they introduced today for like $400,000, except they won’t even sell them to consumers. How is that “consumer technology”? Consumer technology means it’s aimed at people not businesses.",hardware,2026-01-06 06:15:38,21
AMD,ny3vgsf,"For all the good old days sentiment have people looked back at what companies typically present at CES? Look at the heydays of 2016 CES from Nvidia -  https://www.youtube.com/playlist?list=PLZHnYvH1qtObAdk3waYMalhA8bdbhyTm2  Oh look... car companies and AI, with zero content for graphics cards intended for the DIY market.",hardware,2026-01-07 00:09:11,1
AMD,nxykc8r,Nvidia's intel deal was all about short term cpu option. They didn't think that vera was competitive enough from a per core perspective and were looking for something higher perf. Intel became that choice because it ain't an ai rival. They simply found that cpus are another front for differentiation and amd could well take advantage of that to offer competitive advantages in ai cpu. Strengthening intel in client with their gfx ip also pressures amd margins which is another advantage  They're dropping intel the moment their inhouse design gets good enough.,hardware,2026-01-06 05:39:10,4
AMD,ny02sdl,they dont have to. current models are already widely used.,hardware,2026-01-06 13:16:19,7
AMD,ny0cae8,"Within the last 12 month we went form first deepseek release to GTP level neural networks you can run on a decent gaming pc.   Like, the development speeed has been CRAZY, with every few months a significant improved model from one of the chinese companies being released.",hardware,2026-01-06 14:09:48,3
AMD,nxyrhqv,I dont think llms are a general purpose tool. Im hoping for better behind the scenes tech than just gen ai slop.,hardware,2026-01-06 06:36:22,4
AMD,nxyf3na,">bubbles gonna burst soon  Ai is here to stay whether it bursts or not. It's just a question of whether it's overvalued. It's still gonna have significant value even if it ""bursts""",hardware,2026-01-06 05:01:15,10
AMD,nxzo87w,I wish I could agree but I can't. There are too many use cases where it works. The future potential is also massive even if it doesn't reach all the way.  A lot of the places it is used in however are absolutely dumb however.,hardware,2026-01-06 11:34:24,4
AMD,nxyj2bo,It wasn't announced during the presentation but it was announced,hardware,2026-01-06 05:29:33,5
AMD,nxyhsrq,Probably got it mixed up with the rumored 9950x3-double D.,hardware,2026-01-06 05:20:20,5
AMD,nxyqii4,"Yeah, I read too fast and thought it was the 9950x3d being discussed. My bad.   /u/CatsAndCapybaras",hardware,2026-01-06 06:28:08,3
AMD,nxyqkm4,My bad. I mixed up with 9950x3d,hardware,2026-01-06 06:28:38,2
AMD,ny17aht,Not with the way they are inflating money.,hardware,2026-01-06 16:40:21,0
AMD,ny4e0ql,"Doubt it's viable, imagine added latency of transferring frame from gpu to ram, processing it with relatively slow npu and then back to gpu",hardware,2026-01-07 01:47:40,2
AMD,ny3qkj6,Just think about local AI analysing your face impressions all the time and connect this to what you see and read. I can surely see the value in that and how would that be inefficient? Doing that remotely would take huge amount of bandwidth and be impractical.,hardware,2026-01-06 23:43:49,1
AMD,ny4040l,"And the advertising industry.  More than a decade ago, I read an article about an advertising executive bragging about how the future they envision is an advertisement display within a Starbucks store dynamically changing their offers to each specific person. The only way that was going to be accomplished was extensive data collection to accurately predict someone's eating/drinking habits.",hardware,2026-01-07 00:33:06,2
AMD,ny00oyy,"Topaz Labs?! Doubt it. Revenue sure. Not profit. They were still chasing funding support last spring  and spending $2 on compute for every dollar in revenue. Thats the issue, the cost of running ai is expensive. It’s currently all subsidized by VC but that won’t last. We are in the “$4 uber rides” phase of AI, the actual costs of energy and hardware is gonna come back on us before any one firm gets enough market share to dominate.",hardware,2026-01-06 13:03:24,14
AMD,ny2eq2x,"Again. The cost of running the AI makes it unprofitable. Huge cost sinks in these data centers so far. None have returned on investment beyond 100%   Companies firing people for AI agents are mostly just reducing head count, stressing the remaining employees and using AI as a cover.",hardware,2026-01-06 19:56:11,5
AMD,nxziq9b,"Straight up this. I was an attendee at the 2009 CES looking for new product lines for our small computer store. Keep in mind, this was 2009 so the iPhone and iPod were the 'bees knees' of the industry. There was an entire hall of the LVCC that was solely devoted to Apple accessories, and those were all looking for major buyers (1000+ units per order) and our store just didn't have the customer base to support that.",hardware,2026-01-06 10:47:54,16
AMD,nxza0ro,"It was widely slated in significant part because of the always online requirement, which is still considered a dealbreaker for a lot of people",hardware,2026-01-06 09:27:33,20
AMD,nxyuwcp,"I'm mildly annoyed my Switch doesn't really work for streaming video, but at the end of the day it's the only console I own. So IDK if the console market really moved toward multimedia.",hardware,2026-01-06 07:05:30,8
AMD,nxzjw9x,"Unlike with the CD/DVD drives that drove sales in earlier generations (CD/DVD players usually cost around $200 at launch? Ballpark of console prices at the time), multimedia devices were cheaper and more common. There was no real benefit of buying a console for those features. Maybe if said multimedia features had launched with the original Xbox (fantastic multimedia devices when cracked) when there were few alternatives.",hardware,2026-01-06 10:57:57,5
AMD,ny0tz1n,"Yeah 100%. Even the Xbox 360 was actually kind of bleeding edge for it at the time. I had three of them as players around the house, and setup a backend built on WMC and LiveTV DVR functionality via Cablecard tuners. And in addition you had official support for Netflix in like 2008? Maybe slot in a little codec pack so ""unofficial"" media worked as well. That all came together nicely for a period of time as a unified media center for the home. I'm actually *somewhat* nostalgic for the era when paid TV didn't suck, and you could freely and legally digitally record new shows and content, then keep it forever. Even now I still have lots of drm free, raw TS files of major sporting events, stuff like the original broadcast recordings of Breaking Bad, and other such things on my NAS.   Point being, it made a lot of sense to me at the time with the Xbox One why they were trying to continue with that direction, and push those ideas more mainstream, but unfortunately it was just a weird transition era of how people consumed digital media. Cord Cutting was still kinda fringe, yet younger people weren't paying for TV, SmartTVs started becoming a thing for ordinary people. And hey here we are with the norm is that people just use whatever apps are built into their smart TV and call it done.",hardware,2026-01-06 15:39:19,2
AMD,ny25tyt,Microsoft has been too early a lot of times.,hardware,2026-01-06 19:15:32,1
AMD,nxziy7o,"Market always goes the way Microsoft predicts it. They're really good at predicting it, yet people blame Xbox One launch instead of a MF Phil Spencer who literally killed Xbox.",hardware,2026-01-06 10:49:48,0
AMD,nxz4aqw,The only reason I ever bought a console was to get a Bluray player. The fairly new PS3 cost almost half of the cheapest Bluray player at the time. I still use it... For Bluray.,hardware,2026-01-06 08:32:17,12
AMD,nxyvgfv,"You have a source for that.. is that data publicly available?  I mean, I can see it. Anecdotally, my Xbox has more hours of streaming content than gaming.. but I blame having a kid.",hardware,2026-01-06 07:10:26,9
AMD,nxyv48f,you can get an actual media box with similar features to a console's media features for the price of a single game.,hardware,2026-01-06 07:07:25,5
AMD,ny3txbe,good the planet is overpopulation anyway,hardware,2026-01-07 00:01:15,2
AMD,ny1wszs,I doubt they could ever put enough ads into their AI chat systems to pay for themselves. Consumer-facing AI tools are ads directed at investors.,hardware,2026-01-06 18:35:06,3
AMD,ny2qb8s,The products are used by businesses to make products for consumers. It’s not that complicated.,hardware,2026-01-06 20:50:01,3
AMD,nxyqmxi,"Welp, you sure know about CES better than CES themselves then. LOL",hardware,2026-01-06 06:29:11,-17
AMD,nxyuhc2,"intel is still an AI rival.  I see the investment in Intel to be more of a long term play for their fabs. Intel was on a massive discount at the $20 range, and despite sucking right now, still has some cool innovations in the works. Why not throw in some money and potentially secure some fab quotas in 10 years?  Stuff like how good Vera is would probably already be known internally by the time the investment was made. Vera rubin is already in production - which means they likely already have finalize designs with solid working engineering samples a few months ago.",hardware,2026-01-06 07:01:52,5
AMD,ny0npc3,"Some people on reddit are completely blind because of their hatred for AI. The thing is already used by hundreds of millions people around the world, whether you like it or not. It can still be a bubble while still being a generational technological advance, just like the internet once was.",hardware,2026-01-06 15:09:17,10
AMD,ny0q87a,"GPT, generative pretrained transformer",hardware,2026-01-06 15:21:30,6
AMD,ny4s4uy,"I've fucked around with local LLMs for a few years now, and I honestly haven't noticed a massive improvement. Maybe it's all in the ""above 120B"" space, and I just don't have the VRAM or patience to see it, but the newest models are just as brain dead and unreliable as the ones from a year ago, or even the year before that. They're still just gussied up stats models that don't do any actual thinking or even basic programmatic calculation, whatever the ""reasoning"" block might imply. And they're still just as likely, given five questions, to completely fuck up one of them, answer the opposite of what one asked, and just totally make up the answer on a third. Either that, or you have to dial the samplers so tight that you squeeze every drop of creativity out of it, and suddenly you're talking to Google Assistant. I can't imagine doing any sort of serious work with these things when they still oscillate so hard between catatonic and schizophrenic.  Example: Just the other day, I was brainstorming a scene in a short story I had an idea for. Two characters were married, and the wife was talking to her husband's brother about her husband. The third character made a reference to ""our parents,"" meaning his and his brother's parents. The damn thing just *could not* understand, without my explicitly stating it, that he didn't mean his and the wife's parents. It would get it right sometimes if I kept regenerating, as usual, but it really hung a lampshade on the fact that these things are just dumb math.  So, sure, the models are bigger, faster, more memory efficient, packing more into less. But more of what? The same slop, as far as I can tell. Fine for mindless entertainment or casual ""throwing shit against the wall,"" but not much more than that.",hardware,2026-01-07 03:04:07,1
AMD,ny02xze,gen AI is only part of LLMs and LLMs are only part of AI. Stuff like Reinforced Learning is not based on LLMs yet also requires large compute resources.,hardware,2026-01-06 13:17:14,3
AMD,nxyhfkt,"It's absolutely overvalued. There is no chance in hell the massive datacenter build-out can break even unless they run those GPUs wayy past their expected life. All of the buildout is done using heavy leverage. It's going to collapse.    You are correct that AI is here to stay. Even before LLMs it had plenty of consumer and industry use, e.g. machine vision. LLMs are excellent at writing BS boilerplate code. However, it is incredibly unlikely that they can monetize it even close to breaking even on the implementation cost.",hardware,2026-01-06 05:17:40,45
AMD,nxyfc39,"i'm talking about consumer genAI and call center etc, scientific ML is an entirely different thing.  nobody asked for copilot or any of this nonsense, its investors and ceos circlejerking and nothing more  homie they got you running defence on technology that's destroying rural communities damaging the ecology and screwing over creatives, reflect on why you do this.",hardware,2026-01-06 05:02:50,6
AMD,nxylojb,So we have lost the companies so much that none of them even talk about releasing their own things in their keynote? o7,hardware,2026-01-06 05:49:20,4
AMD,nxzr6fm,"I think bundling the Kinect, which made the price $100 more than PS4 was the main reason it failed, and that the always online requirement being an issue was really overstated",hardware,2026-01-06 11:57:14,13
AMD,nxzj4an,"Buddy. I recommend you tuning down ""for a lot of people"" part, because your personal echo chamber =/= a lot of people.  If ""a lot of people"" part was right... THEY WOULD ALREADY ROOL IT BACK! Like literally no one outside of reddit's micro echo chambers give a fuck about ""always online part"", otherwise consoles wouldn't be selling at all, and you would even see that, but most of the lower sales are due to other stuff like Orange Ass.",hardware,2026-01-06 10:51:17,-9
AMD,ny020g7,"I’d say it was more that most people weren’t streaming everything yet, and yet at the same time cable was dying. Just a weird in between era in hindsight. The people who were already setup with 360s everywhere in their house as CableCard DVR players and streaming devices and such were a minority of adult tech geeks. Keep in mind this was before FireTVs and Chromecast even existed too. You’re talking like original Rokus still in use and 2nd gen AppleTVs still in use. Tons of people used Netflix on their PS3 or Xbox 360 as likely a primary “best” method at the time, but realistically the future prevalence of SmartTVs with built in apps killed the need for most consumers, along with physical media of movies/shows and cable TV.   That being said, I imagine a ton of people use their Xbox or PlayStation to watch Netflix today as well, it’s just not a reason you buy one.",hardware,2026-01-06 13:11:39,7
AMD,ny0256u,Not at the time Xbone released you couldnt.,hardware,2026-01-06 13:12:28,2
AMD,ny02ci6,Wouldnt be the first time an event had no clue what their target audience actually was.,hardware,2026-01-06 13:13:42,1
AMD,nxyvr5u,Intel isn't much of a ai rival but they have lot of fab capabilities and with fab space being fully utilized it makes sense to invest in a company who happens to own and run a lot of fabs even if Intel killed off all of their own products they would be fine,hardware,2026-01-06 07:13:06,2
AMD,nxzuol6,> I see the investment in Intel to be more of a long term play for their fabs.  Dead on.   Demand for packaging is massive.   14A with high NA is very promising.  Screaming about products when signing the investment is all well and good but there's a lot more to that story.,hardware,2026-01-06 12:23:15,1
AMD,ny2yw2t,The real question is if it can be monetized. That's the real hurdle they need to get over. Currently all AI companies are burning money trying to estabilish a foothold. They are running on VC money. Question is if they can start being profitable.,hardware,2026-01-06 21:29:02,9
AMD,ny168t3,"Hundreds of millions of people smoke cigarettes too and I wouldn't say they're exactly a good thing.  I'm not one to say where things will end up or how much the good parts of AI will outweigh the bad, but user counts are not the way to measure it.",hardware,2026-01-06 16:35:37,11
AMD,ny2zc0o,"ML isn't really new though. The current AI wave is all gen AI and LLM bullshit, nothing to do with any of the ML models that have been estabilished for years.",hardware,2026-01-06 21:31:05,3
AMD,nxyk5kj,"The companies that are actually running the models have a cost of revenue that is multiples of their current incomes, and that hasn't been getting better over time because the inference demands of frontier models has been increasing on par with or faster than the rate that cost per token of inference has dropped with successive hardware generations (think 'reasoning' models that repeatedly reprocess their initial outputs as an example).  Every single one of the firms in the space are completely dependent on successive waves of VC cash infusions extending their runways, allowing them to defy gravity Wile. E. Coyote-style for *just* a little longer, and press coverage has started to turn in a way that makes me think that maybe, just maybe, VCs are having second thoughts and Wile E. Coyote is starting to look down and realize he's been running in thin air for a while now.  I'm an AI skeptic, but in the way that smart money was a Web 1.0 skeptic in 1999.  AI is still going to exist after the bubble pops, but it's going to be smaller, less expensive, and probably focused on what can run on local hardware to do small useful tasks and extend the capabilities of robotics, instead of piling up mountains of GPUs in attempt to create God from plagiarism.  We just have to hope that there's some sort of containment when the bust happens, so the entire economy doesn't just screech to a halt.  (On that front I'm not particularly optimistic, but what have we got other than hope at this point?)",hardware,2026-01-06 05:37:45,26
AMD,ny03k06,">There is no chance in hell the massive datacenter build-out can break even unless they run those GPUs wayy past their expected life.  GPUs are just half of the cost for datacenters. You can replace GPUs with new models and still use the same building for next 50 years. Also GPUs have never had as long a life in datacenters as today. 9 year old datacenter GPUs that would normally be all over ebay and homelabs are still at full utilization in datacenters.  > All of the buildout is done using heavy leverage.   No. very little of it is. The largest players - google, microsoft, Meta - are building it with their own money. Only OpenAI is leveraging it (and Oracle but who cares about Oracle).",hardware,2026-01-06 13:20:49,2
AMD,ny2g7nb,"RemindMe! 5 years  Has every AI datacenter failed to make a profit?  > There is no chance in hell the massive datacenter build-out can break even unless they run those GPUs wayy past their expected life.  All evidence points to the contrary, today there are literally thousands of ML applications that companies pay for cloud hosting, and essentially every industry expert expects that to continue into the future.  ML is going to revolutionize every area of science and medical research, similar to how the computer did in the 1970s and 80s.  Will be fun to see if /u/CatsAndCapybaras is correct though in a few years, and many others here who are making equally silly predictions.",hardware,2026-01-06 20:03:03,1
AMD,nxyklwq,">There is no chance in hell the massive datacenter build-out can break even unless they run those GPUs wayy past their expected life  It'll be many years before anybody turns profit. That's not the point right now: The race is to be one of the few established players in the AI space once the New Market Life Cycle reaches a consolidation phase.  There's a huge opportunity cost to not participating in the race.   Besides, once the hardware needs to be replaced, they're still left with the model(s) that hardware trained.",hardware,2026-01-06 05:41:13,3
AMD,nxygu5k,">consumer genAI and call center etc  Why do you think that the market value of ai is riding on callcenters? Everything that amd showed on stage are industrial. Health, space, robotics, low latency video translation and summary, image to 3d model generation in minutes, simulations, proactive os for productivity, video gen ai for movies and simulations, script conceptualization etc  Nothing they talked about were chatbots.",hardware,2026-01-06 05:13:26,10
AMD,nxygdqd,"People are reaching for it as soon as any kind of tricky health issue, big purchase or complicated project hits their lives.  My 75 yr old parents now use Gemini all the time.  Not everyone is a 20 - 30 yr old gamer with minimal cares in the world.",hardware,2026-01-06 05:10:09,4
AMD,nxyh43r,">i'm talking about consumer genAI and call center etc     With the amount of improvement both of those things saw in the last 2 - 3 years, they're also not going away lol  >nobody asked for copilot or any of this nonsense  irrelevant. Consumers don't ""ask"" for large paradigm shifts because they don't have the frame of reference of what to even ask for.  >homie they got you running defence on technology  I think you're confusing ""running defence"" for just having a realistic assessment of the situation. Your hope that AI will just all go away and things will go back to how they were before just isn't gonna happen. It'll eventually get less press coverage and massive VC investment as the tech matures, the amount of players in the market consolidate, etc. But it's certainly here to stay",hardware,2026-01-06 05:15:24,5
AMD,nxyhsxu,"> homie they got you running defence on technology that's destroying rural communities damaging the ecology and screwing over creatives  You're talking about energy use, not ai use. If energy is your problem then why would you support gaming at all? Man sits in front of a pc interacting with pixels powered by his 5090 sapping 400+w, 0 productivity output  Have you ever thought about all that power wasted when you turn on mh wilds and your pc goes brrrrr? How did that help the ecology? Did that help rural communities?",hardware,2026-01-06 05:20:22,5
AMD,ny2jyqn,"> i'm talking about consumer genAI and call center etc, scientific ML is an entirely different thing.  ML is really not entirely different.  LLMs are just low hanging fruit to experiment with the technology.  It reminds me of when boomers in the 80s and 90s declared that ""The Internet was Just Chat Rooms""  LOL  LLMs are ML what Go Fish is to card games.",hardware,2026-01-06 20:20:30,1
AMD,ny1sndw,"Who cares about it anyway? 3% more performance for $50-100 more, while discontinuing the 9800X3D. It's just milking the consumers.",hardware,2026-01-06 18:16:31,5
AMD,ny0i55f,"That and a lack of exclusives. The PS3 launch was nearly as disastrous, but they recovered because they had games people wanted to play.",hardware,2026-01-06 14:41:11,4
AMD,nxzvj2c,"You okay there, bud?",hardware,2026-01-06 12:29:11,3
AMD,ny0mty9,"Anecdotal, but my dad actually bought two Xbox Ones specifically to use as multimedia devices. I don't think they ever played a game, but he really liked the cable passthrough feature, bluray, and all the streaming services. Used Xbox One + Logitech Harmony for many years",hardware,2026-01-06 15:04:57,1
AMD,ny1fz4x,"The Chromecast was released in 2013 and cost $35, the same year the XBox One was released and cost $500.",hardware,2026-01-06 17:19:44,3
AMD,ny15x9z,"But it would have been the first time an adult gamer, with low disposable income, in this sub recognizing being mistaken. Alas...",hardware,2026-01-06 16:34:07,5
AMD,ny43qhx,"Did you read the earlier comments in this comment chain? Whole discussion is around whether AI will go away and whether it has users. It absolutely has users, and the amount of users is only increasing, completely different from NFTs.  Whether AI is good or not is a different question. The tobacco comparison is irrelevant when we have another world changing innovation in information technology to compare with. Most of the arguments surrounding “AI bad” can be copied to claim that the Internet is bad too. People create and publish massive amounts of low quality contents, spread misinformation, and propagate hatred through the Internet, so would you say the Internet is a bad technology?",hardware,2026-01-07 00:51:51,1
AMD,ny0i2bn,"What a reasonable and good take. The Dotcom bubble killed many companies, but also left giants like Amazon and Google.",hardware,2026-01-06 14:40:46,3
AMD,ny1g13a,This right here,hardware,2026-01-06 17:19:59,0
AMD,ny2i4uw,"> We just have to hope that there's some sort of containment when the bust happens, so the entire economy doesn't just screech to a halt.  This is a very silly premise.  The US's annual GDP was $29.2T, and total capital investment is projected to grow to $0.5T in 2026.  If it does, said spending will begin to approach 2% of GDP.  2% for a non essential good or service that didn't exist prior to 2024 can not cause ""the entire economy to screech to a halt"", whatever that means.",hardware,2026-01-06 20:12:00,0
AMD,ny2jixc,"> It'll be many years before anybody turns profit.  Google's TPUs in Google Cloud are already profitable, FWIW.    AWS EC2 is also already profitable.",hardware,2026-01-06 20:18:27,1
AMD,nxzb7hv,meanwhile copilot is useless in my OS. I cannot even hand off from my keyboard.  it is not like my computer will know when to move to next webpage or open the next app I wanted after I done with another App.  if an 5090 can autopilot my daily routine on how I use my computer I am so buying that GPU.,hardware,2026-01-06 09:39:04,7
AMD,nxyhnp5,"yeah, but Gemini is losing money. They would have to charge a hell of a lot more in order to even come close to meeting market valuations.",hardware,2026-01-06 05:19:18,0
AMD,nxykifm,"We never asked them to take away headphone jacks from phones, but they did it anyways",hardware,2026-01-06 05:40:28,5
AMD,nxylpl7,lol as if consumer power usage is anywhere close to comparable to how much energy even just model training uses.  You’re telling people to start biking while turning a blind eye to the guy with a private jet.,hardware,2026-01-06 05:49:33,13
AMD,ny3p3fi,Why would they discontinue the 9800x3d? You think they're just going to toss the dies that don't bin well enough in the trash?,hardware,2026-01-06 23:36:02,1
AMD,ny269yl,yeah it wasn't like the PS2 and PS3 release where they were really cheap even just as DVD/Bluray players.,hardware,2026-01-06 19:17:35,2
AMD,ny2imtx,"Yep, we'll be fine.  AI investment is still less than 1% of US GDP in 2025, and in 2026 Goldman Sachs projects that it could potentially reach 1.5%.",hardware,2026-01-06 20:14:18,0
AMD,ny2rgwz,"I don't think it is, and I think you're underestimating the impact that amount of capital loss can have on the market -- the 2008 financial crisis was triggered by only about $1T in toxic assets, after all the dust settled.  As you say, AI infrastructure investment is already reaching half that number *per year,* and all those data centers full of AI accelerators are of very limited utility if there's suddenly not billions of VC dollars surging into generative AI development.  Meanwhile 80% of the stock market's growth in the last year was driven by speculation into AI firms, adding up to several trillions of dollars at risk in a AI-driven market panic.",hardware,2026-01-06 20:55:17,2
AMD,ny2khql,It took 7 years for AWS to become profitable.   Is Gemini profitable yet?,hardware,2026-01-06 20:22:59,2
AMD,nxzkhar,> meanwhile copilot is useless in my OS. I cannot even hand off from my keyboard  Tbh Microsoft is exceptionally useless at making their stuff work across multiple programs. Downside of being a huge company where each program/feature has their own huge dev team that fights for resources against the other teams. Cross compatibility might mean that your team gets cut in the next round of layoffs.,hardware,2026-01-06 11:02:58,2
AMD,nxykg6l,Google is willing to use excess cash as a loss leader until they can make it profitable.,hardware,2026-01-06 05:40:00,7
AMD,nxykosr,So it's a new technology?,hardware,2026-01-06 05:41:49,3
AMD,nxyiis0,The cost of inference has been going down multiple X per year.  That has been counter balanced by people making the models use a lot more tokens and training bigger models.  At some point the model will be good enough.... and one year after that Google will be making eye watering profits off AI.,hardware,2026-01-06 05:25:34,0
AMD,nxyl5d4,"There was a period of time where some phones that still kept their headphone jacks advertised that feature, like LG and Sony. Nobody cared. It didn't shift sales at all.  Most people wanted or already owned wireless earbuds within a year or two of that happening and didn't care that the jack was gone",hardware,2026-01-06 05:45:18,6
AMD,nxyp4l0,"> lol as if consumer power usage is anywhere close to comparable to how much energy even just model training uses  That's true but it ain't gonna be that way forever. Training is yielding diminished improvements so the race to be the 1st with the best model will slow down with players dropping outta the game soon enough. Most of these companies are disregarding power use because they're racing, that ain't gonna last forever and as you can see they are now turning to inference to serve customers and with that comes emphasis on tco and efficiency  Ai is gonna move to the more efficient part of the curve soon.",hardware,2026-01-06 06:16:42,-3
AMD,ny31n8k,"What was dotcom investment back in 1999? It doesn't have to be a massive slice of the pie to cause big problems. 10 stocks acxount for over 40% of the S&P500, something last seen in 1980. It also seems that AI companies are responsible for an outsized amount of the GDP growth, so if that was to collapse we could see a real big drop in GDP.",hardware,2026-01-06 21:41:47,2
AMD,ny2thgi,"> the 2008 financial crisis was triggered by only about $1T in toxic assets  Yep, and it was a problem because of who owned those assets.  Homeowners.  The difference today is that you and I don't have to care if investors lose their shirt investing in non-viable LLM ventures.  In fact, you and I benefit if they fail, because it means more infrastructure for running other more viable ML products and services in the mean time.  Think of it like this.  Imagine billionaires were investing in 8 lane highways everywhere, because they had these weird vehicles that needed 6 lanes.  Suddenly, the 6 lane wide vehicles are not economically viable.  You and I still get to use the infrastructure they built, and now at below market prices.  An imperfect analogy but I think it's enough to get the gist.  > As you say, AI infrastructure investment is already reaching half that number per year  In 2026 yes, and that's a projection that the spending is going to increase 10x.  [In 2025 it was only $61B](https://www.cnbc.com/2025/12/19/data-center-deals-hit-record-amid-ai-funding-concerns-grip-investors.html).  :)   > data centers full of AI accelerators are of very limited utility if there's suddenly not billions of VC dollars surging into generative AI development.  We disagree here.  LLMs are to ML what Go Fish is to card games.  It's just the beginning, and it's oh so trivial compared to what's coming.  > Meanwhile 80% of the stock market's growth in the last year was driven by speculation into AI firms, adding up to several trillions of dollars at risk in a AI-driven market panic.  Oh no, you mean the stock market might recede back to 2024 levels?  What ever shall we do?  :)   We'll be fine is the answer.  :)",hardware,2026-01-06 21:04:28,-1
AMD,ny2o23a,"Gemini is a product that runs on top of Google's datacenters.  My point is that Google's datacenters absolutely already turn a profit, and GCP accounted for 11% of Alphabets total profits in 2025.",hardware,2026-01-06 20:39:40,1
AMD,ny144l1,It's especially mind boggling since they have access to the entire Open AI IP.,hardware,2026-01-06 16:25:54,2
AMD,ny1t1l5,Exactly. I just don't see a path to profit. The amount of money they will have to force out of users will be more than they care to hand over.,hardware,2026-01-06 18:18:14,1
AMD,ny35uyw,> What was dotcom investment back in 1999?  It peaked at around 4.9% of GDP according to the BEA in Q4 of 2000.    https://www.frbsf.org/wp-content/uploads/er19-34bk.pdf,hardware,2026-01-06 22:01:07,2
AMD,ny2r8qx,"Yeah, but *Gemini* loses money. And Azure is profitable, despite CoPilot losing money too.   My point was that everybody is in the 'gain marketshare at a financial loss' phase of AI, which is a phase every new tech market went through, and is nothing unusual.   Big, established players like Google, Amazon, MS, etc. Can leverage their existing profitability to just eat the losses for a long time. The smaller start ups, like OpenAI, dont have a near 'infinite' runway like the big players, but they still have many years left in that runway before they have to worry about profitability.",hardware,2026-01-06 20:54:16,3
AMD,ny2kev5,"> Exactly. I just don't see a path to profit. The amount of money they will have to force out of users will be more than they care to hand over.  So this is just not ever how any technology has ever gone, ever.  The thing you are probably missing is that the cost of the technology itself, and the cost to power it, dramatically decrease over time.  Just as Orville and Wilbur's first plane wasn't market viable for commercial flight, that didn't mean that commercial flight wouldn't be viable.",hardware,2026-01-06 20:22:36,1
AMD,nwwuws4,"> According to the author, you can replicate the AMD B650 Southbridge Expansion Card for approximately 300 yuan (about $42.88). Although the expansion card is not commercially available, OSHWHub has integrated manufacturing services through its sister company, JLCPCB. It allows you to order custom hardware directly using the design files shared on OSHWHub, and JLCPCB will manufacture it.  The important bit, for anyone who'd want to buy one.",hardware,2025-12-31 13:42:23,134
AMD,nwxbyee,Gotta love chinese folks and their freinkenstein stuff. From old HEDT platforms with hacked up consumer chipsets and cheap motherboards as a result (X99/2011-3 being the most famous one) to laptop cpus hacked together to fit into a desktop motherboard and now this.  Maybe next they can start producing cheaper DDR5 sticks from harvested dram ics. That's be cool.,hardware,2025-12-31 15:18:41,62
AMD,nwwqxim,"[The guy also made a video talking about it on bilibili, only in Chinese, but some screenshots might be interesting.](https://b23.tv/BV1AXv4BvE4T)",hardware,2025-12-31 13:17:03,50
AMD,nwwqvuz,Yea. PCI Express has been the standard to connect southbridge chips for a long time. They act like PCIe switches too,hardware,2025-12-31 13:16:45,37
AMD,nwzh34r,"Am I crazy, or is the *outcome* that you basically just have a PCIe switch with a few SATA, M.2, and USB ports attached to it? I guess it's handy in that it bunches a *bunch* of stuff together into a very small PCIe slot. It's PCIe-economical lol  *Very* cool in its own right, but usually you'd just... get a PCIe card for whichever of these you need, right?",hardware,2025-12-31 21:55:35,10
AMD,nwynznm,"Talking by experience, the really low budget boards will NOT run this for a tiny, simple fact.  Most don't have a pcie x4 slot.  Most cheap and budget boards have one x16 pcie slot for your gpu and - at most - two x1 pcie slots  So those a520, a320, even some b350 don't have pcie x4 slots.  Just pay attention to this, folks.",hardware,2025-12-31 19:18:20,8
AMD,nwx4gk5,Very cool but why gatekeep firmware behind a chat group signup?,hardware,2025-12-31 14:38:30,29
AMD,nwwvjf0,I wish this was an option a few days ago. Just ordered a pcie sata card to make up for the lost ports going from a 470x to b850 lost me recently.,hardware,2025-12-31 13:46:15,3
AMD,nwws6ip,Is there a limit to how many times this can be daisy chained?,hardware,2025-12-31 13:25:15,7
AMD,nx10h59,That's actually pretty genius.,hardware,2026-01-01 03:42:46,3
AMD,nwyilo3,"I wonder if this would work on older platforms. It says it requires PCIe 4.0 x4, but shouldn't it just ""downclock"" on older standards since they're backwards compatible? Or does the Promontory chipset just flat out expect 4.0 and shit the bed otherwise?   I have an old Z170 build that isn't worth the effort to sell, and is perfectly usable for what I'm using it for (sitting on my workbench, mostly showing datasheets). I'm stuck on PCIe 3.0, with just one precious M.2 slot. I'd love to expand the I/O and M.2 capabilities, even if the M.2 slots won't run at PCIe 4.0 speeds (like I could give a shit about that).",hardware,2025-12-31 18:50:53,2
AMD,nwzum5e,"I wonder if this would work on a x670? The article specifically mentions b650 boards (lacking a chipset to start with), but could you you drop this into an x670 or x870 to build a small storage server?",hardware,2025-12-31 23:14:14,2
AMD,nwxkx9b,"I would love something like this if it could give me pcie 2 3x8 slots.  Omygosh think of the expansion...  you could have a high speed gpu, high speed nic(25gig+) and hba!",hardware,2025-12-31 16:03:41,1
AMD,nwzbe85,on this - I wonder whats the major differences between this and a pcie switch/bifurcation card? beyond my simplistic understanding of the two,hardware,2025-12-31 21:24:24,1
AMD,nx2hv0j,This is the kind of cool shit AIBs should be making,hardware,2026-01-01 12:06:26,1
AMD,nxtbia2,So this is basically just doing what motherboar manufactuers should have included already - giving more lanes.,hardware,2026-01-05 13:31:54,1
AMD,nwx6x02,someone must have forgotten how daisy chaining's bandwidth actually works,hardware,2025-12-31 14:51:55,2
AMD,nwwqipg,"Yeah but I'd like to buy one that works, easily.  Please?",hardware,2025-12-31 13:14:22,-2
AMD,nwx3gqa,You get 1 more nvme. lol,hardware,2025-12-31 14:32:46,-7
AMD,nwwpn52,"Hello narwi! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-31 13:08:32,0
AMD,nwx0hq3,"> Schematics, a parts list, and comprehensive instructions for building the AMD B650 Southbridge Expansion Card are available at no cost. The author notes that you must flash the card with a special firmware for proper functionality. While the firmware is not publicly hosted, you can obtain it for free by joining the creator's QQ group.  Yea this is awesome",hardware,2025-12-31 14:15:42,69
AMD,nwxab8t,"The key is that jlpcb often has minimum orders.   When you tally it all up it might be around $42 dollars per board, but you'll need to order 5 boards and with assembly costs, shipping, etc you'll be into a few hundred dollars.  That's not to say it's not a reasonable proposition, you just need to get together a few people to make it worthwhile.",hardware,2025-12-31 15:10:08,35
AMD,nwyeab4,"They are already recycling ICs to produce ""new"" super cheap out of production stuff like DDR3 sticks. Before the RAM shortage you could even find 8GB DDR3 stick for only 5 buck on Aliexpress which is a pretty crazy deal for countries without a good used market.  Quality and compatibility between sticks can be a bit hit and miss though but it's nothing a TestMem cannot solve.",hardware,2025-12-31 18:28:53,24
AMD,nwzp1j3,My favourite hacked up Chinese things were the Nvidia 30 series mobile GPUs on PCie cards. They even put the 3080M 16GB on a PCIe card.  Nvidia shut that down ever since then though :(,hardware,2025-12-31 22:41:02,12
AMD,nwycvvl,>Maybe they can start producing DDR5 from harvested ICs  They already do. Hell you can buy your own blank DDR5 PCBs and make your own.   Eg: m[.]tb[.]cn/h.76Qe1PW,hardware,2025-12-31 18:21:47,15
AMD,nx3o3yq,Restricting tech items to China forces them to get creative. The US AI policies towards China will likely backfire. They're going to learn to optimize and get results with less.,hardware,2026-01-01 16:47:11,1
AMD,nwx4k98,It's surprising the article didn't report on the video you linked. Simply having AMD's southbridge running on an Intel B660 was mad.,hardware,2025-12-31 14:39:05,18
AMD,nwwxluy,thanks,hardware,2025-12-31 13:58:41,1
AMD,nwx4ad4,"Intel's version, DMI, has some extra proprietary bits glued on even though it's basically PCIe at its core. So this is relatively novel in the modern era.",hardware,2025-12-31 14:37:31,19
AMD,nx371f4,"a pcie4 or 5 switch from PLX costs hundreds of dollars, the old 3.0 stuff is also 2-3x the price it was before 2021, these things cost only the price of a bunch of old recycled motherboards.  ultimately this is just the end result of egregious price gouging.",hardware,2026-01-01 15:13:40,4
AMD,nwzyhxx,"I'm thinking this might be a higher-end, feature-dense, luxury option for people who buy those ~4x NVMe PCIe cards. With this option, you could add a bunch of SATA ports and some additional rear panel io- maybe a soundblaster card?",hardware,2025-12-31 23:37:40,5
AMD,nx0im84,Somehow it also turns one PCIe 4.0 x4 expansion slot - into two PCIe 4.0 x4 expansion slots.,hardware,2026-01-01 01:42:52,1
AMD,nx0x11s,"Finding a 4 or 8x slot isn't too bad on some boards, but if they are only PCI-E 3.0 that's not a lot of bandwidth for all these devices.  Sure, maybe you aren't pushing the USB bandwidth at the same time as an NVME drive, but you might.",hardware,2026-01-01 03:19:20,2
AMD,nwxom2r,Probably open distribution would attract lawsuits from AMD or something.,hardware,2025-12-31 16:21:54,28
AMD,nwy0ndi,The firmware was made by another person,hardware,2025-12-31 17:21:46,9
AMD,nwyrt6f,firmwares are still often close sourced and not open source it's a form of control over technology. and results in stuff like not sharing firmware.,hardware,2025-12-31 19:38:32,10
AMD,nx0ppcw,"Same shit with Discord and it happens a ton, especially with software that might be on shaky legal ground.",hardware,2026-01-01 02:29:28,3
AMD,nwy598a,This probably uses way more power than a standalone SATA controller and card.,hardware,2025-12-31 17:44:32,3
AMD,nwzvm2t,its not commercially available. you have to purchase the board in (small) bulk and possibly solder things,hardware,2025-12-31 23:20:19,3
AMD,nwwtip1,"if you have more than one x4 slot, you should be able to use multiple, depending on software supprt",hardware,2025-12-31 13:33:44,4
AMD,nwyq0f3,"The chipset in this AIC will handle the request from the chipset on the board and send the corresponding reply. Nothing else.\ It won't turn your Z170 into an AMD board.  This should work without a problem. It uses the AMD chipset, which means it won't be cheap, as if you use a Innogrit or ULS ICs for example.\ The AMD chipset wil provide more SATA and M2 ports, as well handling the speeds and connections better.",hardware,2025-12-31 19:28:59,2
AMD,nwzjyql,"I'm guessing old chipsets are much cheaper & more available than switch cards, and your mobo might not support bifurcation.",hardware,2025-12-31 22:11:56,8
AMD,nx2hz7w,"From a technical point of view, there is no difference. The Promontory 21 chip literally is a PCIe switch and a bunch of SATA/USB controllers. (AFAIK it also contains a tiny embedded CPU for the flashback function.)",hardware,2026-01-01 12:07:32,3
AMD,nwzneyc,"Yes, this shares all x4.. so if you have two nvmes, and access each on, get full x4 speed. If you access both at the same time, they get x2.   If it's bifucfcated, you only ever get half.  You also get more stuff, usb sata, etc.",hardware,2025-12-31 22:31:46,2
AMD,nwws2wl,yeah. maybe a bunch of people should get together and do a group order to have these be made and flashed.,hardware,2025-12-31 13:24:36,13
AMD,nwws80z,Until you get volume that isn’t happening unfortunately.,hardware,2025-12-31 13:25:31,7
AMD,nwx6mvw,Try to extrapolate. Try to see potential. Try to see something more than just the obvious,hardware,2025-12-31 14:50:25,2
AMD,nwwrhoz,or someone want to trade X4 slot for more IO,hardware,2025-12-31 13:20:45,7
AMD,nwwrkp5,Sometimes your requirements change over time.,hardware,2025-12-31 13:21:18,5
AMD,nwz634b,All the cool shit is in China,hardware,2025-12-31 20:55:43,39
AMD,nwya65u,"Or, just get the X series boards, get all the features and not have to do it through hardware that you hope works and ends up costing basically the same.",hardware,2025-12-31 18:08:16,21
AMD,nx033rs,"Give it a few months, they'll probably appear on Taobao, then Alibaba, then AliExpress, then eBay at a significant markup.",hardware,2026-01-01 00:05:43,11
AMD,nx1q4on,"Huh I just ordered from them for the first time ever a small custom board so I could create my own air quality device, and it did default me to 5 boards, but it was literally just over $5 for the order with no rush shipping, super cheap for a high quality board.",hardware,2026-01-01 07:15:58,3
AMD,nwz94ew,"Right before the price hike i got a brand new kllisre brand 1x16gb ddr4 stick for eq. of 15 usd, wish i got more lol",hardware,2025-12-31 21:12:09,3
AMD,nxerg1q,I got chinese brand DDR4 LRDIMM 64GB 3200Mhz JEDEC for like around $60 per dimm before the price hike. It's working fine on EPYC 7153 on ASRock Rack mainboard too. Wish I could get more but they are all out of stock now. :(,hardware,2026-01-03 09:07:36,1
AMD,nwyeao1,that's cool. Now you just need to find a source to harvest dram ics from.,hardware,2025-12-31 18:28:56,4
AMD,nx0b2wy,> Hell you can buy your own blank DDR5 PCBs and make your own.   Yes because the empty PCBs are the more expensive part of a stick :P,hardware,2026-01-01 00:54:31,2
AMD,nx2mnnc,pcie switches are pretty standard for ages.,hardware,2026-01-01 12:50:23,5
AMD,nx1eve7,"would need to check all the PCI-E versions of each interface up to the slot that it goes into on the motherboard, ultimately the maximum throughput would be limited by the link between that chipset and the slot on the motherboard it plugs into and all the devices going through the add-in card would have to share that bandwidth.",hardware,2026-01-01 05:33:08,2
AMD,nx08gpl,I'd say it's more likely that the creator just wants to get people to join their group so they can get feedback about whether the board worked and any problems they may have had and solved.,hardware,2026-01-01 00:38:24,10
AMD,nwzc1xj,IP Lawsuits in China? Against IP of a western company? Has anyone heard of such a thing?,hardware,2025-12-31 21:27:55,-3
AMD,nx1wdcl,It was reverse-engineered and dumped from existing BIOS.,hardware,2026-01-01 08:20:26,3
AMD,nwyeos6,"I don't mind much about power usage if it opens up more options. I only have the one pcie1 slot, and finding a board with one and the right number of 3.5 mm jacks for my old 7.1 amp was tricky.",hardware,2025-12-31 18:30:56,8
AMD,nxu466k,ASMedia PROM21 by itself uses ~7W under full load,hardware,2026-01-05 16:02:04,1
AMD,nwxx3t6,Which wouldn't be daisy chained but parallel.  You would need a M2 to PCIe slot adapter to daisy chain,hardware,2025-12-31 17:03:56,6
AMD,nwzq8qx,certainly - the plx/pex cards are quite expensive relative to this but do support 8x/16x too,hardware,2025-12-31 22:47:51,1
AMD,nwzqqbh,yes I'm interested from a pcie perspective for mi50 clustering... I guess the positive of the plx/pex line of switches is 8x/16x,hardware,2025-12-31 22:50:39,1
AMD,nx04t9j,count me in,hardware,2026-01-01 00:16:15,2
AMD,nwx32qo,You can order them on JLCPCB,hardware,2025-12-31 14:30:32,3
AMD,nwzx0pd,The West does very little for industrial policy like China does.,hardware,2025-12-31 23:28:53,11
AMD,nxei7s0,Can you use an X series motherboard on an Intel CPU? A Threadripper?,hardware,2026-01-03 07:47:29,3
AMD,nx11f3r,A significant markup would defeat the purpose since most would just opt to buy a higher end board.,hardware,2026-01-01 03:48:56,2
AMD,nx30kd3,"For bare boards they're great and super cheap. I'm talking about boards with parts and assembly, especially if you have more specialized parts that use the extended part list and/or can't use the fully automated assembly. Even then, it's not unreasonable pricing, it just adds up.",hardware,2026-01-01 14:32:42,2
AMD,nx1gegt,Typically SoDIMM sticks from what I've seen,hardware,2026-01-01 05:46:08,3
AMD,nx0284f,They are starting to fire up the patent engines pretty hard the last year or two. The US has been pushing for them to take patents more seriously for decades now but I suspect the result won't be what was expected.,hardware,2026-01-01 00:00:11,7
AMD,nx0cydk,This 'meme' is becoming standard reddit ignorance and racism. Companies local and foreign in China have been suing each other for IP infringement for decades.,hardware,2026-01-01 01:06:23,9
AMD,nwzh7my,"And that's the question ha, imagine these daisy-chained 10x deep in some monstrosity  I'm sure there's a limit",hardware,2025-12-31 21:56:17,8
AMD,nwzy4y3,"It would be pretty sweet to have a bifurcated x16 version of this with 4 chipsets on it. 8 m.2 and 16 sata, it's a whole nas on one PCIe card",hardware,2025-12-31 23:35:32,3
AMD,nwzv052,"PLX switches can be reconfigured to split all of the outputs down to x1 if you want, it's just a matter of configuration. One of the cards I have has DIP switches to set what mode it's in. I wondered how this worked for a PLX88048 because it doesn't support setting these from GPIO lines any more (unlike the earlier ones) - but looking at the board it's just got three EEPROM chips and you're toggling chip select on them. It of course still worked fine when I rewrote that config on one of the EEPROMs to have the outputs as x4/x4/x8/x16 rather than all x8.  I do have 8x MI50 hooked up to a LGA2066 board with two PLX8749 switches, running x8 to each. However I had some fun because the PCIe slot breakout boards I bought had the pinout mirrored - so I had to cut and splice the cables to move PERST and REFCLK to the other side of the connector (PCIe lanes being backwards is actually fine, the PLX chip will detect this during enumeration and adjust accordingly).  Another handy trick is with a PLX88048 card in a PCIe 3.0 x16 slot, if you connect PCIe 4.0 SSDs to it, it will talk PCIe 4.0 x4 to the SSDs and 3.0 x16 to your motherboard, so you can use them at full speed.",hardware,2025-12-31 23:16:38,3
AMD,nx04n0i,"I also have a MI50 machine with 9xMi50 32Gbs..   Im currently just using an eypc setup, but switching over to a PEX88080 switch board with 4x PCIE 4.0 x 16 as half my GPUs currently are just on x4 slots making much slower for those for transfers. This way I get my NVME/SAS back and my GPUs all sit on x16 electrical slots. I can then also use inifiniband to connect machines and see if I can get RDMA GPU to IB transfers working.",hardware,2026-01-01 00:15:11,2
AMD,nx0ckta,"The west had strong industrial policy in the past, but later taught to hate it irrationally.",hardware,2026-01-01 01:03:57,10
AMD,nx1gz99,"The ""significant markup"" will just be if you buy on eBay. The other three won't be much more expensive than Taobao, most likely.  Also ""just buy a higher end board"" is good advice if you don't have a motherboard yet. If you have an existing board it's cheaper than buying a whole new one.",hardware,2026-01-01 05:51:00,11
AMD,nxapdcx,Ah it’s not a meme it’s reality that China has a state policy to go after IP and doesn’t respect IP laws like western countries do. This is why people invest less in the Chinese economy and why some companies are not moving some operations to China.,hardware,2026-01-02 18:33:42,0
AMD,nwzx5xh,yes particularly on that latter part for my MI50 build - they're capable of PCIe 4.0 but using on an x399 board with PCIe 3.0 - which I believe would allow them to talk p2p over 4.0 but to the cpu with 3.0.  I'm just trying to find ways to keep down on cost as much as possible atm  EDIT: your build sounds super familiar - are you on the gfx906 discord?,hardware,2025-12-31 23:29:45,1
AMD,nx6t8j6,"Well our industrial policy was working in the mines, steel mills and in factories with often poor working conditions (at least in the UK). Obviously not that pleasant, so we moved into the service sector.   However we did that before the big rise in electronics which are probably nicer industries to work in. By being early and moving on, we missed the boat for things like this.",hardware,2026-01-02 02:55:50,3
AMD,nx00cqu,"> are you on the gfx906 discord?  Yeah I am, that'd be why.",hardware,2025-12-31 23:48:54,1
AMD,nx01v8c,that makes sense - we literally just had a similar conversation this week on the PLX/PEX offerings. small world!,hardware,2025-12-31 23:58:01,1
AMD,nvehqj2,Has AMD made any RDNA 4 GPUs for laptop?,hardware,2025-12-22 17:36:36,253
AMD,nveicyq,"AMD has to actually ship product (and support it) for OEM to use them.  There are other considerations, yes, but multiple manufacturers have publicly stated that AMD just doesn’t have enough available for them to warrant making more than a few models.  Nvidia on the other hand has more than enough capacity to guarantee deals so they can easily just stick with them for an entire product stack and simplify procurement.  (For dGPU products. Future of Nvidia’s business direction withstanding at the moment.)",hardware,2025-12-22 17:39:46,235
AMD,nvei7gh,I imagine AMD don't commit the volume necessary to supply the OEMs   Look at the supply issues on the 9070/XT for most of the year. And the DIY market is small,hardware,2025-12-22 17:39:00,202
AMD,nvekxim,"1. Nvidia has brand recognition and simply sells better.  2. Nvidia can guarantee OEMs as much supply as they can move. AMD discrete GPUs for laptops are as good as vapourware.  3. Nvidia GPUs are more efficient and that really matters in a laptop. AMD GPUs really struggle with idle power draw especially, so even before you start to run anything on them, you’re on the back foot  4. Nvidia has their entire laptop stack (with the exception of a couple of SKUs) available at launch. AMD drip feeds its launches so the hype doesn’t remain.  5. A massive proportion of gaming laptop buyers buy them to do work. Almost none of that works on AMD hardware. Their GPUs are straight up not supported in V-Ray and Corona. They absolutely suck in Blender. A 7900 XTX chugging 350W gets is arse handed to it by a 14 inch MacBook running on battery for example. Nvidia GPUs are better for video editing, especially with the new NVDEC of 50 series.   6. The value argument doesn’t hold for AMD laptop hardware. They have worse features and don’t tend to cost much less. And they all have the same VRAM anyway so that is also not a selling point.  TLDR: Because they are worse products and there is more nuance to the laptop market than there is to the DIY gaming desktop market.",hardware,2025-12-22 17:52:35,77
AMD,nverhi0,"It's more likely due to AMD.    They simply don't ship that much volume and unlike nvidia who is constantly pushing volume of desktop GPUs, AMD seem to prefer the much more profitable AI/datacenter market.   Why integrate a worse, less efficient GPU at almost the same price?",hardware,2025-12-22 18:24:53,17
AMD,nvelbw4,"AMD isn't able to guarantee the same amount of supply as Nvidia (same situation with CPUs and Intel, although that's been improving)  And AMD has also been behind in terms of performance/watt since at *least* 2014, which is the most important metric by far when thinking about laptop GPUs.",hardware,2025-12-22 17:54:33,22
AMD,nvehi2k,Amd never released newer lineup this gen thats why.,hardware,2025-12-22 17:35:25,17
AMD,nvf6c3u,"They don't. AMD just hate the market for some reason and have never been good at supplying it.  Remember that a lot of laptop brands are largely made by a relatively small number of ODMs. Producing a modern laptop mainboard with discrete graphics is where an ODM leans heavily on their knowledge and relationship with Intel and Nvidia for circuit designs etc. Both companies are very focused on mobile and working with these ODMs. Both have a large lineup of products specifically engineered for mobile, AMD... not so much.  AMD hasn't been focused on mobile for a long time and ODMs have complained about the lack of relationship with the company, being left to figure stuff out themselves etc as far as putting together a mainboard featuring AMD products.  This generation they also just plain do not have the competitive products, there is no RDNA4 mobile chip which is strange. In the past there has also been complaints about inadequate supply of mobile dGPUs.",hardware,2025-12-22 19:39:04,14
AMD,nvf7jah,Hard to ship something that doesn't exist,hardware,2025-12-22 19:45:07,17
AMD,nvegyew,Power efficiency is everything in a laptop and AMD GPUs require a higher power draw to get a similar performance.,hardware,2025-12-22 17:32:35,23
AMD,nvghna1,"Likely more profitable to go with NVIDIA, whether that is in better prices on components from NVIDIA, or a lack of supply from AMD, or simply better brand recognition that attracts customers.",hardware,2025-12-22 23:55:06,3
AMD,nvgrjjr,"1. AMD only has enough current mobile dgpu skus to count on 2 hands at most 2. AMD doesn't commit to actually shipping high volume of mobile parts 3. AMD tries to force ""AMD advantage"" with various hardware requirements that makes it a pain in the ass for OEMs to ship either cheap base models or use Intel cpus as an alternative if there is a shortage of AMD APUs.  No sane OEM is going to sell amd dgpus when they have min spec requirements for CPU, Memory, Storage, Screen, and wifi/bt.",hardware,2025-12-23 00:53:24,3
AMD,nvhgaig,"All these choices are done via the GPU maker’s business development team going and getting it done by selling into the laptop OEMs. The BD team is enabled by leadership committing manufacturing resources. It’s not like the laptop OEMs just go decide what to buy and put in the systems. I could also see the laptop OEMs going to the silicon companies and sharing “we have consumer demand for an AMD GPU, can you please make some available to us?”  In short, AMD either hasn’t made it a priority or they don’t have the capability. Or consumer demand doesn’t exist such that either party makes it happen.",hardware,2025-12-23 03:25:22,3
AMD,nvhi5r8,Because Nvidia makes 95% of all GPU products… so laptop makers need to be ensured that mass product will be available. AMD likely doesn’t have the capacity that some SI’s are looking for.,hardware,2025-12-23 03:37:19,3
AMD,nvhsnnp,"Because they are not good and AMD is well know for making half ass bios and then never fixing it, especially on the GPU side of the house.   For example we are all still waiting for AMD to release the Linux nvme raid driver for the built in raid, for ALL AMD CPU platforms. Been what? 10 - 12 years now?",hardware,2025-12-23 04:48:11,3
AMD,nvjlvs5,"its the other way around, AMD only pretends to make mobile GPUs so they can lie to investors or something  seriously they have zero volume, idk why they even bother with it as a product stack. must have a government contract or something",hardware,2025-12-23 13:56:48,3
AMD,nvsznig,"The whole concept of the APU, AMD, and Intel to a lesser degree, are choosing to beef the iGPU in their APU to compete in the low and medium performance gaming laptops against the Nvidia dGPUs.  I personally have the theory that the handheld PCs were a proof of concept of how far AMD and Intel can take their APUs performance wise vs Nvidia discrete solutions in the mobile market.   AMD probably are thinking that if a single chip in the form of an APU can deliver a similar performance to two chips (CPU+dGPU) for 50-80% of the power budget, then they can corner the low and part of the medium performance gaming laptops market, after all you don't see a Nvidia dGPU in any of the current or future X86 handhelds, way too power hungry and ARM alternatives like the Tegra family fall considerably short, not to mention the still messy compability layer to run X86 games on ARM systems.",hardware,2025-12-25 01:08:56,3
AMD,nvf86y9,Supply Issues is most likely   \#1 Reason  \#2 They don't sale.   Its the perfect combo of why bother. R&D isn't cheap you need to recoup cost at the bare minimal for what you put into the device being sold. Companies aren't going to do each other any favors anymore then they do a consumer a favor.,hardware,2025-12-22 19:48:28,5
AMD,nvggnqd,"AMD doesn't have a mobile GPU lineup that has matched NVIDIA's for a while now (at least in the last 11 years), and what doesn't help is the efficiency at idle/low loads isn't great either which isn't a big deal in a desktop but much more important in a laptop, as you don't want heat to stick around in the laptop shell.  Also whilst the Nvidia mobile gpu shennagians weren't great with naming, they are generally more power efficient and performance ""per watt"" is better , especially if you are pairing it with a high end cpu or a thin laptop shell.",hardware,2025-12-22 23:49:14,5
AMD,nves026,"AMD makes limited laptop level GPUs. Additionally, for laptops the market is a bit more split. If people want a gaming laptop, they generally really want a capable machine, and Nvidia has the more significant brand recognition there. If they don’t want that, they generally explicitly don’t want a dGPU. So as an OEM, it’s hard to justify the volume necessary to support an AMD SKU rather than just consolidate around an Nvidia line",hardware,2025-12-22 18:27:25,2
AMD,nvezuog,"I think that the APUs are pretty good for laptops, the RX6600M didn't really take off, I haven't seen any laptop with 7600. If they don't come with a 9060 non-XT for laptop now, then they never will.",hardware,2025-12-22 19:05:59,5
AMD,nvegw55,"Wondering the same thing, and I'd love to see an explanation.",hardware,2025-12-22 17:32:15,5
AMD,nvei62i,"The key reason is just that Nvidia sells better. For an integrated product such as a laptop its very expensive to give consumers the choice, only for 90% of them to go with nvidia.   The technical aspects are not super important, because AMD did price some of their laptop GPUs aggressively to offset a hypothetical inferiority and almost nobody adopted it other than minisforum I think.   Its not a god situation for the market, its really nice to see framework making it flexible, but for many OEMs it seems just not worth it. The nvidia + Intel bundle situation might make it even worse for AMD in the laptop space in the future...",hardware,2025-12-22 17:38:47,5
AMD,nvhflr4,"As an oem manufacturer, why would I want to carry 2 sets of inventory?",hardware,2025-12-23 03:21:00,2
AMD,nvip7tg,Because AMD can't supply them realibly in any meaningful way that the manufacturers can design multiple lines across,hardware,2025-12-23 09:37:52,2
AMD,nvis47a,"Since GPUs come with the laptop, and are not generally replaceable, it usually comes down to a matter of ""can you promise X sales at Y cost"" and NVIDIA being a fuckin trillion dollar company likely has the ability to offer much more competitive pricing. This is just one factor though as I'm sure there are many.",hardware,2025-12-23 10:06:01,2
AMD,nvkk3e3,Costco Canada uses laptops with Ryzen/Radeon in most of what they carry.,hardware,2025-12-23 16:53:16,2
AMD,nvlqz93,I've had a laptop with a 6800s.  It received one driver update. ONE.,hardware,2025-12-23 20:27:54,2
AMD,nvltdmk,"Because they probably still get kickbacks from Intel, or other ""incentives"".",hardware,2025-12-23 20:40:45,2
AMD,nvlz38r,"Lisa ""we're going for marketshare this gen by abandoning mobile, low end & high end""",hardware,2025-12-23 21:11:16,2
AMD,nvqmad0,"AMD historically hasn't been able to supply these manufacturers consistently enough, they don't trust AMD.",hardware,2025-12-24 16:33:54,2
AMD,nvely9g,Cause they like money and amd doesnt make them money,hardware,2025-12-22 17:57:38,7
AMD,nveixhl,because market share is leverage and OEMs don't want to risk their nvidia partnership,hardware,2025-12-22 17:42:36,3
AMD,nvff5vu,"You already got your answer from other users here, but AMD and Intel seem to not even really be trying to unseat Nvidia dominance in the laptop dGPU space anymore. Their currently strategy is to try and undermine it by beefing up iGPUs to the point where some more budget oriented people may question if they even need to step up to a x50 or x60 class dGPU.  AMD's first crack at this with Strix Halo may have been a flop, but the strategy is sound. Intel's releasing their X line of PTL chips in a few weeks. I'm sure AMD is gonna double down next gen with this plan as well.",hardware,2025-12-22 20:24:38,4
AMD,nvehamz,"Thats why I bought a Framework 16.  To have a Radeon GPU cuz Nvidia is utter dogshit under Linux, especially since most laptops don't use a fuckin MUX switch.",hardware,2025-12-22 17:34:20,3
AMD,nvffzdt,Simple numbers game.  Nvidia - 92% market share   AMD - 7% market share   Intel - 1% market share    Which one are you going to go with if your goal is to sell the largest number of gaming laptops possible?,hardware,2025-12-22 20:28:52,2
AMD,nvv4uzf,"educated guess: a laptop with a Ryzen CPU and a Radeon gpu, what a wonderful reason to scream ""monopoly""! ☺️",hardware,2025-12-25 12:58:45,1
AMD,nw12k9d,I have a legion with a Ryzen 9 and a Radeon gpu..,hardware,2025-12-26 14:42:29,1
AMD,nw71v5o,More like why AMD hates laptop manufacturers.  AMD needs to release mobile GPUs first and deliver them in necessary quantities generation after generation. Also they need to provide them across different pricing brackets.,hardware,2025-12-27 14:36:27,1
AMD,nwpwhgt,Wrong question. Why does AMD hate laptop manufacturers and refuse to supply units to them?,hardware,2025-12-30 12:20:26,1
AMD,nvftzjh,"They don't hate them, AMD GPUs simply SUCKS for what Laptop requires most  \#1 Reason: EFFICIENCY, Nvidia destroys AMD gpus in terms of efficiency, not even close, and efficiency is everything in laptop, even 10W higher could become a problem. Nvidia just draws so much less power at the same performance level for many generations already ever since the first RTX. The 5700XT draws the same power as 2080ti while being way slower and has no feature, 7900XTX consume more power than 4090 while being destroyed in everything, even newest 9070XT consumes about 100W higher than 5070Ti even though it's still slightly worse, this might not be relevant in PC at all, but for Laptop this is super crucial, and AMD completely failed at this. Not to mention idle power draw is a problem for AMD gpu.     \#2: Laptops are always meant for WORK and PRODUCTIVITY, not just gaming. If you want gaming only just build pc or get a console, but laptops have to be able to do as many productivity tasks as possible, that's what laptop is made for, a portable pc for work, even gaming laptop is made so that people can work and game on them, and AMD failed this aspect as well, Nvidia not only offer far better feature sets, but also simply work way better and compatible with everything, never need to workaround or tinkering, everything works with Nvidia gpu, and this is massive for laptop     These are the 2 biggest reasons, it's never about stocks or lack of volume that some clueless comments here suggest. That's why AMD thrives with Console instead, because consoles don't need neither of the reasons I mention above, console doesn't care about efficiency since they always plugged in, and console is for gaming only, never for anything else, so AMD has no problem fulfilling the massive volume of console Gpus, because for console use case AMD gpus make sense, unlike laptop",hardware,2025-12-22 21:42:38,1
AMD,nvelvp4,Probably the same reason all the laptops are intel despite them getting clowned on. It's because their agreement says so,hardware,2025-12-22 17:57:16,0
AMD,nveluk8,Probably the same reason all the laptops are intel despite them getting clowned on. It's because their agreement says so,hardware,2025-12-22 17:57:07,0
AMD,nvggjoq,NVIDIA optimus and better efficiency,hardware,2025-12-22 23:48:34,2
AMD,nvf3akf,"I actually saw this talked about somewhere. I think the people said the manufacturers have found that there is just huge demand for nvidia gpus from consumers. Dlss, framegen etc and the fact that they deliver very good performance with reasonably small power, it is a no brainer for them. Having an nvidia gpu in the laptop does free advertisement for them.",hardware,2025-12-22 19:23:26,1
AMD,nvei17v,"Why do most people buy Nvidia?  OEM's follow what sells, Nvidia sell units. Just look at all the posts we still see asking if AMD GPU's will burn up, the number of times a AMD GPU was lower cost but OP ends up going Nvidia as it's the brand that everyone buys.  It's just sales, Nvidia moves laptops.",hardware,2025-12-22 17:38:07,-3
AMD,nveh6sx,"Why do orange basket manufacturers hate bananas?   I noticed that, while looking for a new purse.",hardware,2025-12-22 17:33:47,-1
AMD,nveiort,because intel still lobbying  one exemple: [https://www.reddit.com/r/Amd/comments/16q44ar/eu\_fines\_intel\_400\_million\_for\_blocking\_amds/](https://www.reddit.com/r/Amd/comments/16q44ar/eu_fines_intel_400_million_for_blocking_amds/),hardware,2025-12-22 17:41:23,-12
AMD,nvh91ql,I know it’s bs. Yet virtually every handheld pc maker uses AMD 🤦🏻‍♂️,hardware,2025-12-23 02:40:30,-1
AMD,nvfsakn,"They don't make a dGPU for laptops, just iGPU. Though their iGPU is pretty good for most tasks besides gaming. AMD probably thinks if you wanna game, buy a gaming system like a PS5 or Steam Deck which hosts an AMD chip.",hardware,2025-12-22 21:33:47,0
AMD,nvhlluk,I have a last gen asus a16 advantage edition with a 7700s gpu. Rougly 4060M performance minus the nvidia rtx/ftame gen which i never use anyways,hardware,2025-12-23 03:59:47,0
AMD,nvejjxr,"It's old practices, even when AMD started dominating in CPUs you look at the market place and it's NVidia/Intel everywhere... That eventually changed due to the lunacy of not having the best CPUs ""because reasons""... The GPU side of things is going to be very hard as the stigma that AMD is a budget brand is what drives these companies to avoid good AMD GPUs as they fear the budget stigma will rub off on their premium products.",hardware,2025-12-22 17:45:41,-10
AMD,nvi4m8u,"They used to run too hot for laptops and nvidia drivers used to be the more reliable ones  Companies are very reluctant to change  This is what Steve Jobs noticed in hardware manufacturing when it came to knowing/not knowing how many precisely how many units were produced per unit of time, among other problems and common practices. The answer to everything was “well because we’ve always done things this way”  With Ryzen, there was a clear reason to switch, it had better performance and lower temps, everything you’d want for a laptop  TLDR: no reason other than “it’s always been this way.” No one changes until someone changes.",hardware,2025-12-23 06:22:33,-1
AMD,nvi5h9s,"Because gaming laptops actually suck for gaming, and Nvidia has a stranglehold on the creative market (who actually buy high end laptops) with CUDA.",hardware,2025-12-23 06:30:04,-2
AMD,nvfsoop,"AMD sold their mobile Radeon brand to Qualcomm when they were at their lowest, hence why Adreno is just an anagram of Radeon. Because of that deal, they can't have dedicated mobile GPUs under their Radeon branding...",hardware,2025-12-22 21:35:49,-7
AMD,nveu5h8,Nope. They wont launch a 9070M,hardware,2025-12-22 18:37:57,116
AMD,nvf3blw,question answered,hardware,2025-12-22 19:23:35,62
AMD,nvfs4is,"They made RDNA, RDNA2, RDNA3... almost no laptops.  Backroom deals, pressure or even manipulation from Nvidia might not be the whole story but I suspect it is going to be a significant factor...",hardware,2025-12-22 21:32:55,10
AMD,nvelsr1,"Chicken and egg, AMD hasn't been shipping product because laptop manufacturers do not buy them. They're finding success in their strix halo APUs, and that's likely their future in the laptop space.",hardware,2025-12-22 17:56:53,-41
AMD,nveiq2s,"Yeah, this is by far the biggest issue discouraging adoption by laptop makers.",hardware,2025-12-22 17:41:34,44
AMD,nvelh9x,Well they supplied the entire console market pretty well and their cpu market too so their track record is actually good,hardware,2025-12-22 17:55:18,2
AMD,nvgyss7,"AMD doesn’t actually make anything, do they?  All their chips are made at other fabs.  That means they’re competing with everyone else for the same production capacity.",hardware,2025-12-23 01:38:03,0
AMD,nveqtpa,"True points.  Adding to them that Nvidia is more reliable in hardware and software for decades.  AMD drivers for RDNA used to be problematic and they had higher RMA rates. For a DIY enthusiast this is less of an issue, but for a system integrator/laptop manufacturer it means costs. Even the basic support request is a cost and you may have to service an entire system in case of a RMA, instead of just the dGPU for a DIYer. Combined with AMDs notoriously low margins, a single support phone call can mean a financial loss.  Meanwhile Nvidia is generally stable. While sometimes the drivers cause issues in specific scenarios, there was no general issue with them as their was with early RDNA.",hardware,2025-12-22 18:21:38,19
AMD,nvfra3n,"7 Historically AMD's software stack had been so clownishly bad they were basically unusable. Until a couple of years ago you genuinely couldn't install the normal driver on their mGPU on many laptops, instead you'd get some branded locked down driver with missing features from the laptop maker from around the time they released the model and stuff just wouldn't run on a 2, 3, 4 year outdated driver. Sometimes they'd even have the older driver UI which hadn't been in use for literal years.  AMD being bad at software is not a meme.",hardware,2025-12-22 21:28:30,9
AMD,nvi51ow,"Why the heck are they always coming out with new codec’s lol  It always leaves AMD in the dust, makes it feel like Ryzen and Radeon will always be left behind when it comes to editing and streaming",hardware,2025-12-23 06:26:17,2
AMD,nvfka74,> Nvidia GPUs are more efficient  This is not really true. RDNA4 is just as efficient as Nvidia. rx9070 topped GN's efficiency charts for instance. RDNA2 was also more efficient than Ampere.  AMD also has Radeon Chill which is another tool you can use for power efficiency Nvidia doesn't have.  I agree with your other points.,hardware,2025-12-22 20:51:31,1
AMD,nvexbft,Who uses a laptop for blender or video editing? Literally cherry-picked use-cases to make AMD look bad.,hardware,2025-12-22 18:53:26,-27
AMD,nvhj90c,And they barely released anything last generation either.,hardware,2025-12-23 03:44:24,10
AMD,nvf18dd,Efficiency at low loads is also poor too: https://tpucdn.com/review/sapphire-radeon-rx-9060-xt-pulse-oc/images/power-video-playback.png,hardware,2025-12-22 19:12:59,16
AMD,nvekfcw,Not since Vega,hardware,2025-12-22 17:50:02,-11
AMD,nvhkr1q,"Actually, you could count AMD’s current generation laptop dGPUs on 0 hands, considering they don’t have a single RDNA4 laptop part this generation. We’ll see if that changes at CES next year (in ~2 weeks), but I wouldn’t hold my breath.",hardware,2025-12-23 03:54:13,2
AMD,nvkegfh,Weirdly their mobile chips are far more likely to be found in mini-PCs than laptops. I'm not sure why that is but presumably they've built a stronger relationship with the mini-PC firms.,hardware,2025-12-23 16:25:51,1
AMD,nvjxs9k,"RX 7600S exists in a variant within the cheaper Asus TUF, same as the RX 7700S",hardware,2025-12-23 15:03:08,0
AMD,nvehr27,"I've been debating recently on which GPU to get alongside a mobo upgrade, what do you use to have it switch to the dedicated GPU?",hardware,2025-12-22 17:36:40,1
AMD,nvek8ky,As someone that’s works with corporate GPU applications this is funny. If they were so bad why would they dominate the industry?,hardware,2025-12-22 17:49:06,6
AMD,nvfvvdu,"the idea of a mux switch in a laptop is hilarious, hardware fix for horrible drivers",hardware,2025-12-22 21:52:21,-2
AMD,nvgc595,"These numbers aren't relevant. They look like dedicated desktop GPU market share. In reality, I bet Intel is first in laptops, followed by AMD just due to the fact that most laptops don't need, nor want dedicated GPUs. As for gaming space idk",hardware,2025-12-22 23:22:26,-1
AMD,nvf3pnf,nah intel mobile cpus are just better rn,hardware,2025-12-22 19:25:35,11
AMD,nvi46x5,They’ve put double agents to make AMD mobile gpus suck (because they’re)?,hardware,2025-12-23 06:18:50,3
AMD,nvezdou,LOL,hardware,2025-12-22 19:03:37,11
AMD,nvfopof,"I call it the *OEM-factor*™ … Always gets immediately DENIED as non-existent, of course, since years.  Yet somehow we had *a completely heathy and quite balanced laptop-market up to the early 2000s*, where there were loads of potent AMD-powered offerings with AMD's *PowerNow!*-technology (Dynamic frequency scaling and power-gating for power-saving), and it was almost like fifty-fifty AMD vs Intel …   That was when Intel was still utter sh!t in that department and had even horrendous power-draw in mobile.  Until it all of a sudden all changed the precise moment Intel brought their infamous *Centrino*™ program to OEMs (paying system-integrators for equipping notebooks with Intel-chips)  — It has stayed as such since (+90% Intel).  Then the Intel-exclusive *UltraBook*-brand was the next, which a while ago just rolled over to be Intel *Evo*.  > Intel and Nvidia pay system integrators to use their own hardware over AMD's.  Yeas, and very handsomely at that. Still, *»Nothing to see here folks, just move along!«*",hardware,2025-12-22 21:15:05,-2
AMD,nvekqpa,"And despite the absolute surge of burned connectors since Blackwell, it is still AMD cards that get questions sbout catching fire",hardware,2025-12-22 17:51:38,5
AMD,nvela7a,"Lmao Intel isn't lobbying OEMs to choose Nvidia GPUs.  And also literally read the first paragraph of the article: ""between 2002 and 2007"".  And thirdly, the biggest proof Intel isn't lobbying OEMs (with what money?) Is how many design wins Snapdragon got",hardware,2025-12-22 17:54:19,22
AMD,nvemfxf,"Right... Intel is lobbying OEMs to use Nvidia products..  Maybe every once in a while you should actually read the articles you use as ""proof"".",hardware,2025-12-22 18:00:04,15
AMD,nvelpt7,"AMD had to consistently deliver a better CPU than Intel for several generations before mindshare in the public caught up.  Its going to be the same for GPUs: until AMD offers the better *all around* GPU, several generations in a row, and at a lower price, it's mindshare just isn't going to change in GPU.  And dont forget the impact Halo products have on their downstream products. The fact that the 5090 is the undisputed best GPU gives huge brand prestige to the whole product stack.",hardware,2025-12-22 17:56:28,17
AMD,nvfz8l1,what?,hardware,2025-12-22 22:10:06,1
AMD,nvivpn8,"> Because of that deal, they can't have dedicated mobile GPUs under their Radeon branding...  Oh shit, you'd better inform AMD because they've been using Radeon branding for all their mobile dGPUs all the way up to this very day.",hardware,2025-12-23 10:40:31,0
AMD,nvh56n9,They’ll put a 9050m in laptops after they release RDNA6 in 2030,hardware,2025-12-23 02:16:52,28
AMD,nvg5a6l,Why would laptop manufacturers do this to us?!,hardware,2025-12-22 22:43:10,53
AMD,nvj2qn5,Rdna2 was the only real competitive mobile showing in a decade plus. That was also when they got the most laptops including the flagship lenovo legion 7 in an AMD advantage model. Stop playing the victim and accept they have NEVER been competitive for more than 1 gen in a row and they do not make things easy for oems.,hardware,2025-12-23 11:44:06,25
AMD,nver688,"I fear ""success"" in Halo Strix is related to its limited supply: that is, the Strix Halo large APU price is so high, the demand is also pretty low → AMD can make enough dies to satisfy the small market.  Large APUs will find it tough to crack into the laptop gaming market in terms of market share; the only other large APUs are 1) consoles, which only survive on extreme optimisation, hundreds of millions of units, and they *still* sell at a heavy loss, and...  >As [IGN reports](https://www.ign.com/articles/microsoft-loses-between-100-and-200-on-every-xbox-sold), Spencer confirmed that the loss on each Xbox console is between $100 and $200 dependent on the model.   2) Apple's M-series Pro / Max / Ultra. These are also very expensive and kind of what one would expect a large APU to cost (+ the customary Apple tax).",hardware,2025-12-22 18:23:21,26
AMD,nveqsbl,"Chicken and egg doesn’t really work for B2B. The seller has to provide the product and push the deals, demand doesn’t just appear out of the air for products with (edit: particularly established) competition. The buyer doesn’t particularly care which egg it buys as long as it has enough of them to make what it wants to sell.  (And in this case the consumer doesn’t really care either, the overwhelming majority of any form of prebuilt including laptop is just “it’s in my budget, available, and the page says it’s good”.)",hardware,2025-12-22 18:21:27,60
AMD,nveyg5b,"Strix Halo has been a commercial failure.  There is only one commercially available laptop of it. No major OEMs are even interested in it, and it is now being sold to Chinese brands at a discounted price for mini PCs and handhelds.",hardware,2025-12-22 18:58:58,40
AMD,nvfez9n,"Most people talk about volumes and while that certainly does contribute, the main problem is that OEMs basically expect cpu vendors to handhold them to designing most of the system. Yes you read that right, designing the system as in mobo reference designs, cooling solution and beyond just what you imagine to be normal SW support. When qcom started to double down WoA to promote snapdragon elite, the higher ups complained about how they had to provide massive amount of support to OEMs because they were to used that and they have to massively ramp up on that for OEMs to take them seriously and not just put their SoC in some half assly designed gimped models",hardware,2025-12-22 20:23:41,12
AMD,nvemrk9,"I think APU's will be a big deal for AMD in the near future, they will be fast enough for everything and will keep getting better for the limited power envelope of laptops",hardware,2025-12-22 18:01:39,2
AMD,nven0z3,Supplying the console market is probably one of the reasons they don't have much stock left for laptops.,hardware,2025-12-22 18:02:56,73
AMD,nvf0m0e,The consoles are using old nodes tho,hardware,2025-12-22 19:09:49,12
AMD,nvenqkg,"Right, now look at their GPU track record, the relevant market here",hardware,2025-12-22 18:06:29,26
AMD,nvesa9n,"Desktop CPUs sure, but mobile CPUs they don't seem to supply a sufficient volume of either. Look at Strix Halo, a high margin performance leading part and a year after launch you can still count the number of laptops using it on one hand.",hardware,2025-12-22 18:28:49,15
AMD,nvi0fev,But they choose how to allocate their capacity,hardware,2025-12-23 05:47:07,5
AMD,nvhpn99,Cost wise its not effective either. Its the same reason why AMD fail miserably in the Pre Built space.  9060 xt and 5060ti. 350 v 420. $80 and 20% right?  But when you build a whole PC with the 9060xt for lets say $1000. The same with a 5060 Ti wil cost $1080. Only a 8% difference. A difference many would pay  And the difference gets smaller the higher your base pc is  Same thing applies in laptops where the sum of all parts makes the RTX GPU only slightly more expensive.,hardware,2025-12-23 04:26:49,10
AMD,nviwwwy,"They didn't. They simply were catching up to Apple and Intel who've had 10-Bit 4:2:2 HEVC decoding for a very long time. Even now, I don't think the 50 Series can decode 10-Bit 4:2:2 H-264 in hardware.   It is a big reason why most videographers and video editors use Macs. 10-Bit 4:2:2 HEVC is a very common acquisition codec now and quite frankly, it is laughable and unacceptable that neither Nvidia nor AMD supported them until this year. AMD still doesn't.",hardware,2025-12-23 10:51:50,12
AMD,nvh2us8,"Are you speaking to efficiency under load? Or while idle?  Idle power draw has been a weak spot for AMD GPUs for a while. I haven't looked into it since launch, but at the time initial reviews seemed to confirm this was still an issue for RDNA4.  High idle power draw is bad for laptops, obviously.",hardware,2025-12-23 02:02:46,8
AMD,nvhmegg,"I also agree in everything but that point as well. Don't know why you're getting downvoted, though not surprised since everyone here is speaking anecdotally. Also, while you posted an example from TPU there are other outlets such as [computerbase](https://www.computerbase.de/artikel/grafikkarten/amd-radeon-rx-9070-xt-rx-9070-test.91578/seite-9#abschnitt_leistungsaufnahme_gemessen_spiele_youtube_desktop) that backs it up (pretty much on par on idle, and daily usage/multi monitor)  I've actually made a post here: [https://www.reddit.com/r/hardware/comments/1l2vjuo/mostly\_positive\_reviews\_rx\_9070\_xt\_vs\_rtx\_5070\_ti/](https://www.reddit.com/r/hardware/comments/1l2vjuo/mostly_positive_reviews_rx_9070_xt_vs_rtx_5070_ti/)  Where I shared someone's finding on the matter with RDNA4 vs Blackwell (9070 XT vs 5070 Ti). At that time, when capping FPS, AMD by default simply does a better job with freq to hit a locked FPS target to save power, even doing as well or beating the 5070 Ti. In those comments you'll see (obviously so) that FPS capping saves you power anyways, but the matter is, RDNA4, especially when binned and power constrained (+drivers) can be effectively used in laptops.  Oh and btw,  >RDNA2 was also more efficient than Ampere.  IIRC pre-RDNA3 AMD only reported GPU power and not TBP (it was an [igors lab test](https://www.igorslab.de/en/graphics-cards-and-their-consumption-read-out-rather-than-measured-why-this-is-easy-with-nvidia-and-nearly-impossible-with-amd/))",hardware,2025-12-23 04:05:00,2
AMD,nvjm36w,"Yep. RDNA3 was really the only botched generation efficiency wise (post Vega). But just like with drivers, people will continue to believe AMD GPUs have worse efficiency for the next 10 years, regardless of how each generation actually does.  inb4 but rdna2 had a node advantage. Yes, and?",hardware,2025-12-23 13:57:59,-4
AMD,nvezpob,"That’s like the majority of professionals who need a GPU in their laptop, that’s not that cherry picked.",hardware,2025-12-22 19:05:17,28
AMD,nvezb25,You can’t think of a reason why someone may want to do video editing or Blender work on the go?   Or have a device capable of being a video editing platform anywhere to allow for working from home?,hardware,2025-12-22 19:03:15,23
AMD,nvezl9d,basically everyone using a Mac.,hardware,2025-12-22 19:04:40,16
AMD,nvha33b,"no Blender pros do buy them, as new laptop 40 and 50 series do very good in blender  new laptop “5090” GPU with 24GB VRAM almost matches a 4080 super desktop in blender performance at 175 watts   https://opendata.blender.org/benchmarks/query/?compute_type=OPTIX&compute_type=CUDA&compute_type=HIP&compute_type=METAL&compute_type=ONEAPI&group_by=device_name&blender_version=4.5.0  CAD professionals, with Recent Ai boom a laptop with CUDA support for ML and Ai it's an awesome buy  In steam charts 4060 laptop is the top……  Unlike what reddit thinks,   do you think companies pour millions in for gaming laptop manufacturing R&D to waste?  Gaming laptops do insane number of sales.",hardware,2025-12-23 02:46:47,6
AMD,nvelkrb,"They still do. RX 7600 is slower than a 4060 and needs 40-50W more power to run. To compensate, AMD had to reduce CUs from 32 to 28 and reduce power draw. On the other hand, the RTX 4060 is identical on laptop and desktop, down to the CUDA core count and power consumption. It’s the better product. And given that the 60 series is the most popular laptop GPU series, that really adds up",hardware,2025-12-22 17:55:47,20
AMD,nvetp5y,They are less efficient than nvidia and suck way more power at idle which is a big deal in laptops,hardware,2025-12-22 18:35:43,6
AMD,nvetuoh,"I dont need to switch it myself, since it works in handshake with the AMD DGPU and IGPU with integrated MUX switch.   So basically, desktop idle and browser stuff runs off the IGPU, everything else, DGPU 7700S handles it.   Also, if ya have a 7745h cpu, not worth swapping to the HX since its half the full core size, plus compressed Zen 5c cores.  Dont like that alot",hardware,2025-12-22 18:36:28,3
AMD,nvel0u2,Open source community distros running GUI desktops are a bit of a different experience than headless Linux servers running with enterprise licensed packages or Windows/Mac desktop.,hardware,2025-12-22 17:53:02,20
AMD,nvggnqa,These numbers are the *only* thing that's relevant.  We're talking about gaming laptop GPUs.  Manufacturers are going to sell what people want to buy.,hardware,2025-12-22 23:49:14,5
AMD,nvf6179,I was talking tech wise... also no. A quick gander on the passmark puts the top 10 laptop chips as 80% AMD with Intel taking the number 2 and 3 slots. So I'm going to gather that the intel mobile cpus are at minimum on par.. I'm sure theres a some variance in benchmarks but I'm lazy  https://www.cpubenchmark.net/laptop.html,hardware,2025-12-22 19:37:30,2
AMD,nvhs8e1,"Qualcomm showing up in more and more laptops. ARM is the future, Apple proved that. And Qualcomm is the company best poised to make the ARM chips in the quantity needed, because they’ve been doing it for smartphones for decades.",hardware,2025-12-23 04:45:08,-1
AMD,nvezusp,"I know right! So pathetic what those companies are willing to do to put down AMD, but shoe is on the other foot now and AMD will stomp out intel in the CPU space and is gaining ground as Nvidia leaves gaming to propel the great AI scam.",hardware,2025-12-22 19:06:00,-3
AMD,nvj08ow,"Yeah, it all changed on a sudden because the Pentium M was just so good.",hardware,2025-12-23 11:22:13,1
AMD,nwpy5g7,and by absolutely surge you mean like 3 in total.,hardware,2025-12-30 12:32:50,1
AMD,nvelh9j,"I gave up pointing that out, people want to buy Nvidia and you cant change there mind even if it's lower cost or not setting on fire.  GPU's are like SSD brands, you just plug one in and it works yet a lot of normal/light PC users still think it's dark magic to change GPU brand.",hardware,2025-12-22 17:55:18,-1
AMD,nvey1c4,"Exactly. My biggest piece of advice for tech illiterate people buying a laptop until about 7 years ago was ""make sure you get an Intel CPU"". AMD was producing some absolute donkeys that had no place in the midrange laptops they were going into. It took a while for that sort of advice to cycle out of people's minds, well after AMD started making decent mobile CPUs.",hardware,2025-12-22 18:56:55,3
AMD,nvii28a,"https://www.qualcomm.com/news/releases/2009/01/qualcomm-acquires-handheld-graphics-and-multimedia-assets-amd  https://www.fierce-network.com/wireless/qualcomm-buys-amd-handheld-assets-for-65m  Instead of downvoting me, just do research...",hardware,2025-12-23 08:27:05,0
AMD,nvg9hnc,Do you mean AMD?,hardware,2025-12-22 23:06:58,-12
AMD,nvjpt9x,"Why are there no notebooks with Strix Halo then?  Asus, the vendor AMD even gives preferential treatment, only made tablet - a device nobody asks for - out of Strix Halo so that they don't have to put it in a laptop. Hmm? Absolutely AMD's fault for not making the processor, right?",hardware,2025-12-23 14:19:24,-5
AMD,nvj1xjk,"IIRC Strix Halo has bespoke I/O and compute dies, so presumably either supply is high, there's a plan to productize those some other way later on, or somebody is losing money.",hardware,2025-12-23 11:37:10,1
AMD,nvev6sn,"Next gens RDNA lineup is fully dual use. The top spec goes into the workstation market and high end desktop, the second chip is the 9070XT replacement and a cut down version goes into the next xbox. Third chip goes midmarket gpu and Halo gpu die, and the small one for the ...point mass market apus.",hardware,2025-12-22 18:43:03,-9
AMD,nvet6lj,"No, GPUs aren't direct replacements in laptops. They have to develop two different boards for each model, if they want to offer both manufacturers. Customers are also very picky in gaming laptops. Almost all of them will pick an Nvidia version over AMD at a similar price. Just ask yourself how much cheaper a $1400 laptop would have to be to pick a 9060XT over a 5060ti? If the answer is >$100, then AMD would have to basically give away the chips for free.",hardware,2025-12-22 18:33:12,-8
AMD,nverlg3,This is so massively ignorant to talk about in the PC space. People do care... It's why Nvidia owns 92% of the PC graphic card market.  AMD has been struggling to sell it's products for ages in the GPU space... Both to end users and to board and laptop partners and it's not because they can't produce product...   APUs are AMDs future in the laptop market because AMD CPUs are unmatched in the x86 space. Strix Halo sells because the okay GPU is bolted onto world class leading CPUs.,hardware,2025-12-22 18:25:24,-25
AMD,nvetzk2,AMD has been betting on that for years but still have issues giving good supplies to OEMs  And they also take a long time to actually launch their products to OEMs,hardware,2025-12-22 18:37:08,16
AMD,nvjm5ki,"somehow they have enough silicon to make strix halo IO dies with an entire mobile GPU integrated into the SOC  it is definitely more deliberate, especially since mobile GPU chips are literally just desktop GPUs but in a mobile package and power limited",hardware,2025-12-23 13:58:22,3
AMD,nveofqs,It's hard to judge rn because of AI dram production which definitely is gonna cut into some production for some of the process nodes. Ideally though you'd have thought they'd be at least well suited to gaming laptops with their lower power console focused development.   Especially considering their APUs are almost best in class at this point so you'd thought they'd at least have it down in the lighter laptop selections.   I think its mostly just legacy volume contracts and risk adverse manufacturers keeping intel and nvidia dominating,hardware,2025-12-22 18:09:56,-11
AMD,nvf58ki,Yeah they are but uh track record is kinda always gonna be old. Plus I wouldn't say that the current situation is exactly a good example.,hardware,2025-12-22 19:33:25,1
AMD,nvepc8e,"I am, I'm talking about their console GPU manufacturing. I'm saying that they have good track record for chip manufacturing its at least a half decent indicator.",hardware,2025-12-22 18:14:24,-13
AMD,nvf738n,"AMD uses a single universal tapeout/photomask set to satisfy the vast majority of their entire desktop and server catalogues. Even a substantial proportion of AMD's available laptop offerings over the last 2 years have been -HX ones, which are just the desktop SKUs made into a solderable package. That's why it's always been relatively easy for them to maintain relatively consistent inventory, despite having far far fewer wafers coming in compared to what Intel can get.  The actual monolithic mobile SKUs are fundamentally different die designs to the other market segments. Strix Halo also uses a fundamentally different chiplet packaging method to server/desktop Zen, so neither the CCD or I/O die are compatible and makes it its own third, separate limited production assembly line.",hardware,2025-12-22 19:42:53,18
AMD,nveu96i,It's that actually production problems or do manufacturers just not adopt strixx halos for what ever reason they dont adopt AMD CPUs or GPUs in the laptop market?   Are there volume problems with those laptops? I can imagine strix halos packaging makes it hard to produce,hardware,2025-12-22 18:38:27,1
AMD,nvi2sgv,"Does anyone expect them to prioritize the single GPU buyer, and let large customers get whatever is left, or does the other way around seem more sensible?",hardware,2025-12-23 06:06:53,1
AMD,nvj3l0t,"To sum up, when your one & only selling point is lower upfront cost, you will have a lot of uncertainty and areas of business you won’t be able to compete in.",hardware,2025-12-23 11:51:04,3
AMD,nvxm62h,Oh no wonder photographers and stuff use Macs,hardware,2025-12-25 22:21:11,1
AMD,nvh3707,RDNA4 idle efficiency is good too. You might be getting confused with RDNA3 big GPUs which were chiplet based. They use more idle power because they are chiplet based.  https://www.techpowerup.com/review/asus-radeon-rx-9070-tuf-oc/41.html,hardware,2025-12-23 02:04:49,7
AMD,nvjn3ob,"Wouldn't call RDNA3 botched, it's just a price of doing chiplets. I'm sure they learned a lot from it.  rdna2 had a node advantage but it wasn't just the node advantage that made it more efficient it was also the infinity cache, the new cache hierarchy that improved memory bandwidth efficiency (AMD could have similar bandwidth with less memory bus width). Nvidia followed suite by adding a big L2 cache in the next generation.",hardware,2025-12-23 14:03:53,-4
AMD,nvxinwm,"I can think of plenty that do, and it always comes down to the same two reasons.    1. They (for entirely selfish reasons, unrelated to work) travel between multiple locations and need access to GPU computing power, while away from their (home) office.   2. They don't understand remote compute and streamline decoding. While using the worst internet sources while on the go.   It's not that other, faster and more reliable solutions exist. It's that they are typically ADHD gremlins that need simple, reproducable solutions that work, even if it takes 500x longer and has stronger caveats.   It's not a market issue. It's a user issue wanting a product to confirm to their lifestyle, that wasnt designed to. Laptops are not workstations, and don't have the P2Pr to even compete in the space. But so many refuse to be ""tied down to a desk"" that this is how things are.",hardware,2025-12-25 21:59:39,1
AMD,nvezomv,"Laptops are for convenience. Video editing and blender require some amount of time sitting down to do what you want that it's better off being done on a dedicated workstation.  ""I'm going to do a little blender and video editing on my laptop at the library today"" - said no one ever",hardware,2025-12-22 19:05:08,-18
AMD,nveuqbm,"Nah, idle power on Nvidia is tragic if you more than a single monitor unless they have identical resolution and refresh rate. My 5090 goes from ~40W to 90W by just plugging in a second display while doing nothing",hardware,2025-12-22 18:40:47,-6
AMD,nvez41n,I have the 7940HS and was playing on going to HX 370,hardware,2025-12-22 19:02:15,2
AMD,nveob81,That’s where it’s being used mostly. I know this your hobby but surely you can see the bigger picture.,hardware,2025-12-22 18:09:18,-3
AMD,nvfbe4w,yes tech wise is why intel cpus are better. [https://imgur.com/a/CyxoUZM](https://imgur.com/a/CyxoUZM)  On paper the 9955hx and moreso the 9955hx3d look very good but in practice they fall short.   The 9955hx3d and 9955hx are simply lackluster in terms of battery life with the 275hx giving double the battery life and when unpluged the performance of the 9955hx/hx3d tanks which causes the 275hx to pull ahead when not plugged in. Additionally ryzen cpus have more sleep state issues which again hurts the battery life although that could be considered a windows problem and not on amd.   On top of this the intel chips have thunderbolt support and come with better wifi cards which amd refuses to use.   Then on the ultrabook side of things intel lunar lake is just better in terms of having lower powerdraw and a stronger igpu compared to AMDs lower power options although both intel and amd get smacked by ARM chips so not really that important unless you need x86,hardware,2025-12-22 20:04:51,11
AMD,nvi445l,yeah I would say ARM chips are the best for most laptop users but for gaming you still want x86 at least for now.,hardware,2025-12-23 06:18:10,1
AMD,nwpyiem,"There have been over 50 documented from cablemod alone, wtf are you talking about",hardware,2025-12-30 12:35:30,1
AMD,nviydbu,"Yes, I know about that. It doesn't have anything to do with their desktop/laptop class products.  Also, what do you mean by "" they can't have dedicated mobile GPUs under their Radeon branding...""? There are literally dedicated radeon laptop GPUs available.",hardware,2025-12-23 11:05:16,0
AMD,nvgxn08,Does AMD make things that go “woosh” when you need them to?,hardware,2025-12-23 01:30:53,26
AMD,nvkuzin,Strix Halo is way too expensive and still sucks down too much while doing absolutely nothing.    OEMs find it cheaper just to use an Nvidia dGPU with any mobile CPU and the end result is an equivalent or better mobile product except in Cinebench and Blender. It's absolutely AMD's fault for making something that few people want.,hardware,2025-12-23 17:47:27,10
AMD,nvjusfv,You have to go ask amd why they don’t codesign laptop platforms with oems the way intel does with their evo platform. Nobody automatically deserves commercial success just because they exist. If intel invests money into oem platforms then that’s what amd must pay to do.,hardware,2025-12-23 14:47:13,11
AMD,nwpwjsl,Strx Halo alone cost more than entire notebook.,hardware,2025-12-30 12:20:55,0
AMD,nvnoole,"And Intel is closing in on the APU side as well. Lunar Lake was impressive but also a bit expensive and also seems to had some level of supply issue, but Panther Lake is probably rectifying that and being cheaper to make, too.",hardware,2025-12-24 03:13:06,1
AMD,nwpwyes,Strix Halo is manufactured in tiny numbers in comparison.,hardware,2025-12-30 12:23:58,1
AMD,nvj2zzb,Their problems existed far before AI was a thing. Whatever new thing pops up conveniently becomes their newest excuse.,hardware,2025-12-23 11:46:16,4
AMD,nveplis,"And the consoles are a completely different market    AMD do not commit volume to PC  gaming, look at the GPU shipments, a much better indicator    I didn't say they couldn't, I'm saying they dont",hardware,2025-12-22 18:15:41,23
AMD,nvj8xyi,"[Based on Amazon's best sellers](https://www.amazon.com/s?k=amd+nvidia+gaming+laptop&s=exact-aware-popularity-rank), AMD's monolithic APUs are way more numerous than the desktop-repackage CPUs (which makes sense, they're kinda crap for a *laptop*).  But note that I had to put ""nvidia"" in the search to filter for actual *gaming* laptops, and not [""""""gaming"""""" laptops](https://www.amazon.com/NIMO-FHD-Gaming-Laptop-i7-1165G7-GPU-Computer-Fingerprint/dp/B0F549GSX4?th=1) with no discrete graphics and undisclosed CPU model.",hardware,2025-12-23 12:33:17,6
AMD,nvewppd,"Unless you are doing one specific job, which is LLM inferencing, no matter what the hype says, Strix Halo is very poor value performance wise. RTX 5060 laptops are faster and generally half the price.   Nvidia GPUs are then also way better supported in professional applications than AMD GPUs.  And if you really wanted to do LLM inferencing on a ""budget"", a MacBook Pro is more efficient and faster anyway.",hardware,2025-12-22 18:50:31,15
AMD,nvi350i,"No, but that's the reality    They don't commit the volume to compete in the GPU space, never have    Until that changes, Nvidia are PC gaming, a consequence of sufficient supply",hardware,2025-12-23 06:09:49,3
AMD,nvz4sjn,"Well as an amateur photographer I can tell you it’s just one of many reasons. The display of a MacBook is always perfectly calibrated so the colours look correct. They have great battery life so you can work from anywhere. And they have the fastest single core performance of any computer and as a result, Lightroom works a lot faster",hardware,2025-12-26 04:28:36,3
AMD,nvidh49,"On laptop the goal is to have zero power cost for having a GPU, this requires that the GPU get put into D3cold when not used. So idle figure from TPU's article isn't relevant because you wouldn't want to leave the GPU up in an idle state (with integrated graphics scanning out the display), you'd tear it down completely and power it off.  Doesn't work if you want to eg. load stuff in VRAM like running a LLM though, or hook up a display to a port directly connected to the dGPU :/",hardware,2025-12-23 07:43:07,12
AMD,nviz97w,"Oh wow, finally! That is actually great to read!      For more than a decade, AMD cards, like my old R9 280X, just drank down the juice in multi-monitor idle, with sometimes double the power draw of comparable Nvidia cards.      Even a 3080 is frugal in multi-monitor use compared to a 6800 for example.",hardware,2025-12-23 11:13:22,1
AMD,nvijjc4,Thx for providing a source.  TIL.,hardware,2025-12-23 08:41:46,1
AMD,nvhvt6t,Never worked on the creative/film/event management field I guess?,hardware,2025-12-23 05:11:04,11
AMD,nvf12ov,"That’s a very valid student usage of laptops though, and you very conveniently leave out the work from home usage which does involve sitting down somewhere for some amount of time, while still needing the portability to bring the device between home and the office if needed.  A workstation would mean giving each employee two device for hybrid work, or forcing remote usage, neither of which would be an amazing option compared to just giving each employee a laptop to commute with",hardware,2025-12-22 19:12:12,15
AMD,nvju8t2,Discrete GPUs are not meant to be used as display adapters. [Even Windows has CASO](https://devblogs.microsoft.com/directx/optimizing-hybrid-laptop-performance-with-cross-adapter-scan-out-caso/) now. Shut that pig off when it's not rendering something.,hardware,2025-12-23 14:44:15,3
AMD,nvf1ri4,">My 5090 goes from ~40W to 90W by just plugging in a second display while doing nothing  Yea that's not normal, 2 displays have been fine on nvidia cards for... well as long as I've had an nvidia card so 2017, to stay in idle memory speed state and it's only couple of watts more. Maybe if you go high enough it changes like two 4k240 displays or one/both being 1440p500, but 4k240 + 4k144 brief moment i had that combo didn't do it on a 4070ti and was fully idle state. Or you have some power settings cranked up maybe those effect things. Obviously interacting with like browser stuff with 2 high refresh/res monitors does have more brief spikes than a single panel would, but that's not idle.    3 is a bit of mixed bag, but even that it much better than it used to be like ~4-5 years ago, when 3 monitors of any kind was max memory speed state and can still get idle at 3 panels if the refresh rates of all panels isn't high and even then it might go to half memory speed state instead of full speed on some specific combo. I do remember some talk about how the specific model also might affect things due to... something, rather than just pure refresh/res combo. thought that was mainly for 3(+) panels where it mattered, but maybe that could be what's happening even with just 2 panels.",hardware,2025-12-22 19:15:41,0
AMD,nvfgxyn,"Meh, not worth it tbh.  If it were a X3D version, yeah, but basic 370 is basically 3% faster",hardware,2025-12-22 20:33:56,3
AMD,nvesq0e,"I’m not sure what you mean by “bigger picture”, but the desktop GPU experience is why people in this sub would have the opinion that Nvidia on Linux is a poor experience.   It’s only recently (within three years) that Nvidia started releasing open source kernel drivers for their cards on Linux (because they moved their proprietary code into the device firmware). Prior to this (and even still) it wasn’t uncommon for Linux users to boot into a black screen after upgrade due to an Nvidia related problem.  This is different from large ML or HPC clusters which will use licensed drivers and CUDA libraries curated by someone like SUSE or RedHat for a hefty fee.   With regards to procurement, Nvidia has been popular because of how good CUDA is, how powerful/efficient  Nvidia cards have been since Maxwell, and how good their marketing is.",hardware,2025-12-22 18:30:57,7
AMD,nvibl9c,">The 9955hx3d and 9955hx are simply lackluster in terms of battery life with the 275hx giving double the battery life and when unpluged the performance of the 9955hx/hx3d tanks which causes the 275hx to pull ahead when not plugged in.  The problem is that the laptops that those chips go into often don't really care much about battery life at all. Desktop replacement/thick gaming laptops don't really prioritize battery life.   Even more so for the unplugged performance.   For the thinner gaming laptops that might, such as the Asus G16/G14, you see the -H series be used, or even LNL in that one experimental Acer laptop, an entire mobile class of CPUs you ignore in your comment.   With ARL-H, Intel is on par or has a slight lead in battery life, but also has a good bit worse nT perf/watt scaling.   Overall I still think Intel has the better mobile portfolio, but I don't think it's as lopsided as your comment makes it out to be.",hardware,2025-12-23 07:25:12,5
AMD,nvfjih9,"Cant believe i needed to turn on my vpn for that. I did think there would be variance in the test, is there a reason the amd chip doesnt ever reach its max tdp or is it limited? I'd also like to point out the x3d chip isn't on your graph and they behave wildly different.   I guess the thunderbolt is okay ish? And idk what the wifi card thing is about considering how many laptops just use a separate WiFi chip.   >although both intel and amd get smacked by ARM chips  Not surprising its like the whole point of RISC to be specialised it'll only keep getting better.",hardware,2025-12-22 20:47:29,0
AMD,nwq3xab,Cablemod? you mean the guy who was selling faulty cables and had to recall them?,hardware,2025-12-30 13:12:45,1
AMD,nvj2if2,They make the sound when the bicycle in the meme hit the ground,hardware,2025-12-23 11:42:10,2
AMD,nvkfgs4,"So you do know for a fact they don't do that, or is that just you making it up? It's funny that you on one hand deny one theory without much reasoning but take another one for granted.",hardware,2025-12-23 16:30:46,-1
AMD,nwrpqm7,Why does that not stop the tablet if you think that's actually true? Or the silly handheld devices?,hardware,2025-12-30 18:05:29,1
AMD,nveqya6,"Laptops are closer to consoles in alot of ways, tight oem integration, semi custom is also more common, power envelopes and form factor are much similar too.   Also why should we not include AMD's largest consumer GPU segment? Surely if you want to look at whether they can do the volume for an oem you would look at their total track record for oem gpu shipments? Of which the largest will definitely be the shipments to Microsoft and Sony.",hardware,2025-12-22 18:22:17,-15
AMD,nwpx6xv,amazon best sellers lies to you. Its a marketing algorythm.,hardware,2025-12-30 12:25:42,1
AMD,nveyjmu,You wouldn't put the strix halo in a laptop that could fit a 5060 in it i guess then. You'd be putting it in thin and light laptops where you don't really have the power envelope or space for a discrete gpu. So it doesnt really matter what its perf is vs a discrete gpu cause that laptop chassis isn't gonna fit one.  Also on a side note wasn't it really designed for Microsofts god awful Copilot+ PC rollout that flopped heavily i can imagine the inference based APU coupled with another discrete gpu is why?,hardware,2025-12-22 18:59:26,-4
AMD,nvxjblh,"""My boss is irrational and doesn't plan anything out. Just wings everything and expects me to fix it when it doesn't work, with this POS laptop that takes me 8 hours to render a 720p scene, that I should be mastering in 2160p and scaling to maintain quality. But they are too impatient, so everything looks like a streaming video at 240p on YouTube.""   Yeah, and the expectations in those toxic work environments are not based on reality but forced compromise. Because the people in charge hate that they have to use technology...",hardware,2025-12-25 22:03:39,1
AMD,nvev1yg,"I’m saying that the drum beat of bad driver support isn’t the whole picture. If they want something free with no official support they are gonna have issues.  But saying because of that NVIDIA is bad when they have 100,000s of Linux system running the world at the moment is short sighted.",hardware,2025-12-22 18:42:23,-5
AMD,nwpxvuk,linux desktop is a tiny market and is hardly a drop in the ocean when it comes to Nvidia usage in linux.,hardware,2025-12-30 12:30:52,0
AMD,nvfmzgo,[https://imgur.com/a/PmUFBnX](https://imgur.com/a/PmUFBnX) just over double the battery life compared to the 995hx3d and that is a best case scenario for the 9955hx3d because if you are turning the laptop off many times in the day the disparity will grow due to sleep state issues.  The wifi card thing is a hardware thing because intel makes the better wifi cards and while they do have multiple chipset versions(one CNVio that only works with intel cpus and one non-CNVio) Amd still doesn't use them probably due to cost but also there are some driver/hardware issues as even the non-CNVIO Intel BE200 is known to have issues with AMD cpus,hardware,2025-12-22 21:05:52,5
AMD,nwq4sj4,Why are defending a billion dollar company?,hardware,2025-12-30 13:18:14,1
AMD,nx1q3ia,We don't have faulty cables - the angled adapters are what we recalled. Our cables have been great and are used by people around the world. Countless 40 and 50 series GPU owners are using our 12V-2x6 cables.,hardware,2026-01-01 07:15:38,1
AMD,nvkgezz,"....if they do then where are the results? It's not a ""theory"", evo platform is a documented thing intel does with oems.",hardware,2025-12-23 16:35:25,12
AMD,nwpwni6,"> So you do know for a fact they don't do that, or is that just you making it up?  the laptop manufacturers openly stated this, so yes, we know as much as it is possible to know without actually working there.",hardware,2025-12-30 12:21:41,1
AMD,nx1qsb2,It does? The volume of Strix Halo products is miniscule.,hardware,2026-01-01 07:22:29,0
AMD,nvewoos,"They haven't released mobile RDNA 4, that's tells you all you need to know about commitment   If they actually competed on volume, don't you think it would make sense to actually launch some products?  AMD would rather sell them bundled with a CPU for a premium, why their APUs are so prolific",hardware,2025-12-22 18:50:23,25
AMD,nwqvrjp,"I've no doubt that they're fuzzing it up a bit, but it seems unlikely that the bias is in the direction of hiding higher-end laptops. Surely they want you to spend more money?",hardware,2025-12-30 15:45:29,1
AMD,nvf71ma,"Even then you wouldn't though. AMD says the Max+ 395 can sustain 120W and boost upto 140W. That's not really much less than most 5060s do paired with a Ryzen 7 or Ultra 7.   I have an Omen Transcend 14 with a Meteor Lake Ultra 7 and a 4060. The total power budget for them is \~85W because the laptop is powered via a 140W USB-C power adaptor. The 4060 can pull upto 65W. The newer 5060 model can pull 75W because the CPU is more efficient. As a result, my laptop is slower than a full-power 4060 laptop.  But the same is also true for the ZBook Ultra G1A. Also a premium, high-performance 14"" HP. Also runs through a 140W USB-C power adaptor. Also has the same 85W power limit. Is also slower than the ROG Flow Z13 tablet that has a 180/200W power brick with 20-25W higher power budget. It is also not really faster than my laptop for the GPU.  Of course, the ZBook's CPU is a lot faster. I guess a more fair comparison in that sense would be the 385, but that has a slower GPU.   All of that would be fine until you realise that the ZBook costs a LOT more than the Omen and the laptops otherwise are very similar. Similar battery life, same OLED display, very similar keyboard, trackpad and speakers, similar build quality and general handling.",hardware,2025-12-22 19:42:40,13
AMD,nvifhf3,theyre talking about desktop usage. nvidia lacks some features and gets performance drops in some cases in desktop linux.,hardware,2025-12-23 08:02:09,1
AMD,nvft7y1,Oh what laptop was this test ? I assume it was amd vs Intel versions of the same one.   Also the wifi card thing doesnt matter all that much anyways the perf of intel vs qualcomm vs cnvio is really not going to matter much considering they are all compatible with the same standards so you can get a better one if you want,hardware,2025-12-22 21:38:38,1
AMD,nwqk5l4,I see you have no argument.,hardware,2025-12-30 14:46:44,1
AMD,nx24y9w,"That's exactly the issue! But the existence of the tablet product shows they could be made, no problem, if vendors wanted.  Do you forget the random cheap chinese devices with these chips? They definitely aren't too expensive to be viable.",hardware,2026-01-01 09:52:34,1
AMD,nvf0bie,"Yeah what happened to that? Was it power?  >If they actually competed on volume, don't you think it would make sense to actually launch some products?  >AMD would rather sell them bundled with a CPU for a premium, why their APUs are so prolific  I don't want to be cynical and blame AI but I think they were putting their production into datacenter stuff instead",hardware,2025-12-22 19:08:22,2
AMD,nx1qd8s,"the bias is in direction of what they think you might get lured into buying. If their algorithm thinks you are mire likely to buy cheaper laptops they will show you cheaper laptops. Also another point about that list is that they list every SKU seperately, so if a product has many SKUs it will likely never be in the top list.",hardware,2026-01-01 07:18:21,1
AMD,nvf9deh,"The tdp for the 5060 in the transcend 16 is 140W which is more than the strix halo by itself which is what I really meant. I mean its the whole point of a integrated GPU less power, less space, less engineering complexity unified memory so on and so forth.",hardware,2025-12-22 19:54:30,-1
AMD,nvfyiwu,yes same exact laptop the XMG Neo apart from ofc the superior wifi card on the intel laptop and thunderbolt.,hardware,2025-12-22 22:06:16,2
AMD,nwqu12b,"Well since you asked so nicely, here are just a few examples   [https://www.youtube.com/@NorthridgeFix/search?query=4090](https://www.youtube.com/@NorthridgeFix/search?query=4090)   [https://www.youtube.com/watch?v=p0fW5SLFphU](https://www.youtube.com/watch?v=p0fW5SLFphU)  [https://www.youtube.com/watch?v=Y36LMS5y34A](https://www.youtube.com/watch?v=Y36LMS5y34A)  [https://www.youtube.com/watch?v=kb5YzMoVQyw](https://www.youtube.com/watch?v=kb5YzMoVQyw)  [https://gamersnexus.net/gpus/12vhpwr-dumpster-fire-investigation-contradicting-specs-corner-cutting](https://gamersnexus.net/gpus/12vhpwr-dumpster-fire-investigation-contradicting-specs-corner-cutting)",hardware,2025-12-30 15:37:08,1
AMD,nvf187l,">but I think they were putting their production into datacenter stuff instead    As is Nvidia, who are committing more   But despite that, Nvidia still commit enough volume to supply the gaming market",hardware,2025-12-22 19:12:58,10
AMD,nvg3r3x,You seem really hung up on single port type and WiFi card huh,hardware,2025-12-22 22:34:45,0
AMD,nx1puku,"northridge fix? so the guy fixing the cards burned by the faulty third party cable by cablemod? You do realize he is not representative of the market, yes?",hardware,2026-01-01 07:13:12,1
AMD,nvf3oi6,"I mean for a company having volume problems 73% year on year revenue increase in that segment seems pretty damn good. I mean looking at their financials I'm finding your point hard to believe, they had record radeon sales and increased gaming revenue from 0.5 to 1.3 Bn yet they're having volume issues...",hardware,2025-12-22 19:25:25,1
AMD,nvg4ssu,? no I was just saying they are the same exact laptop spec wise except those two parts although those two parts aren't really important in terms of power draw which would make the comparision accurate.   If they instead had different screens like oled vs ips that would cause a difference in power draw.,hardware,2025-12-22 22:40:29,3
AMD,nx1q6o1,"As mentioned above - our cables are not failing, our recall was specific to the angled adapter. Countless people worldwide are using our 12V-2x6 cables, and many have shown that they have more stability using our cable compared to cables that even come stock with the PSU and other third party competitors.",hardware,2026-01-01 07:16:31,2
AMD,nwloj0c,Please don’t tell me they’re going to slap the ugly HyperX logo on the lid,hardware,2025-12-29 19:55:56,16
AMD,nwm66sz,"Seems like the OLED displays and the inclusion of the 9955HX AMD chips are the big changes for the next year.     Shame the most powerful AMD CPUs are not going to be paired with the best GPUs and cooling in the Max systems.     HP used the 8940/45HX AMD chips in the regular Omen this year already but it's a strange mismatch putting that with a 5060 and a weaker thermal system that requires a power limit restriction. At that point, why bother with a 16 core CPU that's going to be hamstrung from the beginning?",hardware,2025-12-29 21:22:28,9
AMD,nwlzyl4,"Kinda weird to have 15"" relaunching when they have dedicated 14"" and 16"" lineups",hardware,2025-12-29 20:52:24,5
AMD,nwp3xir,just wish the thermals don’t implode and prices don’t make us cry,hardware,2025-12-30 08:04:48,3
AMD,nwozxw1,"wait what, Omen 15? whats gonna happen to the Transcend 14 then  having a 14, 15, *and* 16 inch laptop feels way too close with each other",hardware,2025-12-30 07:28:20,2
AMD,nwlxflb,Tasteful laptop design.   Difficulty: nearly impossible.,hardware,2025-12-29 20:39:59,11
AMD,nwvpp7c,I have never seen the HyperX logo before. Maybe they'll only put the OMEN branding.,hardware,2025-12-31 07:50:11,2
AMD,nwuks1r,the chiplet design from AMD are trash in mobile anyway due to their high idle power consumption.   You almost always gonna pick Intel for the same price or if the premium is small enough.,hardware,2025-12-31 02:53:08,7
AMD,nww2ud7,9955HX is worthless if not paired with 5070ti and up.,hardware,2025-12-31 09:55:07,2
AMD,nwsroc8,"> the most powerful AMD CPUs are not going to be paired with the best GPUs and cooling  I personally think is a shame that the 3xx HX product line isn't more widely available in gaming laptops(probably due to AMD's poor supply). Since going laptops with a ryzen 9xxxHX are more of a desktop replacement machine, due to the TERRIBLE battery life. To me it makes no sense buying a laptop to have it connected at all times for twice the price of a more performant desktop alternative.",hardware,2025-12-30 21:04:41,2
AMD,nwqn2se,>why bother with a 16 core CPU that's going to be hamstrung from the beginning  Well. Considering this has been happening for a long time. I would go for the simplest answers,hardware,2025-12-30 15:02:09,1
AMD,nwvpul7,Yeah and that too after 4 years! Why did they cancel it in the first place?,hardware,2025-12-31 07:51:33,1
AMD,nwv32tv,"From what I’ve heard they’re discontinuing the Victus line so the 15 will be a lower end model with worse build quality and maybe a couple features missing.   The Transcend 14 is a premium model. The Transcend is also only 140W USB-C so the GPU is power limited. The 15 might use the ugly barrel jack. Of course, a 240W USB-C could fix it, but it’s not happening",hardware,2025-12-31 04:48:56,1
AMD,nwqtwhf,> the simplest answers  Because the windows laptop market is so fragmented it’s broken.,hardware,2025-12-30 15:36:31,3
AMD,nwvq0lh,Why do companies keep doing this to us... It's almost like we aren't allowed to have nice things.,hardware,2025-12-31 07:53:05,1
AMD,nwwtfny,"That's too bad.  15 inches is the right size for a laptop that has a screen big enough to play on yet is small enough to carry around without much inconvenience.  16 really lends itself to living on the desk, and 14 is portable but you usually have compromises on thermals, battery and most importantly screen size.",hardware,2025-12-31 13:33:14,1
AMD,nw1bmjg,Will it come with vram,hardware,2025-12-26 15:35:35,280
AMD,nw1goyh,"I thought this was gonna be UDNA, not RDNA5.",hardware,2025-12-26 16:03:11,104
AMD,nw1na46,"Makes sense why sony want release ps6 in the end 2027,iirc rdna2 released in fall 2020 as ps5",hardware,2025-12-26 16:38:23,48
AMD,nw1bjrk,I'd be surprised if it didn't.,hardware,2025-12-26 15:35:09,32
AMD,nw1tmlg,18 months out the rumour might as well be a nothing burger.,hardware,2025-12-26 17:11:56,21
AMD,nw1cqmn,I’ve been hearing 2027 more often from the rumor mill sources and it’s incredibly disappointing. 2026 should have been a banner year for PC parts and now it’s looking like just the new CPUs are launching and nothing else,hardware,2025-12-26 15:41:46,41
AMD,nw1g4rx,in 2 weeks : OpenAI just bought all RDNA5 GPU stock until 2030,hardware,2025-12-26 16:00:12,10
AMD,nw20y8a,Completely depended on ram avaliabllity,hardware,2025-12-26 17:50:33,3
AMD,nw2j8ht,"RDNA6 rumored to launch in mid-2029. There you go videocardzzzzz, no need to post one more rumor about it. 🤣",hardware,2025-12-26 19:25:50,10
AMD,nw1vvrf,Oh neat.  More than a year and a half away.  Must be a slow news day.,hardware,2025-12-26 17:23:53,8
AMD,nw1vwce,"Cool, just in time for them to banish RDNA 3 and 4 to the maintenance branch.",hardware,2025-12-26 17:23:58,7
AMD,nw45xis,"Mid-2027 feels far, but at least it gives RDNA4 time to shine first.",hardware,2025-12-27 00:58:52,2
AMD,nw1c3dy,"Considering the current situation we are in, RDNA 5 GPUs will be BYOV (Bring your own VRAM)",hardware,2025-12-26 15:38:11,4
AMD,nw3he4q,"Damn, it's gonna be the new Vega, isn't it.",hardware,2025-12-26 22:32:52,2
AMD,nw34krp,GPUs have a two-year cadence. It's obvious that the next generation will come H1 '27. Until then we might get some refreshes from team red and green while team blue is on track to actually deliver some proper mid range GPUs.,hardware,2025-12-26 21:22:35,3
AMD,nw20e7q,4090 level raster perf and I box up my 4090s and buy these.  Nvidia drivers on Linux have me at wit's end,hardware,2025-12-26 17:47:39,1
AMD,nw4c5cb,"This will get pushed back. High speed memory will still be in shortage and expensive.  There will be a refresh of RDNA4. Most likely, with more and faster GDDR6 memory. That is doable.  9070XTX or 9075XT 24Gb with 22 or 24Gbps GDDR6 memory. They can use the 9700AI pro board, but just fit lower density memory to half the board 16+8 = 24. Still on a 256bit bus.  The low density ram won't be very expensive.   UDNA seems to be looking more and more like UDNA=RNDA5. AMD is putting a lot of effort into making RDNA the majority of the future for AI. The AI 9600Pro is a great example of that.    AI 9600 Pro with 64GB memory would be nice. That would push AMD into more data centre spaces.    I also think top end AMD graphics will be back on HMBe..",hardware,2025-12-27 01:38:22,0
AMD,nw1nmgn,So AMD lied about RDNA4 not competing on high-end in order to launch RDNA5 faster than Nvidia's 60 series,hardware,2025-12-26 16:40:11,-8
AMD,nw1bc0m,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-26 15:33:57,0
AMD,nw1cr61,That’s DLC,hardware,2025-12-26 15:41:51,159
AMD,nw1il6b,"128bit is 4x 32bit chips to 4x vram controllers in gpu. 3GB gddr7 for 12GB.    192bit is 6 chips. 18GB.   256bit is 8 chips. 24GB.   384bit is 12 chips. 36GB.   512bit is 16 chips. 48GB.    They’ll do 256bit for the xx70 gpu and 128bit for the xx60 gpu as that is the most efficient design to keep the gpu chip smaller. The more vram controllers in the gpu, the larger itll be in area.   If they decide to go “5090” big, it’ll be interesting. They would want to go for 512bit gddr7 to compete but 384 may be possible if they dont wanna go that huge.   But more overall vram is likely, but more vram chips is not. Theyll use the same # of ram chips but at the 3GB density.   I dont know why i typed all of that nonsense… my brain got the tunnel vision and i had to do it. I need my coffee. Byeeeee",hardware,2025-12-26 16:13:19,45
AMD,nw2q8xz,"It's FSR 5, which eliminates VRAM using AI.",hardware,2025-12-26 20:04:00,5
AMD,nw1gtcr,yes. you can subscribe to 1Mb monthly vram for $50,hardware,2025-12-26 16:03:50,20
AMD,nw3me14,With a subscribtion for RAM. Unlock what you need.,hardware,2025-12-26 23:01:35,3
AMD,nw229uc,"Actually, maybe no. Some rumors say lower end parts will actually come with DRAM instead of VRAM. There  have been claims from like AMD or Mark Cerny or someone, that also said they are trying to drastically reduce memory bandwidth requirements of GPUs. So there might be truth tot hat rumor. It's also something they absolutely need to do if they want to create huge APUs for laptops, or other mobile gaming platforms like the Steam Deck 2 or rumored PS6 handheld. Maybe the PS6 even, will use DRAM instead of VRAM.",hardware,2025-12-26 17:57:26,4
AMD,nw21ek2,"No, we’ll have to download it from OpenAI’s Stargate monopoly.",hardware,2025-12-26 17:52:55,2
AMD,nw3r6e0,"I wonder why AMD and NVidia do not just put the RAM in the chip? Doesn't the M5 chip and the Strix Halo chips have the memory right on the die? Seems like they should be able to just do this with discrete GPU's as well? I know it's not simple and would take years to do, just wondering why they have not done it already.",hardware,2025-12-26 23:30:16,2
AMD,nw1nqy1,Honestly don't know which is which at this point,hardware,2025-12-26 16:40:49,63
AMD,nw24pac,From what I can tell udna and rdna5 are referred to interchangeably in leaks. We will likely have to wait for official confirmation to be reasonably sure about either.,hardware,2025-12-26 18:10:02,41
AMD,nw2ia54,It's gonna be some kind of DNA,hardware,2025-12-26 19:20:44,13
AMD,nw5irc4,No one knows not even AMD. At FAD they didn't even commit to a name. Nothing. This confusion will prob continue until Cerny does his Road to PS6 in 2027 :(,hardware,2025-12-27 06:41:54,4
AMD,nw74fkz,"UDNA launched already, as amdgpu-spirv. It’s AMD PTX, not an actual uarch.",hardware,2025-12-27 14:51:54,1
AMD,nw3n4py,UDNA and RDNA5 are the same thing,hardware,2025-12-26 23:05:58,1
AMD,nw3kmip,"Yes, let's hope the memory shortage doesn't ruin those plans.",hardware,2025-12-26 22:51:18,12
AMD,nw4jb7l,"You're on crack if you think the PS6 is coming in 2027 lol. Memory shortages mean Sony will be looking at at 12 month delay at a minimum, possibly more if things don't improve. You can't release a next gen console with the same, or maybe 24GB Vram, that would be a pathetic jump.  Sony will want this console to last 10 years at least.",hardware,2025-12-27 02:24:12,6
AMD,nw63uls,"AMD Radeon alone is nothing burger, unless AMD increase their GPU production by 5 times. Given how AMD only hold 7% dGPU market share now, so yeah 5 times just about right.",hardware,2025-12-27 10:04:33,3
AMD,nw4skc8,you can say that again,hardware,2025-12-27 03:24:42,1
AMD,nw1eo8w,There will be a horrible DRAM shortage likely for the entire year.,hardware,2025-12-26 15:52:21,82
AMD,nw1h9lx,Why? They always release it in 2 years cycle. RNDA 4 is 2025.    Same for RTX 6000. Why everyone talks like it's a big news,hardware,2025-12-26 16:06:15,50
AMD,nw3ktmv,Why would 2026 be a banner year? RDNA4 and Blackwell just launched this year. Normally it's a 2 year cadence for new GPU architectures.    2026 will be great from a CPU perspective. We get Zen 6 and Nova Lake.,hardware,2025-12-26 22:52:26,9
AMD,nw1iv3h,Would you prefer they launch in peak memory hellscape?,hardware,2025-12-26 16:14:48,15
AMD,nw8cehn,And basically nobody in the DIY market will buy a new CPU without some new RAM to put into a new motherboard...,hardware,2025-12-27 18:38:27,1
AMD,nw4izvp,"Going to be a ""slow news"" year. Youtubers are going to suffer with really desperate content in the next year, since there will be very minimal hardware launches.",hardware,2025-12-27 02:22:08,9
AMD,nw4j6s2,"It's going into virtually everything from the PS6, and PS6 handheld, all the way to next generation Xbox. Maybe even Steam Deck 2. It can't be Vega.",hardware,2025-12-27 02:23:23,7
AMD,nw39pml,"Only Nvidia ever had any routine two year cadence and even they've gotten off that a bit with the 50 series.    AMD have a very mixed record and never followed any real routine.  Sometimes they'd release just like one new GPU in a year on a new architecture, and then the next year they'll only release low-mid entries on a new architecture, and so on.    There's no rule that says we couldn't start seeing more like 2.5 years between releases.  Or whatever.  There's no rules about this stuff at all.",hardware,2025-12-26 21:50:19,6
AMD,nw518t8,">Nvidia drivers on Linux have me at wit's end  It's been pretty stable for a while, like two years.",hardware,2025-12-27 04:24:09,3
AMD,nw4jlxw,">9070XTX or 9075XT 24Gb with 22 or 24Gbps GDDR6 memory.  How would they do that, if 3GB GDDR6 memory modules does not exist, and no one is investing in developing it?  What they could do is a create a whole new 500mm^(2) die with a 384 bit bus, 12 memory modules, and 96 CUs of RDNA4. They have claimed that RDNA4 is pretty modular in design, so it wasn't hard to double the 9060xt design, to create the 9070xt. It's really double the chip. So 96 CUs would not be difficult for them.",hardware,2025-12-27 02:26:08,1
AMD,nw1w1tn,"I feel like AMD teases the possibility of releasing before Nvidia every gen since RDNA2, and then they always do the Nvidia - $50 approach.",hardware,2025-12-26 17:24:46,15
AMD,nw22zrl,"AMD will never, ever, launch their products before Nvidia, unless they know for sure how big the performance gap and Nvidia pricing is. Because it will be a disaster for their already weak sale, if they get undercut by Nvidia.    You already know this year they had to delay and then change the price of the 9070xt down to 600$ from 700$, because the 5070ti was 50$ cheaper than they expected",hardware,2025-12-26 18:01:10,12
AMD,nw1x7qx,Where did AMD say that?  Got a quote?,hardware,2025-12-26 17:30:57,8
AMD,nw2d0jj,What are you even talking about? There is no RDNA 4 product that competes with high end Blackwell.,hardware,2025-12-26 18:52:54,5
AMD,nw1ewll,I emailed https://downloadmoreram.com and asked if they would add VRAM to their lineup to help with the shortage.,hardware,2025-12-26 15:53:34,71
AMD,nw5yerp,PC brought to you by Paradox,hardware,2025-12-27 09:10:47,2
AMD,nw1vu5p,"People make this sort of argument in many generations that they'll use these 'in between' memory capacity chips and they just never really do.  Cuz of the lower volume, they tend to not be great in terms of cost per GB compared to high volume memory chips.    Basically, your whole premise relies a lot on Nvidia wanting to give us better VRAM capacity and selling us midtier GPU's with midtier naming and pricing(and low=low, high=high).  That's a lot of bones Nvidia will supposedly throw us in a time when everything is more expensive for them.   I just dont see it.  I think we'll get a continuation of what they're doing now, just with higher prices.  They're not gonna suddenly start to care less about having good margins and there's no indications consumers are ever gonna stop buying their GPU's.  They've got no incentive to be nice to us.",hardware,2025-12-26 17:23:39,18
AMD,nw1uhag,"I would love to see more VRAM but that would mean more expensive cards, which the original question was referencing due to the memory price surge.",hardware,2025-12-26 17:16:26,9
AMD,nw22kb1,> 512bit is 16 chips. 48GB.   you really think you're gonna see $1000 worth of VRAM on an AMD card?,hardware,2025-12-26 17:58:57,11
AMD,nw6814q,"Once you've gone with a giant bus for the top die it's difficult to go back since professional clients expect VRAM to go up every generation. 512 on die and maybe 448 for the 6090 if yields are bad.  Also would expect all the other buses to go down and the gap between top and mainstream to widen. If you look at rumored RDNA5 / PS6 / NextBox specs all the buses are down per tier. PS6 is down to 160-bit from 256-bit on PS5. Buses don't scale as well as logic with node shrinks so if you keep the same bus then more and more of the die will be the bus until the bus is bigger than logic. That's why the 70 class used to be 256 and is now 192, 60 class went from 192 to 128.  Also you can see this when Sony and AMD are talking about 'Universal Compression'. You need to work around this and make the uArch more bandwidth efficient.",hardware,2025-12-27 10:45:40,4
AMD,nw2u7rl,"There was a rumour that RDNA 5 would top out at 384 bits, matching with the rumour it would not be faster than the 5090.",hardware,2025-12-26 20:25:49,1
AMD,nwiyr86,Screw GDDR! They can go with 512bit of LPDDR5X at this point. Just put some decent SRAM L2 cache onboard and get \~600GB/s but with insane capacity,hardware,2025-12-29 10:58:48,0
AMD,nw2r4en,"Maybe RDNA5 is just a video decoder you can connect to a data center with for cloud gaming!  But only for 100 hours a month, more would be an unhealthy habit.",hardware,2025-12-26 20:08:48,5
AMD,nw1n4ot,That's Nvidia's plan.,hardware,2025-12-26 16:37:35,8
AMD,nw3k991,It increases in memory every month by 10% but if you miss a subscription it goes back down to 1Mb.,hardware,2025-12-26 22:49:10,0
AMD,nw5iisn,"It's basically a Zen strategy for RDNA5. Design costs are outragious on N3, so it's better to share GPU chiplets with mobile instead of designing seperate for each market. Makes sense to me.  Considering all the stuff they're prob working on for the first time mobile GPUs might finally not be completely BW choked and Medusa Halo could be insanely powerful.",hardware,2025-12-27 06:39:50,2
AMD,nw3sb0n,Both have had HBM SKUs,hardware,2025-12-26 23:37:07,1
AMD,nw3dyuz,Expensive. It will be expensive. It has been estimated Rubin desktop comes earlier in 2027 which will see a healthy price increase across the board.,hardware,2025-12-26 22:13:28,19
AMD,nw2czrj,"AMD was pretty clear on this when they announced UDNA last year. The exact wording was:  ""So, part of a big change at AMD is today we have a CDNA architecture for our Instinct data center GPUs and RDNA for the consumer stuff. It’s forked. Going forward, we will call it UDNA. There'll be one unified architecture, both Instinct and client [consumer]. We'll unify it so that it will be so much easier for developers versus today, where they have to choose and value is not improving.""  The only way RDNA5 makes sense would be as informal codenames for a group of products that were in the planning phase of development before this decision was made. I seriously doubt there will be any formal RDNA5 branding on anything, that would be like if they had called the Radeon 5700 XT ""GCN6"" rather than""RDNA1"".",hardware,2025-12-26 18:52:48,32
AMD,nwjee9x,"Argh. They kept making this big deal about UDNA being this glorious future, they really need to commit.",hardware,2025-12-29 13:03:50,2
AMD,nw5k19m,"24GB is fine if they fully lean into nextgen paradigms (neural textures, neural rendering (MLPs) and procedural content) and API (workgraphs + clean slate API overall).   12.5GB -> +22GB if they include DDR5 similar to PS5 Pro. But rn most rumour suggest it'll be 30GB so yeah zero chance it'll be 2027. Sony can't commit to anything until this current mess has a real end in sight.",hardware,2025-12-27 06:53:17,7
AMD,nw4nv5j,"Sony could have signed contracts for components before the price increase, and the supplier will be obligated to sell them at the price at the time the contract was signed,at least for several million units",hardware,2025-12-27 02:53:32,2
AMD,nw1f51v,Indeed. The big three are building capacity but the most serious chunk of that is being allocated to enterprise to ensure they can control the price of consumer memory.,hardware,2025-12-26 15:54:52,20
AMD,nw2fhpu,Well be lucky if by Black Friday 2027 RAM prices go back pre surge pricing.,hardware,2025-12-26 19:05:51,3
AMD,nw3kws5,Won't be over even in 2028 but supply and logistics should be less bad by then.,hardware,2025-12-26 22:52:56,1
AMD,nw1glrx,Make it the rest of the decade.,hardware,2025-12-26 16:02:43,-6
AMD,nw1wo6e,"RDNA4 in 2025 already bucks the '2 years' cycle since RDNA3 was 2022.  People were kind of hoping that with RDNA4 being something of a 'stopgap' product line/architecture, that RDNA5 would have gotten things back on track to the same 2 year cadence as before.",hardware,2025-12-26 17:28:04,13
AMD,nw1nc4n,AMD bad.   Need more RAM.,hardware,2025-12-26 16:38:40,7
AMD,nw89y8r,gotta generate clicks and engagement somehow.,hardware,2025-12-27 18:26:18,1
AMD,nw1x197,There's no suggestion that things will be any better by mid 2027.,hardware,2025-12-26 17:29:59,3
AMD,nw1me62,"Yes, because then that would allow a good half a year for prices to fall as few will buy at inflated prices. GPUs already aren’t selling now at inflated prices, and they certainly won’t in 2026 when AMD and Nvidia jack up prices due to claims of a memory shortage",hardware,2025-12-26 16:33:41,-7
AMD,nw3rpwv,50 series was delayed by 1 quarter.,hardware,2025-12-26 23:33:34,3
AMD,nw3ax75,"Its what I got, it's enough for me for the rest of my gaming ""career"", and the drivers won't be an afterthought to the people with access to meddle with them",hardware,2025-12-26 21:56:53,0
AMD,nw587q4,"I am happy to hear that, truly.   For three years it has been hit or miss.  There is always something off.  Currently its primary weirdness is my two portrait screens coming back from standby, or much less frequently upon first boot, in landscape along with poor CP 2077 perf.    The funny thing about Cyberpunk is it was fine until they got frame gen into the last Ubuntu driver update a number of months ago.  Now it's a stuttering mess.   there is a laundry list of shell game/whack-a-mole items that have come, gone, came back, maybe got fixed again.   I have this problem on a two separate 4090 rigs on two different hardware platforms (5800x3d/x570 and 9800x3d/x870e) and the jank is normally quite the sameness for a given distro and update level... i am bad about abating distros that have fallen into disuse.   I am hardly a linux pro but I have a comfort level with it.  I will happily shelve the 4090s when the aforementioned conditions have been met.",hardware,2025-12-27 05:14:54,-1
AMD,nw59wvj,">How would they do that, if 3GB GDDR6 memory modules does not exist, and no one is investing in developing it?     There are several ways to do it.  * Use the Ai Pro 9700 board, but fit only 24Gb. 8 x 2Gb on one side, and 4 x 2Gb on the otherside. The issue of this is halving the bus size for that last 8 GB. So unlikely. * Use the Ai Pro 9700 board, but fit 8 x 2Gb on one side and 8 x 1Gb on the otherside.  1 Gb chips already exist. * Just software lock out the last 8GB.    The board exists, its already double sided, this would be easy and cheap to do. 1Gb chips would be pretty cheap I imagine. Partners already have this board in production and in supply, it would be very low risk for them. It uses parts that are already in the supply chain. Even if they fit it with faster memory, that likely be shared with other products and historically that is what AMD has done. Faster GDDR6 has been around for years at this stage. No new drivers, just a bit of bios code change. Chinese remanufacturers already do this with Nvidia products without manufacturer support.  A whole new die would be expensive. They can certainly do that but that would take years and create an expensive and niche product.",hardware,2025-12-27 05:27:54,0
AMD,nw39497,"I dont think they've ever done any such thing.  Again, can y'all show me the quotes where AMD are saying these things?    I think y'all get all mixed up with rumors and somehow they turn into facts in your heads and somehow it's what AMD said and not just some random person on the internet.",hardware,2025-12-26 21:47:09,-5
AMD,nw3j65y,"These guys are awesome, what with this crazy memory shortage I don't know what we would do without downloadmoreram.com.   If you're downloading some ram don't forget to use code F*cktheRamCartel at checkout for a sweet 13.37% off your order.",hardware,2025-12-26 22:42:58,10
AMD,nwivw41,I can only find DDR4 there :'(   I wish they had DDR5 as well.,hardware,2025-12-29 10:32:50,1
AMD,nw2xhrf,3GB density chips are the new kid in town for gddr7. By mid 2027 it may be the primary density for mass production the same way that 2GB replaced 1GB chips a few generations ago and they stopped producing 1GB chips.   But if 2GB density is still available and significantly cheaper theyll do that and save the 3GB for their pro lineups. It does depend on the cost and whats got the most active production line for gddr7,hardware,2025-12-26 20:44:00,10
AMD,nw5h724,It's the new standard. I don't think they'll bother with 2GB except for very low end. 3GB modules are also much faster on path to 40gbps and a little further out we get 4GB modules. Not inconceivable that future x60 tier cards go down to 96bit bus. Really depends on how much AMD and NVIDIA can improve cachemem efficiency and little compute progression there is.,hardware,2025-12-27 06:28:09,2
AMD,nw2fam2,I can see a 2028 RDNA 5 refresh with 4GB modules.,hardware,2025-12-26 19:04:48,2
AMD,nw5hioo,"They'll just gimp mem PHYs and offer cards with a little more VRAM. Look at rumoured 18GB AT2 card, although there will prob be a refresh based on 4GB to satisfy the VRAM crowd as u/Dangerman1337 said.",hardware,2025-12-27 06:30:59,1
AMD,nw24thg,"Sure why not, they'll just put it on the pro lineup and charge double",hardware,2025-12-26 18:10:38,12
AMD,nw3k5ii,"Yea well no one predicted the memory shortage not even Samsung, Micron and friends.",hardware,2025-12-26 22:48:35,5
AMD,nw5hoz9,"That's based on the cut down AT0 config. The full AT0 die caps out at 512bit like 5090, well at least according to rumours.",hardware,2025-12-27 06:32:30,2
AMD,nw3jyi0,No it'll be faster than 5090 but it won't be faster than 6090. The next gen Xbox is supposed to reach above 5070ti performance and that'll be a smaller chip on a 192bit bus. Plus RDNA5 is getting big architectural improvements and a die shrink unlike RDNA4 that used the same node as high end RDNA3.,hardware,2025-12-26 22:47:28,1
AMD,nw39aka,"There's no way the architecture is exactly the same imho, as the typical workloads on CDNA are too different from those on RDNA.",hardware,2025-12-26 21:48:04,7
AMD,nw2mm7z,It was written at one point that RDNA is still the gaming architecture (as used in APUs as well) and CDNA the computing architecture but the new platform combining both is UDNA (RDNA5 + CDNA4 = UDNA1) while GCN was a different architecture to RDNA,hardware,2025-12-26 19:44:08,10
AMD,nw8t5fw,"Yeah I still think the PS6 is Sony's last traditional 'console'. Everything will be cloud based beyond 2035-2040. They'll want to make this console last a really long time....ie more beefy CPU and RAM initially, with a GPU upgrade 4-5 years down the line.   With the PS5 still selling like crazy, what incentive does Sony even have to launch a PS6 at all until 2029?    I can't see one tbh, devs are only starting to really push the PS5 now.",hardware,2025-12-27 20:05:29,2
AMD,nw8snln,"They wouldn't have signed contracts more than 2 years out for a console lol. They're still designing the hardware, let alone ordering mass quantities of compliments for it. Again, don't expect PS6 till Q4 2028 at the earliest. Distinct possibility of it being a 2029 launch now as well.",hardware,2025-12-27 20:02:49,2
AMD,nw5jq22,Very unlikely. The G7 memory chips they need have barely begun real HVM. Rn limited to RTX PRo cards and select Mobile on NVIDIA side + you don't sign LTAs this early on.,hardware,2025-12-27 06:50:29,1
AMD,nw2b8gt,And if they are reinvesting everything - who knows what will happen once the bubble bursts.,hardware,2025-12-26 18:43:45,2
AMD,nw1hfzk,"Nah, this too shall pass. The current prices are too big of an opportunity, and no-one can lock up dram with patents. Either demand will fall, or supply rises until margins come back down.",hardware,2025-12-26 16:07:12,20
AMD,nw1z48c,RDNA3 was released in DECEMBER 2022. Calling it 2022 is misleading,hardware,2025-12-26 17:41:02,20
AMD,nw3nv3e,"AI isn't just unprofitable, it's bleeding money at a ridiculous rate. Another year/4 quarters of massive losses on AI should hopefully lead to *something* changing.   I can't imagine companies just throwing billions into a blender for longer than another year. Microsoft is already scaling back.",hardware,2025-12-26 23:10:21,3
AMD,nw1vyau,"Claims, lol",hardware,2025-12-26 17:24:15,7
AMD,nw3mugx,"Nvidia already announced a next gen vera rubin successor to the rtx pro 6000 with 128gb vram for late 2026. At 512 bit, it should have 4gb gddr7 vram chips. leaked sk hynix roadmaps also showed higher capacity vram chips becoming a thing iirc so it makes sense.",hardware,2025-12-26 23:04:17,7
AMD,nw37r1q,">3GB density chips are the new kid in town for gddr7. By mid 2027 it may be the primary density for mass production the same way that 2GB replaced 1GB chips a few generations ago and they stopped producing 1GB chips.  Standard chip capacity goes up in doubles.  3GB is new, but it does not mean it's going to be the new standard.  We had 1.5GB chips as well in between 1GB and 2GB.  These sorts of in-between capacity chips dont tend to get used in consumer GPU's.  Not saying they absolutely wont here, just not sure why people are so incredibly confident they will.",hardware,2025-12-26 21:39:47,3
AMD,nw5hw5z,"That'll be very easy to accomplish. It'll prob match or beat 5080. GFX13 = Massive architectural overhaul, N3P and 70CUs.  Very interested to see what RDNA5 is about. Just a shame it's prob +2 years away unless VRAM mess sorts itself out in early 2027.",hardware,2025-12-27 06:34:17,2
AMD,nw4mkyp,It won't AMD are incompetent,hardware,2025-12-27 02:45:18,3
AMD,nw3xazq,What makes you think any of this?did a reputable leak something about this,hardware,2025-12-27 00:07:19,4
AMD,nw3kjx4,It won't be exactly the same but there will be more AI tech in the gaming version that would have normally been exclusive to cdna.,hardware,2025-12-26 22:50:53,6
AMD,nw4jco7,"I was under the impression it was on a logic block basis. For example, in the compute market you obviously don't need video out, texture mappers, RT hardware and the like, but you do need SIMD cores, a memory controller, and matrix math cores. With the split architectures they were pulling these universally required blocks from two different bins. A unified architecture means advances for one use can still benefit the other (say a really awesome scheduler or something). If they really double down on chiplets and can optimize things in ways that aren't obvious to someone like me, they might even be able to share some amount of silicon (obviously not all, but I think it can be more than just SRAM).",hardware,2025-12-27 02:24:28,4
AMD,nw5j2px,Then AMD does what NVIDIA does. Makes a shared foundation with extensions on top (logic blocks and cache customizations) for gaming and HPC respectively.,hardware,2025-12-27 06:44:42,4
AMD,nw32ytp,"I don't personally recall seeing that and very well might have missed it, but it does seem to run counter to the UDNA announcement.",hardware,2025-12-26 21:13:54,4
AMD,nwbhkjy,"You're prob right, especially considering how good GFN has gotten and how hard most normie gamers have with distinguishing frame-gen on vs off in latency. Add a controller to the equation and that becomes even less relevant.  We'll see just remember that prob even more so than PS4->PS5 PS6 has a very long crossgen period extending well into 2030s before it can shine, even if it launched in 2027 which seems next to impossible rn.",hardware,2025-12-28 05:31:59,3
AMD,nw2gzxk,"> if they are reinvesting everything  They are not.  We know from the earnings calls of those big memory suppliers that they are very skittish about building out capacity.  Which also tells you that this shortage is, in their eyes, **very** temporary.  Otherwise, they'd be on the hook for huge lawsuits as shareholders could sue for the missed opportunity.  I have a hunch that DRAM pricing will be a _lot_ better end of 2026 and basically where it is today in mid-2027.",hardware,2025-12-26 19:13:50,11
AMD,nw1iunm,"Or DRAM makers suspect this to be a bubble and will gladly take the money now but won't speed up production. If you build fabs beyond what was already planned you have to know they'll still be needed 5+ years from now, not just right this moment.",hardware,2025-12-26 16:14:44,13
AMD,nw1wdbo,"DRAM makers do not want to overreact and build up a ton of new capacity to meet demand, leaving themselves extremely vulnerable if that demand ever drops.  Why put themselves at such risk when they can just jack up prices 300%?",hardware,2025-12-26 17:26:28,3
AMD,nw1sr49,"Exactly, we have seen this before just not to the extent it is now. Vendors will ramp up production and suddenly there will be a market change and prices will crash. This cycle will likely take longer but it will happen.",hardware,2025-12-26 17:07:18,3
AMD,nw2flj8,RDNA 3 was supposed to be earlier than December 2022 originally.,hardware,2025-12-26 19:06:25,13
AMD,nw38dym,"Oh good lord. smh  Either way, it's a terrible claim.  Nvidia are the ones that (used to) do a 2 year cycle.  AMD has been all over the place, often only doing like one or two new GPU's per new architecture, which could sometimes release every year or so.",hardware,2025-12-26 21:43:16,0
AMD,nw1wool,"Prices went up before supply lost. This is similar to how the middle men of gas and energy will jack up prices of gasoline the same day that crude prices spike. That’s not supply and demand, that’s just market manipulation. We haven’t lost DRAM supply at all, either.",hardware,2025-12-26 17:28:08,2
AMD,nw3ct85,Historically maybe...  If memory stays expensive companies would be more willing to produce in between capacities at comparable $/GB. Because the difference between boards populated with 3 or 4 GB chips would balloon.,hardware,2025-12-26 22:07:04,5
AMD,nw40pfa,"Because there no mention of 4GB production or roadmap.  Right now, 3GB is live in mass production, and they’re already sampling higher speed 3GB chips at samsung, hynix, etc. they’re set on producing 3GB chips. They’re committed to producing 3GB for years. Nobody committed to 1.5GB that’s why it never was a thing it was better to go to 2GB. Nvidia and amd will choose whatever the ram conglomerates choose to make and right now, its 3GB.   https://www.techpowerup.com/news-tags/GDDR7  You can see the news links when scrolling. 3GB is going to stick around for sure. 4GB has no confirmed production yet",hardware,2025-12-27 00:27:35,4
AMD,nxp27we,"Yeap, I mean at this point I'd be fine with Nvidia only releasing the 6090 and 6080 if those are more palpable at high MSRPs than waiting till they can release a 6050 and 6060.   Same with AMD.",hardware,2026-01-04 21:10:12,2
AMD,nw5i5u8,SW and marketing wings are run by clowns. The HW team is very capable. Look at RDNA4 and RDNA5 will be the first time they actually bother to put in the work. More budget and for consoles so it'll be good.,hardware,2025-12-27 06:36:39,1
AMD,nxp0joh,It's from Kepler who's very good about AMD Radeon leaks.,hardware,2026-01-04 21:02:23,1
AMD,nw5hz5y,"No MLID and then Kepler\_L2 didn't contradict any of it. Very early on, hopefully we get more certain and final leaks in 2027.",hardware,2025-12-27 06:35:01,1
AMD,nw5jbwk,"Basically what Kepler has already alluded to. Honestly surprised AMD took ML this seriously with RDNA4, but RDNA5 is probably the first time they built an architecture from the ground up around AI, so yeah a ton of learnings and low hanging fruits from CDNA for sure.",hardware,2025-12-27 06:46:56,5
AMD,nw2ov4m,They could not be sued by shareholders by not increasing capacity. That’s not how fiduciary duty works as they are protected by the business judgement rule.,hardware,2025-12-26 19:56:26,8
AMD,nw2lnke,"Good - And I hope we won't see a build up of coal and other trash sources of energy for ""AI factories"".",hardware,2025-12-26 19:38:51,0
AMD,nw5k95x,Same thing with RDNA 4. Really dissapointing how these launches keep getting pushed back.,hardware,2025-12-27 06:55:15,5
AMD,nw25j0b,This ain’t a market spike on crude oil my man.,hardware,2025-12-26 18:14:19,5
AMD,nw5hcq3,">4GB has no confirmed production yet  It does, Rubin CPX needs it.",hardware,2025-12-27 06:29:31,2
AMD,nxp2p2j,Interesting thought. A staggered launch could very well happen. Yeah if we have to wait for that then high likelyhood of no new GPUs whatsoever in 2027.,hardware,2026-01-04 21:12:26,1
AMD,nw6d5z6,"Some of the ""software"" issues of RDNA 2 and 3 were hardware issues that software couldn't fix. It's not just a matter of one part of the team being good and the other bad. I can remember at least one issue with the video encoder outputting the wrong resolution being a hardware issue.",hardware,2025-12-27 11:35:10,5
AMD,nxp19kw,"Yes but unfortunately they still fall behind everyone else in adopting the future. Somehow Intel got XeSS, XeSS-LL and XeSS-FG before AMD Redstone. Somehow Apple had a DLSS competitor (ai based) before FSR4. It's just so tiring.",hardware,2026-01-04 21:05:46,2
AMD,nw2qt3c,"Thanks for clearing that up, I always thought leaving an ""obvious"" money maker on the table would be something shareholders can pressure the C-level to do.",hardware,2025-12-26 20:07:04,2
AMD,nw268ng,I’m sorry you don’t understand what an allegory is,hardware,2025-12-26 18:18:01,-5
AMD,nw70xzq,A lot changed with RDNA 4. But yeah prev stuff was crap on HW side as well.   I was mainly referring to SW feature set deficit and bad pricing.  RDNA 5 better be good. All that poached talent and hiring has to result in something meaningful.,hardware,2025-12-27 14:30:50,0
AMD,nxp409g,"Agreed. It's a big joke. The old tiring AMD cheapskate strategy. Only pivot when the tech is mature which means by the time its ready it's a joke.  There are some early indications that they're turbocharging things moving forward. Then again we don't know where 60 series will be heading + the SW R&D teams can do all the work they want but if FSR team is complacent and no one gets game devs to implement tech then it's all pointless.    I wouldn't be surprised if NVIDIA completely flips the script with 60 series so AMD better bring some groundbreaking tech nextgen, because I don't expect 60 series to be Ada Lovelace++.",hardware,2026-01-04 21:18:31,1
AMD,nw3ikhx,"Do you not see the circular logic here? It's only obvious if it's obvious. Clearly it's not obvious, else people would be in broad agreement.",hardware,2025-12-26 22:39:34,6
AMD,nw39eli,"The shareholders usually have the power to call a meeting, to ask the c level to course-correct or in rare cases replace them. They might then sue for damages that occurred while that process takes place, but unless there was obvious neglect, not that certain to be awarded.  Edit: to look at it another way, if they could sue for damages easily, where would the money come from? The c level is only personally responsible in rare cases (otherwise nobody would want to do the job). It could come out of the company, but that is just the shareholders giving themselves a dividend, weakening the company.",hardware,2025-12-26 21:48:40,3
AMD,nwniijk,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **GPUs**  |CPU + GPU + RAM Config|Score| |:-|:-| |9800X3D, RTX 5090, 64GB|166,885| |Ryzen 9 9950X, RTX 5090, 64GB|160,564| |9700X, RTX 5090, 64 GB|157,538| |14700K, RTX 4090, 32GB|153,707| |9800X3D, RTX 5090, 64 GB|153,422| |265K, RTX 4090, 48 GB|146,045| |7800X3D, RTX 4090 (UV), 32 GB|142,168| |9950X3D, RTX 4090 (315 W), 64 GB|140,077| |9800X3D, RTX 5080, 32 GB|124,739| |13700K, RTX 4080, 32 GB|110,815| |9800X3D (65 W), RTX 5080, 32 GB|110,233| |5800X3D, RTX 5080, 32GB|107,994| |275HX, RTX 5090 M, 64 GB|106,856| |5950X, RTX 5080, 64 GB|104,847| |275HX, RTX 5090 M, 64 GB (HP OMEN MAX 16)|103,947| |9800X3D, RTX 5070 Ti, 64 GB|101,697| |11900K, RTX 5070 Ti, 64 GB|101,600| |7945HX, RTX 4090 M (Legion 7 Pro)|98,094| |285K, RTX 5070 Ti, 128 GB|97,859| |13700K, RTX 3090, 64GB|91,266| |9950X3D, RTX 3090, 32 GB|84,942| |5800X3D, RTX 5070 (OC), 32 GB|78,660| |13400F, RTX 4070 Ti, 32 GB|77,851| |14900KS, RTX 5060 Ti, 16 GB|62,959| |265KF, RX 9070 XT, 48 GB|55,898| |5800X, RX 9070 XT, 32 GB|49,156| |9800X3D, RX 9070 XT, 64 GB|48,769| |7950X3D, RX 9070 XT, 32 GB|48,123| |5700X3D, RX 9070, 32 GB|41,898| |5800X3D, RX 7800 XT, 32 GB|40,789| |5800X, RX 7800 XT, 32 GB|40,769| |9900KS, RTX 2080 Ti, 32 GB|38,859| |11900KF, RX 7080 XT, 32 GB|38,213| |M4 Pro (14C), 20C GPU, 48 GB|36,881| |9950X3D, RTX 3060, 96 GB|35,615| |5600X, RX 6800, 32 GB|35,349| |5700X, RX 7700 XT, 32 GB|30,313| |13600K, RX 9060 XT, 64 GB|29,954| |7800X3D, RX 9060 XT, 32 GB|29,122| |9700X, RTX 2070 S, 64 GB|27,940| |5800X3D, RX 6700 XT, 32 GB|25,563| |14400F, RX 6600, 32 GB|18,313| |M4 (10C), 10C GPU, 24 GB (Mac Mini)|17,996| |M4 (10C), 10C GPU (Mac Mini)|16,054| |M2 Pro (MacBookPro 16"")|13,425| |M3 (8C), 10C GPU, 16 GB|12,268| |7640U, 760M, 32 GB|5,054| |5700G, Vega 8, 64 GB|0| |4750G, Vega 8, 64 GB|0| |X1E80100, Adreno X1-85, 16GB|N/A|",hardware,2025-12-30 01:38:45,4
AMD,nwnigam,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **1T**  |CPU + GPU + RAM Config|Score| |:-|:-| |M4 (10C), 10C GPU, 24 GB, Mac Mini|677,0| |M4 Pro (14C), 20C GPU, 48 GB|667,0| |M4 (10C), 10C GPU, Mac Mini|653,0| |9950X, RTX 5090, 64 GB|567,0| |M3 (8C), 10C GPU, 16 GB|567,0| |265KF, RX 9070 XT, 48 GB|560,0| |14900KS, RTX 5060 Ti, 16 GB|559,0| |9950X3D, RTX 5080, 32 GB|553,0| |9970 (PBO +200), RTX 5090|537,0| |9700X, RTX 2070 S, 64 GB|533,0| |9800X3D (UV), RTX 5090 (UV), 64 GB|530,0| |9800X3D (65W), RTX 5080, 32 GB|528,0| |275HX, RTX 5090 M, 64GB|524,0| |9700X, RTX 5090, 64 GB|514,0| |14700K, RTX 4090, 32GB|510,0| |13700K, RTX 4080, 32 GB|495,0| |M2 Pro, MacBook Pro 16""|480,0| |7945HX, RTX 4090 M, 32 GB|473,0| |13600K, RX 9060 XT, 64 GB|471,0| |12900K, RTX 4090, 32 GB|460,0| |7800X3D, RX 9070, 32 GB|444,0| |11900K, RTX 5070 Ti, 64 GB|438,0| |X1E80100, Adreno X1-85, 16GB|438,0*| |13400F, RTX 4070 Ti, 32 GB|430,0| |11900KF, RX 7080 XT, 32 GB|417,0| |5800X, RX 9070 XT, 32 GB|396,0| |5950X, RTX 5080, 64 GB|390,0| |7640U, 760M, 32 GB|377,0| |5800X3D, RX 9070 XT, 32 GB|369,0| |5700G, Vega 8, 64 GB|367,0| |6600H (45W), 16 GB, Beelink EQR6 Mini|338,0| |9900KS, RTX 2080 Ti, 32 GB|333,0| |5700X3D, RX 9070, 32 GB|303,0| |5800X3D, RTX 5080, 32GB|272,0|",hardware,2025-12-30 01:38:24,3
AMD,nwpsl9h,"Nice, thanks for posting, here you have my 9800X3D results with just PBO+100 and conservative scalar x1, it's much better than the generic 9800X3D results you posted so far.  screenshot of my configs (balanced power blan, ZenTimings, CO values, etc):  [https://i.imgur.com/kAUmokG.png](https://i.imgur.com/kAUmokG.png)  (CPU Multi-core 6034, single-core 780, single-thread 562, MP Ratio 10.74x)  Tested on most recent Windows 11 25H2 build to this date.",hardware,2025-12-30 11:49:50,2
AMD,nwnf2za,"Their form + Maxon has it termed properly, but Computerbase graph suddenly mixes up the terminology. Maxon is accurate.  |Test|Maxon Terminology|Computerbase Terminology| |:-|:-|:-| |One Thread: 1T|Single Thread|Single Core| |Two Threads: 1C2T|Single Core|Single Core + SMT| |All Threads: nT|Multiple Threads|Multi Core|  We shouldn't use ""Single Core"" to mean two things. Maxon is much clearer.  And, before we have an endless debate: Maxon's Single Core test ***is*** a multi-threaded benchmark test. It limits the thread count to 2. Cinebench pushes *two* parallel instruction streams to the CPU.  Hopefully Computerbase and other outlets stick to Maxon's wording. Or just use the numbers: 1T, 2T (SMT), and nT.  An old but good read from AnandTech:  [https://web.archive.org/web/20221006033815/https://www.anandtech.com/show/16261/investigating-performance-of-multithreading-on-zen-3-and-amd-ryzen-5000?utm\_source=twitter&utm\_medium=social](https://web.archive.org/web/20221006033815/https://www.anandtech.com/show/16261/investigating-performance-of-multithreading-on-zen-3-and-amd-ryzen-5000?utm_source=twitter&utm_medium=social)",hardware,2025-12-30 01:19:21,4
AMD,nwniecd,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **nT**  |CPU + GPU + RAM Config|Score| |:-|:-| |9970 (PBO +200), RTX 5090|18,278| |9950X3D, RTX 5090, 64 GB|9,641| |14900KS, RTX 5060 Ti, 16 GB|9,563| |9950X, RTX 5090, 64 GB|9,235| |9950X3D, RTX 5080, 32 GB|9,166| |275HX, RTX 5090 M, 64GB|8,490| |265KF, RX 9070 XT, 48 GB|8,409| |265K, RTX 4090, 48 GB|8,316| |7945HX, RTX 4090 M, 32 GB|7,474| |M4 Pro (14C), 20C GPU, 48 GB|6,812| |14700K, RTX 4090, 32GB|6,425| |13700K, RTX 4080, 32 GB|6,206| |12900K, RTX 4090, 32 GB|6,024| |5950X, RTX 5080, 64 GB|5,846| |13600K, RX 9060 XT, 64 GB|5,271| |9800X3D, RX 9070 XT, 64GB|5,122| |9800X3D (65W), RTX 5080, 32 GB|5,104| |9700X, RTX 2070 S, 64 GB|4,875| |7800X3D, RTX 5080, 32 GB|4,134| |M2 Pro, MacBook Pro 16""|4,121| |11900K, RTX 5070 Ti, 64 GB|4,017| |M4 (10C), 10C GPU, Mac Mini|3,858| |13400F, RTX 4070 Ti, 32 GB|3,750| |M4 (10C), 10C GPU, 24 GB, Mac Mini|3,723| |5800X3D, RTX 5070 (OC), 32 GB|3,698| |X1E80100, Adreno X1-85, 16GB|3,504*| |5800X, RX 9070 XT, 32 GB|3,485| |11900KF, RX 7080 XT, 32 GB|3,417| |5700G, Vega 8, 64 GB|3,305| |5800X, RX 7800 XT, 32 GB|3,297| |5800X3D, RTX 5080, 32GB|3,027| |9900KS, RTX 2080 Ti, 32 GB|2,988| |5700X3D, RX 9070, 32 GB|2,799| |M3 (8C), 10C GPU, 16 GB|2,689| |7640U, 760M, 32 GB|2,367| |6600H (45W), 16 GB, Beelink EQR6 Mini|2,105|",hardware,2025-12-30 01:38:07,3
AMD,nwr5ble,"[Here's the first pass for my 5700X3D + RX 9070 setup](https://i.imgur.com/xaWb8vh.png). Note that I was at first getting 13k in GPU score, but increased to 34k when I set my Adrenalin tuning to Default. This benchmark might be a lot more sensitive with GPU Tuning (at least in my quick test).",hardware,2025-12-30 16:30:28,1
AMD,nww8o3q,"|CPU|7600X|12400 AVX512 on|5800X|M4 Max|M3 Ultra|M2| |:-|:-|:-|:-|:-|:-|:-| |Cooling|Arctic Freezer 34|TR AXP90-X53mm LP||||| |Motherboard|B650M-Plus TUF|B660M MORTAR||||| |BIOS|3040 (09/12/2024)|A.F0 (01/11/2024)||||| |RAM|32GB DDR5-5600|32GB DDR5-6000||||| |RAM timings|36-38-38-80-135|40-40-40-76-116-2T||||| |GPU|TUF 4070 Super|Gigabyte 2060 Super WF||||| |CPU Single Core|638|527||||| |CPU Single Thread|478|416|388|676|573|473| |MP Ratio|7,69||8,87|11,59|21,09|4,94| |CPU Multiple Threads|3679|CRASH|3441|7829|12082|2336| |GPU|81243|24305||68590|83865|7952|  I benched two of my systems - a gaming PC and a HTPC. Both Win 11-26200, latest nVidia driver. All stock, no OC (except XMP and auto-PBO). 12400 is early batches which had the AVX-512 unlocked. Tried turning off it in the bios - multi-core test still crashed (temps 71, so no overheating either). With the 8GB VRAM requirement, I think the 2060 Super should be the second weakest (non-mobile) RTX GPU after 3050, with GTX 1070 probably being the weakest compatible (min CUDA 5.0 & 8GB VRAM) one.",hardware,2025-12-31 10:49:32,1
AMD,nwnihg9,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **1T w/ SMT**  |CPU + GPU + RAM Config|Score| |:-|:-| |9950X, RTX 5090, 64 GB|763| |9950X3D, RTX 5090, 64 GB|748| |14900KS, RTX 5060 Ti, 16 GB|742| |9970 (PBO +200), RTX 5090|740| |9800X3D (65W), RTX 5080, 32 GB|733| |9950X3D, RTX 5080, 32 GB|730| |9700X, RTX 5090, 64 GB|724| |9800X3D, RTX 5070 Ti, 64 GB|710| |14700K, RTX 4090, 32GB|680| |13700K, RTX 4080, 32 GB|647| |13600K, RX 9060 XT, 64 GB|625| |7945HX, RTX 4090 M, 32 GB|620| |12900K, RTX 4090, 32 GB|602| |7800X3D, RTX 5080, 32 GB|570| |13400F, RTX 4070 Ti, 32 GB|563| |11900K, RTX 5070 Ti, 64 GB|533| |7640U, 760M, 32 GB|518| |11900KF, RX 7080 XT, 32 GB|512| |5800X, RX 9070 XT, 32 GB|497| |5950X, RTX 5080, 64 GB|486| |5800X3D, RX 9070 XT, 32 GB|464| |5700G, Vega 8, 64 GB|462| |6600H (45W), 16 GB, Beelink EQR6 Mini|429| |9900KS, RTX 2080 Ti, 32 GB|420| |5700X3D, RX 9070, 32 GB|411| |5800X3D, RTX 5080, 32GB|304|",hardware,2025-12-30 01:38:35,1
AMD,nwrzaij,"do new Cinebench versions inflate the score of the same CPU?  Antutu's score inflation is such a pain over on phone benchmarking, even just going 1 version up will jack up the exact same phone's score by 100k (which gets extra deceptive when said phone's a budget phone that only got like 500k to begin with, so itd look like the phone got a 20% performance increase out of nowhere)",hardware,2025-12-30 18:49:09,1
AMD,nwnjygb,"who cares, just use stockfish",hardware,2025-12-30 01:46:47,-9
AMD,nwqaqe4,"Dunno what I'm doing wrong but my 5700X3D + 9070 GPU Score is only 13k. My card is only pulling 120W, at stock settings but when I play intensive games like Expedition 33 it pulls the full fat 245W.",hardware,2025-12-30 13:53:45,3
AMD,nxmmzx7,Well my Colorful Vulcan OC 4070ti Super 330W + 3000mhz on core +2000mhz on memory + R7 7700 OC 5500mhz + 6000c26-34-34-56 GPU score I have 113k pts and 103k on stock so I found that memory OC gives most improvement!,hardware,2026-01-04 14:22:34,1
AMD,nwt67qi,I scored 112558 with my 5070 Ti. Posted about it on the PC Master Race reddit,hardware,2025-12-30 22:13:25,-1
AMD,nx08oqp,"I assume it's an average, will have a lot of people who are also running back ground apps or slow ram or maybe even lower power limit sin BIOS etc.",hardware,2026-01-01 00:39:44,1
AMD,nwnvsd3,"SMT doesn't automatically mean 2T as there are implementations with more than two (even if not common).  Single Core+SMT is more accurate (but multi-core for nT isn't very good either) so I'd skip both terminologies in favor of being specific (i.e., 1c3t if it's a 3 thread SMT single core, 2c4t, 4c4t, etc.).  In the context of this benchmark, 1T, 1C (or SMT) and nT probably makes the most sense.",hardware,2025-12-30 02:51:08,5
AMD,nwpmg7l,Interesting how 5800X3D plus 5080 is way below 5800X3D plus 5070.,hardware,2025-12-30 10:56:33,1
AMD,nwvgk7k,"No, completely different, multi-core scores of my 9800X3D across different versions of Cinebench:  R23: 24253  R24: 1454  R26: 6034  They update the CPU and GPU renderers with each new revision of the software",hardware,2025-12-31 06:29:41,6
AMD,nwov9jb,Why?,hardware,2025-12-30 06:47:26,2
AMD,nws4bi2,Same. my 9070xt just did 19k xd,hardware,2025-12-30 19:12:41,2
AMD,nwsy8z7,"& /u/FusionXIII   Try running it after a fresh reboot.  My 9070 XT spit out a 2983 score. The GPU memory controller was reporting 0-1% utilization and the GPU was using ~140W doing who knows what.  After a reboot I got 48532.   With my modest UV/OC (-25mV core, 2714MHz mem) it scores 50308.",hardware,2025-12-30 21:35:35,2
AMD,nx3ktlc,"The GPU bench seems to be all over the place. A friend of mine scored similarly with his 5070 Ti and beat another's 5080. Maxon lists the 5090 score as 166k, but I've seen scores ranging from 150k to 190k on the same 5090 silicon.   Such differences can't be explained by OC, silicon or temperature. Maxon needs to fix their bench.",hardware,2026-01-01 16:29:38,1
AMD,nwnz0mc,">SMT doesn't automatically mean 2T as there are implementations with more than two  Fair, but in my experience, those *usually* use more specific names: SMT**4**, SMT**8**, SMT**16.** I imagine it's unlikely any of those CPUs will work with Cinebench 2026:  >Cinebench 2026 will not execute on unsupported processors.  But I agree with you: the best choice is just using the numbers of how many threads (and whether CB is setting affinities to ensure they're localised to one core or it accepts whatever the CPU allocates). The number of threads removes all confusion & is universally understood across languages.",hardware,2025-12-30 03:09:02,4
AMD,nwvfv43,Because the person that tested with the 5080 might be using stock 5800X3D and the one with the 5070 is doing -30 all-core instead. There's a 9800X3D there scoring 5100 but mine scores 6000+ (PBO+100 per-core curve optimizer). Here's the proof: https://i.imgur.com/kAUmokG.png,hardware,2025-12-31 06:23:56,0
AMD,nwpp8be,"Because it's a better benchmark.   - works on pretty much everything, including linux and phones  - fixed length test so harder to cheese via overclocking/boost  - has proper NUMA support and can work with thousands of threads by design  - has a cluster branch for benchmarking distributed deployments  - opensource so you can verify architecture-specific code and fairness  - can be compiled with CPU-specific optimizations on the target machine (march=native)  - covers a good mix of SIMD, branchy, and memory bound code, known to be [the best stress workload](https://github.com/ThomasKaiser/sbc-bench/issues/55)  - some representative results can be easily found [online](https://openbenchmarking.org/test/pts/stockfish)",hardware,2025-12-30 11:21:18,1
AMD,nwtbzcu,My 9070xt did 48k and my 7800x3d did 4.3k,hardware,2025-12-30 22:42:22,1
AMD,nwt3y23,"Ill try and report back thanks :) edit: it works, got 52k on my 9070xt mercury oc",hardware,2025-12-30 22:02:29,2
AMD,nwtc788,With a -30mv and  +10% power with the OC switch on my 9070xt Hellhound 48438  7800x3d 4381,hardware,2025-12-30 22:43:30,1
AMD,nwrbo1k,It's a different benchmark and not necessarily neither worse or better as such.,hardware,2025-12-30 16:59:57,4
AMD,nwtcaql,Yeah I just needed to restart apparently,hardware,2025-12-30 22:44:01,1
AMD,nwtglnm,Ima keep tweaking my undervolt and overclock I literally just unboxed and installed this card 2 hours ago. Re-download cinebench and just got lucky with the whole new version   Theres someone posting a 5070ti with 110k  I am severely limiting my potential here,hardware,2025-12-30 23:06:43,1
AMD,nwtih62,Nvidia uses cuda so I think the numbers won't be similar. Others 9070 xt i've seen are around 50k also,hardware,2025-12-30 23:16:46,1
AMD,nwto0zy,If i understand CUDA and the parallel stuff it does  One would think essentially around double the score right?,hardware,2025-12-30 23:47:25,1
AMD,nv1xd7z,"Not impossible, but very unlikely. AMD is currently focused on upcoming Zen 6, and I guess their engineers are working on next gen CPU / chipset / AM port. Plus there are probably constraints with TSMC schedule, I think you have to ""reserve"" quite some time in advance for any order.",hardware,2025-12-20 16:34:49,114
AMD,nv1zvgv,"Probably the simplest would be to manufacture 5800x3d again.   (Newer cpus have ddr5 controller, they would have to redesign architecture.)",hardware,2025-12-20 16:48:08,47
AMD,nv2oafl,No.  Considering the tech news from the last few months apparently there isn't much money in the consumer market for something like this.,hardware,2025-12-20 18:56:21,13
AMD,nv1woql,"Maybe, but I'm starting to think they want people to sell off their old stuff. Just a theory of course.",hardware,2025-12-20 16:31:13,28
AMD,nv2guhn,"the thing is you can still get 32gb of ddr5 for two or three hundred bucks. it won't be a great bin, and definitely won't have the overclocking potential of high end hynix a-die but frankly the vast majority of people can/will just run JEDEC and not be able to see the difference. That may be easy for me to say sitting on 64gb of a-die running tight timings, but you/I seriously don't need high end ram to build a pc and run games flat maxed out. Prices have gone up but generally if you could afford to build a PC before the rampocalypse you can still do so now, you're just going to have JEDEC speeds. Which are actually, totally fine if you're using your pc and not a hardware snob (which I am admittedly). ​",hardware,2025-12-20 18:18:11,16
AMD,nv2ikz1,"Last AM4 cpu that got released is the Ryzen 5 5600F, which happened september. So only 3 months  I wouldnt be surprised if AMD comes up with something new or reintroduce 5800x3d and 5700x3d (these cpus only existed for a year or year n half. Which is very short)  Its been 9 YEARS since AM4 got released. AMD still releasing cpu's for it.. crazy",hardware,2025-12-20 18:27:08,20
AMD,nv2w2bo,No  Not enough volume for amd to restart production,hardware,2025-12-20 19:36:57,3
AMD,nv3o0w6,"The production of the X3D variant of the core chiplets has been discontinued. Presumable it was done so that they can use those lines to produce Zen 5 X3D variant. So unless you want them to discontinue the current gen to put old already somewhat obsolete generation back into production, it's not viable idea.",hardware,2025-12-20 22:10:34,3
AMD,nv3kzh9,"If I were a top AMD executive I'd be focusing on these things  1. Getting Radeon THERE for AI purposes   2. Making the EPYC line amazing for data centers   3. Finding ways to optimize costs and cut risks     Targeting budget customers is fairly low on any list I'd have.    There's probably some value in keeping Zen 3 dies in production but they'd get minimal priority for anything new or cutting edge. Minimal development efforts. Zen 4 is already getting ""old"" by industry standards. There's not much point to getting anything newer to work with AM4 IODs either.",hardware,2025-12-20 21:53:25,9
AMD,nv2edbh,5950X3D? Or even 5950X3D2?,hardware,2025-12-20 18:05:29,4
AMD,nvfdxfb,"Not only is it not likely for reasons of moving forward instead of back, you'd also crash the ""market"" the minute there's any supply, and it would likely spike prices on AM4 boards and DDR4 ram. The lack of demand is the only reason AM4 boards and DDR4 are relatively cheap right now. Can't get good AM4 CPUs, so the rest of the linked prices slump. Bring back competitive AM4 CPUs and the prices shoot up to match, and now AMD is stuck trying to sell ancient and EOLed designs into a market that's already buying every AM5 CPU they make.",hardware,2025-12-22 20:18:10,3
AMD,nv36p5d,"First of all, never trust inflated prices on eBay. It's well known that scam artists who have hordes of collectibles or scalped/limited items who will orchestrate sales of items at inflated prices to drive up the market. Yes, they have to eat the cut that eBay takes, but if all of a sudden they have 10-50 items that they can sell for 20-100% more, it's worth it.   Like what people have been doing with VHS tapes on eBay for the last 6 years.   Secondly, AMD hasn't stopped releasing CPUs for AM4. They recently released the 5500X3D for the Brazilian market in June, and the 5600F in others.   The biggest issue is there's only a handful of AM4 boards still for sales and DDR4 RAM in the retail channels is almost entirely gone. Sure you can find used RAM, but minting new products that requires parts that are no longer in production is suicide.   Micron recently said they'd keep making DDR4, but I ain't seeing much available in the US. I assume much of that is still being [shuffled to the enterprise market.](https://prerackit.com/memory-markets-in-turmoil-how-chinas-exit-from-ddr4-manufacturing-triggered-a-server-ram-pricing-crisis-in-2025/)",hardware,2025-12-20 20:34:46,6
AMD,nv7x3xu,Wafers aren't just sitting on shelf to be bought when in demand. They're order well in advance. It's unlikely they resume production of the 5700x3d which old got disconnected a few months ago (new AM4 cpu is completely put of the question) as it's a step backwards and a gamble that people would still be interested in AM4 several months time instead of shelling out a bit more for significantly higher preformance gains from their next generation of cpus in November.,hardware,2025-12-21 16:34:40,2
AMD,nvh40h7,">It got me thinking, is 5000 series AM4 on an old enough node that AMD could restart production cheap? Cheap enough to sell a high end x3d chip to satisfy people holding on to their old platform and RAM while the shortage is happening?  Why this will probably never happen:  * The vast majority of people do not upgrade their CPU. The entusiaste market is probably less than 1%. There is not enough volume. * Selling this processor will cannibalize their own current product line. * The stacked L3 cache could be better utilized on their current CPUs. * All fab production is concentrated on storage, ram, new CPUs and GPUs. New and Old.  The echo chamber that is PC enthusiasts has seriously distorted their perception of the PC market.",hardware,2025-12-23 02:09:42,2
AMD,nv397y8,Before the AI bubble I would have said no way. With the AI bubble making new unaffordable for the next 12-18 months at a minimum I'd say maybe.  People on AM4 aren't going to buy AM5 if they have to spend 200% of their budget to get it. DDR5 RAM prices are really going to cripple the PCbuild industry for the next year.,hardware,2025-12-20 20:48:24,2
AMD,nv3b00x,"There was some rumors that Zen4 was originally going to come to AM4. They could still do it. Maybe a lot of that work is already done. I'm skeptical there is still much DDR4 in production, and it'll soon all be gone. What we got on desktop was mostly left over stuff that servers didn't want. I don't think they would make these CPUs just for consumer when there is so much more profit in putting that silicon towards servers. If Mircon abandoned Crucial memory for consumers, that to me says a lot about where the real focus is for hardware makers. AMD right now just doesn't care much about desktop anything.",hardware,2025-12-20 20:58:02,4
AMD,nv29ebj,"Yes, they have actively done so in the last year and are being pushed to re-release the 5800x3d by retailers.",hardware,2025-12-20 17:39:41,2
AMD,nv4gclk,"Very unlikely and most probably something on the lower end I’d think.   But a 5950X3D would be incredibly cool though, and a nice upgrade for my 5700X3D. 😄  On the other hand, there are a lot of people out there, me included, on AM4 that aren’t planning to upgrade to AM5 any time soon. So that would be a way for AMD to keep selling CPUs to existing AM4 users of whom they otherwise wouldn’t make money from.",hardware,2025-12-21 01:02:36,1
AMD,nv7ewr5,"From one side Zen 6 has to sell but RAM/(SSD) prices may block AM4 to AM5 upgrades (how many still on AM4?). Then if going AM5 is to expensive then people may wait till Zen 7 which may be AM6 and if it's revealed early that Zen 7 is AM6 then it will decrease AM5 interest even more.  Zen 6 will launch ""late"" 2026, so 2027 is the year it will be in mass availability. RAM prices may be more sane at that time. Especially when people will be waiting for X3D (if the base variants get rekt again by existing X3Ds).  Refresh of 5800X3D wouldn't hurt but I doubt they would be offer more without a longer development cycle (a new design) after which it may turn out it's all for nothing.",hardware,2025-12-21 14:56:36,1
AMD,nv9peun,they do release 5000 series x3d chips but using leftover chiplets that were binned too low for the expensive parts. ie the 5500x3d. its not in production anymore.,hardware,2025-12-21 22:04:38,1
AMD,nvamixt,No - production has already stopped for the 5000 series. It would be prohibitively expensive to have TSMC manufacturer last-gen chips in another run.,hardware,2025-12-22 01:12:18,1
AMD,nvferit,"Would be nice when the 5700x3D and 5800x3D would be produced again, but very unlikely.",hardware,2025-12-22 20:22:32,1
AMD,nvhz5no,Sure they could. AMD or Intel could make a modern chiplet CPU with a memory controller that runs DDR400.   There just isn't any financial incentive to do so.,hardware,2025-12-23 05:36:58,1
AMD,nvscq8b,"they did release a ""new"" one this year, the 5500x3D for latam.",hardware,2025-12-24 22:29:39,1
AMD,nw7yvm9,I hope they do. Been thinking of shifting from Intel to AMD for a long time now.,hardware,2025-12-27 17:30:39,1
AMD,nwpeonj,AMD is not designing or manufacturing anything for AM4 now. They found some binned chips to re-release under different name but thats about it.,hardware,2025-12-30 09:45:33,1
AMD,nv2je9u,Why would they do that when they can just sell a new x3d chip at inflated prices and have every single one of them sold before they've even been shipped?,hardware,2025-12-20 18:31:21,3
AMD,nv3fizo,they just released the 5600F in September.,hardware,2025-12-20 21:23:01,1
AMD,nv4hdev,"AM4 is just too good for me to ditch. My 6800XT holds it back in 4k gaming, 64GB 3600Mhz CL16 was dirt cheap and both single threaded and multithreaded performance with my 5950X is more than I really need.",hardware,2025-12-21 01:09:01,1
AMD,nv2i61h,"Considering they launched the Ryzen 5 5500X3D only 6 months ago, i would say they can, the problem is if they will. Unfortunately it can take a long time to design or adapt existing designs, test and then manufacture them in sufficient numbers for release, if they started now it would probably take a year or more to see them on the market. A 5950X3D does sound cool as hell, maybe i would upgrade from my current 5700X3D, though this CPU is perfect for my needs and the gaming i do.",hardware,2025-12-20 18:25:01,-1
AMD,nv33kwe,Imo the 5950X3d is the most likely option as the very last cpu on am4. dual 3XD ofc and with people starting to switch to more expensive and advanced packaging then Cowos there will be enough cowos allotment that amd can buy for older designs,hardware,2025-12-20 20:18:03,-1
AMD,nv3u5oc,"Restart? They never stopped releasing them, with the latest being the  5500X3D released in June this year.",hardware,2025-12-20 22:46:36,-1
AMD,nv3h2vr,"DDR6 is coming out soonish in 2027, so when that happens the cutting edge ai shit will all switch over to that. So ddr5 availability should go up permanently after that. It's gonna be a dry ass desert until then but it won't be forever. DDR5 came out in 2020 so it's already fairly old, making a switch back to 4 even temporarily quite unlikely.",hardware,2025-12-20 21:31:35,0
AMD,nv3i37j,"The only one that would *maybe* make sense, and I stress it's a biiig maybe at this point, would be a 5950X3D. After that, no more AM4 anything. Otherwise, probably best to keep on cranking 5800X3Ds.",hardware,2025-12-20 21:37:17,0
AMD,nv2dc8m,"You can get a 5700x3D, which is a lot cheaper.",hardware,2025-12-20 18:00:17,-2
AMD,nv2pscd,"They could mix tiles, but would be a hard sell for motherboard partners. And why buy a am5 x3d when am4  x3d gives almost the same gaming performance?",hardware,2025-12-20 19:04:02,-2
AMD,nv2towu,"They can restart Zen 3 X3D production, with a dual CCD V-Cache 5950X variant and should do so IMV.",hardware,2025-12-20 19:24:29,-3
AMD,nv1zhnp,AM4 Zen 6 Fan Edition backport manufactured on an Intel node,hardware,2025-12-20 16:46:06,59
AMD,nv25szg,"Dont think they're advocating for proper new designed chips, just respinning up old ones.    As for TSMC, I'm sure if AMD really cared to do this, they could find some way to give up some more leading edge capacity for older node capacity.  Perhaps via agreement with some other company who would love to bump themselves up the waiting list.    I think bottom line is that AMD isn't gonna be overly concerned with things.  They're still gonna be making lots of money on all this AI stuff themselves selling CPU's and GPU's, they can take hit on the consumer side for a bit.    I also think paying high prices for AM4 processors is very stupid.  While DDR5 has certainly ballooned building on AM5 platform, the reality is that total system costs are still only gonna be like 15-20% higher.  Small enough difference where it will probably still be worth it to go with AM5 in the big picture.",hardware,2025-12-20 17:20:08,28
AMD,nv1yle7,"Two years in advance, I believe...",hardware,2025-12-20 16:41:19,5
AMD,nvis9qj,"seeing ""Zen 6"" is crazy when i still am thoroughly satisfied with my Zen 2 3900X. Although i suspect it's finally showing some wear.",hardware,2025-12-23 10:07:30,1
AMD,nv3gsg4,"There are actually older Zen3 parts with the DDR5 controller, too. As I understand it, Zen cores are largely decoupled from the wider system with their IO die interfacing with the various external components, so that may be the only bit which really needs changed to support one platform it another.",hardware,2025-12-20 21:29:58,13
AMD,nv3pc79,"Part of me wants to believe it would be possible for AMD to use the old DDR4 IO die and pair it with newer compute dies with Zen 4 or 5, but even if that was true they have no financial incentive to do it.",hardware,2025-12-20 22:18:06,5
AMD,nv26o2w,Or variants of it we never got (5900X3D or a 5950X3D).   It's not like TSMC 7 and 6 have companies fighting each other for their wafers at this point.,hardware,2025-12-20 17:24:49,11
AMD,nv3t9cs,Im down,hardware,2025-12-20 22:41:17,2
AMD,nv6okgx,"Also, the current price explosion of ddr5 is a bubble by an overleveraged market with inelastic demand that will likely be mostly gone by the time any of these new models would make it to the market.",hardware,2025-12-21 11:48:38,6
AMD,nv6n0ey,The consumer market is waiting for 8k monitors with high refresh rates. And cpus and motherboards with connections that support that.,hardware,2025-12-21 11:34:05,-7
AMD,nv2bsmj,They might want it but the rest of us want DDR5.   Nobody's getting what they want in 2026 except the billionaires.,hardware,2025-12-20 17:52:17,19
AMD,nv356y8,"They want money, and they will do whatever it gives them. Old stuff is sold, makes no profit, so yeah they want more sales. Anyways, stick to ddr4 and if ddr5 demand falls then they will have to adjust.  W11 EOL+ Crypto+ AI ... That is tiny compared to 9.000.000.000 humans that use a pc.",hardware,2025-12-20 20:26:45,0
AMD,nv35o6r,And X3D doesn’t have too much to gain from “good” RAM.,hardware,2025-12-20 20:29:16,15
AMD,nv2imvs,"What about people who just want an upgrade? I would pay a few hundo to get best in-slot CPU on my current platform for a quick bump to give me another year or two.  Right now that's not possible, 5800x3d is no longer manufactured and 2nd hand is prohibitively expensive...  Why not fill that gap with brand new silicon?  The question is - is there fab capacity to make it?",hardware,2025-12-20 18:27:24,-5
AMD,nv3m5ck,It was pretty clear they were dumping and clearing out stock when the 5500X3D launched during the summer.   EPYC with Zen3 and X3D is probably EOL now. Which is the main reason why AMD kept the desktop parts around as well.,hardware,2025-12-20 21:59:56,16
AMD,nv2znuj,"5100x3d in 2 years, trust",hardware,2025-12-20 19:56:18,5
AMD,nwpfouh,to be fair thats just scrapped garbage from downbinned chips that would otherwise had to be melted down as broken. they didnt really manufacture AM4 chips 3 months ago.,hardware,2025-12-30 09:54:46,1
AMD,nv8suys,"Gonna be plain, with IBM and Cisco starting to pivot, avoiding too much waste of Radeon dev time on AI crap may end up being wise.",hardware,2025-12-21 19:14:18,1
AMD,nwpgk0q,"The DIY market is much more than 1% (dell estimates 15% for example). However very few of the DIY upgrade CPUs. The longevity of platforms like AM4 is nice in theory, but for vast majority of PC users it does not matter because by the time they need to upgrade they will be buying next socket anyway.  Funny thing, the store i buy stuff from offers free assembly if i buy PC parts. I can choose to have it delivered already assembled, but only if i also buy a case otherwise they refuse to ship naked assembled mobos. So i just assemble myself, but im sure there are plenty of people who will take this service and wont care about socket at all.",hardware,2025-12-30 10:02:37,2
AMD,nv3uv0a,5500X3D is just a stockpile of chips that couldn't be sold as better chips.   AMD likely was building that stockpile ever since launch.,hardware,2025-12-20 22:50:51,6
AMD,nv8t12e,Assuming that the AI crap doesn't crash before then.,hardware,2025-12-21 19:15:08,2
AMD,nvcswtd,ddr5 is unavailable becuase dram manufacturing is not miang the ddr5 chips that go into desktops and increasingly not even ddr5 chips that go into servers. why do you think ddr6 would be any different?,hardware,2025-12-22 11:42:14,1
AMD,nv2im3w,Even these have gotten insane. Used 5700x3Ds used to go for 150-175 eur. Now they're 300..,hardware,2025-12-20 18:27:18,6
AMD,nvauuxn,u/bobalob_wtf there isn't enough demand to justify a new chip SKU for better raw performance or performance per watt.  It is more likely that they'll do a new manufacturing run of an old SKU.  Will it be cheaper? They may pass on the savings of the older process node to you if it is a competitive advantage.  If now stocks of current SKUs are unavailable then buy used?,hardware,2025-12-22 02:03:22,2
AMD,nv2oi5x,"I think you're right. AM4 was a great, long-lived platform--I'm still on it myself--but I don't think anyone building now should really be looking at AM4.  Maybe build with 16 GB RAM if you're really on a tight budget, and start with an R5 7600, and upgrade to Zen 6 and more RAM when prices come back down out of the stratosphere.",hardware,2025-12-20 18:57:27,15
AMD,nv4araf,">As for TSMC, I'm sure if AMD really cared to do this, they could find some way to give up some more leading edge capacity for older node capacity  Is the 7nm node fully booked?  Would AMD have to give up anything to get more of it?",hardware,2025-12-21 00:28:15,2
AMD,nv89zms,"Nostalgia is one hell of a drug. We saw the same with people exaggerating the longevity of Sandy Bridge during the pandemic. Both SB and Zen3 have been absolute champs, and the latter is mostly great if you already have it, but some people are close to deluding themselves because of desperation over prices and lack of realistic options to build new in the current situation.",hardware,2025-12-21 17:41:08,3
AMD,nv66l4u,"Those 15% are a 200 dollar difference for a 32 GB computer. That can be a very significant fraction of disposable income, and the benefits of a DDR5 CPU aren't that big of a deal in this price range anyway.",hardware,2025-12-21 08:50:34,1
AMD,nv3t9zl,"the mobile chips have a bunch of zen versions (2,3,3.5,4,5) running ddr5/lpddr5",hardware,2025-12-20 22:41:23,12
AMD,nv3lv55,"> There are actually older Zen3 parts with the DDR5 controller, too.  Ye and Zen 4 with DDR4 controller from Zen 3 as well. They both tested the new IO die with the old architecture and Zen 4 with the old IO die during development.",hardware,2025-12-20 21:58:18,10
AMD,nv2fihy,"For the silicon itself no, but if you want X3D parts packaging is the bottleneck.",hardware,2025-12-20 18:11:23,23
AMD,nv8dxu7,The average consumer doesnt even have a gpu that can run 2k. What are you talking about,hardware,2025-12-21 18:01:06,4
AMD,nv3imxt,"Not really, it's closer to less than 2billion people that have a pc and almost all of that is gonna be a basic pc that isn't upgraded. Like with everything a company makes way more profit on a overpriced ""pro"" ai chip than a cheap user version and when you have a trillion dollar order for ai gpus you will have that 50k dollar ai gpu be prioritized over a 1k consumer gpu.  The real problem is that these orders are for speculative demand and so they are just selling all of their future product without regard to how much will actually be needed.",hardware,2025-12-20 21:40:22,2
AMD,nwpffbc,the whole point of x3D is to increase cache hit rates and decrease RAM hit rates. You get more benefit from x3D the less RAM matters.,hardware,2025-12-30 09:52:22,1
AMD,nv36vod,they definitely help a lot.,hardware,2025-12-20 20:35:42,1
AMD,nv2lt5j,"where were you last year when they were practically giving away 5700x3d on ali express? it was very clear at the time it was a very limited time deal as the chips were out of production and AMD was just using up all the silicon that didn't bin well enough to be a 5800x3d. if you were ok with your current performance and it wasn't worth the time when fantastic AM4's were only $150 shipped, why FOMO and panic now? In any case anyone building \*now\* can still get a 7500F which will out perform any AM4 CPU for $150, paired with whatever mobo is cheapest and JEDEC tier ram, then upgrade the CPU and ram in the future and be sitting pretty. Fab space is booked up for years solid, scheduling a brand new run specifically for people who missed the boat on a cheap drop in AM4 life extension isn't going to happen and wouldn't be economical for anyone if it did.  Also, are you aware you can currently just get a 5900x from ali for $250 all in? That's your one step AM4 life extension solution right there, they're available, and quite good chips. 12 core, 4.8Ghz boost, they're more than enough to get you a few more years.",hardware,2025-12-20 18:43:42,7
AMD,nv2ufow,"> Right now that's not possible, 5800x3d is no longer manufactured and 2nd hand is prohibitively expensive... >  >  >  > Why not fill that gap with brand new silicon? >  >  >  > The question is - is there fab capacity to make it?  And would it be profitable for AMD to release it at prices that you and others wouldn't consider prohibitively expensive?  I took a quick look at ebay, and it looks like the 5800x3d is going for just under $500, and the 5700x3d just under $350.  What price would those CPUs have to sell for new for you to consider them a good buy?",hardware,2025-12-20 19:28:21,5
AMD,nwpfi3k,> What about people who just want an upgrade?  they should be looking at AM5 setups.,hardware,2025-12-30 09:53:04,1
AMD,nv3uq6n,It was pretty clear they were making brand new silicon when the 5800XT and 5900XT dropped last year. They might have stopped this year despite several new AM4 releases but availability makes me think they never stopped at all.,hardware,2025-12-20 22:50:02,-4
AMD,nvez17x,"If AMD handled 10% of nVidia's output, they'd basically 2x their market cap.   There are crazier gambits to take.",hardware,2025-12-22 19:01:52,2
AMD,nwpo4zi,Exactly!  AMD AM4 did have a long run tbh but everyone with an AM4 i've built for is either happy with how the system performs or jumped to AM5 or LGA 1700.,hardware,2025-12-30 11:11:36,1
AMD,nvajhct,"Yeah exactly, they were all the ""bad"" 5600X3D yields which are all just 5700X or 5800X bad yields too slapped with an 3D V-Cache die on top. In no world is AMD going to TSMC and purchasing wafers to make exclusively 5500X3Ds or any other old chip. They especially wouldn't bother purchasing wafers for some transient rise in DDR5 and to give customers on older platforms upgrade options. It also takes years to do that sort of stuff and thats using old process nodes and architectures.  Even backporting a new architecture to an old platform would require re-validating for a new platform and for DDR4 memory now, getting partners to release BIOS updates and to get them on board etc. It's not that simple to just move Zen6 to AM4 for instance by replacing an I/O die and calling it a day. I can't imagine AIBs being happy about losing new motherboard sales either.",hardware,2025-12-22 00:54:31,2
AMD,nvnxy59,They will only use ddr6 because it's much better but current producers will keep making ddr5 for a while because it's in demand and will sell. It's only a problem because we stopped making ddr4 because there wasn't much demand for it once ddr5 was cheap and widely available. If this crunch happened last year before ddr4 production was wound down it wouldn't be a huge problem either. This is the worst timing.,hardware,2025-12-24 04:14:01,1
AMD,nvcwwj0,"a used 5700x3d is 300€ and a used 5800x3d is 350-400€. It is just not worth it, you can buy a brand new 14600k+a decent mobo for that amount.",hardware,2025-12-22 12:15:23,2
AMD,nv2k0zf,"Hey Admirable_Bid2917, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-20 18:34:29,2
AMD,nv4cogy,"The exception is if you already have solid DDR4. In that case, it can be a pretty good idea to make a new AM4 build.",hardware,2025-12-21 00:40:00,5
AMD,nv3ekjm,"> I don't think anyone building now should really be looking at AM4.  You're joking, right? As if anyone aside from Brad Pitt's, Elon Musk' kids or some wealthy politician could even afford actually *newly* released stuff these days, given all the price-increases …",hardware,2025-12-20 21:17:44,-16
AMD,nvlzwq5,"I am pretty sure AMD still has 7nm booked for products like the console chips (PS5, Xbox Series) and some long running embedded designs. How much they could reallocate to old Zen 3 AM4 designs is an other question.",hardware,2025-12-23 21:15:36,2
AMD,nvdldlh,"Maybe, maybe not.  But I definitely think they could figure out something to get it no matter, given leading edge manufacturing tends to be more desirable overall.",hardware,2025-12-22 14:52:16,1
AMD,nvaip3w,"Sandy Bridge's longevity wasn't exaggerated, that arch absolutely slapped till 2020. Basically if you had an OC'd 2600K or something you were sitting on that till Intel 12th gen for a great improvement in ST perf. Or you bought Zen3. If you needed MT perf from Sandy Bridge, you likely upgraded to Zen2 or Intel 10th gen for a nice leap in performance. If you really needed MT perf gains, you were likely on Ryzen or Intel Extreme chips anyways almost every new gen.",hardware,2025-12-22 00:50:11,2
AMD,nwpez0b,its like people claiming 1080ti is still great despite being worse than the budget variants of modern cards and having none of the modern features.,hardware,2025-12-30 09:48:13,2
AMD,nv8q2e2,>We saw the same with people exaggerating the longevity of Sandy Bridge during the pandemic.  Did we? Personally I was on Ivy Bridge until 2022,hardware,2025-12-21 19:00:16,2
AMD,nvdltcw,"$200 difference in a $1000-1200 build isn't insignificant, but it's also probably not worth hobbling your system over, either.  Going AM5 gets you better performance for the entirety of its life, it gets you upgrade options in the future, and it also gives you better longevity so that no matter what, better stuff will be available by the time you actually do really want/need to upgrade again.  $200 over a five year ownership period is just $40/year(or like $3/month).  I think people should remember to consider this kind of perspective on things when buying a PC, at least for anybody who cares about getting good overall value.",hardware,2025-12-22 14:54:38,1
AMD,nv2n1vm,"I mean....that's pretty much true for everything now though, isn't it?  Given how Sammy's more or less fucked the entire market, not just DIY, and some hyperscalers are finding it pretty damn hard to jump to newer EPYC platforms, I can see there being enough demand to justify firing the Zen 3 X3D line up for a little bit.",hardware,2025-12-20 18:50:03,-6
AMD,nwpf8ei,pretty sure the average GPU can output in 1080p.,hardware,2025-12-30 09:50:37,1
AMD,nv9n1kt,"I am talking about upgrading from current consumer tech.   Current tech can easily run 2k monitors just fine, even with APUs. My point was not about GPUs specifically, nor about gaming, it was about 8k monitors. Dual resolution monitors are a thing, you know.",hardware,2025-12-21 21:51:53,-2
AMD,nv3tx24,[TechSpot/HUB tested with 6 different RAM kits](https://www.techspot.com/review/2915-amd-ryzen-7-9800x3d/)  There was less than 2% variance between them.,hardware,2025-12-20 22:45:10,15
AMD,nv39i23,"5900x sounds perfect for my needs, thank you :)  Why are 5800x3d going for silly prices then? Just random market insanity?",hardware,2025-12-20 20:49:55,4
AMD,nv3uhr3,People are gonna cry and want $200 5800X3D forgetting it had an MSRP of $450...,hardware,2025-12-20 22:48:37,5
AMD,nv3vm0v,X3D is a separate production line with its' own constraints. There is limited packaging available for X3D that is not shared with the normal SKUs.  X3D being EOL is not tied to the normal Zen 3 SKUs. The normal SKUs they can shurn out as long as there is demand from both server and desktop.  X3D however might very well be supply constrained. And making lower margin Zen 3 variants might cut into scaling Zen5 X3D SKUs.,hardware,2025-12-20 22:55:24,10
AMD,nvhqc95,"They'd be wise, in that scenario to focus on GPPU stuff and only have AI gains that come with overall uplift because, well, bubble.",hardware,2025-12-23 04:31:39,1
AMD,nvc3mxw,"Yep this is exactly the case some people find themselves in. They have something like an i5 6600K or 7600K which are 4c/4t CPUs, completely outdated in 2025. Yet they have 16GB or even 32GB of DDR4 that is still usable! Upgrading to something like a Ryzen 5500, 5600 or 5700X on a cheap B450 or B550 board is a huge upgrade for very little cash, and they spend $0 on RAM as they're reusing what they already have.",hardware,2025-12-22 07:36:15,6
AMD,nvai6f6,"Sure, but the majority of consumers for most of AM4's life cycle were on 16GB of RAM. [Go look at Steam Hardware Survey in May 2021](https://web.archive.org/web/20210512095214/https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam?platform=pc) at the height of AM4's popularity and performance leadership (well into Zen3) only 12% of Windows systems had more than 16GB and people were still rolling with Intel too back then. You'd be hard pressed to find anyone with 32GB of RAM on AM4 really in their old systems if they're still using them. 32GB really only became very prolific with AM5 and Z690 thanks to DDR5 density. I guess if you're okay with having 16GB of RAM it would be okay, but at that point might as well just sit on one stick of 16GB DDR5 or 2 8GB sticks of DDR5 till this whole AI memory shortage blows over.",hardware,2025-12-22 00:47:06,2
AMD,nvqoju1,If it's solid DDR4 it'll probably fetch a good price at the second hand market.,hardware,2025-12-24 16:45:52,1
AMD,nwpetsn,"if you already have solid DDR4, you arent going to be gaining anything from a new chip.",hardware,2025-12-30 09:46:52,1
AMD,nv442xz,"There's a whole 30%ish of the American population (loosely college educated several years into their career and moderately successful small business owners) that are doing better than ever.   The median person (not highly educated, minimal to moderate career development) might be feeling financial pressure, but there's enough of the top third (100 million people) RIFE with cash to prop up entire industries.      There's a lot of people for whom a new computer every few years is something like 1-3% of their disposable income.",hardware,2025-12-20 23:47:22,5
AMD,nvoyk0f,"They could reallocate whatever they like, I suspect.  But I think it's *very* unlikely there will be any significant production of AM4 X3D SKUs.  X3D was created for Epycs, with the consumer parts being essentially the bin rejects - and who the hell is buying Zen3-based Epycs now?  AMD will carry on producing AM4 CPUs for a while - they're cheap to make, thoroughly supported and more than good enough for a pretty wide range of use cases.  I'm still using one myself as a daily driver and it's absolutely fine.  But X3D is an expensive process and AMD aren't going to restart an obsolete version of it to produce low-price consumer SKUs.",hardware,2025-12-24 09:31:59,1
AMD,nvdmof2,"And I'm still on Ivy Bridge today with my 3570k.    But I'm also under no illusions that my system is massively outdated and has been for quite a while and that playing most of the latest heavier hitting games is basically a total impossibility.    You and I were just being cheap/patient bastards.  It had nothing to do with our CPU's genuinely being great CPU's up through 2022, much less today.",hardware,2025-12-22 14:59:17,2
AMD,nv2zgj8,"> I mean....that's pretty much true for everything now though, isn't it?  Not for stuff made with traditional packaging tech",hardware,2025-12-20 19:55:12,8
AMD,nwq20oo,I've only ever heard people and companies say 2k in reference to 1440p so idk what you mean,hardware,2025-12-30 13:00:20,1
AMD,nvhkes2,"You are vastly overestimating the prevalence of ""spec out the CPU socket type for the PC I'm building"" hobbyists as a percentage of the consumer market.  Most of 'the consumer market' buys common PCs off the shelf at big-box general retail store, and 8K dual-resolution monitors have never once entered their mind.",hardware,2025-12-23 03:51:57,2
AMD,nv42y1e,"you can definitely squeeze an extra 2-3% by fully tuning hynix a-die, EXPO profiles are \*really\* loose in all the subtimings. But most people won't bother. As someone who spent a lot of time tuning ram the juice definitely isn't worth the squeeze for normal people.",hardware,2025-12-20 23:40:16,6
AMD,nv3p46b,"Just supply and demand. X3D's are hype (with good reason), the AM4 ones  are out of production, and people who have them aren't upgrading/selling because they are still very capable. The price isn't proportional to performance, the 5950x or 5900x for example are within 5% in performance but half the price since the x3d hype is so powerfull. Not that they don't deserve that hype, but other excellent options are overlooked as a result of how they dominate the discussion around cpus for gaming.",hardware,2025-12-20 22:16:50,4
AMD,nv4dwmk,"I bought a 5700x3d last fall for $135 including shipping and tax, and at that price it was a no-brainer to upgrade.  But I had already decided that $450 for a 5800x3d was too much when I already had a 5600x.",hardware,2025-12-21 00:47:27,3
AMD,nvktyda,"Much of the work on AI optimizations would also carry over GP-GPU.   At some level tensor multiplication is tensor multiplication.   There are cases where one set of tradeoffs is more important in one use case vs another but overall... a rising tide lifts all ships.     My suspicion for these is that much of the reason why nVidia started focusing on ray tracing and DLSS is that the uarch optimizations that happen to be somewhat useful for those are VERY useful for general AI training. I'd have to dig into details though.   I'd actually agree that using machine learning to do upscaling is an overall smarter and more efficient way of doing things than just brute forcing more raster calculations. Frames upscaled by DLSS are something like 200-400% more energy efficient (AI generated info take with caution) and the amount of die space dedicated to tensor cores is pretty minimal, just a few percent.",hardware,2025-12-23 17:42:27,1
AMD,nvcjg95,"I’m not talking about blanket recommendations, please keep in mind steam hardware survey is not representative of commentators here.   Many of us may have picked up 32GB of fast DDR4 during the over supply for really cheap. Or even 64GB like me.",hardware,2025-12-22 10:14:06,0
AMD,nv7fc83,Yup. I got into pc building again this year. Comfortably afforded a 5090/9800 etc etc. the poor are worse off than ever but the PMC middle class- especially the child free are doing just fine.,hardware,2025-12-21 14:59:09,1
AMD,nwq5b3t,"Then every single one of them are wrong, because 2k is 1080p. 1440p is the rarely used 2.5k. Heres a handy graph: https://upload.wikimedia.org/wikipedia/commons/0/0c/Vector_Video_Standards8.svg",hardware,2025-12-30 13:21:25,2
AMD,nvjghah,There is no other reason to upgrade. Consumers have other desirable products to shop for.,hardware,2025-12-23 13:24:32,1
AMD,nvcmowu,"> I’m not talking about blanket recommendations, please keep in mind steam hardware survey is not representative of commentators here.  You never said that lol.  >Many of us may have picked up 32GB of fast DDR4 during the over supply for really cheap. Or even 64GB like me.  Okay and you're the minority of gamers/AM4 users.",hardware,2025-12-22 10:45:12,1
AMD,nwq86m7,Ah okay I get it thanks,hardware,2025-12-30 13:38:52,1
AMD,nuvx0lm,No one is buying this for 800  Stop the sensational headlines  You can get a 9800x3d and 32gb ddr5 cl30 kit at MARKET PRICE for under 800,hardware,2025-12-19 16:33:05,413
AMD,nuvxupa,"Another clickbait article, yeah no shit if you order by highest price sales on ebay it'll look like this. If you instead look at recent sales this month they're all around $390-530.",hardware,2025-12-19 16:37:07,95
AMD,nuzu0bp,Trash fake news. Delete this garbage,hardware,2025-12-20 06:35:55,4
AMD,nuwxrrc,Glad I bought my 64GB of RAM months ago. Cost less that $100,hardware,2025-12-19 19:35:30,5
AMD,nuwq8mq,How many people on am4 actually made the leap to 5800x3d than just the normal 3600 > 5600x/5700x and thats it or 1600x to 3600x to just 5600x?,hardware,2025-12-19 18:56:54,3
AMD,nv0hu86,Why do we still allow Tom's Hardware articles in this sub at all?,hardware,2025-12-20 10:36:00,3
AMD,nuw6vyx,There was a brief period these chips were an excellent buy and very cheap but that ended a while ago. Last time I checked at the middle of this year they were £350+ and now I see they are £450+.  I get that it allows you to max out an AM4 system and if you have a bunch of DDR4 that may be a wise investment still but the days of it being the undisputed price:performance king are long over.,hardware,2025-12-19 17:21:38,2
AMD,nuvtqz8,"Hello I_Love_Cape_Horn! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-19 16:17:26,2
AMD,nuwiuja,Honestly I want a 5600x3d with the lower power draw and TDP. My mATX system is somewhat small and I’m not playing the most demanding games on it but I’d like to get a few more years out of AM4,hardware,2025-12-19 18:20:36,2
AMD,nuzlkb7,"oh ye, i am selling my one for $69420...now go write another article",hardware,2025-12-20 05:24:02,2
AMD,nv03534,"I upgraded my AM4 AB350 system from first gen Ryzen 1600 to 5700X3D and the gaming results lined up with the reviews. But these chips falter with high RT usage games that have even higher demands on the CPU and RAM.    For example, in Stalker2 CPU-limited scenario, 9800X3D was almost 2.5X of its performance. I doubt that non X3D would be more than 20-30% slower, so it'd be better to get AM5 instead.",hardware,2025-12-20 08:04:11,2
AMD,nuwg9pf,"What? I mean if you'd actually wanted to spend 800$ on just the CPU, why not just get a cheap AM5/1851 one and then get yourself some overpriced RAM instead?  This makes no sense at all.",hardware,2025-12-19 18:07:58,3
AMD,nuwyxiv,"A 7600X is faster than a 5800X3D in most games, if you are buying a 5800X3D for $800 you are an idiot.  A 5600x will still be GPU limited at the resolutions and settings people actually play games at.",hardware,2025-12-19 19:41:34,2
AMD,nuvxgle,No shit.  This happens with all older hardware as companies want you to buy new stuff.,hardware,2025-12-19 16:35:14,2
AMD,nuyqqpa,I was thinking to sell my old DDR4 memory....,hardware,2025-12-20 01:59:20,1
AMD,nuz5dtf,This crap of supply and demand can stop anytime now.,hardware,2025-12-20 03:32:36,1
AMD,nuz7xl1,The RAM I own will appreciate faster than gold in the next year or two woohoo,hardware,2025-12-20 03:49:34,1
AMD,nuzgtjq,What enthusiasts are on 3 generation old hardware? I consider myself a pretty big enthusiast and never skip more than 1 generation.,hardware,2025-12-20 04:50:09,1
AMD,nv019a6,Jumping from am4 to am5 is impossible though because of insane prices too,hardware,2025-12-20 07:45:39,1
AMD,nv0nfhr,I just bout a Ryzen 7 7000 series drum 220 brand new.,hardware,2025-12-20 11:32:05,1
AMD,nv0tj1j,Someone wanna give me $1000 for my... DDR3!,hardware,2025-12-20 12:27:28,1
AMD,nv0ypmd,Am4 is dead.   LGA 1700 is where it’s at rn. Having compatibility for either ddr4 or ddr5 ram is much better than am4.   And the lga 1700 CPUs especially the 13th and even more the 14th gen intel CPUs like the 14700k are way faster than any am4 cpu as they compete with am5 CPUs anyways.,hardware,2025-12-20 13:08:24,1
AMD,nv1ur9z,"Maybe I should sell mine, I'm using it in an HTPC that I rarely game on anyway.",hardware,2025-12-20 16:20:57,1
AMD,nv387oo,I just sold mine for 375$ yesterday.,hardware,2025-12-20 20:42:54,1
AMD,nv807rw,"Hey I’ve got a 3800X, I’ll sell it for 3.8 million USD if anyone is interested.",hardware,2025-12-21 16:50:33,1
AMD,nvc21an,"and Ryzen 5 5500 is only $75. sure, having two more cores and 6x the L3 cache is nice, but not 10x the price nice.",hardware,2025-12-22 07:20:33,1
AMD,nuxfsfx,This is more sad than anything,hardware,2025-12-19 21:10:13,1
AMD,nuvzbwi,something something... turntables.,hardware,2025-12-19 16:44:17,1
AMD,nv39ar3,"Pleased my 5950x, 3090ti and 64gb of ddr4 are still keeping up just fine….",hardware,2025-12-20 20:48:50,1
AMD,nuw0llz,"There are still places that do sales on AM5 stuff. You can buy a 7800X3d , 32GB ram and motherboard for $580. There is literally no need to spend $800 for a 5800X3D",hardware,2025-12-19 16:50:28,0
AMD,nuwjz7y,Gamers would rather pay 800 for this than get Intel. The gamer brain rot is real.,hardware,2025-12-19 18:26:07,-3
AMD,nuvxsgy,Computers are quickly becoming a luxury like in the 90s.,hardware,2025-12-19 16:36:49,-2
AMD,nuw8wrz,"they are trying their best to milk the market. Push prices up...   Fucking Tech websites, all in the pocket of other big corpo's making artificial scarcity.",hardware,2025-12-19 17:31:36,-1
AMD,nux8y3k,"A couple of sales, (literally just a few) on ebay is no big deal...there's wacky people doing wacky things out there all the time.  The two or three people are pointing out on ebay are dwarfed by the normal sales flow from normal websites...     ""a thing happened ***once***!  let's all discuss it like it's a regular occurrence and changing the status quo! ""    it's all so tiresome.",hardware,2025-12-19 20:34:13,29
AMD,nuvxsbh,"Check eBay.  [https://www.ebay.com/sch/i.html?\_nkw=5800x3d+cpu&\_sacat=0&\_from=R40&rt=nc&LH\_Sold=1](https://www.ebay.com/sch/i.html?_nkw=5800x3d+cpu&_sacat=0&_from=R40&rt=nc&LH_Sold=1)  $800, no not quite.  Over MSRP?  Yes.",hardware,2025-12-19 16:36:48,24
AMD,nuybnat,"Yep.   https://www.microcenter.com/product/5007092/amd-ryzen-7-9800x3d,-asus-b650e-e-tuf-gaming-wifi-am5,-gskill-flare-x5-series-32gb-ddr5-6000-kit,-computer-build-bundle  CPU, memory and motherboard $679.00",hardware,2025-12-20 00:25:52,3
AMD,nuxd62y,> at [MARKET PRICE](https://youtu.be/5KXrQYWbbIs?t=17),hardware,2025-12-19 20:56:34,1
AMD,nv01auj,What motherbard ?,hardware,2025-12-20 07:46:05,1
AMD,nv2vhyh,"Yeah this headline is stupid. I see *new in box* 5800X3D selling for $500-600 on ebay, and plenty of used sales in the last few days between $380-500. The price has risen a bit, but not that much. It's been expensive ever since production of the 5700X3D ended.",hardware,2025-12-20 19:33:57,0
AMD,nuw7w2c,"And even then, sales VOLUME is what matters. There will always be people who can't do basic arithmetic, who will then buy at these prices, instead of selling their old system and getting something brand new. That doesn't mean that $400 for a 5800X3D is sound market pricing, or that it is worth this much.",hardware,2025-12-19 17:26:34,14
AMD,nux817d,"I bought a pair of 16 gig sticks in an attempt to beat the first round of tariffs. At the time I thought they were DDR4 and I wasn't paying much attention. They came in, I see they are DDR5 and think ""ok, future rig then."" They came in around 120 bucks. Fast forward to today, I check the price of the same pair of sticks, it's over 400. Genuinely considering selling the sticks for a better AM4 CPU.",hardware,2025-12-19 20:29:19,1
AMD,nv1uece,"I went from 2700X to 5800X3D and upgraded ram from 16GB to 32GB, so I'm good until Zen 6 at least. I got it for 340€ like 5 months after release in germany. If I sold it now I would make 50€ profit, its going for around 390-400€ used on ebay. Not bad.",hardware,2025-12-20 16:19:03,1
AMD,nux0gbj,"A 5600x will play games just fine, at the resolutions and settings people play games at you will still be GPU limited on a RTX 5070.",hardware,2025-12-19 19:49:24,1
AMD,nv10bcf,I went from the 5800x to the 5800x3d. Stuck the 5800x in my daughters PC instead of buying her a 5600x like I did for my son's PC.,hardware,2025-12-20 13:19:44,1
AMD,nuwm8c2,"It really only makes sense if you already have an AM4 motherboard. Otherwise, if you're doing a DDR4 build Intel 14th gen outperforms it.",hardware,2025-12-19 18:37:12,6
AMD,nv1zj4d,You can limit the power usage on the 5800x3d or 5700x3d as well if you find a deal on one of those.  I have the 5800x3d and the low power usage is great.  In a lot of games the chip is only drawing ~50W.,hardware,2025-12-20 16:46:19,1
AMD,nuwkmx2,Because no one sane would do that... they simply took the most expensive offer on eBay and made a clickbait title around it... I may list my old PC parts for 10k; maybe they shall made another such article...,hardware,2025-12-19 18:29:21,12
AMD,nuw3s4d,"Or it's discontinued and a sketchy seller is hoping to con someone who doesn't know any better. I've seen Ryzen 5 2600's going for 200+ on Amazon from 3rd parties while official channels were selling Ryzen 5 5600's for 130 and 3600's for 90.  Honestly, whenever I see a price on Amazon that's not remotely close to a whole number or 25 cent increment (eg. 137.53), I pause and see if something's fishy.",hardware,2025-12-19 17:06:12,14
AMD,nuw1zkt,"Seconding this, a lot of flagship CPUs released in the past 20 years are still absurdly expensive. QX9770, FX-60, P4 EE, 6950X, 9900K, etc. some of them more expensive than others, but the top CPU for a dead socket is still a pretty penny",hardware,2025-12-19 16:57:17,3
AMD,nuwafnv,Could also be used by sellers to fulfill insurance replacements.,hardware,2025-12-19 17:39:15,1
AMD,nuzi5cp,I thought we all loved Capitalism?,hardware,2025-12-20 04:59:16,1
AMD,nux0pp4,"You can still buy laptops for $200 on Amazon, they good enough to do most computing tasks.",hardware,2025-12-19 19:50:46,6
AMD,nuwepit,I had a PC in the 90s didn't know it was a luxury back then guess I was a lucky kid,hardware,2025-12-19 18:00:15,3
AMD,nuwjwna,"Compute has never been cheaper and fairer. You can get a Mac Mini right now for $450, which will do all you need, including light productivity, sans gaming and heavy 3D rendering etc, for the better part of the next decade.   All the harder compute that you need you can rent and get the best value, instead of shelling out for hardware that becomes obsolete within 2 years:  * for inference either get the subscription or pay as you go on Replicate, OpenRouter etc.   * for gaming get GeforceNow or similar service for $10/$20 a month. On MacOS it runs natively in AV1 with Cloud GSync, there's virtually no lag and even in very dark scenes you can barely tell it's a stream. Hardware in the back gets upgraded every 2 years, and 5 years of the service cost less than the GPU it's running on right now.",hardware,2025-12-19 18:25:46,-1
AMD,nv13sos,"I get your whole point, but I just want to pinpoint the wacky part, maybe they have a reason for it. Like, one quick example, they have the whole built already working, and the CPU dies or gets fried, or god knows what, and they want the same CPU, idk. Which goes hand in hand with your other point of it being 2-3 sales. The CPU is not being made anymore, so Ebay is the only alternative. Maybe even a collector. There are wacky people out there thought :P",hardware,2025-12-20 13:43:08,0
AMD,nuvyc81,Still too expensive   Still the price of a non x3d zen4/5 or raptor lake + ram kit which outperforms this,hardware,2025-12-19 16:39:29,20
AMD,nuy93oc,Yea I sold mine for $425 two weeks ago after upgrading to 9800x3D for $440,hardware,2025-12-20 00:10:24,4
AMD,nuwmtx7,"People are insane, in what world does that purchase make any sense.",hardware,2025-12-19 18:40:08,1
AMD,nuxvltb,"A fair number of these purchases are probably going to end up with fraudulent charges (item not as described, etc) because eBay tends to side with the buyer.",hardware,2025-12-19 22:36:46,1
AMD,nv2w89z,It's worth whatever it consistently sells at. It seems to consistently sell for >$400 used.,hardware,2025-12-20 19:37:50,1
AMD,nuwqrn0,"It's a lot like the ""car is worth $2000 but needs $2000 in repair"" work making it worthless but they're low credit high APR folks and have cash and $2000 in repairs makes sense to get it going another 2 years",hardware,2025-12-19 18:59:31,1
AMD,nv5shn8,Did you ever use more than 32? How often do you need more than 16 really? I'm asking because I stuck with just 2x8 and never had any issues yet.,hardware,2025-12-21 06:32:58,1
AMD,nv67xle,And they were right.,hardware,2025-12-21 09:04:18,1
AMD,nuzh31n,"Unless you play heavily CPU bound games, which many BRs, and simulation/factory games fall into.",hardware,2025-12-20 04:51:52,7
AMD,nv2wocg,"""Games"" are not a uniform performance load. Tons of games will be CPU bound with *any* CPU, including the 9800X3D, even at 1440p.  As always, know your workloads and purchase accordingly.",hardware,2025-12-20 19:40:14,7
AMD,nuwqvyj,"For productivity yes, but not for gaming.",hardware,2025-12-19 19:00:07,-3
AMD,nuwn1s8,"If you filter on Sold listings, people *are* paying $4-500+ for these.  It’s still insanity even if the headline is sensationalist.",hardware,2025-12-19 18:41:13,8
AMD,nuye7zb,Oh? I have a 6950X sitting in a closet…,hardware,2025-12-20 00:41:41,1
AMD,nvi69cl,"For the older ones it might be people wanting to build top-tier rigs of INSERT_YEAR?  I should get on with my want of a top 2012-2013 build, but knowing me I’d probably go seeking for silly things like Titan Z or ARES 2/3.",hardware,2025-12-23 06:37:02,1
AMD,nuw2ya1,I had a 9900K in my workstation at the office back in the day.  Solid performer for the money.,hardware,2025-12-19 17:02:02,0
AMD,nuxyrzw,Adjusted for inflation it’s around $7000. Yeah they were luxury,hardware,2025-12-19 22:55:10,5
AMD,nwlzxzc,"Got a used Pentium I in 1995 (my first PC) and it was about the equivalent of 80 USD, but that meant a lot more in local buying power.",hardware,2025-12-29 20:52:19,1
AMD,nuxwmxj,"Mac's can run games  (native ports) decently too, considering their loss power draw.",hardware,2025-12-19 22:42:45,2
AMD,nuxegrj,Stop being logical   We need 64gb and a 5090 to browse reddit and watch youtube,hardware,2025-12-19 21:03:20,2
AMD,nwm0c5u,> for gaming get GeforceNow or similar service for $10/$20 a month.  Streaming is not and will not be a valid option for gaming.,hardware,2025-12-29 20:54:14,1
AMD,nuxeceb,"Okay Sam, no need to advertise your shitty services.",hardware,2025-12-19 21:02:43,-1
AMD,nv62uqm,"Even then, it would be cheaper to get a whole new mobo, CPU, and RAM on AM5 and end up with better performance.  It's an absolutely ridiculous price, no matter what. You'd have to just be a blithering idiot to go for it.",hardware,2025-12-21 08:12:41,7
AMD,nv6lbmy,"If that situation forces a sale at $800 though, that does suggest that the market is very shallow.  Generally one goes on eBay and picks the cheapest or 2nd cheapest that doesn't look shady.",hardware,2025-12-21 11:17:45,1
AMD,nuwqh45,"One just sold for $600  Sold Dec 19, 2025    Brand New  [51 product ratings- AMD Ryzen 7 5800X3D Processor - NEW in Sealed Box](https://www.ebay.com/p/4053561416?iid=168015596458#UserReviews)  **$600.00**  or Best Offer  \+$7.50 delivery  Located in United States  [View similar active items](https://www.ebay.com/sch/i.html?_nkw=AMD+Ryzen+7+5800X3D+Processor+-+NEW+in+Sealed+Box&_id=168015596458&_sis=1)  [Sell one like this](https://www.ebay.com/sl/list?mode=SellLikeItem&itemId=168015596458&ssPageName=STRK%3AMEWN%3ALILTX)  gangster1234484 100% positive (522)     Being kind and logical, that's what you're missing :) <3",hardware,2025-12-19 18:58:05,-27
AMD,nv2vx8m,"The RAM kit alone is more than the 5800X3D.  People aren't buying the 5800X3D over a new AM5 processor. They're buying it over a whole new platform, which is ~$1000 including board and RAM.",hardware,2025-12-20 19:36:12,-1
AMD,nuzrjzn,What?  I'm a Top Rated Seller and very happy,hardware,2025-12-20 06:13:52,1
AMD,nwlxys6,Theres also folks who do their own repairs so thats 2000 in repairs becomes 200 in parts + couple of your weekends gone.,hardware,2025-12-29 20:42:37,1
AMD,nv0z33q,14700k slams a 5800x3d both in gaming and especially on productivity tasks.,hardware,2025-12-20 13:11:08,5
AMD,nux0vvp,5800X3D is [13% slower than a 14700K with DDR5-6000 in gaming](https://www.techpowerup.com/review/intel-core-i7-14700k/18.html). Unfortunately I can't find a direct comparison with the 14700K using DDR4.,hardware,2025-12-19 19:51:39,2
AMD,nv10uf1,"For some upgrading to AM5 would indeed cost a lot right now, but I also feel like a lot of people don't realize that the slowest AM5 CPU (7600X) is just as fast as the 5800X3D. Including for gaming.",hardware,2025-12-20 13:23:21,6
AMD,nv20xqj,Nice! Those go for around $125+ on eBay regularly. People want to max out their old rigs,hardware,2025-12-20 16:53:46,1
AMD,nwm06wu,As i am writing this reply from 32GB and a 4070S i can confirm we can do with a slightly less for reddit.,hardware,2025-12-29 20:53:32,1
AMD,nwnhqs9,"It absolutely is. It almost feels local. And on Mac it's AV1 so it even looks almost local. It for sure looks better than anything my PC can pull off.  I can play on my phone with a simple android controller. When Im visiting my folks, all I gotta do is download the app on their smart TV and log in, and pick right up since the saves sync via steam.  Back home I simply turn it on in my TV in the living room, on my laptop wherever, or in the TV in my room upstairs.  Not need to carry anything. Don't care if one TV is occupied. It's always dead silent, since theres no 500w machine that needs to be cooled.  You don't event need to take my word for it, you can try it for like 3 bucks for 48hrs.",hardware,2025-12-30 01:34:28,1
AMD,nuwzngm,"Pretty sure it's just not showing the ""Best Offer"" price it *actually* sold at.",hardware,2025-12-19 19:45:15,21
AMD,nuy0cew,What's so rude about what he said?,hardware,2025-12-19 23:08:57,8
AMD,nv2wy4l,No its not  Newegg has combo deals ram and good board for 400ish dollars  A crappy 7600 beats the 5800x3d,hardware,2025-12-20 19:41:43,3
AMD,nuxw4rc,"There are edge cases where no CPU can touch the X3D chips (e.g. Factorio, Baldur's Gate 3).",hardware,2025-12-19 22:39:50,3
AMD,nv3is85,"Hardware Unboxed's 5800x3d vs 12700kf (with ddr4) comparison was pretty much a wash, particularly if you are >1080p. Nominal win for the 5800x3d for gaming if you are being charitable. (I have both, but haven't done any real comparison testing, particularly because I run 4k.) So add on top some clock speed gains and there you go.",hardware,2025-12-20 21:41:10,3
AMD,nv3suby,Yeah I don't really get it. When they were sub 200 or even sub 150 for a 5700x3d it was an awesome buy but now it just feels like people panic buying.   Maybe they were waiting for zen 6 and just are afraid that ddr5 will be screwed for many years but I don't see the point in paying that much when you can still buy a faster 7700x microcenter bundle  even with 32 gb of ram for 500.  Even with ram like this I don't see why you would pay over like 250 tops for a 5800x3d.,hardware,2025-12-20 22:38:50,3
AMD,nwozpbc,You must be playing something like turn based games or what is the reason you arent aware of the massive input delays?,hardware,2025-12-30 07:26:12,1
AMD,nv788ag,Something to understand anout the average /r/hardware poster is that they believe that X3D chips are magic performance improvers that are instantly better than every other CPU ever produced by impossible margins.,hardware,2025-12-21 14:15:27,4
AMD,nv634a1,"Not sure why you're getting downvoted. Micro Center too has brought back their bundles with RAM for around $500. And, yes, a 7600 beats a 5800X3D. A 7500f goes toe to toe with it.",hardware,2025-12-21 08:15:24,2
AMD,nuy96u7,"Even in factorio once you go to [high spm comparisons](https://factoriobox.1au.us/results/cpus?map=9927606ff6aae3bb0943105e5738a05382d79f36f221ca8ef1c45ba72be8620b&vl=1.0.0&vh=) rather than the the low SPM ones, raptor lake/non X3D zen4/5 does fine.",hardware,2025-12-20 00:10:56,5
AMD,nwlynwf,Basically if your primary worload is larger than Intel L3 cache but smaller than the X3D cache and results in high cache hit rates for AMD and not Intel you get crazy boosts. Works in a lot of MMOs for example.,hardware,2025-12-29 20:46:03,2
AMD,nwlz5ux,"Its not that deep. A person wants to build a PC but does not know much about hardware. So he goes and watch some youtuber techfluencer for advise. He finds a video stating DDR5 is getting expensive, buy DDR4 (recent HUB video for example). He also finds video 5800x3D best DDR4 CPU for gaming. He looks no further and just buys those parts.  Got to remmeber most people dont follow the hardware news like we do. They spend 30 minutes on youtube and make their purchasing decisions.",hardware,2025-12-29 20:48:30,1
AMD,nwpyqrq,"On ShadowPC or PS+ Streaming, yes, there's a delay. On GeForce now, there's barely any.",hardware,2025-12-30 12:37:12,1
AMD,nwlx6fh,Depending on what your usecase the x3D can be magic. For example in mmos that get very CPU bottlenecked when there are a lot of players in the same place x3D due to much higher cache hit rates can mean as much as triple the framerate in raids (when it matters most).,hardware,2025-12-29 20:38:42,1
AMD,nxgkf8w,"Good for what? If you mean good for the price, 9060xt offers better fps per dollar if you have RT off.",buildapc,2026-01-03 16:19:38,2
AMD,nxgkiug,Yes that is good.,buildapc,2026-01-03 16:20:07,1
AMD,nxgmd6n,It will game.,buildapc,2026-01-03 16:28:55,1
AMD,nxgmr1r,Yes.,buildapc,2026-01-03 16:30:43,1
AMD,nxgnx69,The 7500F combos well with everything that isn't a 4090/5090,buildapc,2026-01-03 16:36:14,1
AMD,nxgo4o1,"Depending on what your use case is, you may be better off with a 16GB version card, especially if you're planning to be at 1440p/2160p for resolution. So, in this case it's still a good pairing but I would consider the AMD Radeon 9600 XT 16GB which can typlically be found around the same price as the 5060 Ti 8GB, or the 16GB version of the 5060 Ti or the RTX 5070 if you can swing it.",buildapc,2026-01-03 16:37:12,1
AMD,nxgottp,"Its a perfectly fine cpu and gpu.  I wouldn’t recommend it for 4k gaming, but a 7500f and 5060ti (preferably 16 gig of vram vs. 8) will do reasonably well at 1080p or 1440p.  The next question is price.",buildapc,2026-01-03 16:40:26,1
AMD,nxgqawp,"is the Intel Corei5-12400 better? my budget is like 1500€, probably more because I want to upgrade it over time. CPU 's and GPU prices are rising so that is my main priority",buildapc,2026-01-03 16:47:21,1
AMD,nxgtbzt,>is the Intel Corei5-12400 better?   No and nope.  Think about a 9070 non-XT,buildapc,2026-01-03 17:01:25,1
AMD,nxijify,"The 12400 usually is a little better than a 5600x, worse than a 7600x.  A 14600k is usually as good, or a little better than a 7600x in gaming with the cost of efficiency.    The benefit of LG1700 with the 14600k, is you can find mobo’s that use DDR4.",buildapc,2026-01-03 21:55:17,1
AMD,nxvff5m,"4080 Super is slightly better than the 5070 Ti but it's used (presumably), which creates some risk. 9070 XT is almost but not quite as good as the 5070 Ti, and with that price difference I'd go with the 9070XT. The 5070 should really only be an option if you're planning on doing more video editing than gaming.",buildapc,2026-01-05 19:37:55,31
AMD,nxvxfd9,"9070 XT is the value buy right now - very capable for gaming, a solid price, and 16 GB VRAM means it will remain useful for a long time.",buildapc,2026-01-05 21:01:49,8
AMD,nxv77cy,What are you running (games/programs)? Are you sticking with the 1080 monitor?,buildapc,2026-01-05 19:00:22,7
AMD,nxvoce9,I just upgraded to a 9070xt from the 3080 and it's been great. You should really get a better monitor though because I think all these gpus are overkill for that.,buildapc,2026-01-05 20:19:20,4
AMD,nxyh2by,I just picked up a 9070 XT for $570 at micro center this past weekend. If you go with a different brand I’m sure you can find it cheaper,buildapc,2026-01-06 05:15:03,3
AMD,nxw6v6o,"9070 XT. That's the one and only answer.  The 5070 is easily outclassed by the 9070 XT so that shouldn't even be in consideration.  The 4080 Super is slightly better than the 9070 XT but not by much. The downside is that it comes with the horrible 12VHPWR connector that is known to go up in flames, you only get 3 months of warranty, and you'll be missing out on newer software features for upscaling and frame generation.  For a $70 price difference, I would say that the 9070 XT is worth it.",buildapc,2026-01-05 21:45:25,2
AMD,nxyrz91,"If you generally play games from bigger studios and don't have any special feature needs like noise suppression AMD is probably the better bet. You won't be affected by stuff like AMD Noise suppression being broken for the past few months and can just enjoy the up front discount.  I'd also suggest checking out the basic 9070 if you're planning on staying at 1080p, it's not pushed to the limit like the XT to compete with the 5070Ti so you get some nice power savings for not much performance loss.",buildapc,2026-01-06 06:40:28,1
AMD,ny205kn,"Stood for a similar choice this week. Upgrading from my older RTX 3070 (8GB) to a better graphics card. I chose an RX 9070 XT 16 GB.  Expecting graphics cards to only increase in price in the coming months, so if you want to upgrade, better do it now than to wait longer.",buildapc,2026-01-06 18:50:01,1
AMD,nxvqh1i,"4080S. ~5% behind 5080, though no MFG",buildapc,2026-01-05 20:29:19,1
AMD,nxvqnn4,"Nvidia will show the new dlss and dynamic MFG today, lets see if it gives you some new ideias.",buildapc,2026-01-05 20:30:11,1
AMD,nxvw9av,"All of these cards are overkill at 1080p imo, so I'd go 5070 Asus TUF if you have to get one of these or just stick with your 3080, it's still a fantastic card. That 5070 will be virtually silent under load, it's a very well cooled card.  9070xt is better, but techpowerup doesn't have it THAT far ahead of the 5070, 12-15% on average between 1080p and 1440p, see here: https://www.techpowerup.com/review/asrock-radeon-rx-9070-xt-monster-hunter-wilds-edition/31.html  You say you don't care about Nvidia features, but realistically you probably should, if only just for the support they have. If you're not playing games with reflex and at least SOME upscaling enabled then you're doing yourself a disservice.  4080 would make more sense than the 5070ti at that skyhigh price, but either are insanely overkill for 1080p as I said, so I think you'd be better off spliting your budget with a monitor at that point.",buildapc,2026-01-05 20:56:21,0
AMD,nxvsg6y,Same boat actually.  I have the 3080 10gb and am considering the 5070ti.  I have no clue what the price hikes will be like and also am interested in the 50 supers coming in this year supposedly,buildapc,2026-01-05 20:38:37,0
AMD,nxx8osk,just buy the 3090. 24gb vram makes it no contest,buildapc,2026-01-06 00:57:34,-1
AMD,nxvkfy9,"Thank you for your input, the good part is that the 4080 Super is sold by a reputable business and an actual physical store, so they offer a 3-month warranty should the card fail. But besides that, I'm guessing that the base 5070 is more of a sidegrade in terms of performance instead of a leap forward like a 4080S would be to a 3080?  In conclusion, the 9070XT is the better option considering fps-per-dollar? I know it's cheaper because resellers here generally sell Nvidia cards at a higher profit margin than AMD cards.",buildapc,2026-01-05 20:01:01,9
AMD,nxvudez,"If the 4080 Super was in very good shape (boxes, packaging, and everything) would you consider going for it instead of the 9070XT given that the former is $70 more expensive?",buildapc,2026-01-05 20:47:35,0
AMD,nxv8ksr,"minecraft modpacks with shaders, helldivers 2, arc raiders, dying light the beast, CP 2077, a lot of war thunder. elden ring, black myth wukong, Doom dark ages, among others.",buildapc,2026-01-05 19:06:33,3
AMD,nxvs9jp,"Yeah I'll probably look for another monitor all things considered. People say 1440p is way nicer than 1080p but idk if I'll see the difference, I went from playing on a 768p monitor in 2024 to a 1080p in the beginning of 2025 so that was definitely noticeable haha.  Also, how has your experience been with the 9070XT? It's been on my radar for a while but I haven't gone for it, yet.",buildapc,2026-01-05 20:37:45,3
AMD,nxvsws2,"I don't really mind not having MFG, and the 5080 itself seemed like a very small improvement to the 4080S without that tech in mind. If you had to choose between the 4080S and the 9070XT, which would you go for if the former was $70 more expensive than the latter?",buildapc,2026-01-05 20:40:46,3
AMD,nxvtoq8,"I just checked that out and it seems like 50-series super cards aren't coming out anytime soon, but DLSS 4.5 integration is their next big thing.",buildapc,2026-01-05 20:44:23,1
AMD,nxw0jy7,"Thank you for your input, I live in Colombia so most cards enter our market with insane overpriced values due to tariffs and taxes on electronic goods like these, so I feel like your comment is a more down to earth analysis of my position because of the 1080p monitor hahaha  I will definitely keep that in mind when I drive down to the pc hardware store tonight!",buildapc,2026-01-05 21:16:23,1
AMD,nxvumhv,"Same situation, upgraded to 5070ti when I found a $675 open box at MC",buildapc,2026-01-05 20:48:47,0
AMD,nxvnd7c,"Tom's Hardware has a ranking of raw performance on video cards that ranks every current video card against every other card, and here are their scores on 1080p gaming (the best current card, the RTX 5090, is 100%; everything else is a percentage of that performance):  RTX 4080 Super: 89.7%   RTX 5070 Ti: 85.7%   9070 XT: 85.6%   RTX 5070: 75.5%  So, if you're looking for the best gaming performance for the price, given those prices, the 9070XT is the best choice.",buildapc,2026-01-05 20:14:46,11
AMD,nxvobkm,"Some of the new upscaling tech will only work on the newer cards fsr4, dlss4.5 etc.",buildapc,2026-01-05 20:19:14,5
AMD,nxw6653,3 months warranty is nothing. Especially for a card that comes with a 12VHPWR connector which is known to go up in flames.,buildapc,2026-01-05 21:42:12,3
AMD,nxw2mmg,"For $70? Yeah, absolutely.",buildapc,2026-01-05 21:26:00,1
AMD,nxwi7d6,"I see others have already explained the differences between the cards and any of them will run the games you mentioned at high settings and refresh rate. I second the upgrade to a high refresh rate 1440 monitor, otherwise you won’t actually appreciate the difference. I upgraded to 1440 monitor and 9070XT last year and it was a huge improvement.",buildapc,2026-01-05 22:39:55,3
AMD,nxvt43p,"The jump to 1440p is pretty noticeable. 1440 with high refresh rate is a really nice spot, I'm in the same situation as you and wondering if my card dies where I should go.",buildapc,2026-01-05 20:41:43,3
AMD,nxvtvf8,"If you can't tell the difference between 1080p and 1440p, then you probably can't tell the difference between these different gpus either and should just get the cheapest lol...",buildapc,2026-01-05 20:45:16,1
AMD,nxvu03o,"4080S, definitely",buildapc,2026-01-05 20:45:52,0
AMD,nxwf07v,"Yup, I have a rtx 3080 but I will get a rtx 5070 ti probably. Lets see how the new tec will perform...   Dlss 4.5 will be for all RTX series since 2000. The Dynamic MFG until 6x will be for rtx 50 series only",buildapc,2026-01-05 22:24:03,1
AMD,nxw9afq,"Always happy to help!  If it were my money, I'm keeping the 3080 hands down, it's still very good and none of these cards are faster enough for how much they cost to justify the difference, but that's your call. 5070 is what, like 20% faster maybe?  https://www.techpowerup.com/review/asrock-radeon-rx-9070-xt-monster-hunter-wilds-edition/29.html  Different chart than what I linked before. We're talking paying almost $700 to get a 5070, to go from 122 fps on average to 146. If you went 9070xt it would be 122 fps to 163, or 170-180 for the 5070ti or the 4080.  Those aren't small jumps, but 122 is already VERY nice, and it's better than what the 5060ti or the 9060xt are doing as the current bang for buck 1080p/1440p recommendations.",buildapc,2026-01-05 21:56:33,0
AMD,nxvxntl,"Don’t blame you, nice snag!!",buildapc,2026-01-05 21:02:56,0
AMD,nxvv9fh,"Dang, thanks, I'll take that into account, though I was checking the GPU Benchmark Hierarchy and it showed the 4080S as the 4th strongest card on the market, above even the 9070XT or the 5070TI.   I'm guessing that, compared to a less expensive, unused, newer card like the 9070XT then the AMD option has a little more weight.",buildapc,2026-01-05 20:51:44,4
AMD,nxvw42g,"Yeah, thing is I haven't really had the chance to see what 1440p is actually like since all my friends stick to 1080p at 60hz, but I've been a little blessed with a good job this year so I wanted to buy myself a nice gpu to make the jump to 1440p",buildapc,2026-01-05 20:55:41,1
AMD,nxw1bdz,"The 4080S is the 4th best card on the market (behind the 5080, the 4090, and the 5090), but the difference between the 4080S and the 9070XT just isn't that huge.",buildapc,2026-01-05 21:19:55,7
AMD,nxxkv4r,"Updated drivers put the 9070 XT ahead of the 5070 ti, now (with frame-gen off): https://youtu.be/aWfMibZ8t00",buildapc,2026-01-06 02:03:05,1
AMD,nxyset6,"Argh. I have both sitting unboxed, and I can't decide which one to return. 9070xt in steel legend (though hearing horror stories about ASRock) next to the Asus Prime 5070ti. The difference is $100. I'm only playing unity and unreal engines games in 4k, not DLSS.  I was sold on the Asus with it being a better fan but now im not sure with this video...",buildapc,2026-01-06 06:44:12,1
AMD,nxz2c1h,My friend who does QA for newegg I think said they get lots of return for Asus GPUs because the fans are trash,buildapc,2026-01-06 08:13:39,1
AMD,nxz2eu1,Really? Everyone has been saying the Prime is one of the better ones. That's surprising!,buildapc,2026-01-06 08:14:22,1
AMD,ny3bj7q,"Nobody can tell you what your money is worth to you, but you won't find a better CPU + RAM + mobo bundle for less.  And the jump from 3600->7800x3d is *huge*.   The 7600x3d CPU is also great. They have a bundle which is something like $200 less, but it only comes with 16 GB RAM, so it kinda makes sense to grab the 7800x3d.",buildapc,2026-01-06 22:28:03,62
AMD,ny3csgc,"Those Microcenter bundle deals are about the best options for anyone looking to build new right now. It's worth it as far as if you bought the components seperately, absolutely  And its obviously a major CPU upgrade.",buildapc,2026-01-06 22:34:06,20
AMD,ny3g8u2,Pc builders cry about not having a MC nearby and still make the drive out. These bundles are the last saving grace for people who can’t get inexpensive ram. 3600 will see a massive upgrade and the 7800x3d bundle is the best value. If you have the money don’t hesitate before its out of stock again,buildapc,2026-01-06 22:50:56,10
AMD,ny3f31l,Uhh... That seems like an absolute steal considering where ram prices are at currently.,buildapc,2026-01-06 22:45:13,5
AMD,ny3fpup,Im in the midst of doing the same. I have an R5 3600 and bought a 7800x3d and 9070xt.   I was considering the 9800x3d but the price delta seemed better to get the 7800x3d since I'm shooting for 4k. The benefits of the 9800x3d are more prominent at 1080/1440,buildapc,2026-01-06 22:48:19,2
AMD,ny3dtz1,These are the best deals going,buildapc,2026-01-06 22:39:06,1
AMD,ny3hha6,"That's a great deal.  Just that CPU is $400 by itself.  I stayed on AM4 and upgraded to the 5800XT a few months ago.  It's fine for what I use it for, but definitely still has its limitations and runs pretty hot.  But it'll hold me over for a few more years.",buildapc,2026-01-06 22:57:03,1
AMD,ny3hxdj,"Huge difference if your games are even remotely cpu dependant (see esports games, BF6 and pretty much any fps if you're aiming for high fps). Even single player AAA games can be quite cpu intensive - cyberpunk is one example. If you can afford it, given you've upgraded to a 5070ti I think it's absolutely worth it.",buildapc,2026-01-06 22:59:15,1
AMD,ny3j5j3,Bloody hell. The RAM alone is worth it. I wish we had Microcenter in Europe...,buildapc,2026-01-06 23:05:25,1
AMD,ny3lmhv,Ram looks like its tall? Not sure it will fit under my peerless assassin cooler.,buildapc,2026-01-06 23:18:01,1
AMD,ny44aiy,I just did this deal the past weekend. Upgraded from a 5800 and 3080 to 7800x3d and 5070ti to get ahead of what is probably going to be a long term price inflation on ram and gpus. Had to get the $50 ram upgrade since they were out of the bundle ram but still well worth it. Games are running much smoother and with a bonus less power draw.,buildapc,2026-01-07 00:54:48,1
AMD,ny47jiw,I did that same jump and the gains were amazing.  Breathed new life into my 3080.  I plan to ride it for many many more years.,buildapc,2026-01-07 01:12:16,1
AMD,ny49llb,"Hop on that bundle, it’s totally worth it. I currently picked it up and I’m building around a 5070 ti oc gpu. Plus the cpu pairs nicely with the gpu from what I’ve heard/read",buildapc,2026-01-07 01:23:29,1
AMD,ny49lmu,"I am in pretty similar situation with a 3800x , but slower gpu  in a 3080. I feel bad I didn’t take advantage when 5800x3d deal was around.   For my situation I don’t know if I play enough games to really take advantage of it.",buildapc,2026-01-07 01:23:29,1
AMD,ny4blom,"I made the jump with exactly what you're debating. I've loved every second of my upgrade. Everything runs better, smoother, etc.   It's worth it.",buildapc,2026-01-07 01:34:27,1
AMD,ny4dx6r,"Yes, it's worth it. My 3700X was bottlenecking a 6900XT at 1440p, your 3600 is certainly bottlenecking the even-faster 5070 Ti.",buildapc,2026-01-07 01:47:08,1
AMD,ny4fdia,I got that same bundle in 2023 for 500. Crazy how prices have changed. 575 is still fantastic for 2026 though considering current RAM prices.,buildapc,2026-01-07 01:54:58,1
AMD,ny4gqi3,I’d say so for gaming,buildapc,2026-01-07 02:02:17,1
AMD,ny4hxmi,I drove from Austin to Houston for this deal (cannot wait til our own Micro Center opens up for this year) a few weeks back and I don’t regret it one bit. Those deals fuckin rule. Micro Center fuckin rules.,buildapc,2026-01-07 02:08:46,1
AMD,ny4tzzu,"I'd call that a pretty solid deal, today.",buildapc,2026-01-07 03:14:34,1
AMD,ny51w0b,Considering you can’t get any am4 x3d cpus for less than 350 I’d say this is well worth it.,buildapc,2026-01-07 04:00:29,1
AMD,ny58wzj,"Literally just got home from picking this bundle up, along with a 9070xt. Now the fun part, putting it together lol",buildapc,2026-01-07 04:45:02,1
AMD,ny41zqb,I pair this bundle with the asrock challenger 9070xt and I’m loving it. Pretty much max out any game I play.,buildapc,2026-01-07 00:42:45,1
AMD,ny3d0cm,"Honestly I think this is what I needed to hear. The issue wasn't the price per se as much as how good of a deal it was. It seemed too good to be true in this market, so I was wondering if there was some catch or if there was something better out there.",buildapc,2026-01-06 22:35:08,24
AMD,ny4lowv,You can add 50 dollars on top of the bundle to have 2x16gb rams,buildapc,2026-01-07 02:29:04,4
AMD,ny3o327,"The 9800X3D version of this bundle got both my buddy & I to finally upgrade, and oh man what a leap! (Coming from i7-8700k)",buildapc,2026-01-06 23:30:42,6
AMD,ny4kpch,"It's a steal even when RAM was normal pricing. Considering the CPU alone is almost $400, you're getting mobo and ram for like $200.",buildapc,2026-01-07 02:23:42,2
AMD,ny3wv6b,"Searching around, I found that it's 35mm. I think it looks taller because of how it tapers at the top.  I have a Kingston kit that is 34mm and I still had to raise the front fan up just a smidge.",buildapc,2026-01-07 00:16:22,1
AMD,ny3h4h1,I picked up a 9600x/b850/16gb of cheapy bad timing ram for $380 in the before-times.  Your getting a $200 processor upgrade from that WITH  32gb of ram which is like $350 now? So their advertised savings are pretty spot on. its a crazy good deal.   Anyway my little 9600x is a rocket so that thing will fly.,buildapc,2026-01-06 22:55:16,5
AMD,ny4qggs,I just got a rx7900 xt + R7800x3d and 32gb of ddr5 custom built off a guy on FB market today because I picked up Arc Raiders with some friends and was getting 45-65 fps on my 3440x1440p monitor with a 3080/R5 5600x and ended up getting on average 180-250fps.  I haven't tried my other games but the upgrade was incredible.  Definitely not the exact same but it feels really good to be future proof for the eventual next few years to come and how bad it's going to get lol,buildapc,2026-01-07 02:54:56,1
AMD,ny3oavl,What GPU did you end up going with?,buildapc,2026-01-06 23:31:50,1
AMD,ny4p7sg,I didn't get the bundle since I'm building ITX but I snag a combo deal (buy CPU and get 32GB RAM kit for $199) bought 9800X3D with RAM for $600.,buildapc,2026-01-07 02:48:14,1
AMD,ny3qzf8,"Keeping with my 3070ti for now, usually would do a full upgrade all at once but that one was a bit of an emergency pickup awhile back when my 1080 bit the dust.",buildapc,2026-01-06 23:45:59,1
AMD,ny4sbmh,Sometimes the cashiers will let you swap a mobo and pay the difference,buildapc,2026-01-07 03:05:10,2
AMD,ny4tl1s,"I thought of trying asking them that but they were out of ITX motherboard that I want. So I ended up getting just CPU and RAM.  None the less, now I know for the next time.",buildapc,2026-01-07 03:12:15,1
AMD,ny4tpae,I wish they would do an ITX alternate for those deals too.  Would be cool to build an itty bitty pc,buildapc,2026-01-07 03:12:55,3
AMD,ny4y7d5,"Right? They have ""upgrading"" option in the bundle for different motherboard but none of them is ITX.  ITX needs some love too. This was my first ITX build after my full tower PC went missing during the move so on the next move, I will bring it with me.",buildapc,2026-01-07 03:38:31,1
AMD,ny4zbdw,My second build was in a Corsair 250D and I loved that thing. Was like a fat Xbox I could easily lug to friends houses,buildapc,2026-01-07 03:45:03,1
AMD,nxobfbn,For $20 that’s a steal,buildapc,2026-01-04 19:08:04,22
AMD,nxoc13r,"$20 yes, pcie4 and stronger cpu.",buildapc,2026-01-04 19:10:43,13
AMD,nxoe3zv,Color me suspicious. What is this “offer”?,buildapc,2026-01-04 19:19:59,3
AMD,nxog5t5,"For $20?  Definitely, the 5600x has twice the cache which makes a significant difference in most games.",buildapc,2026-01-04 19:28:58,3
AMD,nxob79y,"I definitely would. However take into consideration if you’re on a very tight budget that you’ll have to buy a separate fan, but this won’t cost a lot anyways.",buildapc,2026-01-04 19:07:04,4
AMD,nxodsse,Yes. I have that exact setup. Worth it.,buildapc,2026-01-04 19:18:35,1
AMD,nxodyr8,"IMSMR, The X is unlocked and you can group the cores and run them all at top burst. Cool accordinly.",buildapc,2026-01-04 19:19:20,1
AMD,nxoeqto,Yes.,buildapc,2026-01-04 19:22:49,1
AMD,nxoffgn,"Defintely, yes.",buildapc,2026-01-04 19:25:50,1
AMD,nxoy0qr,5600x is goated,buildapc,2026-01-04 20:50:33,1
AMD,nxq7g1y,"$20 is insanely good. If it's not a scam, yes do it.",buildapc,2026-01-05 00:28:15,1
AMD,nxoeb60,"absolutely not  edit: x yes, non X no",buildapc,2026-01-04 19:20:52,-2
AMD,nxoi6sy,"He says he already has a Ryzen 5500, most likely he can reuse his fan",buildapc,2026-01-04 19:37:56,4
AMD,nxogsa8,5600 and 5600X are essentially the same thing.,buildapc,2026-01-04 19:31:42,5
AMD,nxokff6,"damn, true",buildapc,2026-01-04 19:48:09,1
AMD,ny4zg7s,im thinking around $1750-2000 usd,buildapc,2026-01-07 03:45:51,0
AMD,ny526fl,not even close,buildapc,2026-01-07 04:02:16,2
AMD,ny533xx,based off ebay sold seems pretty spot on,buildapc,2026-01-07 04:07:59,0
AMD,ny549uw,Appreciate the help! Will end up probably listing it for around 1500 as im trying to sell it fairly quick,buildapc,2026-01-07 04:15:16,1
AMD,nwz58ha,I upgraded from a 5800X3D (already faster) to 9800X3D and for what I wanted it was totally worth it. I’m mostly playing fps games like BO7 and BF6 at 1440p 240Hz and the 5800X3D was showing its limits. If I was playing at say 144-165Hz/fps then it wouldn’t have been worth it. The big factor is what fps are you aiming for and how much is the 1200KF holding you back in your games?,buildapc,2025-12-31 20:51:06,26
AMD,nwz62yd,"Can't give you any firm estimates, but if you already have the RAM (worst cost part right now for anyone upgrading) then yeah, the 9800X3D is an amazing chip for gaming. Your upper limit will generally be limited by your GPU, so instead what you'd expect to see is an increase in your 1% lows, meaning the overall gameplay experience becomes smoother with less small stutters. I had the 12600K paired with my 4070 Ti, and upgraded to a 9800X3D, seeing this kind of improvement, but note also I was on a DDR4 build so part of that was undoubtedly the RAM moving up a generation. But in tests, [the 9800X3D stomps the average FPS and 1% lows of the 12th gen Intel chips](https://gamersnexus.net/cpus/rip-intel-amd-ryzen-7-9800x3d-cpu-review-benchmarks-vs-7800x3d-285k-14900k-more#9800x3d-gaming-benchmarks) often by 50% or more.  In my personal experience and having had it for just under a year now, it just feels more ""stable"". On my 12600K I'd play games that would sometimes have inexplicable micro-stutters in the frame rate which I later learned were those 1% lows I'd read about. The AMD chip smoothed those out and now the games feel markedly more consistent.  Do note though that if you do productivity, e.g. video editing, 3D modeling, etc., the 9800X3D isn't optimal for them. It's no slouch, and I still do that as a hobby myself, but if your daily pay relies on lots of threads then something like the Intel Core Ultra 9 or the 9950X3D is a better choice. But for gaming, the 9800X3D really is the sweet spot for price and performance.  Also, it can be cooled with air alone. I have a Peerless Assassin 120 on mine and it has no trouble at all. Liquid cooling is excessive for this CPU; only do it for the aesthetics if you care.",buildapc,2025-12-31 20:55:42,6
AMD,nwz92hr,"I’m thinking your current setup with the i7-12700 and a 4070 Super is a pretty solid system. Like the folks have said here, if you are seeing low frame rates and/or stuttering in the games you play, then it would be worth it. If you are enjoying your games and they are running smooth with your current system, keep it awhile longer.   Also, what resolution and size is your monitor. That would make a big difference if you should upgrade.",buildapc,2025-12-31 21:11:53,5
AMD,nwzj32e,12700 is great. Id save for a monitor or speaker upgrade.,buildapc,2025-12-31 22:06:56,3
AMD,nwz6dkn,Nah I would until Zen6.,buildapc,2025-12-31 20:57:17,8
AMD,nwz7c1k,Not with memory prices you are better off holding out. They are still producing the 14th generation I would wait for a killer deal on a 14700K. But only if you are finding your CPU constantly running 100% in most games. Or you think you can recoup most of the cost on resale. I upgraded my 12600K to the 14600K and other than a slight increase in my 1% lows not a huge difference. I'm playing at 1440P with a 4070. It was FOMO and the whole future games thing lol.,buildapc,2025-12-31 21:02:31,4
AMD,nwz5k74,"You can look up reviews online. I'd say you're fine with your current setup, and a GPU upgrade is likely to benefit you more.",buildapc,2025-12-31 20:52:53,2
AMD,nwzlh95,"No, if you are money conscious at all. It’s better spent going to a higher tier gpu. 5070ti/5080 and selling your gpu would net better frames in most instances.  I always say though, don’t overlook a monitor upgrade. There’s many things a basic spec sheet won’t tell you when moving to a more expensive display.",buildapc,2025-12-31 22:20:38,2
AMD,nx0pwr7,I just did this upgrade. Kept my same GPU (6800xt). Upgrade 100% worth it,buildapc,2026-01-01 02:30:52,2
AMD,nwzbc5u,"Everything outside 1080p will be anyway limited by the GPU, my guess is 10% on 1080p and max 3-5% on 1440p or 4k, again both gaming options mostly limited by the GPU",buildapc,2025-12-31 21:24:05,2
AMD,nwz9lfa,Since you already have DDR5 memory it will be worth it.  We will not see Zen 6 X3D until Q1 2027 I believe so going AM5 now then flipping the 9800X3D in a year maybe worth it.,buildapc,2025-12-31 21:14:40,1
AMD,nwzbt0j,Yes for shooters big difference if you are aiming for high frame rate with high refresh rate monitor in 1080p and 1440p.,buildapc,2025-12-31 21:26:36,1
AMD,nwzy57s,"Yes, period",buildapc,2025-12-31 23:35:35,1
AMD,nx0d1rf,"When you say mainstream shooters, do you mean like BF6/Warzone/CS/VAL? Then 100% yes worth it. The x3d chips keep frametimes consistent which removes stuttering and helps your aim be consistent.     If you arent doing competitive fps or rts style gaming then no its not really worth it.    Source: I play bf6/wz and have tested 7500f vs 9700x vs 7800x3d vs 9800x3d by buying and returning processors lmao. The game is incredibly smooth on the x3d chips.",buildapc,2026-01-01 01:06:59,1
AMD,nx18yrp,"with the current ram prices, is it worth it?",buildapc,2026-01-01 04:43:56,1
AMD,nx21qsu,"For the games you play no. The 12700K is still a good chip and the fact you have DDR5 RAM VS DDR4 already helps a lot. I would wait for the new generation of Intel or even AM6 on AMD. If you play higher resolution gaming like 4K, the GPU is the one that will hold you back. I say stay where you are. But if you have disposable income, do whatever your heart desires.",buildapc,2026-01-01 09:17:59,1
AMD,nwz8ntr,leaving intel for an x3d will be the best decision you ever make,buildapc,2025-12-31 21:09:42,1
AMD,nwzxrag,I went from a 14700k to a 9800x3d and the difference has felt pretty huge.,buildapc,2025-12-31 23:33:18,0
AMD,nwz6y0v,"Yes it’s worth it. Plus the AM5 platform it better and will still have a few more generations left, whereas you have no upgrade path on your current",buildapc,2025-12-31 21:00:23,0
AMD,nwz414b,Nah,buildapc,2025-12-31 20:44:33,-1
AMD,nwzaehx,what a waste of money. everyone knows GPUs are what you need to max out on,buildapc,2025-12-31 21:19:03,-2
AMD,nx0d9zn,Second this for what its worth. The consistency in frames and improvement in 1% lows for someone latency sensitive is amazing.,buildapc,2026-01-01 01:08:28,7
AMD,nwzcuo2,"It's more nuanced than your black/white answer and video. If you use DLSS at 1440p for example, you should be comparing 720p or 1080p resolution depending on quality level. Also some games like 4X/Sims/WoW scale more dramatically even at 1440p.",buildapc,2025-12-31 21:32:15,8
AMD,nwzmv5d,Also the 1% lows are much better with the 9800x3d.,buildapc,2025-12-31 22:28:38,5
AMD,nwzbtao,"These hardware unboxed videos aren’t worth toilet paper since they don’t test using any upscaling.  Real people use DLSS and when you start using upscaling, you put more load back on the CPU, and the 9800X3D starts showing more of an advantage.",buildapc,2025-12-31 21:26:38,2
AMD,nwzvbwp,"Well you can either be right or wrong. And you're the one that first came in with an ""aktually"" reply in the first place with some stupid ""lmao wrong"" comment. Pretty ironic for you to say this.",buildapc,2025-12-31 23:18:36,5
AMD,nx06ttm,"Hello, your comment has been removed. Please note the following from our [subreddit rules](https://www.reddit.com/r/buildapc/wiki/rules):  **Rule 1 : Be respectful to others**  > Remember, there's a human being behind the other keyboard. Be considerate of others even if you disagree on something - treat others as you'd wish to be treated. Personal attacks and flame wars will not be tolerated.    ---  [^(Click here to message the moderators if you have any questions or concerns)](https://www\.reddit\.com/message/compose?to=%2Fr%2Fbuildapc&subject=Querying mod action for this comment&message=I'm writing to you about %5Bthis comment%5D%28https://www.reddit.com/r/buildapc/comments/1q0mwdt/-/nwzvssl/%29.%0D%0D---%0D%0D)",buildapc,2026-01-01 00:28:32,1
AMD,nx93c93,have you considered 5600X and 5700X?,buildapc,2026-01-02 13:46:50,3
AMD,nx95q0v,The 5600X isn’t that big of an upgrade.. I’d go for a 5800XT or just save for AM5.  Alternatively you can try to hunt down a 5700x3D or 5800x3d chip.,buildapc,2026-01-02 14:00:56,3
AMD,nx9b376,"Some say it's not that big of an upgrade, but according to Passmark it is 31% faster single core than the Ryzen 5 3600 and 25% faster multicore. That should be a noticeable difference. And if that's the extent of what your budget allows, then I'd say go for it.",buildapc,2026-01-02 14:31:51,3
AMD,nx9756v,"Considering you are using a 3060....probably not. The card isn't strong enough to really push your CPU limitations unless you are playing very light wait esports titles.  You aren't feeling your CPU's age in a game like Clair Obscure, its most likely the GPU. At 1080p ultra, the gpu can only give you around 40 FPS. Turn down your settings.  So imagine spending the money on a CPU and you get 0 performance improvement in your game. Where as you could sell your 3060, use that money towards a 9060XT 8GB that will give you 50% more performance.",buildapc,2026-01-02 14:09:17,2
AMD,nxa5pmh,"I would either save up for a 5800x or save up more for a better gpu. Going 3600x to 5600x doesn't seem all that good. You get icp improvement,  but no extra cores. You will be better off saving towards a better gpu. Your CPU will still bottleneck a stronger video card, but you will still get a better gaming experience than upgrading the CPU.",buildapc,2026-01-02 17:02:40,1
AMD,nx930st,It seems to be a 5600x with slightly lower clocks as the rest of the data looks perfectly the same - yes I'd go for the upgrade,buildapc,2026-01-02 13:44:56,1
AMD,nx95hpi,go for it.,buildapc,2026-01-02 13:59:34,1
AMD,nx93lsq,"I have, but It seems like prices just keep increasing lol, not even mentioning its getting harder and harder to find them. I could just barely afford the 5600t",buildapc,2026-01-02 13:48:23,4
AMD,nx96m1w,"Ngl I dont think that's feasible for me, at least not in a timely manner. Getting an am5 chip would probably mean getting brand new ram as well, and a motherboard. And honestly I'm sorta vibin with the 64gb I got.  As long as I can play games like destiny 2, Helldivers and clair fine I'll be okay.",buildapc,2026-01-02 14:06:10,3
AMD,nx99az9,"Really? Cause it feels like my CPU is the bottleneck. I'm playing at mid-low with like 60-70 resolution scaling, and I'm getting like 40-50 frames in game no matter what I do. I am playing at 1080p tho so would that be it?",buildapc,2026-01-02 14:21:40,2
AMD,ny1pwtv,"Coming back here to say, you were smoking crack. Instantly getting 60fps+ on clair, 1080p medium",buildapc,2026-01-06 18:04:26,1
AMD,nx93p1a,Perfect! Thank you.,buildapc,2026-01-02 13:48:54,1
AMD,nx94c5p,"I understand. Asked because where I live, 5700X is same priced as 5600T, so totally worth it.",buildapc,2026-01-02 13:52:44,2
AMD,nx9aijo,I upgraded from a 3600 to 5800xt without changing motherboard or ram. That's the path I recommend.  But you probably won't notice the jump much until you also upgrade the GPU,buildapc,2026-01-02 14:28:35,2
AMD,nxa0y1d,i would 100% get the 5600T on a deal.       can't buy it here from the U.s. i think its a special global version for certain regions  but that 5600T it's the same as 5600X version with l3 cache wise!       its almost the same thing but way more affordable! hell yeah i would,buildapc,2026-01-02 16:40:33,1
AMD,nx94r0j,"No worries, honestly i feel like if I was a month or two earlier I'd actually be able to afford it as well. Which completely sucks but what can you do",buildapc,2026-01-02 13:55:12,2
AMD,nxz5051,9600x,buildapc,2026-01-06 08:39:06,3
AMD,ny3t2cv,"Ultra 5 245k makes no sense and it's overpriced, only CPU on the Intel platform worth considering would be Ultra 7 265K for productivity on budget as it matches 9950X but cheaper. For gaming like it's been for years no competition AMD.",buildapc,2026-01-06 23:56:48,1
AMD,nxzemzk,"Hi , thanks for the answer. Just form my curiosity, may I know why? From more like technical standpoint, then just ""it's better"". In which direction it is better than the Intel ?   Thanks!  (Edit: wording)",buildapc,2026-01-06 10:11:08,1
AMD,nxzjrp8,It has a better upgrade path because AM5 is scheduled to get atleast one more generation of new CPUs with X3D CPUs which are better for gaming,buildapc,2026-01-06 10:56:51,2
AMD,nxp9i07,"For just the CPU and mobo for 1000 euros you can easily get the 9800X3D with 500-600 euros to spare for a motherboard, but the performance difference is pretty minimal. The 9800X3D is THE gaming CPU, which is important for comp games where you'll most likely be playing in 1080p, which is more CPU intense. You're better off getting the 9800X3D",buildapc,2026-01-04 21:43:55,13
AMD,nxpkr91,"For a 1000 you can get 9800X3D, decent motherboard AND RAM. Although 7800X3D could be better bang for the buck, with this budget that you have on your disposal, there is no reason to go for 7800X3D...",buildapc,2026-01-04 22:36:31,3
AMD,nxpc3im,"I was on the fence with the same thing and went with the 7800 to save $100 and I'm glad I did based on what I've read here. I built my PC specifically for games, and that's it. Diesel tech by trade, so all computer work at my job is done with manufacturer software on company tough books.",buildapc,2026-01-04 21:55:53,2
AMD,nxqufqm,I mean depends on what you mean by 'worth it'. The 13700k runs fortnite perfectly fine so I'm not sure why you're looking for an upgrade,buildapc,2026-01-05 02:29:13,2
AMD,nxpuanu,"If you've gone for so long on a 13700k why do you suddenly need the latest and greatest? If you needed it, you would have bought the 7800X3D when it launched. You're paying too much for just an 8 core CPU.",buildapc,2026-01-04 23:23:30,2
AMD,nxpg842,"The 13700k is superior in anything but gaming and even at that, if all you are playing is fortnite, I don't see the pt.",buildapc,2026-01-04 22:15:07,5
AMD,nxprqaw,Might as well go for the 9800X3D. It’s only about 5-10% faster but you have the budget so may as well.,buildapc,2026-01-04 23:10:49,1
AMD,nxpxe7t,You don’t need either for fortnight.,buildapc,2026-01-04 23:38:59,1
AMD,nxq64s1,There is such little difference between the two it hardly matters.,buildapc,2026-01-05 00:21:55,1
AMD,nxqpshu,If you have that budget then the why not the 9800 will crush Fortnite,buildapc,2026-01-05 02:04:33,1
AMD,nxsz3yd,"Since all you are playing is Fortnite, might as well invest that money in shares.  However, f you want the best, get a 9800X3D. If you want the best for your hard earned money, get a 7800X3D. That's gaming, btw. In anything else, the 13700K pisses on both.",buildapc,2026-01-05 12:07:54,1
AMD,nxpo9pw,"Your cpu is already really good, why would you upgrade? Hell even a 10th gen intel is still pretty good.",buildapc,2026-01-04 22:53:30,0
AMD,nxpos5n,not worth it,buildapc,2026-01-04 22:56:03,0
AMD,nxpste0,The amd CPUs will eventually kill themselves.  Why would anyone buy amd until they identify the problem and recall them all??,buildapc,2026-01-04 23:16:08,-10
AMD,nxq0pvr,That’s almost exclusively on asrock mainboards these days for the rest the risk is virtually nonexistent. Also with that reasoning why would anyone buy or have bought a new gpu in the past 3 years? The problem of melting 12hvpwr connectors is far more prevalent.,buildapc,2026-01-04 23:55:36,3
AMD,nxqamx2,"nice try asrock its your fault and not amds, we not falling for this bait",buildapc,2026-01-05 00:44:04,3
AMD,nxqarou,its not but ok,buildapc,2026-01-05 00:44:44,-1
AMD,ny52str,"Since you have a 9800x3d it really doesnt matter tbh, the ram will make almost no difference to your gaming experience.  Just google some tests online.",buildapc,2026-01-07 04:06:06,6
AMD,ny54d74,I'd say you are correct.  The Corsair kit has a first word latency of 11.25 ns.  The Klev is 10 ns.  This assumes they get the Corsair kit running at EXPO speed.  If they do then I don't think it matters as much on an X3D processor.  I bought that Klev kit earlier in the year for I planned build for a friend who went prebuilt instead.  It is a beautiful RAM kit.  Really pretty when powered up.  I couldn't afford to keep it just to have on hand so without knowing what was gonna happen to the market I returned it.  Makes me pretty sad these days.  I'd call the swap more of an alternative than an upgrade.,buildapc,2026-01-07 04:15:51,2
AMD,ny52qt2,It is a downgrade   But you might be able to hit the same timings and settings with some manual configuration   They are pretty close,buildapc,2026-01-07 04:05:45,1
AMD,ny52rr3,"What I read online, though I'm no expert the kit you selected would work better for the chipset due to native 1:1 ratios. Did you also check the QVLof your motherboard btw? This is a similar post: [Selecting a DDR5 RAM for Ryzen 9800X3D. 6000 Mhz vs 6400 Mhz. CL28 vs CL30 : r/buildapc](https://www.reddit.com/r/buildapc/comments/1ih0ofz/selecting_a_ddr5_ram_for_ryzen_9800x3d_6000_mhz/)",buildapc,2026-01-07 04:05:55,1
AMD,ny5779c,"It's not an ""upgrade"" as other have said, but x3D chips aren't as sensitive to memory timings due to all that L3 cache on the chip. Just downclock the 6400 Mhz kit to 6000 Mhz and you'll be fine.   Even if you had the better kit you wouldn't notice the difference on an x3D.  https://m.youtube.com/watch?v=aD-4ScpDSo8  AM5 runs best at 6000 Mhz or 8000 Mhz, but you'll need a good chip and a high end motherboard to hit 8000 Mhz. In between those speeds is finicky.",buildapc,2026-01-07 04:33:53,1
AMD,ny57hdk,You'll be equally fine with either RAM kit thanks to the CPU's extra cache.,buildapc,2026-01-07 04:35:43,1
AMD,ny59u54,"I got a 6400mhz cl32 kit  Tweaked to 6000mhz cl28 with no issues.   Anything over 6000mhz will make the system unstable, so just get the timings dialed in as low as you can.",buildapc,2026-01-07 04:51:05,1
AMD,ny53m41,"6000MHz, looks cool, low CL  in that order",buildapc,2026-01-07 04:11:08,0
AMD,ny56yax,^ This,buildapc,2026-01-07 04:32:16,1
AMD,ny5c6yg,"Ah ok so the differance is negligable then. I guess that being said however, even if there is the slightest advantage to the first RAM id want that as thats what i paid for. It feels a little unfair already paying for something and then being given something else that is ever so slightly worse because they ran out of stock. Also aesthetically the klevv crass looks better IMO lmao.",buildapc,2026-01-07 05:07:03,1
AMD,ny5cqbv,"Well, if they dont have it then you might have to wait for some time or at least demand a kit with a cl 30",buildapc,2026-01-07 05:10:49,1
AMD,ny048lt,"I have it, pairs with ryzen 7 7800x3D, it’s a great card especially for the price. Never had any issues and currently handles everything in high settings at 1440p and over 60fps. Some games obviously a lot higher. Has some ray tracing but not something I would bother with too much. Keeps good temps with a triple cooler like I have (ASU’s Tuf variant)",buildapc,2026-01-06 13:24:51,3
AMD,ny05cn6,"Perfect, what psu do you currently have? it says it needs 1000w, but Ive seen people using 750/800w psu's with it.",buildapc,2026-01-06 13:31:12,1
AMD,ny081hm,I used 850w PSU and never had any problems,buildapc,2026-01-06 13:46:19,3
AMD,ny0cs20,"You should consider the 9060xt 16gb as it costs the same. 15% slower in raster but better RT and you get access to FSR4 which is alot better than FSR3. Also alot better power efficiency and the superior AI acceleration means it will be more futureproof. But if you prefer raw performance for more competitive games, the 7800xt is likely better.",buildapc,2026-01-06 14:12:31,2
AMD,ny0rz0r,Yeah my mind is set on the 7800 for now due to the price it's currently at.,buildapc,2026-01-06 15:29:52,1
AMD,ny403dh,Gonna go 5600x/5700x + 5070 over your 5600g + 5070ti,buildapc,2026-01-07 00:33:00,22
AMD,ny3xtyb,"Not terribly, but you'll definitely be getting less fps than your friend with a better cpu. Some games will be worse than others. If you can get the 5070ti at msrp, I'd do it and just know you'll unlock more performance when you upgrade your cpu down the road.",buildapc,2026-01-07 00:21:20,17
AMD,ny40xd2,"That CPU was the limit for a 3070 Ti, before it started to hinder its performance.",buildapc,2026-01-07 00:37:17,10
AMD,ny49fhr,5600x or 5700x for 32mb cached,buildapc,2026-01-07 01:22:33,5
AMD,ny44l74,"IIRC the G variant only have PCIE 3.0 and it is a weaker CPU compare to the X variant. From my own experience using a 5800X with a RTX 5070, CPU bottleneck is surely a thing, especially when RT is turned on. It is wiser to upgrade the CPU along with the GPU or buy a cheaper GPU to save money.",buildapc,2026-01-07 00:56:23,6
AMD,ny491g8,"Just upgraded from a 5600x to a 7800x3d using the same 9070xt, found slightly better frames the main difference is more stable frame so less stuttering around loading rich content in games. If i knew before, likely wouldn’t have updated.",buildapc,2026-01-07 01:20:25,3
AMD,ny43c1g,Gonna need to upgrade your power supply if you're going with a 5070ti. I've seen mine pull 320 watts after playing with overclocking.,buildapc,2026-01-07 00:49:46,2
AMD,ny43mqz,Not bad enough for it to really matter while you work up the funds for a better CPU and a PSU to match,buildapc,2026-01-07 00:51:19,2
AMD,ny46wyi,"for 1440p? or 4k ??  it changes a lot, as on 4k the effort goes more on the Gpu  but I'd get it paired with at least a 5700x/5800x   (3D version if 1440p cause it'll keep up for more fps)  the 5600G is an APU, not good for high fps",buildapc,2026-01-07 01:08:53,2
AMD,ny4gsee,"doesnt matter. it's a massive upgrade over the rx 6600 and it's part of your upgrade plan. get the gpu now, plug it in and use it. you'll still get a huge boost in performance thanks to it and you'll unlock some more performance later on if you ever upgrade to am5/6 in the future.",buildapc,2026-01-07 02:02:34,2
AMD,ny49zjs,it is very useable and enjoyable but you will get better frames with better CPU like 5600X/X3D. Don't worry too much you can always upgrade your CPU down the road if you can snag a better CPU. Go with it you will not regret it.,buildapc,2026-01-07 01:25:37,1
AMD,ny4cqdv,"So how I have gotten lucky at finding parts MSR check out your local Walmarts nearby and use the Walmart app I found a store that was an hour away from me that selling ddr5msrp and 50 70 tis for 750 which is MSRP, generally you can't buy it it will tell you if it's available in store but unable for pickup purchase stuff, I'm in New York and that's how I managed to snag a deal but you got to find stores that have one, but to your bottleneck it's going to bottleneck pretty hard for that question",buildapc,2026-01-07 01:40:39,1
AMD,ny58v6a,"Just do it. Don't worry about any bottlenecking business. Not 100% sure about that PSU, though.",buildapc,2026-01-07 04:44:43,1
AMD,ny42wep,Isn't anyone going to talk about how 650w is a bit low to handle the card? Upgrade but get a bigger PSU,buildapc,2026-01-07 00:47:31,0
AMD,ny3xv0p,"I mean, it's a huge upgrade. It would be a bigger upgrade if you had a 5800x3d, but you're gonna keep the gpu for years and eventually am5/6 will be affordable. I say do it and see if you can find a 5700x/5800x(t) cheap somewhere, or win the lotto and get an X3d somehow.",buildapc,2026-01-07 00:21:29,1
AMD,ny46d1p,It's not a big deal. Nearly always there will be something that causes bottleneck. Now it will be the CPU. That's it. One day you will upgrade that but now you can buy that 5070ti. Who knows when you will be able to buy vga again?   5070ti on msrp? Don't even think about it just pull the trigger!,buildapc,2026-01-07 01:05:52,1
AMD,ny48lt1,"You forgot your monitor specs. 1080P 240hz, the 5600G will mostly be the bottleneck. 1440P 60Hz, the monitor is the bottleneck. 1440P 120+Hz, either depending on the title. 4K, GPU bottleneck. These are rough guesses, but you get the idea.  Personally, I like for there to be a significant bottleneck on my build because I generally alternate upgrading CPU or GPU. So, I switch which one is the bottleneck each upgrade. So, if a 5070ti will let you crank up details and/or peg your refresh rate in a way you can't right now. Then, be happy with that improvement and in a couple of years, you can do a CPU, MB, RAM upgrade and have your CPU be bottlenecked by the GPU for a few years before you upgrade that again.",buildapc,2026-01-07 01:18:03,1
AMD,ny3zrqc,"id get at least 5700x3d with 5070ti, ngl.",buildapc,2026-01-07 00:31:19,-2
AMD,ny3y7fc,Your overall gaming performance will still be a lot better than it currently is now with an RX6600 regardless of the bottleneck. You would expect to get 100fps+ in most games at 1080p and 1440p. 4K probably won't be bottlenecked.  If the plan is to upgrade later then there's nothing wrong with getting the GPU you actually want now and just accepting you're not getting the *full* benefit yet.  You should also upgrade to at least a 750w PSU when you do move to AM5 though.,buildapc,2026-01-07 00:23:14,0
AMD,ny3yut3,"Any X3D chip would not be a bottleneck but those are hard to come by, a 5800XT would be a slight bottleneck, probably not worth it. You wont be doing too bad though with that CPU for awhile",buildapc,2026-01-07 00:26:35,0
AMD,ny3zc95,I have a 5700g with a 4070 and honestly for single player titles there is very little to no bottleneck . I also play at 1440p which also reduces the chances of a bottleneck but in competitive titles i can notice the bottleneck but it isn't that bad. Frame generation also helps in cpu bound cases . If you're still worried you can upgrade your cpu and also ram but make sure to upgrade your psu to atleast 750w Gold if you actually get the gpu,buildapc,2026-01-07 00:29:06,0
AMD,ny41syu,"If you want a better upgrade, look at the 3700 chip, it's only about 5% slower than the 5700x3d in gaming.",buildapc,2026-01-07 00:41:48,-3
AMD,ny3y8vc,"20% at 1080p , 10% at 1440p and none at 4k Your psu is also the minimum for this build . You have no head room at all",buildapc,2026-01-07 00:23:26,-2
AMD,ny46rye,Yeah all the G series of the Am4 cpus arent good. Have to get the X or X3Ds,buildapc,2026-01-07 01:08:08,-12
AMD,ny468eo,"Definitly the answer.   I just upgraded my 1080ti to a 5070 on a Intel 8700k lol.   Definitly some bottleneck on 1440P 32:9, but it's going to be a nice upgrade when I eventually buy new parts for hopefully AM6",buildapc,2026-01-07 01:05:12,4
AMD,ny43lpe,This is the answer.,buildapc,2026-01-07 00:51:09,2
AMD,ny58yv2,"The 5070 Ti is x16, with a decent amount of VRAM. It'll be fine.",buildapc,2026-01-07 04:45:22,1
AMD,ny470uz,PCIe is not a bottleneck factor for gaming. The 5090 has a 1-4% lower Performance going from pcie5 to 3. Completly fine.,buildapc,2026-01-07 01:09:28,0
AMD,ny43pli,I've had mine pull 320 watts playing cyberpunk after I overclocked the shit out of it.,buildapc,2026-01-07 00:51:45,2
AMD,ny44p93,5070ti tdp is 300 and the cpu is like what 80max? if anything 650w is a bit high,buildapc,2026-01-07 00:56:59,4
AMD,ny45jrv,No. The ship has sailed on well priced AM4 X3D's.,buildapc,2026-01-07 01:01:30,3
AMD,ny42my2,"I'd do at least an i9-14900K or R9 9950X3D, and for RAM, at an absolute minimum, I'll have it at 128GB DDR5 6400 CL30.  /s",buildapc,2026-01-07 00:46:08,5
AMD,ny42ydx,How much worse of a bottleneck would a 3600x be? Asking for a friend that just bought a 5070ti (from a 2070super) and a 5800xt ( from a ryzen 5 3600x)  Hint - that friend is me,buildapc,2026-01-07 00:47:47,2
AMD,ny438ga,"The 5600G does better with single-core performance, making it a better option for gaming. Not to mention, he already has the 5600G. There's no need to get an older CPU when you have a capable one. Save that money for a better upgrade later on.",buildapc,2026-01-07 00:49:15,2
AMD,ny4e3qa,5600 is almost the same as 5600X,buildapc,2026-01-07 01:48:06,3
AMD,ny4bxsd,"If you have enough vram and can use all 16 lanes, yes. When you have an Rx 6650xt running on 8 lanes of pcie 3.0 being driven by a Ryzen 5700G, the performance loss will more than margin of error. Look at the 6500xt, that gpu can drop 50% perf running on pcie 3.0. They are either bandwidth or vram starved.",buildapc,2026-01-07 01:36:18,6
AMD,ny4629h,Buying a new am5 rig would defo be a smarter deal if he can afford it unless he gets a good deal on used am4 x3d,buildapc,2026-01-07 01:04:16,1
AMD,ny45hy8,"Im not joking, even 5700x3d might be the weak link in some scenarios for that gpu. That is a fact.",buildapc,2026-01-07 01:01:13,1
AMD,ny48hwc,"I would just try and see what Performance you are getting and seeing if that Performance is acceptable for you before upgrading the CPU aswell.  Strongly depends on what resolution you are playing. If the gpu is peaked at 100% that's usually a good indicator that the GPU is the bottleneck, if the CPU is 100% then it's CPU.  Thats not the only indicator, but an easy one. Some games are CPU heavy, but most tax the gpu the most.",buildapc,2026-01-07 01:17:28,1
AMD,ny44fg3,"5600g is, nhf, kinda crap due to low cache and pcie3",buildapc,2026-01-07 00:55:31,1
AMD,ny4dd92,"Ah Yeah, having all lanes is important.  I dont really see people upgrading older plattforms with Budget/low VRAM GPUs though. Most just get a new System then or buy used.",buildapc,2026-01-07 01:44:06,1
AMD,ny4g4pd,"Here’s a test of PCIe scaling, both x16 vs x8, and PCIe 4.0 vs PCIe 3.0, while using an RX 6600 XT. On average it loses around 2% going from PCIe 4.0 to 3.0. Nothing like 50%.  Linked to the conclusion: https://www.techpowerup.com/review/amd-radeon-rx-6600-xt-pci-express-scaling/28.html",buildapc,2026-01-07 01:59:01,1
AMD,ny473re,Yeah I know lol,buildapc,2026-01-07 01:09:54,2
AMD,ny44p32,You're overlooking that he already has it on his computer. I say go for the 5070ti and upgrade to a 5700x3d or 5800x3d after you have extra money saved. There's no sense in buying a 3700x just to upgrade it anyway.,buildapc,2026-01-07 00:56:58,0
AMD,ny4hr4i,"My 50% reference was at the 6500xt with it's 4 lanes. And that techpowerup test used a 5800x, which scales much better than a 5700G. I know, i have both cpus. ;)",buildapc,2026-01-07 02:07:47,1
AMD,ny45v6i,"Yeah, but finding a 5700/5900x3d is tough and if you do it's $350+.",buildapc,2026-01-07 01:03:12,4
AMD,ny44yxm,"Agreed, upgrading am4 in 2026 for gaming should be x3d or bust, especially with the gpu he wants to buy.",buildapc,2026-01-07 00:58:24,3
AMD,ny4mtw9,"1) only now do I remember that about the RX 6500 XT. I think I blotted that one out of my memory. They also used a 64-bit memory bus. Yikes.   2) huh, TPU did a PCIe scaling report on that one as well.   3) https://www.techpowerup.com/review/amd-radeon-rx-6500-xt-pci-express-scaling/31.html  4) okay, barring freak x4 cards, it’s probably not a big deal, overall. Even that one only dropped an average of 13%. Did someone at AMD really hate that card?   ¯\\\_(ツ)_/¯   The APUs definitely got gimped over the years. They were often a generation behind, older graphics, smaller L3 caches, and would get saddled with slower PCIe buses.   That being said, the 5600G would immediately become the fastest CPU in this home.",buildapc,2026-01-07 02:35:17,3
AMD,ny47aq2,5800x3d*  I seen a 5950x for 190 on Facebook lol I'm sure he'll be able to find one,buildapc,2026-01-07 01:10:56,0
AMD,ny4qrx9,"Yeah, the 6500xt should never have existed in it's launched configuration. It reminds me of the DDR4 version of the gt1030, what were they thinking? Half the performance for more money. Haha   Yep, the APUs are gimped even in the Ryzen 8000 models. The 4000 skus were even worse, as the L3 cache was half lower again. Still decent cpus though if gaming was not the requirement. Stuck quite a few Ryzen 4500s and 4600gs in office terminals with no complaints.   For general productivity work, i prefer my 5700g over the 5800x. It runs cool as a cucumber and LOVES to boost. For gaming, the 5800x wins hands down. If i had to get rid of one, the 5800x would go bye bye without a 2nd thought. ;)",buildapc,2026-01-07 02:56:40,2
AMD,ny59b5t,"They were thinking that they had designed mobile GPUs, that could be clocked up and put into desktops, and that with Nvidia not serving the lower end of the market very well, and they could take advantage of that to make some money.",buildapc,2026-01-07 04:47:37,1
AMD,ny0yrld,These posts always make me chuckle.  How much software development do you ACTUALLY plan on doing lol?,buildapc,2026-01-06 16:01:16,6
AMD,ny0zc3t,If you hate money and love having overkill PC parts this is a great PC build.,buildapc,2026-01-06 16:03:55,6
AMD,ny13had,"It's an overkill workstation... Sure, it's fine. I don't know if you need that much, but it is fine.",buildapc,2026-01-06 16:22:56,2
AMD,ny1ctd3,"Overkill? Definitely. If you have the money to burn, that is fine. Just build it and enjoy it but you have more or less top of the line everything. I would just look into the Lian Li UNI SL INF reversible fans if you want your RGBs to look nice. Might as well at this point.",buildapc,2026-01-06 17:05:20,1
AMD,ny145ch,"haha :)   hopefully 8 hours a day for a long time, though some companies ban personal computer from running their code so finger-crossed I don't get laid off soon \^\^",buildapc,2026-01-06 16:26:00,2
AMD,ny17yi7,what part would you change first ?,buildapc,2026-01-06 16:43:22,1
AMD,ny18d2a,I mean... Just go down the list and ask why.  Do you need a 9950x3d for your workload?  Do you need a gen 5 SSD (you don't)  Do you need all of the crazy expensive fans and coolers? Do you just want the look? Is it worth it?   It feels like you just selected the most expensive component for every piece.,buildapc,2026-01-06 16:45:12,2
AMD,ny1bfv9,"Do I need them, easy no.   I probably should cut back, but I still want a very powerful PC (moving back to a 9800X3D is probably my first ""cut"")   gen 4 ssd is 60€ less, for twice the speed it seems worth it. Did I miss something ?      My PC is next to me 8-12h a day, I see it more than my wife (sadly) so looks matter.   I was probably led to the lian li look by advertisement as this is my first time trying to build a beautiful PC, if you know something better, please let me know :)",buildapc,2026-01-06 16:59:03,1
AMD,ny1bsyh,I mean Lian li stuff does look great.   You know what you use the PC for more than me.  If you want this stuff go for it.,buildapc,2026-01-06 17:00:42,1
AMD,ny29evo,I have a 3600 with a 6750xt running at 1440p and the GPU is the bottleneck.  If you're sticking with 1080p - go with the 9060XT 16GB/5060 Ti      If you're thinking of pushing 1440p - go with the 9070 XT or if you want to stick with NVIDIA get the 5070 for a cheaper upgrade or 5070 if you have the PSU or don't mind dropping money for it.,buildapc,2026-01-06 19:31:51,5
AMD,ny2248z,It depends on the games you want to play. Newly released games are not an option with that build. I'd recommend getting at least an RTX 5060 Ti or a 9060 XT for 60 FPS on high settings nowadays.,buildapc,2026-01-06 18:58:36,1
AMD,ny22kcv,"Ultimately it's up to you, but there is likely some pretty good value upgrades you can do, and now could be a good time to buy.  Couple questions:  - What would your absolute max budget look like? I can give you some options from that price and down.  - Would you be willing to buy from the used market? Ebay or even Facebook marketplace?  - What games are you playing where you are seeing issues? What games would you like to play that you can't?",buildapc,2026-01-06 19:00:37,1
AMD,ny22kw1,"What is affordable to you? I’d get a new gpu first thought you may possibly need a new psu as well. Then more ram, and then grab a 5000 series cpu. The best easily bought options for am4 are going to be the 5800xt or 5600x. The 5600x is more than enough for most games, very few will miss the extra cores of the 5800xt if at all. If the price difference isn’t that huge between them though just get the 5800xt",buildapc,2026-01-06 19:00:41,1
AMD,ny2agmu,What games will you play? What budget you willing to pay?,buildapc,2026-01-06 19:36:42,1
AMD,ny2b2dq,"I had a very similar build to you (1660ti with a 3600x) before upgrading a couple of years ago. It still surprisingly held up pretty well with new games at the time, but I wanted to upgrade to 1440p with high frame rates.    Whether you need to upgrade depends on what you want to do. Your current build will still run new games on lower settings. if you're not cool with that, then upgrade it.",buildapc,2026-01-06 19:39:27,1
AMD,ny2fvf0,"I had the exact same setup, then very recently upgraded the GPU to a 5060 Ti 16 GB. I'm very pleased with how it's running all my games now (Cyberpunk 2077 runs great). However I am only using it for 1080p, so I'm uncertain how it'll handle 1440p.",buildapc,2026-01-06 20:01:27,1
AMD,ny2ih1i,"Well. I would be careful where you buy an RTX 5080, 5090, etc. People have been getting rocks or 3 GeForce GT 1060s in the mail.",buildapc,2026-01-06 20:13:33,1
AMD,ny2j1w2,Stay with am4. Upgrade the CPU to a 5000 series ryzen. Good selection of gpu's currently depending on your budget 9070xt/9070/5070ti/5070/9060xt/5060ti (16gb).,buildapc,2026-01-06 20:16:14,1
AMD,ny2kxie,"I had exactly same config, went with R7 5700X3D & 4060 ti - they were really cost effective at that time. Works great in Full HD",buildapc,2026-01-06 20:25:01,1
AMD,ny2s09x,"CPU is decent, if you get something like a 9060xt you will be able to play basically anything at 1080p/1440p. Gtx 1660 is quite weak for todays standard, I think this upgrade is worth it",buildapc,2026-01-06 20:57:43,1
AMD,ny2tnrz,If you need a pc buy a new one else hold off.  RAM is insane right now.,buildapc,2026-01-06 21:05:17,1
AMD,ny2wpul,"Yes, 5700x and 3080/3070ti/4070/5070/ti depending on psu  16gb ram is low but if you actually need more you're probably aware",buildapc,2026-01-06 21:19:09,1
AMD,nxhzzaj,"those games are heavily CPU bound, you wouldn't see much uplift from upgrading your GPU there regardless of your resolution  fwiw I also play valorant and ff14. my frames in both those games basically stayed the same when I upgraded from the 3070 to 9070xt  I upgraded my CPU a month after I upgraded my GPU. so when I upgraded from my 10700k to 9800x3d, I got a 400 - 500 fps increase in Valorant (yes lmao) and a 100 fps increase in ff14 idling at my house",buildapc,2026-01-03 20:19:04,3
AMD,nxht1mq,[https://i.gyazo.com/0f3f52823b94186f6e40b07fe4416ee3.png](https://i.gyazo.com/0f3f52823b94186f6e40b07fe4416ee3.png)  My timespy screenie if anyone wants to see that,buildapc,2026-01-03 19:45:07,1
AMD,nxhw2kf,Plugged into the GPU on the motherboard? What exactly do you mean by that?  Also what resolution are you playing at?,buildapc,2026-01-03 19:59:46,1
AMD,nxhxx8f,"In addition to the comments already made, I would like to add that those games generally run better on nvidia cards than AMD, because they were designed to do so (background gimmicks and stuff). The overwhelming majority plays on nvidia cards, so games get way more optimised to cater to the biggest chunk.",buildapc,2026-01-03 20:08:51,1
AMD,nxhz6c4,Cheers for the answers. I play on 1080p,buildapc,2026-01-03 20:15:04,1
AMD,nxi5cb8,"I tried everyting. Then I got a tip and turned a AMD setting off in the bios, then it got resolved. Atleast for my 9060 XT 16GB. I can check later what its called, if not someone knows it.",buildapc,2026-01-03 20:45:58,1
AMD,nxi5fx1,"If you're cpu bound, crank the graphics settings to the max... Might as well and it may help the cpu a bit",buildapc,2026-01-03 20:46:29,1
AMD,nxi94rn,At 1080p your 5600x cpu is holding you back.,buildapc,2026-01-03 21:04:45,1
AMD,nxhwe0c,"In 1080p, the games you play likely don't benefit from a strong GPU.  You can either return the GPU and upgrade the CPU instead or get a 1440p monitor to take advantage of your new GPU.",buildapc,2026-01-03 20:01:20,0
AMD,nxi9u1z,"That's oddly convenient lmao, but yeah I got told otherwise I'd get a fps boost (maybe). I was looking at that cpu, but it's quite expensive for my dumbass",buildapc,2026-01-03 21:08:12,1
AMD,nxhzryd,"You can click the different metrics in the graph to pinpoint what is the bottleneck. Such as CPU load, GPU/CPU temp  Edit: is CPU utility at 100% all the time?",buildapc,2026-01-03 20:18:04,1
AMD,nxi9j0j,Would appreciate it,buildapc,2026-01-03 21:06:42,1
AMD,nxi9mav,"I can give it a shot, I need to wait till pay day to see haha",buildapc,2026-01-03 21:07:09,1
AMD,nxic6ur,Would playing on a higher resolution resolve that? Or the 5600x is just that bad,buildapc,2026-01-03 21:19:47,1
AMD,nxhz8m1,What do you mean by take advantage?,buildapc,2026-01-03 20:15:23,1
AMD,nxiu1e5,"Yeahhhhh, it's the best consumer gaming CPU available so it's definitely not cheap, but you also absolutely don't need it.   I don't know how caught up you are with the current state of PC building, so if I could info dump a bit here:  Unfortunately you came at a catastrophically bad time to upgrade your CPU. Because of the DRAM crisis happening at the moment, DDR5 RAM is absurdly expensive, so that would be another huge additional cost if you wanted to upgrade. On top of needing a new motherboard since any other modern AMD CPU is on AM5, a platform newer than your current motherboard supports (and that expects DDR5 RAM).  If you wanted to stick with your current platform on AM4, your best option would be getting any of its 3d CPUs. But they're also absurdly expensive and impossible to find. They're out of production and heavily coveted, so their used prices are insane right now.  Outside of that, the best upgrade path for your current motherboard is the 5900xt. It's not an insane uplift from what you have now, you can look up performance comparisons to see if it's right for you.  If you were willing to swap motherboards but still stick with a DDR4 platform, you could also check out Intels LGA1700 offerings from their 12th Gen to 14th Gen line of CPUs. Unfortunately again, their 13th and 14th Gen CPUs are hard to recommend, since they suffer from a degradation/oxidation issue which just outright breaks them. Intel implemented several fixes but it's hard to tell if they're 100% good now. Even aside from that, 13th and 14th have worse performance if you don't have DDR5, so...   Which leaves you with Intels 12th CPUs. The higher end should outperform the 5900xt I believe? But again, check to see if the performance lift is right for you.  It's possible to upgrade to DDR5 still, if you wanna get out cheap you can fish for a deal (which has increasingly become harder) or maybe cough up the money for 16GB. It's a cluster fuck right now if you couldn't tell lol",buildapc,2026-01-03 22:47:58,1
AMD,nxibq87,[https://i.gyazo.com/3a337a8fbafedb2426a22831e5efb9c7.png](https://i.gyazo.com/3a337a8fbafedb2426a22831e5efb9c7.png)  No it's not,buildapc,2026-01-03 21:17:31,1
AMD,nxihrzk,"AMD fTPM switch, disabled. Try and see if it works. Did it for me.",buildapc,2026-01-03 21:46:57,1
AMD,nxieex2,"A higher resolution will use more of your gpu, you won’t get a higher frame rate than now though.   So instead of getting 100fps at 1080p you might end up still getting 100fps but at 1440p. It’s all going to depend on the game though",buildapc,2026-01-03 21:30:37,1
AMD,nxhzypk,"Because you have a powerful GPU now, you can swap to a higher resolution without losing much framerate, maybe not at all even. Your GPU has lots of capacity to do that.",buildapc,2026-01-03 20:18:59,0
AMD,nxj0sor,"That's some solid information, I appreciate it! I'm wanting to make my pc last for awhile, so spending a little more on a good cpu isn't too much of an issue. Kinda kept up to date on the news and it sucks because I've only really started getting into it haha. I also didn't realise my motherboard was that outdated, my pc building skills are very questionable and getting windows again might be a pain in the ass. How good would a 3d cpu do me on my current setup? Dunno if I fancy a ""slight"" improvement. Also what do you mean by a 16gb?   Appreciate the extra info too!",buildapc,2026-01-03 23:23:17,1
AMD,nxjynp2,"Yes that looks like timespy should look like, with your graphics card maxed during the GPU test 1 and 2.  Are you gaming at 1080p? In that case the cpu was already close to being the bottleneck for the games you used to play.",buildapc,2026-01-04 02:25:37,1
AMD,nxi98fh,"Oh I see, I might consider it. Been on a 27 inch for awhile",buildapc,2026-01-03 21:05:15,1
AMD,nxl6jky,"You wouldn't have to (typically) buy or install Windows again, that should just work even if you make an upgrade.  The 3D CPUs by AMD are best in class for gaming, imo it's worth looking at video comparisons of them if you wanted to see what they're like compared to everything else. For 1080p gaming they provide huge performance uplifts bar none. Though the 5800x3D is matched with recent, higher end CPUs that use DDR5.   As good as they are, some people find them overkill for higher resolution gaming, and I think that's a fair criticism too. The higher res you go, the gap closes \*a lot\* since that is more of a GPU bound scenario. Even in GPU bound scenarios, they are better at 1% and 0.1% lows, so there is still a benefit. I got the 9800x3D cause I spend the vast majority of my time playing CPU intensive games, even though I run 1440p.  They're basically the Ferrari of gaming CPUs, but once again, you absolutely do not \*need\* them. It's like getting a 5090 as a CPU, it sounds nice until you look at the price and potentially how overkill it is. There are plenty of cheaper upgrades you can make that will give you very nice performance uplifts.  I'd also argue if your 5600X is treating you fine, you could just keep that and ride it out, even if it is a bottleneck.  Also by 16GB I just meant 16GB RAM. Which would be a downgrade for you since I see you have listed 32GB RAM in your original post.",buildapc,2026-01-04 07:19:47,1
AMD,nxlxb38,"I was looking at quite a few yesterday new + used. There's not many 3d chips like you said that's compatible lol. Was considering a 5800x3d used but wanted to do abit more research first. I am the same as you with whole cpu heavy games so I am leaning towards it. The 5600x is ""okay"" can wait to see what comes up. but I'd personally like an upgrade. I don't suppose you know any cpus that will give that significant uplift unless it is just the 5800x3d",buildapc,2026-01-04 11:20:13,1
AMD,nxm36c9,"If you wanted to stay on your current motherboard, your best bet outside the 3D CPUs is probably the 5800xt. It's unclear to me if the 5900xt would have more gaming performance, its main draw is extra cores which games don't really use.  Hardware Unboxed has a very relevant video for you on this, and if I'm being honest, the uplift from 5600x to 5800xt seems incredibly miserable. You can check it out for yourself: [https://www.youtube.com/watch?v=RijAyVshtok](https://www.youtube.com/watch?v=RijAyVshtok)  If you wanted to stick to a DDR4 platform, it would be worthwhile to look up how the 5600x compares to the 12700k, but that seems very pitiful as well unfortunately.  My personal opinion is neither of these upgrades are worth it at all.  If you're looking for any worthwhile gains, I think it would be best to look at moving to a DDR5 platform based on your budget and what you need.",buildapc,2026-01-04 12:10:02,1
AMD,nxu2md8,"Sorry for the late reply, been doing ALOT of research and homework lmao. When you say DDR5 platform, are you meaning a different motherboard? I don't think my motherboard is compatible with intel cpus (probably wrong) so I haven't had a proper look at those. Checked out the video and like you said it doesn't look like a huge difference. About to look at the 5900xt. I've also heard playing on 1080p with my 9070xt is a ""bottleneck"" too. But I've also yet to confirm that considering I don't have a 1440p monitor.",buildapc,2026-01-05 15:54:51,1
AMD,nxugpty,"\> When you say DDR5 platform, are you meaning a different motherboard?  Yeah! Upgrading to DDR5 would require getting a new motherboard. As you noticed, switching to Intel would require a new motherboard as well. AM5 (AMD current gen) requires DDR5, AM4 (AMD last gen, what you're on) requires DDR4.  For Intel its a little more confusing, LGA1700 (Intel last gen) has both DDR4 and DDR5 motherboards. You must choose whether your board is DDR4 or DDR5 while buying, as they are not backwards/forward compatible. So you have to choose one!  LGA1851 (Intel current gen) requires DDR5.  \> I've also heard playing on 1080p with my 9070xt is a ""bottleneck"" too.  Well it's not a bottleneck in the traditional sense, where if you upgraded you'd get more frames. You aren't gonna get better performance by upgrading to 1440p, but the 9070xt slam dunks 1440p so it would be a worthwhile upgrade yeah",buildapc,2026-01-05 17:00:23,1
AMD,nxuq4hi,"Ah I figured! My brain was in a shambles tryna understand the intel part LOL. A question too, is my RAM decent? And okay, I might be considering that too",buildapc,2026-01-05 17:43:57,1
AMD,nxrxis0,25 processes 😭,pcmasterrace,2026-01-05 06:33:54,20
AMD,nxrye1q,Why is there only 1GB of RAM seen by the OS? Is that the max amount the first AMD64 could handle at the time? I thought the 64-bit architecture was theoretically supposed to be able to handle up to 4TB of RAM?,pcmasterrace,2026-01-05 06:41:08,5
AMD,nxru9l1,Oh wow.  You're going to upset a lot of Intel fan-bois by stating fact that AMD64 destroyed Intel's Itanium CPUs.  Here's the back story as told by the guy that created Task Manager.  https://www.youtube.com/watch?v=WyX8TO3awfw,pcmasterrace,2026-01-05 06:07:24,22
AMD,nxry1mg,Itanium had lackluster performance for its own native software as well!,pcmasterrace,2026-01-05 06:38:16,4
AMD,nxs3ims,"The Greatest Generation had a word for what IA64 became, a ""boondoggle"". It's where something shifts and drifts so far from its original intention, becoming so complex and expensive along the way, that it no longer fulfills any of it.  What Intel wanted and what HP wanted were well aligned. HP wanted a server/high-performance computing (HPC) product to replace the older PA-RISC, while Intel wanted a monopolised line of products it could name its price on.  Itanium essentially killed every other RISC architecture of the 1990s. MIPS was sold off by Silicon Graphics, and Compaq (owners of DEC) ended work on the Alpha, both intending their future HPC machines to be IA-64. Of the RISC workstation platforms of the 1990s, only SPARC remained in development, by Sun Microsystems (soon to be gobbled by Oracle).  The point behind IA64's ""EPIC"" (explicitly parallel instruction set computer) architecture was to move almost all the CPU's instruction handling front end into software, let the compiler handle it all once then it didn't have to be done every time the code ran. So the instruction stream had to already have all its parallelism worked out, the compiler needed intimate details of the CPU's architecture.  By the time Merced actually shipped, Intel was de-hyping it in every presentation. The narrative became ""Wait for McKinley, though buy Merced now if you want to get familiarity with IA-64"". Merced had become so large on silicon that Intel's 180 nm process couldn't actually manufacture it so it starte being cut back. IA32 hardware support was first to go, but then the caches got trimmed, the exact same problem AMD was to later have with Bulldozer - and the exact same outcome.  On natively compiled IA-64 bit code against natively compiled IA-32 code a piddly little AMD K6-III from three or four years earlier at just 450 MHz was up to twice as fast as Merced running at 800 MHz. In other tests, Merced was four times faster than AMD's market-leading Athlon.  Well, anyway, we're getting a bit long and you're getting a bit bored. McKinley, the version Intel had wanted to make, fixed most of this and really was a HPC floating point monster. McKinley went more or less unchanged to 130 and 90 nm as Madison, which was dual-core and had larger L2 caches.  The future was actually looking bright for Itanium in 2001, even after Intel had cut its revenue forecast for Itanium from $35 billion in 2002 (1999 projection) to $7 billion by 2005 (2001 projection). Actual sales figures, by 2005, were barely over $2 billion.  Three things conspired against it.  1. If you wanted Itanium, you went to Intel, and Intel's pricing was catastrophic. You could buy a quad-socket Xeon system for the price of a dual-socket Itanium, and the Xeons would be faster. 2. Intel had not invited AMD to the Itanium party, when IA64 took over the world, as Intel felt was its entitlement, it would be an Intel monopoly. AMD disagreed here, so AMD extended IA-32 into AMD64, a drop-in replacement for existing systems in almost all cases. In AMD's K8-sporting Opteron's first year, it out-sold Itanium four to one: Michael Dell canned his exclusivity agreement with Intel and started selling Opteron servers. Dell wanted to sell servers, after all. 3. Very much unwilling to have Intel in control of the destiny of Windows, Bill Gates went on stage and told the world that WindowsXP's future updates would support one 64-bit architecture, and it was AMD's. Intel's share price was down over 5% by the time Gates had done speaking!  Microsoft did make a version of the NT6 codebase for IA64, but never a client version, only ever Server 2008 (NT6.0) and Server 2008R2 (NT6.1)  Itanium did lumber on, with the basic Madison architecture being more or less unchanged as it shrank to new processes, gained more cores, etc. The last Itanium, the 9700-family, was ""Kittson"" in 2017. The highest model ran at 2.66 GHz and had 8 cores. Intel's top end Xeon of 2017, based on Skylake-SP, had up to 28 cores.  The final shipping system was the HPE Integrity server/HPC, which stopped accepting IA-64 orders in 2021 and, since 2015, had been the only shipping IA-64 product. Linux removed IA-64 from mainline kernel support in 2021, Linus Torvalds quipping:  *""HPE no longer accepts orders for new Itanium hardware, and Intel stopped accepting orders a year ago. While Intel is still officially shipping chips until July 29, 2021, it's unlikely that any such orders actually exist. It's dead, Jim.""*",pcmasterrace,2026-01-05 07:25:14,4
AMD,nxso76w,"Is this in a VM?  I remember trying to install Windows 7 on a 1st gen Ryzen, a config that is officially supported and it absolutely did not like the USB implementation.    I had to modify the Win 7 ISO with drivers to get any input device working at all.   So I kinda don't see someone installing such old software on real hardware.",pcmasterrace,2026-01-05 10:38:06,2
AMD,nxy0h9c,"System starts up with that CPU ...   Nearly 4Ghz, 8 Cores, 16 threads and drawing 65w without overclocking.  ... Yep, looks normal.  That would look like science fiction in a 2003 system  - from memory and quick research you would need dual Xeon's to even come close to the performance for a higher power draw - But at least the ram quantity would look sane.",pcmasterrace,2026-01-06 03:29:32,1
AMD,nxs89iy,Not even arch is that light haha,pcmasterrace,2026-01-05 08:08:36,8
AMD,nxse1rp,No copilot and other bloatware. Awesome build.,pcmasterrace,2026-01-05 09:03:31,3
AMD,nxsqe2d,Probably because it's a beta-build. The official support at least on the os-side is 16Gb or 64Gb (with PAE on). Or he got that freaky 1Gb DDR5-Stick or just a VM.,pcmasterrace,2026-01-05 10:56:59,5
AMD,nxu4hdd,It's running as a VM.,pcmasterrace,2026-01-05 16:03:32,2
AMD,nxtocps,It's a virtual machine,pcmasterrace,2026-01-05 14:44:29,1
AMD,nxrx6jm,Not an Intel fanboy but OP's fact checking sucks. The licensing agreement between Intel and AMD preceds the IA-64 debacle and IA-64 never meant to be backwards compatible to begin with.  But I would have loved to be a fly on the wall at Intel's board meeting after AMD's announcement of AMD64. I guess there was a lot of panicking.,pcmasterrace,2026-01-05 06:31:03,7
AMD,nxte51j,> Oh wow. You're going to upset a lot of Intel fan-bois by stating fact that AMD64 destroyed Intel's Itanium CPUs.   On what planet do you actually see anyone being even slightly annoyed by this lol,pcmasterrace,2026-01-05 13:47:24,6
AMD,nxtnqm5,who installs win 7 on ryzen xddd,pcmasterrace,2026-01-05 14:41:12,1
AMD,nxu58ij,"Ah I see, my bad",pcmasterrace,2026-01-05 16:07:04,1
AMD,nxrxliz,"If you've not already done it, I do suggest watching the video that I linked.  That guy is a former developer at Microsoft during that time - again, he is the guy that created Task Manager, so he had true insight.",pcmasterrace,2026-01-05 06:34:32,5
AMD,nxz3fg4,">and IA-64 never meant to be backwards compatible  It's still a big reason it never caught on, however.",pcmasterrace,2026-01-06 08:23:56,1
AMD,nxyf08n,"I personally know people in the IT industry that believe ""Intel makes the best everything.""  I work with 1 of them on a daily basis.",pcmasterrace,2026-01-06 05:00:36,3
AMD,nxtqjsl,I did on an AliExpress B450 mobo it just to try.,pcmasterrace,2026-01-05 14:55:57,2
AMD,nxvht28,"It's definitely a good backup card for non-iGPU systems. Also great for tinker PCs with a spare PCIe slot, as you watch it steadily grow into a 42U Homelab.",pcmasterrace,2026-01-05 19:48:57,2
AMD,nxvjh0v,Is it already time for me to fall into the homelab rabbit hole...,pcmasterrace,2026-01-05 19:56:34,2
AMD,nxvjw7h,![gif](giphy|oWjyixDbWuAk8),pcmasterrace,2026-01-05 19:58:30,2
AMD,nxk63kz,"Only 10 fans? What are you planning on playing, solitaire or minesweeper?",pcmasterrace,2026-01-04 03:07:13,2
AMD,nxleyjh,ah! you have build THE BOAT! sweet!,pcmasterrace,2026-01-04 08:35:11,2
AMD,nxl9rnw,"Hahaha, you are right, I am playing CS2, Valo, Overwatch. Maybe I will try Cyberpunk once again.",pcmasterrace,2026-01-04 07:48:29,1
AMD,nxqghs7,"3x 8-pin, 12VHP is an unneeded risk. Just grab the cheapest 9070XT.",pcmasterrace,2026-01-05 01:15:06,2
AMD,nxqmoqx,"As a Sapphire Nitro+ 9070 XT user who has the 12VHPWR cable. I recommend to VOID AWAY FROM THE 12VHPWR.  I got the Sapphire Nitro+ 9070 XT, thinking it's no big deal since it's way under the 600W limit of the 12VHPWR that melts on Nvidia, so no big issue for AMD, right?  ... I was wrong. My shit melted after 4 months, and I had to get it RMA'd. It was the 2nd ever recorded case of the GPU melting. A few months pass by, and another 3 have melted. Just don't give yourself the headache and get a 3x 8 9070 XT.  I recommend looking into the Sapphire Pulse or Pure!",pcmasterrace,2026-01-05 01:48:08,2
AMD,nxs2pzb,3x8 Pin.,pcmasterrace,2026-01-05 07:18:06,1
AMD,nxr4dt9,Asus also has a 3×8 option.,pcmasterrace,2026-01-05 03:23:28,1
AMD,nww8h4o,honestly save a bit more and try get the 9060XT 16G. better long term buy and will save yourself from having to upgrade again sooner than later,pcmasterrace,2025-12-31 10:47:42,2
AMD,nww8r00,"9060xt, 16GB if you can 8GB is fine if not. Lower CPU overhead + PCIE X16 on AMD help a lot.",pcmasterrace,2025-12-31 10:50:17,2
AMD,nww8yr6,the 9060xt,pcmasterrace,2025-12-31 10:52:14,2
AMD,nwwajza,"The 9060xt has twice the pcie bandwidth, meaning in vram limited scenarios it’ll have less issues. Also the 9060xt is closer to a 5060ti so I don’t fucking now why it’s still a comparison, it’s cheaper and it also has less cpu overhead! If you say you want the DLSS for upscaling to 4k just get a stronger/16gb card cuz both ain’t doing that really well and FSR looks great at 1440p/4k. And if you only play 1080p and that’s why you hadn’t upgraded yet I highly doubt you’ll ever need to use upscaling in the near future as both cards here are easily doing 1080p if not vram limited",pcmasterrace,2025-12-31 11:06:50,1
AMD,nwway6s,The 5060 is not a competitor to the 9060XT - That's the 5060 Ti.,pcmasterrace,2025-12-31 11:10:28,1
AMD,nwwek05,"Before buying any of the two, I gotta point out that your rig is... dated, for lack of a better word.  Any of the two will fit and function. The question is: how much will your CPU limit the performance of your new GPU (aka bottlenecking). If you don't care about this or do not plan to go for an entirely new rig, then go AMD.",pcmasterrace,2025-12-31 11:42:47,1
AMD,nwwfahu,"99% of people will tell you to get the 9060xt 16gb..  My personal experience woth the card (and its rma replacement) have been less than good.  Constant freezing, random crashes, reboots, bizzarley high temps and wildly spining fans, drivers delete themselves -- anything that could go wrong went wrong.  First gpu and replacement were tested in 6 different PC's between my house and a few friends and all yielded the same results...unfortunate because ive been team red moatly for 25 years now.  Nvidia is overpriced...and tend to like to catch on fire in some cases these days... but id still look into one of their offerings.  But",pcmasterrace,2025-12-31 11:49:10,1
AMD,nwwfhab,Think that these Cards doesn't exist and try to buy the 16GB VRAM Versions.,pcmasterrace,2025-12-31 11:50:45,1
AMD,nwwao31,"AMD, always! leave NVIDIA in the past.",pcmasterrace,2025-12-31 11:07:52,1
AMD,nwwy56b,"Look outside of “ray tracing” nvidea has proven and stated they are a technology company that happens to make gpus, they show no love or support for it like they did 10+ years ago, AMD is that crazy cousin who has been telling you for years what could be possible for possibles sake, not money, and honestly it shows! I have a full amd build and honestly could not be happier with it!",pcmasterrace,2025-12-31 14:01:53,1
AMD,nwx3fvg,With the bottle neck how does that work and how does it show up ?,pcmasterrace,2025-12-31 14:32:38,1
AMD,nwx81lh,"It means that if the CPU is too slow to fully keep up with your GPU, then you are not getting the most out of it or, in extreme cases, it may even cause stutters. I don't know exactly which CPU you have (you may have a typo there), but if it's the same generation as the GTX980 GPU you currently have, I'd definitely watch out for that.",pcmasterrace,2025-12-31 14:58:03,1
AMD,nwyh6vn,Does overclock the CPU make a difference ?,pcmasterrace,2025-12-31 18:43:46,1
AMD,nxym8h8,"I know that the 9070xt has less power draw than other cards using this connector, but the fact that sapphire were aware of all the issues this connector has been causing for the last several years, and decided. Yep that's what we want! Amazing  Sorry  for your loss OP, good luck with the rma.  Edit:  I thought this might be a good place to add a link to derbaur's latest video.   https://youtu.be/5oYJn1yTUXE?si=DTDLozo0ZuJV1JNm  He is marketing his latest equipment for monitoring the connector, which is fine if you want that.  But for me the interesting part is that he could induce an imbalance in the voltage just by push / pulling individual wires within the connector.  So no matter how well the connector itself is seated this problem is still very much liable to occur.  It's a shit specification.",pcmasterrace,2026-01-06 05:53:35,248
AMD,nxymfe4,"From what I've been told on here Sapphire is blaming all these burned 9070s on bad cable connectors and not taking any blame for the socket on their end (why would they, its a business after all). Regardless of where the burn starts or which company is at fault none of this shit would be happening to begin with if they hadn't decided to use that shitty connector. So as I have said in other threads, Sapphire can fuck all the way off with this connector.",pcmasterrace,2026-01-06 05:55:05,100
AMD,nxyvvcy,https://i.redd.it/7cendzf1jobg1.gif,pcmasterrace,2026-01-06 07:14:08,128
AMD,nxyvqwk,This is why I bought the XFX 9070XT. 3x8pin connectors is the safest and superior way to go.,pcmasterrace,2026-01-06 07:13:02,155
AMD,nxyzp70,Funny I was discussing this defective power port few days ago.  The fact it's still being sold till date shows how much of a joke consumers protect laws are.  Buy a fire hazard then they blame the consumers or the cables they used instead of admitting they came up with a defective product,pcmasterrace,2026-01-06 07:49:14,31
AMD,nxz7gsl,They really gotta dump this power connector spec. Its so ass.,pcmasterrace,2026-01-06 09:02:39,20
AMD,nxywrsw,Thank god I canceled my nitro+ order and went with xfx mercury instead,pcmasterrace,2026-01-06 07:22:16,13
AMD,nxz2k17,it's crazy how graphics cards are both skyrocketing in price and also demonstrating a significant degrade in quality.  fuck these shit ass companies,pcmasterrace,2026-01-06 08:15:43,21
AMD,nxyr2cv,I'm sick of this damn connector. I had a 9070xt Taichi and returned it because I was nervous about it. I hope you get sorted out asap.,pcmasterrace,2026-01-06 06:32:46,16
AMD,nxz3zad,I so happy buyed 9070xt with old style connectir,pcmasterrace,2026-01-06 08:29:12,7
AMD,nxzddoa,Bad design. That connector should simply cease existence.,pcmasterrace,2026-01-06 09:59:34,5
AMD,ny082nn,Card number 6? .....🤨,pcmasterrace,2026-01-06 13:46:30,7
AMD,nxyrghm,"I wanted to look up what this card drawings during gaming and I found this:  [Sapphire Radeon RX 9070 XT Pulse Review - Power Consumption | TechPowerUp](https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-pulse/41.html)  The 9070xt on average draws more power than all Nvidia cards except the 4090, 5090, and a few high end 3000 series GPUs.",pcmasterrace,2026-01-06 06:36:04,20
AMD,nxyp8z2,Why anyone would buy a GPU with that plug still blows my mind.,pcmasterrace,2026-01-06 06:17:41,26
AMD,nxyskn8,How is it possible that the cable burns when the Nitro+ has these two fuses on the rails?  https://preview.redd.it/saklpjpndobg1.png?width=819&format=png&auto=webp&s=c412eca471e5add8c4dcd3e13b9185ba7914abd2  [Sapphire Radeon RX 9070 XT Nitro+ Review - Beating NVIDIA - Circuit Board Analysis | TechPowerUp](https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-nitro/5.html)  (disclaimer; its more of a riddle than a question),pcmasterrace,2026-01-06 06:45:35,17
AMD,nxzbxuo,"I was looking to get one on launch. When I found out about the power connector, I thought that my Rx6800 xt is not so bad after all.   I'll never buy anything with that power connector.",pcmasterrace,2026-01-06 09:46:02,4
AMD,ny0a6ux,https://preview.redd.it/ng21tra3jqbg1.jpeg?width=1284&format=pjpg&auto=webp&s=2f324591274aa6a9781222e9f9dfd300cdee6521,pcmasterrace,2026-01-06 13:58:12,5
AMD,nxz8hjv,Every time I see a post like this the more glad I get going for the sapphire pulse 9070 XT instead of the ones with 12vhpwr connectors.  Question did you use a PSU with the new connector native to it or the adapter I assume it comes with? Just curious.,pcmasterrace,2026-01-06 09:12:42,7
AMD,nxzqq8z,I hate this melting pile of junk so much.,pcmasterrace,2026-01-06 11:53:50,3
AMD,nxzrfgu,It's funny that some people still call it user error.,pcmasterrace,2026-01-06 11:59:09,3
AMD,ny0341a,"Hey, sorry to hear that and thanks for sharing.  These failures do seem to be pretty random.  But I would say, to give yourself the best chance of avoiding this issue, you should buy an ATX 3.1 Compatible PSU and use the 12v-2x6/12VHPWR 600W cable that comes with the PSU to connect your high end GPU.  Avoid aftermarket cables and extensions.  And avoid using the adapter cable that comes with the GPU. These adapters just introduce another possible point of failure. They are provided so you can connect your GPU to a PSU that is not ATX 3.1 Compatible, but why not just buy a new PSU to pair with your new GPU for the peace of mind.  I hope Sapphire treats you right and provides you with a replacement under warranty.",pcmasterrace,2026-01-06 13:18:12,3
AMD,nxyqm7j,seems like connectors with bright colored connectors are more prone to faliure,pcmasterrace,2026-01-06 06:29:01,4
AMD,ny0chet,Hey OP was your plug being pulled down? Like from the weight of its own cable? Only the one row of pins is burnt so it seems like the cable was being pulled downward and the top row disconnected.,pcmasterrace,2026-01-06 14:10:53,2
AMD,nxynz4j,"This is the 3 post I saw about this gpu power connector burning up, in the last 2 months. even tho this card uses less power than RTX card.   As this point I recommend you to get the rx 9070 xt alloy pulse version, it will give you a peace of mind because it uses the standard 8+2 connector",pcmasterrace,2026-01-06 06:07:24,4
AMD,nxyzgt1,can we PLEASE stop endorsing buying cards with this connector. It is factory-built ewaste.,pcmasterrace,2026-01-06 07:47:06,4
AMD,nxyyhil,Can never brain why people get gpu with flame generated connector when 8pin exists,pcmasterrace,2026-01-06 07:38:05,3
AMD,nxyt2pf,Ah damn...i just got this card like 3 weeks ago. Does anybody know if undervolting can potentially help? Been running mine at -90mv since day one. Way less heat and higher clock speeds.,pcmasterrace,2026-01-06 06:49:51,2
AMD,nxzqqvf,"this is quite a weird phenomenon by itself. the card is rated at less than 400w, while the 12vhpwr cables are rated at 600w. even with transient spikes, this shouldn't happen.",pcmasterrace,2026-01-06 11:53:58,2
AMD,nxzrz5o,"All I needed to know, is that even a Sapphire public speaking employee said this was an unfortunate decision to use this connector. Even then, you can't deny the track record of this connector.",pcmasterrace,2026-01-06 12:03:17,2
AMD,nxyx66l,This adapter that comes with the Nitro is shit please dont use it if you get that card use a native PSU,pcmasterrace,2026-01-06 07:25:58,2
AMD,nxyo740,"It's obvious that a connector that small shouldn't be used for high power... or even medium power.     I straight up wouldn't buy any card that requires one, it's just too shady.",pcmasterrace,2026-01-06 06:09:12,1
AMD,nxyr8zf,Just curious is your power supply ATX 3 OR 3.1? Has 3.1 fixed these issues? Glad I just got the 9070 xt Reaper it still uses 8 pin connectors.,pcmasterrace,2026-01-06 06:34:18,1
AMD,nxyslk6,is it the dimple variant like the two splits look like or the better leaf spring variant?  https://preview.redd.it/nfz0ogwtdobg1.png?width=1520&format=png&auto=webp&s=4a381af88a282a0ccc4f953c1a892ff30a270406,pcmasterrace,2026-01-06 06:45:48,1
AMD,ny010bf,Yooooo wtf?! I didn't know any AMD GPUs even had the 12VHPWR.,pcmasterrace,2026-01-06 13:05:25,1
AMD,ny03362,I'm honestly not surprised WoT is crashing...,pcmasterrace,2026-01-06 13:18:04,1
AMD,ny0ajng,"Would love to buy the Nitro+ 9070XT, but would only do so when they've done a 2.0 revision and replaced the 12V HPWR with 3x 8 pin connectors.  I want the card to last at least 2 years, and not 12 months especially in the current climate.  Asus TUF 9070XT or one of the Red Devil cards.",pcmasterrace,2026-01-06 14:00:10,1
AMD,ny0his8,"Yeha, the nVidia fire wire is becoming the permanent ""hot stuff"" ...   Graphics card manufacturers should bring back the old connections",pcmasterrace,2026-01-06 14:37:55,1
AMD,ny0iszz,I hope it won't happen to me,pcmasterrace,2026-01-06 14:44:38,1
AMD,ny0ntvz,"more proof that its not the GPU (nvidia/amd) that causes this but a really bad design. Whoever designed 12VHPWR (12-Volt High Power) connector should make like a tree, and get outta here!",pcmasterrace,2026-01-06 15:09:55,1
AMD,ny0pflg,"That looks like a really cheap cable adapter. Still, fuck this connector. Get a quality cable and a quality PSU with native connector to help mitigate.",pcmasterrace,2026-01-06 15:17:41,1
AMD,ny0uv0b,DJ Khaled,pcmasterrace,2026-01-06 15:43:26,1
AMD,ny0wc1k,I considered buying this card but looks like I made a good call by going with Pure instead,pcmasterrace,2026-01-06 15:50:13,1
AMD,ny15y1g,Looks at my freshly purchased 9070XT Taichi...um guys I was told this was a NVIDIA specific problem,pcmasterrace,2026-01-06 16:34:13,1
AMD,ny174lu,So glad I bought the powercolor reaper at launch,pcmasterrace,2026-01-06 16:39:36,1
AMD,ny17d6v,"My Sapphire Nitro 7900 XTX pulls over 500w on the regular, but the 3x 8-pin PCIe connectors seem to handle it just fine. I will never buy a gpu with this connector at this point.",pcmasterrace,2026-01-06 16:40:41,1
AMD,ny18yfq,"I know numerous people that have had zero issues with their 12VHPWR cards (4080, 4090, 5080, and 5090) and the supplied cables despite heavy use (AI work, VFX work/rendering, CAD modelling, 3D design, and tons of high-end gaming on the weekends).  We also have numerous 12VHPWR cards at work that run FEA simulations for days or are used for producing product renderings that we supply to customers in our bids and tenders. None of those PCs have had issues, and the two 4090 PCs I built in 2023 (one of which my sister uses daily for VFX work and game level design) haven't shown any signs of overheating the connector.  I do a ton of electrical troubleshooting at my job so it's hard for me to solely blame the connector design for issues after the boneheaded stuff I've seen people do (and lie about) over the last decade. I've seen firsthand how timid people can be when it comes to seating connectors on circuit boards and I've lost count of the amount of times I've come across wires/cables that are strung as tight as a piano string.  I've also seen a lot of posts of people knocking caps and resistors off their GPU boards, I've seen too many posts of gouged up mobos and busted connectors, I've seen some *abysmal* cable management and cable routing posted here, I've seen a plethora of posts from who clearly lack critical thinking skills, and I've seen way too many silly questions or mistakes posted here to give the average user the benefit of doubt.  The 2022 Class-Action lawsuit brought against Nvidia would have reached a settlement by now, or actually made some progress in the courts, if the issue was as widespread as everyone here makes it out to be.",pcmasterrace,2026-01-06 16:47:54,1
AMD,ny1crpz,Was it an adapter or a direct cable from the PSU to the card? Was it overclocked?,pcmasterrace,2026-01-06 17:05:07,1
AMD,ny1iu18,Does anyone know if I can return my Nitro+ that I bought at microcenter with the protection plan at launch and get a different card without this connection?,pcmasterrace,2026-01-06 17:32:47,1
AMD,ny1mo5f,What's your PSU?  No problems with mine. 1200w PSU with dedicated 600w cable on both sides.  Also tried it with adapter with 3x 6+2pins works like a charm.  Of course some unit have flaws but mostly is human error that leads to this.,pcmasterrace,2026-01-06 17:50:12,1
AMD,ny2cofm,I like how I went from a combustible EVGA 1070 FTW to a combustible 9070XT lol,pcmasterrace,2026-01-06 19:46:50,1
AMD,ny2fhd1,Why don't they just make 2 8pin connectors? Problem solved,pcmasterrace,2026-01-06 19:59:39,1
AMD,ny37009,I feel bad for you but I have to question why you got the 12 pin when you had the option of the good old standard 8 pin that has no chance of melting?,pcmasterrace,2026-01-06 22:06:28,1
AMD,ny3gtu6,The only reason I bought a 5080. Didn't want to mess with these plugs on the 5090's until it gets fixed. Wild it's been a year and things are still burning up.,pcmasterrace,2026-01-06 22:53:49,1
AMD,ny42gyo,Recently just rebuilt my 4080 build with a new PSU (Lian Li Edge) and used the 12vhpwr cable instead of the 3x8 pin adapter I was using with my old PSU. Should I swap over to using that again? Or is it the same risk as using the direct 12v to 12v?,pcmasterrace,2026-01-07 00:45:16,1
AMD,ny4cnrm,Card 6? If it has damaged 6 cards this is user error or faulty power supply. Otherwise basically every single one of these cards would have the issue. The connector is a poor design but at the power level of this card should not have this issue.,pcmasterrace,2026-01-07 01:40:15,1
AMD,ny4knvb,Looks like it isn't just a NVIDIA problem. This worries me because I thought this was mostly a 5090 problem because of its high power draw. Now I am scared for my 5080.,pcmasterrace,2026-01-07 02:23:29,1
AMD,nxzjowh,"I have the same card and it works flawless.  Only difference to your setup is that I don't use an adapter cable.  Connected the 12V-2x6 cable to my ATX 3.1 PSU (12V-2x6 rated for 600W), I've no problems whatsoever.",pcmasterrace,2026-01-06 10:56:11,1
AMD,nxztehh,"Surprise surprise, another adapter lol. Why people have the money for these new GPUs but don’t upgraded something as cheap as their psu is hilarious. It’s deserved in these cases.",pcmasterrace,2026-01-06 12:14:00,1
AMD,nxyuoih,"Shame. I wanted to get a nitro plus because I love how it looks. Decided to get ""just"" a pulse version instead, because that one doesn't use 12vhpwr",pcmasterrace,2026-01-06 07:03:36,1
AMD,nxzhd4f,"The worst thing is that NVidia will never admit that the connector was bad, because if they do they'll get sued into oblivion.",pcmasterrace,2026-01-06 10:35:46,1
AMD,nxzqjvw,Had no idea they even did 9070xt with that connector lmao. 3x8-pin gang,pcmasterrace,2026-01-06 11:52:30,1
AMD,nxzs74g,"I bought this exact card a few weeks ago and thought it had three 8-pin connectors and was relieved. Then i opened the box and was immediately dismayed that it was a 12VHP to 3x 8-pin. I have had to disassemble my computer multiple times in the past weeks as things keep going wrong, and, still not out of the woods, I'm going to have to do so again tonight. I'm legitimately worried. Why anyone would use this godforsaken connector is beyond me.",pcmasterrace,2026-01-06 12:04:59,1
AMD,nxzidmy,"Tight bend (by sapphires design) and an awful fan curve with stupid fan stop likely contributed a lot.  Also the 9070XTs are already operating far beyond a sweet spot, anyone could do -20% power limit and most wouldnt notice a loss in performance.  I have a 9060XT Pulse, and the card was sitting the whole time at 60°C when browsing, while my Gigabyte 9070 sits at 35°C with fan stop off thanks to the Gigabyte Tool (the AMD Driver occasionally resets itself for whatever reason) so I switched to the minimum driver install which doesnt have the fan controls etc.",pcmasterrace,2026-01-06 10:44:49,-1
AMD,nxzn36t,"Pro tip as a fellow 9070XT + Linux user - make sure you are limiting your frame rate. Not maxing out your GPU at 100% the whole time will mean that you are pulling less power and are less likely to have issues like this.  To achieve it easily in Linux, get GOverlay and set it to always-on and limit the frame rate in there to about 5Hz lower than your monitor's max refresh.",pcmasterrace,2026-01-06 11:25:07,-1
AMD,nxz29hg,"tb, It’s wild! Those fuses might not be fast enough to react before the cable fries. Total bummer.",pcmasterrace,2026-01-06 08:12:59,0
AMD,nxzr2a2,I went out of my way to ensure the 9070 XT I just bought had the 2x8 connectors instead of 12VHPWR. I hate it.,pcmasterrace,2026-01-06 11:56:22,30
AMD,nxyplbt,Thank you for the condolences,pcmasterrace,2026-01-06 06:20:30,19
AMD,nxyuvji,there's 5070ti and 5070s that have burned which use equal or less power than the 9070xt lol,pcmasterrace,2026-01-06 07:05:18,31
AMD,ny0dd5j,ASRock did the same for the Taichi card. Absolutely insane. Made it an absolute no-go for me.,pcmasterrace,2026-01-06 14:15:39,4
AMD,nxyr3w9,Also iirc Sapphire designed it so the 12VHPWR bends at a tight 90 degree arc before it plugs into the card which is just begging for an uneven connection.,pcmasterrace,2026-01-06 06:33:08,16
AMD,ny05zka,"Its almost like the connector needs a brace. I also wonder if flipping the GPU, causing the connector to be pulled down by gravity, aids at all.",pcmasterrace,2026-01-06 13:34:49,1
AMD,ny0llpl,"I'm glad I went with the ASUS 9070 XT, which uses a traditional 3x PCIe power connector",pcmasterrace,2026-01-06 14:58:50,1
AMD,ny17wtc,"To be fair you have to go out of your way to get a 9070(XT) with 16pin. There's 3 SKUs with it, likely because they didn't want to go to a 4x octopus cable.",pcmasterrace,2026-01-06 16:43:09,1
AMD,nxzr3lg,"Honestly sapphire isn't what people praise it for. It doesn't come close to EVGA. In my opinion it is more like gigabyte. The company that is actually close to EVGA is PowerColor  Edit: For people downvoting. Why? Why do you dislike me calling out a company that puts a connector known for melting on their most expensive GPU? Their support horrible as well, at least in Germany. Trying to avoid all kind of responsibility or help at any possibility. To a point that you need an extra PC so you can RMA a god damn GPU",pcmasterrace,2026-01-06 11:56:39,-6
AMD,nxynm60,A PR rep said he believed that all reported cases were with third party adaptors (which may have been true at the time). I do not believe he claimed the connector was without fault.,pcmasterrace,2026-01-06 06:04:29,15
AMD,nxzxtbk,"Behold, mother of all bombs.",pcmasterrace,2026-01-06 12:44:52,6
AMD,nxzpnry,"Same. For months I was eyeballing this beautiful card by Sapphire, but after seeing all those failed connector post, I went with the XFX Mercury 9070xt just to have a peace of mind",pcmasterrace,2026-01-06 11:45:39,26
AMD,ny0cb4b,https://preview.redd.it/kz38inm6lqbg1.jpeg?width=641&format=pjpg&auto=webp&s=f7aacfaf91d38e3ceff6ef1f87431f5f59f5b218  me sleeping knowing my ASUS PRIME OC has 3x 8pin power,pcmasterrace,2026-01-06 14:09:55,16
AMD,nxzzfmq,3x? Dang my power color hound only has 2. Still pulls 400w TBP though.,pcmasterrace,2026-01-06 12:55:21,12
AMD,ny0d8yh,"yeah i specifically chose a 9070xt that had 8 pin plugs too. ended up with an asus prime which is just fine for me, pairs with a 5700x3d quite well.",pcmasterrace,2026-01-06 14:15:02,6
AMD,ny07eur,Same. Went for XFX Mercury. Brutal beast. No bonfires.,pcmasterrace,2026-01-06 13:42:49,4
AMD,ny1mlyw,"I beg to differ. The problem is not in the connector per se. [See for yourself](https://youtu.be/bRzl-5HGHes).  https://preview.redd.it/aa1pzapnorbg1.png?width=2786&format=png&auto=webp&s=fedbdf73ea53de84eecaf3c3f417ee31eda0d862  Rather, it's about AMD and NVIDIA removing the amp load-balancing chips from the boards. That's why those problems started in the previous gen (4090/7900XTX) and are NOT exclusive to 12VHPWR.",pcmasterrace,2026-01-06 17:49:56,2
AMD,ny0lwch,"Went to MicroCenter and got the Steel Legend version for two reasons: No 12VHPWR and it was the cheapest version available (at the time, I haven't kept up with current costs).",pcmasterrace,2026-01-06 15:00:17,1
AMD,ny0odns,same. Bought the Asus TUF version due to the 3x8pin connector. I like the look of a single cable but 3x8pin is just the safer option.,pcmasterrace,2026-01-06 15:12:36,1
AMD,ny1h9le,"also, they look much better imho. Especially if you have custom sleeved cables.",pcmasterrace,2026-01-06 17:25:36,1
AMD,nxzlrxc,People really like go to danger place,pcmasterrace,2026-01-06 11:14:06,1
AMD,nxyrvt1,Probably a good move,pcmasterrace,2026-01-06 06:39:39,5
AMD,ny0tstc,Im surprised this is the only comment I've seen about this statement. 6 cards??? Not saying the connector isnt shit but clearly something else is at play here.,pcmasterrace,2026-01-06 15:38:31,3
AMD,ny35wzj,he's definitely doing this to himself somehow.  the odds that these cards independently fried are probably lower than winning a billion dollar lottery.,pcmasterrace,2026-01-06 22:01:23,3
AMD,nxyxcx7,My XFX Mercury OC sits at 374 watts when the power is at +10. Definitely will not got past 374 but ya it'll hit 3400mhz too ha,pcmasterrace,2026-01-06 07:27:41,4
AMD,nxytufe,Max power consumption chart shows the 5080 higher as well.,pcmasterrace,2026-01-06 06:56:23,4
AMD,nxyppgf,"Because its all nvidia and a chunk of amd are using anymore. If you want the higher performance numbers, its your only option sadly. I thought I was safe with a 5080s lower power numbers, but these posts make me a little anxious.",pcmasterrace,2026-01-06 06:21:26,22
AMD,nxyuslf,"Because that one ""Tech Jesus"" said it was a ""user error"" and nobody cared about anything said, shown and tested afterwards.  And because there are corp shills raging around tech subs.  And of course, because a lot of people do not do any research before purchasing things.",pcmasterrace,2026-01-06 07:04:35,8
AMD,nxyqody,"In fairness nitro is much better looking card compared to pulse , i really had to stop myself from buying nitro when it went on sale and get the pulse version simply because of the reported issues with the single connector last month , I'm glad I did seeing all the recent reports",pcmasterrace,2026-01-06 06:29:31,2
AMD,nxz4jav,99% are fine,pcmasterrace,2026-01-06 08:34:35,3
AMD,nxzf1ye,Fuses do not stop arcing which is what causes burning issue.,pcmasterrace,2026-01-06 10:14:54,16
AMD,ny16we9,"If I'm remembering how this board is set up, all the 12V pins go into the same power plane/copper pour right where they hit the board.  The fuses only protect whatever circuit is downstream of them.  They can't protect against an imbalance in pins upstream of them because all those pins are tied together on the board.",pcmasterrace,2026-01-06 16:38:34,2
AMD,ny15ie8,"Yeah, that’s why I went from my sapphire 6800XT to my Sapphire 9070XT pulse",pcmasterrace,2026-01-06 16:32:14,1
AMD,nxzk3cw,"I've yet to see a single post with the nitro+ 12vhp getting burned with a direct connection from the PSU to GPU, every post has been with the included adapter.",pcmasterrace,2026-01-06 10:59:38,4
AMD,nxzalhf,"It's the adapter he has, you can see the 3 cables coming out of the connector in the 3rd picture.  He should've used native, all burnt connectors I've seen were from adapters.",pcmasterrace,2026-01-06 09:33:11,5
AMD,ny2wtcn,"Yeah, if they accept the rma and take care of it; I’ll probably trade the new card in and replace it with one that has 8 pins at micro center or something. It’ll be a loss but selling it to someone else feels like setting them up for failure. I don’t want to worry about it.",pcmasterrace,2026-01-06 21:19:37,2
AMD,nxyrf7a,"The connector with color is there to make sure people know it completely sitting the socket, and if the color are visible then it not sitting correctly inside the socket.",pcmasterrace,2026-01-06 06:35:45,3
AMD,nxyruk4,May be that manufacturers switched to that due to the early problems with legitimate human error in some cases not fully seating the connector. Total guess though,pcmasterrace,2026-01-06 06:39:21,1
AMD,nxyrca2,It’s when people use the adapter cable that it’s prone to failure. I’ve never seen a post about this GPU and a burnt connector unless they were using the adapter cable and I pay close attention because I have the same GPU,pcmasterrace,2026-01-06 06:35:05,-4
AMD,ny2wbu3,"It was on a test bench so gravity would have leveraged the pins perpendicular to where the damage occurred, so no.",pcmasterrace,2026-01-06 21:17:21,1
AMD,nxypwlk,"Yeah, I’m really unsure what to do if they allow me to RMA it because I really don’t want that connector in my house.",pcmasterrace,2026-01-06 06:23:04,8
AMD,nxyr44r,It uses more power in gaming than all Nvidia GPUs outside the 4090 and 5090 and a few 3000 series cards.   [Sapphire Radeon RX 9070 XT Pulse Review - Power Consumption | TechPowerUp](https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-pulse/41.html),pcmasterrace,2026-01-06 06:33:11,7
AMD,nxyz5gj,Or just any other card that has the 6+2 connectors (you wrote 8+2).  Very happy with my big PowerColor 9070 XT Red Devil that's sporting 3 6+2 connectors.,pcmasterrace,2026-01-06 07:44:11,2
AMD,nxzkk50,I mainly think that encouragement needs to be a lot higher to buy or own a ATX3.1 PSU rather than use these adapters. Every failure has been with this exact adapter.,pcmasterrace,2026-01-06 11:03:39,3
AMD,nxyyzgz,Because they have the brain to seat the connector properly.,pcmasterrace,2026-01-06 07:42:38,-9
AMD,nxzu0el,Undervolting can't help bad quality and QC of the connector.,pcmasterrace,2026-01-06 12:18:26,1
AMD,nxzz089,"If it's drawing all of those watts from fewer pins, the current through said pins will exceed the design spec.  E.g. the pins might be rated for 120W each (i read somewhere that there's a very low overhead on these connectors) so losing just one would push the rest to the limit and perhaps cause heating, lose connection on two or more and you're way over and into ""letting out the magic smoke"" territory.",pcmasterrace,2026-01-06 12:52:37,1
AMD,nxypsmt,"Positive reviews and the aesthetics sold me on it, and I regret it now. Figured there was no way it could be a problem at half the rated power draw of the connector. Really hate to be wrong lol",pcmasterrace,2026-01-06 06:22:10,2
AMD,ny1che1,Wall outlets are designed for 3700 W and the contact area is far from being 6 times bigger.,pcmasterrace,2026-01-06 17:03:50,1
AMD,ny3h8ig,"Nope. The 3 and 3.1 is exactly the same, only slightest shorten sense pins only. Cable still catches fire as always cause is not the revision or cable that is the problem, is the connector.",pcmasterrace,2026-01-06 22:55:49,1
AMD,nxyrpmt,Totally believe the 8 pins are the way the way to go. I used the supplied adapter connected to a seasonic focus spx 750w platinum.,pcmasterrace,2026-01-06 06:38:13,1
AMD,nxz7dyz,Hi    4-leaf didnt work out on long term.     https://www.reddit.com/r/pcmasterrace/s/6gJNor71CZ    Too lazy to write my own paragraph so i will use Jonnys one. 😄,pcmasterrace,2026-01-06 09:01:54,0
AMD,nxzk82w,"The problem is that people dont connect the cable till it ""clicks"", and also another thing is that you gotta push it in like maniac. When I got mine 5070ti like almost month ago, I had to plug it in OUT of pcie slot cuz I had feeling I would bend the motherboard in half then rather this cable going full in. So yeah, just gotta make sure there is no that little gap there and we goochy.",pcmasterrace,2026-01-06 11:00:45,5
AMD,nxzl2tb,"The cable curve isn't tight at all, and a fan curve wouldn't do this either, I've got a nitro+ and the ambient currently is 20c with the card at 40c with no fan on. 40-60c inside the chip is nothing and the connector might be at 30c at most which is well within spec",pcmasterrace,2026-01-06 11:08:09,3
AMD,ny2xp7u,"I was doing all of those things, and it still occurred.",pcmasterrace,2026-01-06 21:23:38,1
AMD,ny02rrx,I bought a 9070XT with 3x 8pin specially for this reason,pcmasterrace,2026-01-06 13:16:13,19
AMD,nxzxe34,"YEP, me too, I got a PURE card partly because of this",pcmasterrace,2026-01-06 12:42:00,7
AMD,nxz9ci9,Just when you thought that 'lesser' cards were safe from this problem.  Dammit.,pcmasterrace,2026-01-06 09:21:03,19
AMD,nxzlcmb,"I haven't seen any 5070 burning, do you have source?",pcmasterrace,2026-01-06 11:10:27,-1
AMD,ny23fmu,"The challenger model has 2x8 connectors. I just got it and have zero worries about this issue. This thing is a beast btw, returned my 5070  that I paid the same price for.",pcmasterrace,2026-01-06 19:04:33,1
AMD,nxys58n,"I’d disagree with that, unless you try to pull it really taught for some reason. It really has a pretty gentle radius on it if you use the whole space provided on the card.",pcmasterrace,2026-01-06 06:41:53,25
AMD,nxypg3c,"Sorry to disagree with the PR rep, but that was their included adapter that failed and it was fully seated. I never put the backplate on after the first week so I could keep an eye on it and make sure it stayed seated.",pcmasterrace,2026-01-06 06:19:19,48
AMD,ny05rrw,Asus TUF 9070xt here. 3x 8 pins makes me sleep well at night.,pcmasterrace,2026-01-06 13:33:36,12
AMD,ny0vbcd,i think im the only one that thinks the nitro is ugly,pcmasterrace,2026-01-06 15:45:31,-4
AMD,ny04r3m,My Vega 56 has 3 8 pins...,pcmasterrace,2026-01-06 13:27:46,3
AMD,ny014yb,[https://www.xfxforce.com/shop/xfx-mercury-amd-radeon-rx-9070xt-oc-gaming-edition-with-rgb](https://www.xfxforce.com/shop/xfx-mercury-amd-radeon-rx-9070xt-oc-gaming-edition-with-rgb),pcmasterrace,2026-01-06 13:06:13,2
AMD,ny56bk3,This is the first I've ever seen of such an issue with 8pins as opposed to the avalanche of examples of 12VHPWR.  I think that's telling in and of itself regardless.,pcmasterrace,2026-01-07 04:28:10,1
AMD,ny169nq,"Yeah, one is unfortunate, two is unlucky but six??? There's something bigger going on here.",pcmasterrace,2026-01-06 16:35:43,2
AMD,ny516ma,"I understood it as ""collectively amongst the community"" rather than him buying six cards and having the same issue.",pcmasterrace,2026-01-07 03:56:11,1
AMD,nxz5jrw,Mercury FTW. When they showcased the Nitro i wanted that right up until i saw it uses the 12V high failure cable,pcmasterrace,2026-01-06 08:44:23,8
AMD,ny0wgz1,ive seen mine spike over 400w lol xfx mercury 9070xt oc,pcmasterrace,2026-01-06 15:50:51,1
AMD,nxyuzga,"Yes, you can certainly draw more with the 5080, but I am specifically referencing the average power draw during gaming. I was surprised to see the 9070xt drawing more than the 5080 in that specific use case.",pcmasterrace,2026-01-06 07:06:15,5
AMD,nxyw0er,"there have been (very few) 5070ti that also had a burned connector. If i recall i even saw a 5070 once.  it seems any card with the connector is susceptible but at the end it still is a combination of user error+bad luck, best of luck you never run into those issues",pcmasterrace,2026-01-06 07:15:23,3
AMD,nxyrqjq,If people only knew how to vote with their wallet.,pcmasterrace,2026-01-06 06:38:26,0
AMD,nxyufmz,'Temporary' higher performance,pcmasterrace,2026-01-06 07:01:27,1
AMD,nxzst1o,"I'm sure they saw a tiny manufacturing cost reduction and said, ""press print"". Smaller boards, one less socket, win-win for them.",pcmasterrace,2026-01-06 12:09:36,1
AMD,nxzsj0s,It took one picture the first time someone posted it to know it wasn't worth the risk.,pcmasterrace,2026-01-06 12:07:30,4
AMD,nxzfdjr,aka not seated fully.,pcmasterrace,2026-01-06 10:17:49,-3
AMD,ny2tb46,Not the point.  It leaves only poor placement of the cable why they burn.  It is just simple as that.,pcmasterrace,2026-01-06 21:03:39,2
AMD,ny2k7m7,"This was my understanding as well. If they were able to balance the power or separate the power planes it would have been less of an issue.   Returned my Nitro+ to get a power color hellhound and never thought about the power connector again, though I wish my GFX card was prettier lol.",pcmasterrace,2026-01-06 20:21:39,1
AMD,nxysrud,"probably true, but i doubt coloured connectors handle heat better than normal black ones",pcmasterrace,2026-01-06 06:47:18,1
AMD,ny5a3z9,You're being Downvoted but you're right. Same with most 5090's burning being MSI.,pcmasterrace,2026-01-07 04:52:54,2
AMD,nxyvhch,I choose to believe you because I want to believe.,pcmasterrace,2026-01-06 07:10:39,1
AMD,ny2yxwk,"I'm not quite sure what you mean by that, but anyway sorry that really sucks. This stupid connector needs to go.",pcmasterrace,2026-01-06 21:29:17,1
AMD,nxyqj2s,"Starts to feel more like ""when"" than ""if"".",pcmasterrace,2026-01-06 06:28:16,5
AMD,nxyqoov,"Let try and see if they will allow it, btw the damage on the your connector is less than the last one I saw, same card as your. But the entire connector and socket is melted",pcmasterrace,2026-01-06 06:29:35,1
AMD,ny3a5es,True. I'm on am4 and even I have an ATX 3.1 PSU,pcmasterrace,2026-01-06 22:21:27,1
AMD,ny1c0e4,Ah OK. Bummer. Otherwise its a powerhouse of a card and the hidden cables look great in my build.  https://preview.redd.it/9zqff7msfrbg1.jpeg?width=3826&format=pjpg&auto=webp&s=25de676eac24373f4ba6ecae67424f715d819a51,pcmasterrace,2026-01-06 17:01:39,1
AMD,nxytrie,"For the draw of your card, either undervolt (9070 xt does really well undervolted as otherwise it ramps up to close to 400w) or invest in the new power management boards coming out. There's one by derBauer and one by aqua computer.  https://forum.aquacomputer.de/weitere-foren/english-forum/114449-new-ampinel-active-power-management-for-graphics-cards-safe-and-intelligent/",pcmasterrace,2026-01-06 06:55:42,0
AMD,nxzs8z3,"You are using a PSU from 2021 without ATX 3.0 or 3.1 and blame the melting on the connector.  12v-2x6/12VHPWR errors mostly occur with using adapter cables. I know, that the adapter came with the GPU, but do not ever use that.  **Always run a 12v-2x6/12VHPWR native with a compatible PSU.**  I have the Nitro+ 9070XT myself and a 4070 (12GB) prior to that. I've never used an adapter cable and have **never** had any problems during the entire time I have been using them.",pcmasterrace,2026-01-06 12:05:23,6
AMD,nxzo5hh,"That's true. When I see posts about failing connectors it's people using adapter cables and old PSUs. I prefer using a 12V-2x6 cable native over a splitter cable anytime. Yes the adapter cable shown came with the card.. I can't proof it, but in my opinion the adapter cable and unbalanced load caused the melting.",pcmasterrace,2026-01-06 11:33:47,1
AMD,ny33l77,That's a bummer! Glad I went with the normal PCIE connector GPU then.,pcmasterrace,2026-01-06 21:50:40,1
AMD,ny046b6,Yes i also went for the puke for this reason,pcmasterrace,2026-01-06 13:24:28,8
AMD,nxzbaus,"And to make it more fun, Nvidia and partners can also avoid honoring warranty if you used your own PSUs cables and not the adapter, since they call the PSU cables third party xDDD",pcmasterrace,2026-01-06 09:39:57,13
AMD,ny10hmr,Each pin is only rated for 9.2A so it's possible to burn the connector even with a low-power card if the load becomes asymmetric enough across the pins.,pcmasterrace,2026-01-06 16:09:15,2
AMD,nxzir0k,"The problem is almost never related to how tight the connection, its just a bad design so they either ditch it or make it mandatory for every card to have a load balancing system for the pins.",pcmasterrace,2026-01-06 10:48:04,6
AMD,ny1ihoa,Were there any signs that something was wrong before this happened?  I also have this card and have been noticing weird shit happening when I'm playing Tarkov and streaming in discord.,pcmasterrace,2026-01-06 17:31:12,1
AMD,nxyq3qb,>I never put the backplate on after the first week so I could keep an eye on it and make sure it stayed seated.  Does this translate to that you twiddled it to make sure it has not popped out on its own?,pcmasterrace,2026-01-06 06:24:42,-24
AMD,ny17ei0,"Not just you, don't worry.",pcmasterrace,2026-01-06 16:40:51,1
AMD,nxyxts6,I'm guessing it's because it's a more efficient card due to it's better performance overall? Though that's implying there's either an FPS limit or bottleneck...,pcmasterrace,2026-01-06 07:31:59,-6
AMD,nxz2pka,"If a connector is supposed to be plugged in by the user (and I don't see an option to hire a certified connector plugger inner), it should be good enough to withstand imperfect conditions. Overall, if the connector is fully seated (which the OP claims it was), it should not burn. IMO, it's just a bad design",pcmasterrace,2026-01-06 08:17:09,14
AMD,nxysp9s,"Too many sheep. Those of us who care about these things are a tiny fraction of their user base. 90%+ of nvidias users just go ""nvidia good. I want good.""",pcmasterrace,2026-01-06 06:46:41,-11
AMD,nxzth9r,plenty of people also had issues with the traditional PCI express cables.   Get the 4 spring terminal version instead of the dimple terminals and you will be fine.,pcmasterrace,2026-01-06 12:14:34,-1
AMD,nxzgc5n,"Yes arcing usually is due to poor mechanical contact, but if the connectors are poorly made even if the user does everything right its still going to arc.",pcmasterrace,2026-01-06 10:26:29,13
AMD,nxzkbj6,It's more likely that these adapters are either poor quality or have huge QC issues. Every burned card has used this adapter so far.,pcmasterrace,2026-01-06 11:01:34,1
AMD,ny5akn3,Getting downvoted on pcmr for posting correct information that people don’t like is pretty much an everyday occurrence lol,pcmasterrace,2026-01-07 04:56:00,2
AMD,ny3xg1f,"I'm not gonna blame people for not having a atx3.1 psu, I've been in pc gaming for over 10 years and the only reason I got an ATX3.1 PSU is because my last one had a slight water accident with my custom cooling.....",pcmasterrace,2026-01-07 00:19:22,2
AMD,ny2xd1h,"I used the Provided adapter and followed the specific instructions from the manufacturer to use it, so while you’re right to a degree. Without specific clear instruction with the product from the manufacturer to use a specific psu it’s still on them. If they had that, I agree user error at that point. But that’s not the current reality",pcmasterrace,2026-01-06 21:22:05,1
AMD,ny02u4c,"Which seems legit if you have some knock off power supply, but if you have some top of the line seasonic that seems unreasonable. Idk",pcmasterrace,2026-01-06 13:16:36,5
AMD,ny2u3gq,Black screens while gaming,pcmasterrace,2026-01-06 21:07:19,1
AMD,nxyqp2e,"If you mean did I wiggle the connector regularly to see if it was staying in, no lol. I had it on an open air test bench and looked at it with a flashlight to see if there was any movement over time. I wondered if the problems with the connector might have been caused by it slowly backing out over time with heat from the card causing it to expand and when the pc is off maybe it contracts due to the cooler temperature. Kinda like the world’s slowest inch worm. Probably a tin foil hat theory, but it was all for naught",pcmasterrace,2026-01-06 06:29:41,22
AMD,ny17oln,5600x lol,pcmasterrace,2026-01-06 16:42:06,-6
AMD,nxz8p90,">Sheep  Cringe lmao  I buy the best because I can afford it and want the best performance. When their most powerful card can’t even beat Nvidias second most powerful, I have to buy NVIDIA.   When AMD releases a more powerful card than NVIDIA I will gladly give them my money.",pcmasterrace,2026-01-06 09:14:47,6
AMD,ny4s6r0,"true, but when you can reduce the risk of losing a £600 gpu (like the one OP got) for example by spending an extra £100 on a new psu, you might as well do it (then again op couldve got a 9070xt that has a better connector/connectors)",pcmasterrace,2026-01-07 03:04:25,1
AMD,ny03csi,"Ask Nvidia that, not me.  Massive bullshit, but consumers keep rewarding them for it.  There is enough idiots that instead of using their consumer rights, just buy another 5090 🤦🏻",pcmasterrace,2026-01-06 13:19:38,8
AMD,ny2ex71,"Thing is, they can't tell if you used a crappy 200w PSU or a $1000 PSU",pcmasterrace,2026-01-06 19:57:06,0
AMD,nxyujs3,https://preview.redd.it/m530l2pxgobg1.jpeg?width=1325&format=pjpg&auto=webp&s=1f925aba74297297f9a8cb6e8daeab043f86744d  [Minitek® Pwr CEM-5 PCIe® Connector System | Wire To Board Connectors | Power | Signal Connectors](https://www.amphenol-cs.com/product-series/minitek-pwr-cem-5-pcie.html),pcmasterrace,2026-01-06 07:02:26,-5
AMD,nxyvh0b,>looked at it with a flashlight to see if there was any movement over time.  The connector has a locking mechanism.,pcmasterrace,2026-01-06 07:10:34,-13
AMD,ny001qq,"At the risk of your house burning down, potential death, so you can play some shitty AAA title 4 hours a week at 4k instead of 1440.  'Sheep' is too kind.",pcmasterrace,2026-01-06 12:59:16,-7
AMD,ny4sq09,"I absolutely agree, there should be a warning to let people know that they should use a proper PSU, rather than this adapter.  Although saying this, I don't think I've seen too many 5080-4090-5090 that have had this issue compared to the amount sold, so it must be an issue with this specific adapter.",pcmasterrace,2026-01-07 03:07:24,2
AMD,ny4222a,Every single nvidia consumer can stop buying their gpus and yet nvidia wont change anything because gaming gpus are a very small fraction of their revenue,pcmasterrace,2026-01-07 00:43:05,1
AMD,ny2knma,"No, they can't, so I don't disagree with it.",pcmasterrace,2026-01-06 20:23:45,0
AMD,nxzgmet,not designed to mate is just funny though,pcmasterrace,2026-01-06 10:29:07,5
AMD,nxz4dfk,Have you actually used one though? That mechanism does basically nothing.,pcmasterrace,2026-01-06 08:33:00,22
AMD,ny020bx,Thats funny because some AMD cards have the same connector and thus the same risk LMAO.  Your envy amuses me.,pcmasterrace,2026-01-06 13:11:37,4
AMD,nxz4sht,>Have you actually used one though?  Yes. I have Sapphire 9070 XT Nitro+  >That mechanism does basically nothing.  ![gif](giphy|TseBjMu53JgWc)  Thats when you dont plug it in properly.,pcmasterrace,2026-01-06 08:37:04,-18
AMD,ny42i3j,I am dying because he is commenting on a post where an AMD connector failed,pcmasterrace,2026-01-07 00:45:26,1
AMD,nxz5192,Maybe it's different on there but I got a 4080 FE and it does absolutely nothing. Are you one of these that finds the need to try and defend the multi million corpos?,pcmasterrace,2026-01-06 08:39:24,20
AMD,nxzjnol,"I have the Nitro+ and the latch seems to be doing its thing, can't pull it out without first pressing on the latch and even then it's super stiff to move.",pcmasterrace,2026-01-06 10:55:53,5
AMD,nxz5ndb,Oh Im sorry.  tHe MuLtI mIlLiOn CoRpOs aRe JuSt ChEaTiNg aNd RoBbInG uS! SeLlIiNg FlAwEd PrOdUcTs! lEtS gEt ThE pItTcHfOrKs BuBbA!  tHe 12vHpWr Is A pRoDuCt Of WiCtHcRaFt!,pcmasterrace,2026-01-06 08:45:23,-16
AMD,ny2d7tt,"Absolutely not worth it if you already are on a 13700.  Good ""cheap"" coolers are;   Phantom Spirit.   Artic freezer 36.   Frozen a720.   Dark rock pro 5.   There are many more but those are 4 solid choices. Choose your poison.",pcmasterrace,2026-01-06 19:49:19,19
AMD,ny1q0p8,"What graphics card you have, would be a  better investment to upgrade that and keep the processor. I'll wait till next amd gen personally if you want to move.",pcmasterrace,2026-01-06 18:04:55,6
AMD,ny2s1wn,"Why the fuck do you even think that you need a CPU upgrade? A 13700k will last you for at least another 5 years minimum.  If playing in 1440p or higher resolution switching to a 9800x3d will net you literally only 1-2% more FPS at the absolute best. But sure, if you want to burn you money go ahead and do so...",pcmasterrace,2026-01-06 20:57:56,8
AMD,ny1cz0b,"The switch is worth it especially for gaming, the 9800x3d is a beast and temps are pretty manageable at stock settings. Your hyper 212 might be okay but id personally upgrade to something like a Noctua NH-D15 or Arctic Liquid Freezer II just to be safe and i grabbed mine off ebay using a search tool called Ubuyfirst and i found a cooler like $30 cheaper than other marketplaces.",pcmasterrace,2026-01-06 17:06:03,77
AMD,ny1n5cu,I personally wouldn't switch. The 13700 is a very capable CPU and it can push any GPU without bottleneck.,pcmasterrace,2026-01-06 17:52:17,41
AMD,ny1qwjp,"Is your i7 not meeting what your wanting?   This is an odd upgrade, a whole on platform change for a few percentage points.   If i was you, i’d save my money on the cpu upgrade, buy a better cooler like an NHD15 (how the hell is a hyper212 cooling a 13700) then upgrade to the next gen of AMD or buy a better video card for meaningful gaming performance increases.   the NHD15 will last you years and years across multiple upgrades.",pcmasterrace,2026-01-06 18:08:51,25
AMD,ny3etbe,">Now im planning to switch to amd ryzen 7 9800x3d  It's a 10-15% increase in performance. Imo, not worth the cost to switch (new mobo, new cpu, new cooler). It sounds like you have no good reason other than wanting to spend money.  The only reason u ever want to upgrade is because your system can't run the game.",pcmasterrace,2026-01-06 22:43:53,3
AMD,ny3rj4p,"My i7 12700K doesn't bottleneck a 5080.  Any upgrade to a 13th gen would be fairly marginal IMO and you are just throwing money away.  You'd be better off waiting to see what the Nova Lake CPU's look like in late 2026, or wait for the next big thing from AMD.",pcmasterrace,2026-01-06 23:48:52,3
AMD,ny1q0lf,"Yes, but not in your case. You should have gone AMD instead of the Intel chip to begin with. That's when you should have switched. Now it's not worth it imo.  The 13th gen cpu is still a relatively new chip and a very capable chip at that. There is no need to switch. You would be wasting money. I'd just keep using the Intel cpu until you need a new PC in ~5-7 years.",pcmasterrace,2026-01-06 18:04:54,41
AMD,ny1z9p7,I’m probably an aggravating person but here it goes anyway : What does your current CPU not do that you’re looking for ? What does your current workflow look like? What’s your budget ? What’s the intention behind a switch to AMD ? I.e. a focus on Adrenalin? Or considering an AMD GPU for FSR4 and assessing compatibility?   You have a capable chip it’s a matter of what your needs are . I hope this helps,pcmasterrace,2026-01-06 18:46:08,29
AMD,ny1rg7r,"Why are there so many people asking about upgrading CPUs when they don't need to? AM5 is good and X3D CPUs are really good, but AM5 is halfway through it's life cycle and more importantly, you won't see a significant improvement in performance usually if you're playing at 1440p or above, as the GPU will be the bottleneck by then.",pcmasterrace,2026-01-06 18:11:15,5
AMD,ny2xbos,Stop. Why are you upgrading? Specifically why? Your pc should last your some more years. There is no need to spend any money to go away from that CPU.,pcmasterrace,2026-01-06 21:21:55,5
AMD,ny1ox71,A 13700 is fine.  You seem to have an obsession with temperature. That's the problem here.,pcmasterrace,2026-01-06 18:00:02,17
AMD,ny16omb,"Only if you're making a big switchover. Don't get amd just to get amd cuz it's ""better"", that's just fanboy shit  My i5 14400 had better single core performance than my r7 5700x which directly translates to, better for games",pcmasterrace,2026-01-06 16:37:36,19
AMD,ny1lmwy,No.  Your CPU is completely fine.  Barely any uplift in performance in a handful of titles is not worth an entire new CPU / mobo / ram combo.,pcmasterrace,2026-01-06 17:45:38,59
AMD,ny30vue,I’d just spend that money towards a better gpu,pcmasterrace,2026-01-06 21:38:15,2
AMD,ny376jj,If your games play fine no,pcmasterrace,2026-01-06 22:07:20,2
AMD,ny3ck3l,Stick with Intel for the next few years. You'll get more out of your system by sticking with that CPU style.,pcmasterrace,2026-01-06 22:32:59,2
AMD,ny1680x,Why switch at all? The 13700 is a totally capable chip?,pcmasterrace,2026-01-06 16:35:30,29
AMD,ny1wazf,13700 is still one of the best non x3d chips on the market??  https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/18.htmlhttps://www.techpowerup.com/review/amd-ryzen-7-9800x3d/18.html   and thats tested with 6000mt/s ram. if you have higher speed itll be even better.,pcmasterrace,2026-01-06 18:32:49,5
AMD,ny1tj35,Make sure your bios is up to date but otherwise it’s a fine chip.,pcmasterrace,2026-01-06 18:20:23,2
AMD,ny1daav,Not worth it unless you are actually experiencing problems / limitations with your setup.,pcmasterrace,2026-01-06 17:07:30,12
AMD,ny1p5z8,For gaming? No.,pcmasterrace,2026-01-06 18:01:06,2
AMD,ny1w3bf,"While the 13700 is no slouch, I went from a 12700k to a 9800x3d and felt a good uplift in performance. Same with my wife's PC, who went from a 5800x3d to a 9800x3d.   However, we mostly play very CPU heavy games and MMOs where CPU gets hit hard and our old gaming builds went towards upgrading other systems.  If that's not you, if you're playing mostly GPU heavy titles - you'll see improvements in your 1% lows. But I'm not sure you're going to see $500+ worth of improvements.  I'm using a pricey NHD15 on my 9800x3d. Just had my CPU pegged at 100%, 130 watts, for a few minutes and was just passing 60c on a fairly quiet fan profile.  My wife is on a $40 Thermaltake Phantom Spirit Evo. Her temps are just about the same as mine.   And if you're using your 13700 for productivity, you'll take a hit there going to the 9800x3d.",pcmasterrace,2026-01-06 18:31:50,2
AMD,ny2yko2,If ur cpu ain't going over 70c like ever while gaming ur not overclocked or using its full potential,pcmasterrace,2026-01-06 21:27:36,1
AMD,ny2z9r6,"It's a bad time overall. If your chip is giving you degradation issues, as they are susceptible to, then maybe. I upgraded late 2024, dropped 12900k for 9800X3D and it was absolutely worth it. Prices today would definitely change my mind. It's a tough call though.",pcmasterrace,2026-01-06 21:30:48,1
AMD,ny33v1n,I wouldn't switch to either unless I'm doing a generational upgrade and my current PC isn't performing to my expectations.,pcmasterrace,2026-01-06 21:51:54,1
AMD,ny34b9o,"If a cooler can handle a 13700 it will do a 9800X3D with no issues.  The 9800X3D might run hot depending on what you play, but it's designed to run that way. It uses less energy than a 13700.",pcmasterrace,2026-01-06 21:53:58,1
AMD,ny34c8b,"13700 is fine.  And that cooler can easily handle a 9800X3D, they are actually easier to cool (160 watts peak vs 220).",pcmasterrace,2026-01-06 21:54:05,1
AMD,ny35an0,I just switched from the i9 11900kf to the ryzen 7 9800x3d and it was an insane improvement.,pcmasterrace,2026-01-06 21:58:30,1
AMD,ny36f4e,"I use a 212 halo on 7800X3D, not the best, but gets the job done. If you’re just reusing what you have, it’s enough. Just don’t expect it to be optimized since X3D can run hot. I’m actually considering going with peerless assassin when I replace my case.",pcmasterrace,2026-01-06 22:03:43,1
AMD,ny3ejmk,"Thermalright Peerless Assassin 120 SE is a legendary $35 air cooler and should be more than sufficient for the 9800X3D  With that said 13700 is a very capable chip, still one of the top 10.",pcmasterrace,2026-01-06 22:42:34,1
AMD,ny3f7oy,Considering your current cpu draws a fuckton more power than a 9800X3D you should be fine  By running hot they probably just mean the idle temps as X3D chips do tend to idle around 40,pcmasterrace,2026-01-06 22:45:50,1
AMD,ny3gtf6,"The 9800X3D doesn't run hot. It's cooler than the previous and equivalent x3d, the 7800X3D. I would only change if you have 6000 MHz DDR5 (i don't think DDR4 can reach that speed, just wanted to be precise). Cause AMD doesn't like above 6000 MHz  I'm also surprised no one is suggesting the peerless assassin. It was one of the best coolers some months ago. If you really want to see comparisons, check gamers nexus on youtube, for the 9800X3D and for coolers.  I went from a 10700k to a 9800X3D and the uplift was amazing. The 10700k was still solid with my 4070 Ti (now I have full AMD, got a 9070 XT too, just went with that cause RTX 50 series are a mess and I couldn't find a 4080 Super and due to speculation that GPUs are also going to increase, I found a 9070 XT for 600€). Also, I have a Artic Liquid Freezer II 420mm in push pull. Unless it's compiling shaders, it usually doesn't go above 70° Celsius, it's on the 60s.  Edit: also most people are asking you why are you upgrading the 13700... I think people here are unaware or already forgot the problems Intel 13th and 14th series have, even with the BIOS upgrades, they could still deteriorate. And OP could try and sell thr 13700 and mobo to recover some money. Is it a waste? Depends on your point of view. Also OPs question is not money wise overall, he only said he doesn't want to waste another 150 on a cooler if not needed.",pcmasterrace,2026-01-06 22:53:45,1
AMD,ny3mubn,Get a good 120-140 cooler and you are gold,pcmasterrace,2026-01-06 23:24:16,1
AMD,ny3mv14,"If you game at 1440p or 4k, its not worth it.  Hugely worth it for 1080p.  This is coming from someone who swapped from a 14700k to a 9800x3d, so I know.",pcmasterrace,2026-01-06 23:24:22,1
AMD,ny4dcjm,"Depends on kind of games you play and the monitor you have, if you are playing competitive titles like CS:GO, valorant, COD and other FPS titles and have a 240hz or better monitor, and good enough GPU for 9800x3d then yeah go ahead and upgrade. If you are playing story mode single player AAA games than it’s absolutely not worth it especially since you already have a 13700k.",pcmasterrace,2026-01-07 01:44:00,1
AMD,ny4ielb,What would you do with that amazing CPU if you bought a new one? She's a beast.,pcmasterrace,2026-01-07 02:11:19,1
AMD,ny4j1uj,wait for AM6,pcmasterrace,2026-01-07 02:14:47,1
AMD,ny5buzx,"You will see gains in 1080p medium settings with a 5090, ot this is your game go for it and be prepared for the ddr5 proce hammer if you havent one kit yet Other than that i would notbchange except ypu have gpu issues (dont get gpu over 80% but cpu is at 90)",pcmasterrace,2026-01-07 05:04:46,1
AMD,ny1djx6,"X3D chips dont run that hot. I got an AIO for my 7800X3D and its never ran hotter than 60, its absolute overkill. A dual tower cooler will be perfectly fine.  You will get some improvements but dont expect a night and day difference. I switched from intel to AMD and I have been happy since.",pcmasterrace,2026-01-06 17:08:43,1
AMD,ny1lk3x,I made the switch after 9900k and never looked back. Best decision I ever made,pcmasterrace,2026-01-06 17:45:17,-2
AMD,ny1rrop,You don’t really need to upgrade and I can’t add much more than my anecdotal experience. I’m using a $60 dual tower thermaltake for my 9800x3D and my cpu is nice and cool.,pcmasterrace,2026-01-06 18:12:38,0
AMD,ny1y6p2,"I went 14900kf to 9950x3d, didn’t really change my gaming performance but my temps surely went down and I don’t have random crashes from chip degradation now so it’s a win win in my books. However if you don’t have any issues right now there is no reason to upgrade especially with prices the way they are right now",pcmasterrace,2026-01-06 18:41:22,0
AMD,ny2b3mm,"Absolutely worth it. After Intel finally admitted they knew exactly which CPUs were cooked, but still refused to tell you, and opened up their RMAs I immediately started planning an AM5 build. Returned the motherboard, ram, and CPU to microcenter, walked out with my AMD parts, lived happily ever after.",pcmasterrace,2026-01-06 19:39:37,-1
AMD,ny4y9f8,"can you use 2-3 coolers, at once?",pcmasterrace,2026-01-07 03:38:51,-1
AMD,ny1m7zf,"I will try to find a good deal but if i dont i will stick to this cooler, thanks for the advice",pcmasterrace,2026-01-06 17:48:15,12
AMD,ny53rrl,Arctic freezer II and Noctua products are the most overpriced bullshit ever for no good reason either.   [https://gamersnexus.net/megacharts/cpu-coolers#200W-normalized-100](https://gamersnexus.net/megacharts/cpu-coolers#200W-normalized-100)  Noctua at least has good temps for an air cooler but the freezer II 420mm loses to a fucking og galahad 240mm at full load,pcmasterrace,2026-01-07 04:12:07,1
AMD,ny1pjns,This right here ⬆️,pcmasterrace,2026-01-06 18:02:48,7
AMD,ny243o2,Even my 7800X3D can't push BF6 to the maximum frame rate my monitor's refresh rate allows for (280.),pcmasterrace,2026-01-06 19:07:36,6
AMD,ny3fgwn,NHD15 is total waste of money when Peerless Assassin does almost the same job for like 1/3 of the price.,pcmasterrace,2026-01-06 22:47:05,8
AMD,ny3amd3,"Exactly, if it ain't broke don't fix it",pcmasterrace,2026-01-06 22:23:42,4
AMD,ny3nobb,"D15 at this point is just a waste of money, Phantom Spirit 120mm offers the same performance for 1/5 of the cost.",pcmasterrace,2026-01-06 23:28:34,2
AMD,ny37v5a,Why is he switching? If there are no cpu issues save your money and wait? Ddr5 prices are fucked rn,pcmasterrace,2026-01-06 22:10:35,13
AMD,ny37o0t,"This.  At best you get marginally better average framerates, and better 1% lows...  Which, unless your 1% lows are bad...  You wont even notice.",pcmasterrace,2026-01-06 22:09:39,7
AMD,ny3v3ia,"Some people spend more time wondering about what their experience COULD be instead of just living their current experience. ""Grass is always greener"" creates fomo and drives consumer spending.    Enjoy what you have and unless there is some massive barrier preventing you from enjoying a game or performing your workflow in a reasonable manner, don't spend a dime.",pcmasterrace,2026-01-07 00:07:16,1
AMD,ny2r8ek,Because this and other hardware related subs became an AMD fanboy echochamber that tells everyone all other CPU's are sooooo bad and too many people fall for that nonsense.,pcmasterrace,2026-01-06 20:54:13,3
AMD,ny1dawo,"In alot of situations, that 3d Vcache also directly translates to better gaming performance.   That said, neither single core performance or 3d Vcache are a catch all for all games or use cases. Some will make better use of one, others will make better use of the other. There's also other factors as well. For example, if you're most of the time limited by the GPU's performance, a better CPU isn't going to make all that significant of a difference.   Basically, I'd say there's simply not enough information to determine if that CPU change is the most practical move.",pcmasterrace,2026-01-06 17:07:34,6
AMD,ny1tbtf,"13700 might be ddr5, but I agree. It’s still a very strong cpu and there won’t be much, if any, noticeable difference outside of professional competition gaming.",pcmasterrace,2026-01-06 18:19:29,21
AMD,ny3czft,If he plays one/few games a lot that benefit massively from the cache then It’s definitely worth it.,pcmasterrace,2026-01-06 22:35:01,0
AMD,ny1okw2,"Seconded; unless you're playing extremely CPU-dependent games (which it doesn't sound like you are), the difference between your chip and a 9800X3D is not going to be worth the cost of the CPU and new motherboard.",pcmasterrace,2026-01-06 17:58:32,19
AMD,ny1osnl,Yes but 9th gen is a bit older now this guys 13th is still perfectly fine for a couple of years.,pcmasterrace,2026-01-06 17:59:28,11
AMD,ny16y14,"You seem to be linking to or recommending the use of UserBenchMark for benchmarking or comparing hardware. Please know that they have been at the center of drama due to accusations of being biased towards certain brands, using outdated or nonsensical means to score products, as well as several other things that you should know. You can learn more about this by [seeing what other members of the PCMR have been discussing lately](https://www.reddit.com/r/pcmasterrace/search/?q=userbenchmark&restrict_sr=1&sr_nsfw=). Please strongly consider taking their information with a grain of salt and certainly do not use it as a say-all about component performance. If you're looking for benchmark results and software, we can recommend the use of tools such as Cinebench R20 for CPU performance and 3DMark's TimeSpy and Fire Strike ([a free demo is available on Steam, click ""Download Demo"" in the right bar](https://store.steampowered.com/app/223850/3DMark/)), for easy system performance comparison.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/pcmasterrace) if you have any questions or concerns.*",pcmasterrace,2026-01-06 16:38:46,0
AMD,ny1qkcm,"It’s really not worth it overall, the 13700 is more than plenty for gaming at any frame rate or resolution",pcmasterrace,2026-01-06 18:07:21,34
AMD,ny1s16t,Or get an air cooler from Thermalright. They're dirt cheap and really good.,pcmasterrace,2026-01-06 18:13:48,9
AMD,ny3njdb,"Don't waste your money on D15, buy a cheaper Phantom Spirit 120mm which offers the same performance.",pcmasterrace,2026-01-06 23:27:51,3
AMD,ny4ata3,"I don’t have like exact numbers but I replaced my 13700k with a 9800x3d and only saw around an average 10FPS increase in some games. Maybe 15-20FPS in some UE5 games (paired with a 5090). Best improvements have been more stable temps, not having to worry about the 13th/14th gen degradation issue, and noticeably better 1% lows. Games at least feel more stable. Upgraded my wife’s rig from a 10700k to a 9800x3d (3080) this weekend and it has been significant for upgrade all around.   If you’re worried about part availability/longevity that’s a fair reason to upgrade at the moment if you already have a good GPU. In my case it was degradation as an excuse to replace it. I wouldn’t do it for thermals or in hopes of significantly better performance though when that money could probably be spent better elsewhere.   If thermals are really an issue for you just try and find a decent AIO and be done with it, I hated trying to micromanage the 13700k when I had it on air. You can find arctic freezers in the 100$ range sometimes",pcmasterrace,2026-01-07 01:30:07,1
AMD,ny1ofgo,I have the liquid cooler for the 9800x3d and my temps never go over 60 at max loads.,pcmasterrace,2026-01-06 17:57:52,0
AMD,ny3jhm4,You can get a motherboard and 9800x3d from msi us store and the 9800xed is only 400 but you HAVE to pair it with a motherboard...got 2 tomahawk x870e for 249.99 each and 2 9800xed for 400 at christmas...board isnt on sale anymore but still worth it,pcmasterrace,2026-01-06 23:07:07,0
AMD,ny3mayt,I have the phantom spirit EVO which was about 60-70CAD and it cools my 9950X3D really well and I was about to buy a artic freezer 3 or the NHD15 or Dark rock pro 5 which all were 3x the price,pcmasterrace,2026-01-06 23:21:30,2
AMD,ny3efiw,He most likely has DDR5 already.,pcmasterrace,2026-01-06 22:42:01,1
AMD,ny1w146,CPU is not ram specific.  Just updated one of my pc’s with 14700 on a ddr4 board.  Why?  Cause I already had 64gb of it.,pcmasterrace,2026-01-06 18:31:33,8
AMD,ny3uzd6,Ram may not be compatible with AM5 board or x3d chip though.,pcmasterrace,2026-01-07 00:06:41,1
AMD,ny3usm1,For like a thousand us dollars for a few % uplift in a few games?  Maybe if you didn't actually fucking work for your money... Which you clearly didn't.,pcmasterrace,2026-01-07 00:05:43,2
AMD,ny1ry4o,And then the cost of buying a new CPU after the ASRock mobo kills the 9800X3D,pcmasterrace,2026-01-06 18:13:26,7
AMD,ny3crwo,Unless he plays games that have massive gains from the 3d cache.,pcmasterrace,2026-01-06 22:34:01,-8
AMD,ny59r8z,Lol.,pcmasterrace,2026-01-07 04:50:33,1
AMD,ny3rspv,"Still not worth it as he would have to buy a new mobo, too. And then all the hassle of selling his old stuff. Just rock it until AM6 imo.",pcmasterrace,2026-01-06 23:50:16,2
AMD,ny2wxcl,12th-14th gen can use either DDR4 or 5 but not both. But what he was worried about whether OP has DDR5 RAM already since switching to AM5 from Intel 13th would be very expensive if OP doesn't already have DDR5.,pcmasterrace,2026-01-06 21:20:06,5
AMD,ny3yazv,"Yeah, Intel 12th, 13th, and 14th gen can be either ddr4 or ddr5",pcmasterrace,2026-01-07 00:23:45,1
AMD,ny3w5l6,"Firstly I said if he plays those games a lot, for example if I play Tarkov 90% of the time then 9800x3d is a no brainer.  Secondly I said massive gains not “few %”.  Maybe if you’d comprehend that massive != “few %”, and that I wrote “if he plays those games a lot” you’d understand my point. But you clearly lack that comprehension.",pcmasterrace,2026-01-07 00:12:42,0
AMD,nxypnuy,"they arent the same price on the links you sent. the rampage is 300 more. at the same price I would suggest the 9070 xt build. However, if they are 300 apart I would suggest the lenovo 5070.",pcmasterrace,2026-01-06 06:21:05,1
AMD,ny313vc,I have codes,pcmasterrace,2026-01-06 21:39:17,1
AMD,ny1a7n1,intel basically peaked with sandy and ivy bridge then made 30 more versions of it,pcmasterrace,2026-01-06 16:53:32,12
AMD,ny1iexz,You and me both. Went from i5-750>i5-6600k>i9-9900k>9800x3d,pcmasterrace,2026-01-06 17:30:50,4
AMD,ny1cvuk,"I loved my 3770k. Delidded it with liquid metal mod, added blistering fast 2400MHz DDR3 Ram to it, overclocked the CPU to 4.5 GHz, added a bios mod to support nvme boot (2.0 x4 to m.2 Adapter)  I did literally everything the Z77 Pro3 Motherboard was capable of. But still - it was time to retire that system.  I would say that almost nobody thought that AMD will be THAT BIG. And we all love it to see Intels Ass kicked (just a bit)",pcmasterrace,2026-01-06 17:05:38,1
AMD,ny1dgsa,"Its my next upgrade to AMD, running 13700kf at the moment so i'm good for a while yet. no problems with the CPU so no need to change it.",pcmasterrace,2026-01-06 17:08:19,1
AMD,ny1h1jn,One. Of. Us... One. Of. Us...  haha Nice build!,pcmasterrace,2026-01-06 17:24:35,1
AMD,ny2k23t,Nice upgrades,pcmasterrace,2026-01-06 20:20:56,1
AMD,ny3bqbc,Wish you the best. Hope you dont get usb 2.0 errors or blackscreens with iGPU. Ist my first time amd 9000x3d with asus x870e-e too 😄,pcmasterrace,2026-01-06 22:29:00,1
AMD,ny3man5,I think they might finally start shipping new 18a nodes this year. which means they finally should be comparable to tsmc again. But I will believe it when I see it,pcmasterrace,2026-01-06 23:21:27,1
AMD,nxzld0p,"Is it cheaper? The 7900xtx is a beast with allat vram but if it’s the same price, it depends on what kind of games you play, if you want the best frame gen you should probably go with the 9070xt but for raw power the 7900xtx is the way to go",pcmasterrace,2026-01-06 11:10:33,1
AMD,nxzltqh,Kinda the same price  The build I’m goin for is around 2644€   Comparing with the used one  The only difference my planned build has is the 9070xt and 1 tb ssd   And the games I play…. Depends…. I mean.. I’ve never used frame generation ever..  but I do tend to use dlss on my current laptop on every game if there’s an option.   Oh btw for got to mention  He’s asking for an exchange with my laptop.   It’s a legion pro 7i gen 9 i9 14900hx Rtx 4090  32 RAM 2 tb nvme,pcmasterrace,2026-01-06 11:14:33,1
AMD,nxzn9us,"Fsr is like dlss but it’s monitor side and works on every game, it’s a cool option but my gpu is a lil too old for it to work flawlessly, 6950xt",pcmasterrace,2026-01-06 11:26:36,0
AMD,ny36fbc,"that.. isn't true at all.. just google this ""is FSR like DLSS"" . Read up on things before saying nonsense like this",pcmasterrace,2026-01-06 22:03:45,1
AMD,ny3c6f1,"I said the wrong thing, I was talking about RSR",pcmasterrace,2026-01-06 22:31:09,1
AMD,nxzp0tq,"Any dual tower, 2-fan air-cooler or 240mm aio or better will be perfectly fine.",pcmasterrace,2026-01-06 11:40:42,1
AMD,nxzp2q1,"Your current cooler will be fine. I have mine cooled by the Arctic Liquid Freezer III 360mm, and it runs perfectly well. A 420mm cooler would, I feel, be massive fucking overkill, not to mention you need to be sure you can even fit a 420mm rad in your case.  https://preview.redd.it/78za2bvnupbg1.jpeg?width=3072&format=pjpg&auto=webp&s=6319c16ef0f3053c59c0c08efddcbe46bce2bcb5",pcmasterrace,2026-01-06 11:41:07,1
AMD,nxzuvs6,"Your h150i is still good. Unless it has pump issues or a leak, there is no reason to change it",pcmasterrace,2026-01-06 12:24:40,1
AMD,nxzx1t5,"A 420mm AIO is overkill even for the 9950X3D, even more so with a 9800X3D. But if what you’re after is a quiet build, then it’s not a bad choice (just make sure it will fit in your case tho, not all cases support a 420 mm AIO, and Arctic’s has a thicker radiator as well). However, your current AIO will get the job done. If all you want is something that gets the job done, keep your current AIO (unless it’s developing problems). Only get a 420mm AIO if you’re specifically after a quiet build.",pcmasterrace,2026-01-06 12:39:43,1
AMD,ny0hseb,"I have one of those on 9800x3d...with some tweaks on bios for cpu,on a be.quiet case with 7 fans,in idle temps on cpu are 38-42 celsius at room temperature...in games rises to 50 max,mayne 60 on  building shaders,etc for a few minutes...but that cooler is a beast.Go for it",pcmasterrace,2026-01-06 14:39:20,1
AMD,nxtc7uq,May I recommend any cooler that has 2 fans? These CPUs are going to get hot no matter what so you want to give a bit more juice to the cooling.   I had a Noctua NH-U12A when I first bought my 7700x but I have since upgraded to the DeepCool Assassin IV (which is definitely over $100) because it's so damn pretty. Both have been perfectly fine for my cooling needs.,pcmasterrace,2026-01-05 13:36:07,5
AMD,nxtecrl,Peerless assassin 120. It will be fine for the CPU until you start using it at 85% and above constantly then it will struggle. But for gaming and general tasks this should be plenty. You can also go for something from noctua or arctic or some brand that is about the same size as what I just recommended. Or get any 240mm or 360mm AIO.,pcmasterrace,2026-01-05 13:48:39,5
AMD,nxtaikw,"Probably a Cooler Master or something, but I wouldn't try to get the cheapest cooler when it can potentially ruin your CPU and destroy your computer. Try to find a Noctua or something good on sale.",pcmasterrace,2026-01-05 13:25:56,1
AMD,nxta9wa,"Go with basically any 360mm / 240mm AIO, you'll be fine.     Also, when exactly did Intel lie to their costumers?",pcmasterrace,2026-01-05 13:24:27,0
AMD,nxtuk0o,Do you think a double fan air cooler will fit in my case ? I don’t mind buying another case but I need to plan !  But no I am not opposed to a dual fan cooler. I think they look cool actually !   I have a Corsair 275r. Do you think PCpartpicker will give me an accurate estimate if it will fit ?,pcmasterrace,2026-01-05 15:16:23,1
AMD,nxturue,Huh so you think the Peerless will only be good for up to 85% usage ? Should I get something better ? I’m not opposed to spending a little more I just want to stick with air cooling,pcmasterrace,2026-01-05 15:17:28,1
AMD,nxtggwb,The faulty i9s a couple of years ago. They knew about them and did not tell anyone until issues started happening.,pcmasterrace,2026-01-05 14:00:46,2
AMD,nxtwk5m,PC part picker might but it is up to you to research as thoroughly as possible. It's possible to look up the dimensions of all the hardware! Oftentimes in case manuals there is info about what size hardware is available to you.,pcmasterrace,2026-01-05 15:26:06,2
AMD,nxv0lbb,Well that is only what I think. But peerless assassin 120 is rated for CPU up to 245w so there shouldn't be a problem but that is ideal conditions 245w testing. You might experience thermal throttle if you use it at 100% for a bit at start and for a bit after some time when the cooler warms up. But I doubt you will ever be able to use this CPU at 100% I just said 85% as a bit safer spot as I can't know for sure. I had core ultra 265k with this cooler and it handled the CPU fine with 80°C max when compiling game shaders.,pcmasterrace,2026-01-05 18:30:57,2
AMD,nxvb48y,"I have my 9800x3D on air using a Phantom Spirit Evo (also from Thermalright). IIRC when I was researching it, most testing find it out performs the Peerless Assassin, but is a hair more expensive.  Either choice would probably be fine for you though.",pcmasterrace,2026-01-05 19:18:03,1
AMD,nxu6gp8,Ty I’ll look into it !,pcmasterrace,2026-01-05 16:12:48,1
AMD,nwy8zqz,what is your wattage when you're running like this at peak,pcmasterrace,2025-12-31 18:02:30,46
AMD,nwy575j,"What kind of performance increase do you see? I was told that when using a secondary GPU for frame generation and Upscaling, you don’t get any of the artifacts or other performance issues normally tied to LS. Is that true?",pcmasterrace,2025-12-31 17:44:15,34
AMD,nwy51hx,How many games with no FSR support is 9070 XT struggling with?,pcmasterrace,2025-12-31 17:43:30,7
AMD,nx0blzd,"If I see a Fractal case, I upvote 👍",pcmasterrace,2026-01-01 00:57:49,4
AMD,nwz2llj,"Congrats on the build, this looks amazing.  I have heard about Lossless Scaling but I didn't know you could do it with dual GPUs like this. I have recently built a new PC myself (got lucky with the RAM prices in Brazil and bought it two weeks before the prices exploded here too) and I'm running an XFX Mercury 9070XT.  While it would be prohibitively expensive to do something like this now, you've given me an idea of what to do 5 or so years down the line when it might be worth buying a new GPU and using my current one as the extra.  Edit: typos",pcmasterrace,2025-12-31 20:36:37,3
AMD,nx0psu7,What's the game in picture 4? Thanks.,pcmasterrace,2026-01-01 02:30:08,3
AMD,nwzcha2,my PC does not have close to enough space for two gpus (at least ones I would use for this kind of thing),pcmasterrace,2025-12-31 21:30:13,2
AMD,nwzdlb6,Whats the performance difference compared to just running everything on 9070xt including loseless scaling?,pcmasterrace,2025-12-31 21:36:21,2
AMD,nwzqgw6,I'm debating if it's worth getting a rx 6400 low profile to upscale/framegen so I can run games easier with my 3080ti currently running a 4k oled 165hz monitor.   I can't go with a better gpu than a rx 6400 low profile because there's not enough space between my watercooled 3080ti and my bottom 360 rad + fans.   Would it be worth it op ?   (I don't want to spend 1000€ on a new 5070ti/5080 with just a 20/30% raw performance boost + getting a waterblock for it),pcmasterrace,2025-12-31 22:49:09,2
AMD,nx3btri,inb4 future dual gpu cards that handle this inherently so you dont have to fiddle with settings,pcmasterrace,2026-01-01 15:41:07,2
AMD,nx7hm3f,Hey OP. Are those extensions in your GPU 8pin slots and mobo 24pin? Or did your psu come with those? They look awesome,pcmasterrace,2026-01-02 05:37:34,2
AMD,nwy7h4t,"I'm going to do this too, but with a 6600XT and a R9 390 that I got off marketplace.",pcmasterrace,2025-12-31 17:55:10,2
AMD,nwy3fur,Whats the purpose?,pcmasterrace,2025-12-31 17:35:37,3
AMD,nwyb0p4,"Dear OP , I have a question as I've never tried LS, would you expect much of an improvement with a 9070 XT and a RX570? I have both cards and I just wanted to know if it's worth bothering?",pcmasterrace,2025-12-31 18:12:28,1
AMD,nwycb5e,Do they make PSUs big enough to handle two red devils? 🤔,pcmasterrace,2025-12-31 18:18:55,1
AMD,nwycoh6,Is there a tutorial you used to help you set this up? It seems really cool,pcmasterrace,2025-12-31 18:20:46,1
AMD,nwyi1m0,Do you think a 7900xtx with a 5700xt slave would be worth implementing? I saw the guide and I'm just not sure it would be worth it for my setup. I'm on the AM4 platform still with a 5800x CPU. I can outperform what my 2k 165 monitor can display most games so unless it can enable 4k scaled to 2k seamlessly without additional latency maybe. Has anyone tried this on a similar setup? Could you tell?,pcmasterrace,2025-12-31 18:48:04,1
AMD,nwysyku,Would this work with a Rx 6800 Xt paired with a Radeon Vega 64? The vega doesnt even have FSR.,pcmasterrace,2025-12-31 19:44:35,1
AMD,nx0jit0,So how does this work? Do you get more performance like with Crossfire?,pcmasterrace,2026-01-01 01:48:47,1
AMD,nx0r4qn,"Lmao, all this to try mimicking what a simple Ada GPU can do with MFG.",pcmasterrace,2026-01-01 02:39:12,0
AMD,nwzvi1t,This is so trash,pcmasterrace,2025-12-31 23:19:36,-3
AMD,nwypdlq,"I don't get it . Can't think a practical scenario using LS on second gpu , especially when old titles are super light, games with dlss2+/fsr2+ can use optiFG (optiscaler using the upscale inputs for FG) and modern games have FG baked in ....   maybe on laptops with weak gpu and LS using the igpu ....",pcmasterrace,2025-12-31 19:25:38,0
AMD,nwyyi5f,"I guess it's 300W + 150W + 100W, add 50W and it would be around 600W total consumption, on regular use. I took a 1000W PSU cause I didn't trust my 6 year old 850W to hold the extra power there.",pcmasterrace,2025-12-31 20:14:15,21
AMD,nwy6oxq,"True, ""performance"" it's the same if you meet the PCI bandwith requirements on both cards, but the perceived fluidity it's huge.  The X2 scaling on lossless works incredibly well, even maybe X3, everything starts to show heavy artifacting after that.  The best thing is that you allow to juice all the 9070XT power, while the 6600XT deals with scaling and framegen, and even with windows and extra app graphic usage (like recording or streaming).  The only issue is increased latency, but let me tell you it's almost non-perciabable, compared to higher latency provoqued by AMD or NVIDIA standard single card frame gen.",pcmasterrace,2025-12-31 17:51:29,44
AMD,nwy5upt,I came here to ask this.,pcmasterrace,2025-12-31 17:47:26,1
AMD,nwybvem,"It's quite noticeable, actually. If your CPU comes with an integrated graphics unit (iGPU/APU), it also works great.",pcmasterrace,2025-12-31 18:16:42,-1
AMD,nwy5pp9,"None, but I prefer pure power with FSR set, only, Native full quality or completely disabled.  Still, I want to run later a 4K 144hz TV setup, and there I will need to mix both. Still, LossLess allows me to run FSR on the second GPU too.",pcmasterrace,2025-12-31 17:46:46,10
AMD,nwyde0h,Helldivers 2,pcmasterrace,2025-12-31 18:24:21,1
AMD,nx0pxd6,Lords of the Fallen. You welcome!,pcmasterrace,2026-01-01 02:31:00,3
AMD,nwze4e3,"Real performance it's the same, 9070XT runs well on PCI 5.0 x8. Lossless-wise you are able to run X2 flawlessly, with reduced lag and not losing 10% performance, generating 240fps from 120-190 (depending on games). Also the 9800x3D helps a lot preventing any kind of % lows.",pcmasterrace,2025-12-31 21:39:13,1
AMD,nwzqtfg,"No it won't be, 4k 165hz it's very demanding, for example, my 6600XT would be right on its limit on 4k 120hz, and right now it goes 70-80% on 2k 240hz HDR. Try looking for a 9060 or an Intel arc.  Still, you can try.",pcmasterrace,2025-12-31 22:51:09,2
AMD,nx7tbpt,"Thanks. They are 180° adapters, it helps a lot to deal with double cards and cable management. The Mobo also has one of 90°",pcmasterrace,2026-01-02 07:14:51,1
AMD,nwy7rxj,"Nice! Just beware the PCI bandwith, but depending on your desired frames it should work fine for fullhd-2k 144hz (applying fsr for 2k).",pcmasterrace,2025-12-31 17:56:37,0
AMD,nwy50nm,"Well, the purpose of the dual GPU it's to let the second GPU deal with the frame generation, reduces latency by quite compared to AMD or Nvidia frame gen solutions. There's no way an 9070XT can go at Max settings 2K beyond the 100-180fps on demanding titles, so the secondary GPU interpolates generated frames to ""fill"" the video output and show fake 240fps.  Because lossless frametime it's stick to original frames, but if 9070XT frametime it's already good (100-180fps, for example), the difference between the 240hz perceived video signal and the already good frametime it's almost unnoticeable.  Also, it allows me to run LLMs better once a change to a stronger secondary GPU, and, if I needed more GPU strength on professional applications, use double GPUs without buying a 5090 for the price of this whole PC.",pcmasterrace,2025-12-31 17:43:24,5
AMD,nwy43ki,"Reading can be difficult, but it’s possible.",pcmasterrace,2025-12-31 17:38:53,13
AMD,nwyblk7,"Depends on your final framerate goal. It should work for FullHD and FSR 2K, I can´t tell you the max framerate, but there´s more qualified people than me on the lossless scaling subreddit with several bechmarks and secondary gpu´s lists.",pcmasterrace,2025-12-31 18:15:20,2
AMD,nwydk9w,"Sure they can, I believe that something like a good 1000W or an standard 1200W PSU should handle both GPUs at max power limit and OC.",pcmasterrace,2025-12-31 18:25:13,3
AMD,nwydhzi,"Get 2 GPUs, get the app, run app and configure.",pcmasterrace,2025-12-31 18:24:54,3
AMD,nwyti3o,"It should work, lossless has FSR software integrated.",pcmasterrace,2025-12-31 19:47:26,2
AMD,nx0c4jw,have the same setup its great up to 1440 p 240 hz but you cant go higher than that because of the pcie 3.0 x8 bandwidth and the displayport of the vega.,pcmasterrace,2026-01-01 01:01:05,1
AMD,nwzu0ew,u/bot-sleuth-bot,pcmasterrace,2025-12-31 23:10:35,10
AMD,nx0opx9,Ur not OP blud,pcmasterrace,2026-01-01 02:22:50,5
AMD,nwyfhmb,"Non-perciabable, good to know",pcmasterrace,2025-12-31 18:35:04,19
AMD,nwz4qr1,Gamers nexus did a deep dive video on it recently. It also studied the latency aspect if you're interested,pcmasterrace,2025-12-31 20:48:27,4
AMD,nwyhd8f,Thank you for letting us know you came here to ask this,pcmasterrace,2025-12-31 18:44:39,7
AMD,nwyd20y,"That will depend on the iGPU, tried on the 9800x3D iGPU and works terribly. Maybe an AMD laptop with a good Vega Apu might squeeze well. But can't assure it.",pcmasterrace,2025-12-31 18:22:40,7
AMD,nwz2m26,Most igpus are not powerful enough to run LS. Only top end apus like a 890m will do ok,pcmasterrace,2025-12-31 20:36:41,2
AMD,nwz4nnl,"I used it on my ally X, and was not a fan of",pcmasterrace,2025-12-31 20:47:59,1
AMD,nx0kj8e,Maybe at 1080p30 lol,pcmasterrace,2026-01-01 01:55:19,1
AMD,nwzrjhq,Thanks for the heads up ! Issue is I can't find an intel arc gpu or an nvidia gpu powerful enough and as low profile as the rx 6400... Well I guess I'll stick to play games on 1440p on my 4k monitor for the time being 😅,pcmasterrace,2025-12-31 22:55:24,2
AMD,nwyibqj,how to check the pci bandwidth? what should i check before doing this,pcmasterrace,2025-12-31 18:49:30,1
AMD,nwyl1n8,"Nah, 1080p 180hz",pcmasterrace,2025-12-31 19:03:13,0
AMD,nwy6yle,How do you set that up software-wise?,pcmasterrace,2025-12-31 17:52:44,3
AMD,nwylwxx,"The tests I saw with frame gen using LLS and A 2 GPU combo made the timings worse, not better.",pcmasterrace,2025-12-31 19:07:38,2
AMD,nwyfqav,"Kinda silly to downvote the guy. There's a ton of tech jargon in the description and if you don't already know what it is, it can be very difficult to parse whats actually going on.",pcmasterrace,2025-12-31 18:36:19,8
AMD,nx3nrop,"Its ok i wasn't familiar with LS, may use it one day when i go to 4k,  letting 6900xt scale my 1440 9070xt",pcmasterrace,2026-01-01 16:45:22,1
AMD,nwyehju,"Cool, may have to look into that a bit more, cheers dude.",pcmasterrace,2025-12-31 18:29:53,2
AMD,nwyh9i7,It would be my luck that both would surge at the same time lol,pcmasterrace,2025-12-31 18:44:08,1
AMD,nwytv7p,Would i get that much gain though? Vega i believe is an 8gb card. Although it was fast for it's day.,pcmasterrace,2025-12-31 19:49:22,1
AMD,nx0enj6,For real??? You're running the vega and the rx 6800x??,pcmasterrace,2026-01-01 01:17:20,1
AMD,nwyg3bn,It don' even perciate!,pcmasterrace,2025-12-31 18:38:11,6
AMD,nwzpco4,I'd watch that. This thread is first time I've heard of this,pcmasterrace,2025-12-31 22:42:48,2
AMD,nwylaaj,Thank you for thanking him!,pcmasterrace,2025-12-31 19:04:26,7
AMD,nx0cf8p,You're welcome.,pcmasterrace,2026-01-01 01:02:58,1
AMD,nwyygd4,I have a laptop with an hx 370 and Rtx 4050 and the Radeon 890m is GREAT with Lossless Scaling.,pcmasterrace,2025-12-31 20:13:59,2
AMD,nxmr79j,"AllyX only have an iGPU. Means you sacrifice main GPU power to run LS. People here talking about running scaling on iGPU, while game runs on main GPU. Dunno to be honest, sounds like it should still be better, maybe it wouldn't be able to push above 60fps, but iGPU should be enough to x2 if you have 30 fps. What will feel terrible tho.",pcmasterrace,2026-01-04 14:46:14,1
AMD,nx004mv,"There are some LP 5050s, they are rather expensive for what they are though. You could also get a PCIe riser and just put the GPU anywhere inside or outside of your case.  Here's a spreadsheet with some real and estimated performance data if you want to look into it further, in case you didn't already know about it: https://docs.google.com/spreadsheets/u/1/d/17MIWgCOcvIbezflIzTVX0yfMiPA_nQtHroeXB1eXEfI/htmlview#gid=1980287470",pcmasterrace,2025-12-31 23:47:32,3
AMD,nwykwrp,Your motherboard should list the bandwidth of all PCIe slots in the specs.,pcmasterrace,2025-12-31 19:02:31,2
AMD,nwyl3gf,"Completely pointless, I know",pcmasterrace,2025-12-31 19:03:28,2
AMD,nwy7ddh,"First enable PCI bifurcation on your Mobo if you have that technology on your motherboard, connect HDMI/DP to secondary gpu, set primary GPU to render 3D apps on windows and start lossless scaling app while playing.",pcmasterrace,2025-12-31 17:54:41,3
AMD,nwymg9h,"Timings will be worse than no framegen at all, but better than any other type of single card framegen.",pcmasterrace,2025-12-31 19:10:22,1
AMD,nx11k70,https://preview.redd.it/mpo8z2wrtnag1.jpeg?width=4096&format=pjpg&auto=webp&s=bb4edc812bf04eca76be598a598e022006eda672,pcmasterrace,2026-01-01 03:49:50,1
AMD,nwz28lj,Can u get preciate by throw gpu at birthcontrol?,pcmasterrace,2025-12-31 20:34:37,3
AMD,nx0fb19,"I could buy a lp 5050, found it ""cheap"" enough but my motherboard only does 8x and 4x when using dual gpus (both connectors are pcie gen 5 tho).   Would it be a huge bottleneck ?",pcmasterrace,2026-01-01 01:21:33,1
AMD,nwym9ww,"Well, if you dont have to buy a new mobo, I would definetly try at least, the good think of the lossless app its that you can toogle a lot of stuff to lower the framegen stress on the secondary GPU and it may work with the 390. But I would consider to use at least a 580.",pcmasterrace,2025-12-31 19:09:29,1
AMD,nx7ydqz,"No, but you can get preganté",pcmasterrace,2026-01-02 08:01:53,1
AMD,nwz0kf4,"Wym, on PCI Gen5 motherboards the PCI lanes share the Gen5 lanes through the CPU. Other motherboards don't have bifurcation cause they don't have more than one PCI x16 that goes 5.0, usually PCI x16 X4 if your lucky, and goes through the mobo controller.  Modern motherboards not dedicated to AI take the PCI Gen5 lanes, besides one single slot for a GPU, to the M.2 lanes.",pcmasterrace,2025-12-31 20:25:25,-1
AMD,nwzx5mh,"The Gigabyte B850 AI TOP, which OP uses in his PC, can run two PCIe slots in x8 mode or one in x16. That is bifurcation.",pcmasterrace,2025-12-31 23:29:42,0
AMD,nx06ta9,"'PCIe Bifurcation is, as the name suggests, the halving of available lanes. This is very typical on consumer boards by taking one GPU and one slot at x16 and allowing for two slots and GPUs with each running at x8.'  https://sabrent.com/blogs/storage/pcie-bifurcation-lanes  Both what you mentioned and what is being done on this motherboard would be considered bifurcation. Just think about it, the second slot on that board cannot function without taking lanes from the first slot, you have to bifurcate to make use of it. The fact that the second slot is present on the same pcb and this process doesn't require additional hardware doesn't change that.",pcmasterrace,2026-01-01 00:28:27,0
AMD,ny4f2p7,Pretty sweet build,pcmasterrace,2026-01-07 01:53:22,2
AMD,ny4kbpr,"Nice build. Is there a more recent gpu option for a similar price? The 6950xt was very fast before. It is still a good card, but modern games have different demands.",pcmasterrace,2026-01-07 02:21:40,1
AMD,ny4nhmp,I mean ive seen many videos of it running modern fps at 240,pcmasterrace,2026-01-07 02:38:51,1
AMD,ny4nqvi,"[Holy shit](https://tpucdn.com/review/sapphire-radeon-rx-9060-xt-pulse-oc/images/doom-eternal-1920-1080.png) I did not think the 6900XT was that fast! God damn, gotta down vote myself.",pcmasterrace,2026-01-07 02:40:14,1
AMD,ny0pv58,"Those kind of voltages should split the atom, you aint got no PC its a nuclear bomb",pcmasterrace,2026-01-06 15:19:47,9
AMD,ny0reox,"nah, that's underclocking and overvolting by just a tiny bit",pcmasterrace,2026-01-06 15:27:11,6
AMD,ny1390k,"If you were able to apply that voltage your cpu would have a frequency of 0mhz so, sure",pcmasterrace,2026-01-06 16:21:54,1
AMD,ny1q3tq,FUCK no lol      leave it stock dude its a gaming cpu but if you really want to mess with it then look up how to enable PBO in the bios.,pcmasterrace,2026-01-06 18:05:19,1
AMD,ny2bj5y,Casually putting almost 40 ronnavolts through your cpu huh?,pcmasterrace,2026-01-06 19:41:36,1
AMD,ny2en10,"Ah yes, nuclear fusion overclocking",pcmasterrace,2026-01-06 19:55:48,1
AMD,ny0w63a,Launch codes incoming….😂,pcmasterrace,2026-01-06 15:49:28,1
AMD,ny2q3y7,At least some code will be launching…,pcmasterrace,2026-01-06 20:49:05,1
AMD,nxxa4mv,Should fit a good hole in the line up,AMD,2026-01-06 01:05:16,48
AMD,nxxufn2,I’d love a max+ 388 in an itx motherboard to make a killer 1080p steam machine. The 395’s cpu is overkill but an 8 core cpu + 8060s + 32gb ram would be awesome. And even better if amd decided to release fsr4 int8.,AMD,2026-01-06 02:55:19,32
AMD,nxx8w1k,FSR4n't.,AMD,2026-01-06 00:58:39,121
AMD,nxx8hg6,Should've had these from the jump.  The prices are gonna suck...,AMD,2026-01-06 00:56:27,30
AMD,nxxfv3c,I like how they didn't announce a single product using these.,AMD,2026-01-06 01:36:07,23
AMD,nxy5bzs,"at this point idc any innovations (if there's any to begin with) if I can't afford it, are people that rich nowadays that nobody is hyped for mid-range stuff? Not saying I like mediocre performance, but I do like seeing a good price to performance kind of product",AMD,2026-01-06 03:58:01,7
AMD,nxxapco,It's going to be embarrassing when fsr5 isn't backwards compatible with rdna 4,AMD,2026-01-06 01:08:22,18
AMD,nxzd1gc,"Nice to see that they are real but there's still only 2 laptops available after a year, and no 16 inch option.  Lenovo has still yet to have their event so it's still early to say there aren't any new strix halo laptops coming but it's not great so far. Personally I'm just after an efficient lightweight 16 inch laptop with more than 8gb vram and that's ideally not over £2500.  Ryzen AI 388 + 32 or 64GB ram would be ideal, at least if it ends up being expensive, I don't have to worry about running out of vram.",AMD,2026-01-06 09:56:21,5
AMD,nxybevt,I like the 388+.  Am I crazy for wanting this on a handheld? Basically a cheaper GPD Win 5.,AMD,2026-01-06 04:36:29,8
AMD,nxzk04g,8 cores is all u need. actually why not offer 85c cores only as the c cores would still be way more than enough for such a gpu with all the powerlimits/thermal limits.,AMD,2026-01-06 10:58:51,2
AMD,nxy4rx4,More RDNA3 crap.  AMD is Radeon's worst enemy.,AMD,2026-01-06 03:54:39,6
AMD,nxxofr1,"All I want from AMD is a balanced APU, a solid CPU and a solid GPU. I genuinely don’t understand this weird obsession companies have with slapping a midrange GPU next to an absurdly overkill CPU. Why can’t we get a Ryzen 5 or Ryzen 7 level CPU paired with something like an 8060S?   Instead, it’s always a top of the lone Ryzen 9 or Intel i9 equivalent paired with what’s basically a midrange GPU. All that does is jack up the cost for no real benefit and push the final price way higher than it needs to be. A 12 core CPU driving a 60 TFLOP GPU is still massive overkill, and it just feels like wasted silicon and money to me.",AMD,2026-01-06 02:22:26,4
AMD,nxzd536,"iGPUs are becoming an exciting field. The Intel ARC B390 is showing itself to be a bloody awesome little iGPU & if this progress continues, I wager my next PC will probably just be a mini PC with an iGPU.",AMD,2026-01-06 09:57:17,2
AMD,nxy6c9h,Hopefully a laptop drops with one of these below or around 2k so I can get one device for gaming and productivity on the go and at home,AMD,2026-01-06 04:04:06,1
AMD,ny0w38e,This is the only thing i care about the 388 Strix Halo and in a Mini ITX motherboard or smaller i can put in my own cases.  When does this release?,AMD,2026-01-06 15:49:06,1
AMD,ny3awz1,"Now put it in in a proper laptop with a 17"" screen and a full keyboard (i.e. with numeric keypad and no annoying multi-function limitations).  Strix Halo has been so badly wasted.",AMD,2026-01-06 22:25:06,1
AMD,ny49b5d,"Paper launch without any brand got it, waiting for a tablet 12-14"", 388, 64GB",AMD,2026-01-07 01:21:54,1
AMD,nxyblhe,I like the 388.  Am I crazy for wanting this on a handheld? Basically a cheaper GPD Win 5.,AMD,2026-01-06 04:37:43,1
AMD,ny08ms5,"This would be interesting to see on a handheld. Considering that we're also seeing the new X Intel CPU on handhelds, I wonder if AMD can beat it in price/performance since it's already faster",AMD,2026-01-06 13:49:35,0
AMD,nxxlts8,So a hx 370 upgrade with 8060 the graphics. Okay how much?,AMD,2026-01-06 02:08:16,-1
AMD,nxxetpu,"the 395's 16 core CPU is just absurdly overkill for most people, the 388/392's 8/12 core CPUs are much more sane (and definitely cheaper)",AMD,2026-01-06 01:30:33,40
AMD,nxyk7sb,If you're planning on steam os you'll have fsr4 through decky.,AMD,2026-01-06 05:38:13,11
AMD,nxzdx3r,IMHO it won't be cheap enough over CPU + dGPU combo - and it's system you can upgrade.,AMD,2026-01-06 10:04:35,3
AMD,ny0yxwt,This 100%.  This little motherboard could open so many fun opportunities for mods into all sorts of pc cases and broken console cases maybe custom 8bit style keyboard pcs.    Any word on release date?  I currently have the Jonsbo NV10 with a low profile 5060 at my desk.  But I want the 388 Strix Halo for the living room.  I wonder how it compares to the Steam Machine which is what I have been eyeing for the living room.,AMD,2026-01-06 16:02:05,1
AMD,ny2jzde,"In my experience, the 385 already fits the bill. I got the Framework Desktop mobo, shopped around for deals on the other components, managed to get a sub-4L build for about $1000. Installed Bazzite, logged in to Steam, and was gaming within minutes.  The only lingering flaw - and granted, it IS a pretty major flaw at the moment considering the use case - is that Sleep doesn't work properly and seems to cut power to both USB and the network card even after waking. I'm confident I'll resolve whatever the issue is, but until then, being sure to power down after every session isn't a very difficult thing to do, especially since the only thing I'm using it for is gaming.",AMD,2026-01-06 20:20:35,1
AMD,ny1yzpm,"I use it on my GPD Win5 anyway. Still the best upscaling (1080p, performance mode)",AMD,2026-01-06 18:44:55,2
AMD,nxxgqzr,"AMD's CES stream is still an hour away, they'd prolly save product announcements for the stream",AMD,2026-01-06 01:40:53,11
AMD,nxxkrqf,"Strix Halo is ridiculously expensive and needs super fast RAM, I don't expect to see any remotely affordable products with that any time soon.",AMD,2026-01-06 02:02:34,11
AMD,nxz1rei,"And then Asus / Lenovo are gonna shadowdrop a midrange model or two, with Lenovo being months behind Asus. Typical AMD release basically.",AMD,2026-01-06 08:08:19,3
AMD,nxzu291,The ASUS TUF 14 is going to have a version with the 392.,AMD,2026-01-06 12:18:48,2
AMD,nxzv50y,the [Asus TUF A14 is actually getting the Ryzen 392](https://youtu.be/h27w0PXFBgk?si=C15cREUl4y4NaBYp&t=307)! the literal midrange gaming laptop  https://i.redd.it/2nywodrp0qbg1.gif,AMD,2026-01-06 12:26:30,4
AMD,nxyxyrp,"these kind of are mid-range, even low end, but pricing probably won't be",AMD,2026-01-06 07:33:16,1
AMD,nxzavfw,"This should bring down prices to sane levels. Only enthusiasts were getting 8060s for gaming when it was paired with 395. With these, much more saner options, I’d expect laptops and handhelds around 1k, although ddr5 price will mess this up.",AMD,2026-01-06 09:35:52,1
AMD,nxxgqef,People should call AMD out more for this behaviour. At least Nvidia backports DLSS back to 20 series.,AMD,2026-01-06 01:40:48,24
AMD,nxzfe3u,the [Asus TUF A14 is actually getting the Ryzen 392](https://youtu.be/h27w0PXFBgk?si=C15cREUl4y4NaBYp&t=307)! the TUF A14 series has never went over £2000  https://i.redd.it/pnrflrgzepbg1.gif,AMD,2026-01-06 10:17:57,6
AMD,nxzbz22,"Lol cheaper, if people actually want this i expect it to be priced the same or more honestly. AMD is Nvidia when it comes to iGPUs.",AMD,2026-01-06 09:46:20,4
AMD,ny08wml,"GPD win 5 already has Max 385 version, what do you actually mean by ""cheaper"" lmao.  GPD with Max+ 388 will be more expensive than their Max 385 version",AMD,2026-01-06 13:51:06,3
AMD,ny09e3d,My first thought reading about this too,AMD,2026-01-06 13:53:47,1
AMD,nxycku8,Wrong order lol.,AMD,2026-01-06 04:44:15,1
AMD,nxxvfd8,"I’m hoping the 8 core model is the sweet spot. The 395 is way overkill in the cpu department and ram department for the average user.   Not all of us want gobs of ultra fast ram to run hot dog/not hot dog. Some of use just want a decent apu for low power, quiet couch gaming.",AMD,2026-01-06 03:00:48,5
AMD,ny0jfbe,thats what the 388's gonna be   it's an 8 core CPU with 8060s,AMD,2026-01-06 14:47:50,1
AMD,nxz6kkm,kills low end gpus and cpu sales is a likely cause,AMD,2026-01-06 08:54:05,0
AMD,nxxze0o,"nah, this is a full fat desktop ccd, not strix point",AMD,2026-01-06 03:23:22,8
AMD,nxxt2tn,395 is around $1500-2000 so probably 1k plus since hx370 is very expensive still,AMD,2026-01-06 02:47:54,1
AMD,nxxf7c3,Agreed. Although I did buy a 395+ framework desktop 😂,AMD,2026-01-06 01:32:36,17
AMD,nxxlffp,395 isn't meant for most people.,AMD,2026-01-06 02:06:06,11
AMD,nxzz8d7,I was looking into this and there is a use case you may be missing. This will give you steam machine performance in a 2-3L PC case. Getting a dGPU into something like the MS-A2 (closest comparison I can come up with easily) requires jumping through a ton of hoops and will be more expensive besides.   This has a very niche use case compared to what you suggested and if money is a concern at all your solution is probably better.,AMD,2026-01-06 12:54:03,4
AMD,nxzq1lh,I guess we wait and see,AMD,2026-01-06 11:48:37,0
AMD,ny2p7nn,I've been using an 8700g for the past couple of years and it's fine for 1080p/low gaming but in all these SFF systems it's the GPU that's the bottleneck.  I have no doubt the 8050s is a great improvement on the 780m but I'd still rather have the 8060s,AMD,2026-01-06 20:44:59,2
AMD,nxxn5ta,Read the article. Amd didn't announce any device.  These are likely from the full press kit released ahead of the keynote.,AMD,2026-01-06 02:15:33,9
AMD,nxxlneo,“Needs super fast ram”  Why? Any ram should suffice or is this a downside of an APU?,AMD,2026-01-06 02:07:19,1
AMD,nxzbo78,"Seems like you are right, as I just heard from the Youtuber Crimsom Tech that Asus TUF A14 will have a variant with Strix Halo.",AMD,2026-01-06 09:43:28,2
AMD,ny0ttzf,OOOOh yeah baby. If we get 'full performance' over a USB Type C connection im in.,AMD,2026-01-06 15:38:40,2
AMD,ny09adv,"Honestly their most surprising announcement, and that's counting the Zephyrus Duo, the GoPro Proart PX13 and the Flow Z13 x Hideo Kojima colab",AMD,2026-01-06 13:53:14,1
AMD,nxxhz8b,they will just get DLSS 4.5 too now.,AMD,2026-01-06 01:47:30,10
AMD,nxy5c43,"People even excused AMD for *actual* fake pricing.  I remember launch day of RDNA4 people here and on r/radeon going to buy the card, but the subsidized stock was gone so they bought it for $800... when their whole reason for buying was the fake MSRP.",AMD,2026-01-06 03:58:02,10
AMD,nxxugid,Right but at one point they didnt back dlss to the 10 series. They can't. Amd also cant just slap the same fsr on rdna3 without drawbacks. They gotta make a version for rdna 3.5. Rdna3 should be getting one as well.,AMD,2026-01-06 02:55:27,4
AMD,nxzo2jg,"Thanks so much for sharing this, it completely flew under my radar, it's definitely one I'm going to be keeping my eye on as I liked the design on the 2025 model. I would ideally prefer 16 inch but I love how this laptop has 2x m.2 so it will make it really easy to just slot my old SSD in and go.  I'm not worried much about the price, I wouldn't mind going a bit over £2000 for a 64gb model, definitely one I'm going to keep an eye on.",AMD,2026-01-06 11:33:07,2
AMD,nxzkthf,They said 16 inches,AMD,2026-01-06 11:05:54,1
AMD,ny1rkdh,The 385 has a worse GPU than the 395. The 388 doesn’t.,AMD,2026-01-06 18:11:45,2
AMD,nxycs1l,Nah I phrased it deliberately.  ATi was a top tier graphics card company; AMD bought them and turned them to shit.,AMD,2026-01-06 04:45:36,5
AMD,ny0zo8s,The 385 with 32CUs and 8 Cores is $799 for just the ITX board + 32 GBs.    I hope this comes in at $999 or less.,AMD,2026-01-06 16:05:29,2
AMD,nxzsdza,"I was considering a framework desktop, how are you liking it?",AMD,2026-01-06 12:06:26,3
AMD,ny01npc,"Yes, if you are into very-SFF then this can do things other builds can't. Current Framework and Chinese Strix Halo 395 boards are priced at like $2700+ and with current DRAM shenanigans 388 with 32GB RAM may loose the price advantage it initially would have. Steam Machine will win this one by a big margin I guess.",AMD,2026-01-06 13:09:28,3
AMD,nxxphl5,I CAN HOLD OUT HOPE IF I WANT!,AMD,2026-01-06 02:28:09,6
AMD,nxy5k61,They only announced the AI MAX chips,AMD,2026-01-06 03:59:23,3
AMD,nxxm3gn,"Massive iGPU that shares bandwidth with the CPU. Both need a ton of bandwidth so if you just put ""any RAM"" you're gonna lose a ton of performance.",AMD,2026-01-06 02:09:43,15
AMD,nxxtumz,The major thing holding igpu’s back from better performance is ram speed - ddr5 just isn’t fast enough to work as vram. What’s unique about strix halo is that it uses soldered lpddr5x instead of ddr5 modules to improve gpu performance.,AMD,2026-01-06 02:52:06,6
AMD,nxxua01,Appreciate the teachings of my fellow redditors,AMD,2026-01-06 02:54:28,1
AMD,ny0unmi,"the 2026 TUF is claiming up to 95W maximum power usage, so even 100W chargers can give full performance",AMD,2026-01-06 15:42:30,2
AMD,nxzlpej,Remember Zen5% as well,AMD,2026-01-06 11:13:30,2
AMD,nxxvnpj,"That's what people are expecting.  Sure we all know that FP8 isn't on RDNA3... But there's other instruction sets that could work, there's Some AI hardware there to be used there at some level. No one is expecting to get FSR4 on an rx 580 or an RX 5700XT... maybe 6000 series, but 7000 series should've been guaranteed.  It's the LEAST they could do.",AMD,2026-01-06 03:02:06,6
AMD,nxzsp5u,"yea idk why not a single tech journalist or content creator noticed while they were on Asus's booth, literally only Crimson Tech noticed",AMD,2026-01-06 12:08:47,3
AMD,nxycxhf,Ohhhh fair enough. I thought you meant that Radeon are holding AMD back (which they are rn).,AMD,2026-01-06 04:46:37,2
AMD,nxz8xly,"they weren't top tier, they were also below nvidia most of the time. same as today really.",AMD,2026-01-06 09:17:04,2
AMD,ny0aol9,"Radeon's most successful period was under AMD from 2008-2013. They carried AMD for a lot of that time while their CPU division faltered.  Also ATI were in financial trouble when AMD bought them, so it was unlikely they could have continued without a buyout.",AMD,2026-01-06 14:00:57,1
AMD,nxzwjzh,"It's cool. Using it exclusively for llm models. It acts as the backend for OpenWeb uI. Since the nic is only 5Gb, I keep the models on the device. Eventually I'm going to put it in another case so I can use the x4 slot for a 25G card and centralize my model storage.  But performance is strong all things considered. It doesn't chug when trying to generate a result. I think the biggest delay is the time from when it loads the model requested into memory before it starts processing",AMD,2026-01-06 12:36:20,4
AMD,ny04gbm,"Looking through the CES news, I saw that AMD teased a reference dev box. They called it the Ryzen AI Halo. A AI Max+ 395 with 128GB. If there is even just a hint of them dropping this thing with a ""reasonable"" price tag in Germany, I'll just grab one of those.  My hopes for a decent sub-2L chassis with a 388 and 64GB of RAM feel rather slim. I rather avoid the possibility of filing a warranty claim against a Chinese brand.",AMD,2026-01-06 13:26:04,1
AMD,ny05p0g,"I’m still waiting to see what we actually get from the steam machine personally, We still need specs and price point. My current htpc gaming setup is 15L which is tiny considering what it’s packing (3900x+3090) but I wouldn’t be opposed to something in the USFF if it’s at a “reasonable” price and competes in the same ballpark.",AMD,2026-01-06 13:33:10,1
AMD,nxznsii,They also announced the 400 series with actual product pictures  https://www.servethehome.com/wp-content/uploads/2026/01/AMD-CES-2026-Ryzen-AI-400-Availability-800x450.png,AMD,2026-01-06 11:30:50,4
AMD,nxxmh9n,"Still learning this whole AI chip deal. The power efficiency of Core Ultra cpus interest me, but at what cost is always my first thought.",AMD,2026-01-06 02:11:48,1
AMD,nxy921b,It's really that they use 8 memory channels rather than the typical 2,AMD,2026-01-06 04:21:09,2
AMD,nxzn0yw,"The usage itself is not unique, the unique thing is that it's mandatory. You won't find Strix Halo with SO-DIMM memory because AMD doesn't allow it.",AMD,2026-01-06 11:24:36,0
AMD,nxxye8m,"There is INT8 and I used to work on there last I know is that them corporate heads dont want to backport to be released RDNA3/RDNA3.5. Their RDNA4 was stopgap and fooled consumers enough to buy them and they might not even know if RDNA4 will be even supported in the future, haha.",AMD,2026-01-06 03:17:39,6
AMD,nxyd39f,"Oh they def are, but when AMD bought ATi, AMD CPUs were crap while Radeon was still good.  They basically coasted on Radeon's residual quality until their CPUs got good then stopped caring.",AMD,2026-01-06 04:47:41,2
AMD,nxz9c8p,They’d lagged a bit but they still competed in the high end and in the early 2000s made halo products like the 9800 XT. Since their acquisition they’ve exclusively been “the cheaper alternative” and since Turing they’ve been generations behind.  The only new graphics tech I can think of from AMD era Radeon was the TressFX hair rendering (which basically only ran on Radeon cards).,AMD,2026-01-06 09:20:59,5
AMD,ny0865s,How easy for it to set up. What model+ config did you use if you don't mind me asking,AMD,2026-01-06 13:47:03,1
AMD,ny464pc,That ryzen ai halo box is for developers not general consumer,AMD,2026-01-07 01:04:38,1
AMD,nxyxijq,"it's a 256 bit interface instead of the usual 128 bit, ie. 8 32-bit channels vs the normal 4 (which are usually still referred to as dual channel since we are used to talking in 64-bit channels)",AMD,2026-01-06 07:29:07,6
AMD,nxzvtpk,Framework said they asked amd about upgradeable ram (sodium or lpcamm I dont recall exactly) and amd tried but couldn't get the signal integrity right without sacrificing bandwidth (and let's not kid ourselves those large igpus are dependent on the insane 256GB/s bandwidth that strix halo has) At least that is what I remember reading back when the framework desktop was announced,AMD,2026-01-06 12:31:15,3
AMD,ny0mzob,"Do you enjoy working there? You don't seem happy, while all of your posts about your competition seems positive. Really strange.",AMD,2026-01-06 15:05:45,1
AMD,nxzbxgb,what about the Mantle that changed the game for APIs?,AMD,2026-01-06 09:45:55,1
AMD,ny0vm8e,Using LM Studio on windows because I didn't feel like going through the linux config.   I have the following models:  glm-4.7 q2  mistralai small 2512  mistralai 3 14B  Llama 3.3 70b  granite 4h tiny  gemma 3 27b  qwen3 4b  gpt-oss 120b  qwen3 coder 30b  qwen3 4b thinking  deepseek 8b,AMD,2026-01-06 15:46:56,5
AMD,nxzwdwj,"Yeah, all correct, signal integrity when using DIMM memory is a bitch. Not to mention the complexity of the resulting board itself would be pain, you need 4 slots at least. Yikes.",AMD,2026-01-06 12:35:09,3
AMD,ny3em3j,Because the competition is launching more exciting products that I wish my company would launch. To me laptops are really important segment as I can use it to explain to people what I used to do at work easily but eith Strix Point/Halo being so damn expensive and uncompetitive it just brings me a tinge of sadness.,AMD,2026-01-06 22:42:54,1
AMD,ny3pvyi,That aside what happened to the rumored 9950X3D2? That did not materialize correct? Just the 9850X3D?,AMD,2026-01-06 23:40:14,41
AMD,ny3teoh,amd trying to not have a dogshit naming scheme challenge (impossible),AMD,2026-01-06 23:58:34,39
AMD,ny3gk6z,Another year another AMD Marketing fuck up,AMD,2026-01-06 22:52:29,46
AMD,ny3q9zd,So the G series APU's are dead?,AMD,2026-01-06 23:42:17,16
AMD,ny42lt1,"AI laptops... AI processors... urgh, make it end already. :(",AMD,2026-01-07 00:45:58,15
AMD,ny4mq5q,So it sounds like the 400G (formerly thought to be 9700G) is basically a refresh of the HX370 mobile chip with a little more than double the default TDP.  I wonder if that means an increased GPU frequency (both 8700G and HX370 are at 2900MHz).,AMD,2026-01-07 02:34:43,2
AMD,ny4abhg,Yokatta i didnt wait for it,AMD,2026-01-07 01:27:25,0
AMD,ny3xu0w,Probably canceled to two limited to no competition there's no need to release it.,AMD,2026-01-07 00:21:20,21
AMD,ny41jgi,According to computerbase.de AMD comfirmed it: https://www-computerbase-de.translate.goog/news/prozessoren/x3d2-bestaetigt-der-amd-ryzen-9-9950x3d2-mit-doppeltem-3d-v-cache-kommt.95665/?_x_tr_sl=de&_x_tr_tl=en&_x_tr_hl=de&_x_tr_pto=wapp,AMD,2026-01-07 00:40:26,4
AMD,ny3q2mg,Moore's Law is Dead channel lol,AMD,2026-01-06 23:41:12,11
AMD,ny443xp,"If I were to guess, why charge a lower amount now when AMD can charge a higher amount later. And probably, even the 7800x3d is faster than the competition in gaming so AMD just doesn't have competition when it comes to gaming.",AMD,2026-01-07 00:53:49,2
AMD,ny50gqt,They just didn't announce it.   They probably want a softer announcement of the 9950x3D2. Because it won't make games run faster.,AMD,2026-01-07 03:51:54,1
AMD,ny4ekl9,"It's basically the whole industry right now, intel 300 pantherlake series cpus are just as confusing as amd' cpus",AMD,2026-01-07 01:50:39,16
AMD,ny4xqyj,Is it? This is successor to the APU 300 series. 400 seems like a rather logical decision. This isn’t meant succeed the CPU 9000 series.,AMD,2026-01-07 03:35:51,3
AMD,ny3wd2j,not for them its not. now they can charge more,AMD,2026-01-07 00:13:47,-4
AMD,ny3st6q,"Naming scheme yes, CPUs probably not.   Remember, it's AMD. Their marketing people generally cannot allow themselves to have a consistent naming scheme for more than 2 generations. Plus there probably was an order from the top to integrate ""AI"" somehow so it's even more visible.",AMD,2026-01-06 23:55:30,26
AMD,ny3vv0t,"Aren't they just moving to most everything having an iGPU, like Intel?   That was kinda the whole reason for the G, I thought...",AMD,2026-01-07 00:11:12,4
AMD,ny4a1wi,Who's going to buy them when the RAM costs more than the CPU?,AMD,2026-01-07 01:25:59,12
AMD,ny4mt2i,Maybe it will be another Microcenter exclusive.  :-),AMD,2026-01-07 02:35:10,6
AMD,ny44u7y,This is highly likely. AMD simply is too good and they are waiting and delaying and milking current X3D options. The minute Intel has something remotely close you’ll see new releases.,AMD,2026-01-07 00:57:44,12
AMD,ny4w6o5,It's a bizarre decision to cancel something like that because of competition when there wasn't competition in the first place.,AMD,2026-01-07 03:26:53,2
AMD,ny3t8ks,"the existance has been known for awhile. private overclockers have been known to have had engineering samples of them i think reported on GN, as well as the cpu had actual benchmark leaks for an [ID existance](https://overclock3d.net/news/cpu_mainboard/amd-ryzen-9-9950x3d2-cpu-benchmark-results-leak/).  Not all products are released, but its hard to deny they didn't exist.",AMD,2026-01-06 23:57:42,18
AMD,ny4wgoq,"With how inaccessible RAM is becoming to consumers, I'd sooner believe it's canceled because it's a niche, expensive product that people are going to be priced out of. They can't sell it for more later if RAM keeps climbing to a point where no one considers buying DDR5-based PCs worth the cost.",AMD,2026-01-07 03:28:29,1
AMD,ny4ctrb,"Yeah end-users and consumers sure love... ""AI"" plastered all over everything. Especially when OpenAI and co. are why everything costs too damn much.",AMD,2026-01-07 01:41:10,8
AMD,ny3vtf6,"Job security for their marketing team lol, needless and confusing name scheme changes every so often 😅",AMD,2026-01-07 00:10:58,10
AMD,ny3wr9o,"The G naming scheme has existed since zen+, so I dunno wtf you’re yapping about. Ryzen AI has also been a thing for years now.",AMD,2026-01-07 00:15:49,4
AMD,ny4iozh,"G is supposed to be a much better iGPU, the main 7000 series desktop has 2CU RDNA2 while the 780M in the 8700G is 12CU at a higher boost clock. They're usually refered to as APUs rather than iGPUs.",AMD,2026-01-07 02:12:53,7
AMD,ny5coz1,"G = gaming, otherwise the non-g 7000 processors would also be G because they have graphics",AMD,2026-01-07 05:10:33,1
AMD,ny4ao9k,"Individuals no, but businesses on fixed upgrade cycles and people who desperately need one would just pay the price anyway.",AMD,2026-01-07 01:29:21,6
AMD,ny4qtia,"Microcenter my beloved, just picked up an open box 7800X3D from them today.",AMD,2026-01-07 02:56:54,2
AMD,ny43wyg,So many people don't realize this. Remember when the roadmap for an Nvidia board partner leaked shortly after the 30 series launch and the 20gb 3080 was on it? It never actually materialized but recently someone posted an engineering sample of it.,AMD,2026-01-07 00:52:48,4
AMD,ny4kcnm,"I bought 32 GB DDR4 ram in 9th of June last year for 101 e. Just checked the price of that exact same kit from where I bought it and it's now 303 e. Really drove home how insane the prices are getting.  Also looking at price charts, it seems my timing was pretty good, since price for DDR4 shot up in mid June and has kept rising since. So, I probably should also mention, I ordered a new GPU on Sunday.",AMD,2026-01-07 02:21:48,5
AMD,ny4uk73,"I got a 7600X3D the other week for my living room gaming box.  It will no doubt still be GPU limited, but it was pretty inexpensive for an AM5 chip.",AMD,2026-01-07 03:17:42,2
AMD,ny4m8hu,"Redid my comp just this past week (9800x3D, 32GB RAM, etc.) and helped some family on that front with redoing hardware or picking out new laptops. Prices are higher than I like but some of the builds were going on their 8-9th year of operation. Better to eat the cost of DDR5 now when it's at local stores for about 200-300, than be forced to eat it later in the year when stuffs more apocalyptic. The 5800x3D I swapped out in my build already sells for more than a new 9800x3D on ebay judging by listings.   Some friends are also swapping their GPUs while they are still MSRP.",AMD,2026-01-07 02:32:03,2
AMD,ny3n7mq,Intel moment.,AMD,2026-01-06 23:26:08,33
AMD,ny3ps9y,Wouldn't trust a CPU with AI in it's name,AMD,2026-01-06 23:39:42,41
AMD,ny4cu1g,"Me initially: Well the Ryzen AI 300 series is also stupid already, the AI 7 350 has 2/3 the GPU cores of the R7 7840U while the AI 5 340 has HALF the GPU of the R5 7640U, not to mention the AI 5 330 having only FOUR cores despite the ""5"" in there, what else could they possibly do?  Me after looking at the article: Wow, they're actually reinforcing my decision to go for Intel with my last laptop purchase last year, once again AMD never fails to disappoint, what the fuck is that AI 7 445 doing there with only 6 cores and HALF of the AI 7 350's already anemic GPU (relative to the R7 7840U)? Not even going to look at the other models, I'll probably pop a blood vessel somewhere if I do.",AMD,2026-01-07 01:41:12,7
AMD,ny4b4ga,Not sure how they deduced that there are fewer p-cores,AMD,2026-01-07 01:31:49,3
AMD,ny4yyre,"Looking at the actual content of the article, they're basing everything off of ghz specifications, and it's really only mid range and lower mobile cpus. This is a refresh cycle, not a new generation.  The higher end units have more graphics stuff and no lesser ghz... and there's no benchmark data. It's all assumptions based on ghz specifications only.",AMD,2026-01-07 03:42:58,2
AMD,ny4yfb0,Wow that website is abhorrent. privacy invading commercial trackers must be allowed or you have to pay up?  At least reader mode bypasses it.,AMD,2026-01-07 03:39:49,1
AMD,ny3qvmr,"Not that I don't trust them, but I don't need them. I've got a handful of GPUs that I can do my AI goonery on.  An NPU or high powered iGPU is a waste of silicon for me. I hope they come out with a crop of decent regular desktop CPUs next gen.",AMD,2026-01-06 23:45:26,5
AMD,ny4oa0e,"AMD has always gained traction in the mobile market, and then do something stupid to kill it.  Fucking 20yr cycle at this point.",AMD,2026-01-07 02:43:09,9
AMD,ny52jn4,To be honest the clock for those iGPUs are bumped a lot and they will be RAM bottlenecked anyway. Cutting down WGPs is an efficient way to save some cost.,AMD,2026-01-07 04:04:32,2
AMD,ny4uj21,You don't have a handful of GPUs in a notebook though. The point of these chips is for power efficient devices where getting 50-60 trillion operations a second out of a few watts does have a lot of value.,AMD,2026-01-07 03:17:30,1
AMD,ny40jpe,I'm still using a 1080 Ti gpu ;(,AMD,2026-01-07 00:35:20,1
AMD,ny56pt9,"Which would be cool if they're cheaper for the user, but they're not, certainly not for the Framework 13 which is what I was looking at, going from the 7640U to the AI 5 340 costs $130 extra, and for that I get... what, half the CPU cores dropped to compact versions AND the GPU chopped in half? It's not like the clockspeed of the GPU doubled (not even close, 2600 vs 2900 MHz) either so ain't no way that half assed version is going to totally match the old one, nevermind *improve* as one might expect when buying something newer.  So in my case since at the time the 7640U model was constantly going in and out of stock while the AI 5 340 just costs more for less I ended up splitting the difference and getting the Core Ultra 5 125H model instead, at least Intel only cut 12.5% of the GPU cores instead of 50% when dropping from the 7 tier to the 5 tier.",AMD,2026-01-07 04:30:42,1
AMD,ny436mn,"I've got one too, a real workhorse. I haven't put it out to pasture yet.  I'm on Linux, so I'm going to build a Windows 7 ""console"" for all my old games. That's where the 1080ti will find its home.",AMD,2026-01-07 00:49:00,3
AMD,ny4kaat,"My main machine has the 1080ti with a 7970x, on 8.1     Maybe someday I can afford to buy a 3090 Ti, to be used on Windows 8.1",AMD,2026-01-07 02:21:27,2
AMD,nxzscpw,So everyone is missing the point. If you already have a 9800x3d - this literally isn't for you.,AMD,2026-01-06 12:06:11,285
AMD,nxz82lt,Basically a binned 9800X3D?,AMD,2026-01-06 09:08:35,156
AMD,nxz8lq2,But will it survive on Asrock boards longer or shorter?,AMD,2026-01-06 09:13:51,140
AMD,ny0gz4f,"This is actually good for the market, unless they stop making 9800x3ds, it should lower the prices even more, cause it's not ""the best"" anymore.",AMD,2026-01-06 14:34:58,24
AMD,nxzob6w,"Dang, no sign of the 9950X3D2",AMD,2026-01-06 11:35:03,28
AMD,nxzqsan,by 2-3%.  yay.,AMD,2026-01-06 11:54:16,38
AMD,ny08gto,Asrock getting ready for another killstreak,AMD,2026-01-06 13:48:40,9
AMD,ny0l8t8,"Yawn, I just want more PCI lanes so that half the motherboard doesn't get disabled the moment you populate more than one m.2 slot.",AMD,2026-01-06 14:57:01,7
AMD,ny06443,If this makes 9800x3d cheaper? Hell yes then!,AMD,2026-01-06 13:35:32,3
AMD,ny0m3qk,So now it’s time to upgrade from my 2700X!,AMD,2026-01-06 15:01:19,3
AMD,ny42ieg,Like 2% faster,AMD,2026-01-07 00:45:29,3
AMD,ny014py,"Fine with binned chiplets but I hope they do the same for the I/O die too. Unless Zen 6 is coming this year, they could've done a refresh of Zen 5 with a newer I/O die - faster memory, higher FCLK & lower power consumption!",AMD,2026-01-06 13:06:11,6
AMD,ny1xv07,gonna upgrade from my 9950x3d then once the 9950x3d2 comes out im buying that. putting it all in an asrock motherboard paired with a 2060 super,AMD,2026-01-06 18:39:57,2
AMD,ny1g35n,The same people celebrating this launch are the same people who shat on the KS releases from Intel. Genuine hypocrites.,AMD,2026-01-06 17:20:14,2
AMD,ny0q4au,Cool when are they gonna put the 5800X3D back into production so I can max out AM4 without paying a price gouger $500?,AMD,2026-01-06 15:20:59,2
AMD,ny051fy,can it beat 420 euro 9800x3d?,AMD,2026-01-06 13:29:25,1
AMD,ny12ch8,"nty, I'll wait for the 10800X3D :)",AMD,2026-01-06 16:17:45,1
AMD,ny1mv8s,"Can you achieve the same on the 9800X3D with an overclock? Never tried overclocking mine, never felt I needed more performance, but it would be interesting if the 9850X3D is just a factory overclocked CPU like Intel's KS CPU's.",AMD,2026-01-06 17:51:03,1
AMD,ny2frha,Have a 9700X. Debating if I should go for this.  Edit: Nah I told myself I’d wait for the last AM5 chip and ride it through the next many years.,AMD,2026-01-06 20:00:58,1
AMD,ny2xv81,This Ferrari does 250mph and this one does 257mph,AMD,2026-01-06 21:24:24,1
AMD,ny3h9ql,its the exact same cpu just clocked faster.,AMD,2026-01-06 22:56:00,1
AMD,ny3n2de,"3D AI XPENT V-Cache .. It's called L3, no matter that AMD is trying to sell you lol.",AMD,2026-01-06 23:25:23,1
AMD,ny3wi0l,"I'm currently have an I9-10850k and am feeling quite a bit of cpu bottleneck at 1440p. I was on the fence for a long while and sprung for a 9800x3d today, I can decide to return it if I'd like, should I? I managed to snag a kit of 64 gb 6000mhz cl30 ddr5 and a decently priced 5080. I'm unsure whether I should have waited for either this, a small price drop of the 9800 or have waited for zen 6, all of it is still on the way",AMD,2026-01-07 00:14:30,1
AMD,ny3witq,So how does it compare to... 9950x3d?,AMD,2026-01-07 00:14:36,1
AMD,ny3xk21,"I'm in the process of building a new PC so I may as well hold until this launches. If I can't find one, I'll just settle with the 9800X3D",AMD,2026-01-07 00:19:56,1
AMD,ny40lxq,"No it didn’t, they just released a newer, faster version.  Weird title.",AMD,2026-01-07 00:35:39,1
AMD,ny4r3ao,Faster?    By how much compared to the 9800x3D?    By how much compared to the 7800x3D ?    By how much compared to the 5800x3D ?,AMD,2026-01-07 02:58:23,1
AMD,ny1nipk,Cool cool now fix all the crashing issues with AMD GPUs,AMD,2026-01-06 17:53:55,1
AMD,ny1fign,My 9950x3d still good though....right? 😉,AMD,2026-01-06 17:17:38,1
AMD,ny00xcm,It’s only a refresh it seems like. 2-3% performance increase is literally nothing. You can simply undervolt and adjust the boost clock to get to this level of performance increase for free.,AMD,2026-01-06 13:04:53,-10
AMD,ny05q4g,I have a 7800x3d atm. Not enough of an upgrade to make me purchase. Gonna wait for the next gen,AMD,2026-01-06 13:33:21,-1
AMD,nxzwar3,Soon we might see a lot of used 9800X3D for around $300 ish. Yay!,AMD,2026-01-06 12:34:33,-5
AMD,nxzjh6o,"my 9800x3d is already oc'd near 5.6 ghz, pass.",AMD,2026-01-06 10:54:21,-16
AMD,nxzapw6,"The real question is, is it actually bringing anything worthwhile over a 9800x3d. Gonna take a bit of convincing.",AMD,2026-01-06 09:34:22,-17
AMD,nxzylvt,Not really for anyone except those with more money than sense. You likely got Zen 6 later this year also 🤷‍♂️,AMD,2026-01-06 12:50:03,64
AMD,ny0i9m4,Same for the people who have the 7800x3d I assume then ?,AMD,2026-01-06 14:41:52,6
AMD,ny1oguf,"Yeah but also if you don’t why not just get the cheaper 9800x3d? 2% improvement is nothing, and I doubt this thing will cost the same I’m expecting a $100 up price   Not to mention extra heat and the ongoing issues the 9800x3d already has. Honestly I went for the more trusted 7800x3d recently as they barely increased from that",AMD,2026-01-06 17:58:02,3
AMD,ny0xyi2,"Tbf if you're at a point where a 9800X3D isn't enough, chances are you're either going to go for a 9950x3D or wait for Zen 6  I'd have thought that be obvious enough",AMD,2026-01-06 15:57:37,6
AMD,ny0h2z6,"Yes but it's gonna cost substantially more than the improvement it delivers, it's always the same with these higher binned chips, they withhold perfectly fine CPUs from going on sale and apply a useless OC that those who want it can just do it by themselves to sell them for a higher price.     They're a net negative",AMD,2026-01-06 14:35:33,2
AMD,ny26rze,Today on Shit people say who never used a PC for Work:,AMD,2026-01-06 19:19:53,1
AMD,ny2ge3w,"If you have a 9800X3D and want this, it might just be worth it to try and overclock it. Not all will make it but it's also only a few hundred MHz difference.",AMD,2026-01-06 20:03:54,1
AMD,nxz8kug,yep,AMD,2026-01-06 09:13:36,42
AMD,nxzmzoy,Yup.,AMD,2026-01-06 11:24:17,9
AMD,ny01hiy,for 2-3% more performance at likely 10% more cost,AMD,2026-01-06 13:08:24,9
AMD,nxzuo6p,Will wait and see about the memory controller.,AMD,2026-01-06 12:23:10,3
AMD,ny3g05n,It has more cache too it seems. 8MB.,AMD,2026-01-06 22:49:44,1
AMD,ny04ra4,I don’t know of any 9800 that can hit 5.6,AMD,2026-01-06 13:27:48,-9
AMD,nxze4lo,mobo explodes on startup,AMD,2026-01-06 10:06:30,59
AMD,ny1gq86,They are selling them both. They won’t lower the price they’ll jsut make the 50x model more expensive,AMD,2026-01-06 17:23:10,5
AMD,ny5e7i9,It doesn't make sense to stop making them because these are just highly binned,AMD,2026-01-07 05:21:16,1
AMD,ny06lnp,"Is that the official name?    Disapointing, It could have been RyzenX 9 9950X3DX2X PRO AI Black Edition.",AMD,2026-01-06 13:38:18,18
AMD,ny22w39,"The X4D can time travel, but as a precaution they only move forward in time at a static pace equivalent to everything around them   &nbsp;  /$",AMD,2026-01-06 19:02:06,2
AMD,nxzw1za,It makes no sense for AMD to release a CPU like that until Zen 6 when they solve the CCD latency.,AMD,2026-01-06 12:32:52,1
AMD,ny007m9,Its wasted silicon either way.,AMD,2026-01-06 13:00:19,0
AMD,ny0lp2l,"Hey, there's a 9% increase in a single game!",AMD,2026-01-06 14:59:18,13
AMD,ny44ltr,"Wait, what do you mean, are there M.2 limitations with AMD? I've been looking at upgrading relatively soon, but I have four M.2 drives - will I have an issue with this?",AMD,2026-01-07 00:56:29,0
AMD,ny3qizr,"Yeah but you overclocked the 2060 Super, right?",AMD,2026-01-06 23:43:35,1
AMD,ny1n7yz,"To be fair, it does look like the performance difference will be bigger with the 9850X3D than on the KS series. But we'll see later this month when the first reviews are out.  I'm sure Hardware Unboxed will rip them a new one if it's just a factory overclocked chip for a price premium, but rumors seem to suggest it will cost the same as the 9800X3D costs now.",AMD,2026-01-06 17:52:36,2
AMD,ny0xlbx,>high demand product  >high price  I don't wanna use the word entitled but...,AMD,2026-01-06 15:55:57,10
AMD,ny12k9f,You *do* know by bringing back an older product in production means that it will take space in the fabrication plants and so they'd have to stop production of their current and future products right? Or are you just being idiotic and think that these just suddenly poof into existence?,AMD,2026-01-06 16:18:44,-3
AMD,ny1z1sj,"Perhaps, but not guaranteed to be stable which is why they're binned and branded differently.",AMD,2026-01-06 18:45:10,1
AMD,ny3gs7l,"Do as your edit says. You're gonna probably notice nearly nothing unless a game is mega super CPU heavy or you upgrade to a 5090. Some games that are super CPU heavy may not be bottlenecked by cache either, single core speed still holds true for some.",AMD,2026-01-06 22:53:35,1
AMD,ny1w29c,What crashing issues ? I’ve had a 9070xt for about 5 months now and had like one crash.,AMD,2026-01-06 18:31:42,4
AMD,ny3gfhh,Latest drivers seem kinda weird on my computer but I rolled back to October and I've been flawless since. I'll probably install the next new driver version and hope it runs well.,AMD,2026-01-06 22:51:50,1
AMD,ny04wx8,it's not a refresh it's a bin,AMD,2026-01-06 13:28:42,16
AMD,ny15d8t,"Goes without saying, no?",AMD,2026-01-06 16:31:35,3
AMD,ny05gdk,Only if they’re stupid,AMD,2026-01-06 13:31:48,14
AMD,nxzv8w1,No way that's so crazy.   I had my 14900k and overcooked it to a bajillion Superhertz and immediately bought the 14900ks the second it came out.  You're missing out.,AMD,2026-01-06 12:27:16,8
AMD,ny1ylma,I think people still on AM4 are much better off waiting for AM6 at this point tbh,AMD,2026-01-06 18:43:12,1
AMD,nxzbkcl,"This isn't for people with a 9800X3D though, more for people planning to upgrade as another choice if they want the best of the best at a price premium (probably not good value) or if they're fine ""settling"" for the 9800X3D.",AMD,2026-01-06 09:42:26,35
AMD,nxzl6rf,It's never really a good idea to upgrade to one model higher than what you have. You won't see much performance difference that way. People who do that have lots of spare cash to burn.,AMD,2026-01-06 11:09:05,5
AMD,ny056l2,"Has amd released the x3d cpus the same time as the x cpus? If youre looking for vcache, youd probably be waiting until next year",AMD,2026-01-06 13:30:15,32
AMD,ny0y7tk,If 12 core ccds are real I’m selling my 5800X3D the moment its confirmed,AMD,2026-01-06 15:58:47,6
AMD,ny2rqtm,"Nah, zen 6 gonna be launched next year",AMD,2026-01-06 20:56:32,4
AMD,ny0x0r1,"Why does it always have to be money and “no sense” haha. That shit always kills me. People act like buying a CPU upgrade must mean you’re a millionaire 😂. I can buy that CPU when it releases and think nothing of it but I’m no rich guy. It’s like $600 not $25,000.",AMD,2026-01-06 15:53:21,8
AMD,ny3tioa,Considering how riddled with issued zen 5 was I think I'm gonna wait this time lol,AMD,2026-01-06 23:59:08,1
AMD,ny16s5b,It's like upgrading from a I9 to a KS sku 😭.,AMD,2026-01-06 16:38:02,0
AMD,ny2s2sh,"Yep, the difference between 7800x3d and 9800x3d is 10% at most",AMD,2026-01-06 20:58:02,1
AMD,ny2hbx8,"Pretty much. Remember that the main advantages of the 9800X3D over the 7800X3D are the ability to OC, slightly better temps and slightly higher clocks (500MHz base and 200MHz boost). The FPS difference in anything is minimal between the 2. The difference between the 9800X3D and 9850X3D will be miniscule.  Edit: Anyone that is down voting this has zero idea what they are on about. The 7800X3D and 9800X3D offer simialr performance. There is a difference but it is hardly worth noting. This new chip is nothing but an overclocked 9800X3D and those few hundred more MHz are not going to make it somehow much faster. The one area that a 9800X3D really shines over its predecessor is multi-core performance.",AMD,2026-01-06 20:08:16,-2
AMD,ny3ior5,Looks like the price difference will be around $30.,AMD,2026-01-06 23:03:03,1
AMD,ny120k5,"Not necessarily.  There's a realm where the extra mhz matters but you don't need a 9950x3d.  Admittadly it probably doesn't apply to many, but people act like there is no chance of that being the case.  Also that there is no announced release date for Zen 6, let alone x3d Zen 6.  So while you could do the decades-old - just wait for new product - there is something to be said for just buying the thing that will give you a benefit today.",AMD,2026-01-06 16:16:13,3
AMD,ny11w5c,"It's only $20 more expensive than a 9800X3D lol. If that's ""substantially more"" for someone, they can't afford even a 9800X3D.",AMD,2026-01-06 16:15:40,3
AMD,ny0kvqd,I agree. Consider it a KS for AMD.,AMD,2026-01-06 14:55:11,2
AMD,ny1t1vb,"Or as the product process matures they are able to get more chips that are capable of running at higher frequencies so it justifies splitting them off as a higher SKU. Mostly happens with GPU refreshes (IIRC, off the top of my head) but I don't see why the same wouldn't apply.",AMD,2026-01-06 18:18:17,1
AMD,ny1216t,"There already are leaks that it's only $20 more expensive, which is 4% more cost.",AMD,2026-01-06 16:16:18,16
AMD,ny0b5f8,i'd be willing to bet that they probably didn't release any of the samples that could because they were saving them for this,AMD,2026-01-06 14:03:32,8
AMD,ny1gtdq,"I don't have the data, so trust some rando on the internet but a few weeks ago I saw my 9800x3D hit 5.5 while playing AC Shadows. But to be clear I run my system with PBO/PBO enchantment 90 level 3.",AMD,2026-01-06 17:23:34,2
AMD,ny4zgm2,Gpu capacitors fry themselves at max temperature instead of max framerate for the ultimate assrock experience.,AMD,2026-01-07 03:45:55,1
AMD,ny0g7xi,"It's not announced, so that's a name the community made up.",AMD,2026-01-06 14:30:57,4
AMD,ny11l5h,That's a stupid name. It should be the Ryzen X9 9950X3DX2X XTX PRO AI Golden X-Dragon,AMD,2026-01-06 16:14:16,3
AMD,ny4lqv1,"Check your motherboard manual or mobomaps.com, it depends on your chipset and lanes configuration.",AMD,2026-01-07 02:29:22,1
AMD,ny4rjhd,"Check mobo manual, from my readings minimum b650? supports both m.2 at full speed.",AMD,2026-01-07 03:00:51,1
AMD,ny50n6q,"Its not just AMD. It plagues consumer chipsets in general. Neither intel nor AMD have enough PCI lanes IMO. You can't just glance at a motherboard with say 3 or 5 m.2 slots and expect to use them all at once because there aren't enough lanes to populate every slot on the motherboard. Its been a problem ever since m.2 nvme became popular.   On many boards, once you populate the 2nd or 3rd m.2 slot, depending on the chipset being used, you'll lose access to the 2nd X16 slot entirely as at that point you've already utilized all of the pci express lanes available.  In addition, a lot of boards will at the same time drop your GPU lanes down to 8x or 4x once you've added a 2nd or 3rd nvme drive.   If you plan on using a mini-itx board its not so much a problem because typically they only have one x16 slot and a few m.2 slots. So in total you're talking say 20 to 24 pci-e lanes between your GPU and 1-2 x4 m.2 drives. On full ATX is where you often need to make compromises. Its really annoying especially if you're looking at high end boards that are often expensive and have 4 or more m.2 slots.  It rarely makes sense to go full ATX these days imo.  If you want a full size board and the ability to utilize everything at once you're essentially pushed to a threadripper build.",AMD,2026-01-07 03:52:59,1
AMD,ny3rgr6,i overclocked my power supply and undervolted my ram don’t worry,AMD,2026-01-06 23:48:31,1
AMD,ny0y106,"Out of all the comments on Reddit, *this* is the one you decided to call out? I'm not the first to suggest that AM4 chips should go back into production.",AMD,2026-01-06 15:57:57,2
AMD,ny134gi,"Don't complain about computer market dynamics on Reddit, worst mistake of my life.  Bro bit my head off for not 100% understanding a market which is actually pretty fucking complicated.",AMD,2026-01-06 16:21:18,0
AMD,ny3qegl,There was a rumor not long ago that AMD have decided to make Zen 7 an AM5 product. So he might have to wait a while to upgrade.,AMD,2026-01-06 23:42:55,1
AMD,ny3hpxz,Ugh you’re making me want to revert back too. We got almost the same set up beside I have a B650P. 😭,AMD,2026-01-06 22:58:15,1
AMD,ny05pnt,So a refresh,AMD,2026-01-06 13:33:16,-16
AMD,ny1c8rn,I did consider the 9800x3d and sell my cpu to a friend. 9850x3d is ok. But the leap i hoped for.  that will come with the next ones.,AMD,2026-01-06 17:02:43,0
AMD,nxzc24e,"Yeah, but the problem is, this is competing with the 9800x3d and has to have a worthwhile value proposition. If it's just higher tested silicon with a higher factory clock and more power draw, am I really going to buy it over hitting 5.5 on a 9800x3d under the same settings for less money?",AMD,2026-01-06 09:47:09,-13
AMD,ny03tz6,"Yup. Only exception is if you have a family member or friend who would use your current part. Otherwise there's no point.   Or if you have a change in use cases -- e.g. if you start doing video editing and not just pure gaming, then upgrading from 9800X3D to 9950X3D for more cores would make sense.",AMD,2026-01-06 13:22:29,2
AMD,ny0fvc0,"For zen 5, they released them about 2-3 months later, so within the same year.",AMD,2026-01-06 14:29:05,18
AMD,ny121ya,If 12 core ccds are real then I’d be selling my 9800x3d for one lol,AMD,2026-01-06 16:16:24,7
AMD,ny45sgk,yeah pretty sure its 2027,AMD,2026-01-07 01:02:48,1
AMD,ny4rera,They are not going three years without a new Zen release. Zen 5 was back in 2024.,AMD,2026-01-07 03:00:08,1
AMD,ny2rz42,"still huge money to drop for a cpu dude, you may not be rich...but still you are not a poor guy ahahha",AMD,2026-01-06 20:57:35,3
AMD,ny2iuo9,Given my hands on experience with the 9800x3D in most cases even turning on eco mode doesn't cost any tangible gaming perf. So higher clocks literally would be pointless for most people.,AMD,2026-01-06 20:15:18,2
AMD,ny333fj,Great for you mate. Having $600 of disposable income is not common for a lot of people around the world. Maybe in a sheltered world view that seems common place. But your statement reeks of the arrested development quote.,AMD,2026-01-06 21:48:23,-2
AMD,ny36twl,Much better if you main games like Tarkov,AMD,2026-01-06 22:05:39,3
AMD,ny43283,"> Pretty much. Remember that the main advantages of the 9800X3D over the 7800X3D are the ability to OC, slightly better temps and slightly higher clocks (500MHz base and 200MHz boost). The FPS difference in anything is minimal between the 2.  Zen5 x3d is >30% faster than Zen4 x3d on Satisfactory and BG3",AMD,2026-01-07 00:48:21,1
AMD,ny48cfm,"That does make it better at least, thought they’d be far more greedy",AMD,2026-01-07 01:16:38,1
AMD,ny2j6ln,> There's a realm where the extra mhz matters but you don't need a 9950x3d.  I think CS2 and R6S players already hit high enough frames. Even losing mhz turning on eco mode with a 9800x3D doesn't seem to impact heavy AAAs much from what I've seen.,AMD,2026-01-06 20:16:51,1
AMD,ny14lh4,"If we just take the actual listed prices we have so far it's 20% more than current asking price for 9800X3D's for maybe 5% avg uplift in FPS. That is a substantial price increase for sure. Anything but the same price/perf is bad in PC hardware, has always been.   And these days that 80 or 100$ difference is huge. Wont even cover the new RAM prices.",AMD,2026-01-06 16:28:03,-1
AMD,ny23486,Damn that's great to hear. If you were in the market for a 9800x3d no reason to not buy this one then.,AMD,2026-01-06 19:03:07,2
AMD,ny0ti7i,"That's what I was thinking. Put your best fab'd 9800 chips off to the side, wait a little over a year and rebrand those as 9850 because they can boost 400 MHz higher, with no other changes to them AT ALL. This is kind of typical slimy business behavior, but whatever lol.",AMD,2026-01-06 15:37:08,3
AMD,ny2kkeq,"I believe it, recently did a build and some turbo mode thing was on by default with the board and it was pushing like 5.45 all core out of the box without PBO or additional tuning. Updated the bios so that could be properly disabled though. Don't need the extra heat.",AMD,2026-01-06 20:23:20,1
AMD,ny0l331,AFAIK it's confirmed by benchmarks showing the CPUID string. See [https://browser.geekbench.com/v6/cpu/15755183](https://browser.geekbench.com/v6/cpu/15755183),AMD,2026-01-06 14:56:14,0
AMD,ny0z8u6,The 58's MSRP was 450$. The fact that you're only paying a 50$ premium is crazy.,AMD,2026-01-06 16:03:30,0
AMD,ny3zhok,"Well they can get zen 6, then get zen 7 a while after release if it's a good value like some people have done with am4.",AMD,2026-01-07 00:29:52,1
AMD,ny3pwue,"Wonder if it's an RDNA 3 issue. For me the biggest issue is using discord streams while tarkov was running. It would crash so bad it took out windows and fried the drivers. Had to safe mode, DDU, then reinstall October drivers.",AMD,2026-01-06 23:40:22,1
AMD,ny0fckk,Better binning is not what a refresh is lmao.,AMD,2026-01-06 14:26:18,21
AMD,ny0ghtc,"Literally the exact same CPU. The ones that have the least defects and clock the highest go into the 9850X3D bin, the ones that don't go in the 9800X3D bin. Hence why it's called binning. Not a refresh.",AMD,2026-01-06 14:32:26,12
AMD,nxzcp4i,"Probably not you or me, but more people just wanna get the ""higher number"" without checking specs a la 14900ks over 14900k. GPU really only has the premium performance with the halo product for gaming.",AMD,2026-01-06 09:53:08,8
AMD,nxzm3hv,"Saw the CES leaks, it's definitely worth it with it only being $20 more expensive than a 9800X3D. Hard to believe anyone buying a new AM5 system to cheap out on the $20 instead of just getting the 9850X3D. I'd still personally recommend a 7800X3D instead for more than $100 cheaper, but people looking at a 9800X3D definitely will wait for the release of the 9850X3D. Only problem is since these are higher binned 9800X3D in reality, supplies might be extremely limited.",AMD,2026-01-06 11:16:48,3
AMD,ny09c2f,"These kind of products are never designed for sensible buyers. The goal is to keep an halo gaming product and solidify the halo gaming price tier, while being more aggressive (than now) on the 9800x3D.",AMD,2026-01-06 13:53:29,2
AMD,nxzjrjq,"I believe it has a better IMC.  EDIT: nope, I guess not.",AMD,2026-01-06 10:56:49,1
AMD,ny0vhgp,"Literally no idea why you're getting downvoted to hell, but you're exactly right. It's only a 3% 400 MHz add to the boost clock. This just means they're using the same 9800 chip but with higher quality silicon that can hold that higher boost stable. They were definitely saving these for a later date.",AMD,2026-01-06 15:46:19,1
AMD,ny1iunq,"I was kinda hoping the 9600x3D would be out, but honestly I might get this or a 9950x3D and give my wife my current 9800X3D and run it in 65w mode.",AMD,2026-01-06 17:32:52,2
AMD,ny0p52l,"Oh okay. Nice, sounds like amd is moving in the right direction with their releases",AMD,2026-01-06 15:16:18,7
AMD,ny3b9li,Tjos*,AMD,2026-01-06 22:26:47,1
AMD,ny1d4tg,"Where are you looking at actual listed prices that say it's 20% more? At least in Amazon 9800X3D is at 500 while Newegg has it at $470. The MSRP of the 9850X3D is $510 from what's being leaked around CES, that's far from a 20% price increase.",AMD,2026-01-06 17:06:48,3
AMD,ny33s9f,When a new cpu drops older ones usually drop so yes there is a reason to still get a 9800x3d,AMD,2026-01-06 21:51:33,1
AMD,ny0wl6i,AMD does a billion things wrong but reserving a better bin for a different SKU isn't one of them when the 9800X3D only needs to hit 5.2ghz to be within spec and overclocking isn't and never is something promised.,AMD,2026-01-06 15:51:23,6
AMD,ny5e0gc,There's literally nothing wrong with this,AMD,2026-01-07 05:19:51,1
AMD,ny0r272,"Yeah showing up on benchmarks is one thing, but is the name showing from the device?",AMD,2026-01-06 15:25:31,2
AMD,ny12wcz,Before RAM went crazy the price was $200. I guess I'm not allowed to complain about market conditions on Reddit though.,AMD,2026-01-06 16:20:16,3
AMD,ny20a04,"What did you see? Most of what I can find is just 5.6 GHz, 580 bucks. There is some mention of 50 dollars more than the 9800, but no mention of 20.   More than that, is it actually just higher binned, or is that just our assumption? I can't find anything in text based stuff about that, but I am on limited internet rn.",AMD,2026-01-06 18:50:33,0
AMD,ny222ew,"The reviews are going to be a big deal. If these chips have no overhead to OC further, or are just slurping a bunch more power, the 9800x3d will come out looking like the value king somehow.",AMD,2026-01-06 18:58:23,1
AMD,ny0gx42,It doesn't. It's just a bin.,AMD,2026-01-06 14:34:40,1
AMD,ny0q4mm,The only reason they did release so close to each other was because the zen 5 X versions were flopping really hard on sales.,AMD,2026-01-06 15:21:02,11
AMD,ny2akqq,"Currently listed for 430 at microcenter, which i assume is the goto for US. The ""leaked"" listing price was 511-550. So i just called that 20% since that upper limit is most likely bogus.  It gets worse for EU customers since the chip hasn't been at MSRP for a year. It even got steep discounts around black friday - so if you take that into account it just looks even worse. But if i had to guess, I would say discounts on PC hardware is gonna be harder to come by in 2026.",AMD,2026-01-06 19:37:13,-1
AMD,ny3qlmn,"Hell, there's reasons to still get a 5800X3D if they were still in stock. AMD struck gamer gold with these chips.",AMD,2026-01-06 23:43:58,1
AMD,ny0xwuy,"Then tell me what is so different about this chip aside from the higher boost clock? It's just a higher quality bin. Released just over a year after the 9800X3D? I'm not saying it's wrong, it's just so obvious. The current 9800X3D can usually boost up to 5.7 if you're lucky, but even 5.6 is reachable and stable with a modest PBO and CO tweak. These chips just mean that 5.6 is guaranteed out of box.",AMD,2026-01-06 15:57:24,1
AMD,ny0uguz,"Yes, that field is the CPUID string reported by the CPU itself. The regular X3D reports as ""AMD Ryzen 9 9950X3D 16-Core Processor"".  EDIT: The CPUID brand string can be altered after boot on AMD CPUs, so it's not definitive, but cheating is highly unlikely: basically the only ones who could pull it off are those with a dual-V-Cache chip (perhaps an engineering sample) that reports a different brand string. since the cache amounts on these benchmarks are correct for what would be expected from the X3D2.",AMD,2026-01-06 15:41:37,0
AMD,ny189sj,"For the price of a 58, you could put that to a 9800 bundle.",AMD,2026-01-06 16:44:47,0
AMD,ny0qole,Yeah I guess it was just a rumor. Bummer,AMD,2026-01-06 15:23:43,1
AMD,ny103yk,"You can't prove without evidence that wasn't just a coincidence.     They weren't a great step up, but do you actually have sales numbers",AMD,2026-01-06 16:07:31,2
AMD,ny45l8n,"People would definitely want microcenter, but probably only 10% of people have good access to it. Everything seemingly is really cheap there. As for EU customers though, good for you that you have it so cheap, but Black Friday prices really isn't representative of the launch comparison. MSRP comparison are more used because that doesn't move at all, it depends on the stores themselves what discounts or price gouging they actually do. At least for CPU there isn't any ""false"" MSRP like GPUs. So for most of the US the 9850X3D only really is $20 more expensive. Hopefully it will bring the 9800X3D to $400 and that definitely is the better value, but that speaks more to the price cuts on the 9800X3D and not what the launch price of the 9850X3D is. No manufacturer uses a current price of a product to price their future products, they usually use the MSRP.",AMD,2026-01-07 01:01:43,1
AMD,ny0z7jc,"It's akin to Intel's limited KS skus. Not cutting out Silicon Lottery (shut down a while ago)'s business, that's it. Btw, it's the exact same shit they're doing on laptop. Gorgon point is literally just higher core, mem, and npu clocks nothing else.",AMD,2026-01-06 16:03:20,4
AMD,ny2vgij,"Their issue is being forced to buy overpriced ddr5 ram by AM5, not the cpu itself. DDR4 is quite cheaper",AMD,2026-01-06 21:13:26,2
AMD,ny13fk0,"There was actual public sales numbers, in this video they share some: https://youtu.be/dKtUJ2nDxZo?si=gzjXe1nvDgQDHpnz   Today Zen5 sales are composed 90% from 9800X3D",AMD,2026-01-06 16:22:43,2
AMD,ny184yq,"I think they also sorta reversed what they did with Zen 3 X3D - instead of selling the lower bin first, they sold it after the higher bin was all sold out. This is presumably why we have 9850X3D instead of 9700X3D",AMD,2026-01-06 16:44:10,1
AMD,ny14bxi,90% seems fake and sourced from a bad source like mindfactory   There is no way 90% of cpu sales are that expensive,AMD,2026-01-06 16:26:50,12
AMD,ny3lh5c,"I work in a pc shop and realistically, it’s probably 70%+. And those that dont buy the 9800 usually settle for the 7800, or have a use case for the 9950x3d. The 7700/9700x is basically dead to consumers and, at least recently, the average 7600x buyer has been priced out due to ram being so expensive.",AMD,2026-01-06 23:17:15,1
AMD,nxwmv1b,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-01-05 23:03:22,1
AMD,nxwyv0v,Are those prices Legit or speculation?,AMD,2026-01-06 00:06:12,23
AMD,nxyfsqf,Is this basically a 9800x3d binned to the 9950x3d spec?,AMD,2026-01-06 05:06:01,9
AMD,nxwvs4o,No thanks. Iam in the zen6 x3d waiting camp...,AMD,2026-01-05 23:50:12,29
AMD,ny07ufa,Are we getting the 9600X3D?,AMD,2026-01-06 13:45:14,2
AMD,ny3wbnf,we need a 9999X3D,AMD,2026-01-07 00:13:34,1
AMD,nxx1rss,Those terms are not mutually exclusive.,AMD,2026-01-06 00:21:23,-29
AMD,nxyqr5d,"Close, but actually a bit ~~lower~~",AMD,2026-01-06 06:30:09,2
AMD,nxwyvkf,I'll wait for whatever the best cpu my b650 supports is,AMD,2026-01-06 00:06:16,16
AMD,nxz9who,"I'm expecting Zen 6 X3D to be announced at CES 2027, with Feb/March availability.",AMD,2026-01-06 09:26:26,3
AMD,nxzsp6t,Apparently that will be another year at least...,AMD,2026-01-06 12:08:48,3
AMD,ny0xwt5,You sure do like waiting.,AMD,2026-01-06 15:57:24,1
AMD,nxxavg9,"No one is asking you to upgrade, not even AMD.",AMD,2026-01-06 01:09:16,22
AMD,nxy9gam,The 9800x3d is already so fast no gpu can keep up   The 9850 is just for funsies,AMD,2026-01-06 04:23:42,1
AMD,nxxm69l,These comments are the most nothing burger comments ever. You clearly know what they are asking and you still don't answer their question lmfao.,AMD,2026-01-06 02:10:09,43
AMD,nxx5hbn,"fine, be pedantic, he's asking if they're OFFICIAL or speculation",AMD,2026-01-06 00:40:34,10
AMD,nxytsxw,"It's actually higher, the X3D CCD has 5.55 max boost, this one has 5.6.",AMD,2026-01-06 06:56:02,8
AMD,nxz68wg,"Same, I will buy the best single-CCD X3D CPU that AM5 supports when it is out, then swap to AM6/7 a long time later.",AMD,2026-01-06 08:51:02,3
AMD,nxzajh9,"Actually the 5090 can get bottlenecked sometimes by the regular 9800X3D, those that have infinite money as a punishment in life will surely get the new 9850X3D or the 9950X3D V2 (not just because of the performance, but mainly because they despise the idea of not having the latest and shiniest to show in their photos and benchmarks).",AMD,2026-01-06 09:32:38,2
AMD,nxyu42v,"Ah, I just read the 5.7 and forgot its not both dies",AMD,2026-01-06 06:58:41,5
AMD,ny28on7,Sounds like a good plan to me,AMD,2026-01-06 19:28:32,1
AMD,nxxvai3,Self aware,AMD,2026-01-06 03:00:03,7
AMD,ny1sql7,Ironic,AMD,2026-01-06 18:16:54,1
AMD,ny1nat6,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2026-01-06 17:52:57,0
AMD,nxb7crr,"I own a 9070 XT and my steam survey always fails to register it, results in it determining that I have Radeon graphics….",AMD,2026-01-02 19:58:38,314
AMD,nxb05hf,"congratulations, I guess",AMD,2026-01-02 19:23:58,107
AMD,nxdqjbs,More 5090s than 9070s is diabolical.,AMD,2026-01-03 04:15:33,19
AMD,nxb08ia,"It's common knowledge that the non-XT is just the cut down die from the XT, there should be at least three XT's to every non XT.",AMD,2026-01-02 19:24:22,72
AMD,nxb40f3,I was expecting atleast the 9060XT being there lol.,AMD,2026-01-02 19:42:31,10
AMD,nxaygo1,Yay lol. Where's the XT? ;/,AMD,2026-01-02 19:15:53,17
AMD,nxbtkl3,"It was there for couple months already, under dx12 if you sorted by API",AMD,2026-01-02 21:46:58,5
AMD,nxcpscv,RDNA4 cards are great,AMD,2026-01-03 00:38:16,5
AMD,nxb1erl,"Why is the less desirable, ""only exists to upsell you to the XT"" card showing up before the 9070XT?",AMD,2026-01-02 19:30:01,18
AMD,nxcuv48,"Hey, I’m part of that .22%",AMD,2026-01-03 01:06:33,4
AMD,nxbbprc,"Why do people wait until they get ripped off to rush out and buy the card? They probably sell more cards at over MSRP because people run out and panic buy cards at 100-200 dollars more than MSRP, like fools. I guess people just wanna give all their money to big corporations. 🤷‍♂️",AMD,2026-01-02 20:19:54,3
AMD,nxb17pa,"I'm still kinda cranky about the ""AMD made a ridiculous number of 9070XTs, and everyone can get it at MSRP at launch!"" messaging leading up to launch.   So I sell my 3070 right before launch. Log into Newegg the instant sales open up, and BAM! All of the MSRP models instantly disappeared.   Then I scramble and eventually get a Hellhound off Amazon. It's a good card, mind you. Having an extra $160 is also good, though.",AMD,2026-01-02 19:29:04,15
AMD,nxb916v,"I've seen lots of 9070 models discounted to less than 550€ (VAT included) last November, while only a couple of 9070 XTs could be found with a similar discount, so that checks out I guess",AMD,2026-01-02 20:06:47,5
AMD,nxbkx08,"I'm on linux mint 22 and steam recognizes my 9060 xt as ""radeon graphics""",AMD,2026-01-02 21:05:06,2
AMD,nxbo3sv,This is probably only visible because the steam survey is broken this month; it shows 131% share for DX12 GPUs.,AMD,2026-01-02 21:20:32,2
AMD,nxbj4px,Yall are getting hardware surveys?,AMD,2026-01-02 20:56:25,1
AMD,nxbk2ly,happy to be one of them,AMD,2026-01-02 21:00:58,1
AMD,nxbm6s3,The first AMD GPU in a while to show up during the run of the generation,AMD,2026-01-02 21:11:18,1
AMD,nxd5tsc,I’m have yet to see the survey pop up since I got my 9070 XT. Curious to see that GPU’s percentage in the market.,AMD,2026-01-03 02:10:03,1
AMD,nxggkps,"For the people wondering were the XT is that card has shown up as AMD Radeon(TM) Graphics for most people since launch, same deal with the non XT, so there's also 9070 cards that get lumped in there too",AMD,2026-01-03 16:01:17,1
AMD,nxidm3b,I have 9070xt mobo and rams ive run out of money lmao,AMD,2026-01-03 21:26:43,1
AMD,nxbbqc3,this survey is worthless anyways,AMD,2026-01-02 20:19:59,-7
AMD,nxc3ia1,Because steam surveys are skewed,AMD,2026-01-02 22:37:05,-2
AMD,nxbeczv,"Do you also have the iGPU enabled? My hardware survey on Steam shows the GPU as a RX 9070, RX 9070 XT, RX 9070 GRE, which is what seems to get reported on Linux.  However, the system shows it is a RX 9070 XT (lspci, LACT).",AMD,2026-01-02 20:33:00,71
AMD,nxbo3qm,You have to make PCIEx graphics your primary in BIOS and turn off iGPU (cpu integrated GPU which is identified as amd Radeon graphicsl,AMD,2026-01-02 21:20:32,30
AMD,nxbiady,"I don't know how to ask this in the nicest way, but are you sure you've plugged your cable into the GPU and not the motherboard?",AMD,2026-01-02 20:52:18,10
AMD,nxcz0o4,"Same here , same here.",AMD,2026-01-03 01:30:36,1
AMD,nxjynxs,"When I had old laptop with intel and Nvidia I was getting steam survey regularly, after new pc with amd, almost never.",AMD,2026-01-04 02:25:39,1
AMD,nxpmnd7,I just installed my 9070xt and the os is yet to recognize it and give me proper drivers,AMD,2026-01-04 22:45:31,1
AMD,nxg6rlg,"I have never seen proof of this, even though i have seen this being claimed dozens of times",AMD,2026-01-03 15:12:51,1
AMD,nxehd0q,You know what I find interesting? The last 15 top threads on this sub were all from videocardz and were all posted by RenaltMC lol. *Do they work for the website??*,AMD,2026-01-03 07:40:09,19
AMD,nxb8ny4,"At MSRP, the price-to-performance math on the non-XT just doesn't make much sense to me.",AMD,2026-01-02 20:04:57,29
AMD,nxbfvqw,"Everyone says to go for the xt but if you’re really on a budget trying to stock up before prices explode? The non-xt is perfectly good unless you have a real crazy 4K setup and even then it’ll probably be okay with some tweaking.  Maybe you gotta buy some more ram, or an nvme before they go up too, so your budget’s thin? Don’t spend the extra for the xt you’ll still be happy.",AMD,2026-01-02 20:40:34,3
AMD,nxdba9c,In the real world the 9070 is often significantly cheaper than the 9070 XT which was often selling overpriced nearly the same as 5070 Ti. People often go for the cheaper alternative options and that is exactly what the 9070 non XT offers.,AMD,2026-01-03 02:41:59,2
AMD,nxfln60,"I know AMD dont make as many GPU's as Nvidia does, but I've still felt something has always been off about how AMD GPU's get reported on Steam Surveys.",AMD,2026-01-03 13:11:41,9
AMD,nxaymvt,its still shy,AMD,2026-01-02 19:16:42,22
AMD,nxb6kbg,It gets registered as amd radeon graphics,AMD,2026-01-02 19:54:47,8
AMD,nxctwdv,Still too overpriced,AMD,2026-01-03 01:01:04,-8
AMD,nxb58dc,"Because I bought the 9070 for $580 when the cheapest 9070 XT was still selling for $700+.  The price discrepancy has, historically, been a lot more than $50 for these cards.",AMD,2026-01-02 19:48:22,47
AMD,nxb4vnv,"Because it's cheaper, simple as.",AMD,2026-01-02 19:46:41,22
AMD,nxbdc17,9070 is placed comfortably in the midrange market. 9070 XT is in the awkward $600+ range which is entering enthusiast territory. Once you're spending that much on a GPU more people tend to go Nvidia.,AMD,2026-01-02 20:27:54,12
AMD,nxb2ue6,Still 16gb but less money? People really struggle to understand gpus who don’t spend their time reading/watching reviews and stuff.,AMD,2026-01-02 19:36:54,7
AMD,nxbbnwk,The non XT only loses out on 15% of the performance at most while consuming 27% less power. I guess you could undervolt the XT but why tinker with that if you can spend less money?,AMD,2026-01-02 20:19:39,4
AMD,nxg3obv,"I bought my 9070xt at msrp on launch, they raised it after selling out",AMD,2026-01-03 14:56:31,2
AMD,nxb8h1p,You should know by now to NEVER sell your existing hardware until you have the new hardware in your hands. Yes it means you need more cash flow to do an upgrade but make it a rule to not even consider selling your current hardware to finance an upgrade.,AMD,2026-01-02 20:04:01,34
AMD,nxbldfa,"The fact that AMD is *still* getting a pass for the fake MSRP thing is really disgusting to me.  Yeah GPUs often get marked up and stuff but this gen had an explicitly fake price just to get reviews.  I remember all these reviews for 5070 Ti being like ""An amazing card, but you'll only be able to find it for $1000"" and the same people about 9070 Ti were like, ""almost as good as 5070 Ti, but only $600!""  Then I go to the shop and all the 5070 Ti models are $750 meanwhile the 9070 is $900.",AMD,2026-01-02 21:07:20,12
AMD,nxchjtb,"> I'm still kinda cranky about the ""AMD made a ridiculous number of 9070XTs, and everyone can get it at MSRP at launch!"" messaging leading up to launch.  After RDNA2 launch and the Frank ""$10"" Azor bet debacle I'm surprised anyone still trusts Radeon marketing. Even when they have good products they bungle supply and messaging horrendously.",AMD,2026-01-02 23:53:01,3
AMD,nxei352,"How about the ""AMD is outselling NVIDIA by like 20x"" according to one guy tweeting about one store he gets numbers from in Germany. Surely that would have reflected in sales data by end of year. Oh wait it didn't. Surely it would have shown up on Steam surveys at some point during the last 8 months. Oh wait. Surely all these tech youtubers banging the drum on how AMD finally beat NVIDIA this generation aren't playing up the hype for more views on yet another fleeting video. Oh.",AMD,2026-01-03 07:46:23,2
AMD,ny0fq7z,"In AMD's defence, they thought that 80% of the people looking to spend $700-ish on a GPU would have already bought an Nvidia card and thus wouldn't want a 9070XT.",AMD,2026-01-06 14:28:20,1
AMD,nxboyx2,"That's why I went for the regular 9070. $100 cheaper on sale, same amount of vram and having it use less power and run cooler gives me a bit of peace of mind for longevity.",AMD,2026-01-02 21:24:40,3
AMD,nxbl9sr,To you maybe. It does show trends over time.,AMD,2026-01-02 21:06:49,3
AMD,nxg6w4y,Cope,AMD,2026-01-03 15:13:31,0
AMD,nxbojws,Yeah that didn't affect me and the survey but the second I slot in my old 3080 it asks Everytime lol,AMD,2026-01-02 21:22:40,27
AMD,nxcft5f,"It wouldn't actually matter, GPU passthrough is a thing.",AMD,2026-01-02 23:43:21,10
AMD,nxp61l7,It's clearly no longer a community reddit sub. It's been locked down and only a select few approved posts are allowed. Sometimes we don't see a new post here for a couple days. Nothing but news from the approved posters are allowed. Everything else is shoved into that megathread that no one even reads.  They've damn near killed the sub and it's ridiculous. It's just news articles with a comment section in another format now.,AMD,2026-01-04 21:27:58,5
AMD,nxban45,The TDP is 80W less which might be important to some people.,AMD,2026-01-02 20:14:36,36
AMD,nxbcwq8,"It's likely by design. If they're having a high yield for the XT die, then there aren't many rejects to cut down for the 9070. If they had a lot of rejects it would be cheaper to move it, if they don't have a lot of rejects then it exists to upsell you to the XT.",AMD,2026-01-02 20:25:48,7
AMD,nxc2qqp,"As someone who bought an RX 9070 a week ago, I didn't feel like buying a new power supply and a new case.",AMD,2026-01-02 22:33:08,3
AMD,nxs58a0,"Bought the non-xt recently, because the price difference was 14pct, which puts the XT out of budget for my needs ($828 vs $727, Danish prices 💀)",AMD,2026-01-05 07:40:47,1
AMD,nxbw5cc,Even where you live matters. In eastern Europe XT is 100-150$ more expensive.,AMD,2026-01-02 21:59:36,7
AMD,nxf3lpv,atm I can get a 9070xt cheaper than the 9070 in my usual shop,AMD,2026-01-03 10:51:31,1
AMD,nxbtapu,"There are plenty of arguments to buy the non XT, it's a good card but my point is that the XT greatly outnumbers the non XT while only one shows on the survey. Makes the survey seem less accurate",AMD,2026-01-02 21:45:37,1
AMD,nxhat94,The dGPUs shipment numbers tell the same story.,AMD,2026-01-03 18:21:22,2
AMD,nxkthw3,">but I've still felt something has always been off about how AMD GPU's get reported on Steam    Surveys.  I've felt the same thing, especially in recent years and sale data. I just asked Steam to compare my system specs.  A lot of people are buying AMD cards right now, so we'll see what's going on by the end of the year.",AMD,2026-01-04 05:35:45,2
AMD,nxehfep,Must be the weather,AMD,2026-01-03 07:40:44,3
AMD,ny0daxl,"Let's not forget that while the price is entering enthusiast territory, the performance is only upper midrange. AMD has greatly benefited from Nvidia being able to move the price:performance goal posts in the absence of meaningful competition.",AMD,2026-01-06 14:15:19,1
AMD,nxbxltx,"Not anymore. Nvidia is planning to reduce its production of video cards this year, and its prices have already increased: https://overclock3d.net/news/gpu-displays/nvidia-plans-heavy-cuts-to-gpu-supply-in-early-2026/.",AMD,2026-01-02 22:06:51,-1
AMD,nxbkp5r,Yeah they needed to make the xt fall between 5080 and 5090,AMD,2026-01-02 21:04:02,0
AMD,nxc6f4v,"Cuz undervolting is super easy and you lose hardly any performance, if any at all.  Obviously some people wont bother, but it really is braindead easy and take seconds.",AMD,2026-01-02 22:52:15,2
AMD,nxbarag,"Not just ""by now"", that was the case for multiple years leading up the the release.",AMD,2026-01-02 20:15:11,9
AMD,nxbeqzh,"Apparently, every eight years, the Internet goads me into doing something stupid when I should know better.   In 2017, I pre-ordered Mass Effect Andromeda.   In 2025, I believed when they said this GPU launch would be different.  I can't wait to see what 2033 has in store.",AMD,2026-01-02 20:34:56,7
AMD,nxbyc9y,"Prices on old hardware generally drops when new stuff is released, so I understand someone trying to get the maximum value out of his old GPU might choose to sell a bit early.  Better have a backup card though, or at least an IGP enabled CPU.",AMD,2026-01-02 22:10:33,2
AMD,nxcmt9h,"Radeon prices were much better in Europe, at launch there were a few models at msrp but 90% sold above that which was annoying, but prices dropped significantly over the next few months pretty much down to msrp for some models.  Nvidia cards have been way overpriced the entire time however",AMD,2026-01-03 00:22:02,1
AMD,nxei6lo,"At this point you have to think Frank ""whoopsies"" Azor and Jack ""I meant to do that"" are actually NVIDIA employees in disguise lol.",AMD,2026-01-03 07:47:12,0
AMD,nxbwsdj,This one seems really fucked with stats not making sense at all though.,AMD,2026-01-02 22:02:46,2
AMD,nxfnm1o,"What's very annoying is that they dont allow you to view those longer trends over time.  You only get the past handful of months and that's it.  Trends often need longer to present, especially when things can fluctuate they way they don Steam surveys.",AMD,2026-01-03 13:24:10,1
AMD,nxf05p7,"Dude, I'm 100% convinced steam survey is a scam.",AMD,2026-01-03 10:22:12,23
AMD,nxcmvyf,It does actually matter because a lot of motherboards dont support it and it’s usually not enabled.  Just look at the weekly posts in buildapc of people having poor performance because they plugged their monitor into the motherboard gpu.,AMD,2026-01-03 00:22:26,10
AMD,nxvhc7o,"Yep I've tried to post a few things in the last few months and all were auto hidden and mods never responded to messages.  > /r/AMD is in manual approval mode, this means all submissions are automatically removed and must first be approved before they are visible to others",AMD,2026-01-05 19:46:46,1
AMD,nxyg4nt,"Agree 10000%   Find the other AMD related and general hardware subs, they're much more interesting to read and post in than this one",AMD,2026-01-06 05:08:21,1
AMD,nxbgvn9,I can set it to consume 70w less and doesn't loose any meaningful performance,AMD,2026-01-02 20:45:26,25
AMD,nxbeohs,"That is huge regardless, mostly when even the XT version can surpass 300w, and if electric bill is something to be concerned about (depending on region), well the price adds up to higher levels...rx 9070 is pretty efficient and the best GPU, XT in case you need more power but just like 10% more.",AMD,2026-01-02 20:34:35,9
AMD,nxc8714,"If you go mid to high end gpu, then you better afford a mid to high end psu. I'd argue Psu is more important than gpu if you want some future proofing",AMD,2026-01-02 23:01:40,4
AMD,nxbyu78,"And there are also two fan versions, which XT does not. I don’t regret it.   Well, given that I have a 1080p machine, I do regret that I bought too much graphics card for my purpose. I could’ve gone with a 9060XT.",AMD,2026-01-02 22:13:04,5
AMD,nxbkg5g,This is the reason I chose the 9070 over the XT. Same reason I chose the 6800 over the 6800XT,AMD,2026-01-02 21:02:50,2
AMD,nxcp6oc,"It almost got me, because I knew I was thermals limited, but I figured an undervolt and power limit would give me enough extra performance and headroom to be worth $50...  ...and then I installed the thing, and it runs cooler than the 200ish watt card I used to have that weighed at least three times as much. So I can easily run it at full temp.  The Powercolor reaper has an incredibly good cooler on it. It may be small, but it is mighty.  ...and probably the entire reason for that is the flow-through design, so, y'know... Apparently my whole case's airflow is just a lot better now, because my CPU temps dropped too.",AMD,2026-01-03 00:34:57,1
AMD,ny0b2xd,"Very few people walk into a shop thinking about the power draw envelope that they'd like to buy within, lol",AMD,2026-01-06 14:03:09,1
AMD,nxbujzw,"It's pretty much the same card, you can flash the 9070 to the xt. Power delivery, heatsinks and boards are all the same.",AMD,2026-01-02 21:51:46,2
AMD,nxeu7tb,I compared non XT and XT prices locally and XT is roughly 50-60€ more expensive than same cards non XT version.,AMD,2026-01-03 09:32:13,0
AMD,nxc62in,AMD has plenty of motivation to make the same kind of allocation changes as Nvidia.  Prices will inevitably go up as well.  Nobody is spared from the RAM problems.,AMD,2026-01-02 22:50:25,5
AMD,nxef2p5,AMD will do the same.,AMD,2026-01-03 07:20:30,1
AMD,nxehpuk,"It says ""reportedly"". They left that shit outta the link but then you see its still a rumor.   Besides, with higher prices on electronics in general, people are going to be buying less because everything else is more expensive.   Both companies will likely cut some amounts....because they cannot ORDER enough memory. It's not really by choice. The market is there with more demand than supply....",AMD,2026-01-03 07:43:12,1
AMD,nxch9f7,"> Obviously some people wont bother, but it really is braindead easy and take seconds.  You haven't met the average end-user if you think it's that simple. Even with Wattman (assuming that's still a thing on Radeon). The average end-user can install an app and maybe a web browser, anything beyond that? Or anything that needs reading? Forget it.",AMD,2026-01-02 23:51:24,2
AMD,nxbf45w,"To be fair Andromeda was a decent game, just much inferior to it's predecessor",AMD,2026-01-02 20:36:45,5
AMD,nxdiub0,> Radeon prices were much better in Europe  Seems to frequently be the story. Any time I've seen someone gushing about the prices it's either been portions of Europe or in range of a microcenter. Otherwise a lot of the time it's frequently too similarly priced to Nvidia to actually choose it unless you're a big time Linux user or specifically protesting Nvidia.,AMD,2026-01-03 03:27:16,2
AMD,nxfnadc,Nvidia cards are overpriced by their MSRP already.  Not by any lies.,AMD,2026-01-03 13:22:09,-3
AMD,nxchyzn,"Valve uses AMD hardware in their products, is pushing Linux, and heavy support of more open software. They don't really have an incentive to fudge it especially in a way that disfavors AMD.",AMD,2026-01-02 23:55:24,3
AMD,nxfvmhf,"But you can check old news articles on the HW survey, for example https://www.techpowerup.com/265526/steam-hardware-survey-march-2020-intel-cpus-nvidia-graphics-cards-rising  It's better than no data at all. :)",AMD,2026-01-03 14:11:50,1
AMD,nxkzn2b,lol i got downvoted for saying its worthless - there's a video out by a youtube right now... think last months numbers are bugged.,AMD,2026-01-04 06:22:18,5
AMD,nxg03kh,"People should always bear in mind that the survey is 100% voluntary. If you do not deliberately opt in to the survey, you are left out of the survey. It's not something that happens automatically when you log on to Steam, as apparently many seem to think. IE, it's not representative of 100% of the people who use Steam, and never has been. Although in recent years, Valve has gotten better about it, the survey used to include any number of obsolete GPUs no longer being sold.",AMD,2026-01-03 14:37:07,4
AMD,nxy4fz3,It’s weird tho because valve uses all amd cpus and gpus for their hardware,AMD,2026-01-06 03:52:41,1
AMD,nxdh8kf,"The poor performance is due to not setting the correct GPU as the 'high performance' option in Windows. It takes about 10 seconds if you don't know what you're doing.   GPU passthrough is an OS feature, not a motherboard one. It only stops working from the motherboard side if you disable some PCIe slots, or if you don't have an iGPU in the CPU.  And, in fact, GPU passthrough can lead to substantially higher performance in games when the dGPU only has 8GBs of VRAM, since if the display is connected to the iGPU, then Dawn's memory requirements are allocated to system memory, reducing the VRAM usage on the dGPU by about 1 GB.",AMD,2026-01-03 03:17:29,5
AMD,nxcnei7,"It's not about motherboard support. We've had GPU passthrough at least since the Skylake era, in 2015 - if not longer. That's when I personally noticed it worked. Windows also began making better use of multi GPU set ups (integrated plus dedicated) where it would automatically select the correct GPU.   I don't believe *any* current day motherboard wouldn't support GPU passthrough. Nor would Windows 11/10 not support GPU passthrough.",AMD,2026-01-03 00:25:13,4
AMD,ny1n4ul,"Sure, but you can set the non-XT even lower.",AMD,2026-01-06 17:52:13,1
AMD,nxbocvr,The 9070 is more efficient at default settings.  The XT can be set to the same power settings for equal or better efficiency.,AMD,2026-01-02 21:21:45,12
AMD,nxblplx,It's not huge at all. People worried about the bill only have to downclock to their tastes. At the same power XT will always be faster.,AMD,2026-01-02 21:08:58,1
AMD,nxetro5,"Lets save electricity!  Oh no, why is electricity getting more expensive?  Lmao",AMD,2026-01-03 09:28:19,-5
AMD,nxcgn9j,"PSU and mobo are always undervalued, but the most important parts for system longevity.  Every machine I've ever had to repair for someone where components crapped out (that didn't involve end-users overvolting) involved a mediocre PSU and a cheap board.",AMD,2026-01-02 23:47:59,5
AMD,nxbzfdd,You'll be well situated if you ever want to jump up to 1440p at least. 😎,AMD,2026-01-02 22:16:05,2
AMD,nxfkmef,"Very much depends on the model.  Plenty of 9070XT's use higher end boards and cooling to handle the higher TDP.    This is why I wish these companies would stop using such needlessly high TDP's to begin with.  Card makers simply have to build to handle that minimum, instead of leaving that only for premium/overclocking models.",AMD,2026-01-03 13:04:59,2
AMD,nxfkvow,"Makes sense to splurge the extra if you can, but also, some people have a budget.  And sometimes it will be the case that even the non-XT is pushing at or beyond the limits of somebody's budget already.  It can be dangerous game to play the ""But if I just spend a little more"" game. :p",AMD,2026-01-03 13:06:42,2
AMD,nxcgy47,"Heck AMD already barely allocates production to GPUs, it's last on their list of things to use silicon for.",AMD,2026-01-02 23:49:39,1
AMD,nxd5pym,No “average end user” is buying a Radeon dGPU instead of buying Nvidia or far more likely a prebuilt.,AMD,2026-01-03 02:09:25,0
AMD,nxbfx0q,"It honestly wasn't terrible.   When I eventually came back & finished it a few years later, it felt like a somewhat-better-rendition of a Ubisoft game... in space.",AMD,2026-01-02 20:40:45,3
AMD,nxfmn19,"Had the best combat and exploration of the series.    Also its technical misgivings were hugely blown out of proportion.    Like not having perfect lip sync for an RPG with many thousands of voiced lines, before we had better automated tech for that kind of thing.  It was overall in a much better technical state than previous Mass Effects(especially the first one!). Lastly, the dialogue writing in that game was not actually any worse than in the original games.  People have nostalgia glasses for Bioware writing, especially Mass Effect trilogy-era.  It always had very stilted dialogue in plenty of occasions, and felt very sterile at times, too.  I know this cuz I replayed through the remastered trilogy somewhat recently.   Andromeda didn't capture the same magic for sure, but I dont think anything was ever going to.  I dont think any new one will be able to, either.",AMD,2026-01-03 13:18:05,1
AMD,nxev3ja,Some years ago I had problem where I never got hardware survey even when I changed my AMD GPU from one to other meanwhile my friend got hardware survey on his Nvidia powered laptop every 6 months.,AMD,2026-01-03 09:39:47,3
AMD,nxcuxbx,I don't disagree. Just saying this months data collection is not very good.,AMD,2026-01-03 01:06:54,1
AMD,nxji6di,Unfortunately I am opted in the only want I can get it is to force it to come up  I don't see that as very genuine I haven't ever needed to do that,AMD,2026-01-04 00:54:40,1
AMD,nxg4ehy,">Although in recent years, Valve has gotten better about it, the survey used to include any number of obsolete GPUs no longer being sold.  That means it's worse! None of the data is usable if it doesn't represent anything but cherry picks.",AMD,2026-01-03 15:00:27,-2
AMD,ny1us45,"My comment rather reflected in the performance not dropping by much while setting it much lower.    I only do this summer, it heats my small case good with full power lol Maybe it's time for a full tower case",AMD,2026-01-06 18:25:56,1
AMD,nxdjbs4,"I'm one of those people who cheaped out on the motherboard (but not the power supply after a cheap one blew with a very load bang). I've been using a $75 B350 for about 8 years now.  One thing I've noticed is that the boards VRM isn't very good. It can't supply more than 90 amps to the CPU, and the heatsink is too small to keep the VRM at an optimum working temperature (they get really inefficient when they're too hot).  That was never an issue until I installed CP2077. It literally crashed every time I played it and after a LOT of frustrating troubleshooting I learned it was simply a heat issue, i.e it was solved by positioning a fan over the VRM. So I'm pretty sure if I had spent another $10 on a board with a better heatsink I would not have spent all that time troubleshooting.   You can save a few dollars with a cheap motherboard but I think you have to be more attentive to it's weak points, and that takes time which really just negates the initial savings you made. I don't think most people need $200+ boards, but when it comes to the VRM something in the mid range is a real step above the budget choices.",AMD,2026-01-03 03:30:17,6
AMD,nxddisj,"You can somewhat cheap out on the mobo and be fine, but if you cheap on the psu, it's a bomb waiting to explode.",AMD,2026-01-03 02:54:59,3
AMD,nxbzz8r,That’s true.,AMD,2026-01-02 22:18:56,0
AMD,nxg5ycs,"Random sampling. I've had surveys pop on all types of builds Intel/Nvidia, AMD/AMD, and AMD/Nvidia.",AMD,2026-01-03 15:08:36,-2
AMD,nxe3ymp,"i saw a video that showed there methodology - it wasn't very confidence inspiring... honestly you have all the game data right, like how many people are playing what game etc...  just make everyone opt in with an option to opt out....  maybe they'll include all these bots and farmers too which would probably be embarrassing for valve",AMD,2026-01-03 05:49:43,1
AMD,nxgjqc9,How is it cherry picking? Its just a volunteer system.,AMD,2026-01-03 16:16:20,4
AMD,nxdodga,">(but not the power supply after a cheap one blew with a very load bang).  I've actually had a Seasonic blow up recently. First PSU I've ever had pop, tripped the breaker when it blew too. Shot a massive red ember. But to Seasonic's credit all the rest of the hardware seems to have survived unscathed, which is exactly what you want in that sort of scenario.   >You can save a few dollars with a cheap motherboard but I think you have to be more attentive to it's weak points, and that takes time which really just negates the initial savings you made.  I look at it like plumbing. Sometimes saving a couple bucks isn't worth the headaches later. Don't have to spend a fortune, but you want to hit that baseline quality where you don't have to worry about edge cases.   > I don't think most people need $200+ boards, but when it comes to the VRM something in the mid range is a real step above the budget choices.  Yeah for most people I'd say sub $200 is plenty, varying some with needs. I have bought pricier 300-500 boards before, but that was always for special setups. Either HEDT CPUs when that was still a thing, or just needing a lot of expansion options. My next board is going to be on the pricier end (for me anyway boards that cost what GPUs do is insanity) simply because I need a lot of m.2 slots.",AMD,2026-01-03 04:01:45,4
AMD,nxfjtyk,"Motherboard and PSU are two things people should never totally cheap out on.  Find a good deal sure, but dont just buy the cheapest thing.  Motherboards are almost assuredly the most common failure point for PC's.  So much on them that can fail.  And you really dont need to spend much more to get something decent quality.  Power supplies similarly aren't too expensive for something good.  And if they fail, they can fry your whole PC in an instant.  A good motherboard and power supply can last a long time, so it's a very good investment for what will probably only be like $40-50 higher total cost combined.  Feel like this should sort of notice should be a sticky on top of every PC building forum/subreddit. lol",AMD,2026-01-03 12:59:38,2
AMD,nxdhagd,"Realllllly depends on how the person defines ""cheaping out on the mobo"". Some people look at it as getting an matx with a budget chipset. Some look at it as getting the cheapest board they can find on aliexpress or ebay or whatever that may have some really subpar caps and iffy VRM configs.",AMD,2026-01-03 03:17:48,2
AMD,nxhmvwv,"Literally what I quoted. If they are omitting old cards you won't get a real picture. You actually never will, as it is voluntary, and someone said opt-in as well.",AMD,2026-01-03 19:16:11,2
AMD,nxih4ea,I dont think you understand what cherry-picking means.,AMD,2026-01-03 21:43:46,2
AMD,ny4dy78,"Another strix Halo mini PC, I'm shocked 🤯🙄",AMD,2026-01-07 01:47:17,2
AMD,ny4w0be,"Announced the thing which has been on sale for a year, cool.",AMD,2026-01-07 03:25:54,1
AMD,ny4w30d,"So what does this have that the Framework Desktop, HP, Dell, and any of a dozen other mini-PC Ryzen AI HX 395+ systems don't have?  It sounds like the same hardware.  It would be capable of running the same software.",AMD,2026-01-07 03:26:19,1
AMD,nxwcbr5,This just looks like a ryzen ai 300 rebrand no?,AMD,2026-01-05 22:10:58,135
AMD,nxwid9j,They STILL not putting RDNA4 into their APUs?,AMD,2026-01-05 22:40:43,71
AMD,nxw8sp7,"Brand new product, dGPU has no access to FSR4 upscaling, ray reconstruction, etc.    Nice.",AMD,2026-01-05 21:54:17,175
AMD,nxwbgpa,Wow amazing. 1.5 years later and they've only managed a slightly higher core clock and still on a GPU architecture released 3 years ago.,AMD,2026-01-05 22:06:51,57
AMD,nxwtol8,So this is the new updated hx 370?,AMD,2026-01-05 23:39:07,13
AMD,nxya4oz,Intel’s stuff looked a lot better. Load of better handhelds coming based on panther lake thats are looking amazing.,AMD,2026-01-06 04:28:06,8
AMD,nxwmg3d,60 TOPS seems like a good number. But does it really do anything now?,AMD,2026-01-05 23:01:13,3
AMD,nxyapk2,F RDNA3 all my homies hate RDNA3*.  *in post-2025 products,AMD,2026-01-06 04:31:52,3
AMD,nxyutkv,Hopefully they're a lot cheaper this time.,AMD,2026-01-06 07:04:50,3
AMD,nxyv7mi,"Is RDNA3 the iGPU version of skylake? 7840hs 780m -(clock bump)> 8845hs 780m -(more cores)> hx370 890m -(clock bump)> hx470 890m, looks awfully like what intel did from 6th to 9th gen, which allowed AMD to leapfrog them (hell b390 seems to have leapfrogged 890m already, by comparison iris xe which was pretty trash for gaming was launched at the same time as the 780m)",AMD,2026-01-06 07:08:16,3
AMD,nxwdg8k,"That shit is old and barely faster than my 7840U, the only redemption would be launching with FSR4 INT8",AMD,2026-01-05 22:16:25,12
AMD,nxw81d2,"""We have decided to lower our production of consumer-grade CPU's to increase production of the Ryzen AI line""",AMD,2026-01-05 21:50:47,7
AMD,nxw7khd,😪,AMD,2026-01-05 21:48:37,1
AMD,nxxxvub,Will this be faster in gaming than the 395?,AMD,2026-01-06 03:14:43,1
AMD,nxy7ue9,sooo...another round of handhelds incoming,AMD,2026-01-06 04:13:32,1
AMD,nxyti6n,Badly needs a better iGPU. A upgraded 890m would imho sell extremely well.,AMD,2026-01-06 06:53:31,1
AMD,nxyzt9p,8500 mhz memory speed?  on a ryzen chip?? stock??? When did this happen?,AMD,2026-01-06 07:50:15,1
AMD,nxzdqif,nothing burger for 9000 series hahahaha,AMD,2026-01-06 10:02:52,1
AMD,nxzkjke,The fact that it's still RDNA 3.5 and not 4 is baffling.,AMD,2026-01-06 11:03:30,1
AMD,ny0ay0m,Why must everything emphasize AI these days?,AMD,2026-01-06 14:02:23,1
AMD,ny12900,"60 TOPS NPU feels like such wasted silicon at this point, would rather have them put those transistors into more CUs or something actually useful for gaming. especially when the thing still cant even run FSR4 lol",AMD,2026-01-06 16:17:18,1
AMD,ny53tw3,So it's a 300 series refresh? Didn't zen 5 mobile come out first? Is zen 6 going to launch on desktop first(not counting server),AMD,2026-01-07 04:12:29,1
AMD,nxwspo8,Just slightly better binning.,AMD,2026-01-05 23:33:53,52
AMD,nxzo7lg,AMD is completely asleep in the mobile space. Unfortunately,AMD,2026-01-06 11:34:16,4
AMD,nxwntzp,"IIRC, they said they weren't going to bring RDNA4 to APUs. Next bump will be to RDNA5 (which might be UDNA).",AMD,2026-01-05 23:08:21,40
AMD,nxxgwnu,"No new silicon, this is just a rebadge.",AMD,2026-01-06 01:41:43,9
AMD,nxxt0ez,"Some say their engineers are busy working with samsung to integrate RDNA4 into exynos processors, so there's no upgrade on igfx ip for this gen.",AMD,2026-01-06 02:47:32,2
AMD,nxwb1rc,Seriously?,AMD,2026-01-05 22:04:53,32
AMD,nxwfotj,Does anyone know why they do this? I though RDNA 4 was straight up better per amount of silicon. And it's been out for a while too.,AMD,2026-01-05 22:27:25,16
AMD,nxweoxq,u mean iGPU,AMD,2026-01-05 22:22:28,13
AMD,nxwq0vx,This isnt for gaming tho,AMD,2026-01-05 23:19:40,3
AMD,nxwg12t,AMD confirmed that RDNA 4 isn't coming to mobile (i don't remeber where exactly i read it) but next graphic for mobile is RDNA 5,AMD,2026-01-05 22:29:05,23
AMD,nxx1fz7,"No, these AI coprocessors are mostly because Microsoft wanted them for windows but they aren’t doing anything useful.",AMD,2026-01-06 00:19:41,22
AMD,nxx1083,The 50 TOPS NPU of the AI 300 series is currently only used by most software to process the tokens of the prompt. Afterwards it switches to using GPU. I don't know though what it will be useful for when more support arrives.,AMD,2026-01-06 00:17:24,9
AMD,nxx2ln1,"It's 1-2% of a 5090, so it's not useful for most applications. Surprisingly slow given the amount of silicon area they spend on it.",AMD,2026-01-06 00:25:41,4
AMD,nxz3lej,You could use the OnnxRuntime and play around with local llm's. But thats about it. 99% of AI tools just call to a server and do nothing on your device,AMD,2026-01-06 08:25:30,2
AMD,nxzu62l,"I see this as a similar situation like the RTX 2060.   Back in the day, there was the 1660 that was pretty close in performance and was a fair bit cheaper.   DLSS didn't really exist back then, outside of some demos with hilariously bad image quality. It was more than unusable.   And there was really nothing else to use the tensor cores for.   Fast forward to today and that GPU is still usable because of the early investment in hardware.   Will this happen with NPUs?  No idea. But it could.   My personal problem is how annoying it is to access them. We're only now getting to a point where I can, at least on windows, just run inference on directML via onnx but we're still a ways off from ""it just works"".   If I build an app that ships with a small neutral network for inference, I really want it to ""it just works"" without having to debug a bunch of different inference backends.   And that's kinda required to make a ""killer"" app for these things in the future.",AMD,2026-01-06 12:19:35,2
AMD,nxz4t4v,It's an essential tax to make Satya Nadella feel warm and fuzzy inside. Surely you wouldn't prioritize spending the die space on something that would benefit performance right??,AMD,2026-01-06 08:37:14,1
AMD,ny5dd21,"Iris Xe is trash in relative to the modern day, but back in 2021 when it was first launched (2 years earlier than 780M), it was pretty decent, nearly rivaling AMD's Vega iGPU and even the Radeon 660M, but the Radeon 680M easily sped past them all.",AMD,2026-01-07 05:15:15,1
AMD,nxwqqqg,AI!,AMD,2026-01-05 23:23:26,8
AMD,nxw8hzt,"This isn't strix halo or instinct related, this is their consumer product.",AMD,2026-01-05 21:52:55,13
AMD,nxy9wbw,Not really I think. The clock bump for the CPU could give you a few FPS.  Edit: Oof. I thought they were at least releasing a 495.  With the 475 at 16 CUs it’s a definite no.,AMD,2026-01-06 04:26:36,4
AMD,nxz3f7a,the 16 CUs are already bottlenecked by the RAM bandwidth,AMD,2026-01-06 08:23:51,2
AMD,nxz8yzj,"it's LPDDRX, not DDR5   They are not the same with bus and bankgroups   Even 7200 DDR5 will be faster than 8533 LPDDR5X",AMD,2026-01-06 09:17:26,2
AMD,nxwyk5l,"It’s a new stepping, rather than just better binning",AMD,2026-01-06 00:04:37,35
AMD,nxwok6b,Wonder how many years that will be then.,AMD,2026-01-05 23:12:03,21
AMD,nxypfpv,no such thing as UDNA ( go read the entire interview and evalute statements within context ),AMD,2026-01-06 06:19:13,-1
AMD,nxwdgny,Yes. AMD still has not indicated if they are going to release FSR4 for RDNA3.5 and older GPU's. These new AI proecessors with RDNA3.5 can only use FSR3.1.,AMD,2026-01-05 22:16:28,73
AMD,nxwiz85,AMD has been on record for a while now that they are not going to be putting RDNA4 into Mobile products . They're waiting for rdna5/udna1,AMD,2026-01-05 22:43:48,33
AMD,nxzcf31,This APU is relevant for thin and light gaming class,AMD,2026-01-06 09:50:31,2
AMD,nxx8zqy,Even the copilot app itself is just a web app and purely processes on the cloud with zero effect from the NPU.,AMD,2026-01-06 00:59:12,15
AMD,ny1blc9,It can blur a video using the NPU. Still eats up my CPU resource though..,AMD,2026-01-06 16:59:44,1
AMD,nxxsq6h,Lol,AMD,2026-01-06 02:45:59,2
AMD,nxx89mn,"In other words, kaby lake",AMD,2026-01-06 00:55:17,23
AMD,nxzh4yh,Did they fix the slightly lower IPC (than Krackan Point) issue?,AMD,2026-01-06 10:33:46,2
AMD,nxwy8yu,"Probably late 2027, if not early 2028.  Zen 6 and RDNA5 consumer models are slated for mid- 2027. So usually new APUs are 6-9 months after that.  Was hoping for Steam Deck 2 with FSR4 and updated APU. It’s starting to show it age after all.   Guess I am waiting for that.",AMD,2026-01-06 00:03:00,25
AMD,nxxaqc7,Medusa Halo is supposed to be released next year (2027) and is rumored to have RDNA5 graphics.,AMD,2026-01-06 01:08:31,5
AMD,nxy7y1a,The Phawx already said 2027 in his review video for the GPD Win 5,AMD,2026-01-06 04:14:10,1
AMD,nxwehm5,That’s so shitty. AMD never fails to miss an opportunity,AMD,2026-01-05 22:21:29,68
AMD,nxxugxg,"Meanwhile, Nvidia just announced DLSS 4.5, bringing 2nd gen transformer models to RTX 2000 GPUs.",AMD,2026-01-06 02:55:31,13
AMD,nxxtkq4,"To be fair, i don't think the iGPU in these chips are meant for gaming...   Its a shareholder bandwagon CPU.",AMD,2026-01-06 02:50:35,5
AMD,nxz2a4q,This is just damaging to their brand at this point.,AMD,2026-01-06 08:13:09,0
AMD,nxwn7pv,but why?,AMD,2026-01-05 23:05:11,10
AMD,nxz675x,"this is also monolithic lol  monolithic is not harder to scale until you reach the reticle limit, which is like 2x the size of the 9070 XT",AMD,2026-01-06 08:50:35,2
AMD,ny2zpyo,and might make mini pcs and laptop a lot closer to desktops while sucking way less power  just built a 9950x3D workstation but was left with some bad taste in mouth.   i'm going mini pc+dgpu on docking station if i've managed to overspec my music workstation,AMD,2026-01-06 21:32:52,1
AMD,nxxf80f,"Right, there’s a few cool things it could be doing but all of the real copilot dogshit is going to the cloud.",AMD,2026-01-06 01:32:43,5
AMD,nxxbma2,"Kaby Lake weren't that bad, at least they added HEVC support to QuickSync, this is more like Kaby Lake to Whiskey Lake but worse (bc clock improvements here barely matter whilst everything important is basically the same as it had been)",AMD,2026-01-06 01:13:16,21
AMD,nxx5jrh,"I'm hearing 2029 before RAM pricing regains any sort of sanity.   So, not too upset about this wait. I'm gonna be on the sidelines hoping my PC doesn't die for the next 4-5 years.",AMD,2026-01-06 00:40:56,9
AMD,ny0hf4f,> That’s so shitty. AMD never fails to miss an opportunity  The older cards lack the hardware to *effectively* use FSR4,AMD,2026-01-06 14:37:22,-3
AMD,nxz78gz,"Remember when all the Nvidia people were saying that AMD cards didnt have tensor cores, well this is the result.  Yes, you can run FSR4 on a different path and get it to work, but not perform. Maybe on the very highest end RDNA3 cards it would be ok but on a dGPU's, no.",AMD,2026-01-06 09:00:25,6
AMD,nxxgrl2,"Because validating a new processor die is very expensive, so they’re saving that money. They didn’t have RDNA4 ready for the initial release, and they’re not making an update now,  Because let us be clear here - it is the same silicon as AI 300 series. Krackan Point and Strix Point rides again.",AMD,2026-01-06 01:40:58,21
AMD,nxwpzpa,I imagine an artificial constraint so people buy their rdna 4 GPUs. A mini pc with rdna 4 would be a killer product that many people would buy over building an rdna 4 system.,AMD,2026-01-05 23:19:30,3
AMD,nxwspyo,Best guess is because they're directing as many GPU dies as they can into the Enterprise sector with the AI boom.,AMD,2026-01-05 23:33:56,1
AMD,nxxj3xs,On my “AI laptop” I tried a few things and only some webcam effects I actually kinda use,AMD,2026-01-06 01:53:36,3
AMD,nxxg70m,"Skylake had HEVC support. What Kaby Lake added was 10-bit HEVC support and a few other bits and pieces (I think VC1 decoding?). It was very much a bugfix release, as Skylake already had support for the most common case of HEVC encode and decode. This is like Kaby Lake Refresh (which only repaired the HDCP 2.2 implementation and increased clocks).",AMD,2026-01-06 01:37:55,14
AMD,nxx0g7p,"Not really. AMD is doing very well in terms of the CPU market, console collaborations and AI hardware, but not very consumer friendly right now.",AMD,2026-01-06 00:14:31,13
AMD,nxz64p3,"Why the downvotes? I'm more of an AMD fan than I am Intel, but Lunar Lake alone had superior graphics and battery life (admittedly, with soldered RAM, which I dislike). If Intel is serious with Panther Lake, they might actually surpass AMD again, and that's not even taking into account that I only find one AMD laptop for every five Intel ones, or that the Intel laptops have thunderbolt or better hardware.",AMD,2026-01-06 08:49:57,4
AMD,ny14tni,But why does new products also lack the «hardware» to effectively use FSR4. It’s anti-consumer.,AMD,2026-01-06 16:29:05,6
AMD,ny15y82,"While RDNA 3 isnt quite as good at generating FSR4 frames, it still can and the improved image quality is worth the loss in frame rate from fsr 3.1",AMD,2026-01-06 16:34:14,1
AMD,nxz63ks,>it is the same silicon as AI 300 series  I missed that part,AMD,2026-01-06 08:49:40,1
AMD,nxz7ghc,"No, it's because RDNA4 just didnt perform very well and they want to focus on RDNA5 / UDNA1 because they are merging consumer and datacenter GPU architectures.  There's is no point doing all the work to integrate RDNA4 dGPU's when it is a dead end. May as well just push on with the next gen and do the work with an arch that is going to have legs.",AMD,2026-01-06 09:02:34,4
AMD,nxww84k,"My best guess to add on is that Sony and MS don't want them to drop an APU that is superior to current gen consoles and an affordable RDNA 4 APU would easily do so, even with half of the CUs of the 9060... Which is odd, because they already did that with the Vega APUS of Zen 2/3 versus the PS4/Xbox One, but I suppose at the time neither company thought AMD would make even faster APUs.",AMD,2026-01-05 23:52:29,4
AMD,ny3btye,Tbf Kaby Lake-R was important on mobile... first U processors w/4 cores,AMD,2026-01-06 22:29:29,1
AMD,nxx8ftf,In laptops they’ve been stagnant at 20%-25% market share since Renoir.,AMD,2026-01-06 00:56:12,7
AMD,nxzaj32,"> that's not even taking into account that I only find one AMD laptop for every five Intel ones, or that the Intel laptops have thunderbolt or better hardware  Sadly, I think that speaks more about how sticky and entrenched Intel's decades of corporate partnership, mindshare and incentive programs are and not necessarily an indicator of a better product.",AMD,2026-01-06 09:32:32,-1
AMD,ny1edgg,"Because those ""new"" products are really several old products in a trench coat.",AMD,2026-01-06 17:12:27,2
AMD,ny2lam0,"Possibly because there's a hard lower bound to performance of that new hardware to be made useful - AI upscaling that takes a big proportion of the frametime is is a hard sell.  Even Nvidia uses a massively cut down version of dlss on the switch 2.  That doesn't explain the strix halo, though, but if that's a lower volume part it might not be high on the priority list to replace.",AMD,2026-01-06 20:26:43,1
AMD,nxwz00y,Nah its just AMD being AMD and continously shooting themselves in the foot.,AMD,2026-01-06 00:06:55,4
AMD,nxzk722,Laptop market share is harder to move as it's not reliant on them. The partners likely have incredibly long contracts that would cost a lot to break out of them; the retooling would also likely be a supply chain nightmare,AMD,2026-01-06 11:00:31,1
AMD,ny1n4cj,And? Still a missed potential,AMD,2026-01-06 17:52:10,1
AMD,nxx0qls,Especially when Intel Xe igpu has caught up this gen albeit with poorer drivers  AMD resting their laurels,AMD,2026-01-06 00:16:01,4
AMD,nxzzygi,It's been years.,AMD,2026-01-06 12:58:40,2
AMD,nxxxogm,"AMD is actually really behind. I used to work for them and left because of the corporate heads. Whatever nonsense they are launching won't be backwards compatible and competitive unfortunately, same for CPU.",AMD,2026-01-06 03:13:33,-6
AMD,ny08j54,Its a company that only knows how to shoot themselves in the foot what else do people expect from them.,AMD,2026-01-06 13:49:02,1
AMD,ny54fcx,"Would be pretty interesting if this one introduces CopprLink/CDFP, unlikely but would be cool if they did.",AMD,2026-01-07 04:16:13,1
AMD,nwsqn80,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-12-30 20:59:51,1
AMD,nwstlkq,"Wouldn't be surprising if AMD treats this as a ""stepping"" and just silently discontinues the 9800x3d, unless yield/binning still leaves a lot of lesser chips",AMD,2025-12-30 21:13:44,159
AMD,nwu95kn,Bring back 5800x3d!,AMD,2025-12-31 01:45:41,34
AMD,nwzwsg3,The gaming market's going to be s*** when everything is completely overpriced for the next 5 years years. So but they can pric  this at whatever they want but it's not going to matter much if you can't afford ram or storage.,AMD,2025-12-31 23:27:28,5
AMD,nx5f0g2,Will upgrade my 7800x3d and give it to my son,AMD,2026-01-01 22:07:09,3
AMD,nwvdg51,I just got a 9800x3d and it runs at 5400 with PBO I wonder what this will do,AMD,2025-12-31 06:04:31,1
AMD,nx73efp,"With RAM, storage and GPU prices skyrocketing, these are going to be a tough sell.",AMD,2026-01-02 04:00:48,1
AMD,nxmq1ez,"I suspect this is trying to aim at those still on 7000-chips, because no one on a 9000 will get this.  The RAM pricing is gonna make this a very tough sell to anyone on anything older.",AMD,2026-01-04 14:39:47,1
AMD,nwxo1x0,I want to know when this is launching...upgrading my GF to my 7800x3d from her 3700x and i want the 9850x3d,AMD,2025-12-31 16:19:07,1
AMD,nwumi1u,"as much as i want this to be true, there a part of me saying i hope not, because i just bought this shit. 😝",AMD,2025-12-31 03:03:17,0
AMD,nwv3net,"9850x3D is 108mb 3D cache not 96 !! this chart is wrong, videocardz should check twice before posting their crap.",AMD,2025-12-31 04:52:50,-4
AMD,nwuepjz,I shouldn’t have to get a 9950 to max the amount of cores of a 5 year old intel processor. Gimme cores AND power,AMD,2025-12-31 02:17:44,-10
AMD,nwt84ne,"They will release it. Then after a month or two the 9800x3d gets a little cheaper to help push it out of channel, then this completely replaces it.   The 9850x3d and 9950x3d2 are basically all about pushing everything else of the channel preparing for Zen6.   Zen6 cometh.   With DRAM as high as it is, they are hoping people will upgrade.",AMD,2025-12-30 22:22:49,69
AMD,nwtepzy,"Its basically a guarantee that this is the plan, that's a common business practice and it wouldn't be the first time AMD has done this. But its not some underhanded tactic to trick consumers, its just a logistical choice to move old stock.",AMD,2025-12-30 22:56:42,4
AMD,nwsx5ov,"As long as the price is right, there is no problem. Especially because 9800x3d is overkill for most gamers",AMD,2025-12-30 21:30:26,20
AMD,nwuxqdj,"I’m still rocking one, such a good chip. Gonna keep it and throw it in secondary build",AMD,2025-12-31 04:13:23,9
AMD,nwuedf7,I really wish they would have brought back production too given the RAM crisis. Would be a good upgrade for my old 5600x system.,AMD,2025-12-31 02:15:46,8
AMD,nwx6tjh,"Couldn't have happened at a worst time, especially since DDR4 is still relatively cheap and easy to get. Built a starter PC for my little cousin's christmas present and was hoping to use a 5800x3d since I had leftover DDR4, but they were impossible to find for a reasonable price.",AMD,2025-12-31 14:51:24,3
AMD,nx21cxa,"You had infinite opportunities to buy 5800x3d or 5700x3d, you literally slept on it",AMD,2026-01-01 09:13:47,4
AMD,nwugtlt,Id actually like to see some 6 core x3d parts.   A 9600x3d would be a killer gaming cpu for a budget limited buyer,AMD,2025-12-31 02:30:13,0
AMD,nx0sa6k,How many times do you need to buy RAM per platform?  Just once or at most twice right if you want to upgrade later.  You can go through multiple CPUs on the same RAM if you want to with DDR5 if you buy a good kit to begin with.  By the time you feel like upgrading again you will need DDR6 anyways.,AMD,2026-01-01 02:46:54,1
AMD,nxbw4h3,"6ghz with PBO  edit: Actually, more like 5.8ghz",AMD,2026-01-02 21:59:29,1
AMD,nxpawx3,I was thinking the same. Since everything else is going up in price they had to release something to bring in income from the gaming sector. Not really a real upgrade for 9000 users.,AMD,2026-01-04 21:50:26,1
AMD,nxtekqy,I have a 7800x3d and I don't feel any need to upgrade with prices the way they are. With how things are people are only going to upgrade when you really need to...no more upgrade just because for the next few years,AMD,2026-01-05 13:49:55,1
AMD,nxz53t3,"Why would someone with 7800x3d get this? I went from 9600k to 9800x3d. I think most people are coming from even older CPU when they are doing upgrade. It's only small group of PC part enthusiastic who upgrade that often, because it doesn't make sense at all in gaming perspective. I do understand upgrading to the best, if you are doing some heavy work related processing with it, but these 7-series processsors are not for that use. In gaming there is very little MEANINGFUL difference between 7800x3d and 9800x3d.",AMD,2026-01-06 08:40:05,1
AMD,nwv6xlk,Toms hardware says 96,AMD,2025-12-31 05:15:45,5
AMD,nwx2vd0,"Single CCD Zen 5 X3D CPUs have 640 KB (L1) + 8 MB (L2) + 32 MB (L3) + 64 MB (v-cache L3) = ~104 MB total cache. Probably what you saw stated somewhere.  108 MB L3 cache makes no sense, where would the additional 12 MB (over 96 MB L3) come from? Another layer of V-cache (several layers would be possible according to AMD) would be another 64 MB. All the other cache is just the maximum in one regular Zen 5 CCD.",AMD,2025-12-31 14:29:22,2
AMD,nwwdq31,These cores are not directly comparable.,AMD,2025-12-31 11:35:31,7
AMD,nwtcb74,"With all the recent holiday sales, the 9800X3D has already been sitting steadily at around $100 less than the original price where I live. I can see this being the new normal price in the next while, then the 9850X3D comes out at around the same MSRP.",AMD,2025-12-30 22:44:05,20
AMD,nwuzvmw,"Ah you *think* it’s about pushing the last of Zen 5 out of the ecosystem before Zen 6.   In a “finewine” power move, AMD releases Zen 5-D4 AM4 alongside Zen6.   This move keeps existing contracts alive, though sadly this comes at the expense of the removal of any remaining Bristol Ridge support motherboards may have had remaining.   If you have an AM4 Bristol Ridge chip, don’t update to Zen 5 DDR4 Edition BIOS.   /s",AMD,2025-12-31 04:27:21,-1
AMD,nwvh208,Unfortunately a 9950X3D2 does not actually exist. The latency between CCD's makes it not viable and the benchmarks that were made seem to be fake.,AMD,2025-12-31 06:33:49,-5
AMD,nwv7sef,"this overkill is nonsense clearly u dont play any high refresh rate games, mmos or any games",AMD,2025-12-31 05:21:54,7
AMD,nwvpc3a,I guess that’s true since they are still selling the 7800x3d,AMD,2025-12-31 07:46:51,1
AMD,nwuzzkk,Happy 4 u man. I still am on my 3600. And it seems i will be using this untill my next upgrade whenever it will be (ram pricing allowed). I need desperate to upgrade to a 5800x3d/5700x3d but there is none out there. Only in 2nd hand market for stupid price like €350 for 5700x3d used no warantee. Which is no no....,AMD,2025-12-31 04:28:05,3
AMD,nwurq80,The 5800xt is a pretty solid upgrade too,AMD,2025-12-31 03:35:16,1
AMD,nx2dsit,I didnt slept on it. I was planning to upgrade my cpu with the next Gpu genaration. But Ramocalypse changed the plans.,AMD,2026-01-01 11:25:32,3
AMD,nwur2kc,"It’s coming for sure, they’re stockpiling the lower binned dies and release them when it’s plenty enough or might end up as another microcenter exclusive",AMD,2025-12-31 03:31:08,-1
AMD,nx0ttp0,And if your cou is $200 cheaper but your ram is $500 more you're not really getting ahead now are you?,AMD,2026-01-01 02:57:32,3
AMD,nxcpblh,"lol no, not with 1.3v max",AMD,2026-01-03 00:35:42,1
AMD,nxwh0ui,Lol no it won't.,AMD,2026-01-05 22:34:02,1
AMD,ny0y5q1,I was more thinking the people with non-x3d 7000 cpus,AMD,2026-01-06 15:58:31,1
AMD,nwvaqh1,Article on TechPowerUp from Dec. 17th says the same—96MB.    www.techpowerup.com/344188/amd-ryzen-7-9850x3d-listed-at-usd-553-slightly-above-ryzen-7-9800x3d,AMD,2025-12-31 05:43:32,3
AMD,nwxacf1,"I dunno what's a reasonable low for the 9800x3d, but I would consider moving to one if I found one for under $300. I have a 5800x3d now and game at 4k so it's not pertinent, but I wouldn't mind catching it at a great price. If I could get a decent mobo and the 9800x3d for $400 I'd be really tempted.",AMD,2025-12-31 15:10:18,3
AMD,nwv783h,"Zen5 would be a whole new microcode. That would be a pretty big ask. Jumping two generations on am4.  AMD would do it if it believed that say high profile markets like servers were at risk. IF mobo makers thought it would sell more mobos. Maybe. If they were like it has to be a x570/B550 board and they were making those and needed to sell them. People could keep their ram, and just update their mobo and CPU.   Once AM5 - Zen 6 is released with its IPC/Clocks and more cores, Zen 4 is less of a threat. If AMD wanted to put out 7800x3D onto AM4, they would be hugely popular.   They would just need to fit the Zen4 core with the older memory controller. This might be cheaper than restarting zen3 x3d production. It would still be significantly slower than a 7800x3d, but would be significantly faster than a 5800x3D and anything else on DDR4.",AMD,2025-12-31 05:17:51,3
AMD,nwvya4l,"Eh? AMD literally makes Eypc CPUS with up to 8 CCDS with V-Cache.  Its just not faster for normal desktop workloads like gaming or microsoft word or web browsing. Its been possible since March 2022 retail.  [https://www.servethehome.com/amd-milan-x-delivers-amd-epyc-caches-to-the-gb-era/](https://www.servethehome.com/amd-milan-x-delivers-amd-epyc-caches-to-the-gb-era/)   What changed is Registered DDR5 is unobtainable and costs between a car and a house. AI workloads can use V-Cache particularly in training or processes out side of LLMs. CFD, compression, also use V-cache across two dies with many cores.   AMD Is likely to launch V-cache on multiple CCD on AM5, and V-cache for Thread ripper.",AMD,2025-12-31 09:11:22,11
AMD,nwtsk12,Guild Wars 2 has never played so gloriously during raids I cannot accept this slander. I went from 80fps to...92fps!,AMD,2025-12-31 00:12:15,4
AMD,nwu2vhu,Cs2 and Tarkov would like to talk,AMD,2025-12-31 01:09:14,4
AMD,nww3iuc,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-12-31 10:01:29,1
AMD,nwvatk7,"You played wow with a potato back in my day. You dont need a nasa pc. And at a point, not even pros see the refresh rate difference",AMD,2025-12-31 05:44:11,5
AMD,nwv1py7,"Thanks man, and You could grab a 5800xt that’s an awesome chip. And would a big upgrade from your 3600, also I actually got this 5800x3d chip used but there was warranty because it was sold and shipped from Amazon and have had 0 issues with it. Used CPUs are more reliable parts to buy used I would say especially with am4 if the pins aren’t bent I would put down a lot money that it would work. So it either works or it doesn’t not much room for error but I get it.",AMD,2025-12-31 04:39:48,1
AMD,nx3k2u0,"Hope you learnt a lesson here, 7600X3D, 7600X, 7600, 7500f, 7400f were available more than 2 years and all these times you could upgrade.  Here's another tip I give you, upgrade your CPU to 5800 XT, 5800X or 5700X right now if you don't want to regret it next year.",AMD,2026-01-01 16:25:39,1
AMD,nx4kklo,"Ahead of what?  What is the baseline we are referring to in this scenario?  If the new CPU and new RAM aren't significantly faster than what you had for the money it would cost, don't upgrade obviously.  There are multiple generations of people that were already on DDR5 already.  It's not new technology.  AMD moved to DDR5 over three years ago, and Intel earlier than that.  There are a lot of people in the world that can upgrade to the newest AMD CPUs without buying a new kit of RAM and there's no reason for AMD to stop selling new SKUs in the meantime while they have quite literally no control over the DRAM manufacturing market and the problems it faces today.",AMD,2026-01-01 19:30:35,0
AMD,nwvce24,"Yeah its just 9800x3d but boosts to 5600 mhz instead of 5200, thats it... btw u should remove the amp link, use ublock origin it does that auto...",AMD,2025-12-31 05:56:15,6
AMD,nwy2c6h,Unfortunately not worth it at the moment with DDR5 prices the way they are.  The time to upgrade was when the 9800 first launched.,AMD,2025-12-31 17:30:09,7
AMD,nwwdjv1,Interestingly amd still hasn’t released zen 5 epyc with 3d vcache,AMD,2025-12-31 11:33:58,5
AMD,nwyap07,Ah yes EPYC = Ryzen yeah? You game on EPYC CPU's? Actually make a proper comparison and not this PC vs Server stuff.  And yes AMD will most likely release V-Cache on multiple CCD's for Ryzen when they have resolved the latency problems which is not until Zen 6. So a 9950X3D2 makes no sense at all.,AMD,2025-12-31 18:10:50,3
AMD,nx0q31u,"Enterprise LLMs arent using V-cache.  Maybe finance could use it for lower latency transactions, or some special scientific workload that I have no idea",AMD,2026-01-01 02:32:04,3
AMD,nwultco,My framerates in cyberpunk 2077 on my i7 9700k at 1440p was around 60fps but with my 9800x3d it skyrocketed to over 100fps in the benchmark alone!,AMD,2025-12-31 02:59:11,5
AMD,nwx6goj,"idk about recently now that they've sent a bunch of addons to the shadowrealm, but retail wow with a bunch of addons is an absolute PC destroyer. going from 5600x to a 9800x3d quadrupled my FPS just sitting idle in main city.",AMD,2025-12-31 14:49:29,5
AMD,nwv5qfs,I am gaming on 4k with my 7800xt. So 5600x wouldnt be.much of an upgrade cause i need to increase the minimum 1% low fps. 3600 generaly is holding strong for my games usage. As fir the second hand picks i was on about i mean markets like ebay aliexpress allibaba. Etc... If i could find that cpu to a logical price from a normal market or a trustworthy 2nd hand retailer i would pull the triger.,AMD,2025-12-31 05:07:15,1
AMD,nx4ff4b,I ordered 2day a 5700x3d (tray) for 310 Euros from a vendor from Aliexpress. I will receive it 1st week of February...,AMD,2026-01-01 19:05:01,1
AMD,nwvjim2,"Sorry, I was on mobile. I always use Firefox + uBlock on desktop. I have FF + uB on mobile but I kept chrome as default for no good reason. I'm going to switch my home screen browser shortcut to Firefox now.",AMD,2025-12-31 06:54:24,2
AMD,nx03cvh,"honestly, this is already a hot chip, I don't think the extra 400mhz will be worth the extra heat but that's just my take (I am going off based on how the 5800x3d temps jumped compared to the original 5800x)",AMD,2026-01-01 00:07:17,2
AMD,nxvddyb,"im so tempted to get this and jump. But, just cant i think waiting for AM6 would be better and ride the AI memory bubble and hope for the best.",AMD,2026-01-05 19:28:29,1
AMD,nwzftcj,They said they don't have any plans too either btw,AMD,2025-12-31 21:48:33,2
AMD,nwv6ggu,At 4k I definitely get that brotha good luck with the build watever you do  🤜,AMD,2025-12-31 05:12:22,1
AMD,nx4gxec,Even better,AMD,2026-01-01 19:12:27,2
AMD,nx1gob3,"Even right now the extra 200  mhz isnt worth the extra power, at stock turbo boost with pbo 2 it uses like 1.1-1.15 voltage or smth like that? And then at +200 frequency (the max) uses like 1.3v, it's just not worth it unless u have the cooling to spare, which arguably this cpu is easy to cool regardless if it uses 100 or 150 watts, but in the latter case ur fans have to spin faster which make more noise, and as a speaker use, i hate noise. And i hate watercooling cuz pump cant be turned off and it has a coil whine.",AMD,2026-01-01 05:48:27,2
AMD,nxw0skh,"If you aren't already on AM5, I would probably leave it for now. AM5 is going to support CUDIMM, but everything is very expensive right now.  AM6 is a long way off. There is likely another 2 generations on AM5. DDR6 is also a long way off as HBM will take all the future high node capacity for AI.",AMD,2026-01-05 21:17:29,1
AMD,nx1i8c6,Milan X and Genoa X were probably niche tbh.,AMD,2026-01-01 06:01:59,1
AMD,nwv84d5,Thanx bro 🤜,AMD,2025-12-31 05:24:20,1
AMD,nx4ld5c,i just hope it will be legit. Because i am always stressed when i shop from the Chinese....Also it is tray so it will not have AMD warranty...,AMD,2026-01-01 19:34:34,1
AMD,nx2cgt6,"I agree, I have a noctua g2 on mine currently but my case fans drown it out, I just wish that pumpless thermosyphon which was teased last year makes it to the market eventually, I could replace the g2 and give more breathing room for my 5090 for cooling",AMD,2026-01-01 11:11:44,1
AMD,nx2d6hp,Coil whine is not what you think it is.,AMD,2026-01-01 11:19:07,1
AMD,nx2fdh5,What do u think i think it is?,AMD,2026-01-01 11:42:00,1
AMD,nwir9qe,When they are going to release the 8 core version of HX3D for mobile?,AMD,2025-12-29 09:50:04,7
AMD,nwpix6j,They don’t even make a standard 8 core non X3D desktop replacement cpu anymore (no 7745HX or 7645HX successor).,AMD,2025-12-30 10:24:15,3
AMD,nwnjd9j,Why is this being covered in the news as if it's a specific RDNA2 problem? It sounds like it's isolated to specific cards and people who let their GPU sag. It's almost 1:1 tied to heavy cooler cards.   I haven't had as much of an artifact from my 6900xt from xfx with a z support bar all these years later and that thing is massive.,AMD,2025-12-30 01:43:30,17
AMD,nwndav3,well that's good to hear they still honor cracked gpus,AMD,2025-12-30 01:09:15,3
AMD,nwpemdr,Is it still possible to send my AMD GPU in for out-of-warranty service? I feel like my 6900 XT is on its last legs.,AMD,2025-12-30 09:44:59,2
AMD,nxg5bbe,"Yes, I bought the reference AMD 6900XT...10.5""...because of an old case I was using at the time that wouldn't support the longer, heavier cards. It's now running in the wife's machine, still running fine.",AMD,2026-01-03 15:05:14,1
AMD,nwe24vp,Looking forward to seeing the Crosshair X870E Hero NEO board (just for curiosity since I already have the non-NEO version),AMD,2025-12-28 17:05:58,31
AMD,nwforp0,"> ROG  I can't wait for them to cost more than the CPUs going into them./s  Seriously though, I feel the chipset offerings for higher-end systems is lacklustre, especially considering the insane prices some of these boards go for.  X870 is just a B650E in fancier terms and X870E is basically X670E but with USB 4. I feel X870E in particular is bad as it takes up 4 PCIe lanes and provides less options to the user (you could always add USB 4 through a PCIe expansion card and basically have an X870E, if needed).  X870 may also be seen as a downgrade on X670 as it offers less M.2 slots due to having only one BIOS chip instead of two, like X670, X670E and X870E. Some X670 boards also received unofficial PCIe 5.0 support on the main x16 slot, which brings it on par with X670E.  I feel AMD should've changed the chipset uplink to PCIe 5.0 x4 instead of PCIe 4.0 x4 for X870 and X870E, as this would bring the bandwidth between the chipset and the CPU to parity with Intel's Z890 (runs at PCIe 4.0 x8). This would also enable the use of an extra PCIe 5.0 NVMe SSD without leaving too much performance on the table as it will no longer be bottlenecked by PCIe 4.0 between the chipset and the CPU.",AMD,2025-12-28 21:48:20,22
AMD,nwfafra,"Well, just like Gigabyte released their (Elite, Pro, Master, Extreme) ""X3D"" refreshed versions... sound like Asus wants to do the same.  I got the Aorus Elite X3D board and im super happy with it. Curious what the NEO boards have to offer.",AMD,2025-12-28 20:38:18,9
AMD,nwepsqm,What does neo mean? Did they say anything about that?,AMD,2025-12-28 18:58:59,12
AMD,nwe6968,Will there be more workstation boards or is it just Pro WS B850M-ACE SE?,AMD,2025-12-28 17:26:26,5
AMD,nweqmg6,"Of course, right after I buy a new X870E board...",AMD,2025-12-28 19:02:46,4
AMD,nwj3rx8,"My preinstalled io shield on the B450-F is so loose that I can barely plug in a usb connector anymore. Can I fix it? Nope, a giant plastic shroud is blocking access. I used to swear by Asus, but wow their products are trash these days. I can't even control the LED's anymore because I need something called ""armory crate"" and not just the standalone ""aura sync"". Fuck asus and their crap software, which btw wont even install anymore either.",AMD,2025-12-29 11:42:37,3
AMD,nwfsiwd,"Finally, will be keeping eye on what these revised boards do.  MSI got the Max, Gigabyte has X3D, Asus has NEO (lol) and dunno ASRock yet but I'm guessing people have trust issues. MSI messed up with my godlike I got last year so they replacing it with a godlike x. Was buggy as hell so if the moment my replacement acts up I'm replacing it quick with gigabyte or this asus.",AMD,2025-12-28 22:06:56,2
AMD,nwfy7js,"I wonder if there is a new chipset for Z6 next year. Seems awfully unnecessary with a whole new lineup now, mid-generation. The 8xx boards were already an opportunity to improve designs and add slightly newer 3rd party chips like Wifi+BT along with USB4, two years after 6xx and Z4. And the 8xx boards were the *same* chipset hardware. What the hell is this then supposed to offer?",AMD,2025-12-28 22:35:49,2
AMD,nwi9fnk,Thunderbold 5 (80gpbs USB) or it didn't happen.,AMD,2025-12-29 07:04:44,2
AMD,nwr6g6u,there will be alot of expensive stuff announced at CES,AMD,2025-12-30 16:35:42,1
AMD,nwuavrc,"Btw, why there are no available X570 around? All I see B550… maybe there are no huge difference maybe but just my curiosity.",AMD,2025-12-31 01:55:34,1
AMD,nxc286v,"I wonder if they'll refresh the Apex, I want to get the next iteration of the Apex if they do since I want 2DIMM boards.",AMD,2026-01-02 22:30:27,1
AMD,nweqi35,"I really wanted the X870E Crosshair Hero, but the rear I/O was so disappointing at that price.",AMD,2025-12-28 19:02:11,7
AMD,nwgxy2x,there are several x870e boards which allow for the USB4 ports to be disabled (or shared with an m.2 or pcie slot). For whatever reason ASUS doesn't allow for this yet. If AMD somehow got their hands on some USB4v2 chips it wouldn't matter at all since those essentially operate at pcie 5.0 output anyways.,AMD,2025-12-29 01:49:56,7
AMD,nwhuans,At least you get a faulty PCIE slot and a warranty not worth the paper it’s printed on.,AMD,2025-12-29 05:04:31,1
AMD,nwnerfe,what did people expect when it comes to pricing?   Companies are for profit and not defending them only explaining the natural course of whats happened.   AMD took Intel's 2 generations per socket and kicked it to the curve.   Companies are selling 2-4times fewer boards since people upgrading chips aren't upgrading there boards anymore.  The only way to make the same profit was to price boards higher. People though they could have there cake and eat it to.   That isn't even taking into account inflation and all that.,AMD,2025-12-30 01:17:33,1
AMD,nwtmowt,PCH link is because the controller doesn't nearly saturate 4.0 bandwidth; in fact neither do the 5090 or 9070. The board already has 5.0 PCB traces so it wasn't just for RF or VRM reasons; they likely saved money on substrate design..  The other stuff is mATX and ATX problems  Anyone who cares to argue go look at benchmarks of a 9070 XT running on A620 next to 9070 XT running on X870E on OEM clock and voltage.. A620 is hard-locked x8 4.0 so they can save money on PCB shielding.. Widely-demonstrated negligible FPS avg. difference.......,AMD,2025-12-30 23:40:00,1
AMD,nwfoqzi,"Have you had any problems with the bios? I was thinking about picking up the Elite, but I haven't seen many reviews of the X3D version yet.",AMD,2025-12-28 21:48:14,4
AMD,nweyagy,It means 20% higher cost.,AMD,2025-12-28 19:39:01,46
AMD,nwezs4b,Neoprice. Basically they are increasing price to account for inflation until 2030,AMD,2025-12-28 19:46:11,18
AMD,nwf79ri,Small improvements/changes just to refresh the lineup so they can ask for MSRP prices again.,AMD,2025-12-28 20:22:28,10
AMD,nwg9kx6,"maybe new things like more usb in the io shield, better vrm (maybe +16 phases or more), led debug display, clear cmos button, better pcie button and maybe rgb logo plate  oh! and a +10%-20% price increase .\_.",AMD,2025-12-28 23:36:32,6
AMD,nwehhri,"I don’t think so, workstation motherboards usually takes longer. The Pro WS B850m just launched last month.   Saying this as I just bought the Pro WS B850m last month, but I picked it because of mATX & 10GbE. So unless the new range will have these I won’t be interested.",AMD,2025-12-28 18:20:32,5
AMD,nwettvr,workstation board with 24pcie lanes?,AMD,2025-12-28 19:17:39,3
AMD,nwf1ccn,Me2 :D Last week godlike edition x.,AMD,2025-12-28 19:53:38,2
AMD,nwfqkw7,"Well, my board came with bios version F2 and had a couple of random reboots in the first few days and then did some RAM tests. Turns out after updating to F4 it solved my RAM stability issues. Right now its great.  Just forget the base Elite and get the Elite X3D. It has better VRM, metal backplate, more ports, more headers, full bandwidth wifi 7, 5G Lan (instead of 2.5G).",AMD,2025-12-28 21:57:12,4
AMD,nwfcg3u,The X is MSi’s GodLike refresh. You’re good.,AMD,2025-12-28 20:48:00,1
AMD,nwfr950,"Good to know, yeah the x3D seems just objectively better, and it's PCIE lane sharing split makes way more sense than the original board. I wonder if these new Asus boards will also get better lane sharing",AMD,2025-12-28 22:00:32,5
AMD,nwfkvcn,"It's identical to the Godlike, even. Only differences are cosmetic",AMD,2025-12-28 21:29:14,2
AMD,nwfsv98,Check [this review](https://youtu.be/DCzTATSHVMM) of the Aorus Pro X3D (which is basically the same as the Elite X3D just with a bit better VRM and a tiny RAM fan included). The X3D turbo mode 2.0 is very good. Gigabyte really did some magic with these X3D cpus.,AMD,2025-12-28 22:08:41,3
AMD,nxy85c1,That article claims it on par with the 4050 laptop. Jesus christ,Intel,2026-01-06 04:15:26,31
AMD,nxygoos,this is nice but the handheld market could use less ultra 9 and 7s and more ultra 3s.  the closer they can get to the nintendo 2DS XL in size while being under $400 the better.,Intel,2026-01-06 05:12:20,25
AMD,nxy9s8q,"Hopefully they deliver. Amd needs a shake up in the APU market, mostly the GPU side of it.",Intel,2026-01-06 04:25:51,22
AMD,nxywtjt,I am definitely looking to get a gaming handheld PC with PTL in it. Gonna cost a fortune probably but it's my first and intend to stick with it for a long time  The only thing that would stop me is if they skimp on ram... Which might be a very real problem,Intel,2026-01-06 07:22:44,2
AMD,nxz3n59,77% faster while using 80% more power.  I rather see power matched benchmarks.,Intel,2026-01-06 08:25:58,6
AMD,nxzqm0j,"happy about more handheld focus, genuinely have put in more hours on my steam deck than my pc setup this year. i have my eyes on ARM going forward as well",Intel,2026-01-06 11:52:57,1
AMD,nxzumfv,Really excited to see these chips on handhelds.,Intel,2026-01-06 12:22:50,1
AMD,ny14mg9,fun to see them tout XeSS MFG on mobile gpus while the B580 still doesnt have it....,Intel,2026-01-06 16:28:10,1
AMD,ny3a7m6,"I’ll always want a really good handheld besides my PC. Currently own a Legion Go S and the Switch 2 so this is good for everyone. AMD stays on their toes and if intel is good and gives us a SteamOS native device, I’ll definitely try them next upgrade. The",Intel,2026-01-06 22:21:45,1
AMD,ny52cw8,"I'm looking forward to this, especially for a gaming handheld/mini pc device, having a gpu that is nearly capable of a RTX 4050 with that power profile could be game changing.  Plus all the existing Intel XESS features are icing on the cake, although support for that scaler is flakey. I'm just hopeful that more games will support XESS.",Intel,2026-01-07 04:03:23,1
AMD,nxyuq77,"Who cares, gives cheaper powerful GPUs for 2k, 4k gaming",Intel,2026-01-06 07:04:00,-18
AMD,nxyagzo,"I looked at the benchmark scores they put out and it looks pretty promising, apparently the 12XE core variant can score somewhere around 6300 on Time Spy graphics (https://www.notebookcheck.net/Early-Intel-Panther-Lake-iGPU-benchmark-impresses-with-50-faster-performance-vs-Lunar-Lake.1138923.0.html).  Intel is comparing a 4050 with low wattage (60W system TDP IIRC) so it's not as good as the full powered 4050 which scores around 8000 on Time Spy. On low powered 4050s though like the one in the XPS and other thin and lights it will compare pretty evenly. It also outscores basically any 3050 on the market since the highest powered ones get around 4500-5000 on Time Spy (which was already matched pretty decently by the old 8XE core GPUs)",Intel,2026-01-06 04:30:18,19
AMD,nxzdq9g,"It will depend on game to game basis. Some perform well on iGPUs, some tank hard due to memory bandwidth or whatever issue they have with it.",Intel,2026-01-06 10:02:48,3
AMD,nxznnuh,"Yeah, RDNA 3.5 lasted way too long. Admittedly, RDNA4 was a special case where they gave up on a mobile version in favour of getting UDNA ready but that's their own fault. Hopefully, this pushes them to make UDNA a mobile focused architecture as well and perhaps push more cores in igpus to take back the integrated graphics crown. Competition is very good for the consumer.",Intel,2026-01-06 11:29:47,7
AMD,ny16bso,agreed. these rehashed mobile chips with bad upscaling are well beyond their lifespan.,Intel,2026-01-06 16:36:00,3
AMD,ny086nv,They did against the 285h and it's a similar margin. Lunar lake has a power burst max wattage below panther lakes max sustain power here so they can't compare 1:1 properly,Intel,2026-01-06 13:47:07,5
AMD,ny0hpkm,82% of 890M with 30% more power draw with native resolutions,Intel,2026-01-06 14:38:55,4
AMD,nxyitdu,Panther Lake with an iGPU being able to play the newest games on medium/high settings in a thin notebook is pretty crazy,Intel,2026-01-06 05:27:44,13
AMD,ny2qsag,"And then UDNA has been nowhere to be found, probably coming next year. AMD completely blew their lead in the APU space.",Intel,2026-01-06 20:52:10,2
AMD,ny0v0jb,rdna? dude their vega lasted too long they got very complacent in their igpu department,Intel,2026-01-06 15:44:10,3
AMD,ny2t2go,"At this point, we'll be lucky if they even care about consumer cards at all.  It's AI all the way these days.",Intel,2026-01-06 21:02:33,1
AMD,ny55lgh,"Low to medium , not high",Intel,2026-01-07 04:23:33,1
AMD,ny2rjyw,"Eh, they will still have the best igpus on the market for a while. If they price the 388 well there is hope for them. But it's never going to sell the volumes Intel will.  UDNA is a major architecture overhaul on par with the the introduction of Ryzen and RDNA. A year is a long time but AMD only really needs a single gen to recover this gap if they so wish. But UDNA will need to be made with versatility and low power application in mind.",Intel,2026-01-06 20:55:40,1
AMD,ny2tvmr,"Unlike Nvidia they actually can make a lot of money relative to what they do right now if they get consumer marketshare. Iirc, Nvidias gaming revenue still beats AMDs enterprise earnings.",Intel,2026-01-06 21:06:18,1
AMD,nxtihn4,The biggest issue was that it was crippled by the ported meteor lake memory controller dies its that simple,Intel,2026-01-05 14:12:18,27
AMD,nxtjtfv,"Very good explanation, I own a 285k and I can say the stock experience is average, but the platform is great and coming from 14900k, the temps and power efficiency are impressive. Once fully tuned, 9000c38 A-die, 36 d2d and 34 ngu, gaming is on par with 14900k, but more efficient. I think nova lake will be amazing.",Intel,2026-01-05 14:19:43,32
AMD,nxvnqr5,">if you judge Arrow Lake solely by the frame rate counter in Cyberpunk 2077 at 1080p  Am I allowed to take into account that Intel went all the way from ""7"" to ""3"" lithography which is more than 2x improvement to achieve almost nothing?",Intel,2026-01-05 20:16:32,6
AMD,nxtkrwq,Yea with a z bord and the 200s boost and fast ram 15th gen is finally matching or supasing 14th gen,Intel,2026-01-05 14:25:03,5
AMD,nxti8go,It’s not necessary for consumers to buy an inferior product from a multibillion dollar company now backed by the global superpower’s government.,Intel,2026-01-05 14:10:52,39
AMD,nxtflnr,"https://chipsandcheese.com/p/skymont-in-gaming-workloads  None of the youtubers mentioned about core-to-core latency, improvements on the schedulers and execution ports setup.  The e-cores are really great in a 4 group cluster.",Intel,2026-01-05 13:55:51,18
AMD,nxugtl3,">We need to stop looking at the Core Ultra 9 285K through the lens of a typical generational refresh  That's all what consumers care about. They don't care if ARL on paper or on theory is some great reset. Perf, power, and cost is what's important.   >he 285K is suffering from the acute growing pains of decoupling the compute complex from the uncore in a way that creates a distinct latency penalty that enthusiasts are mistaking for regression.  It's not being ""mistaken"" for a regression, it quite literally is one.   The problem is also that AMD also has disaggregated their compute from their IMC, and yet has *better* latency on *less advanced* packaging.   >The controversy here isn't that Intel failed to push frequency; it is that they deliberately chose to execute a hard pivot away from the monolithic brute force strategy of Raptor Lake to a disaggregated chiplet design that prioritizes area efficiency and performance-per-watt over raw, latency-sensitive throughput.  Nothing about ARL's current design prioritizes ppw or area efficiency over RPL's design from a chiplets vs monolithic perspective. ARL isn't enabling higher core counts from going chiplets, that seems to be left to NVL according to rumors. And chiplets carries an area penalty over monolithic designs anyway.   >The removal of Hyper-Threading from the Lion Cove P-cores is the most contentious yet logically sound decision engineers could have made given the thermal constraints of modern silicon.   This makes no sense   >By removing the simultaneous multithreading logic, specifically the duplication of architectural state and the complexity required in the reorder buffers and schedulers to handle two threads, Intel was able to physically widen the core and increase the L2 cache per core to 3MB without blowing up the die size  SMT costs Zen 5 less than 5% in area btw. Just throwing that out there.   >The result is a P-core with significantly higher IPC than Raptor Cove  It's not though. This has been a significantly worse ""tock"" in terms of IPC uplift compared to something like SNC or GLC.   >but this raw single-threaded throughput is being masked by the interconnect latency.  Maybe gaming or some benchmarks are, but for the large part, no.   You can see this two ways, LNC's structural gains (core width, ROB capacity, etc etc) have smaller gains, percentage wise, over their predecessor versus something you would see in GLC vs SNC, or SNC vs SKL.  And also LNL's uncore is dramatically improved over ARL, and yet you see the same unimpressive IPC gains over MTL/RWC (which is the basis for ARL's mid mem fabric).   >the architectural overhead of the Foveros packaging means that ring bus latency is higher.  No? The ring runs at a different frequency than the D2D?",Intel,2026-01-05 17:00:52,12
AMD,nxu3n2t,Love my 285K rig.,Intel,2026-01-05 15:59:37,4
AMD,nxv9k2w,Designing a consumer product line around the niche of top of the line workstation is not a good decision for the average consumer.  I only once met a person with video production workstation that has more than an I7 or R7.  Previous gen I5 were amazing combo of multi core and single core . They rivaled amd r7 in preformance . Now a person wanting mid level cpu's would pay preformance tax due to it being made for the few people needing extreme amount of cores since those people would not buy dies that actually were designed for it like Xeon.,Intel,2026-01-05 19:10:59,3
AMD,nxtg5aw,"this is a lot of words, being honest this writing feels like ai (but in good sense, right to point without a bunch of bs)   i would agree this architecture is very much limited by d2d and without 200s or just pushing d2d can be kinda underwhelming in performance but for sure as first gen product is very solid and makes me personally excited for nova lake as it seems they plan to fix and improve on their current architecture  also i believe clockspeed difference was merely responsible to 13/14th gen failures which were caused by excessive idle voltage.  i would say adding DLVR was kinda smart as well as it reduces your power consumption significantly at idle especially with proper tuning.  Edit: fixed typos, autocorrect being silly with me",Intel,2026-01-05 13:58:56,9
AMD,nxtm1ov,as a person who made an upgrade from lga1700 13950hx ES laptop mutant to 265kf this cpu feels soooo smooth in win 11 despite on a huge 75 NS the ram latency and not ability to reduce latency this new E core with 800 score at 5 ghz in cpuz single core make using the pc so nice and ofc in single core game like cs2 I've loose a lot of performance with 13950hx at 5.6ghz I've had 980fps in dust 2 fps benchmark and on 265kf only 800... BUT in a multi core load game I got fps improvement but any way this upgrade is worth it for ppl who is not a pc enthusiast and don't want to tune the lga1700 CPUs,Intel,2026-01-05 14:32:00,3
AMD,nxto7vg,"It smells like slop in here. Shit post rather than a shitpost, congratulations.  I like my 265K. It performs well for my purposes and has reduced my personal power consumption considerably vs AM4. It is behind AM5 in my testing for broad term ""gaming"" when specifically chasing framerates, but compared to a 9700X or 9800X3D it is absolutely stellar at doing stuff while doing other stuff.",Intel,2026-01-05 14:43:46,6
AMD,nxtzro3,"AMD had a very similar experience with their first generation of Ryzen CPUs. One of the differences here is Intel had a competitive product prior to their architectural shakeup. Had Intel totally croaked for years and not been competitive in the CPU space the narrative would be completely different and everyone would be singing their praises.  Aside from that I think the biggest problem with this new architecture is simply there's little reason to buy in. It was only recently (if memory serves, I could be wrong on this) announced that the next generation of CPUs will share this platform and later ones will be on a new socket. When AM4 was announced we knew that it would persist for multiple generations and now with AM5 - why would you buy a board that will be obsolete when its time to upgrade when you could buy into a platform that will support your next 1-2 CPUs? Especially with the old intel socket performing just as well on a more mature platform, for most end users this first core ultra series just isn't worth investing.",Intel,2026-01-05 15:41:28,2
AMD,nxua32l,"""they deliberately chose to execute a hard pivot away from the monolithic brute force strategy of Raptor Lake to a disaggregated chiplet design that prioritizes area efficiency and performance-per-watt over raw, latency-sensitive throughput.""  That good for them, but we don't want that. I would take better latency over improvements that pretty much only save money to the companies.",Intel,2026-01-05 16:29:42,3
AMD,nxu27tg,"IMO corrupt tech sites and tech YouTubers are behind ARL’s failure for two reasons.   The first: AMD MCM processors without 3D cache are bad for gaming, and it’s hard to find this information in 90% of charts to warn intel about the consequences of going down this path, even though they should have done their own tests and experiments.   The second: in my own tests, the 265K was 15% faster than the 9700X on launch BIOS. Through BIOS updates, that gap extended to 20%, making it only 7%-10% slower than the 14700K. Yet on some very questionable charts, the 265K is shown as slower than the 9700X.   For experienced users, RPL CPUs’ temperatures can be lowered by 20°C by turning Hyper-Threading off and undervolting. For inexperienced users, ARL is better, as it runs 20°C cooler out of the box.   I believe that in 2026 we deserve raw, unedited video benchmarks that start from the desktop, show full system specs, then enter game-by-game benchmarking—no more charts with zero evidence to back them up.   When I tried to confront HUB with my benchmarks, I got blocked to cover up their lies and corruption. If they truly cared about which CPU is faster, they would share their in-game benchmark scores and discuss them in a scientific way, but that’s not their goal. The numbers go up and down for the highest bidders.   Lastly, I believe Nova Lake, with its expected 144 MB cache, will be faster than Zen 6 X3D by 10–20% as 9700x in real world is 20% slower than 265k and if both got the same IPC uplift NL will end up on top.   My own tests    14700k vs 9700x 30% faster https://youtu.be/1f6W6nkDS4o?si=chFUAeBWzybQopaL   265k vs 9700x 23% faster https://youtu.be/PuB0Dg-Jvyk?si=SmGJUFtYj-OjjpQh   Tech sites got same results that clearly show RPL and ARL CPUs are only slower than 9800x3d and faster than everything else from AMD    https://www.pcgameshardware.de/Ryzen-9-9950X3D-CPU-281025/Tests/Benchmark-Release-Preis-vs-9800X3D-1467485/2/     https://www.purepc.pl/amd-ryzen-9-9950x3d-test-recenzja-opinia-cena-wydajnosc-gry-programy?page=0,55",Intel,2026-01-05 15:52:58,5
AMD,nxts3mv,"For me.   I will go with Intel because of reliability (I know about chip degradation) but for me I bought 13900K from first day I undervolted using offset and but Max turbo frequency to 5.5Ghz . so my chip never tried to boost 5.9Ghz with crazy voltage.  Next is the most important is Everything just works. The boot is faster. wakes from sleep. its a more mature. I have another amd pc with r5 2600, where I found some stability issue.  Another one that is important to me. is idle power consumption. My 13900K can idle at 6 watts. Imagine 24 cores idling at 6 watts. where as 6 core zen idle at 15-20watts.(and that is low side many user reported 25+watt). its all because of chiplet.  I didn't test the new Arrow Lake as this uses tile. so i cant comment on idle power draw. if anybody has test, let me know.",Intel,2026-01-05 15:03:58,4
AMD,nxwaygs,Wall of slop.  Make your points in a more concise manner.,Intel,2026-01-05 22:04:26,2
AMD,nxmkdsp,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2026-01-04 14:07:07,1
AMD,nxxn31i,I compare Arrow Lake to Zen 1 or the first iterations of Ryzen. It is quite the techinal hurdle but it allows for future generations to bake very well. Intel was just playing catchup from having 14NM^6 nodes lmao.,Intel,2026-01-06 02:15:07,1
AMD,nxyburv,the problem for me the platform cost value over lga1700 isn't where it needs to be. namely I'm not interested in trading 32GB of DDR4 for just 8GB of DDR5.,Intel,2026-01-06 04:39:27,1
AMD,ny3llfj,That's one big wall of excuses,Intel,2026-01-06 23:17:53,1
AMD,nxtna7j,"You are correct that they needed to make a big architectural change as the 14th gen was clearly having issues and was being pushed too hard to make up for them, and ARL is the first step to that reset.  From their engineering perspective, it makes sense.  I wouldn't buy it though.  NVL... different story.",Intel,2026-01-05 14:38:45,1
AMD,nxvbwve,"No 285k is not shaving off 80-100W from 14900k, Set 100W PL1=PL2 on both and then compare. 285k is not even that much better than 9950x in MT. No amount of words can explain ARL flop, it took intel 3 gens after rocket lake fiasco to catch up, just to waste all that effort on ARL. Show me any other silicon design with advantage of 2 nodes and a new architecture just to be slower than the predcessor.",Intel,2026-01-05 19:21:42,1
AMD,nxwb6x7,"""If you are buying a 285K solely for (1080P) gaming, you are buying the wrong product for the wrong reason. ""  Fixed it for you. I game in 4K and I didn't buy a Ultra 9 285K to play games in 1080P to get high(er) framerate at the expense of stuttering. The ultras are outstanding CPUs.",Intel,2026-01-05 22:05:33,1
AMD,nxwilo7,"Intel's latency problems have been around for a while now. Arrow Lake just threw gasoline on a fire that was already burning. [The 14900K had a latency of about 90ns for memory access, which is awful compared to the 10900K's 66ns and the 3950X's 73ns latency.](https://chipsandcheese.com/p/examining-intels-arrow-lake-at-the). The 285K sits at 106ns.  The 9900X sits at 82.43ns, so it seems like latency is going up across the board in general.",Intel,2026-01-05 22:41:53,1
AMD,nxtfm4y,I guess.,Intel,2026-01-05 13:55:55,-1
AMD,nxte2yd,No one has time to read all that,Intel,2026-01-05 13:47:04,-11
AMD,nxvicu7,This is a very long way of saying Intel has chose to prioritize competing with Apple’s SoC and ignoring gamer’s and PC enthusiast’s wants/needs.,Intel,2026-01-05 19:51:27,0
AMD,nxuwzmt,"I’m just waiting for Nova Lake and if it gets delayed into 2027 and if TSMC does the packaging for some chips there may be issues  with supply. TSMC does not like that Intel has IFS and they play dirty, real dirty.  Nova Lake flagship could be at or above $1000, I may go with Ultra 9 285K with the fastest pcie5 nvme and ram, by then hopefully ddr5 is accessible.",Intel,2026-01-05 18:14:53,0
AMD,nxtrgru,My same experience with both of my 265k systems. They have been extremely stable for me and very efficient. Never have to worry about temps and they perform well with a 5080 and 9070XT. Contrary to all the media rhetoric I enjoy gaming with them.,Intel,2026-01-05 15:00:40,12
AMD,nxu6pg9,Basically the same or even less FPS than a 9800X3D consuming 80 watts,Intel,2026-01-05 16:13:55,1
AMD,nxuzcbk,"Exactly, tuned 285k is just 2-3% away from max tuned 14900KS + DDR4 at 4300MHz CL16.  All Z890 boards are so cheap so I grabbed an APEX with a 265K. Only costed me $600",Intel,2026-01-05 18:25:21,1
AMD,nxvj13w,"Hi I'm sorry to ask but what does this mean?     9000c38 A-die, 36 d2d and 34 ngu     is that an app or something?",Intel,2026-01-05 19:54:33,1
AMD,nxvtdhm,"that is exactly the point, if it was just architecture we could live with it and consider it an scurve of innovation, but the process proves that this design is going nowhere",Intel,2026-01-05 20:42:56,6
AMD,nxwwv5w,"New process nodes don’t improve  performance on desktop processors when clock speeds and core counts stay the same. A 10nm monolithic ARL could have performed better and cost them less, although the newer process does improve power consumption which is essential for laptops.",Intel,2026-01-05 23:55:52,0
AMD,nxum4ri,"And nobody was saying you're required to. The entire point of the post was to say regardless of your thoughts and purchasing habits, ARL was deliberate, and even smart. Those who look for single metrics by which to judge ARL are missing the point.   Yes, if that single metric is all you care about, by all means go spend your money elsewhere, but ARL is a step sideways so future generations can take leaps forward.",Intel,2026-01-05 17:25:38,7
AMD,nxtkrhd,"This is way too true. I want a healthy Intel and AMD, but I'm also not going to act like Intel being now backed by the US Government and MAGA, as well as securing a well funded partnership with nVidia, doesn't make me feel a lot less like they need consumers to pity buy things from them to encourage innovation.",Intel,2026-01-05 14:24:59,4
AMD,nxtic59,Yes because they do core to cores transfers via their shared l2 rather than the l3/ring. It's sick. Skymont in general is so underrated for how enormous of a performance gain it was. They literally fixed all the ecore issues it's the pcores that flopped,Intel,2026-01-05 14:11:26,15
AMD,nxuiree,">In highly parallelized rendering workloads like Blender or Cinebench, the 24-thread Arrow Lake design is often matching or beating the 32-thread Raptor Lake parts, which proves that the removal of Hyper-Threading was not a net loss for total throughput  So matching perf with a last gen part, after you hit a double node shrink **and** a massive E-core IPC gain and a P-core tock too is fine?   >The ""rent"" paid in silicon area for HT was no longer worth the ""yield"" in multithreaded performance,  This was a mistake according to LBT himself.   >This implies that Intel’s next step must be an aggressive overhaul of the interconnect topology, perhaps moving towards a mesh or a more direct active interposer solution for desktop parts if they want to reclaim the gaming crown from AMD’s X3D parts  Moving to a mesh wouldn't help much, and Intel's mesh's have a reputation for being insanely slow on their Xeon parts.   How much more advanced packaging does Intel have to use to match the latency of AMD using iFOP?   > But if you analyze the architecture, the Lion Cove P-core is a marvel of width and prediction capability that is simply being strangled by the packaging logistics  It's not. LNC is both not that all that wide, all the ARM cores beat it in that metric, and the prediction capabilities of LNC is bad- it's a literal regression vs RWC (last gen) in accuracy. It's worse than the E-cores branch prediction accuracy. And it's much worse than Zen 5's as well.   >and the floating-point performance is stellar.  This specifically is not the case. While in previous generations Intel was very competitive with AMD in spec 2017 FP, with ARL vs Zen 5 we see an almost 15% gap.   >The 285K is the cooler, more efficient, strictly professional grown-up in the room that unfortunately forgot how to play games because it’s too busy trying to figure out how to talk to its own memory controller across a microscopic bridge.  Idk why you are trying to trivialize gaming when it pushes a huge percentage of the market, and is why Intel has been repeatedly telling investors they have lost the high end DT market.",Intel,2026-01-05 17:09:57,7
AMD,nxu08ua,"As someone who “writes like AI” in part because of a learning disability that made it hard for me to write, I tend to organize my thoughts very deliberately. Using lots of punctuation, dashes, etc is now often interpreted as having used AI, although I have no idea whether OP did or didn’t.",Intel,2026-01-05 15:43:44,10
AMD,nxviton,">this is a lot of words, being honest this writing feels like ai (but in good sense, right to point without a bunch of bs)  It doesn't point out a bunch of BS, it introduces a bunch of new BS that is just straight up, factually wrong.   >i would agree this architecture is very much limited by d2d  Not single core performance like he is implying it is.   Just look at ARL-H vs MTL-H for example.",Intel,2026-01-05 19:53:36,3
AMD,nxv2i2x,"Soon you will not be able to tell what is AI and what was before, where you were living in the Stone Age. Fooz better buckle their seat belts and get rekt, it’s about to get a Bit-Funky.",Intel,2026-01-05 18:39:29,2
AMD,nxzan7c,AMD Unboxed are not serious and should be ignored at every opportunity. They have a long history of doing what you've said they've done to you. They get in childish arguments on twitter when they aren't blocking people who have data that disagrees with their clear bias.,Intel,2026-01-06 09:33:39,1
AMD,nxu6lhk,I have one 13900k since day one... But like all enthusiastic guys I did some benchmarks and the voltages were a concern. So I tuned the bios after just a few hours of working.  3 years have passed and I have 0 problems. It's like a rocket 🚀 very fast and reliable,Intel,2026-01-05 16:13:24,8
AMD,nxzllb5,"Very nice, do you also use a contact frame?",Intel,2026-01-06 11:12:32,1
AMD,nxx7vlh,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:53:14,1
AMD,nxz46az,"Could also add 1440p into the mix, at max settings with ray tracing, the CPU is going to matter less and less.",Intel,2026-01-06 08:31:05,1
AMD,nxwmvqc,For sure.  I still use 10900k which benefits massively from memory frequency. When running 4500 cl16 it sits in 33-36ns territory. Lowest latency cpu around was 2020.  Doesnt hold up vs today's cpus but the user experience is great.,Intel,2026-01-05 23:03:28,1
AMD,nxxq0wj,"Really, 90ns on DDR5-3600, what else can go wrong here...",Intel,2026-01-06 02:31:08,1
AMD,nxuz1x1,"Are you using a B580, I’m wondering how it performs.  I have a 5070 OC and an army of Alchemist and Battlemage cards.  I may try to get a 9070 this year and OC / solder it to XT performance. There is a great deal of fake hype around the 9070 XT, Lisa and her army of gawkers really pulled out the hype train for the 9070 XT.  Many influencers (somebody who can post a video to snoozetube) received 9070 XTs for free so they could gimp primp them out to the masses. It has left a stale, rotting smell in my loins.",Intel,2026-01-05 18:24:04,2
AMD,nxwegrj,"Did you used just ""stock"" profile ie NGU and D2D at auto and Intels default profile? Or Intel 200s boost?",Intel,2026-01-05 22:21:22,1
AMD,nxuzm9f,"Both 14900K and 285K won't get frametime spike when the L3 Cache is maxed and no random stutter/issue.  If I want a good gaming experience, I would go for the 14600k instead of the 9800X3D. Way cheaper and works way better.",Intel,2026-01-05 18:26:34,0
AMD,nxvwkhz,"These are the bios overclocking settings, ram speed is 9000mhz, d2d die2die is 3600mhz, ngu chip fabric 3400mhz fully manually tuned",Intel,2026-01-05 20:57:48,4
AMD,ny0mjxi,> New process nodes don’t improve desktop performance when clock speeds and core counts stay the same.  Netburst was awesome!,Intel,2026-01-06 15:03:34,1
AMD,ny1yrwf,"> New process nodes don’t improve desktop performance when clock speeds and core counts stay the same.  This is correct. The node shrink itself does not give IPC gains, what it gives is energy efficiency and area. The reason why people associate node shrinks with better performance is because when they switch a processor to a newer node they typically increase cache sizes and/or improve base/turbo clocks.",Intel,2026-01-06 18:43:59,1
AMD,nxvi8ry,The problem is that MTL for all means and purposes should have been that test bed product. Or even lakefield tbh.,Intel,2026-01-05 19:50:56,5
AMD,nxxqpo4,"Nothing smart about it.. they just couldn't do any better than release a half unfinished product because the company is corrupt as hell, fully relying on US gvt injecting  tons of money that ends up in a few top manager pockets instead of being used for restarting the core architecture from scratch.",Intel,2026-01-06 02:34:57,1
AMD,nxubozd,"We were backed strongly by Biden admin too, the CHIPS act money was what Trump gave us, but demanded the stock in return instead.   Which I think is ultimately good for us American citizens. Too many times companies just got hand outs, even Bernie Sanders approved the Intel stock deal. https://www.reuters.com/world/us/us-senator-sanders-favors-trump-plan-take-stake-intel-other-chipmakers-2025-08-20/",Intel,2026-01-05 16:37:08,12
AMD,nxu3xx9,Brotha everyone is tryina get a piece of that maga pie. Or vice versa. It would probably be unlawful to go against maga on fiduciary responsibility alone,Intel,2026-01-05 16:01:00,2
AMD,nxtmb1f,This!,Intel,2026-01-05 14:33:25,5
AMD,nxv74ph,"On my 13600k I also see a shared L2 for each 4-core E-core cluster. Was there a regression between then and now that they've resolved? Or is there some hidden behavior where this shared L2 couldn't actually be used to core-to-core communication without going through the ring?  If you have a source with more info, I'd greatly appreciate it.",Intel,2026-01-05 19:00:02,2
AMD,nxvzfjz,"AI is the aggregation of all the rules and examples fed to it.  You write according to proper ""rules"" or clear organization (which is very much not ""vernacular"" level writing), then boom, you and AI aren't all that different.  It's bloody annoying to try organizing thoughts or points to be easily digested instead of a wall-of-text like you've been doing since Mavis Beacon taught you typing only to have people bitch about the style and ignore the content.",Intel,2026-01-05 21:11:12,2
AMD,nxv1yn0,There are so many of us lol,Intel,2026-01-05 18:37:05,0
AMD,nxzllwo,Do you use contact frame for the cpu?,Intel,2026-01-06 11:12:41,1
AMD,nxzluhl,"yes, i do",Intel,2026-01-06 11:14:44,1
AMD,nxv14y0,I do have a B580 but haven't paired it with the 265k cpus yet. The B580 is currently in a i5-13600 system.,Intel,2026-01-05 18:33:23,1
AMD,nxwoxfl,"Stock. One system uses a Z890 mb with 8200mhz cudimm and the other uses a B860 MB with 8000mhz cudimm. I tried 200S boost on the Z890 and it didn't benchmark much faster at all for the games I play, plus I had occasional lockups. It wasn't worth leaving it on for me so everything is at the Intel default profile.",Intel,2026-01-05 23:13:58,1
AMD,nxzc32p,Lmao,Intel,2026-01-06 09:47:24,3
AMD,nxv0qyc,"Lol, Userbenchmark guy making things up.",Intel,2026-01-05 18:31:40,10
AMD,nxw9ntc,thank you,Intel,2026-01-05 21:58:18,2
AMD,ny41irt,"Clock speeds haven’t meaningfully improved since 32 nm Sandy Bridge. The 2500K and 2600K could overclock to around 5.0 GHz, and modern CPUs still run at roughly the same frequencies. Core counts are also similar—Intel’s 14 nm 7980XE had 18 Skylake cores. Cache increases are possible on older nodes as well, so newer process nodes mainly improve efficiency these days, which is contrary to what most expect of them.  A 10nm monolithic ARL could have performed better at least in gaming on desktop.",Intel,2026-01-07 00:40:20,1
AMD,nxvyiba,"I suspect they just didn't have enough time to change anything. MTL releasing in Dec 2023, internal testing I'm sure yielding some set of data, and external reviews giving other feedback, even by the release of MTL, ARL has probably been in the pipeline for years and probably locked into certain design choices regardless of the feedback and testing.  Additionally I suspect that on mobile chips/laptop gaming rigs there's less focus on each individual part because  a) few people hardcore game on laptops b) the latency can be blamed on other things since laptops are a prepackaged consortium of parts and it's harder to isolate, and  c) therefore laptops tend to be evaluated as a whole rather than the individual pieces that comprise them.   Therefore the ""latency issues"" only became a massive kerfuffle when the offending cpu could be isolated and tested alone, and reviewers needed something to complain about.   That's even if Intel was considering latency as the issue everyone made it out to be. Intel could have looked at it and considered the latency worth the cost to improve in other areas and serve as intels seminal desktop entry utilizing disaggregated silicon. Then gamers came and lost their shit that the newest Intel chip deprioritized something that impacted their precious fps- even though the impact was ""the new one is approximately the same to maybe a little worse as last gen in most games"".   Notwithstanding the fact that the 200 series chips retain healthy gen-to-gen perf uplift and a _massive efficiency improvement_ in productivity and general computing, and boosting the NGU and D2D clocks (which are _very_ low from the factory, and can be done with the app that Intel _has specifically designed for tuning their chips_ and is freely available (XTU)) brings the gaming performance to ""approximately the same to a little better in most games"". Contrary to what some people may think these chips are not solely or even majority used for gaming and there are other use cases Intel can to think about/chose to prioritize.   To be fair, gamers and tech enthusiasts are the ones who will care the most and therefore have a disproportionately large impact on the kind of publicity the chip will get. So, was this intels smartest PR decision? Maybe not. But I think it was still a sound engineering decision, regardless of whether or not they had feedback or data on the issue, even if it didn't go over very well with their loudest customer base.  And this is all overshadowed by the fact that if you stuck even the most hardcore of gamers on a blind trial and told them to identify what kind of chip they were gaming on, I have a hard time believing any of them would be able to tell with any certainty which is which.   Once you get to 60 fps, the vast majority of people stop caring. Whats five fps going to do to radically change your gaming experience so much that it's unplayable? Which brings me back to one of my original points: If you care _that much_, you can go spend your money _elsewhere_.",Intel,2026-01-05 21:06:54,3
AMD,nxxrp83,Pouring free gvt money into Intel is only making the company high managers less interested in restructuring and improving the company when instead they can just keep going at snails pace while cashing in. Gvt money is only supporting growth of corruption inside the company and stalling progress.,Intel,2026-01-06 02:40:23,1
AMD,nxun1ug,"I'm aware of CHIPS.  Like I said, I want Intel to be competitive. I hope if their new layouts mature into something that can retake marketshare that the same care will be taken to preserve AMD. AFAIK, no one came to lend them money or sign large multifaceted partnerships at that time. This ""must save Intel,"" movement is something that is borne out of how big Intel is. They're too big to fail, it would have too many implications for the economy. It's not tit for tat, but as they are both American companies and AMD has significantly more invested in my country, I don't feel particularly motivated to help bail out Intel when they have the equivalent of the iron rice bowl rolled out for them at the moment. I'll buy what works best for my needs at the time I'm buying, as always, and if Intel can make a better gaming focused, with some imagery stitching on the side, processor than AMD can at the time I'm buying, that's the direction I'll go.  Ultimately the 200 series is what the Ryzen 1000 series was, but more stable as Intel has always had better support both internally and from vendors. I have no doubt Intel may catch AMD. My worry will be, what happens when they put their boots on AMDs throat, especially now that nVidia has more control over the situation than ever before?",Intel,2026-01-05 17:29:53,1
AMD,nxv6i3j,"It's so strange to me that we've hit a spot in consumer and enterprise computing where politics is now a factor. It's a different game when companies have to worry about that side, far too many consumers make decisions based on their politics when all that does is cause other issues. Obviously I won't go farther than that in a tech focused discussion lol, but I will say again that it's a different game and I don't think anyone wins if it becomes the norm.",Intel,2026-01-05 18:57:13,1
AMD,nxty6pw,I upped the tREFI of my memory in my MSI Vector with Intel Ultra 9 275HX and got some fps gains in Fortnite/Valorant/Hogwarts Legacy at 1200p,Intel,2026-01-05 15:33:56,2
AMD,nxv7w1f,Yeah pre arrow lake it was particularly shit as it would instead go through BOTH l2 and then to l3 to communicate between cores like a shitty ISP route to a game server   That is obviously worse than normal but core to core communication being done through even shared l2 is very rare so even without that quirk it's not expected  Go to chips and cheese.com they have a ton of information about this stuff. Like their skymont article in this case it details all the huge upgrades,Intel,2026-01-05 19:03:27,3
AMD,nxx7gwg,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:51:03,1
AMD,ny47bmi,No I don't need it... You need a 360mm AIO that's all... After that it's all about bios settings.,Intel,2026-01-07 01:11:04,1
AMD,nxzm8r1,which brand do you use?,Intel,2026-01-06 11:18:02,1
AMD,nxyej0v,"Thanks so in short:  You use only ""intel default profile"" and enabled XMP on your CUDIMM's right?   No further manual tweaks under NGU or D2D and RING values or any other critical tweaks pertaining to voltages no?",Intel,2026-01-06 04:57:20,1
AMD,nxypf2c,"What? The deal under Trump was for 10% of the stake in shares. The government can sell the shares at any time to get a return. Since the stock has doubled since, it looks like it was a great deal for tax payers.",Intel,2026-01-06 06:19:05,2
AMD,nxuzom9,"It's because AMD doesn't manufacture chips, we do. That's what the funding was for, to revive manufacturing leadership in the US not to save failing architectures.  And yes as a consumer buy what works best for your needs and budget. That's the best thing about Ryzen and AMD's resurgence.  I don't get your ""boots on throat"" comment, but to think AMD hasn't done anything mischievous in the past is, well a lot has happened between the two companies in 40 years.",Intel,2026-01-05 18:26:52,5
AMD,nxv19d7,"A strong Intel IFS is a strong US. Many people get disillusioned and deceived through all the cognitive dissonance on social media, especially gamers. Unfortunately they are easy to manipulate.  Many people are buying INTC in the US to retire on, we will see this more and more as we approach 2030. INTEL IFS has to succeed otherwise the US will be doomed in this century. Even India is getting into the semiconductor industry now and Intel is working with them. We need IFS to be on top, cream of the crop, I need a taste.  You are defeating yourself by getting wrapped up with all the geopolitical propaganda, go take a walk in the woods and get away from it all.  Checks are in the mail.",Intel,2026-01-05 18:33:57,-1
AMD,nxv1ubv,Which way is up...?,Intel,2026-01-05 18:36:33,1
AMD,nxw36zg,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.  Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2026-01-05 21:28:35,1
AMD,ny1tgj8,thermalright.   Any frame will do. get whatever is cheap,Intel,2026-01-06 18:20:04,1
AMD,nxx7x5q,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:53:28,1
AMD,nxv4op7,"Okay, so we've gone right off the rails of a logical discussion. I fail to see how we went from talking about a failing architecture, which I don't think of Intel's product line as, to the whole bailout discussion. Maybe because I said AMD didn't get bailed out? I think maybe you forget that AMD had to offload global foundries, I don't think anyone even blinked when that happened and it's likely because that was a different era.   My post had nothing to do with the bailout versus architecture (or saving it) , apart from mentioning that I don't believe as a consumer I should feel motivated to buy Intel over AMD, or any other American firm, at the moment.   To be very clear, as this is personal for you, I do own Intel equipment including an Arc graphics setup for one of my children. I'm not anti-Intel or anti-American at all.   I know the history between AMD and Intel fairly well. Oversimplifying, AMD as it is doesn't exist without Intel. AMD only grew because they were the most successful second source producer for Intel, and the most successful at riding the thin grey line between patient infringement and unique implementation of similar IP that kept them alive while everyone else in the X86 space either died or became irrelevant to the consumer or enterprise space. After Itanium, the story levels out with both companies becoming effectively unwilling AMD64 codependents, and I'm saying that humorously.   Intel would gladly own the X86 market outright. So would AMD. At the end of the day, we need the two driving innovation through competition. Even if the CHIPS Act, the government stock acquisition, and the nVidia partnership are solely aimed at bringing more, needed, chip manufacturing to the US, it could create a situation where that amount of leverage puts Intel into a hyper dominant position again in the near future. Honestly, I hope I'm wrong.",Intel,2026-01-05 18:49:13,2
AMD,nxwdvf9,"Hey sorry to interject like that, can you ask around what ppl in the team think the safe VCCSA voltage for raptor lake is beyond standard spec? You can dm the answer if u want. I'm from Russia so it's not like I'll go run RMA'ing this stuff just because you told me that info",Intel,2026-01-05 22:18:29,1
AMD,nxv1r9g,"Did you even read my post, or are you a bot? Seriously asking. I didn't bring up geopolitics. I live in Canada my dude. AMD has their graphics office still right beside the TO airport, and that's only a side comment. AMD is American too. My concern isn't about anything you just said lol.",Intel,2026-01-05 18:36:10,0
AMD,nxv9flb,65535,Intel,2026-01-05 19:10:25,1
AMD,nxv805h,"My initial comments were on, ""I'm also not going to act like Intel being now backed by the US Government and MAGA"" and what the funding was for.  Edit: and my response was about who also backed the funding.",Intel,2026-01-05 19:03:57,1
AMD,nxwhiw8,"All I can recommend and say is follow whatever guidelines you're given officially and make sure your BIOS is updated. All those teams work to make sure it's delivered to the customer. Otherwise it's all random, some parts can handle higher voltage, some can't.  The term silicon lottery is real and just a nature of small scale manufacturing, EM and quantum effects these days.  I will say, with our new CEO a lot of these customer issues are now streamlined internally. Used to be layers between engineering and customer interaction, so I expect better responses and reliability than before.",Intel,2026-01-05 22:36:31,1
AMD,nxv9vyk,"Ah, so you mean *lower* the TREFI then, from what JEDEC or XMP specified?",Intel,2026-01-05 19:12:28,1
AMD,nxv944s,"On that side, it'll depend on what the US does with the stock it has. Canada bought GM stock during the 2008 crash, and then quietly sold it off as GM recovered. If the US does that, I don't really see an issue.  The MAGA part was a half hearted comment, made to follow the weirdness of the whole situation and comment I was replying on as well. As I said in a separate post:  ""It's so strange to me that we've hit a spot in consumer and enterprise computing where politics is now a factor (speaking about how decisions beyond national security are now driven by politics and trying to be on the right side at any moment when it matters). It's a different game when companies have to worry about that side, far too many consumers make decisions based on their politics when all that does is cause other issues. Obviously I won't go farther than that in a tech focused discussion lol, but I will say again that it's a different game and I don't think anyone wins if it becomes the norm.""  I'm going to add, light heartedly, I do hope we see AMD/nVidia/Qualcomm/etc manufactured and final packaged products coming out of Intel Foundries one day. I'm not sure how or if it will work, but the situation seems dead set not to allow significant further nodes beyond 2 nm or packaging to occur outside of North America. If the US wants viable national chip production, Intel is the better option, I hope it works out in a way that maintains design level competition while meeting national security goals.",Intel,2026-01-05 19:08:59,1
AMD,nxwo9wf,"I get that. But it’d be nice to have a “this voltage is safe for 99% of the cpus and this voltage is the LD50 for the cpu”. That would probably improve customer relations but your legal team might be very, very unhappy with that lol.   Anyway cheers for the response, with the way things are looking up for intel, i might be buying some stocks soon",Intel,2026-01-05 23:10:36,1
AMD,nxvnm9c,JEDEC 5600 cl 40 kingston fury sodimm 2x 16 gb,Intel,2026-01-05 20:15:57,1
AMD,nxws02u,"I mean, you can just figure that out yourself and buy new CPUs till you get one working :P",Intel,2026-01-05 23:30:06,1
AMD,nxvpz7b,"Yeah, but what was TREFI before you lowered it?",Intel,2026-01-05 20:27:01,1
AMD,nxvvs3t,10000 ish,Intel,2026-01-05 20:54:09,1
AMD,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
AMD,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
AMD,nxyngsv,"Hello Intel\_support - I have just one important Question ( do you really work at company Intel , or you are just being knowledgable )  Why does this matter? - because I would like to work with Intel - I can teach your senior engineers to make Quantum Processors  PLUS I LOVE INTEL! ( that's the only Processor I swear in )  Currently I am using some i-5 from year 2018 - I have no issues with it.  I am not AMD hater - but AMD cannot compare to Intel. Class difference is BIG.  Intel is like Mercedez Benz- and AMD is like Fiat.",Intel,2026-01-06 06:03:18,1
AMD,ny2u31x,"With the crazy RAM prices, I'm looking to move to a 13600K or 14600K to keep using the 64GB of DDR4 from my ancient 7700K build. Do we users generally consider Vmin Shift Instability to be fixed at this point through the series of BIOS and microcode updates?  Related: Should I expect something in the range of a 10% performance drop from any of the reputable reviews, due to the fixes? Also, are efficiency-cores pretty much working as intended at this point, or is thread scheduling still a concern on them where your high performance thread ends up on an e-core?  Thanks all!  Note: This question is not for Intel\_Support. The answer from your side would obviously be ""Yes!"". :)",Intel,2026-01-06 21:07:15,1
AMD,nxwkozf,"u/Chelostyles Thank you for your inquiry regarding the CPU and RAM upgrade for your Dell Vostro 3670. As much as I'd like to provide my technical insights on this upgrade path, I'm not in a position to provide specific suggestions since this involves hardware modifications to an OEM system.  For the best compatibility outcome and to ensure optimal system performance, I strongly recommend reaching out to your system manufacturer directly. They can provide definitive guidance on supported CPU upgrades (i5-8400 to i7-9700) and maximum RAM configurations for your specific model. We don't want to inadvertently bypass any warranty terms and conditions on your system by providing modification recommendations that might affect your coverage.  Your system manufacturer's technical support team will have access to the exact specifications, BIOS compatibility matrices, and supported hardware configurations for your Vostro 3670 model. They can confirm whether the motherboard supports the i7-9700, the maximum RAM capacity (64GB vs 128GB), and any potential limitations or requirements for these upgrades.  This approach ensures you get accurate, manufacturer-validated information while maintaining your system's warranty protection.",Intel,2026-01-05 22:52:24,1
AMD,nxwjdkt,"u/I_like_carsyay  XPS 15 9530 hardware does support Resizable BAR, which is why Intel's system detection shows it as available for both your Arc A370M and Iris Xe graphics. However, the system manufacturer has designed their BIOS interface to prioritize stability and user-friendliness, often managing advanced PCIe features like ReBAR automatically in the background rather than exposing manual configuration options. This approach ensures optimal system performance while reducing complexity for users. I recommend checking for the latest BIOS updates from your OEM's support site and contacting their technical support team, as they would have the most current information about how ReBAR is implemented on your specific model and whether any additional configuration steps are needed to fully utilize this feature.     I've posted an article below in case you haven't yet come across it:  **Helpful Resources:**  *  [What Is Resizable BAR and How Do I Enable It?](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html)",Intel,2026-01-05 22:45:46,1
AMD,ny3upu3,"u/QunatumLeader Hi, thanks for your interest!  You can find and apply for all of our jobs online at [http://](http://jobs.intel.com/)[j](http://jobs.intel.com/)[obs.intel.com](http://jobs.intel.com/). We don’t currently accept submissions via social.  Good luck!",Intel,2026-01-07 00:05:20,2
AMD,nw3e1uz,that is the most non-descript render of a laptop possible,Intel,2025-12-26 22:13:56,3
AMD,nw638sa,So light it visibly doesn't have any ports?,Intel,2025-12-27 09:58:34,2
AMD,nur68kw,"Does intel 10A still come out as scheduled in 2027? I googled it and found out intel said the 10A will come out in 2027, but this was old news in 2024.",Intel,2025-12-18 21:37:42,15
AMD,nuu5n9y,I wonder how intel and other companies are going to manage for next year? Prices for memory and SSD’s are predicted to go even higher putting off many buyers from getting a new PC build or laptop.   This makes me concerned Nova Lake won’t sell as well because of this.,Intel,2025-12-19 09:41:00,5
AMD,nuthq66,It's shameful to see LBT posing with 14A wafers when all the groundwork for this was setup by Pat Gelsinger. The entire Intel board should have been sacked instead of Pat.,Intel,2025-12-19 05:59:22,13
AMD,nur0ojq,"GFHK also has 14a for Razor and Coral Rapids in 2H 2027, so I'm taking what they are saying with very little credibility.   Plus, we had very similar rumors during 18A, and that went nowhere. Fool me once...",Intel,2025-12-18 21:10:09,11
AMD,nutolrl,Unbelievable till official announcement,Intel,2025-12-19 06:56:47,2
AMD,nusrcmh,can't they use it to make more ram ?,Intel,2025-12-19 03:04:41,3
AMD,nur9juo,good news,Intel,2025-12-18 21:54:21,2
AMD,nvzjgd1,They can't even sell 18A to NVDA what are they doing on 14A really ?,Intel,2025-12-26 06:29:30,1
AMD,nur0nvy,"Lisa So Sue Me wants a taste of the Lip? Am I living in a different dimension? I callled out So Sue Me on X, is she jumping on Big Blue’s Back?  Is anyone Dollar Cost Averaging INTC? It will still be awhile before IFS is firing on all cylinders. The Lip said he would stop high end chip production for external customers (If No One Took A Byte) in order to get $$$ to build out Ohio Fab.   Let’s get it done. I’m driving distance from the Ohio Fab, any chance Intel will give me a tour?",Intel,2025-12-18 21:10:03,-17
AMD,nusksfh,"If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  [https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots](https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots)  *""Intel's previously-unannounced Intel 10A (analogous to 1nm) will enter production/development in late 2027, marking the arrival of the company's first 1nm node, and its 14A (1.4nm) node will enter production in 2026.*  ***\[Edit: to be clear, this means 10A is beginning development, not entering high volume manufacturing, in 2027\]*** *The company is also working to create fully autonomous AI-powered fabs in the future.""*",Intel,2025-12-19 02:26:13,13
AMD,nur6gwd,"14A probably won't be ready for 2027, much less 10A.",Intel,2025-12-18 21:38:53,13
AMD,nutent6,10A & 7A are in R&D phase,Intel,2025-12-19 05:35:17,3
AMD,nurn23y,It's gonna be 2026 soon and 18A is launching at the very start of 2026. A double node shrink in like 2 years doesn't exactly sound very possible.,Intel,2025-12-18 23:07:17,4
AMD,nuu144l,"Remember, these are just names/nicknames. 10A? The difference between 14A and 10A is probably equivalent to the difference between 14nm and 14nm+",Intel,2025-12-19 08:55:31,2
AMD,nurpt6m,And yet here you are.,Intel,2025-12-18 23:23:22,10
AMD,nutpnod,"Brother, don't hint at your place of employment when you have your full face in your profile as well as you commenting in NSFW subs.",Intel,2025-12-19 07:06:02,3
AMD,nuteqb3,There will probably still be another of layoffs next month 😂,Intel,2025-12-19 05:35:49,2
AMD,nutezdb,"Yes, perhaps it’s better if you post it on the r/intelstock subreddit instead 🤪",Intel,2025-12-19 05:37:45,1
AMD,nv78q7z,"Ram should be at a more reasonable price in 2027 according to Moores Law is Dead. Maybe not $100 for 32GBs, but maybe below $200 🤞",Intel,2025-12-21 14:18:36,2
AMD,nvi0fpp,They have contract.,Intel,2025-12-23 05:47:11,1
AMD,nw3qq9x,"TBH I feel LBT is doing a good job. I was hesitant at first, but he's making a lot more sense than Pat's crazy descent into spending crazy amount of cash with no business in sight.  Speaking as a shareholder.",Intel,2025-12-26 23:27:31,2
AMD,nutoo6g,"The entire Intel board probably should have been sacked, but Gelsinger as well. He failed at his main mission and drove the company into a crisis. That kind of thing should have consequences.",Intel,2025-12-19 06:57:22,2
AMD,nuv6jd0,Who was it that decided to exit the SSD business.  They sold off a cash cow for pennies on the dollar.,Intel,2025-12-19 14:18:21,0
AMD,nutu4bu,Nvidia is at least some what believable. AMD though?,Intel,2025-12-19 07:47:24,5
AMD,nuu5f18,"I thought that too. At least they'd have some money coming in. But apparently it takes years to rejig the plants to churn out RAM instead of CPUs. And they're heavily invested in getting the next gen CPU fabs working.   Pivoting to RAM just doesn't make sense, unless they magic'd up a new type of RAM that's cheap to make and has super low latency - which is one thing I've always thought they ought to do.   Imagine if external RAM ran with super low latencies like CL1 or CL2 or something. You wouldn't even need branch prediction and prefetch and massive caches in the CPUs.",Intel,2025-12-19 09:38:44,3
AMD,nutophj,"""news"" needs a lot of quotes around it...",Intel,2025-12-19 06:57:41,1
AMD,nuuzsz6,This isn't wallstreetbets. We don't talk like that here.,Intel,2025-12-19 13:40:01,5
AMD,nusti41,">If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  Yup, and to make it even more obvious, the same graph also has Intel 14A showing up early 2026, and 20/18A showing up at the start of 2023*,* so clearly it's not the date of when the node is going to come out (or even start HVM).",Intel,2025-12-19 03:17:42,7
AMD,nurmeyo,"Dunno why this is being downvoted, the CEO of Intel himself said that 14A is a 28-29 node in the Q2 2025 earnings call.",Intel,2025-12-18 23:03:37,11
AMD,nuy09wl,enough info about how intel names products exists to know. if it didn't increase in transistor density per mm it would not be called 10A.,Intel,2025-12-19 23:07:36,6
AMD,nutpt6n,"I think everyone knows there will be continued Q1 and possibly Q2 layoffs.   Return to office didn't lead to enough voluntary attrition. Leadership wants to hit a magic number which sounds good for financial reports, not what is actually viable to run things.",Intel,2025-12-19 07:07:25,5
AMD,nw3rzlk,That crazy amount of cash being spent by Pat is what enabled 18A and 14A. They HAD to buy multuple $250 Litho machines from ASML in order to make that possible. Pat was playing catch up after years of under-investment by Swan and Krzanich. It was necessary and LBT is getting the credit. You don't appear to understand the lead times required in the semi industry. Pat understood that. The mistakes Pat made were trying to build a fab in Ohio and not cutting headcount and getting rid of dead weight sooner.,Intel,2025-12-26 23:35:12,1
AMD,nuuu28f,The thing intel is doing rn is literally pat's groundwork isn't it?,Intel,2025-12-19 13:04:50,9
AMD,nutv81y,Still a tall order imo unless it's some defense chip for RAMP-C,Intel,2025-12-19 07:57:50,1
AMD,nv0hjyu,"If they're following industry standards I'd say it depends on how good AMD's next gen is. Intel doesn't need direct access to AMD designs to etch chips for them, and designers make way more than per wafer than foundries do.  If AMD has superior designs to intel again they could finally ship out some damn chips for laptop OEMs. It would hurt intel more than the revenue would benefit them imo since client has really been carrying intel for the last six years and demand for AMD chips has been high despite the drip feed of strix chips. honestly I'm considering an AIO/NUC/whatever the new name is with strix halo and unified LPDDR5 to upscale old footage without having to use my daily desktop. imagine if it was available at scale.",Intel,2025-12-20 10:33:05,1
AMD,nuu642g,"they don't have to make faster ram, just make it, right now, some ppl don't really care about speed",Intel,2025-12-19 09:45:35,2
AMD,nuu0o0x,"So, risk production in late 27/early 28 and HVM in 2029 I suppose?",Intel,2025-12-19 08:51:11,2
AMD,nuvsda6,YEs it is. He did make mistakes. He was hiring like crazy at the beginning of his term. And he should have started cutting sooner. But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.,Intel,2025-12-19 16:10:45,4
AMD,nuwuwrt,"Nothing they're doing *right now* is a success story. Remember that they don't actually have customers, and that is first and foremost what got Gelsinger fired. As things stand, the foundry as a whole is a failure. If things turn around, that will have to be under Lip Bu.",Intel,2025-12-19 19:20:39,1
AMD,nuu5jf7,I wasn’t aware 14A is part of the RAMP-C initiative. I thought it was only Intel 16 & 18A that are currently covered by RAMP-C?,Intel,2025-12-19 09:39:58,1
AMD,nuubzhq,I think so. Maybe optimistically we see a 14A product in late 28'.,Intel,2025-12-19 10:41:56,4
AMD,nuwv4b8,"> But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.  No, that was just more wasted money. 18A doesn't even use the high-NA machines Intel bragged so much about. It seems they tried blaming their struggles in foundry on the equipment instead of the broader org culture and talent.",Intel,2025-12-19 19:21:43,0
AMD,nv78ydu,"As much as I hate to say it, Intel arc was also a mistake.",Intel,2025-12-21 14:20:00,0
AMD,nvl276b,get out of here with your sensable comments. we only circle jerk on this sub,Intel,2025-12-23 18:22:21,1
AMD,nuu6whm,It can expand in future ? My point is how can we believe such stuff at face value without actual proof.,Intel,2025-12-19 09:53:22,1
AMD,nuzuyhs,14A does use the High-NA machines. They didn't buy them with no plan to use them That would be stupid.,Intel,2025-12-20 06:44:37,4
AMD,nvl210i,"no it wasnt. GPU's are surpassing cpu's eventually if not now.  a major part of amds success  was buying radeon all those years ago. when intel realized how utterly shortsighted they had been, they pushed arc heavy even though it wasnt going to succeed that well.  this was the right choice, as otherwise they would look like a dinosaur.",Intel,2025-12-23 18:21:32,1
AMD,nuu7dr5,"It can expand in the future but this is a trial, it’s not yet a long term commitment until the outcome of the project is known (final evaluation won’t be until 2026/2027). 14A is not part of RAMP-C, it’s still in phase III trial with 18A. There’s been no additional RAMP-C design calls via NSTXL that I’m aware of",Intel,2025-12-19 09:58:04,1
AMD,nv05iea,"> 14A does use the High-NA machines. They didn't buy them with no plan to use them  They bought the very first high-NA machines, claiming it was for 18A. Now they won't be used until a node that hits volume in '28/'29, by which point TSMC will have (or rather, already has) much better machines. So what exactly was the point?  > That would be stupid.  Is that not a perfectly apt description for Intel's foundry strategy in recent years? It sounds like they really drunk the coolaid with their attempts to blame the 10nm failures on the lack of EUV.",Intel,2025-12-20 08:28:08,2
AMD,nvl37xc,Yeah. The real mistake was LBT and the other Intel board members nerfing the r&d budget.,Intel,2025-12-23 18:27:21,1
AMD,nv074kj,14A will have volume production in 2027.,Intel,2025-12-20 08:44:44,3
AMD,nv63f2d,Didn't Intel say in a presentation that 2027 is risk production for 14A? https://www.techspot.com/news/107736-intel-doubles-down-foundry-ambitions-unveils-18a-14a.html  https://www.youtube.com/watch?v=5Jbj4RQBXbo&t=818s,Intel,2025-12-21 08:18:24,1
AMD,nv088jg,"Lip Bu himself is saying '28-'29. At this point, there isn't a chance in hell it's ready for volume in '27.",Intel,2025-12-20 08:56:04,0
AMD,nvqcqv3,I just wanted arc to succeed 😔,Intel,2025-12-24 15:43:21,1
AMD,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,54
AMD,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
AMD,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
AMD,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
AMD,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,11
AMD,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,8
AMD,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,5
AMD,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,6
AMD,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,2
AMD,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
AMD,nspzeik,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,7
AMD,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
AMD,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
AMD,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
AMD,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
AMD,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,6
AMD,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
AMD,nsv64t7,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,3
AMD,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
AMD,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
AMD,nsyv727,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
AMD,ntimkr9,Back in the day you could overclock a 2600k from 3.4Ghz to 4.5Ghz on a $25 Hyper212 cooler. The performance gains were incredible as Sandy Bridge scaled very well at higher clocks.   Now days CPUs come overclocked already.,Intel,2025-12-11 19:25:58,29
AMD,ntifd3j,"""It's crazy to think that a cpu from 2009 can be easily overclocked.. 2.9Ghz to 4.1Ghz is crazy !""  You could overclock huge amounts on earlier generations - I used to run Pentium 4 1.6GHz chips at 3.2GHz on air-cooling, more on phase-change cooling.",Intel,2025-12-11 18:50:27,28
AMD,ntikk9s,"I ran my i5 750 2.67Ghz for years at 4Ghz without any issues. I benched it some at 4.2Ghz even, but it was not fully stable.  The X58 CPU are even better tho. And even if you had insane OC potential back in the days it was not as good as it sounds, since the turboboost was higher than the stock frequency that is listed.",Intel,2025-12-11 19:15:50,7
AMD,ntjwvoj,"Lol a 15 year old computer running Windows 11, meanwhile Microsoft telling people to upgrade 5 year old laptops for win10 being EOL.",Intel,2025-12-11 23:27:29,6
AMD,ntivoqo,X5690@4.6GHz on Rampage III Extreme 😘,Intel,2025-12-11 20:12:02,6
AMD,ntj26xa,it is crazy that intel sold you same technology at downclocked speeds to make a nice model range with different prices.,Intel,2025-12-11 20:45:29,3
AMD,ntiq1p0,Sick stuff. I still got my i7 930 at 4.2Ghz running just fine. These types of chips overclock like crazy.,Intel,2025-12-11 19:43:28,3
AMD,ntkwagl,Be nice. Give it another stick of ram!,Intel,2025-12-12 02:58:53,3
AMD,ntoxmhs,Q6600 G0,Intel,2025-12-12 19:02:37,3
AMD,ntiqlci,"Cool. Glad it worked for you. I have dual xeon server, maybe i should try it. But its production server dont wana break my apps. Lol",Intel,2025-12-11 19:46:15,2
AMD,ntjb06t,My 2500k did ~4.8 ghz and my 6950x did 5.2 ghz. Its base clock was like 3.2ghz and this was using 128GB of quad channel DDR4.  It was “stable”,Intel,2025-12-11 21:29:41,2
AMD,ntkz9ut,45nm is crazy in 2025,Intel,2025-12-12 03:16:12,2
AMD,ntosqvz,500W power draw when,Intel,2025-12-12 18:38:31,2
AMD,nthl3mn,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-12-11 16:23:16,1
AMD,ntk2ims,"I used to run my i3-540 at 4.2GHz, air cooled on what is effectively worse than a Hyper 212 Evo. I miss the old days when I could overclock the snot out of them. These days I guess they're binned to almost their max potential out of the factory so most of the time I'm undervolting them.",Intel,2025-12-12 00:01:05,1
AMD,ntk9tcx,Well done. Still using two H55m machines with OC (x3450 and i5 661).  They also OC decently at stock voltage keeping turbo and all power savings. My X3450 does 2.6 -> 3.3Ghz(3.8 turbo). The advantage is that it idles quite low at 50-60W.   But for gaming and rendering it's better to go all in as you did. Most chips can do anywhere from 3.8 to 4.2 all cores IME.,Intel,2025-12-12 00:44:00,1
AMD,ntl2d0n,nah my 40 logical processors would smash through it all  x2 xeon e5-2680 v2,Intel,2025-12-12 03:34:33,1
AMD,ntk4sw2,"is that better? I dont need to dive into setting anymore, the CPU maker do it for me with warranty.",Intel,2025-12-12 00:14:36,4
AMD,ntsgvaj,"There is still more to work with, especially if one does not fossilize on static all core OC, but does 2-step TVB fueled dynamic OC, Ecores are Aldo the source of much happiness on arrow",Intel,2025-12-13 09:30:18,1
AMD,ntja1e8,I miss overclocking. Felt like you were getting a bargain. Now I don’t even try.,Intel,2025-12-11 21:24:52,14
AMD,ntjnkj4,"Not as big an OC as yours, but I had a pre-built from FutureShop.  It was their home brand name.  Found a BIOS for the board that wasn’t theirs.  Managed to get 3.2GHz out of a 2.4GHz Pentium 4 on pre-built from FutureShop cooling.",Intel,2025-12-11 22:34:57,3
AMD,ntihjuu,"Wow, soo cool",Intel,2025-12-11 19:00:59,1
AMD,ntnudso,The motherboard doesn't accept other stick of ram. Only my corsair ram work,Intel,2025-12-12 15:49:11,1
AMD,ntmrk4p,How did you even get a 6950x to boot at 5.2ghz? Most of them hit a wall around 4.3ghz,Intel,2025-12-12 12:03:28,3
AMD,ntkag8u,"Has its ups and downs. Now that I'm older and have less time to tweak things and mostly just want shit to be stable, I see ""pre-overclocked with maybe 5% performance left on the table"" as a pro. The con is that chipmakers just jam a ton of power through it to make it happen, and the option of buying a half-price chip and spending an entire sleepless weekend tweaking it yourself to get 95% of the more expensive chip's performance is gone.",Intel,2025-12-12 00:47:48,4
AMD,ntjl0xd,"Same - the complexity and heat rose a lot and the gains because less significant - with multi-core chips and turbo frequencies there just isn't much headroom in them.  That and I work fixing issues with computers all day, I just want my own PC to work.",Intel,2025-12-11 22:21:04,6
AMD,ntlnjst,"Thats because these older CPUs were surprisingly energy efficient. Also mostly because now modern CPUs are powerful enough where overclocking is pointless. Even my i3-12100 being overclocked would be pointless, even if its only a 80 watt CPU",Intel,2025-12-12 05:56:24,2
AMD,ntwqjex,He couldn't without LN2.,Intel,2025-12-14 01:36:27,2
AMD,ntnxj8h,"It was short lived, over ~7 years I had to pull back the multiplier from 52 to 44 to keep it stable.  I retired the system this year.  It was a full open loop from EK.  2x Pascal Titan X in SLi",Intel,2025-12-12 16:04:28,0
AMD,ntmp3kk,"I used to overclock everything, now I undervolt everything lol",Intel,2025-12-12 11:44:00,2
AMD,nonhxk4,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is “slow” is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already “beating” AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Intel,2025-11-13 16:13:13,51
AMD,nootzxi,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Intel,2025-11-13 20:08:27,11
AMD,nrd4uj2,"I've gotten one and honestly it's amazing, easily the best laptop I've ever used so far.   I was skeptical about the battery life claims but I've genuinely found that using it for about 8 hours straight for coding, only drains the battery maybe 50%.  I've set it to only charge up to 80% max for battery health conservation, and I've regularly coded for 12 hours straight on the medium performance profile and haven't needed to charge until I got back home.  (This is for the Ultra 7 258v cpu variant btw)  Also this is while running Fedora with KDE Plasma which makes the battery life even more impressive as it's one of the heavier distros running cutting edge hardware and I've heard that Linux has less battery optimization compared to windows.    Screen isn't anything to write home about but the 100% srgb one looks good enough and is bright, 60hz looks kind of bad but I know that it saves a lot on battery.   Keyboard feels very nice as far as laptop keyboards go, having it be easily swappable is lovely as I wore out the keys on my old laptop, and I want this thing to last.   Linux hardware compatibility is perfect so far, even the fingerprint sensor works out of the box on fedora.   My only real complaint is that the plastic it is made out of is a major grease magnet and if I touch it without having immediately washed my hands, even if my hands weren't dirty, it'll leave dark patches from oils. Also it would be nice to have swappable RAM but I think 32gb ought to last a very long time anyway.   Genuinely seems like arguably one of the, if not the, best laptops for actually getting work done. Maybe it's not as fancy or sleek, but it just works. It's like the 2001 Toyota of the laptop world, it's not winning prizes for looks, but it'll never die, gets good mileage (battery life), and is easily repairable. Maybe not the laptop you want, but definitely the one you need (excluding people who need something like a dedicated GPU or really need super high CPU performance).",Intel,2025-11-29 11:19:13,5
AMD,noon81e,"Intel beats AMD in software (drivers, firmware) … I got think pad 780M laptop by company I work for. Randomly display won’t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Intel,2025-11-13 19:34:25,11
AMD,non7ozt,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Intel,2025-11-13 15:23:02,6
AMD,nom9a0s,lol. Even in the cons it says weaker multicore than AMD….?   This article seems like AI wrote it,Intel,2025-11-13 11:53:59,0
AMD,nr8651t,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Intel,2025-11-28 15:11:41,1
AMD,nopvuqn,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Intel,2025-11-13 23:25:31,1
AMD,nonh9ew,Suck at gaming.,Intel,2025-11-13 16:09:54,-13
AMD,notgml4,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Intel,2025-11-14 15:00:36,8
AMD,noy6f36,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Intel,2025-11-15 08:22:42,8
AMD,npap4we,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Intel,2025-11-17 10:42:37,3
AMD,ntz6joo,Isn’t the keyboard one of the most important characteristics?,Intel,2025-12-14 13:39:49,1
AMD,nopn323,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Intel,2025-11-13 22:36:30,-1
AMD,ntz6z28,"Hey I’m looking at the exact same laptop that you have. Can you tell me about the build quality and if there’s any keyboard flex when pressing down on it? Please tell me. I’m going to use it for word, excel, reading lots of pdf files and ebooks and watch movies. Will it be enough for that?",Intel,2025-12-14 13:42:38,1
AMD,nop3ehp,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Intel,2025-11-13 20:56:10,6
AMD,nongtqn,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Intel,2025-11-13 16:07:47,4
AMD,nousjfs,Soldered ram is a lot faster. So no.,Intel,2025-11-14 18:58:54,1
AMD,noniq16,Yes but now RAM costs a ton of money,Intel,2025-11-13 16:17:05,0
AMD,nomcmkj,Is multicore performance the only consideration when buying a laptop?,Intel,2025-11-13 12:19:56,28
AMD,nqyoc8i,What kind of issues?,Intel,2025-11-26 23:00:59,2
AMD,ntz73fr,What kind of issues with Intel? I thought it was the AMD that had tons of issues,Intel,2025-12-14 13:43:27,1
AMD,nonhqb5,It is not a gaming laptop,Intel,2025-11-13 16:12:13,18
AMD,np9o16h,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Intel,2025-11-17 04:44:52,2
AMD,np3siex,It's an enterprise grade product you buffoon.,Intel,2025-11-16 06:03:41,4
AMD,npd9987,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Intel,2025-11-17 19:32:27,2
AMD,np8gg6z,Build quality.    Thinkpads are solid machines that are easy to fix.    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Intel,2025-11-17 00:13:54,1
AMD,ntzg92j,"So build quality will be subjective, from what I can tell, it's got very good build quality in terms of ""real"" factors such as durability. But it definitely feels less ""premium"" than similarly priced consumer grade laptops. The plastic is plastic so it will flex a little bit, but the parts all seem very well put together and it does feel ""solid"" overall.   I haven't really noticed keyboard flex, but I have noticed a slight amount of flex where my palms rest, particularly on the right side, where the smart card reader is, which makes sense as it is just a big hole in the side of the laptop. I plan on getting a dummy smart card to fill the gap and hopefully that should reduce it.   Overall whilst the internal chassis is metal, the outside is just plastic. I imagine that is good for durability, as it ought to be able to absorb shocks, but, as I said, it definitely makes it feel less ""premium"". They key press feel of the keyboard definitely does feel very nice as far as laptops go though. Obviously it's still nothing compared to a good mechanical keyboard but for a laptop it's very nice.   I bought this laptop for longevity and durability, so given that It's only just come out, I can't really say much about that, but the prestige of thinkpads of previous generations kind of speaks to their reliability. Plus it's apparent that they are still quite easy to repair and Lenovo has video guides on replacing loads of the parts.   And for your use case the battery life should be very good. It seems the Intel chip was designed to be very efficient during periods of downtime and something like viewing a PDF or editing a document has a LOT of downtime for the CPU",Intel,2025-12-14 14:40:19,2
AMD,norwnxs,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Intel,2025-11-14 07:46:09,4
AMD,noruygl,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Intel,2025-11-14 07:29:24,3
AMD,noxc6wn,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Intel,2025-11-15 03:57:11,2
AMD,nop71cl,All the more reason to make it upgradable,Intel,2025-11-13 21:14:31,-2
AMD,nomhldl,Lunar Lake isn’t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Intel,2025-11-13 12:55:02,15
AMD,nomp84g,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Intel,2025-11-13 13:42:02,5
AMD,nu0g7o7,This was back when the 14th generation were having issues.,Intel,2025-12-14 17:47:28,2
AMD,nonivqb,"It's $2,000 so no excuse.",Intel,2025-11-13 16:17:52,-9
AMD,nu6fuik,Thank you so much for this valuable and comprehensive information! I really appreciate it:),Intel,2025-12-15 16:29:10,1
AMD,npd8o9g,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Intel,2025-11-17 19:29:33,3
AMD,nomwgxm,"Lunar Lake already beat AMD, nobody buys AMD laptops",Intel,2025-11-13 14:23:36,19
AMD,non5ael,i stopped at $2100 for a Thinkpad T14,Intel,2025-11-13 15:10:49,1
AMD,nu0kv9z,"Ah okay, got it thanks.",Intel,2025-12-14 18:10:16,1
AMD,nowos5a,wrong  Nobody Supply AMD laptop     There fixed for u,Intel,2025-11-15 01:22:32,2
AMD,nov79aa,"I do, and many of the people I know do.",Intel,2025-11-14 20:14:59,-1
AMD,nonhh1g,Nobody pays that much.,Intel,2025-11-13 16:10:57,7
AMD,np79214,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Intel,2025-11-16 20:19:32,-1
AMD,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,11
AMD,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,5
AMD,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
AMD,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
AMD,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,2
AMD,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
AMD,np6680l,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
AMD,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
AMD,nmg20dw,Oh... for sure 😁,Intel,2025-10-31 22:15:08,3
AMD,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
AMD,nn1h3l3,Great work dude! Only 200MHz to go 😉,Intel,2025-11-04 11:15:21,2
AMD,nmilk0q,Car coolant in the freezer 😁,Intel,2025-11-01 11:12:18,2
AMD,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
AMD,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,2
AMD,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
AMD,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
AMD,np7d3w5,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Intel,2025-11-16 20:40:17,1
AMD,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
AMD,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
AMD,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
AMD,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
AMD,ngieos7,And largely against the non-x3d lmfao.,Intel,2025-09-27 17:21:03,79
AMD,ngif1q6,Aren't they just showing that AMDs CPUs are better for gaming?,Intel,2025-09-27 17:22:52,29
AMD,ngmmadi,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Intel,2025-09-28 10:33:19,3
AMD,ngiqxv3,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Intel,2025-09-27 18:23:57,-16
AMD,ngp4a4b,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Intel,2025-09-28 18:56:21,0
AMD,nglqard,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Intel,2025-09-28 05:26:07,30
AMD,ngiw9gz,I assume they compared with CPUs in a similar price range,Intel,2025-09-27 18:52:05,30
AMD,ngl774g,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Intel,2025-09-28 02:59:30,5
AMD,ngj2d2a,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Intel,2025-09-27 19:24:33,52
AMD,ngmt8qo,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Intel,2025-09-28 11:35:30,12
AMD,ngn0xy1,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Intel,2025-09-28 12:33:09,3
AMD,ngix2qg,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Intel,2025-09-27 18:56:22,34
AMD,ngiy5wa,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Intel,2025-09-27 19:02:07,9
AMD,ngir4n9,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Intel,2025-09-27 18:24:57,4
AMD,ngj6exq,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Intel,2025-09-27 19:46:01,2
AMD,nhi1lee,"Now install windows 11, lol",Intel,2025-10-03 06:08:42,1
AMD,nglqeum,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Intel,2025-09-28 05:27:08,2
AMD,ngkj8l9,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Intel,2025-09-28 00:25:26,13
AMD,ngkwq9d,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Intel,2025-09-28 01:51:37,20
AMD,ngmy2qz,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Intel,2025-09-28 12:12:58,4
AMD,ngmx0bu,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Intel,2025-09-28 12:05:06,1
AMD,ngn1g9m,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Intel,2025-09-28 12:36:42,6
AMD,ngiyobq,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Intel,2025-09-27 19:04:51,-3
AMD,ngizavn,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Intel,2025-09-27 19:08:11,1
AMD,ngjh5f6,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Intel,2025-09-27 20:42:11,4
AMD,nhk7a3s,I did same performance on all processors.,Intel,2025-10-03 15:24:53,1
AMD,ngm7i8q,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Intel,2025-09-28 08:05:53,3
AMD,ngkykmi,That sounds like an AMD Stan argument circa 2020,Intel,2025-09-28 02:03:12,18
AMD,nh261w2,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Intel,2025-09-30 19:23:27,1
AMD,ngn25ws,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Intel,2025-09-28 12:41:35,0
AMD,ngjdprn,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Intel,2025-09-27 20:24:13,11
AMD,ngj1pgu,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Intel,2025-09-27 19:21:01,7
AMD,ngkqt0z,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Intel,2025-09-28 01:12:59,1
AMD,ngj49y7,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Intel,2025-09-27 19:34:44,3
AMD,ngk1b4v,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Intel,2025-09-27 22:35:49,3
AMD,nhka12e,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Intel,2025-10-03 15:38:08,1
AMD,ngmkwf8,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Intel,2025-09-28 10:19:51,10
AMD,ngnw7zu,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Intel,2025-09-28 15:28:44,4
AMD,ngl05xq,Expand ?,Intel,2025-09-28 02:13:17,-2
AMD,nh27g09,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Intel,2025-09-30 19:30:10,1
AMD,ngtvgdv,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Intel,2025-09-29 14:03:26,6
AMD,ngw45vk,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Intel,2025-09-29 20:34:34,0
AMD,ngjg69y,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Intel,2025-09-27 20:37:03,4
AMD,ngqfmrw,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Intel,2025-09-28 22:53:49,1
AMD,ngk26op,did you do it,Intel,2025-09-27 22:41:09,1
AMD,ngmi765,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Intel,2025-09-28 09:53:01,1
AMD,ngk22ke,can you reset settings then choose ray tracing ultra preset.,Intel,2025-09-27 22:40:28,2
AMD,nhle3iw,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Intel,2025-10-03 18:52:42,1
AMD,ngnx0bd,because they exclusively exist in DIY build your pc enthusiast bubble,Intel,2025-09-28 15:32:30,4
AMD,nhvyteo,Pricing was aggressive. A 12 core 3900x was 400 usd.,Intel,2025-10-05 13:26:56,1
AMD,ngl3zfu,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Intel,2025-09-28 02:38:06,13
AMD,nh2848m,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Intel,2025-09-30 19:33:29,1
AMD,ngtxj1d,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Intel,2025-09-29 14:14:15,3
AMD,ngmif3t,"Okay, I did it",Intel,2025-09-28 09:55:13,2
AMD,ngmglra,"No, I didn’t remember good",Intel,2025-09-28 09:37:10,1
AMD,ngk41a9,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Intel,2025-09-27 22:52:22,2
AMD,nhp053x,Thanks for solidifying opinion that your benchmarks are fake,Intel,2025-10-04 10:05:07,1
AMD,nh2a34y,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Intel,2025-09-30 19:43:15,1
AMD,ngk5zrq,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Intel,2025-09-27 23:04:17,2
AMD,nhp2exv,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Intel,2025-10-04 10:27:16,1
AMD,ngbpsza,Cam someone confirm or is this gas lighting?,Intel,2025-09-26 15:51:04,20
AMD,ngbym0c,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Intel,2025-09-26 16:33:40,16
AMD,nghesqk,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Intel,2025-09-27 14:17:43,5
AMD,ngc7w1b,Intel comeback real?,Intel,2025-09-26 17:18:16,6
AMD,ngf1ik5,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Intel,2025-09-27 02:43:51,2
AMD,ngbsck8,3D v-cache has entered the chat.,Intel,2025-09-26 16:04:06,8
AMD,ngbr9eb,Take it as a grain of salt. Intel marketing LOL,Intel,2025-09-26 15:58:30,4
AMD,nh5ixeo,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Intel,2025-10-01 08:33:03,1
AMD,ngfguk7,Thats cool ...but lets talk about better pricing.,Intel,2025-09-27 04:36:12,1
AMD,ngfrla1,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Intel,2025-09-27 06:09:56,0
AMD,ngc573e,Tech Jesus has entered chat :).,Intel,2025-09-26 17:05:31,-11
AMD,ngcjbbq,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Intel,2025-09-26 18:13:09,-11
AMD,ngbqjhe,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Intel,2025-09-26 15:54:51,42
AMD,ngdvx9l,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Intel,2025-09-26 22:23:43,13
AMD,nge3sfi,What do you mean by gaslighting in this case?,Intel,2025-09-26 23:10:51,3
AMD,ngcf9aj,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Intel,2025-09-26 17:53:19,1
AMD,ngcutw5,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Intel,2025-09-26 19:10:03,1
AMD,ngl3adb,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Intel,2025-09-28 02:33:31,1
AMD,ngdfut5,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Intel,2025-09-26 20:55:42,1
AMD,ngbxbws,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Intel,2025-09-26 16:27:28,-7
AMD,ngfqkoh,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Intel,2025-09-27 06:00:33,2
AMD,ngfebe1,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Intel,2025-09-27 04:16:03,2
AMD,ngfqbry,Nova Lake bLLC about to ruin Amd X3D party.,Intel,2025-09-27 05:58:18,1
AMD,ngc2ju0,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Intel,2025-09-26 16:52:57,19
AMD,ngc2czl,I always wondered if Intel marketing budget is higher than the R&D budget,Intel,2025-09-26 16:52:01,-8
AMD,ngfrkpn,Intel Arrow Lake is much cheaper than Amd Zen 5.,Intel,2025-09-27 06:09:47,4
AMD,ngemp1j,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Intel,2025-09-27 01:07:45,7
AMD,ngeo8em,only an AMD fan would worry about replacing their shit CPUs under 3 years,Intel,2025-09-27 01:17:36,0
AMD,ngbzwzr,Hardware unboxed isn't a reliable source.,Intel,2025-09-26 16:40:06,8
AMD,ngf1ob8,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Intel,2025-09-27 02:44:54,8
AMD,ngealuz,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Intel,2025-09-26 23:51:44,1
AMD,nge8xbh,Telling people that its performance is better than it actually is?,Intel,2025-09-26 23:41:39,3
AMD,ngca7el,The ones with similar pricing not performance,Intel,2025-09-26 17:29:11,8
AMD,ngigkrj,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Intel,2025-09-27 17:30:41,2
AMD,ngfrgqn,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Intel,2025-09-27 06:08:45,0
AMD,ngerbdr,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Intel,2025-09-27 01:37:32,2
AMD,ngezf04,Quite common for AM4 in my experience.,Intel,2025-09-27 02:29:45,0
AMD,ngihhii,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Intel,2025-09-27 17:35:17,1
AMD,ngerg53,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Intel,2025-09-27 01:38:23,0
AMD,nggftxh,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Intel,2025-09-27 10:08:25,-1
AMD,ngeozwu,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Intel,2025-09-27 01:22:31,-2
AMD,ngg1fuo,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Intel,2025-09-27 07:42:27,-1
AMD,ngerrz8,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Intel,2025-09-27 01:40:28,11
AMD,ngeao9a,Sooo they are in the YouTube space for the money not for the love of tech,Intel,2025-09-26 23:52:08,5
AMD,ngfq1bg,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Intel,2025-09-27 05:55:39,2
AMD,ngdp9bd,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Intel,2025-09-26 21:45:51,-3
AMD,ngc0yus,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Intel,2025-09-26 16:45:15,11
AMD,ngeb3z7,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Intel,2025-09-26 23:54:48,6
AMD,ngmlzg8,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2025-09-28 10:30:22,1
AMD,ngtp3t3,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Intel,2025-09-29 13:28:22,1
AMD,nh5i8gf,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Intel,2025-10-01 08:25:38,1
AMD,nh5il7n,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Intel,2025-10-01 08:29:26,1
AMD,ngc36bx,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Intel,2025-09-26 16:55:54,3
AMD,ngcbde9,Sure but charts seem about right to me,Intel,2025-09-26 17:34:45,1
AMD,ngcxbac,APO is game specific. I'm referring to what has changed overall.,Intel,2025-09-26 19:22:34,4
AMD,nil3hc6,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Intel,2025-10-09 12:28:52,2
AMD,nimk9vp,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Intel,2025-10-09 17:05:36,2
AMD,njlc3cu,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Intel,2025-10-15 09:13:26,1
AMD,nkp7gzk,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Intel,2025-10-22 00:53:17,1
AMD,nksv7pa,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Intel,2025-10-22 16:17:48,1
AMD,nm887uc,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Intel,2025-10-30 17:38:48,1
AMD,nmj83z4,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Intel,2025-11-01 13:55:37,1
AMD,nnc1z1l,"Hi, I’m at my wit's end with my build and would really appreciate some advice.  My PC has been plagued by random crashes, CRC errors, and installation failures for months:  - Random application crashes, often citing `KERNELBASE.dll`, `ntdll.dll`, or `ucrtbase.dll`. - Frequent CRC errors when extracting large ZIP or RAR files. Retrying usually works. - Software installations fail with data corruption or unpacking errors, only to succeed when I try again. - Games crash or stutter randomly. - Very rare BSODs.  Specs:  - **CPU**: Intel Core i9-13900K - **Motherboard**: ASUS ROG STRIX Z790-H GAMING WIFI - **RAM**: 32GB (2x16GB) G.Skill Trident Z5 RGB (DDR5-6400 CL32, Model: F5-6400J3239G16GX) - **GPU**: MSI RTX 4090 Gaming X Trio - **Storage (OS Drive)**: Crucial P5 Plus 2TB NVMe SSD - **OS**: Windows 11   This is what I've already tried (everything passed):  *  **Memtest86:** Completed multiple passes with no errors. *  **Prime95 & Intel XTU:** Seems to be stable. *  **FurMark:** GPU stress test is stable. *  **Storage Health:** All drives pass SMART and manufacturer-specific self-tests. *  **System Integrity:** `sfc /scannow` and `DISM /RestoreHealth` complete successfully. *  **Updates:** All drivers, firmware, BIOS, and Windows are fully up-to-date. *  **Physical:** Cleaned the case, re-seated components, and replaced thermal paste. * **XMP**: I've disabled/re-enabled XMP multiple times, doesn't make a difference.  I have trouble finding a consistent way of reproducing the issue. Today I tried a 7-Zip benchmark which failed once with a ""decoding error"", but I wasn't able to reproduce it afterwards. I couldn't get Intel Processor Diagnostic Tool to fail after multiple hours. So...  *   **Is this a memory instability issue?** Could the RAM be faulty?  *   **Or, could this be a faulty CPU core?** I found [this post](https://www.reddit.com/r/intel/comments/15mflva/tech_support13900k_problems_when_using_multiple/) where a user had identical symptoms (CRC/7-Zip errors) that were only resolved by replacing a faulty 13900K, even with XMP off.  Thanks in advance for any help.",Intel,2025-11-06 00:15:38,1
AMD,noppamh,"# Alienware 34 - AW3425DWM resolution issues  [](https://www.reddit.com/r/ultrawidemasterrace/?f=flair_name%3A%22Tech%20Support%22)  I just got the AW3425DWM, and my laptop is a Dell Inspiron 15 5510, which is not a gaming laptop. I'm not a gamer.  When connecting through the HDMI port on the laptop and monitor, I can't set the resolution to 3440x1440; I can go up to 3840x2160, but not the monitor's native resolution. However, when I connect through the USB-C port on the laptop to the DisplayPort on the monitor, I can set the resolution to 3440x1440 without any issues. The downside is that I've lost the only Thunderbolt port I have available.  Is there a workaround for this issue? If I use an HDMI to DisplayPort adapter, will I be able to set the resolution to 3440x1440?  I understand that the HDMI 1.4 port usually can handle 21:9 resolutions, but with the latest Intel drivers, it isn't giving me the option for 3440x1440",Intel,2025-11-13 22:48:26,1
AMD,noqvatt,"Hi, can someone help me please? I'm trying to generate a video on AI playground after installing it but it just keeps loading for videos and images don't show up. That's my it it'll specs:   Processor: Intel(R) N95 (1.70 GHz) Installed RAM: 32.0 GB (31.7 GB usable) System Type: 64-bit operating system, x64-based processor Graphics Card: Intel(R) UHD Graphics   -That's a part of what's in the console:    No key found for setting negativePrompt. Stopping generation oa @ index-fOc02QH8.js:61 w @ index-fOc02QH8.js:61 await in w V @ index-fOc02QH8.js:22 pt @ index-fOc02QH8.js:61 await in pt V @ index-fOc02QH8.js:22 c @ index-fOc02QH8.js:61 await in c Il @ index-fOc02QH8.js:14 Sr @ index-fOc02QH8.js:14 n @ index-fOc02QH8.js:18 index-fOc02QH8.js:61 uploadImageName b99fb28ea4440a2f51ce53cd5c529554e5965b66f5f5a8506b9b973b66e754bd.png index-fOc02QH8.js:251 [comfyui-backend] got prompt (anonymous) @ index-fOc02QH8.js:251 (anonymous) @ VM5:2 emit @ VM4 sandbox_bundle:2 onMessage @ VM4 sandbox_bundle:2 index-fOc02QH8.js:61 updating image {id: '6eb88f1b-f433-4541-a421-40619ac9fdc2', imageUrl: 'data:image/svg+xml,%3C%2Fpath%3E%3C%2Fsvg%3E', state: 'generating', settings: {…}, dynamicSettings: Array(3)}       With: RuntimeError: UR error     (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:251 [comfyui-backend] Prompt executed in 116.86 seconds   (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:61 executing Object",Intel,2025-11-14 02:55:16,1
AMD,nq4im71,Will XeSS 3 and Intel multi framegen be available for Iris xe graphics igpus?,Intel,2025-11-22 01:57:26,1
AMD,nq9ltvl,"Putting together a 4k gaming 5090 machine, deciding between the 285k or 265k. Does the extra L3 cache of the 285k make any difference, or does the ~5ns less latency of the 265k make more of a difference?   I plan on a 2dimm board to push memory and OC slightly, but nothing crazy.",Intel,2025-11-22 22:42:16,1
AMD,nqoo97z,"Hi all,  I ordered a contact frame from Thermal Grizzly because the temperature of my CPU wasn’t great, and I was tired of the fan noise.   I installed the contact frame in August, and everything was fine until two weeks ago, when the desktop suddenly stopped booting. The fans were spinning, but there was no POST, no debug LED — nothing. I thought the motherboard was dead.   However, when I removed the AIO head and tried to boot again, it worked!  Since then, I’ve been experiencing intermittent no-POST issues, especially after gaming sessions.   To get it to boot, I always have to adjust the screws on the AIO head. If it’s too tight: no POST. If it’s too loose: CPU temperatures are high. For example, in Hogwarts Legacy, I get around 60 FPS with an RTX 4090, while the CPU runs at around 18% usage @ 85°C. Which is not normal at all, as I used to run the game at around 100–110 FPS.  I tightened the screws on the contact frame using my thumb and index fingers.   I’m looking for advice from anyone who has encountered a similar issue, because at this point my only idea is to remove the contact frame and reinstall the original one.  For context, my setup is from November 2022, and I installed the contact frame in August 2025. I have never removed the CPU from the socket since the first installation.  Thank you all.",Intel,2025-11-25 10:53:08,1
AMD,nr6yiuz,Anyone know why the performance of my 13700k is so much better when using a pre micro code bios on a z790 board? This even after applying the latest micro code update through windows,Intel,2025-11-28 09:31:06,1
AMD,nsz9e66,"My hp elitebook 830 g8 notebook, had some problems booting up and it shows blinking lights (caps key) and it keeps trying to boot up without success, but i after getting it repaired now it boots up but sometimes it still does the same thing and boots up after awhile  I was already using the latest versions of the BIOS and ME firmware, and trying to download and update them again did nothing. However, avter updating the BIOS (to the same version), it showed a message saying ""HP Sure Start detected that the Intel Management Engine Firmware is corrupted"", but it only did so once.  What it does consistently is it restarts once or twice when i boot it. It shows the logo, then turns off, and then boots normally.  In the BIOS, it shows in system information that ""ME Firmware Mode: Recovery Mode"".  Windows also takes much longer to boot than usual, can take up to a full minute.  I can recall that the whole system was super slow at some point, but that was before I started diagnosing any of these details. It works fine now after boot.  Tried all sorts of things, installing ME drivers doesnt seem to do anything, and IDK what to do now.  help",Intel,2025-12-08 18:41:34,1
AMD,ntcs1o8,"My processor with integrated 11th Generation Intel graphics (Intel Core i3-1115G4) completely lost Vulkan support after installing the latest version, 7080 (coming directly from the previously installed version, 6987). I'm testing the drivers one by one, but it looks like Vulkan support has been completely removed. Did this only happen with my processor, or with all 11th Generation processors?",Intel,2025-12-10 21:00:34,1
AMD,ntn8xto,"I'm on Win11 with Killer WiFi 7 BE1750x 320MHz Wireless Network Adapter. PC keeps crashing/restarting. Here is an error:  The computer has rebooted from a bugcheck.  The bugcheck was: 0x00020001 (0x0000000000000011, 0x0000000000210720, 0x0000000000001005, 0xffffe700010059a0). A dump was saved in: C:\\WINDOWS\\Minidump\\121225-18625-01.dmp. Report Id: 189a9fba-5969-4b6c-8199-d8b6a03a1a34.  Copilot had me disable all Killer services except Killer Network Service, but the issue persists. Now it tells me to uninstall the drivers and block them from reinstalling and just use the generic drivers.",Intel,2025-12-12 13:56:20,1
AMD,ntoe22w,"I recently bought a 2025 LG Gram 17 (Intel 258V + 140V). Great laptop for productivity tasks which is its primary use for me. I might occasionally game on it if I'm traveling. I only really play Final Fantasy XI these days which was released 2003 so this processor has no problem running it at max settings and \~60 FPS even in Silent mode.  That's when it is plugged in though.  As soon as it is unplugged the performance drops like 80%. I don't mind the performance hit when I'm doing productivity tasks, but would like control over it when I want.  I've tried various things to improve this while on battery:  * Making sure it is designated as ""High Performance"" in Windows. * Making sure Advanced Power settings are the same between plugged-in and battery. * Making sure in Intel Graphics Command Center that Display Power Savings are off. * Making sure in My gram that performance is set to high. * Changing anything anywhere I can find that might be limiting performance on battery.  I haven't started digging through the BIOS yet and don't know that I will. At that point it is too much of a hassle as opposed to some quick toggle(s) in the OS.  Any other places I should be checking or suggestions to address this related to the CPU itself?",Intel,2025-12-12 17:25:39,1
AMD,nttrt4h,"Hey everyone, looking for second opinions because this behavior doesn’t seem normal.  Specs:  CPU: Intel i5-14600KF  Cooler: Scythe Fuma 2 (dual tower, dual fan) with oficial LGA 1700 mounting kit.  Motherboard: ASUS TUF Gaming B760M-PLUS WiFi II  Case airflow is fine.  Darkflash DLX case with 9 fans.  Ambient temp \~25–30°C (Brazil)  Before this build I had a Ryzen 7 5800X on an ASUS TUF board with the same case and airflow, and temps were completely normal. No overheating issues like this.  but now, I'm facing the following:  Idle temps sit around 60–75°C  Any moderate load (opening anything, even the bios) causes instant spikes to 95–100°C. When acessing the Bios, the temp shown is 78-88°C  CPU thermal throttles immediately  Heatsink stays barely warm, even when CPU reports 95–100°C  Fans ramp up correctly  I’ve tried:  Re-mounted the Scythe Fuma 2  3 times (check [https://imgur.com/a/ehBnxLn](https://imgur.com/a/ehBnxLn) for how the termal paste was when I remounted  the last time)  Re-mounted the CPU.  Checked power limits  Limited PL1/PL2 to 65W → temps drop but CPU becomes extremely slow, 1˜3GHZ, but still hitting 50-60ºC on idle and 80-90ºC on the rest, 88 on the bios as well.  Undervolted –0.06V, didn't solve.  Has anyone else seen this behavior with 14600KF + ASUS B760 TUF boards?  My friend (chatgpt) recommended a contact frame and said that it would fix it. What do you think about this?  Any input appreciated. Thanks!",Intel,2025-12-13 15:34:32,1
AMD,nupcpsu,"This isn't tech support, but my thread got locked and the mods said to post it here.  My 14700k is starting to give instability symptoms, so I opened a ticket with Intel.  The next day, I got an email asking for more information and was told they would call yesterday if they hadn’t received it. I replied to the email with information and my order confirmation, but as of 4:00 yesterday, I didn’t get a reply.  So, I went into the ticketing system and didn’t see my reply. I added a new comment with the information from my email and received almost an immediate email from the support person granting the RMA.  He asked for shipping information and if I’d choose option 1 or 2. For option 2, he asked for my agreement to the process and my billing address, name on the credit card, and expiration. He said he’d call to get the credit card number.  I replied to the email and pasted the information in my ticket. Oddly enough, I didn’t see his second response in the ticket.  Is that really how this goes down? There’s not an order system where I can input my credit card information? I have to wait for a call from some random support person and give him my credit card number over the phone?",Intel,2025-12-18 16:06:09,1
AMD,nv6u3b9,"Hi,  I’m trying to register my account for claiming master game key card, but when i put my phone number to complete registration it always “phone number unreachable”.   I already submitted the ticket for support, i worry about not get any replies because of xmast and new year holiday.   Can you help me to complete my registration, i just want to claim the game and play in peace.   I tried to look for solution but nothing have same problem with me ? Is there any possibilities that on weekend the system can’t use A2P SMS for Indonesia Region",Intel,2025-12-21 12:36:51,1
AMD,nvbb6wv,"Intel ARC b580 apparently unable to run games in the snowdrop engine, but in such a way that it's almost impossible for benchmarkers/testers to catch.  I've been trying to run Avatar Frontiers of Pandora on my arc b580 for over a year now, and I keep having the same extremely unusual crash.  If I freshly reinstall the game, or change my operating system, or switch arc driver version, I can play the game normally, at regular framerates, with no issues, for around an hour. MAYBE two if I'm lucky.   The game then crashes, with seemingly no trigger. No thermal issues, no unusual resource usage, no memory leak, no framerate issues, no stuttering, not even a crash report. The game will just close as if I had closed it manually.   I don't crash in the same location every time, it crashes with different game settings, xess enabled/disabled, different monitors, different ram configurations. I can have the framerate capped or uncapped. GPU maxed out or not, power draw high or low. I can't consistently recreate the crash in the same way every time. The only certainty is that it WILL crash eventually.  After crashing once, It will then crash on game launch, every single time. The game opens, the epilepsy warning flashes up for a second or two, and then the game crashes in the same way, with no freeze, no crash report, just the game closing.  Since the game runs fine for an hour or so after I first install it, any benchmarker testing the gpu will likely never see the crash, because they install the game, run benchmarks, do their tests, and then close it before seeing anything wrong.  The following are fixes that I have tried, and have NOT worked.  * Updating graphics drivers (to both the latest absolute driver, and latest WHQL certified driver). * Downgrading graphics drivers (going all the way back to the first game ready drivers for the b580). * Updating windows (latest windows 10 and latest windows 11 versions). * Running a memory diagnostic * Enabling/disabling XMP. * Verifying integrity of game files. * Forcing the game to run in the Vulkan engine instead of snowdrop. * Performing a complete reinstall of the game. * Replacing one of the .dll files with a ""repaired"" one. (this apparently fixed a crash for some people). * Forcing the game to boot in dx11 instead of dx12.  The following are fixes that I have tried, and HAVE worked.  * Running the game on my AMD integrated graphics.  Obviously running the game on an igpu doesn't offer playable performance, but the game will run, and I've yet to find any crash, even leaving the game open for several hours.  Due to this issue, I'm on the verge of selling my b580 and replacing it with a different card from another manufacturer that hopefully won't have the same issue.  Has anyone had a similar issue and knows the fix? Although I suspect the issue has to be resolved with a driver update.",Intel,2025-12-22 03:46:16,1
AMD,nw4g1zt,VR Support for ARC GPU? :),Intel,2025-12-27 02:03:19,1
AMD,nwenzma,"Hello, I'm experiencing an intermittent but severe system hard freezes on a prebuilt PowerSpec G441 desktop purchased from Micro Center 2 years ago. The freezes occur during gaming, mixed workloads (gaming + video playback), remote desktop usage (TeamViewer), and occasionally shortly after logging into Windows with no workload running.  When my PC hard freezes:  \- Mouse is unresponsive  \- Keyboard is unresponsive  \- No audio sound  \- No BSOD or error message     My PC does not recover and requires a manual power button reboot.  After a forced reboot, Windows loads normally and allows me to log in but the system may freeze again within 1–2 minutes, even at idle. Other times, the system may run normally for days before the issue reappears.  No relevant error logs are generated at the time of the freezes. Event Viewer only shows Kernel Power events related to the forced shutdown.  So far I've tried all possible solutions to resolve my issue:  \- Updated motherboard BIOS to the latest available version  \- Updated Windows 11 fully and also reinstalled Windows 11 through USB  \- Performed clean GPU driver reinstallations  \- Verified CPU and GPU temperatures (normal under load)  \- Ran hardware stress tests and diagnostics (CPU, GPU, memory, storage) and no failures detected  \- Tested each monitor individually (dual monitor setup)  \- Swapped HDMI/DisplayPort cables and ports  \- Tested my PC without background applications  \- Returned my PC to stock settings (no overclocks, no undervolts, no XMP changes beyond default)  I've also took my pc to a local repair shop, which they could not reproduce the issue but suggested it may be CPU or PSU related issue. However, when I took my PC to Micro Center, technicians ran similar diagnostics and stress tests but were also unable to reproduce the freezes. They have also tested the PSU voltages and it passed. Despite extensive testing, the issue remains unresolved and continues to occur consistently now.  Specs:  Operating System: Windows 11 Pro 25H2 (Build 26200.7462)  CPU: Intel Core i7-13700KF (stock settings)  GPU: NVIDIA GeForce RTX 3070 Ti 8GB (stock)  Motherboard: MSI PRO Z690-A WIFI (MS-7D25), BIOS v5.32  RAM: 32GB (2×16GB) DDR5-5600 (G.Skill)  Storage: WD Blue SN570 1TB NVMe SSD  Cooling: NZXT Kraken 240mm AIO liquid CPU cooler  PSU: 750W 80+ Gold (PowerSpec OEM, included with prebuilt)  Display: Dual monitor setup, tested individually  I gave a call to Intel and now waiting for a call back. Thank you and I appreciate any guidance!",Intel,2025-12-28 18:50:36,1
AMD,nwjovjk,"Does my 12700f support TME?  In specs, there's no single line about TME. 12700's specs declare TME support. Processor Identification Utility does not mention TME, `hwinfo64` shows TME in gray, and `cpuid -1 | rg TME` prints `TME: Total Memory Encryption = false`. There can i check whether my cpu support TME or not?",Intel,2025-12-29 14:08:35,1
AMD,nwkfoww,"I have an i5-13600kf and a asus strix b760-i motherboard and mostly it runs fine but in specific games I get random shutdowns. Most commonly hell divers and now expedition 33.  I found the vmin instability issue and patched my bios to the latest version now but I still get it happening. Is there any suggested settings changes that have been found to help or is the only course to look at an RMA? I’d rather not be out of a PC for a while so trying to seek help to avoid that if possible.  What’s I find odd is I never get any errors or BSOD. Just the monitor goes black and the system locks up, the fans are still running but it has to be shutdown by holding the power button.",Intel,2025-12-29 16:25:47,1
AMD,niosucw,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-10 00:13:23,1
AMD,niowtna,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-10-10 00:37:02,1
AMD,nicof1i,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-08 01:46:19,2
AMD,njq2uvf,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Intel,2025-10-16 01:31:17,1
AMD,nkq87np,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Intel,2025-10-22 04:59:31,1
AMD,nkxufey,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Intel,2025-10-23 11:35:54,2
AMD,nkwr1mw,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Intel,2025-10-23 05:27:23,1
AMD,nmj7ctn,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Intel,2025-11-01 13:51:07,1
AMD,nmsah2r,"u/vincococka ,Yes you can use that memory kit safely as long as you don’t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of Intel® Core™ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Intel,2025-11-02 22:47:21,1
AMD,nnd464j,"u/SuperV1234 If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor  need a replacement.",Intel,2025-11-06 04:10:00,1
AMD,np77ndt,"u/triptoasturias Before I share any recommendations, could you confirm if this is your exact system-[Inspiron 15 5510 Setup and Specifications | Dell](https://www.dell.com/support/manuals/en-my/inspiron-15-5510-laptop/inspiron-5510-setup-and-specifications/specifications-of-inspiron-15-5510?guid=guid-7c9f07ce-626e-44ca-be3a-a1fb036413f9&lang=en-us)? Also, may I know which driver version you’re using and where you downloaded it from—was it from Dell or the Intel Download Center?",Intel,2025-11-16 20:12:25,1
AMD,np8jpxo,"u/triptoasturias Your concern is related to **port and bandwidth limitations**. Most Inspiron models only support **HDMI 1.4**, which is limited to 4K at 30Hz or 2560×1440 at 60Hz. Ultra-wide resolutions like 3440×1440 often aren’t exposed because they’re outside the standard HDMI 1.4 spec. **USB-C to DisplayPort**: This works because DisplayPort has much higher bandwidth and supports ultra-wide resolutions natively. HDMI-to-DisplayPort Adapter, Unfortunately, **passive adapters won’t work** because HDMI and DisplayPort use different signaling. You’d need an **active HDMI-to-DisplayPort converter**, but even then It will still be limited by the HDMI 1.4 bandwidth from your laptop. So, you likely **won’t get 3440×1440 at 144Hz,** maybe 3440×1440 at 30Hz or 50Hz at best.",Intel,2025-11-17 00:32:25,1
AMD,np8oqxv,"u/mercurianbrat This spec can run **basic image generation workflows** (CPU mode or lightweight models), but **video generation and heavy diffusion models will struggle or fail** on this setup. AI Playground’s minimum requirements are currently tied to Intel Arc GPUs with 8GB or more of allocated VRAM.  Currently you can download the installer for discrete GPUs.  We will also publish an installer that will run on Intel Core Ultra-H with built-in Intel Arc GPU (please keep in mind that [Windows allocates half of the system RAM as VRAM](https://www.intel.com/content/www/us/en/support/articles/000020962/graphics.html) for integrated GPUs, so 16GB or more of system RAM are required)  and Intel Arc GPU discrete add in cards with 8GB or more of memory. AI Playground takes up 8GBs of  HDD/SDD requirements: 8GB w/o models,  \~50GB with all models installed.",Intel,2025-11-17 01:00:59,1
AMD,nqg4c7m,"u/mano109 As a general corporate policy, Intel Support does not comment on information about products that have not been released yet.  **Visit** our [Newsroom](https://newsroom.intel.com/) for the most recent announcements and news releases.",Intel,2025-11-24 00:10:02,1
AMD,nqg5tvg,"**265K all the way.** At 4K with a 5090, you're GPU-bound anyway. The 265K's lower memory latency beats the 285K's extra cache at that resolution, plus you save money for better RAM or cooling.",Intel,2025-11-24 00:18:36,1
AMD,nqry4gw,"u/hus1030  The mounting pressure from your AIO cooler can directly affect whether the system successfully completes POST. When the cooler is tightened too much, it can cause the CPU or motherboard to bend slightly, which may lead to poor or lost contact between the CPU and the socket pins. This prevents the processor from initializing properly, resulting in a no-POST condition. Installing a contact frame changes the pressure distribution compared to the stock retention mechanism, so overtightening the AIO screws can amplify this issue. On the other hand, if the screws are too loose, the CPU temperatures will rise because the cooler is not making proper thermal contact. To avoid these problems, ensure the AIO screws are tightened evenly in a cross pattern and do not exceed the manufacturer’s torque specifications. If the issue persists, you may need to verify that the contact frame is correctly installed or temporarily revert to the original retention bracket to rule out pressure-related problems.",Intel,2025-11-25 21:43:16,1
AMD,nrmm6fq,"u/BudgetPractical8748   Intel Default Settings may impact system performance in certain workloads as compared to unlocked or overclocked settings.  As always, system performance is dependent on configuration and several other factors.",Intel,2025-12-01 00:19:07,1
AMD,ns3syx7,Nope got cash back,Intel,2025-12-03 18:08:27,1
AMD,nt12wk5,"u/Any_Information429 Your HP EliteBook 830 G8 is experiencing boot issues due to corrupted Intel Management Engine (ME) firmware, which is a critical low-level system component that manages hardware initialization. This corruption is causing the blinking caps lock light, multiple restart attempts before successful boot, and the extended Windows startup times you've been experiencing. The BIOS showing ""ME Firmware Mode: Recovery Mode"" confirms this diagnosis. Since these issues began after your recent repair, it's likely that the Management Engine chip connections were disturbed or the firmware became corrupted during the service process.  To resolve this, you need to perform a forced recovery of the ME firmware by downloading the specific firmware version for your EliteBook model from HP's support website-[HP EliteBook 830 G8 Notebook PC Software and Driver Downloads | HP® Support](https://support.hp.com/us-en/drivers/hp-elitebook-830-g8-notebook-pc/38216726) and using specialized recovery tools to reflash the Management Engine. You also have check BIOS settings to ensure proper ME configuration and temporarily disable fast boot to allow complete initialization. If the firmware recovery doesn't resolve the issue, this may indicate hardware-level damage to the ME controller that occurred during the previous repair, which would require professional chip-level service or potentially warranty coverage since the problem originated after authorized service work. The good news is that once the ME firmware is properly restored, your system should return to normal boot times and eliminate the restart cycles you're currently experiencing.  USB flash recovery method is definitely worth trying first - it's designed specifically for these types of firmware corruption issues and should get your laptop back to normal boot times without all those frustrating restarts. Check here: [Support Search Results | HP®️ Support](https://support.hp.com/us-en/search/videos?q=BIOS) BIOS Videos",Intel,2025-12-09 00:22:32,1
AMD,ntk9thw,"Hi @[Content\_Magician51](https://www.reddit.com/user/Content_Magician51/) Upon checking, there is a new driver version available which is 32.0.101.7082. You may try this and use DDU method to make sure that you performed a clean driver installation. Here are the links of the latest driver and the steps on how to perform DDU.  [Intel® 11th – 14th Gen Processor Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/864990/intel-11th-14th-gen-processor-graphics-windows.html)  [How to Use the Display Driver Uninstaller (DDU) to Uninstall a...](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)",Intel,2025-12-12 00:44:01,1
AMD,ntyyo2o,"u/MISINFORMEDDNA  I took a look at your crash error, and here's what's going on. That error code you're seeing (0x00020001) is actually what's called a ""hypervisor error,"" which basically means it's a problem with Windows' virtualization stuff rather than directly being caused by your WiFi drivers.  The real culprits are more likely things like memory issues, BIOS problems, or conflicts with virtualization features like Hyper-V. I'd suggest running a memory test first (just search for ""Windows Memory Diagnostic"" in your start menu), and if you have any virtual machines or Docker running, try shutting those down temporarily, we'll probably need to dig deeper into hardware or system-level issues to really fix this one.",Intel,2025-12-14 12:43:17,1
AMD,ntz1is6,"u/strumpystrudel So what you're experiencing is actually pretty normal behavior for your laptop when it's unplugged - that 80% performance drop is totally expected and here's why. When your laptop is plugged into the wall, your CPU can run at much higher power levels (probably around 28W or more), but when you switch to battery, it gets severely limited to maybe 8-15W to preserve battery life. This is especially true for ultrabooks like the Gram that prioritize being thin and light over raw performance. The thing is, a lot of this power management happens at the hardware level with Intel's built-in systems, which is why all those Windows power settings you tweaked aren't really making a difference - the CPU is basically ignoring them and doing its own thing to save battery.  Now, for a game like Final Fantasy XI, you should still be able to get it running decently on battery with some tweaks, but expecting that same smooth 60 FPS at max settings is probably unrealistic given the fundamental power constraints of ultrabook design. Most ultrabooks see this kind of 60-80% performance hit on battery for any sustained workload, so you're definitely not alone in this.   But honestly, this is just how these thin and light laptops are designed to work - they're amazing when plugged in, but they have to make compromises when running on battery to actually give you decent battery life.",Intel,2025-12-14 13:04:59,1
AMD,ntz4fy4,"u/Aggravating_Gap_203 I'd recommend running Intel's Processor Diagnostic Tool first to rule out any hardware defects with the CPU itself. Just download it from Intel's website, run the test, and let us know if it passes or fails. While you're at it, try loading your BIOS defaults and make sure your power settings are at Intel's recommended specs - PL1 should be around 125W and PL2 around 181W for your 14600KF-[Intel® Core™ i5 processor 14600KF](https://www.intel.com/content/www/us/en/products/sku/236778/intel-core-i5-processor-14600kf-24m-cache-up-to-5-30-ghz/specifications.html)  [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html)  Your friend is actually spot on about the contact frame recommendation. Before you buy one though, try remounting your cooler one more time - make sure you're tightening the screws in an X-pattern and that everything is perfectly aligned. Sometimes it just takes that perfect mount to get things working right. Let us know what the Intel diagnostic shows and we can go from there!",Intel,2025-12-14 13:25:40,1
AMD,nurwhxx,Hi [RadioFr33Europe](https://www.reddit.com/user/RadioFr33Europe/) I sent a direct message to gather more details for me to review the case and check the status of your replacement request.,Intel,2025-12-19 00:03:08,1
AMD,nvbnrfy,"Hi [Designer-Let-7867](https://www.reddit.com/user/Designer-Let-7867/) For issues related to game bundles and how to claim it, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry/issues/error message during claiming.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-12-22 05:14:39,1
AMD,nvfrjth,"u/earwig2000 Let me check this internally. From what I see, you’ve already tried a lot of steps to address the game crash issue. I’ll share an update here as soon as I have more details, and I might need to collect some info from you for further analysis.",Intel,2025-12-22 21:29:55,1
AMD,nvmgqub,"u/earwig2000 Have you tried doing a clean installation of the graphics driver using [DDU ](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)and then installing the latest version from our [download center](https://www.intel.com/content/www/us/en/download/785597/intel-arc-graphics-windows.html)? After that, please retest the game. If the issue still persists, could you share your PSU make, model, and wattage? Also, check if Resizable BAR (ReBAR) is enabled in your BIOS. Finally, review the Event Viewer for any error messages or crash-related events, this will help us determine whether the problem is driver-level or application-related.",Intel,2025-12-23 22:44:39,1
AMD,nwglbda,u/outlander94 Kindly check this article: [Is Virtual Reality (VR) supported on Intel® Arc™ A-Series and...](https://www.intel.com/content/www/us/en/support/articles/000093024/graphics.html),Intel,2025-12-29 00:38:16,1
AMD,nwgmhqt,"u/Kai-juu We trust the technician’s diagnosis of the system. However, since the unit is a prebuilt, it is likely to have a tray processor. Based on our warranty terms and conditions, we can only replace boxed processors. For a faster turnaround time, please first verify whether the processor is tray or boxed using our website. Once you confirm the type, I can guide you through the next steps.  [Warranty Information](https://supporttickets.intel.com/s/warrantyinfo?language=en_US)  [Where to Find Intel® Boxed Processor Serial Numbers (FPO and ATPO)...](https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html)  [Warranty Policy for Intel® Boxed and Tray Processors](https://www.intel.com/content/www/us/en/support/articles/000024255/processors.html)",Intel,2025-12-29 00:44:27,1
AMD,nwohvpq,"u/Sk7Str1p3 I can see you've already tried several tools and are getting mixed results, which is definitely frustrating.  To help you out better, could you share what's driving this inquiry? Are you working on a security project, dealing with compliance requirements, or troubleshooting a specific feature? Understanding your use case will help me point you toward the right verification methods or suggest alternatives if needed. The more context you can provide, the better I can assist you!",Intel,2025-12-30 05:04:29,1
AMD,nwvjztq,"u/pheoxs you may try to follow this article [Computer Randomly Reboots or Shuts Down](https://www.intel.com/content/www/us/en/support/articles/000035903/processors.html) if the issue persist, disable Turbo boost If the motherboard BIOS allows and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor is affected and need a replacement.",Intel,2025-12-31 06:58:32,1
AMD,nktqa25,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Intel,2025-10-22 18:45:49,2
AMD,nmk5tv3,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Intel,2025-11-01 16:57:33,1
AMD,nnel13c,"Hi, I've tested disabling turbo and I still experienced issues. I've created a support request, case number 06728608, please take a look ASAP.",Intel,2025-11-06 12:05:51,1
AMD,np8oivj,"Actually, this is not the case. After spending hours on this issue, I've finally found the 31.0.101.4502\_A14 drive at Dell's website that is working without any problems with the 3440x1440. So your claim that HDMI 1.4 is limited to 2560x1440 is false. HDMI 1.4 can go further to 3840 x 2160 without any problems. It's unacceptable the lack of support for UWM from Intel graphics cards and drives. Many previous drivers let you choose 3440X1440 resolution. Why did Intel stop supporting this resolution in the last graphics drivers?",Intel,2025-11-17 00:59:39,1
AMD,nqkt81n,I was going with the 265K over the 9800X3D since the Intel stuff seems to get better 1% lows and smoother experience at 4k and above. But does DLSS change that? Does DLSS lowering the render resolution push the 9800X3D back into the lead?,Intel,2025-11-24 19:16:09,1
AMD,nvmmo42,"Tried running a clean driver reinstall using DDU. (I'm pretty sure I did this last install too but did it again to double check) and that didn't fix the issue.  ReBAR is enabled, it was on by default.  My PSU is the [Thermaltake Toughpower 650W Gold](https://www.thermaltake.com/toughpower-650w-gold-modular.html)  Windows event viewer did pick up the crash [(imgur link)](https://imgur.com/NhS5e6l), not sure what to make of it though.",Intel,2025-12-23 23:18:45,1
AMD,nwgvzmr,"u/Intel_Support Thank you for your guidance. My system is a prebuilt, so the processor never came in a box and is likely a tray CPU. Micro Center told me to reach out to Intel for warranty support, so I just want to confirm, should I be working with Intel directly or do you recommend contacting PowerSpec/Micro Center for the RMA? Thank you again, I look forward for the next steps!",Intel,2025-12-29 01:38:35,1
AMD,nwr1jrz,"Yes, I am information security hobbyist, I have a research project on countering attackers with physical access to the machines. Unfortunately, my current hardware is average gaming PC - i7 12700f with h670 motherboard and I cannot update hardware.",Intel,2025-12-30 16:12:50,1
AMD,nkvsdop,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Intel,2025-10-23 01:24:29,1
AMD,nnh8tyn,"u/SuperV1234 Hi, thanks for the update. I’ve reviewed case number and confirmed that it’s currently being handled by our **warranty team** for replacement. You should be receiving further instructions from them shortly.     [Guide to pack your faulty CPU](https://www.intel.com/content/www/us/en/content-details/841997/guide-to-pack-your-faulty-cpu.html)",Intel,2025-11-06 20:19:42,1
AMD,np8q866,"u/triptoasturias this explains, The generic Intel® driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing Intel generic graphics driver will overwrite your handpicked OEM graphics driver (in your case Dell driver). Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see this [article.](https://www.intel.com/content/www/us/en/support/articles/000096252/graphics.html)",Intel,2025-11-17 01:09:54,1
AMD,nvz4tef,"u/earwig2000 ﻿Thank you for sharing this information. I will begin investigating the issue and attempt to replicate it on our end. I'll post an update here or notify you directly once there are any developments. If I need further details, I'll reach out to you here. I appreciate your patience as I work on this matter.   [](javascript:void(0);)",Intel,2025-12-26 04:28:47,1
AMD,nwnx92g,"u/Kai-juu According to our warranty policy, RMAs for tray processors must be handled by the original place of purchase, as clearly stated in the article I referenced.",Intel,2025-12-30 02:59:10,1
AMD,nwvn4dm,"u/Sk7Str1p3 I see, let me loop in our product support engineer on this one and dig into it a bit more. I'll circle back with you once I have a clearer picture on the TME support situation.",Intel,2025-12-31 07:26:19,2
AMD,nnd9ral,Unfortunately my Dell warranty support has ended and so far the forum there has not been able to help either. The removal tool works but Killer just keeps coming back.,Intel,2025-11-06 04:50:53,1
AMD,nnd9w3h,"I also went to Dell, typed in my service tag # and cant find any other ethernet drivers.",Intel,2025-11-06 04:51:52,1
AMD,nvz6y15,"If it helps, I'm also using   CPU - Ryzen 5 7600   RAM - Corsair Vengeance 32gb DDR5 6000mhz cl36  SSD - Crucial p3 plus 1tb  Motherboard - MSI b650m-a",Intel,2025-12-26 04:44:54,1
AMD,nx27aj0,"Hello, have you received a reply yet?",Intel,2026-01-01 10:17:28,1
AMD,nwgjs6p,u/earwig2000 Please check your inbox; I’ve sent you a message.,Intel,2025-12-29 00:30:28,1
AMD,nx7bu85,u/Sk7Str1p3 ﻿I’m still reviewing the details to ensure I provide you with the most accurate information. I’ll update you as soon as I have a clear answer.  [](javascript:void(0);),Intel,2026-01-02 04:55:46,1
AMD,nxygejg,"u/Sk7Str1p3 I'm pleased to confirm that TME is indeed supported on your i7-12700F processor. TME is a security feature that encrypts the entire system memory using AES encryption with a hardware-generated key. The i7-12700F is part of Intel's 12th generation Alder Lake family, which includes TME support as a standard security feature.  However, TME functionality requires both processor support and proper system configuration:  Verification methods:  * BIOS/UEFI security settings * System diagnostic tools that enumerate processor security features  Important considerations: TME enablement depends on the motherboard BIOS implementation and may be disabled by default on many systems.  To assist you further, I need the following information:  * Make and model of your motherboard, as the TME feature may be disabled in the BIOS settings  For your reference, please find the official documentation: 12th Generation Intel® Core™ Processors Datasheet, Volume 1 of 2  [655258](https://cdrdv2.intel.com/v1/dl/getContent/655258) (Page 40) - This document contains the specific technical details about TME support for your processor.  Once you provide your motherboard information, I can guide you on how to check and potentially enable TME in your system's BIOS settings.  Please let me know if you have any additional questions.",Intel,2026-01-06 05:10:19,1
AMD,nxzgti7,My motherboard is Asrock H670-M Pro RS. firmware version: 18.01,Intel,2026-01-06 10:30:53,1
AMD,ny5a7jq,"Your Intel i7-12700F processor does support TME (Total Memory Encryption), but the issue lies with your motherboard implementation rather than CPU capability. The ASRock H670M Pro RS with firmware 18.01 appears to either disable TME at the platform level or doesn't expose the necessary BIOS controls for consumer users. This explains why your diagnostic tools show ""TME = false"" and why HWiNFO64 displays it in gray. While you could check for TME-related options in your BIOS under Advanced → CPU Configuration or Security sections, consumer-grade H670 motherboards often omit these enterprise security features from their interfaces. Your best course of action is to contact ASRock technical support directly, providing your exact motherboard model, BIOS version, and specifically asking about TME support and whether future firmware updates might include these controls.",Intel,2026-01-07 04:53:34,1
AMD,nfolbmr,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Intel,2025-09-22 23:20:45,52
AMD,nflslxh,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Intel,2025-09-22 14:31:25,45
AMD,nfm76rf,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Intel,2025-09-22 15:42:11,34
AMD,nfnaznn,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Intel,2025-09-22 18:52:47,10
AMD,nfpb0n3,"i think it's bad for us, consumers",Intel,2025-09-23 02:12:30,3
AMD,nflx3jg,Was the team up really to crush AMD or Nvidia's answer to enter China?,Intel,2025-09-22 14:53:26,7
AMD,ng4ik7u,AMDware unboxed only cares about AMD anyway,Intel,2025-09-25 13:37:58,3
AMD,nfm1wz0,This hurts the arc division way more than this could ever hurt amd.,Intel,2025-09-22 15:16:45,15
AMD,nfoivfo,They will crush user's wallet,Intel,2025-09-22 23:04:45,2
AMD,nftk1b6,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Intel,2025-09-23 19:23:17,2
AMD,nfv8a1x,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Intel,2025-09-24 00:40:38,1
AMD,ng0xe2q,Remember Kaby Lake G? No? This will also be forgotten soon.,Intel,2025-09-24 22:04:55,1
AMD,ng2ck7r,Yes.,Intel,2025-09-25 03:11:04,1
AMD,ngzb138,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Intel,2025-09-30 09:53:37,1
AMD,nhckqoj,Foveros baby!,Intel,2025-10-02 11:51:31,1
AMD,nfmdikv,AMDUnboxed on suicide watch.,Intel,2025-09-22 16:12:26,0
AMD,nfn7em1,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Intel,2025-09-22 18:34:17,1
AMD,nfmh1rz,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Intel,2025-09-22 16:29:30,1
AMD,nflsfzz,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Intel,2025-09-22 14:30:36,-1
AMD,nfmihzp,Ohh noooerrrrrrrrr,Intel,2025-09-22 16:36:30,0
AMD,nfvhp2d,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Intel,2025-09-24 01:35:50,0
AMD,ngcdph8,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Intel,2025-09-26 17:45:54,0
AMD,nfnsqrd,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Intel,2025-09-22 20:29:14,-2
AMD,nfmr987,welcome to the Nvidia and amd duopoly,Intel,2025-09-22 17:17:38,-6
AMD,nfmy4sf,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Intel,2025-09-22 17:49:51,3
AMD,nfn29jk,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Intel,2025-09-22 18:09:19,-6
AMD,nfma1mz,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Intel,2025-09-22 15:55:41,4
AMD,ng59q6y,a partnership doesnt mean they get free reign over license lol,Intel,2025-09-25 15:49:13,2
AMD,nfodhll,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Intel,2025-09-22 22:29:59,-2
AMD,nfm0v8n,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Intel,2025-09-22 15:11:42,21
AMD,nfm237h,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Intel,2025-09-22 15:17:35,6
AMD,nfm66b6,Why would it?,Intel,2025-09-22 15:37:21,9
AMD,nfm5ru9,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Intel,2025-09-22 15:35:26,11
AMD,nfmevnp,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Intel,2025-09-22 16:19:02,2
AMD,nhzfr23,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2025-10-05 23:57:54,1
AMD,nfm4tdl,Past != Future,Intel,2025-09-22 15:30:48,3
AMD,nfnb9ui,"nvidia also used to make motherboard chipset, with mixed success.",Intel,2025-09-22 18:54:18,2
AMD,nfoib8h,FSR 4 looks like the later versions of Dlss 2 did,Intel,2025-09-22 23:01:07,7
AMD,nfo2bk7,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Intel,2025-09-22 21:21:54,2
AMD,ng9kwgg,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Intel,2025-09-26 07:01:28,0
AMD,nfmqv0q,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Intel,2025-09-22 17:15:46,6
AMD,nfoz7q1,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Intel,2025-09-23 00:52:30,0
AMD,ng6jjhd,Never said that.,Intel,2025-09-25 19:28:08,-1
AMD,nfm1xh4,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Intel,2025-09-22 15:16:49,14
AMD,nfm6de6,Why do so many people think that this will kill ARC?,Intel,2025-09-22 15:38:18,11
AMD,nfm8iqb,The market for Arc is the same as for Nvidia.,Intel,2025-09-22 15:48:31,10
AMD,nflxpfk,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Intel,2025-09-22 14:56:21,3
AMD,nfmpyxd,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Intel,2025-09-22 17:11:35,3
AMD,nfncgcw,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Intel,2025-09-22 19:00:37,2
AMD,nfpd2vm,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Intel,2025-09-23 02:26:29,1
AMD,nfq8t4o,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Intel,2025-09-23 07:05:35,0
AMD,ng2cqnz,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Intel,2025-09-25 03:12:17,1
AMD,ng2dv4q,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Intel,2025-09-25 03:20:00,0
AMD,nfm6s3o,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Intel,2025-09-22 15:40:15,5
AMD,nfmtpsf,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Intel,2025-09-22 17:29:18,13
AMD,nfm7mbh,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Intel,2025-09-22 15:44:16,-2
AMD,nfm91t2,Nvidia does not have an A310 competitor.,Intel,2025-09-22 15:51:01,-1
AMD,nftpodz,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Intel,2025-09-23 19:50:16,2
AMD,nfq8uoi,I have not lied,Intel,2025-09-23 07:06:03,1
AMD,ng2iom6,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Intel,2025-09-25 03:54:05,1
AMD,ng17you,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Intel,2025-09-24 23:06:01,2
AMD,nfonq2p,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Intel,2025-09-22 23:36:31,3
AMD,nfm9qnn,Intel doesn't have a current gen A310 competitor either.,Intel,2025-09-22 15:54:15,8
AMD,nfma2t0,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Intel,2025-09-22 15:55:51,1
AMD,nfm0jnd,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Intel,2025-09-22 15:10:08,2
AMD,nftspxa,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Intel,2025-09-23 20:04:43,2
AMD,nfrlmki,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Intel,2025-09-23 13:46:08,0
AMD,nfu9p3r,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Intel,2025-09-23 21:25:34,2
AMD,nfpz6gm,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Intel,2025-09-23 05:25:09,3
AMD,nfqz1sj,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Intel,2025-09-23 11:29:37,2
AMD,nfrltqc,The later versions of Dlss 2 look like Dlss 3,Intel,2025-09-23 13:47:11,2
AMD,nfq8fx8,Nvidia probably feels the same about their low end SKUs.,Intel,2025-09-23 07:01:31,1
AMD,ngn9bw4,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Intel,2025-09-28 13:27:33,1
AMD,nfrmbyw,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Intel,2025-09-23 13:49:53,0
AMD,nfq8hgn,Yeah lol,Intel,2025-09-23 07:02:00,1
AMD,nfrmiwb,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Intel,2025-09-23 13:50:54,2
AMD,nfrmnqg,"""Everyone I don't like is biased""-ass answer",Intel,2025-09-23 13:51:36,1
AMD,naz5fcr,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Intel,2025-08-27 16:46:20,47
AMD,nazj7k0,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Intel,2025-08-27 17:50:18,15
AMD,ncitd5t,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Intel,2025-09-05 08:24:57,1
AMD,naz95i7,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Intel,2025-08-27 17:04:24,1
AMD,naz6p5r,Just hodl until you get the biscuits,Intel,2025-08-27 16:52:25,38
AMD,naz7hoi,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Intel,2025-08-27 16:56:15,11
AMD,nazkvn7,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Intel,2025-08-27 17:57:46,3
AMD,nazb0q3,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Intel,2025-08-27 17:13:24,2
AMD,nazazfk,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Intel,2025-08-27 17:13:14,32
AMD,nb330i7,I thought Arrow Lake refresh was in the cards for 2025.,Intel,2025-08-28 05:57:34,1
AMD,nb04wuk,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Intel,2025-08-27 19:33:00,-1
AMD,nazl60x,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Intel,2025-08-27 17:59:03,4
AMD,nb8e4jm,This entire thing is a mobile roadmap so why are you here?,Intel,2025-08-29 00:35:26,2
AMD,nb1w9ji,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Intel,2025-08-28 01:01:42,3
AMD,nazbb9i,"Yeah if it's Surface roadmap, it's a nothing burger.",Intel,2025-08-27 17:14:47,8
AMD,nb02q5l,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Intel,2025-08-27 19:22:51,1
AMD,nb171ho,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Intel,2025-08-27 22:38:31,3
AMD,nb1xsvd,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Intel,2025-08-28 01:10:35,1
AMD,nb1vmub,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Intel,2025-08-28 00:58:05,3
AMD,n73y5u9,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Intel,2025-08-05 20:10:52,33
AMD,n73iurw,"I like how they just throw random words around to pad their ""article"".",Intel,2025-08-05 18:43:24,66
AMD,n73t59w,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Intel,2025-08-05 19:42:58,28
AMD,n741jok,"""leaks""",Intel,2025-08-05 20:28:54,7
AMD,n74lp88,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Intel,2025-08-05 22:16:47,5
AMD,n77fxeb,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Intel,2025-08-06 10:20:26,1
AMD,n7koh28,Scared of their rumor?  Lets release our rumor!,Intel,2025-08-08 09:59:01,1
AMD,n758aa3,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Intel,2025-08-06 00:22:30,1
AMD,n76sf5r,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Intel,2025-08-06 06:42:22,-2
AMD,n74sw92,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Intel,2025-08-05 22:56:59,-6
AMD,n75af32,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Intel,2025-08-06 00:34:46,4
AMD,n754qqc,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Intel,2025-08-06 00:02:20,9
AMD,n7582e3,bLLC is not stacked cache,Intel,2025-08-06 00:21:15,4
AMD,n767h6n,What do you consider random? The article was perfectly clear.,Intel,2025-08-06 03:54:03,10
AMD,n748zl3,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Intel,2025-08-05 21:08:09,14
AMD,n741l8k,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Intel,2025-08-05 20:29:08,2
AMD,npq13eo,"Isn't the latency similar? The advantage of the 3d cache is an even larger cache size, while the advantage of what intel is going to do with bllc is that there is less of a thermal issue so the clocks can be higher.",Intel,2025-11-19 19:51:12,1
AMD,npq0u97,Probably only on the skus with less cores.,Intel,2025-11-19 19:49:56,1
AMD,n76jhxw,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Intel,2025-08-06 05:24:22,1
AMD,n7s4doe,Yeah. Definitely just you,Intel,2025-08-09 14:25:48,5
AMD,n757qwt,You could literally make that claim with any CPU performance increase.,Intel,2025-08-06 00:19:24,9
AMD,n7615sm,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Intel,2025-08-06 03:11:36,7
AMD,n77a60y,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Intel,2025-08-06 09:28:27,2
AMD,n76ahn8,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Intel,2025-08-06 04:15:00,6
AMD,n757yiy,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Intel,2025-08-06 00:20:37,1
AMD,n749ios,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Intel,2025-08-05 21:11:01,17
AMD,npq1inb,"No, because it would be the same die. Just won't fit.",Intel,2025-11-19 19:53:18,1
AMD,n76z9ct,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Intel,2025-08-06 07:45:28,2
AMD,n77cfm2,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Intel,2025-08-06 09:49:46,4
AMD,n77cdot,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Intel,2025-08-06 09:49:17,5
AMD,nam3zf1,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Intel,2025-08-25 17:06:43,0
AMD,n75cvtk,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Intel,2025-08-06 00:48:56,2
AMD,n7ddfzp,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Intel,2025-08-07 06:10:13,1
AMD,n7dcz9u,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Intel,2025-08-07 06:06:08,1
AMD,nax8p6j,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Intel,2025-08-27 10:26:21,1
AMD,n7ddyd1,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Intel,2025-08-07 06:14:41,1
AMD,n7deaek,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Intel,2025-08-07 06:17:37,1
