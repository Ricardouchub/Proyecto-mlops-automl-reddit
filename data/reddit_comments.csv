brand,comment_id,text,subreddit,created_utc,score
Intel,nzd1gfc,"that amt of hair in a clean room, steve should wear a hooded rain coat lol.",hardware,2026-01-13 15:00:01,160
Intel,nzcvzjv,I thought that was Denis lol,hardware,2026-01-13 14:32:09,60
Intel,nzd3sjp,"Yeah this is a really surprisingly open factory tour, at least compared to when Sapphire took Linus and Alex along. It feels unprecedented to see this much access and insight, but that's the goodwill nurtured by Steve paying off in spades. The lament about AMD and Nvidia ditching the sub-250 side is real, so having Arc actually be a usable option down here is going to be significant later on.",hardware,2026-01-13 15:11:40,38
Intel,nzfe7p7,"Pretty good video with a lot of detail I haven't seen in previous factory tours. I probably still won't use an Intel GPU for gaming, but for a media server it's probably sufficient.  Side note: It's funny how whenever a GN video gets posted, the same cast of characters comes out and writes essays criticizing the video, apparently without watching it. I guess that's what it means to ""make it"".",hardware,2026-01-13 21:41:25,23
Intel,nzd7t05,"**GN:** NVIDIA and AMD abandoned this segment!  #Reality:  **B580:** $249 USD  **9060 XT 8GB:** $299 USD  **9060 non-XT:** $259 USD  **5060:** $299 USD  **5050:** $249 USD  I wouldn't say this is abandoned. I will say though the actual factory tour is cool, great content. But a dumb headline/title for the video.",hardware,2026-01-13 15:31:02,107
Intel,nzf5m3z,"Amazing video, GN is so good with content like this",hardware,2026-01-13 21:01:37,9
Intel,nzd8jmh,"rtx 5050 149 mmÂ² 128bit has similar performance to B580 272 mmÂ² 192bit. Selling for same msrp, winning by not ""trying"". Love how 5050 is actually at msrp now cheapest b580 is $349+  ""intel's gpu division seems like the only place in tech right now where the customers arent getting shafted these days""  People just want their Nvidia gpus cheaper  Nvidia crashouts making people hype an even worse product at current prices lol  [https://imgur.com/a/8iRI87Q](https://imgur.com/a/8iRI87Q)",hardware,2026-01-13 15:34:35,11
Intel,nzgtt65,"Love the Sparkle heatsink aesthetic, just wish there were some higher end card offerings from them instead of just Intel's lower midrange GPUs.",hardware,2026-01-14 02:14:38,4
Intel,nzd585j,He probably just single handedly killed 6 wafers worth of intel GPUs.,hardware,2026-01-13 15:18:37,2
Intel,nzd6pvl,AKA 'NVIDIA and AMD are going where the real money is.',hardware,2026-01-13 15:25:50,-1
Intel,nzcwvps,Finally a video that isnâ€™t â€œAI badâ€ or â€œcompany X badâ€,hardware,2026-01-13 14:36:46,-23
Intel,nzhjk4i,"Woah, a GN video that is actually interesting and informative for once instead of just ranting about the fact hardware companies exist to make money, not please gamers.",hardware,2026-01-14 04:51:57,-2
Intel,nzdfxes,That hat is so useful,hardware,2026-01-13 16:08:37,-5
Intel,nzdz3jo,Making them isn't important selling them is.  The reality is that this segment doesn't actually exist it has no buyers in it.  Seems intel is truly doomed trying to win segments that if they even exist aren't big enough to pay back their R&D even if the dominate them.,hardware,2026-01-13 17:48:25,-14
Intel,nzdogu2,"To be fair this is not a clean room, just SMT assembly. Basically, soldering components. A lose hair might be a bit of a problem (kind of like in any factory), but his hair is tied and doesn't seem to be an issue.  At an actual clean room (where the actual silicon is processed and etched), the standards can become very crazy. Some parts of it aren't even accessible to humans, just automated lines to avoid contamination.",hardware,2026-01-13 16:47:26,95
Intel,nzd8zmt,"Gaming Jesus could walk on wafers without corrupting a single tile, His body is that pure.",hardware,2026-01-13 15:36:40,-20
Intel,nzd5ucm,"same thought, just feels disrespectful",hardware,2026-01-13 15:21:37,-31
Intel,nzdzd0h,"He desperately needs to learn how to take care of his hair. I have no idea why nerds think a dry, frizzy mess of hair is some kind of enviable quality.   I feel like I'm walking into friday night magic every time he pops up.",hardware,2026-01-13 17:49:38,-19
Intel,nzhet37,"Intel hired him on the spot when they saw the ""Live, Laugh, Liao"" sign",hardware,2026-01-14 04:19:24,9
Intel,nzd0sl2,"who knew he can into GPU manufacturing? I thought he was a video editor and first real ""why are you employed"" guy on LTT.",hardware,2026-01-13 14:56:41,15
Intel,nzipwgx,I'm pretty sure Denis has neither long hair nor beard. Though to be fair I haven't seen him recently.,hardware,2026-01-14 11:05:09,1
Intel,nzhufav,"Itâ€™s crazy. I think Steve and GN are doing the lords work with their nonstop, accurate coverage of the absolute shit state of the computer component markets and the megacorps that perpetuate it. And their seemingly endless support and advocacy for *consumers*.   But then I come in this sub and see people are actuallyâ€¦ against this? I donâ€™t understand it. Probably the same mfs that bought MKBHDâ€™s wallpaper app lmfao",hardware,2026-01-14 06:14:39,9
Intel,nzd9rkp,"Wasn't that the B310? That was supposed to be the $100 bracket. To be fair that's an almost useless tier nowadays because iGPUs are capable of similar performance or even outperforming the cards in those brackets. Like the GT710 make no sense nowadays (their last attempt was what, the GeForce GTX 1630?), even though they sold like hotcakes for offices and for people who just wanted HDMI outs.",hardware,2026-01-13 15:40:18,61
Intel,nzdtxiv,"They're talking about the A310 and A380, of which Nvidia doesn't have anything made in this decade to compete with and AMD has the 6400 that came out 5 years ago.  The only cards with modern features in that price segment that consumers can directly buy are the A310 and A380.",hardware,2026-01-13 17:24:22,40
Intel,nzgaa45,He(Lucas) stated $100 was the market that was abandoned.  10:37  > Steve: So why still making A310?  > Lucas: Because what's the competitor have? Nvidia? like... GT710? GT1030? (laughs) No way. So literally Nvidia AMD already give up the segment of this like... $100 price card.,hardware,2026-01-14 00:24:50,13
Intel,nzfp313,This guy was talking about the A310 which is a $100 GPU. Basically said that the only other options at that price bracket are either a GT 710 or a GT 1030. And from AMD you can still get an old RX 550. The A310 may be slow but it beats those two gpu's by a mile.,hardware,2026-01-13 22:32:49,11
Intel,nze3uy1,> 9060 non-XT: $259 USD >  >   LOL. Good luck finding it. It's OEM exclusive,hardware,2026-01-13 18:09:42,10
Intel,nzdcsu3,The title is a literal quote from Lucas,hardware,2026-01-13 15:54:17,26
Intel,nzdb9mw,The title is in quotes. That is from Sparkle.,hardware,2026-01-13 15:47:16,9
Intel,nzejfd4,Now do SR-IOV and 16GB RAM for under $400.,hardware,2026-01-13 19:18:41,2
Intel,nzhv1ka,B580 has 12gb of vram though,hardware,2026-01-14 06:19:46,1
Intel,nzitx93,"Sad that all of those are 8gb cards. Even if comparable or faster than the B580 aside from that, the lack of vram makes them far inferior products imo.",hardware,2026-01-14 11:38:31,1
Intel,o07mcza,I just wish all of those options were more compelling tbh. I think what Steve was trying to say was that AMD and Nvidia have just half assed that market segment at the expense of the consumer. All these being 8gb cards for $250-$300 is kinda ridiculous in 2026. Iâ€™ll take a b580 over a 9060xt 8gb or 5060 any day of the week.,hardware,2026-01-18 01:21:19,1
Intel,nzenlzf,Go to r/PCMasterrace and they will downvote you into oblivion for even MENTIONING the possibility of gaming on a 5060 let alone 5050 XD,hardware,2026-01-13 19:37:44,2
Intel,nzdm48p,The title is just playing the YouTube algorithm game. It's stupid but they have to do it.,hardware,2026-01-13 16:36:44,-7
Intel,nze22z8,"> Love how 5050 is actually at msrp now cheapest b580 is $349+  Just an FYI but B&H has the [Acer Nitro B580 for $249.99](https://www.bhphotovideo.com/c/product/1874395-REG/acer_dp_z4bww_p01_nitro_oc_arc_b580.html) and the [Intel Limited Edition model for $259.99](https://www.bhphotovideo.com/c/product/1869297-REG/intel_31p06hb0ba_arc_b580_limited_edition.html). So you can get B580 for MSRP, but for how long who knows.",hardware,2026-01-13 18:01:44,21
Intel,nzdnidn,"die area is not the only cost indicator. B580 actually uses N5 fab, which is likely cheaper than N4, used by 5050. In reality, B580 only has about ~15% more transistors and if we assume N5 is cheaper per transistor and N5 has higher yields (by being more mature) i'd say their die cost might be very similar.  While yes, it has wider memory, the chips are clocked lower, so they can buy slower bins, reducing per chip cost.   B580 has more power draw, which in turn costs more for power delivery and cooling.  All in all, AIB manufacturer likely has lower margin per card as it stands, so i wouldn't be surprised if intel is taking a lower margin on the gpu/gddr combo to get more market share.  Nvidia on the other hand optimized their cost REALLY well, as they have been doing GPUs for almost 30 years.",hardware,2026-01-13 16:43:04,6
Intel,nzhfs1m,"Nvidia is getting such good performance out of such a small die because they're the best. Simple as. They've been doing GPU's for decades. It's not unexpected that at this stage Intel needs to use a larger die to match the performance - it would be incredibly surprising if that wasn't the case.  But a small *part* of that die size advantage comes from that narrow 128bit bus, and *part* of B580's appeal is its wider bus and subsequently more VRAM.",hardware,2026-01-14 04:25:57,1
Intel,nzppqcd,"I see b580 for â‚¬ 260. That's VAT included. rtx5050 is similarly priced.     And i found $300 lot on US amazon.  Considering other post, maybe search better than 1 place or something.",hardware,2026-01-15 12:00:19,1
Intel,nzd4pls,"Isn't the title pretty much ""AMD & NVidia bad""?",hardware,2026-01-13 15:16:07,58
Intel,nzd1pn1,Although even then the thumbnail is framed negatively.   But I much prefer these to Steves endless negativity ragebait.,hardware,2026-01-13 15:01:19,4
Intel,nzcyycp,> finally a company that isnt bad   ftfy,hardware,2026-01-13 14:47:24,-25
Intel,nzklu8k,"GN makes videos like these all the time, you just aren't watching them.",hardware,2026-01-14 17:20:56,3
Intel,nzgji2p,"Did you even watch the video? they are selling, and they are selling out, so much so that they want to ramp up production so they can push out more.",hardware,2026-01-14 01:15:59,8
Intel,nzfbt5x,I do not know if the numbering scheme from my workplace is common across the industry but the smt assembly would be in a class 5 or 6 clean room and the fabrication itself would be a 1 or 2 class clean room,hardware,2026-01-13 21:30:27,19
Intel,nzglawf,"I think it still matters to a certain point, thats why everyone in the factory is wearing a hat. The Factory boss decided to roll RNG dice and say *""Fck it, that small hat is fine, even tho wearing it is pointless now; I'll just pray nothing bad happen*"". lol  What hilarious is when you think about what going through uninformed factory-employee's head, after they saw some guy(Steve) walk-into the factory like that. Definitely a lot of ""WTF"" moment going through their mind lmao.",hardware,2026-01-14 01:26:15,3
Intel,nzdb445,"The factory boss told me not to bother tying my hair (""ä¸ç”¨ä¸ç”¨ä¸ç”¨, æ²¡äº‹å„¿æ²¡äº‹å„¿æ²¡äº‹å„¿. è¿™æ ·å¯ä»¥çš„"") when I started to put it under the hat... and after asking for a larger hat or hairnet.",hardware,2026-01-13 15:46:33,129
Intel,nze10ie,"> same thought, just feels disrespectful  It's a good thing you were there to personally witness the interaction so that we'd all know exactly how disrespectful Steve was being before holding everyone at gunpoint to force them to let him shoot the video without first fixing his hair.",hardware,2026-01-13 17:56:59,11
Intel,nzdfs8o,Nothing really surprises me with regards to gamers nexus at this point.  Edit: Downvote me all youâ€™d like. Theyâ€™ve been leaning incredibly hard into the rage bait type content of late.,hardware,2026-01-13 16:07:57,-55
Intel,nze0k30,I'm sure getting beauty tips from random redditors who have never left the basement is of utmost importance to Steve.,hardware,2026-01-13 17:54:57,18
Intel,nzi9ktg,"So, pray tell, who declared you the holder of absolute truth in terms of hair? What authority do you hold that allows you to determine what other people should do with the hair on their body?",hardware,2026-01-14 08:31:05,3
Intel,nziqu89,The other guy.,hardware,2026-01-14 11:13:11,1
Intel,nzklo11,This sub is genuinely weird. They complain about PCMR when the level of discourse here is much worse and a lot more mean spirited.,hardware,2026-01-14 17:20:09,4
Intel,nzk72ao,"They are truly rage baiting drama mongers sadly. The fact that the causes they support happen to be semi legit and consumer friendly in nature does not take away from the fact that they are essentially drama and negativity for profit at this point, and often out of touch with reality. You can be positive, polite, and have reporting standards while still calling out shitty behavior and ripping bad practices to pieces.   Companies are not your friend. That includes GN. They stoke hardware enthusiast anger for their own gain. Personally i think the harm they do to the space/peoples mentality is farm more than the amount of good most (key word most, fire risks and stuff deserve prompt drama) of their coverage does.   Even this title takes a topic and very educational opportunity to provide actual hardware coverage and immediatly tryâ€™s to stoke negative furry about some aspect of the hardware industry. Its just non stop with them. I dont think its healthy for anyone to watch then honestly, which is sad as i used to enjoy their review/news. I hope for their sake they are not actually so perpetually angry and they are doing what they do to knowingly manipulate their audience for increased views/engagement. I cant imagine living life so constantly angry at every possible point over computer hardware, as much as i am saddened by the current state hardware and love gaming.   As tough as it is with rising prices and such and sad to think of what we could have instead, modern hardware, gaming, and such is in a nearly historic good state (realistic cost to perf is probably lowest in history other than brief periods such as the 30 series launch), and is a very cost effective hobby compared with other ones for the time you get out of it. Many great games launch all the time (even if big name traditional games have largely went to trash). They are easier to run with hardware (like the ARC gpu) being quite cheap and capable. People, like GN, need to temper their overexagerated frustration with a dose of real world perspective and objectivity.",hardware,2026-01-14 16:13:54,-1
Intel,nzdm2vc,"Might be just the thing for older machines and a GNU Linux (or BSD like) migration. Or as a pass through GPU to Jellyfin, Emby, or Plex for media transcoding. Edit: or Small form computing tied together with a iGPU enhancing game performance...",hardware,2026-01-13 16:36:33,13
Intel,nze76iz,yeah there is a reason the a310 cards they are making are 4 hdmi out.  Gotta know your market,hardware,2026-01-13 18:24:10,17
Intel,nzeeiko,I assume those are being marketed to OEMs who make digital billboard systems or something. No idea why else you would want 4 HDMI ports on a card.,hardware,2026-01-13 18:56:32,5
Intel,nzddzc6,"> Wasn't that the B310? That was supposed to be the $100 bracket.  It's the A310, which is Alchemist and it's pretty much dogwater for anything beyond being a 'display out' card. The claim that NVIDIA has abandoned that segment is stupid... They've had offerings in this segment for years, plus anyone smart will just go buy a used GPU, your money goes way further. For example, the GTX 1650 performs basically 10-15% better, has better drivers, better encoding and generally is better supported. It's older, but I mean Alchemist wasn't exactly impressive either when it released and pretty much Intel has moved onto Battlemage and Celestial driver optimisations instead.   Plus let's be real here I went and searched and I found only weird places tend sell the brand new A310, the only local computer shop I found selling it in Australia for instance is a big one which is good surprisingly, but they had it for $189 AUD, a total rip tbh. A used 1650 is like $100 AUD and a used 1650 SUPER is like $120 AUD. No reason to buy an A310 tbh, pocket the cash and move on. Or if you're really intent on spending around that much buying a used RTX 2060 for like $20 AUD more, so a total of $200-210 AUD is better. Then on the AMD side you have the RX 6400 which had an MSRP of $159 USD and it's again a solid 10-15% faster, but much better off buying a used 6500 XT or 6600. Neither company has abandoned the segment, they had offerings for years and the used market basically obliterated any point to buying a brand new card like this.  >  To be fair that's an almost useless tier nowadays because iGPUs are capable of outperforming the cards in those brackets.  Yep this too. Honestly, I mean it's cool they're showing how they make cards on this factory tour, but to be like ""NVIDIA and AMD abandoned this segment"" is stupid when it comes to the A310. Almost anything these days is better than an A310.  >  Like the GT710 make no sense nowadays (their last attempt was what, the GeForce GTX 1630?), even though they sold like hotcakes for offices and for people who just wanted HDMI outs.  GT710 hasn't made sense for like 8 years at least, even when it was relevant people laughed at it, but it did the job for 'display out' and such which was all that mattered. GTX 1630 was okay but it was supposed to be $149 USD MSRP and it came out for like $200 USD in most stores due to GPU shortage at the time, not much NVIDIA could really do about that.",hardware,2026-01-13 15:59:39,-5
Intel,nzdyz47,They haven't made anything because the market has moved on. Intel might be making these but are they selling them?,hardware,2026-01-13 17:47:52,-12
Intel,nzdhs0m,"[Is it in reference to this moment in the video?](https://youtu.be/YwrUxG26ulk?t=648) If so, he doesn't say that as a literal quote, he says ""give up the segment"". Unless there's another quote somewhere else which I missed which may be possible or maybe it was edited out or cut from the video? I can't remember everything he said tbh but there was a lot of good information in this video and I think the title is better off without it. If it was called ""Intel Arc GPU Factory Tour with Sparkle"" I would have insta-clicked to watch anyways.",hardware,2026-01-13 16:17:04,13
Intel,nzdelag,You know what you're doing with the title... It's honestly unnecessary to use it on a factory tour video tbh.,hardware,2026-01-13 16:02:26,41
Intel,nzejqso,Stop defending your clickbait.,hardware,2026-01-13 19:20:08,12
Intel,nzf28eu,do you wanna address this then? Its kinda cringe ignoring the rest of the post  >**GN:**Â NVIDIA and AMD abandoned this segment!     >**B580:**Â $249 USD  >**9060 XT 8GB:**Â $299 USD  >**9060 non-XT:**Â $259 USD  >**5060:**Â $299 USD  >**5050:**Â $249 USD  Why include that in the title then too?,hardware,2026-01-13 20:45:49,0
Intel,nzg52uc,"Obligatory ""lol stupid pcmr amirite"" comment.",hardware,2026-01-13 23:56:47,1
Intel,nzdplnj,"The RTX 50 and 40 series are using the TSMC 4N node which is a custom version of the N5 node for NVIDIA. But anyway the N5, N5P, N4, N4P, N4X are all 5 nm class node, so have around the same price for the wafer. And I wouldn't be suprised that NVIDIA is paying less for these considering the volume compared to Intel orders.",hardware,2026-01-13 16:53:27,16
Intel,nzdpf4g,Well their gpus are much more expensive than amd & nvidia who arent even trying. When they try Intel wouldnt even have a chance  Nvidia increasing their entry gpu volume  [https://videocardz.com/newz/nvidia-reportedly-shifts-rtx-50-supply-toward-rtx-5060-and-5060-ti-8gb-in-2026](https://videocardz.com/newz/nvidia-reportedly-shifts-rtx-50-supply-toward-rtx-5060-and-5060-ti-8gb-in-2026),hardware,2026-01-13 16:52:06,2
Intel,nzi4dj7,Take a positive factory tour video. How can we spin it to make outrage?,hardware,2026-01-14 07:42:00,3
Intel,nzdqnhb,Yep,hardware,2026-01-13 17:07:48,5
Intel,o0a80gr,Baby steps.,hardware,2026-01-18 12:59:19,1
Intel,nzky9ti,Recently he's been making far more rants and less informative videos.,hardware,2026-01-14 18:16:33,-1
Intel,nzn1tla,those number grades are an ISO standard (14644) so they are indeed common.,hardware,2026-01-15 00:12:34,3
Intel,nzdueea,Thanks Steve,hardware,2026-01-13 17:26:35,32
Intel,nzdyjcv,But you still didn't to say hi to me at PAX West 2016 in front of the LEGO USS Missouri battleship...,hardware,2026-01-13 17:45:51,1
Intel,nzdtnpi,"makes sense, I coulda been more charitable in the way I said it",hardware,2026-01-13 17:23:03,1
Intel,nze79m5,"relax dude, steve already replied, no need to whiteknight",hardware,2026-01-13 18:24:32,-14
Intel,nzdtsrk,"he replied in a comment to me to say that he was told to leave it alone, I guess assembly isn't as careful as the initial production is.",hardware,2026-01-13 17:23:45,5
Intel,nzdjfsy,"Given the impeccable, spotless, damn near saint-like moral & ethical code of Steve & GN, and their recent consumer advocacy and stepping on some very powerful toes, your comment sounds an ***awful*** lot like an astroturfing smear campaign meant to breed sentiments against Steve & GN.",hardware,2026-01-13 16:24:37,9
Intel,nze6q5q,Imagine defending the hair of a guy who looks like he judges anime conventions in his spare time.,hardware,2026-01-13 18:22:13,-12
Intel,nzdq8iu,"I get it, for those with old boxes. But intel has great transcoding according to self hosters, and the powr consumption is much better vs old i5 pairing with those dedicated cards.",hardware,2026-01-13 17:05:17,9
Intel,nzej077,day traders love having a zillion stock tickers running.  i'm sure there's more applications where a heap of monitors is useful.,hardware,2026-01-13 19:16:47,9
Intel,nzeua57,A lot of digital displays make use of DP MST to avoid the use of home run cabling.,hardware,2026-01-13 20:08:25,3
Intel,nzelrbi,The a310 and a380 are fantastical for a media server!,hardware,2026-01-13 19:29:14,12
Intel,nzdi8nv,Your wasting a lot of words defending a company about to rerelease a 4 year old GPU (3060) because they canâ€™t get memory for the current model.,hardware,2026-01-13 16:19:11,14
Intel,nzsvw7m,The A310 was released in 2022. Do your think Sparkle would still be producing them in 2025 if they didn't sell?,hardware,2026-01-15 21:18:53,1
Intel,nzdjgo9,"Sure it was paraphrased for the title, but that's just semantics.   AMD and Nvidia ""giving up"" vs. ""abandoning"" the segment mean the same thing either way, given Lucas' intention behind the statement.",hardware,2026-01-13 16:24:44,4
Intel,nzfgwnz,He is ignoring the post because the poster didn't watch the video and is spreading BS. The segment they are talking about is $100 cards.,hardware,2026-01-13 21:53:45,16
Intel,nzdqmql,"maybe the cost for the raw wafer, but that's not all TSMC will charge nvidia for. You also need to account for yields, which could be different depending on the type of node.",hardware,2026-01-13 17:07:41,0
Intel,nzlh47j,"Lately there's been a lot of bad shit happening in the industry and a lot less good tech to talk about. Shocking, right?",hardware,2026-01-14 19:40:42,6
Intel,nzp1ajx,Thanks you ðŸ˜Š,hardware,2026-01-15 08:17:06,1
Intel,nze7n1w,But then I won't earn my free toaster after the 11th white knight attempt.,hardware,2026-01-13 18:26:08,-2
Intel,nzdxfeg,"> Given the impeccable, spotless, damn near saint-like moral & ethical code of Steve & GN  You have to be joking, right?",hardware,2026-01-13 17:40:46,8
Intel,nzdlwqs,Yeah. Iâ€™m definitely an astroturfing bot account. You got me. My profile certainly *reeks* of botting / astroturfing ðŸ˜‚,hardware,2026-01-13 16:35:48,-14
Intel,nzi46nl,This has to be sarcasm.,hardware,2026-01-14 07:40:11,-2
Intel,nze72qg,"Imagine being as shallow are you are while still posting on reddit behind an anonymous username.  Let's see how your hair looks, mate. You're giving off pure incel vibes here.",hardware,2026-01-13 18:23:43,13
Intel,nzdii5s,"You know I also talked about AMD right? Not just NVIDIA. Regardless, you think Intel isn't also going to have memory issues soon? They might just divert all memory they have to the SKUs that are selling.",hardware,2026-01-13 16:20:22,6
Intel,nzedwr3,>  a literal quote from Lucas  Does not line up with  > paraphrased,hardware,2026-01-13 18:53:49,14
Intel,nzdz5pq,Yields wouldn't meaningfully differ within the same family. Certainly not by enough to remotely cover for the die size difference.,hardware,2026-01-13 17:48:42,6
Intel,nzfzkw5,I'm starting to think that r/hardware is the circlejerk sub and I just haven't yet found the actual hardware sub that it's parodying,hardware,2026-01-13 23:27:14,1
Intel,nzegtti,Intel B570/580 already use GDDR6 which is what Nvidia is trying to achieve with the 3060 release. Intel presumably won't be effected.,hardware,2026-01-13 19:06:58,2
Intel,nzel47j,"...Yes, that's why I said it's a semantics issue.  The meaning is the same: The paraphrased quote isn't a statement made by GN like u/KARMAAACS implies.   What makes it worse is that Lucas said that in response to Steve's question about ~$100 A310 cards and why they're still producing them (adding that they see a healthy demand for them from their customers).   It's like he didn't watch the video and just reacted to the title.",hardware,2026-01-13 19:26:20,-10
Intel,nyracr1,"Solid product, nice foundation. Improve ST and intel will comfortably keep their mobile market",hardware,2026-01-10 08:56:00,114
Intel,nyr9ktg,"I think the ideal would be to get to a point where the flagship ""mainstream"" iGPUs (-H series, for Intel) compete with Nvidia's contemporary x50 GPUs, and then have big iGPU chips (Strix Halo, NVL-AX?) to compete with x60+ level.",hardware,2026-01-10 08:48:47,42
Intel,nyrhzxa,"Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.Â    Intel couldn't care less about that, they just need to be better than 890M and the game is done.",hardware,2026-01-10 10:07:58,95
Intel,nysrktd,Iâ€™d love to see its support outside of the approved games demo list. Intel has great hardware but their drivers and game support have always been the biggest question.  Whatâ€™s the point of hardware if you canâ€™t apply it to what you need.,hardware,2026-01-10 15:26:18,14
Intel,nyrvona,"If these chips end up cheap enough that they can replace the standard Intel CPU + 50/60 tier mobile Nvidia dGPU it will be very interesting.  I'm not sure they will be able to in the short term, Nvidia pricing on low end mobile dGPUs is very aggressive ($600 5050 laptops are the proof) but hopefully it isn't long before this type of powerful iGPU becomes a common thing.",hardware,2026-01-10 12:09:03,19
Intel,nyrjnuw,"This is against a 50W TBP RTX 4050 Laptop (which should be more at ease around 90-100W)  Not saying it's bad, but you can't compare Laptop performances without including TDP configuration and behavior.",hardware,2026-01-10 10:23:13,24
Intel,nyrfm1b,"""taking on strix halo"" -> result 50% of strix halo performance, ok.",hardware,2026-01-10 09:45:51,29
Intel,nyuib68,"TWELVE efficiency cores?.... that's nuts.   Anywho, these results look good. Assuming CPU and battery life are comparable or better than Strix Point/Gorgon Point, Intel might have a nice little advantage.",hardware,2026-01-10 20:27:23,8
Intel,nys66a2,Wtf is this article? Strix halo is another class product. Takes on strix halo being more than 50% slower?,hardware,2026-01-10 13:24:55,12
Intel,nyszn5m,"I'm sorry but nothing was more embarrassing than that guy from AMD the other day saying it doesn't matter because Strix Halo, a chip in so few devices that's an absolute behemoth, is still faster. Panther Lake is an absolute achievement for Intel. With the right drivers, they're going to have the perfect chip to forgo low end dGPUs.",hardware,2026-01-10 16:06:23,13
Intel,nyrf0ck,I will need at die fast ultrabook with 12hrs+ battery  Its Not a gaming product,hardware,2026-01-10 09:40:16,6
Intel,nyrhl2f,Website doesnâ€™t load with adblocker,hardware,2026-01-10 10:04:14,4
Intel,nyrng08,"I'm hoping for a thin and light 16 inch laptop with Panther lake and a B390, as it'll be perfect for photo editing, as Adobe seems to prefer Intel over AMD graphics and a discrete GPU is overkill.",hardware,2026-01-10 10:57:35,4
Intel,nyr7n57,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-10 08:30:43,1
Intel,nyvbh6q,I am more interested in next gen desktop APUs,hardware,2026-01-10 22:53:09,1
Intel,nz5usc0,"Compared to my 890M based laptop, the 890M numbers here are about 15-20% lower than what I'm seeing at the same settings.  This is likely due to power targets?  Even if the uplift is just 60% instead of 80%, that's still an impressive achievement for the B390M.  It's a shame that AMD appears to have dropped the iGPU ball in 2026.  Relying on the Strix Halo is not an option here.  It's pretty much impossible to find a good laptops that use it.  The upcoming HX470, and still without FSR4, isn't going to close the apparent large gap.  It seems that AMD forgot to stay hungry and they'll end up losing whatever ground that they'd gained in the last few years.  Mind you, Intel is known to play pressure games with laptop makers too in order to limit AMD adoption, which just makes it even crazier that AMD isn't doing what's required to keep the pressure on.",hardware,2026-01-12 13:57:45,1
Intel,nz6400s,"I think the B390 could be faster than even an RTX 5050 35W (As it could beat an RTX 4050 at 60W).       These thin and light laptops that Panther Lake is built for use way underpowered GPUs. Honestly, it makes sense why the Dell XPS 14 only has the B390 graphics. Before it used an RTX 4050, but it ran at just 30W of power.       Now that Integrated Graphics have beat the -50 Tier of GPUs I don't think we'll even see an RTX 6050 or RX 9050",hardware,2026-01-12 14:47:08,1
Intel,nzdzgjh,"They've basically maxed out the 128-bit normal socket iGPU now.  For them to beat it they need more memory bandwidth - they can put in a bit more cache, but realistically they'll need a quad channel bus (or maybe they can wait for LPDDR6 at 14.4+).  They can probably have a bit more physical room in the next generation of sockets, but without more bandwidth it isn't *that* useful.",hardware,2026-01-13 17:50:05,1
Intel,nyveqz5,"the 140v also got a 25% speed boost post launch, if something similar happens than this could be as good as a 5060 mobile... which is wild! I hope it dosen't cost as much as halo strix!",hardware,2026-01-10 23:10:01,1
Intel,nys0jf3,Needs a conroe vs fx62 moment. It doesn't look promising.,hardware,2026-01-10 12:46:32,0
Intel,nz7yfdg,Why aren't they comparing the AMD 8060S in the current Strix Halo flagship to the Intel B390? Probably because it doesn't go intel's way... interesting.,hardware,2026-01-12 19:54:01,0
Intel,nystund,And yet maybe 5% of customers will buy this version because its absolutely irrelevant for them whether their laptop would have an Iris iGPU from 2014 or a 2500watt RTX 5090.,hardware,2026-01-10 15:37:56,-6
Intel,nyrt6fi,"Thats great. If you are nvidia making dedicated gpu, then better make something that is not shit. 4050 is a joke",hardware,2026-01-10 11:48:14,-11
Intel,nyrumb5,"But how much does it cost? It mentions it having 16 cores so I'm guessing it's going to be overpriced if you don't need CPU performance, just like Strix Halo.",hardware,2026-01-10 12:00:16,35
Intel,nyrnltb,They need a 25% IPC increase to get back to the leading edge in CPU and honestly i don't see it with their current architecture. They need a new radical design   Edit: getting downvoted for what?. Currently Apple and QC have a very solid lead. Even ARM beats Intel and AMD in general CPU workloads and Intel/AMD have been very slow to update their uarch focusing on clock speed over efficiency and IPC,hardware,2026-01-10 10:59:01,1
Intel,nyyecmo,"AMD Ryzen AI Max+ 388 just dropped cheaper than the 395 with the same GPU, it will be cheaper than the panther lake.",hardware,2026-01-11 11:18:06,0
Intel,nyrbnk5,"Depends on Intel's & amd power targets. I dont think its rly feesible for them to target cpu + gpu power usage, 100W combined at least?",hardware,2026-01-10 09:08:17,15
Intel,nyxp50b,then we wouldnt have the 50 gpus anymore. The XX30 and XX40 GPUs died because of iGPUs competing with them.,hardware,2026-01-11 07:25:59,1
Intel,nyt45q3,> Too expensive for any meaningful customer to adopt and have real mainstream products.   So basically every decent APU ever made. Too expensive to the point it bumps into dGPU territory and not powerful enough to be a direct replacement.,hardware,2026-01-10 16:27:51,34
Intel,nyrnxmm,> Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.Â  >  >   Story of AMD APUs.,hardware,2026-01-10 11:01:59,41
Intel,nys70pu,"AMD aimed Strix Halo at AI users first and foremost, thinking those folks would pay the high premiums.   But of course anybody serious about AI would have an Nvidia GPU, and so many other AI users are still just using cloud-based services anyways.",hardware,2026-01-10 13:30:14,20
Intel,nytpb98,"AMD has always had lower supply compared to Intel and yet AMD client continues to grow. Strix Point at launch had little products (Asus being the only OEM per usual) and yet they still continue to grow, at a smaller scale relative to Intel. Strix Halo is still continuing to have designs made, it wouldn't be a 'failure' if we are still getting Strix Halo products at CES...  I wrote a [comment in a previous post](https://www.reddit.com/r/hardware/comments/1q7d67m/comment/nyhh23c/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) on the reality of the state of Intel and AMD in the mobile segment. Intel is really dependent on CCG, it is double in revenue to DCAI. They invest in what makes them money. Compared to AMD, client and DCAI is doing well, for client CPUs and GPUs are doing well, putting no pressure in mobile, in fact their strategy has remained the same in the past couple the years and even if marginally their % share is sufficient for them. Intel pushes a lot of supply for mobile, while AMD is smaller, it is all relative in the necessary investments they need to make in order to supply demand. Do I wish AMD stop stagnating in designs, yeah, RDNA3 needs to go, but these companies have motives in what they do.",hardware,2026-01-10 18:08:04,10
Intel,nysq66f,"We all saw it coming a mile away, when it came out in 2025 it was competing against discounted 4060 laptops as low as just $1000. Too little, too late, too expensive, dated on arrival with RDNA3.5, etc. But for some reason this sub and r / amd always have such a hard on at the concept of a ""big APU"" that in practice would never be economically sensible.",hardware,2026-01-10 15:18:57,16
Intel,nyrnx0m,Arc 140T is already on par with the 890m for most tasks excluding games.,hardware,2026-01-10 11:01:50,14
Intel,nyrimhl,Commercial failure indeed. Laptop with dGPUs at same price perform better. Laptops with solid CPU perf are much cheaper.,hardware,2026-01-10 10:13:42,6
Intel,nz23thg,AMD gave Strix Halo zero chance to compete by barely selling any of the lower end models. An 8 core with 32 CUs would be a great mini PC.,hardware,2026-01-11 22:50:43,1
Intel,nyt5d7h,"I dunno Battlemage was a big step forward on driver compatibility and every month Intel improves all their Arc compatibility. I'd be honestly surprised if Xe3 was worse than their current offerings. I'm sure it still has shortcomings as all Intel GPUs will because they're simply starting fresh, but even my A750 is pretty good right now at playing anything I throw at it.  The only aspect Intel kind of messed up that annoys me is their video encoder, once it gets pegged to 100%, it absolutely tanks your performance on the capture to the point where it skips frames and lags. It never used to do that and the driver also used to include capture software, now they just offload it to people having to download OBS and removed the capture aspect of the driver. Kind of dumb when both NVIDIA and AMD include it as a driver option.",hardware,2026-01-10 16:33:28,14
Intel,nz0bqe4,Driver support is better than ever and will continue to get better now that Intel has found its footing in the gaming GPU business (yes that includes igpus),hardware,2026-01-11 17:57:04,3
Intel,nysl2hn,"If TSMC does raise price on their node, Nvidia doesn't find another node for their lower-end bins and Intel can keep the price on their own node down low, we could see Nvidia simply slowly phasing out the -50 series like they used to do with the MX series.",hardware,2026-01-10 14:51:29,13
Intel,nyrlr4x,"The 4050 still has a sizeable memory bandwidth advantage, so it's still very surprising that the B390 comes so close.",hardware,2026-01-10 10:42:21,16
Intel,nyvfe2o,"oh damn, this should be a lot higher up! Most laptops have them clocked much higher so expecting 4050mobile performance is kinof a lie...",hardware,2026-01-10 23:13:25,3
Intel,nyribi5,"I get the feeling everybody is still unsure where these PTL chips slot in to and what to compare these against actually. Once we get more info on pricing, power consumption, CPU performance etc. we will get some actually useful comparisons.",hardware,2026-01-10 10:10:57,22
Intel,nys7fi0,I actually think they meant to say Strix Point in the headline there.,hardware,2026-01-10 13:32:44,11
Intel,nyrhzye,"[HP ZBook Ultra G1a 14](https://www.notebookcheck.net/HP-ZBook-Ultra-G1a-14-review-Powerful-MacBook-Pro-alternative-for-work-and-game.994758.0.html) would've been a better test  Load average: 83.3W   Cyberpunk 2077 ultra \* 110.9W = 80.7fps  Baldur's gate 3: 99.4fps   B390 wattage?   If pantherlake is designed for battery, is it better if it loses performance?",hardware,2026-01-10 10:07:59,9
Intel,nyxpaeu,So how many sub 1000 dollar laptops we have with Strix Halo?,hardware,2026-01-11 07:27:18,3
Intel,nzd2sww,It's definitely going to come down to pricing and availability,hardware,2026-01-13 15:06:48,1
Intel,nysgi6l,The new MSI Prestige 16 looks nice.  They all seem to lack Thunderbolt 5 though.,hardware,2026-01-10 14:25:49,8
Intel,nyt7vgu,"It seems the revived Dell XPS 16 will have the â€œB390â€ and no dGPU, as another option. LTTâ€™s video on it said Dell is quoting 27 hours of battery life in â€œgeneral tasksâ€ and 40 hours of video playback. Obviously remains to be seen how real those manufacturers claims are, but hereâ€™s hoping.",hardware,2026-01-10 16:45:16,4
Intel,nyrvj09,"$1100 for a little MSI 13"" laptop with one. there are also quite a few CPU SKUs that have the B390.",hardware,2026-01-10 12:07:46,41
Intel,nyt4dcl,"Those 16 cores are 4 performance cores, 8 efficiency cores and 4 â€œLow Powerâ€ efficiency cores. This is only doubling the core count of Lunar Lake, by adding the two plain efficiency core clusters. Or keeping the same core count as ArrowLake mobileâ€™s 285H (not HX!), trading 2 performance cores for 2 â€œLow Powerâ€ efficiency cores.  Iâ€™m not 100% on this but I donâ€™t think Stryx Halo used AMDâ€™s C cores, so it basically had an entire 9950x attached to the iGPU.  Prices should be more normal, as this is more part of Intelâ€™s normal lineup.",hardware,2026-01-10 16:28:50,17
Intel,nyych3q,There's an Ultra 5 chip with the B370 (10 Xe cores instead of the 12). Shouldn't be too costly,hardware,2026-01-11 11:00:58,2
Intel,nyvbkeg,It's 16 cores but it's only comparable to Strix Point 12 cores and not Strix Halo.      The highest end Intel chip here only matches the number of P cores in the M5,hardware,2026-01-10 22:53:37,2
Intel,nyrpy0o,"I really don't think that is so important for mobile devices though.  All Intel needs to do is be ""good enough"" and the OEMs will use them in flagship models.",hardware,2026-01-10 11:20:06,41
Intel,nyrzdl1,I doubt anybody is going unseat Apple from the ST throne in the near future.,hardware,2026-01-10 12:37:56,12
Intel,nyrolyj,Well unified core is supposed to be happening in the next couple of gens. Frequencies also seem to have taken a hit on 18A but I'd expect that to improve with time as usual,hardware,2026-01-10 11:08:05,11
Intel,nytxbwv,Not really. They just need to not completely bungle gaming and latency sensitive performance like with Arrow.,hardware,2026-01-10 18:45:01,2
Intel,nyylyv1,"This is about mobile devices, and since a high performing IGPU is included, the question is no longer how well the CPU performs in a system with a 5090 (what most cpu benchmarks focus on) but how well this IGPU/CPU combination performs compared to other IGPU/CPU combinations. I am positive the CPU is not the limiting factor in this IGPU performance tier, so ""leading edge CPU performance"" is not really relevant.",hardware,2026-01-11 12:23:19,1
Intel,nyrc6je,"Don't think it's completely absurd. Should get some efficiencies from less interconnect overhead and lower power memory, so not quite 1:1 with a dGPU. If we were to budget, say, 40W for the iGPU in gaming and 20W for the rest, should be perfectly in line with the higher end laptop SKUs.",hardware,2026-01-10 09:13:21,17
Intel,nyrnpsr,Intel Arrow Lake already uses 80W just on the CPU side in multicore,hardware,2026-01-10 11:00:02,-1
Intel,nytxoe2,"It would be viable if AMD released their own small PCs with it to cut the MSRP of products, but they aren't interested.",hardware,2026-01-10 18:46:36,7
Intel,nyummcb,And yet AMD managed it for the PS5... it's clearly possible.  Of course we don't know the cost breakdown there as far as PS5 pricing goes.,hardware,2026-01-10 20:49:14,4
Intel,nyrpk3i,They just need to make the next iteration cost less. Most of strix halo's issues were the sky high price.,hardware,2026-01-10 11:16:37,6
Intel,nyspmud,"""Local LLM"" is such an incredibly niche thing I can't believe the tech nerd internet is so obsessed over it. Any real life business use case of AI is cloud based no question asked.",hardware,2026-01-10 15:16:05,19
Intel,nywuwh7,well said,hardware,2026-01-11 03:51:39,0
Intel,nytll12,Too much listening to MLiD who has a boner for APUs,hardware,2026-01-10 17:50:40,11
Intel,nytno6k,"Idk one can easily flip your statement. Panther lake coming in **2026** competing against continuing discounted 4050s prob less than 4060s. I don't dislike Panther Lake nor am I defending Strix Halo, but I wouldn't say your argument is a rather good one.",hardware,2026-01-10 18:00:26,-3
Intel,nyrtebm,https://m.youtube.com/watch?v=ymoiWv9BF7Q   It's already at least on par for reasonable power profiles unless you play stuck to the wall.,hardware,2026-01-10 11:50:08,5
Intel,nytp8ri,"Huge step forward, I just wish the didnâ€™t struggle with older and brand new games. Itâ€™s a great card if you are willing to do troubleshooting and know computers but I wonâ€™t recommend them to family yet.",hardware,2026-01-10 18:07:45,6
Intel,nz0sex5,I hope they start supporting dx11 stuff. Thatâ€™s a ton of games.,hardware,2026-01-11 19:10:35,1
Intel,nytykfy,Isn't TSMC planning to increase pricing on n2 by 20-30%,hardware,2026-01-10 18:50:42,6
Intel,nz0bimn,Nvidia will find another cheap node to use. Samsung will gladly oblige,hardware,2026-01-11 17:56:04,1
Intel,nyrsbne,"153 GB/s vs 192 GB/s is not that ""sizeable""  And the comparison against ""HP OmniStudio X 32-c0077ng"" is weird, even in the linked test they have GPU-Z screenshot displaying 1375Mhz memory speed instead of 2000 Mhz on most other RTX 4050 Laptop Review.  I don't understand this comparison against an All-in-One, and I'll wait for more in depth reviews to draw some conclusion.",hardware,2026-01-10 11:40:59,12
Intel,nyrm798,"PC World had power consumption tests under gaming loads. It pulled 60W through USBC with Cyberpunk, so probably 35-40W for the gpu. When they unplugged it, the benchmark numbers stayed the same. So it also pulls 60W on battery.  Unless the manufacturer actually configured the device to simultaneously pull energy from the cord and battery under full load.",hardware,2026-01-10 10:46:26,12
Intel,nyxt6pz,About as much as we have PTL laptops,hardware,2026-01-11 08:02:38,2
Intel,nysiaor,"TB5 isn't a big deal, although I don't like that they have a numpad keyboard, and usually MSI speakers are terrible.",hardware,2026-01-10 14:35:56,2
Intel,nyrxjeo,But can't you get a laptop with a 5060-5070 at that price?,hardware,2026-01-10 12:23:52,17
Intel,nyvevcg,"Damn, that's really good! it's pretty much macbook air pricing.",hardware,2026-01-10 23:10:39,1
Intel,nyvvmm4,"> Iâ€™m not 100% on this but I donâ€™t think Stryx Halo used AMDâ€™s C cores, so it basically had an entire 9950x attached to the iGPU.  I have a Strix Halo.  What you wrote is exactly what it is.  It's essentially a 9950x (so all P-cores) with a fat iGPU attached, and with a 4 channel memory controller instead of 2-channel.",hardware,2026-01-11 00:39:01,14
Intel,nyxcvyp,"The biggest difference between the P and E cores is fMax. The larger the core count becomes, the lower the all core clocks become, the smaller the gap between P and E core performance becomes.   The IPC difference between the two is like ~10%  At a certain point along the wattage curve, given a certain number of cores, there will be a point where E core performance can potentially meet or exceed what you would've gotten has you had too many P cores.    Its also more than just trading 2 P cores for 2 lpE cores. The lpE cores in ARL-H were *so* weak, they were functionally useless. In practice, it'll be more like trading 2 P cores for 4 lpE cores  edit: to be more specific, In ARL-H, below 5W per core, E cores outperform P cores. If you have 16 cores and are running all core workloads, then at 60W, each core is receiving less than 4W.",hardware,2026-01-11 05:45:32,5
Intel,nz7k3ly,No it's firmly ahead of strix its right in between. Strix point uses 8 ecores too and it gets demolished in multithread benchmarks as expected,hardware,2026-01-12 18:48:32,1
Intel,nyvb2cd,"It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.     AMD made them lower margins for laptop chips because they weren't very competitive. If they want fat margins, they need to be the best",hardware,2026-01-10 22:51:00,3
Intel,nyuqay0,Single Core is very important when Intel is doing these designs that lack P cores throughout. The cheapest X2 Elite has the same amount of P cores as the most expensive Panther Lake SKU,hardware,2026-01-10 21:07:30,2
Intel,nysbrxo,"Qualcomm is already super close with Oryon V3...  Perf/Watt for that single thread isn't close I guess, but absolute performance is breathing down Apple's neck for sure.  Also, don't compare Geekbench scores on windows vs Linux/Apple/Android... Windows just does something negatively about it and the difference is 5-7% vs non-windows.",hardware,2026-01-10 13:58:36,9
Intel,nyvsos5,>Well unified core is supposed to be happening in the next couple of gens.  I would be shocked if this has much to do with a large performance uplift. I imagine it would have to do more with rightsizing core area and power draw.,hardware,2026-01-11 00:24:01,2
Intel,nyuqf9s,Chasing above 5Ghz is stupid on laptops. It only matters for desktops,hardware,2026-01-10 21:08:07,4
Intel,nyu42wk,You are overly focusing on gaming. I mean general CPU performance,hardware,2026-01-10 19:16:57,1
Intel,nyrcv9u,Rtx 5050 is 61% faster than B390. I doubt if they change the wattage configuration and stick to 60W they'll match it. Unless the 5050 is capped to more reasonable wattages like 60-80W. Plus the 60W budget for Intel/amd will be used for other compotents and the apu budget reduces.,hardware,2026-01-10 09:19:52,7
Intel,nysfyyg,"I'm talking out of my knowledge base, but I think the switch from heat pipes to custom vapor chambers means we are less bottlenecked at power density / pulling heat from the chip and more constrained at what the radiator/fan system can push out of the system.",hardware,2026-01-10 14:22:47,2
Intel,nz3zzs3,"They announced a first party Strix Halo PC at CES, but it'll probably be really expensive.",hardware,2026-01-12 04:57:58,3
Intel,nyzybbo,"> And yet AMD managed it for the PS5... it's clearly possible.  Well for two reasons:  1. Sony bankrolls the R&D of the APU and it's underlying architectures which allows AMD to make it for basically cost and have a low BOM on it. They didn't pay as much as they normally would for the R&D, tapeout, testing etc.  2. It's a console APU, it literally has to be cost effective to make sense, otherwise it becomes like Strix Halo and SONY goes out of business. Also most consoles are sold on launch for a small loss with SONY and Microsoft recouping those lost funds off game sales, online subscriptions and store revenue. Then over time they tend to shrink console APUs on newer nodes which makes it more power efficient and less expensive to produce as a smaller chip on a newer node typically has better yields, it also allows SONY or Microsoft to put in lower quality components like less heatpipes in a new revision or Slim console, for similar thermal headroom and save on BOM cost.   I mean there's a reason why they do not offer the PS5 APU as an off the shelf product, only the cutdown bad yields go onto being some cryptocurrency mining board or some Linux APU and with the performance being cut its usually worse value than buying off the shelf dGPU parts like a 5060 or something.  I don't know why you're seriously arguing that APUs for Desktop and Laptop PCs are a viable product. For one, they've never been viable, not once. Even Strix Halo which is honestly the best APU I've ever seen has been ruined by its high cost. Don't get me wrong, I like the idea of an APU, an all in one chip that does it all. But unless you're like Intel and you're willing to do a tile based design and or basically have a true chiplet where you can link lots of smaller dGPU tiles together it doesn't really work. You're just better off buying dedicated CPU and GPU parts for better price to performance. If you don't believe me, I can buy an [RTX 5070 Laptop right now for $1900 AUD](https://www.centrecom.com.au/msi-katana-15-hx-14xwgk-156-qhd-i7-16gb-ram-512gb-rtx-5070-gaming-laptop-black) and that will easily outperform Strix Halo which has less performance and typically costs over $4000 AUD... [Even a lowly 4060 laptop fairs better.](https://youtu.be/RycbWuyQHLY)  The only thing APUs excel in is this, if you want something relatively cheap but capable. i.e it can run a game at 30 FPS with medium settings at a low resolution. i.e something like Panther Lake or Apple's M series chips. But if you want true performance, just go out and buy an RTX X060 series laptop it's far better price to perf each generation.",hardware,2026-01-11 16:53:47,7
Intel,nyvw6mt,"What is possible? PS5 uses GDDR6 instead of DRAM. And consoles are heavily subsidized by digital purchases. I bet AMD makes good money on PS5 (and Xbox X/S), Sony & Microsoft just subsidize the shit out of it with their 30% cut from selling games. Even the Steam Deck is barely profitable for Valve. High-end APU is just a waste of sillicon.",hardware,2026-01-11 00:41:51,10
Intel,nyrrj9f,Even their Ryzen 5 AI 340 laptop are too expensive and you can buy an older gen Ryzen 5 with Nvidia GPU laptop for same price or even lower with much better GPU performance.,hardware,2026-01-10 11:34:10,29
Intel,nyxoopw,Local AI (not just LLM) is universal on mobile and getting to be universal in corporate computers. You just dont see it. The background blurring in Teams meeting? 5x more battery efficient with AI. But its just going to be integrated into Teams and fire up if hardware supported without asking you.  >Any real life business use case of AI is cloud based no question asked.  All AI use cases at the place i work for is local due to confidentiality issues. We cannot and will never be able to use this on cloud. Unless the world completely flips its ideas about confidentiality i guess.,hardware,2026-01-11 07:22:00,6
Intel,nyu206m,"Panther Lake is a normal CPU, not some special ""big APU."" It doesn't make much sense to flip the argument the way you did.",hardware,2026-01-10 19:06:56,9
Intel,nysp4h4,"140T (Arrow Lake) isn't the same as 140v (Lunar Lake) though, the former is usually quite a bit weaker and inconsistent in games despite slaying all the synthetics.",hardware,2026-01-10 15:13:21,11
Intel,nysjpec,"It is probably because the AIO was one of their most, if not the most recent RTX 4050 tested (March 17th 2025) which probably enabled them to compare in newer title like F1 25 in the article, as it has already been around for like 3 years while RTX 5050 was released last year and received more attention in its place overall. From their database, the next most recent thing with RTX 4050 they reviewed was the Yoga Pro 7 in January 2025 with a 60W RTX 4050 (45 watts + 15 watts Dynamic Boost), which scored 50.8fps in Cyberpunk 2077 at the same setting and thus a bit lower than the AIO, so I would say the AIO is at an okay spot for a RTX 4050 to be compared to this Arc B390.",hardware,2026-01-10 14:43:54,8
Intel,nysoib9,"> 153 GB/s vs 192 GB/s is not that ""sizeable""  25%? What's sizeable?",hardware,2026-01-10 15:10:05,22
Intel,nyrwfu6,"It would be easier to compare mobile parts if laptop OEMs didn't lock down their BIOS and EC registers, blocking anyone from actually tinkering with the (godawful) default configs for TDP, boost behaviours and fan curves on most common laptops  You can buy the bestest Intel Core Ultra 9 285h but if some engineer at HP thinks that 45Â°C idle is too warm it will either throttle to the point that you wish you were using the Nintendo DS browser or crank the fans to Mach 3...",hardware,2026-01-10 12:15:09,6
Intel,nyxwid7,had no idea Strix Halo is this popular.,hardware,2026-01-11 08:33:14,2
Intel,nysx853,"the new Prestige 16 actually [doesnt use a numpad](https://www.notebookcheck.net/MSI-debuts-Prestige-16-AI-and-Prestige-16-Flip-AI-with-Panther-Lake-H-Core-Ultra-X9-388H-and-Arc-B390-graphics.1197009.0.html)!  and the flip version is especially intresting, they managed to tuck the stylus *under* the laptop with a slot that can also charge said stylus",hardware,2026-01-10 15:54:42,7
Intel,nys9rsz,"Laptops with 5060 at sub- $1000 weren't launch event laptops at CES. They came later as fairly cost optimized, ""compromised"" laptops that cheaped out on most of the total laptop in order to fit that CPU/GPU in its budget.   PTL-X is PTL-H with a ~60mm GPU tile. A 4050 is a binned ~160mm chip. Edit: that *also* requires its own VRAM and cooling  Intel is also on record saying 18A cost structure is flat vs Intel 7. I imagine costs between PTL-X and RPL-H + dGPU are much more competitive than you think, with the only caveat being discounts on old excess inventory and not having to redesign a new laptop (although I imagine the RAM pricing increases makes the total price different between the two shrink even more)",hardware,2026-01-10 13:46:47,36
Intel,nyrxu16,"5060 yes, but it's less power efficient",hardware,2026-01-10 12:26:08,22
Intel,nywhxd0,"5060 -5070 cannot be fitted into ultrabook or thin & light models. those item are power hungry and high temperature, need to fit it in bulky laptops which are bigger heatsink , more room space.",hardware,2026-01-11 02:38:34,4
Intel,nyt5pt1,"technically, but it will be a shitbox in basically every other aspect (and stuck with 16/512)",hardware,2026-01-10 16:35:08,12
Intel,nyrycdm,i've been looking rn but have only seen those at $1400+,hardware,2026-01-10 12:30:01,-2
Intel,nyvtubh,"> It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.  Last year's leadership QC laptops had to be heavily discounted shortly after reaching market. Clearly there's more to it than IPC.",hardware,2026-01-11 00:30:06,4
Intel,nyxgar9,"The X2 Elite *may* be an amazing CPU. But customers don't buy mobile CPUs. They buy full, complete laptops, and that includes all of WoA's issues. Customers have so far, by and large, mainly rejected WoA. The biggest demographic of people who research and care about strong CPU performance are people who'd also want to play games, and QC has yet to demonstrate that that's viable.",hardware,2026-01-11 06:11:34,10
Intel,nyxo27r,But this product has 4 P cores?,hardware,2026-01-11 07:16:32,3
Intel,nysorh4,Do we have 285H/HX370 scores on Linux for comparison?,hardware,2026-01-10 15:11:26,4
Intel,nyu3bo0,I was going to wait for this - but driver support comments basically said wait for it to mature.,hardware,2026-01-10 19:13:18,2
Intel,nyw054m,"it is presumably lead by the e core team that's doing a lot better so we'll see, but at the very least saving area from debloating p cores would allow a bit more cache that the cores would love.",hardware,2026-01-11 01:02:05,3
Intel,nyurnsr,Chasing 5GHz is only stupid if it costs more power than it'd save. The lower end panther lake SKUs clock their cores a lot lower compared to LNL so it's likely just a node thing,hardware,2026-01-10 21:14:19,2
Intel,nz5bvpr,Apple and Qualcomm are both doing that right now though. It's cheaper than blowing up the area of the core to increase IPC.,hardware,2026-01-12 11:55:36,1
Intel,nyvv21f,But for non -gaming tasks arrow beats zen5,hardware,2026-01-11 00:36:07,7
Intel,nyxogf6,Gaming is the only segment where your previuos comment made sense though.  Everything else Intel is still leading edge.,hardware,2026-01-11 07:19:59,1
Intel,nyrdymf,"Yeah, I'm not talking about PTL. Clearly it's too far off. But clearly there's a lot of room left for Intel (and current AMD APUs) to catch up. Also worth noting that that 5050 is given 100W, which is particularly high for that chip. Gap obviously closes when the TDP is more reasonable.",hardware,2026-01-10 09:30:19,11
Intel,nyvf643,"it may not be this generation, but at the rate iGPU performance growing; pretty soon xx50 chips is no longer relevant. \*its not like Nvidia can make fat profit anyway.   Fyi, Nvidia has abandon their low profit margin xx30 line up, or Geforce MX series in laptop.",hardware,2026-01-10 23:12:14,3
Intel,nz57ion,Nah. the price of Strix Halo is the cost of the PS5 itself. AMD has fat margins for laptops and desktops,hardware,2026-01-12 11:20:38,2
Intel,nys1wlu,Laptops with dgpu always has poor battery life. Even tinkering with the best power optimizations. These ryzens have nearly double the real world battery life from my experience.,hardware,2026-01-10 12:56:23,8
Intel,nywmg0a,"Depends on what you consider to be â€œhigh end mobile gamingâ€, the laptop 4060 is currently the 2nd most popular gpu on steam, and thatâ€™s the level strix halo targeted",hardware,2026-01-11 03:03:40,0
Intel,nyxpn7n,"What youâ€™re describing is just inference. Runs on a phone Soc. Minimal memory requirement. Like faceID on the original iPhone X over eight years ago. Strix halo provides no additional benefit over strix point or lunar lake. If there are business use cases that use outlook or Microsoft 365 or Teams, they are using cloud based copilot. Thatâ€™s the mainstream business use case at present.",hardware,2026-01-11 07:30:29,1
Intel,nyu45c4,"The statement is directly comparable. 'Big APU' Strix Halo can literally be fit into a [handheld ](https://gpdstore.net/product/gpd-win-5/)and a [surface type tablet](https://www.ultrabookreview.com/71207-amd-strix-halo-asus-rog-flow-z13/). Regardless of the effective yields due to it's size, it is coming out another year later when compared to a 40 series gen, and a tier lower than the 4060. If you want to game, like many have argued with Strix Halo when it launched, just get a discounted RTX 40 dGPU laptop... Panther Lake has a great iGPU, don't get me wrong, but the argument isn't good.  A better one would be \~10-25W Panther Lake would be competitive than Strix Point/Halo and so on, not 'Strix Halo isn't economically sensible' because it's still on the market, with CES designs still being announced.  Some people in the sub think that if they aren't the ones the product is directed to (which is pretty much gamers), then they believe 'well it must've been a failure'.",hardware,2026-01-10 19:17:17,-3
Intel,nyt80oc,"Oh I'm blind lol, my bad.",hardware,2026-01-10 16:45:57,8
Intel,nyt0j4v,Yeah I'm bewildered by this take that it's not sizeable.,hardware,2026-01-10 16:10:35,8
Intel,nz5etqc,Easily offset with a slightly bigger cache.,hardware,2026-01-12 12:17:50,2
Intel,nyw80nh,"Don't worry, the engineer at HP also made sure you can never exceed 35W continuous power draw by giving it an undersized vrm and no vrm cooling",hardware,2026-01-11 01:45:48,2
Intel,nyt058y,"Oooo neat, close to perfect for me.",hardware,2026-01-10 16:08:46,7
Intel,nyu7ff7,"> Intel is also on record saying 18A cost structure is flat vs Intel 7  The 12Xe GPU die is on N3E, not 18A. Though I still agree with the conclusion that PTL should still end up relatively affordable, and cheaper than an equivalent dGPU.",hardware,2026-01-10 19:33:11,12
Intel,nyxbhd7,"Power efficiency is a curve. There will exist points along that curve where the B390 is more efficient than the 5060.   Efficiency is more complicated than just ""perf/watt at specifically both chips maximum power draw""  edit: May have misunderstood your comment. Thought you were saying B390 was less efficient than a 5060",hardware,2026-01-11 05:35:14,1
Intel,o058umn,Asus G14 would like a word.,hardware,2026-01-17 18:11:44,1
Intel,nyt7ejr,>(and stuck with 16/512)  Those typically have open ram and ssd slots. It's the premium thin models that have them soldered on.,hardware,2026-01-10 16:43:03,9
Intel,nysokej,https://www.bestbuy.com/product/asus-tuf-gaming-a16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-32gb-ram-nvidia-geforce-rtx-5070-1tb-ssd-jaegar-gray/JJGGLH8Y2Z  [Proof that the deal at least exists at the time of this comment](https://imgur.com/a/XYQm2fn),hardware,2026-01-10 15:10:23,9
Intel,nywgma9,"QC last year had a bad product.Â  It was competitive vs AMD and Intel but Qc was selling those for 50% less than Intel or AMD chips. OEMs at first decided to price these at Intel prices then it settled at Intel -100/200â‚¬   QC laptops still sold what QC and partners expected and OEMs are increasing new models for X2 (design wins went from 60 to 100+)   X2 has a 25% advantage vs Panther Lake and it will still be cheaper because QC is an underdog. If QC captures market share and reaches 10-15%, then Intel will start to sweat and then margins will be hit. I don't think QC gets anywhere near that till like 2028/2029. The laptop market is VERY slow to move. AMD had a better product for several generations and it only netted them +10%   While QC and Mediatek/Nvidia don't hit a bigger marketshare number. Intel and AMD won't need to lower prices",hardware,2026-01-11 02:31:29,1
Intel,nyt5fdi,"[285H GB6 Windows \~2900](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+windows)   [285H GB6 Linux \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+linux)  [HX370 GB6 Windows \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+windows)   [HX370 GB6 Linux \~3000](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+linux)  I'm just eyeballing results on geekbench browser 1st page but surprised by the HX370 results. Intel is all over the place but that may make sense since Intel is actually found on tons of laptops compared to AMD shiny hunting experience.  There was one source i had found testing X Elite on Windows and WSL2 on the same machine that showed Geekbench performing higher on WSL2 than native on windows but it may have been a yt video. Perhaps i'm mistaken.  AFAIK , the Windows Tax is real for Geekbench. this particular search on HX370 showing otherwise is a fluke imo. You can search other chips too, like 285K 3350win vs 3500linux  Unfortunately, i cannot bother looking for more controlled setups that had the same exact setup with both Linux vs Windows compared to definitely prove this, but i have seen those in the past here and there.  EDIT:   I found a source that compares GB6 Windows vs Linux relatively recent   [AM5 W11 vs Linux Performance Comparison in GB3,5,6 - Ryzen AM5 - HWBOT Community Forums](https://community.hwbot.org/topic/236884-am5-w11-vs-linux-performance-comparison-in-gb356/)  The Windows tax is still real.  [Qualcomm Snapdragon X2 Elite Extreme X2E-96-100 Processor - Benchmarks and Specs - NotebookCheck.net Tech](https://www.notebookcheck.net/Qualcomm-Snapdragon-X2-Elite-Extreme-X2E-96-100-Processor-Benchmarks-and-Specs.1127282.0.html)  Idk what actual source notebookcheck used here, but if x2 Elite Extreme reaches 4080 in GB6 on windows then +4% for linux would be 4240... Whether that's comparable to Apple M5 or not i'm not gonna say more on the subject...       I'd say Qualcomm is gonna be within spitting distance to Apple... Sure SD2X Extreme is highest end unicorn SKU vs base M5, that's valid argument, but still... within 10% of Apple i consider within spitting distance.",hardware,2026-01-10 16:33:45,4
Intel,nyt6hqk,I hope somebody tests Panther Lake GB6 on both linux and windows.,hardware,2026-01-10 16:38:46,0
Intel,nyvsukh,">Chasing 5GHz is only stupid if it costs more power than it'd save.Â   Chasing any GHz much above Vmin would cost more power than the performance it would bring, no?",hardware,2026-01-11 00:24:52,2
Intel,nyytnv8,"They are not. Like I said they are 25% behind Apple and Qualcomm in ST. Multicore the X2E can go up to 2x the performance of Arrow Lake and Panther Lake is just a refresh on 18A   Now there's more competitors.Â  They have 5 total. AMD, Nvidia, ARM, Qualcomm,Â  Apple",hardware,2026-01-11 13:19:34,1
Intel,nyrg65a,3nm will give 6050 another 20%. Whatever changes amd/intel do at power limit needs to be impressive. Otherwise I still see cpu + gpu combo yielding better perf.   Not perf/W or maybe perf/$,hardware,2026-01-10 09:51:05,12
Intel,nyrz0br,LPDDR6 coming hot with 50% bandwidth improvement...,hardware,2026-01-10 12:35:09,5
Intel,nyxwgwn,"Obviuosly. all AI *usage* is inference. Inference requires plenty of memory btw, it all depends on the model you want to run.  No, that is not the business case use.",hardware,2026-01-11 08:32:52,6
Intel,nyyeomk,"The Strix Halo was intended for local ai, the OpenAI OSS 120B fp4 model (or a 240B fp4 50% pruned like MiniMax 2.1) is run at 50 t/sec on a Strix Halo, or about 5 000 000 tokens day - 75$ if using Sonnet 4.5.  So in 20 days you get the money back (a 96GB RAM Strix Halo during 2024 has been sold for1480$ by a lot of OEMs and the 128 GB RAM - for 1600$-1700$), not to say you keep home your AI work",hardware,2026-01-11 11:21:07,3
Intel,nyyfatm,"if only AMD released a 384 bit bus Strix Halo with support to LPDDR5X 10700 MT/s, that would double the bandwidth - from actual 260 GB/s to 520 GB/s, putting it in the M4 Max category, which Apple is selling at 4000$",hardware,2026-01-11 11:26:37,1
Intel,nyua4zp,"They're completely different product classes. One is priced for mainstream and the other is decidedly not. One is purpose-built to go up against discrete GPUs, the other is not. That's why flipping that statement just doesn't work.",hardware,2026-01-10 19:46:22,6
Intel,nz565l5,"Donâ€™t even get me started! My current HP laptop straight up doesnâ€™t support any type of fan control on Linuxâ€¦  So even if I throttle my CPU manually based on temps, the vrm WILL burn a hole in my desk during prolonged use  I even found the basic EC registers for fan speed, but there is some other magic register that keeps resetting them. And trying to find the magic register might involve frying the board if you hit a voltage-related EC",hardware,2026-01-12 11:08:51,1
Intel,nyt1ch3,"ikr, im also heavily considering the Prestige 16 Flip atm (even tho I am an unhappy owner of a 8 year old MSI Thin...)",hardware,2026-01-10 16:14:27,6
Intel,nz39yac,Not using LPDDR is part of what makes it a shitbox.,hardware,2026-01-12 02:28:11,3
Intel,nz7jgiw,Screen is still dogshit,hardware,2026-01-12 18:45:44,0
Intel,nyv9rhl,"It's not just Geekbench, Linux usually has higher performance",hardware,2026-01-10 22:44:30,2
Intel,nyvzib4,"depends on the workload and the efficiency curve, but there is the race to sleep concept. Even assuming hanging around at low freq the voltage can sustain is always better power wise - which i don't think is true as you're dropping a lot of performance, you still have to power all the uncore around it  I saw someone run a couple tests on intel/amd for iirc a game server workload, and while intel peaked a lot higher from aggressive boosting, the amd cpu consumed more energy overall",hardware,2026-01-11 00:58:42,1
Intel,nz4duhh,"Neither apple nor qualcomm are real competition in a sense that Apple has its own segregated market that does not crosscompete and qualcomm practically does not exist in segments Intel is in.  ARM is hurting them in servers, but not really relevant for a laptop discussion.",hardware,2026-01-12 06:45:51,2
Intel,nyu8aqs,"> 3nm will give 6050 another 20%  But are Nvidia willing to use cutting edge nodes for their low end GPUs? If they don't move to N3 before Intel/AMD have an N2 GPU, a gap will remain. And of course LPDDR6 should be a big deal for bandwidth.   Obviously not treating this as a forgone conclusion, but doesn't seem like an unreasonable target for this part of the lineup.",hardware,2026-01-10 19:37:26,3
Intel,nystsgw,The only TRUE disable option on windows is to disable through bios for most laptops. Which becomes extremely tedious if you want to use the dGPU without constantly restarting.  I have never owned a laptop with a dGPU that didn't misbehave constantly and not fully idling.,hardware,2026-01-10 15:37:37,7
Intel,nyy6xsc,"Plenty of ""daily use cases"" have very minimal hardware requirement, the original iPhone X FaceID ran on a device with 3GB ram and it was sufficient for FaceID purpose. And I don't know nor care your particular business use case, since you made zero specific clarifications I only had to bring up one mainstream example which is Microsoft 365 and its cloud based subscription based Copilot feature.",hardware,2026-01-11 10:10:21,1
Intel,nyyfwoo,Probably that's what gonna happen with medusa halo. On N2. It will actually match Apple M6 Max pricing.,hardware,2026-01-11 11:32:03,3
Intel,nyue00o,"I am not talking about product classes though? The original statement is trying to say that an **SoC can compete with dedicated iGPU** regardless if Strix Halo is bigger. They are trying to say that it was obvious it was **going to be a flop, when competing against a 4060 that at the time was being sold at a discount**. **Panther Lake is literally coming out another year later one tier below a 4060 and a gen old**.  Yes, they are different product classes, but Panther Lake SKUs that have 10-12 Xe3 cores will most definitely be >$1000 with laptops. ""Mainstream"" pricing is subjective in this class, unlike GPUs where there are 5060s and 5080s segments. At CES, there are surprisingly dGPUs still being paired with Panther Lake, heck, Strix Halo was designed purely for it's iGPU, even the engineers stand by this (PCIe slots are being released in miniPCs because that's what the market wants).  Also, this ""big APU"" argument is based on chiplets/tiles. Strix Halo isn't monolithic, same as Panther Lake. They both have the same design strategy that makes it economically viable to tape out in the first place.  I am not trying to say that STX-H is better than PTL, PTL was like the only thing I was looking forward to at CES, but this whole thread surrounding around how STX-H is a failure doesn't make sense at all.",hardware,2026-01-10 20:05:42,-2
Intel,nyt2jkc,"I hadn't long bought a Zenbook S16, but if MSI can get a decent spec with the B390 under Â£2k then maybe.",hardware,2026-01-10 16:20:10,3
Intel,nz3mb97,Their target audience is more likely to complain about upgradability.,hardware,2026-01-12 03:35:16,2
Intel,nz9ccsy,New goalpost?,hardware,2026-01-12 23:57:14,3
Intel,nyxhb1b,"Race to sleep has value to a point. Does someone on battery want to, say, increase power consumption 4x to race to sleep 2x faster?",hardware,2026-01-11 06:19:34,3
Intel,nyxoaio,> there is the race to sleep concept.  i hope we can excise this concept as soon as possible. It leads to worst design choices.,hardware,2026-01-11 07:18:33,3
Intel,nzbv20u,"I don't think amd ryzen will match M6 Max pricing (amd is selling them at 400$), as those miniPC are manufactured by a lot of noname companies, making a true competition  There are 37 such ryzen ai max 395+ products [https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them](https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them)  And there are also nvidia with their dgx project, Qualqom with their Snapgragon X Elite 2, a lot of RISC-V platforms like tenstorrent with 512 GB/s (but only 32 GB VRAM at 1399$), so even apple will need to double the bandwidth in their upcoming M5 pro/max in order to stay competitive with actual prices",hardware,2026-01-13 10:18:10,1
Intel,nyyn7pe,"I assume at least intel and amd do some research there for how much the cpu should boost if the oems don't, and also have to consider user impact from lower performance but I guess that's more fighting against windows getting slower.   Presumably with current nodes 5GHz is always beyond the point of being worth it but no reason that has to carry into future gens",hardware,2026-01-11 12:33:11,1
Intel,nzbx72p,Medusa halo isnâ€™t strix halo if going by what you think it is going to be. It will be much bigger and on N2.,hardware,2026-01-13 10:37:51,1
Intel,nxxeocb,Great.  Does this mean AMD will finaly stop pricing Strix Point as if it was made out of gold ?,hardware,2026-01-06 01:29:45,317
Intel,nxxrlux,"One of the biggest things the current AMD driven handhelds lack is a decent upscaling option, so getting native XeSS support on a fast GPU would be a HUGE performance boost.",hardware,2026-01-06 02:39:52,112
Intel,nxy2sll,"I think the LPE cores and them going at chiplets a second time after Meteor lake is paying off. This chip is more efficient than lunar lake, a chip that could do 0.62W idle lol.",hardware,2026-01-06 03:42:57,37
Intel,nxy9wxm,"This is exciting. Hope some decently priced handhelds can drop, RAM prices notwithstanding.",hardware,2026-01-06 04:26:42,8
Intel,nxxr090,am confused. this is battlemage too right? because its a B series. but its supposed to be all new. and the old gen was battlemage too on the 200V series. so what is going on here?. is this just a bigger GPU or is this Xe3 so that would be Celestial.,hardware,2026-01-06 02:36:35,22
Intel,nxxhg0r,brah they straight up claiming it's equivalent to a 4050 on stream >!(a 60W RTX 4050)!<,hardware,2026-01-06 01:44:36,51
Intel,nxykdxb,"Even if that claim were overstated by 2x, would still be a colossal L for amd.",hardware,2026-01-06 05:39:32,20
Intel,nxyc1w5,Xps is a huge seller for Dell and they are straight up using Panther Lake and XE3. They are exclusively going intel. Intel is 100% securing up there dominance in Labtops. In the process also taking business away from Nvidia.,hardware,2026-01-06 04:40:47,19
Intel,nxxgruz,I hesitate to trust Intel's charts. But I am interested if Intel will actually get companies to adopt panther lake for their handheld pc. They did not have much luck with lunar lake.,hardware,2026-01-06 01:41:01,21
Intel,ny1ifg6,"Assuming intel also keeps those mobile CPUs a good price, this could be really good. Hopefully as well they add the B390 in their high power desktop CPUs, seeing a core ultra 5 with an iGPU like this would really mitigate the need for a dedicated GPU right away Mostly because iGPUs on other generations were bad, and only a select few Ryzen CPUs had the 890M. Budget systems could become much better for gaming on the low side for graphical intensive games",hardware,2026-01-06 17:30:54,3
Intel,nxz4ji6,>Intel reference platform; Memory: 32GB LPDDR5 9600;  I wonder how much difference that makes and if we'll even see laptops with such RAM in this economy...,hardware,2026-01-06 08:34:38,4
Intel,ny34ie7,"I'd love to see benchmarks comparing it to lower end discrete GPUs (like 5050, B570, etc). Could be a boon for ultra low cost builds depending on what price point it lands at.",hardware,2026-01-06 21:54:52,2
Intel,nxxyz1h,How many compute units does it have?,hardware,2026-01-06 03:20:58,4
Intel,ny1ev6y,XESS and native frame gen is going to make handhelds monsters with Panther Lake in them.,hardware,2026-01-06 17:14:42,2
Intel,ny0ibg4,I think people need to be ready for the fact that OEMs aren't going to use lpddr5x-9000,hardware,2026-01-06 14:42:06,1
Intel,nxxd70q,"We'll see. Every year they claim they're faster, and every year they have been proven not to be",hardware,2026-01-06 01:21:44,-24
Intel,nxxgaz9,NOTE: this might be because it has MFG (Multi-Frame-Generation).  We have to see reports to see if its true or not.,hardware,2026-01-06 01:38:30,-11
Intel,nxxoge8,"This ain't gonna matter. It's the sku with 50% more igpu cores compared to lunarlake, which already has an igpu that's larger than the hx 370, it's real expensive. Imagine a hx370 with 26cu instead of 16, that's the price range you're lookin at  Any system built with this is gonna need to run at extremely high mem speed to feed the really large igpu which in the current market with insane ram prices is gonna be priced out of most people's budget. Are ya prepared for a gaming handheld that costs north of $1500?  And since this is gonna compete against nvidia's entry level mobile gpus oems are gonna have to choose between nvidia and intel for the gaming brand on laptops. Amd learned this through the hard way that most oems would choose nvidia over a large igpu.",hardware,2026-01-06 02:22:32,-11
Intel,nxykt3q,Haven't they been making similar claims for all their failed GPU's?,hardware,2026-01-06 05:42:44,-11
Intel,nxyax11,"I mean Intel has never fudged the numbers before when they were behind, or do something crazy like literally bribe people.... Oh wait.... Uh.... Oh.....   Jokes aside, with what the current and future state of the market looks like, people might have to get used to iGPU graphics.",hardware,2026-01-06 04:33:13,-16
Intel,nxyljzq,"To be fair it kind of is, the die size is huge, larger than an RTX 5070 die",hardware,2026-01-06 05:48:23,58
Intel,nxxotcs,"Intel laptops were already better tbh, AMD had nothing to compete with Lunar Lake, and Arrow lake pretty much was better at high perf efficiency. Zen 6 better not be delayed or AMD will be buried under intel, qualcomm and apple all launching a real next gen shortly",hardware,2026-01-06 02:24:30,87
Intel,nxz1mq8,"Nah, because it's an ""AI chip"" and AMD will market it as so. AI equals fancy even though the AI capability can't match a regular desktop computer for far less.  Intel is probably gonna strategically match AMD in price.",hardware,2026-01-06 08:07:06,6
Intel,nxxvwrz,"With how wide the memory bus is, how much RAM it requires, nah the price is going up.",hardware,2026-01-06 03:03:31,22
Intel,nxzxy26,it has soldered ram ... so it's better then gold!,hardware,2026-01-06 12:45:44,3
Intel,ny04wij,I guess that depends on Intel pricing too. Considering it's using both the latest TSMC and Intel foundries in one chip package. Not to mention the LPDDR5 9600.,hardware,2026-01-06 13:28:38,3
Intel,nxxhzsm,You mean OEMs.,hardware,2026-01-06 01:47:35,-11
Intel,nxyn46u,No?  People will still value AMD more ue to brand so Intel will have to rely on volume for revenue  For reference only yesterday on this sub we had people talking about Intel lacking efficiency in comparison in mobile space,hardware,2026-01-06 06:00:32,-13
Intel,nxy8gkb,Crazy AMD haven't updated their iGPU to RDNA 4. I know they're probably waiting for UDNA but it would have been almost 3 years on the same architecture by the time we get the UDNA refresh next year (if they even bring it to their APUs right away). Sort of disappointing.,hardware,2026-01-06 04:17:21,66
Intel,nxxun0v,"tbf the most important issue is, few games implemented XeSS, just like AMD FSR.  And I think XeSS 3 being implemented in more games is a net positive for AMD GPU too.",hardware,2026-01-06 02:56:27,22
Intel,nxz85mh,"With everything happening around NVIDIAÂ´s price increases and AMDÂ´s lack of providing updates where it hurts, it **feels like** AI-Datacenters are more important right now for them (like the last 2 years).  But who can resent them as Intel had products that where not so much competitive that time.    Arrow lake (Desktop) at least closed on efficiency, but lacks a bit of gaming performance still, hopefully Nova Lake will be the step required to push more competition.   On GPU the same, AMD does not compete with NVIDIA in higher segments while NVIDIA is fairly comfortable with their setup and increases prices because they want to milk customers to increase their ridiculious margins (up to 70%) that they are used to from AI-Chips.   And now Intel also provides Multi-Frame generation, while a niche for me still, starting to compete with AMD and closes up to NVIDIA in terms of Software support, which they lacked the most and fixes a lot of problems.   Now let them release a B770 that is rumoured to be fairly mid/high range and we can hope for competition that actually learned from bad products recently and tries to make it better.",hardware,2026-01-06 09:09:24,10
Intel,nxz92l2,"Handhelds is a tiny tiny market, basing your product stack around them would be monumentally stupid.",hardware,2026-01-06 09:18:25,1
Intel,ny0x7rg,"It may not beat LNL in very low power envelopes (LNL was designed for ~10W, PTL for 15W+), but it's a much, much better baseline than what Intel's historically had in client. Even just extending vaguely LNL-tier efficiency across the stack is a very big deal. Looks like Intel finally has a respectable SoC architecture. Now just need to get the cores and such in shape.",hardware,2026-01-06 15:54:14,14
Intel,ny4y069,"I mean the Xbox Ally X handheld is considered a $999 ""console"" so it sets the floor for what the Steam Deck and other handhelds would be priced at.",hardware,2026-01-07 03:37:21,1
Intel,nxxwdna,"It's branded as a Battlemage for some reason, but the architecture is Xe3. It's much closer to Celestial than it is to Battlemage.",hardware,2026-01-06 03:06:08,55
Intel,nxxt4ce,"Battlemage is the brand name. The actual architecture of Lunar Lake is Xe2, same as desktop Battlemage, but they never explicitly called it Battlemage, only â€œArc Graphicsâ€.  What is meant to be desktop Celestial is Xe3P, but desktop Celestial is likely cancelled or significantly scaled back. Alchemist was a massive flop, and by the time the B580 came out to salvage Arcâ€™s reputation the axe had probably already swung.",hardware,2026-01-06 02:48:07,9
Intel,ny7ef2l,It's a mid-gen refresh of battlemage.,hardware,2026-01-07 14:33:10,2
Intel,nxxpiop,"They claimed ""10% faster"" than 4050   https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Famd-is-done-v0-8op4m6l6bmbg1.jpeg%3Fwidth%3D1851%26format%3Dpjpg%26auto%3Dwebp%26s%3Df229e1ff0e364a6db90715de23ba799261ffe9e3",hardware,2026-01-06 02:28:19,52
Intel,nxxqhlr,APUs are always way worse at gaming than synthetics when compared to a DGPU due to memory bandwidth limitation and power sharing with the CPU among other things like cache set up etc.  when they compare them to GPUs its always synthetics unless you get benchmarks of games,hardware,2026-01-06 02:33:43,22
Intel,nxxl8hj,"Where's the bandwidth coming from?   Reviewers were saying that the 890m was bandwidth starved, so how can this chip be neck and neck with a recent dgpu with multiple memory channels",hardware,2026-01-06 02:05:05,10
Intel,nxy0hn7,60W is the laptop power draw. It looks like 30W for the 4050  this is the laptop they used for the comparison https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor,hardware,2026-01-06 03:29:36,6
Intel,nxy6cgs,"At best, itâ€™s a 16% difference between a 100 watt and 60 watt RTX 4050 I believe, based on synthetic performance  Edit: Intel used a 30 watt 4050, this comment is incorrect",hardware,2026-01-06 04:04:08,1
Intel,ny0ce80,What do you mean that a refreshed Strix Point canâ€™t compete with an updated architecture?,hardware,2026-01-06 14:10:24,5
Intel,ny0j8e4,"I got downvoted everytime I brought this up, but this is precisely why Nvidia wanted a deal to have an Nvidia iGPU tile on an Intel APU: Large iGPUs in thin and lights are going to get good enough over the next few years to make them the new entry-level graphics option for people. This directly threatens Nvidia's consumer laptop volume in the entry segment. Intel is claiming close to 4050 performance at this lower TDP, and that's certainly good enough for many to not have the tradeoffs of having a dGPU in their laptop.  If the new market is moving towards putting GPUs on the CPU package instead of discretely on the board, Nvidia doesn't want to place all of their hopes on WoA becoming better, and are hedging by doing both their own SoC *and* an x86 APU with Intel.  The XPS line dropping Nvidia discrete all together is proof of this. In these sub 70W total laptop power markets, a discrete GPU is just eats into the power budget too much.",hardware,2026-01-06 14:46:50,9
Intel,nxzgk6d,"In the ultraportables market (like XPS), integrated graphics just make so much sense (energy envelope; cooling system required; battery life; etc); and that's already substantial and before considering the cost of a NVIDIA mobile dGPU itself.  I don't understand why AMD decided to price Strix Point and Strix Halo so ridiculously -- it's their market for the taking.",hardware,2026-01-06 10:28:32,9
Intel,nxzf0cx,I think theyâ€™re trying to take away business from Qualcomm/arm on windows before it takes off,hardware,2026-01-06 10:14:30,3
Intel,nxyetge,"Lunar Lake was a expensive product which didn't make sense in handhelds, Intel just didn't have anything else so they slapped that on the MSI Claw. Now the options should be much better considering they are selling a lower core Xe3 version for cheap too",hardware,2026-01-06 04:59:20,10
Intel,ny2yll4,"I'm just curious how they handle the need for such high speed RAM on desktop though? I guess this is an application where CAMM2 will be required, I don't think DDR5-9600+ is possible without it and this is presumably pretty key to the performance.",hardware,2026-01-06 21:27:43,6
Intel,nxzgr0g,"It certainly would make a huge difference because iGPUs are very memory bandwidth bound; and as the name suggests, LPDDR5 9600 has literally twice the bandwidth of the JEDEC standard 4800.  Unfortunately I doubt we'll see reasonably priced laptops with LPDDR5 9600 -- even as an add-on option. I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800; and many SKUs that were 2x16GB are now 1x16GB; yes **single channel**.... they charge you extra if you want 2x8GB.",hardware,2026-01-06 10:30:16,14
Intel,nxyivuh,"It has 12 Xe3 cores. Intel doesn't use the term Compute Units, AMD does.",hardware,2026-01-06 05:28:14,14
Intel,nxzfdoo,X9 and X7 have 12 Xe cores and the best Ultra 5 has 10 Xe cores,hardware,2026-01-06 10:17:51,3
Intel,nxxf8he,??? lunar lake has already shown to be faster than the 890m.  73 percent though seems like a bit much since panther lake was claimed to be around a 50 percent increase over lunar lake,hardware,2026-01-06 01:32:47,63
Intel,nxxhysy,"They did make a graph specifically to compare the performance of HX 370 and this Arc B390 while they were both using 2x upscaling, which is where this 73% number comes from. In another graph featuring supposedly ""native"" 1080p, they claimed Arc B390 was 82% faster than the HX 370 (why don't they just call it Radeon 890M though...)",hardware,2026-01-06 01:47:26,28
Intel,nxxhhap,"No, intel claims 73% with upscaling (both) and 82% native",hardware,2026-01-06 01:44:47,17
Intel,nxy83by,"If it was only a 73% gain *including* MFG, then that would be a serious performance regression. If they were using MFG in their graphs, it would easily be 200% - 300% ""faster"" at the same ""real"" performance",hardware,2026-01-06 04:15:06,5
Intel,nxxhnbn,"The graphs all listed games and I didn't see any synthetic benchmark scores were listed, so yeah.",hardware,2026-01-06 01:45:42,23
Intel,nxxsvz8,The relative proportion of the die isn't as important as the die size itself and the node ofc.   Lunar lake for example has an estimated die size smaller than the hx370 so even if they did make the die bigger I don't think that is going to massively raise the price. Not to mention intel owns the foundry unlike AMD who are outsourcing to TSMC. This isn't in the realm of a strix halo competitor with a 300 mm\^2 + die size.   Dell for example has already refreshed the XPS line with intel panther lake and cut out the option for a dedicated gpu.,hardware,2026-01-06 02:46:52,21
Intel,nxxhf0d,"There are 50% more GPU cores here than on Lunar or Arrow Lake. CPU is still 16 cores as well compared to Arrow Lake, just shifted from 6+8+2 to 4+8+4.",hardware,2026-01-06 01:44:27,28
Intel,ny08rcj,"Do you mean Strix Halo?  Halo is made up of THREE dies. Two are regular CCD and one is a ~300mm2 graphics die. Total die area is around 440mm2 IIRC.  It's expensive, but not THAT expensive.",hardware,2026-01-06 13:50:17,30
Intel,nxxs32m,Doesn't this depend on use case? AMD laptops are more capable for gaming and the iGPU can also use the lesser version of FSR. Intel is obv better for productivity.,hardware,2026-01-06 02:42:30,14
Intel,nxzgbtp,"I disagree, Arrow lake HX seems to be more expensive than AMD HX as least on Lenovo Legion Pro setup.   I would have buy Arrow lake for the same price but AMD is cheaper by $200.",hardware,2026-01-06 10:26:24,1
Intel,nxy0jsm,Eh? It's a standard 128 bit memory bus.â€‹,hardware,2026-01-06 03:29:57,37
Intel,nxz8yd7,"You can't price it higher than people are willing to pay, how high that is I have no idea, people bonkers buying CPU only laptops at these high prices if gaming is something they really want to do.",hardware,2026-01-06 09:17:16,1
Intel,nxyjv70,Oems magically dont price intel variants as if they were made out of gold?,hardware,2026-01-06 05:35:35,6
Intel,ny2zhn1,"Lmao check the data, Intel has 79% of the laptop market share currently",hardware,2026-01-06 21:31:48,5
Intel,nxz7zsg,"Reddit isnâ€™t indicative of anything really, most casual laptop buyers donâ€™t even know what AMD is.",hardware,2026-01-06 09:07:48,11
Intel,nxzevm6,"I wouldn't be surprised if this is because the team has chosen to focus efforts on UDNA because that's the architecture next-gen consoles would use. They only have so much talent and headcount on their graphics division after all, and consoles have much higher volume (even tho low margins) and thus take priority.",hardware,2026-01-06 10:13:17,34
Intel,ny7dxyt,AMD is planning on again using RDNA 3.5 on their next mobile chips as well.,hardware,2026-01-07 14:30:42,4
Intel,nxy8549,Not an ideal solution but Optiscaler exists,hardware,2026-01-06 04:15:24,15
Intel,nxzf3cy,"NVIDIA has increased margins but they haven't been that terrible. Part of the compounding issue at play is limited TSMC capacity; with both gaming and DC on the same TSMC node.  Ampere (crypto bubble ignoring) was priced well and many excellent cards in there since it was on Samsung, a cheap fab; while DC/workstation chips got TSMC.",hardware,2026-01-06 10:15:15,7
Intel,ny0vvvv,Who said anything about basing the entire product stack around handhelds?,hardware,2026-01-06 15:48:11,7
Intel,ny13z79,"That's not quite right. Power levels are determined by the frequency of a given CPU core. The LPE cores in Lunar and Panther lake both clock up to 3.7 GHz, so given the added IPC of the new Panther lake e-cores and better process node, it is more efficient. Base power levels tell you nothing really.",hardware,2026-01-06 16:25:13,2
Intel,nxz3zvx,"[It's actually closer to Battlemage than Celestial. Straight from Tom Petersen](https://youtu.be/P2AsCkKi-vs?t=1576)  >""Unfortunately that Xe3 name got decided years ago, it's actually spread around the Linux stack. Changing the name of that would have been very, very painful. So, that's why you're seeing this disjointedness abut Intel Arc ""B"" series. **Well, [Panther Lake] is B series because it's similar to Xe2** and we want to be transparent with our customers. Panther Lake has a new and improved GPU, that GPU is bigger and **it's very similar to B series.**""",hardware,2026-01-06 08:29:22,21
Intel,nxyzwrf,"Xe3 isn't Celestial, only Xe3P will be. See [https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake](https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake)",hardware,2026-01-06 07:51:09,13
Intel,ny0y47o,"Yeah, it's a proper generational jump. Intel marketing is just dumb, and the comments from Peterson claiming Xe3 is somehow a smaller jump than Xe3p are just laughable.",hardware,2026-01-06 15:58:20,5
Intel,nxyhb1s,The reason is marketing (the Battlemage brand is hot and filled with good will ATM so resetting to celestial so soon is not ideal regardless of panther Lake being xeÂ³) Peterson addressed this a bit ago.,hardware,2026-01-06 05:16:46,1
Intel,nxysdu1,Xe3p was alr confirmed coming im sure Celestial happens,hardware,2026-01-06 06:43:57,2
Intel,nxzg5fp,"We will see, while Intel's PR and marketing is extremely confusing, Intel did confirm Xe3P will come to desktop; and at least from driver updates (as a very happy B580 owner) driver support has been constant and lively.  I had some issues with an older Civ game, I reported an issue in [their app](https://www.intel.com/content/www/us/en/support/articles/000057021/graphics/other-graphics.html) with screenshots/etc, and while I never got any notification, the game works perfectly now a few months later. Dunno if they read those reports, but my card keeps getting better.  I actually think a MSRP B580 is another card that will age like fined wine -- YMMV depends on games you play, but in Australia they have been regularly sold slightly below international MSRP and represent phenomenal value in the price class.",hardware,2026-01-06 10:24:47,1
Intel,nxzdyw4,That's bloody good for an iGPU. It's been nice to see them finally get to respectable performance over the last few years. Intel in particular has really upped their iGPU game & it shows.,hardware,2026-01-06 10:05:03,20
Intel,nxxpg2g,not to mention it is a little skewed as they threw in a title which pushed the vram limit on the 4050 making the b390 over 800 percent faster in that title which obviously messes with the average.,hardware,2026-01-06 02:27:55,20
Intel,nxxsozj,It's 10% faster geomean across 45 games,hardware,2026-01-06 02:45:47,28
Intel,nxzfaqu,That can be resolved if either Intel or AMD decides to unlock quad-channel on consumer chips and mobos. It's artificial market segmentation; the die area needed to deliver more (LP)DDR5 channels is absolutely minuscule; for a huge boost in iGPU performance.,hardware,2026-01-06 10:17:06,2
Intel,nxy0rol,Cache. Lots of it.,hardware,2026-01-06 03:31:13,25
Intel,nxxpegk,"They are using 9600mt/s lpddr5x, could also have a lot of cache, (iirc 890m configs were nerfed in cache because they wanted to put a npu instead), and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.",hardware,2026-01-06 02:27:40,12
Intel,nxxumwa,Panther Lake still has a 128 bit memory bus so only models with 9600 mt/s will get slightly faster shared memory bandwidth than Lunar Lake.   I wonder how this will manifest in games as the only performance leaks have been from Geekbench and 3DMark which may not be as bandwidth intensive as real games and applications.,hardware,2026-01-06 02:56:26,4
Intel,nxy8e9t,"This seems to be correct, since checking NotebookCheck for the 30 watt 4050 shows that itâ€™s around 70% faster than the HX370 in games, which is roughly where Intel places their iGPU.  The performance difference between a 30 watt 4050 and full 140 watt 4050 is around 41 percent performance based on Time Spy",hardware,2026-01-06 04:16:57,15
Intel,nxy2xyi,"basically cheating tho, rtx cards in dell laptops are barely getting enough watts to even turbo",hardware,2026-01-06 03:43:51,3
Intel,nxzpm42,"That's the only way to do a fair comparison, really.   Because the 45 watts that Intel chip uses is shared for the entire chip.   So it's still 45w Intel + igpu vs 60w Intel+gpu",hardware,2026-01-06 11:45:18,1
Intel,ny0na4n,"Itâ€™s a super strong generational gain though, itâ€™s like the jump from Vega 8CU to rdna2 12CU. The kind of single gen gain you see once in 5 years at most",hardware,2026-01-06 15:07:11,10
Intel,ny2zoh2,The thing about that... what sort of tile are we expecting them to package up? As you say if we can get 4050ish performance from an Intel iGPU then they really can't be far off 5050M... and maybe even 5060M performance in future.  Do you think they'll offer something like a 5070 tile? that almost seems excessive (and difficult to actually package from a thermal point of view in a laptop) but it seems like the 5050/5060 sort of tier is going to be pretty well covered as a traditional iGPU soon.,hardware,2026-01-06 21:32:40,1
Intel,nxzk1m0,AMD actually introduced lower tier Strix Halos in this CES; and the first budget laptop thats gonna use it is [the Asus TUF A14](https://youtu.be/h27w0PXFBgk?si=Pa7UQhinywF-uFMj&t=306),hardware,2026-01-06 10:59:13,3
Intel,nxxklk7,They're a lot more accurate than whatever the fuck Nvidia has been doing where you have to decode their bar graphs for proper scaling lmao,hardware,2026-01-06 02:01:38,49
Intel,nxzfsf4,"I'm pretty sure Intel threw lots of ""marketing money"" for the MSI Claw too. There were heaps of MSI Claw promotional booths / draws at shopping malls / public places in Australia and it was heavily discounted.  I picked one up for about $550 AUD (after rebates; tax included), which is like $369 USD inclusive of tax.",hardware,2026-01-06 10:21:31,2
Intel,ny13km9,> Lunar Lake was a expensive product which didn't make sense in handhelds   What do you mean? All the tradeoffs LNL made were pretty good fits for a handheld.,hardware,2026-01-06 16:23:22,2
Intel,nybz77q,"I don't think that will really be an issue, laptops can be configured with soldered 8 channel RAM like AMDs Z2 extreme, or they can still manage easily with regular DDR5 6400Mhz sodimms, which run at 102.4GB/s  Plus the CPUs that have intels new B390 iGPU are 4P/8E CPUs, so I doubt there will be much issues from low ram speeds. Something like the Radeon 890M have done fine with such speeds",hardware,2026-01-08 03:26:50,1
Intel,ny13z0b,"> I've been eyeing Dells and Lenovos, and basically all SKUs that previously had 6000 are getting substituted with 4800   You're looking at normal DDR, not LPDDR. LPDDR5-9600 *is* a JEDEC spec, and already available in mobile.",hardware,2026-01-06 16:25:12,5
Intel,ny020mx,"The majority of the lineup still only has 4. Will be interesting to see what the pricing and performance is on those since these will likely be quite limited. What's also a bit crazy is there's three different nodes being used for the various GPUs, and the full 12 unit one is probably on N3.",hardware,2026-01-06 13:11:40,2
Intel,nxxlxrw,"I doubt it's exactly 73% outside of cherrypicked games, but it should not be shocking that it's significantly faster than rehashed rdna3.",hardware,2026-01-06 02:08:52,-3
Intel,nxxjlfd,"Every Intel marketing benchmark for like a decade or so, but especially their GPUs seem to do far better in their benchmarks than they do in reality.",hardware,2026-01-06 01:56:13,-20
Intel,nxxhxjk,"Ice lake, Alder lake, Metor Lake",hardware,2026-01-06 01:47:15,-20
Intel,ny0aepy,"Different poster than OP.  Compute tile on Lunar Lake is 140mm2 on N3E with a small 46mm2 controller N6 tile. Strix Point (HX 370) as a whole is 233 mm2 on N4P. Lunar Lake is clearly cheaper, but given the newer node and packaging not massively so, likely by around 20-25%.  Panther Lake, with the B390, is going to be significantly more expensive than Lunar Lake. The B390 GPU has 50% more CUs, and that is very likely still on N3 or some variant (Intel only labeled this as external on their deck). CPU size 4xP+12xE as opposed to Lunar's 4+4, which should still be significantly more area with the core upgrades despite being on A18.  The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.  Intel Foundry in general isn't any cheaper than TSMC. With Intel being practically the only user and development expenses it's likely more expensive than TSMC despite TMSC's margins. For all purposes it's an accounting trick to hide CCG's and DCAI's 5-15% operating margins if you divide the foundry losses per group revenue.",hardware,2026-01-06 13:59:25,1
Intel,nxxpvi7,"Yes, although the actual low power 8 core successor to Lunar Lake is the 335/365 with half GPU cores and slower RAM.",hardware,2026-01-06 02:30:19,-8
Intel,ny0b9nu,"even if you exclude the CPU CCDs the graphics die alone is bigger than a RTX 5070Ti mobile GPU, which also retails for \~$2000-$3000, same as Strix Halo laptop",hardware,2026-01-06 14:04:11,9
Intel,ny393aj,"This is slightly splitting hairs but the 8c CCDs in Strix Halo is actually NOT the same chiplet as the ones in desktop zen 5 parts. It iirc is produced on a smaller node, slimmed down, and has different ( or no) TSVs.  It is similar to design and cache sizes to desktop however, but the changes to the CCDs were done to improve low power performance characteristics. They are likely a bit more expensive than Desktop CCDs.  I believe it is discussed in a chips&cheese deep dive.",hardware,2026-01-06 22:16:26,4
Intel,nxycvko,"AMD have largely been ""winning by doing nothing"" due to their better driver support stack for gaming on iGPUs, rather than actually throwing superior hardware at it.  It's almost ironic how AMD's mobile chipsets are now the ""Intel 14nm+++++"" of this generation.  Constant minor refreshes or even straight-up re-badges of old chips.  Now that Intel Arc has been around a while now and is getting quite capable.  I suspect Intel have a real opportunity to overtake AMD this generation in the iGPU space (ie. handheld and mini-PCs), especially since the new AMD APUs are just **another** refresh with a clock boost and Strix Halo is not scaled or priced to be actually affordable by normal people in that market.  XeSS can also act as a massive force-multiplier in power-constrained scenarios like handhelds.  AMD really shot themselves in the foot by either not building or not allowing FSR4 to function on RDNA3/3.5, which all current and now next gen AMD handhelds are stuck on.  Given how effective DLSS is on the Switch2, one could only imagine how kickass a Nvidia chip in a handheld PC could be with the far more ubiquitous DLSS support.",hardware,2026-01-06 04:46:15,78
Intel,nxyz1xa,Now they are not. The panther lake igpus are undisputed winners (excluding the 395+ from amd since it's just not gonna be mainstream). You can get a 358H or 368H and you'll have solid laptop for igpu gaming far cheaper than the 395+,hardware,2026-01-06 07:43:17,16
Intel,nxz8aui,"For business apps laptops have been good enough for 10 years now, iGPU and battery life is really the only differentiator.",hardware,2026-01-06 09:10:50,7
Intel,nxxvlpa,"Intel is plenty competent for gaming, and has XeSS which is way better than FSR3.",hardware,2026-01-06 03:01:47,39
Intel,nxxx4hv,Lol? No 6 or 8 core 3dvcache laptops and no 5080 or 5090 laptops. Strix Halo is a joke for gaming as well,hardware,2026-01-06 03:10:21,-9
Intel,ny1dwcs,The 9955HX + 5070Ti is $2240 and the 275HX + 5080 is $2540.   When both 5070Ti configurations are on sale they should be the same price.,hardware,2026-01-06 17:10:18,4
Intel,nxy23yz,My mistake I was thinking of Strix Halo,hardware,2026-01-06 03:39:00,34
Intel,ny0f479,"It's about compromise. I don't *want* a 4lb laptop. I don't want a laptop that runs hot when web browsing. Or a laptop that has loud fans, or gets poor battery.  I have a desktop for gaming and other demanding tasks. For a laptop, I, and most of the market, want it focused on portability. Light weight. Cool running. Long battery. These big iGPU PTL laptops are really interesting because they provide *good enough* gaming without sacrifice to the non-gaming livability of the device.",hardware,2026-01-06 14:25:02,1
Intel,ny02zjc,"It took me way too long to convince my sister the AMD laptop I bought her isnâ€™t going to blow up in her face and lose all her data, the Intel(and now Apple) CPU brands are very strong.",hardware,2026-01-06 13:17:30,2
Intel,ny0wejw,"> most casual laptop buyers donâ€™t even know what AMD is  We're past that point now. Even ""normies"" have heard of AMD from news.",hardware,2026-01-06 15:50:32,1
Intel,ny006yw,"its always ""fix it next generation"" with AMD.",hardware,2026-01-06 13:00:12,24
Intel,ny7wgeg,This is unfortunate news  (â•¥ï¹â•¥),hardware,2026-01-07 16:00:09,1
Intel,nxyndbc,"I mean yeah it's not ideal, but you could argue it's the same with XeSS or FSR 4 on RDNA 3. Since the OP said ""there's no decent upscaling on AMD handheld"", therefore I assume Opsticaler is out of the question too.",hardware,2026-01-06 06:02:32,7
Intel,nxzib6q,"Well you said it, it's TSMC capacity, meaning also a priority issue. They prioritize AI over consumers and then increase the price by reducing availability, meaning the same chip costs more, meaning more margin.  Seeing they increase the 5090 to roughly 5k (USD) is just the beginning and as I know all companies will use the increasing memory prices to say they must increase the product price, just not proportional to the memory costs.  next step: then they will use this to move more to streaming instead of owning",hardware,2026-01-06 10:44:14,-2
Intel,ny15kho,"> Power levels are determined by the frequency of a given CPU core   There are SoC and platform level targets that depend on a lot more than just clock speed for the same cores. Consider how LNL's PMICs scale vs FIVR/DLVR. Or what operating point benefits the most from the on package memory.Â   Especially at really low power, the cores are not your big concern. Consider the difference at 10W between 50% of your budget available for compute and 80%.Â   > so given the added IPC of the new Panther lake e-cores   We're talking a couple percent. DKT is a tick.Â    > and better process node   Very much unproven.Â    If you want to give credit somewhere, pretty much all of it should go to the SoC and GPU teams.",hardware,2026-01-06 16:32:30,3
Intel,ny0idmq,"Xe2, Xe3, etc. are the ""real"", more accurate names. Battlemage, Celestial are the marketing names.  Intel's decision to label the new Xe3 iGPUs as ""Battlemage"" is certainly an interesting (odd) choice - my best guess for this decision is that next year, Xe3P discrete will launch alongside Xe3P iGPU in NVL, and they're saving the new Celestial naming for that launch event.  Xe2 -> Xe3 is the bigger change.",hardware,2026-01-06 14:42:24,7
Intel,ny1hu62,"Peterson states explicitly it's to take advantage of good Battlemage branding, around 1:30 of this video. [Intel Talks Xe3 Improvements For Gaming - YouTube](https://www.youtube.com/watch?v=Bjdd_ywfEkI)",hardware,2026-01-06 17:28:12,3
Intel,ny0xhzi,"> Xe3p was alr confirmed coming  Not for client dGPUs, which are what get the Battlemage/Celestial brand.",hardware,2026-01-06 15:55:31,4
Intel,ny0xj91,> Intel did confirm Xe3P will come to desktop  They have not.,hardware,2026-01-06 15:55:41,1
Intel,nxy8fjz,"They also might be getting better value out of the ""2x scaling"" choice for benchmarking. Notice how they are behind Nvidia in all the none scaled titles except Dota2 that I saw.  Still very good results for a iGPU, but they are not entirely honest numbers either.",hardware,2026-01-06 04:17:11,14
Intel,nxxshuz,It's 1 game out of 45 in geomean which devalues outliers. ~~9.9% faster instead of 10% faster if you take it out.~~  Edit: Oh no it's actually 6 FPS on the 4050. Yeah that's way too big for geomean to smooth out.,hardware,2026-01-06 02:44:44,24
Intel,nxy7k6w,And the fact they showed 45 games shows how confident they are in this product.  I remember the Intel slides with 5 hand picked titles we used to get just a few years ago.,hardware,2026-01-06 04:11:44,27
Intel,ny17afz,You mean in desktop? Or do you want mainstream mobile to go quad channel?,hardware,2026-01-06 16:40:20,3
Intel,nxzgpgy,I wonder how 96MB cache would do had Intel put that much on it.,hardware,2026-01-06 10:29:53,3
Intel,nxyig6s,They have 16 MB of L2 just for the GPU alone lmfao,hardware,2026-01-06 05:25:03,11
Intel,ny08xk6,> and also could be a synthetic benchmark or specific game that isn't very bandwidth heavy.  They're benching 45 games dude.,hardware,2026-01-06 13:51:15,7
Intel,nxyiiu1,"Also there's like 45 games on display here, it's not just 3dmark",hardware,2026-01-06 05:25:35,6
Intel,nxy8oam,41 percent difference in performance compared to a full 140 watt in Time Spy. Honestly a bit surprised it isnâ€™t more performance difference.,hardware,2026-01-06 04:18:43,4
Intel,ny55i5l,"No, it's disingenous. Because everyone would think 60w 4050 = 60w on gpu alone",hardware,2026-01-07 04:22:58,0
Intel,o0fmoja,"And it's not like the ""last gen"" GPU in lunar lake was bad either, so we are starting from already good and making the jump up.",hardware,2026-01-19 06:26:10,1
Intel,ny384o8,"Not really sure. I believe it's Hammer Lake that's debuting the Nvidia tile, and that's rumored for a 2029 launch, so still quite a ways off, and 2 generations ahead of Blackwell.  The only rumors I'm aware of that it's going to be a pretty big iGPU",hardware,2026-01-06 22:11:51,4
Intel,ny7dl7u,"Nvidia's graphics have shown to be more efficient for space than both Intel and AMD, so whatever they use it will likely be better than what Intel can currently put out.",hardware,2026-01-07 14:28:52,2
Intel,nxzkp7q,Fantastic -- but at least six months too late ;),hardware,2026-01-06 11:04:52,4
Intel,nxz5md2,You don't like graphs with zero scale claiming their latest 100W GPU is somehow a gazillion percent better than a 4090 or something?,hardware,2026-01-06 08:45:06,9
Intel,nxy8v6h,Wattage limited 4050 to 30 watts is the only slide thatâ€™s suspect.  Itâ€™s around a 41 percent performance loss based on Time Spy from the 140 watt 4050.,hardware,2026-01-06 04:19:56,7
Intel,ny7raf2,I think they meant that the chip is very pricey which sucks because the handheld is already low-margin otherwisr and can't be priced too high else it got undercut by its competitors.,hardware,2026-01-07 15:36:28,1
Intel,ny0l0pv,The standard 4Xe models use the extra die space they save to have more PCIe lanes. that large iGPU adds cost and doesn't make much sense to use that chip if you're gonna add an Nvidia dGPU,hardware,2026-01-06 14:55:55,3
Intel,nxxp4au,"yeah just looking at the game sample I can see a few that really don't perform well on RDNA architecture at least relative to nvidia(idk what really constitutes an ""intel favoured"" title)   Like stalker, csgo 2, civ vii, dying light the beast, and delta force ik run a lot better on nvidia relative to amd so im guessing the same holds true for intel vs amd.   A couple titles amd does well in were thrown in there too though like God of war and Cod but im guessing the real performance difference is more like 40-60 rather than the claimed 70-80.   Pretty large sample though which is nice so the numbers can't be that off.",hardware,2026-01-06 02:26:09,10
Intel,nxxtyhj,Why not?   It's 50% more cores + architectural improvements + clock  speeds,hardware,2026-01-06 02:52:42,12
Intel,nxzgxn6,"Please provide a **single** example in the past ~5 years of an Intel marketing benchmark that is materially inaccurate or untruthful.  NVIDIA is the one playing it loose with BS charts, AMD generally has a good track record (with some exceptions), and Intel on the GPU side has been pretty accurate. For example, these benchmarks have 45 games (!!) and use geomean to reduce outliers.  While I disagree with their choice of LPDDR5 9600MHz (hah, imagine a single consumer product shipping with that in this DRAM market), it is not untruthful.",hardware,2026-01-06 10:31:54,8
Intel,nxxkps6,All were pretty accurate.,hardware,2026-01-06 02:02:15,17
Intel,nxxkqio,But lunar lake igpu actually perform better than 890M.Like comparison of core ultra 7 and z2 extreme in handheld like msi claw.,hardware,2026-01-06 02:02:23,15
Intel,ny0lzbq,">The ""mainstream"" variant also has the 4xP + 12 x E CPU, so even with the GPU being cut to 4 units it's likely somewhat more expensive than Lunar Lake.     The mainstream unit that's more directly comparable to LNL is the same core count (4+0+4) with a smaller iGPU tile. It'll be cheaper.  The 4+8+4 w/ 4Xe is the direct replacement to ARL-H, and that should also be cheaper than ARL-H.",hardware,2026-01-06 15:00:42,1
Intel,nxxvizf,"True, but then still, that's not a removal of CPU cores like they said it was.",hardware,2026-01-06 03:01:22,11
Intel,ny3ft4h,That is due to the Nvidia tax and AI bubble rather than the production cost of the chip. Even Apple ships cheaper silicon than that.,hardware,2026-01-06 22:48:46,3
Intel,nxyibby,"This is very topical and cyclical of Intel/AMD. Intel did really poorly for like a half a decade which was unusual but usually they go back and forth. One gets lazy and incompetent, the other curated a masterful product that becomes dominant for a while and then they get lazy and it flips around.  Intel is planning on socketing a ton of cache on their next breed of chips which will massively boost their gaming perfomance and they have pretty darn efficient chips now too.",hardware,2026-01-06 05:24:04,17
Intel,nxygca9,Thank you for the thorough explanation! Very excited for the future of miniPCs and handhelds since there's so many games I'd like to play on the go.,hardware,2026-01-06 05:09:52,2
Intel,nxz2wtv,"Yup, I am very happy to learn how wrong I was thanks to other people in this thread as well.",hardware,2026-01-06 08:19:04,6
Intel,nxzn2om,"For business apps 10 years ago yes, now even Office has bloated itself up so much it's genuinely taxing even on the Apple chips  And well, the better the chip, the more outrageous the user workload gets. I appreciate the modern laptop chip's ability to import a CSV the size of Excel's row count limit and make a pivot table out of that, but now that it *can* do that I'm *expecting* that to be possible as quickly and as efficiently as possible.",hardware,2026-01-06 11:25:00,6
Intel,nxxy7wk,I was under the impression that XeSS needed a dedicated GPU? If it can run on iGPU that's a whole different story.,hardware,2026-01-06 03:16:38,-11
Intel,nxz8pk3,"Their GPU's only look good when compared to 1 generation old bottom tier GPU's of their competitors. Its wild the praise they get.  Same thing will happen here, AMD will release a new iGPU architecture and Intel will be left comparing to out of date CPU's no one buys anymore.",hardware,2026-01-06 09:14:52,-6
Intel,nxxynsf,Sorry I should have specified that I'm talking about budget laptops with iGPUs.   I would sooner build a pc than even think about a 5080 laptop with 3dvcache options.,hardware,2026-01-06 03:19:11,16
Intel,ny01arz,"Yeah; meanwhile NVIDIA just released DLSS4.5 for **every single RTX GPU**... yes all the way back to Turing. It runs a lot better on more recent cards, but it's available on every single RTX GPU if you want to.",hardware,2026-01-06 13:07:13,18
Intel,ny6k0v5,Except with UDNA it might be the first time over a decade AMD isn't phoning it in.,hardware,2026-01-07 11:22:51,1
Intel,ny0x2bs,"XeSS and FSR 4 on RDNA 3 both use downgraded versions of those upscalers, that either look worse, perform worse, or both. In the case of FSR 4, it's a leaked one-off model that people got their hands on. All I really meant by ""decent"" was having an officially supported modern upscaler without all the downsides.  An Intel GPU running XeSS would presumably get the full version of XeSS without the performance hit and with good visuals.",hardware,2026-01-06 15:53:33,3
Intel,nxzipnz,I can currently buy a brand new 5090 in Australia for $2841 USD with express postage included; I'm not sure why it's 5k USD in your region; but there's no reason you should be paying 5k USD. Which country are you in?,hardware,2026-01-06 10:47:45,5
Intel,ny4uh4q,"The ultra X9 388H has a base TDP of 25W and minimal assured power draw of 15W. Meanwhile the ultra 7 155U has base TDP of 15W and minimal assured power draw of 12W. Both these numbers are lower for the meteor lake chip, yet the Panther lake chip is waaay more efficient (+2x). The base power level doesn't mean anything. It might be the point where the chip had the most perf/watt, but that doesn't mean that the performance at lower wattages is the same.",hardware,2026-01-07 03:17:13,-1
Intel,ny0xx01,"Battlemage, Celestial, etc are named they (usually) use only for the dGPUs, even if that does correlate with the B/C-series naming. I think at some point this is just reading the tea leaves. The name's misleading for the tech difference.",hardware,2026-01-06 15:57:25,4
Intel,nxyibw6,XeSS FG has lower overhead than Nvidia IIRC,hardware,2026-01-06 05:24:11,6
Intel,nxyaxye,If you actually do the maths it'd go down to (1.1^(45)/9)^(1/44) = 1.049 = 4.9% faster,hardware,2026-01-06 04:33:24,14
Intel,ny3bk9e,"Oh wow that's a lot later than I expected, I was thinking this year or next.  Yeah no clue in that case.",hardware,2026-01-06 22:28:12,3
Intel,nxzfk6p,Infinity percent better at a feature the older GPU used for comparison does not support!,hardware,2026-01-06 10:19:28,1
Intel,ny0kqkf,"I don't really think that's ""suspect"". They said they're limiting the total laptop power on the 4050 to match the total laptop power of the PTL chip. If you want stronger performance out of a 4050, you're gonna need to have much higher power draw than the PTL laptop",hardware,2026-01-06 14:54:29,2
Intel,nxyj1tk,"CSGO is known for running like utter shit on Intel Arc, you can check r/IntelArc for details LOL. The game selection looks pretty reasonable to me.",hardware,2026-01-06 05:29:27,8
Intel,nxy8v14,"Yeah all depends on pricing, 6 core ultra 5 model is however technically downgrade from last generation and the same core config as the i3 1315U.",hardware,2026-01-06 04:19:54,-2
Intel,nxyk9c8,Honestly not that unusual. It takes an average of around 4-5 years to develop a processing unit from the ground up. If we assume each one does this when they get mushroom stamped by the other for being lazy it accounts for the 5 years gaps till they show back up with something to sell.,hardware,2026-01-06 05:38:33,22
Intel,ny0dmdk,"I think AMD is getting a bit lazy when it comes to consumer graphics. I think their attempts at laptop have been really half-assed given just how good their IP portfolio is.  But when it comes to their core businesses, they're definitely been keeping the heat on and have been quite aggressive. They're datacenter first and foremost, and that trickles down to amazing desktop CPUs too. They're heavily focused on building out their Mi series too...but they're just dropping the ball in laptop and consumer GPU",hardware,2026-01-06 14:17:03,7
Intel,ny39k1v,Pantherlake also has an oddity in that it has MUCH higher L2 cache than even desktop zen 5 parts. I'm curious to see its CPU performance in low resolution scenarios.,hardware,2026-01-06 22:18:39,1
Intel,nxxyz2v,"The good version of XeSS runs on any chip with XMX units (Intel's version of tensor cores). Lunar Lake, Arrow Lake mobile, and now Panther Lake have GPU tiles with XMX units, so they get the same XeSS as discrete Arc cards.",hardware,2026-01-06 03:20:59,29
Intel,nxyty97,"Dedicated hardware, not dedicated GPU. The new Intel CPUs have iGPUs with the necessary hardware.",hardware,2026-01-06 06:57:17,7
Intel,nxy622y,"It needs dedicated GPU hardware to run faster, but theyâ€™ve started incorporating it on Lunar Lake and Panther Lake",hardware,2026-01-06 04:02:21,5
Intel,ny0e8rw,Intel has been very aggressive in the iGPU space. AMD isn't going to have any real updates to their iGPUs until 2027 the earliest.,hardware,2026-01-06 14:20:20,9
Intel,ny2bpjc,"> Same thing will happen here, AMD will release a new iGPU architecture   ... Based on what history? AMD's iGPU has not significantly changed in years. It's still hugely memory bottlenecked and no matter how many times they add an extra 2 CU's, it will still be memory bottlenecked.  IIRC someone disabled 2 CU's on their 7000 series APU and their in-game FPS almost didn't change because the bottleneck was actually memory access.  Intel ARC is actually very good on this metric. Intel doesn't exactly need to sling anything better than ""slightly more Battlemage on a better transistor"" to completely swamp out AMD iGPU in this space.",hardware,2026-01-06 19:42:26,1
Intel,ny2y1c0,"Intel Panther lake base tdp is 25w, around the same as AMD Strix Point/ Gorgon Point. Why will they compare it to a 55w tdp Strix Halo?",hardware,2026-01-06 21:25:10,1
Intel,nxy5y1b,"Even on the budget laptops category the new Ryzen 7s suck compared to the Intel Lunar Lake options, they seem to be priced closer with Lunar Lake getting stuff like nice displays. In the really budget category I feel like they are tied on value and I don't know how sales affect that. This is partially cause AMD went cheap on the mid-range kraken point chips and also had to fit in the still dead weight 40 tops NPU for Microsoft. So it only has 8 GPU cores.",hardware,2026-01-06 04:01:39,9
Intel,ny6ka8x,"Yeah but basically unusable on pre 40 series. But at least NVIDIA gives users the choice.  AMD should just stop the BS pretending and just enable the full FP8 model across RDNA2-3 with FP16 emulation. But it prob runs so bad that they won't, far far worse than DLSS 4.5 on 20-30 series.",hardware,2026-01-07 11:24:58,2
Intel,ny6ob4j,"It would be good if that is true, but so far ive seen nothing that would inspire me confidence in AMD. And yes i remember the AMD patents you posted last year.",hardware,2026-01-07 11:55:47,2
Intel,ny7ebjk,"Yeah, it's DLSS4>FSR4>XESS (Intel)>=DLSS3>XESS (fallback)>FSR3      quality wise.",hardware,2026-01-07 14:32:39,1
Intel,nxzj5en,"First custom design OEM are fast, here 4400â‚¬ on Amazon https://amzn.eu/d/idxVW9M  And you know how this goes, one starts the other follow.  Here in the US for a normal founders edition for 4.2k USD + TAXâ€¦ one article from the first of January quoted ot that time being at 3.7, like 5 days ago.  https://www.newegg.com/nvidia-founder-edition-900-1g144-2530-000-geforce-rtx-5090-32gb-graphics-card-double-fans/p/1FT-0004-008V4?source=f",hardware,2026-01-06 10:51:32,-1
Intel,ny5eqws,"When I talk about ""design targets"", I'm not referring to an arbitrary TDP. There are very specific decisions each SoC made that have tradeoffs at different power envelopes.   Also, the context was LNL which is an entirely different beast from MTL.",hardware,2026-01-07 05:25:06,2
Intel,ny0ypvn,">The name's misleading for the tech difference.  Yeah, that's my point. People are reading too much into the ""B series"" naming scheme for B390.  As you said, ""(usually) use only for the dGPUs"". So if Xe3P is launching as a discrete Celestial Card, then it would make sense to have Xe3P tile be part of the ""Celestial"" launch, rather than Celestial Discrete being ""one year later than Celestial integrated""",hardware,2026-01-06 16:01:03,4
Intel,nxyf825,Oh dang you're right lmao.  The 4050 has SIX (6) FPS at 540p high. I thought OP was exaggerating with 800%.,hardware,2026-01-06 05:02:05,4
Intel,nxyahr2,Yeah those kinda suck. Should be Ultra 3s given they're basically WCL spec.,hardware,2026-01-06 04:30:27,2
Intel,ny2mxg2,"More like 10 years, 5 to realized that they are getting stomped in the face, and another 5 to actually make something of it.",hardware,2026-01-06 20:34:21,3
Intel,nxyak62,"Man, there are so many older and less demanding titles I'd love to play through on the go, but knowing that Lunar Lake laptops have better displays for the price is really good. Thanks for the info!",hardware,2026-01-06 04:30:54,6
Intel,ny6po5w,"If you're referring to the April dump, heck even the August dump (analysis of Kepler\_L2 patents) then that's not close to the complete picture. A lot of new patents have surfaced since that expand upon the design in many ways, but I'm waiting for the last RDNA5 to be made public before making a potential follow up post.  But regardless even if they fix HW situation completely they'll prob fail spectacularly with SW stack as they've done so far with FSR Redstone and FSR4 game adoption. Even hear a lot of people complaining about having to use Optiscaler, even in newer games.   Also NVIDIA will no doubt move the needle a lot nextgen yet again. They already did with DLSS 4.5 and DFG and something tells me that DLSS5 is gonna be even worse for AMD. They better prepare for what's to come.  Worst case it's a complete massacre. I can see the following scenario happening:  **HW:** NVIDIA invests all their silicon budget into fixing 5090 scaling bottlenecks (16 GPCs instead of 12, revamped scheduling etc...), fixes other problems with 50 series (redesign cachemem mostly) + goes Brr on ML and to some extent RT. Raster goes up 35-40%, everything else goes up multiple times.   Worst case ML HW gets bumped to 4-8X NVFP4 rate, although 2-4X sounds more likely.  **SW:** NVIDIA uses this new insane ML HW to make new DLSS models. DLSS5 goes all in on NVFP4 and is faster than DLSS4.5. DLSS5 SR and RR for 50 series + 60 series which is lightweight and fast on new GPUs (high FPS), and a new DLSS ULTRA SR for 60 series (released across stack but painfully slow for anything pre 60 series) striving for maximum Image quality. The smaller model will be better than DLSS4.5 and the big model another tier entirely (DLSS3 -> 4 leap easily on top of DLSS4.5).   They also make DRS compatible with DLSS SR and RR so users get greater flexibility here similar to DFG for framegen.   FG will also release in two versions one light and heavy. Will also work with Reflex 2. It's possible only the big model will be frame extrapolation + limited to 60 series. Should make FG result in lower ms instead of higher + overall image quality far superior and basically all issues solved up to at least 4X.   Oh and a flood of MLPs and a demo showcasing the absurd visuals the 6090 can push. Moves goalpost past ReSTIR PT and will look borderline offline render quality. Very close to Blender renderers. IDK how they'll do it but MLPs are borderline magic, so prob doable.  Thinking about it more you're prob right and even if RDNA5 HW is amazing even beats 6090 in PT, a DLSS5 feature suite this impressive + moving goalpost to MLP based neural rendering will make RDNA5 irrelevant. As always SW and marketing will kill any momentum from HW side. Really hope I'm wrong but don't think so.  Sorry for the rambling.",hardware,2026-01-07 12:05:46,2
Intel,nxzkmr7,"That's a marketplace listing, it's basically eBay, because Newegg is out of 5090FEs directly.  You can get it on the overpriced StockX for far cheaper: https://stockx.com/nvidia-geforce-rtx-5090-32gb-graphics-card-900-1g144-2530-000",hardware,2026-01-06 11:04:15,6
Intel,ny12fxe,"That would make some sense if they *did* plan a Celestial launch, but that's a big ""if"" and is just creating confusion for now. And it'll be even worse when NVL mixes Xe3 and Xe3p.Â    You also have Intel marketing actively making the situation worse like that Peterson interview people keep quoting to justify this nonsense. As if Xe3p isn't much more incremental than Xe3.Â    It's a particular shame when the product itself is actually good.",hardware,2026-01-06 16:18:11,1
Intel,nxyb20u,"Yeah, the first 6 core i/u5 series since 11th gen. :/",hardware,2026-01-06 04:34:08,-1
Intel,nxyoap3,"Yeah idk why but they typically got OLEDs exclusively, though could be a US market thing. I would also note I was mostly looking at decently built midrange to high-end laptops. I think AMD is more common in the plastic crap box design and may be a better value there, but those also typically seem to have a ton of older rebadged processors instead of the newer Kraken Point unless something changed.",hardware,2026-01-06 06:10:02,2
Intel,nycu2b3,"I enjoy reading your optimism. I hope it all comes true, but it sounds a bit too good to be true given the recent hardware developements. The 5090 scaling issue is that we stopped resolution scaling. If you go beyond 4k the 5090 scales a lot. VR resolutions report the 5090 being as much as twice the framerates of 4090.",hardware,2026-01-08 06:52:50,1
Intel,nydd2fq,I've rewritten prev reply to provide more info.  I'll also link the scheduling patent here in case anyone reading this thread is interested: [https://patents.google.com/patent/US12153957B2](https://patents.google.com/patent/US12153957B2)   It sounds like gains in workgraphs scenarios will be be even greater.,hardware,2026-01-08 09:42:10,2
Intel,nyd5bcu,"Yeah prob not realistic. I just tried to outline a nightmare scenario for AMD. As for the RDNA5 stuff we'll see how good it ends up being.  Agreed serious issues fs. RTX 5090 scheduling is brain dead. 16 SM GPCs, one central scheduler for 170 CUs. The smaller the internal res the harder it is to keep things going. Someone smarter than me could prob make a core scaling efficiency chart for different resolutions clearly showcasing how RT > raster and derive different formulas for 1080p, 1440p, 4K etc... . There's simply no reason why it has to be this bad moving forward.       But it's also interesting to entertain that RDNA5 could be a nightmare for NVIDIA. If NVIDIA doesnâ€™t fix scheduling AMD's nextgen could be a real nightmare scenario for them. The modular and decentralized scheduling will be a gamechanger and based on what patents have said scaling is almost perfect and can scale to [arbitrarily large configurations](https://patents.google.com/patent/US12153957B2), yes they used that wording. AT0 will function like 8 x AT4 instead of running into massive scaling issues. In fact based on what the patent has said it might be even better. Consider each scaling domain with a local cache independent of the L2, where the global command processor only acts as a distributor of work, not an orchestrator. Gains will be observed across the stack but expecting IPC gains to scale with number of CUs. Is this a big deal for RT and 4K native? Yeah but even more so for lower res gaming.    And assuming they reduce CPU overhead even further in new uarch AMD will easily take the max FPS crown although I suspect NVIDIA can finally address their driver overhead issue after booting Maxwell-Pascal. Weâ€™ll see who comes out on top in CPU overhead nextgen.  I thought most of that gain vs 4090 was due to extra BW? But yeah high end perf scaling falls apart at sub 4K internal res.",hardware,2026-01-08 08:30:40,1
Intel,nyjq3tx,"I dont think much can be done with overhead. AMDs overhead is already small, basically letting the API go directly to GPU as it is. While for Nvidia side, isnt most of the overhead related to how Nvidia handles DX12? in that case i dont see it going away for a long time.",hardware,2026-01-09 05:42:46,2
Intel,nyfqcxo,"Interesting, GN usually gets Tom to do discussions like these but instead decided to publish whatever that previous video was on 'Intel pulling an Nvidia'. I bet GN will probably have their own video with Tom, but I appreciate DF a little bit more with this discussion.  At around 21min, it's interesting to hear his talk on cross-vender SR, mentions how they'd like to work more on Nvidia's Streamline and a candid talk about DirectSR and how it isn't really the concrete solution for the work on cross vendor SR. At around 23min, Alex brought up something interesting about research they've published before on joint denoiser and SR. He kinda skirts around it, but continues on suggesting they have more plans on it. He also then continues on the state of PT, DXR 1.2, obviously it isn't a real focus with something on their iGPUs, but any future HW, will be their primary goal to tackle. Alex mentions Valve/Linux, and Tom says it isn't entirely their focus right now, at least for gaming.",hardware,2026-01-08 17:45:35,42
Intel,nyhyq7q,"Super interesting that he randomly announces that Intel will be dropping a pre built shader program for Panther Lake. And not build with the new Microsoft framework/infrastructure, but just on their own?? How can Intel randomly drop this, but nvidia and amd canâ€™t??",hardware,2026-01-08 23:46:46,12
Intel,nyhbk2i,"Seems like we're not the only ones that think FG isn't ideal rn. I really hope Intel succeeds in their efforts to pair Framegen with reprojection, but it'll prob be NVIDIA that gets there first. Might be the killer app for 60 series, but pure speculation of course.  The stuff about using AI to smoothe frames is interesting as well.  Things prob gonna change a lot in the coming years. We'll see if it's for the better.",hardware,2026-01-08 21:55:47,18
Intel,nyfhyad,Are PC games becoming more stuttery or we're just paying more attention to it?,hardware,2026-01-08 17:08:37,27
Intel,nyfezgd,This will probably piss off MLID since he hates Tom Peterson,hardware,2026-01-08 16:55:39,12
Intel,nyikqbo,This future of gaming is ridiculous.  Aggressive upscaling (360p) and one-in-four frames actually rendered and the rest FG?   For what?  Path tracing?  Nanite?,hardware,2026-01-09 01:41:36,-5
Intel,nyhaj0j,You still watch GN? Dude only farms drama after realizing how much clicks they generate.,hardware,2026-01-08 21:51:23,42
Intel,nyhmk4k,Thank you for the summary!,hardware,2026-01-08 22:45:36,3
Intel,nyjni69,"He's talked about it before I believe in a previous interview, might have been with PC World from memory or perhaps GN. Regardless, it wasn't exactly new iirc. [Anyways this is definitely old news.](https://overclock3d.net/news/software/intel-plans-to-make-shader-stutter-a-thing-of-the-past-with-arc/)",hardware,2026-01-09 05:24:04,10
Intel,nyja51l,"What do you mean? The Shader delivery program was launched on an AMD handheld, so I presume they will use the MS advanced shader delivery infra.",hardware,2026-01-09 03:59:46,7
Intel,nyimpss,"Intel already talked about this a couple months ago. It's not an announcement here, but it seems people are more interested in AMD and Nvidia news so those threads don't get as much traction.",hardware,2026-01-09 01:52:08,7
Intel,nyflgv9,I feel like more people paying more attention since the marketâ€™s grown a lot.   I remember old games I played in the early 2000s having micro and regular stuttering depending on the game. I chalked it up to â€œhuh guess itâ€™s loading in data as I playâ€ when I didnâ€™t know much.,hardware,2026-01-08 17:24:06,45
Intel,nyfmkrk,We are paying more attention to it but I suspect as we push higher frames with new engines and techniques the micro stutter is getting worse. It just starts becoming less perceptible to people than say the micro stutter from SLI and other stuff that caused issues in the past.,hardware,2026-01-08 17:28:57,21
Intel,nyfuxga,"I think digital foundry answered that question on their podcast. Compared to 15 years ago the frame rate is higher on average for most gamers, but it also stutters more. So it's huge fps with huge drops and hangs.",hardware,2026-01-08 18:05:15,20
Intel,nyfrkdx,"Worth remembering no one even gave a shit about stuttering enough to measure it until someone at I think it was anandtech back in the late 00s was so fed up with shitty perf on his crossfire system he started doing 1% lows on benchmarks.   Back in the 90s people would run like 6x SLI voodoos and not even care that the game hitched every 10 seconds down to sub 30s fps lol...  Edit: it's also key to remember that a lot of the games that stutter on PC these days stutter in the exact same places for the exact same reasons, and worse considering the lack of CPU power, on consoles. Console gamers just don't give a fuck lol....   DF did a good video showing this truth with the Silent Hill 2 remake.",hardware,2026-01-08 17:50:48,30
Intel,nyg5v26,"With a lot of PS3 games dipping to 20s, I guess we are just paying more attention to it now",hardware,2026-01-08 18:52:01,9
Intel,nyj9uk1,There was DF Clip of this exact question.  https://www.youtube.com/watch?v=pxsfT4c-F-Q,hardware,2026-01-09 03:58:01,4
Intel,nyfihv5,They're more surgery stuttery.,hardware,2026-01-08 17:11:01,3
Intel,nyfksy4,Everyone hates MLID,hardware,2026-01-08 17:21:10,44
Intel,nyfg44c,Funny piece of lore. Why so?,hardware,2026-01-08 17:00:30,14
Intel,nykk9lw,Who's MLID?,hardware,2026-01-09 10:03:38,4
Intel,nyjx60j,If the end visuals are better who cares. we already did a lot of such things in engine just didnt tell the players about it. One in four frames actually rendering shadows is a thing for example. Heck some games go as bad as once a second shadow updates.,hardware,2026-01-09 06:38:04,17
Intel,nykocr4,"Well, I think that most people (myself included) don't really care how it's done if the end result is looking good and feel good to play, off course something like 30fps based with MFG to 120 is bs, but I quite regularly use 60 -> 120 using frame gen because It feels okay input wise to me at 60, and the added visual smoothness is quite nice. So it all depends how it's implemented and talked about.",hardware,2026-01-09 10:40:00,11
Intel,nykneij,"yeah i had to tap out a couple months back. it's a shame, they did great work, but i refuse to support ragebait.",hardware,2026-01-09 10:31:33,17
Intel,nyhh357,"Doesn't help that there isn't any particular new hardware to review or anything new besides AI, the PC industry has hit stagnation in the consumer market.",hardware,2026-01-08 22:20:10,26
Intel,nyickvj,"In other news, guy who is being served turds at restaurant loudly complains about receiving fat dooks instead of food.     *""man that guy is such a whiner""*",hardware,2026-01-09 00:58:06,0
Intel,nykpeuy,"No it didnâ€™t sound like they were going to use the MS shader delivery program. Or no, they said they want to, but they have their own solution that theyâ€™ll launch before that.   And I also donâ€™t think the MS solution is ready. The ROG ALLY XBOX also doesnâ€™t have this feature already as far as I know",hardware,2026-01-09 10:49:11,2
Intel,nykoz3l,"> The Shader delivery program was launched on an AMD handheld  i assume you're referring to the steamdeck, which to my understanding was done by valve... so the point kinda stands, you just add valve to the preamble.",hardware,2026-01-09 10:45:26,2
Intel,nyg0pvl,"I dunno I remember many games, especially based on Quake engine, being buttery smooth if you could get to reasonable FPS",hardware,2026-01-08 18:30:02,18
Intel,nyjwwd6,also more cause for stutering. Back then we could compile shaders real time with no siginficant issues because they were small. Now we have to compile shaders real time that are huge to the point where we pre-compile half of them before we even start the game.,hardware,2026-01-09 06:35:52,8
Intel,nyfy60k,Console gamers *of certain genres* care.   The FGC rejected the idea of playing Street Fighter IV & MvC3  on PS3 because the Xbox 360 port had better input latency.   Even the PS4 port of USFIV wasn't liked.     Part of why I stopped going to tournaments is because SFV & Tekken 7 on PS4 always felt *off* compared to PC.,hardware,2026-01-08 18:19:10,11
Intel,nyg711t,"Not really. In many cases, console versions of games just don't stutter whereas PC games do because modern games are mainly designed for consoles and then ported to PC.  A couple of good recent examples are Wukong and Outer Words 2. Neither of those games on consoles have the horrid stutters that are prevalent on PC.",hardware,2026-01-08 18:56:57,10
Intel,nymcty0,MLID more like MID,hardware,2026-01-09 16:26:52,2
Intel,nyfkp1n,Heâ€™s been calling Tom a â€œsnake oil salesmanâ€ for performance claims on Alchemist,hardware,2026-01-08 17:20:43,12
Intel,nzdp1mq,"Moore's Law is dead, he's a YouTuber who makes predictions and purports to have insider information from Nvidia/Intel/AMD but in reality he is better described as FanFiction for PC enthusiasts.",hardware,2026-01-13 16:50:02,1
Intel,nym0alm,"I worry about things like latency.  Or what happens when you move suddenly, or fire, and the whole image falls apart.  If the hardware can't do it all yet, then wait a few years instead of using these... methods.  And just to be clear, I think very highly of  Mr. Tom 'TAP' Petersen; I'm just not liking where this is all heading.  Are you happy with Borderlands 4 or Outer Worlds 2?  If you are then we live in two different worlds.",hardware,2026-01-09 15:30:18,-2
Intel,nyjn1ql,"There's plenty of old stuff he needs to review. I've said it before but I will say it again and beat it like a drum: He STILL has not reviewed the 9060 XT 8GB for instance. Plenty of content he could have farmed off that, especially BEFORE the RAM shortage where having an 8GB card was some sort of sin in his eyes (except for some reason he ignored it but raile roaded the cheaper RTX 5050, lol did someone say bias?). Instead, he just tore the 9060 XT 8GB down and never touched it again after that teardown. I'm sorry but there's GENUINELY stuff he could be doing instead of farming clicks about 'NVIDIA bad' and 'Intel pathetic', but that won't get him views from drama farming NVIDIA which is what he craves these days. So sad to see a big channel like his who got big off doing solid technical content become a drama-hype ""news"" channel.",hardware,2026-01-09 05:20:56,25
Intel,nyk71tg,No wonder that strategy has proved successful. There are a lot of people that only care about complaining and bitching. Which I get up to a point but it starts to feel childish and pathetic quickly.,hardware,2026-01-09 08:02:47,20
Intel,nysczdh,"I love how reddit down votes you,  reddit posts all day complaining about the industry, GN does a video on it   Redditors ""whiney cry baby engagement farmers """,hardware,2026-01-10 14:05:38,-2
Intel,nyogfa3,"Valve does have one on Steam, but Microsoft announced a store agnostic, eventually hardware vendor agnostic one launched with the Asus ROG Xbox Ally X last year.  It was supposed to be working already but there's no sign of it just like everything else Microsoft releases about gaming half-baked like their gaming UI, their attempt to unify upscaling, and Directstorage.   So I imagine they're referring to what Microsoft called ""Advanced Shader Delivery"" that they've done little with but name and announce to sell more Asus Pretend-Xbox's.",hardware,2026-01-09 22:10:03,3
Intel,nyoi6ny,Iâ€™m talking about the Xbox Ally as the other commented said,hardware,2026-01-09 22:18:25,1
Intel,nygmjs6,We considered solid 60 or 75 smooth back then. Now at least I complain as soon as I can't stay above 100.,hardware,2026-01-08 20:05:12,15
Intel,nyhs8ie,"Shader compilation stutter is a PC problem thatâ€™s been especially bad in UE4 and 5 since the transition to DX12. Other types of stutter and bad frame rates used to be equally bad on consoles, or even worse in the 360/PS3 era.",hardware,2026-01-08 23:13:28,10
Intel,nygf3pi,Outer Worlds 2 is fairly consistent for a UE5 game. To me its just very heavy,hardware,2026-01-08 19:32:17,4
Intel,nygrcy6,"funny coming from MLID, notorious snake oil salesman",hardware,2026-01-08 20:26:46,47
Intel,nyflbj9,"Eeeegh, considering MLID overall story - can see that, can see that.  OTOH usage of such terminology - snake oil salesman - reminds me of that very infamous Intel presentation...",hardware,2026-01-08 17:23:27,15
Intel,nymda4b,You say that as if we will reduce our base frame rate. But the direction of travel is to use MFG to drive the 480Hz and above monitors that will become more and more common in the future.,hardware,2026-01-09 16:28:52,6
Intel,nyqwqj9,I dont like borderlands franchise and i havent played Outer Worlds 2 yet so i cannot comment on them from personal experience. However things like Reflex/Reflex2 has actually decreased latency for me.,hardware,2026-01-10 06:52:45,1
Intel,nylgvk7,"I havent really seen it as some sort of drama farm, I think a lot of GN's videos do show how bad the current computer hardware hobby is doing and Im glad someone is focusing on that",hardware,2026-01-09 13:55:07,1
Intel,nysckbg,Whata wrong with calling companies out? Like redditors cry all day but stop at GN when the channel brings up issues within an industry.  Sorry a person isn't running their channel the way the reddit collective wants.,hardware,2026-01-10 14:03:14,-2
Intel,nysv24y,"honestly, i had completely forgotten that was a thing.   i wish i had more faith in microsoft to pull this off, but i remain skeptical in basically anything they try until proven otherwise.",hardware,2026-01-10 15:44:02,1
Intel,nyguz4e,"People on gaming forums used to go around insisting the human eye could not distinguish greater than 60fps.  It was accepted as gospel in many places/by many people.  Granted, that was in the age of CRTs and almost no one had actually seen >60, but funny to think back on.",hardware,2026-01-08 20:43:03,12
Intel,nyjx1ak,snake oil salesmen dont like competition.,hardware,2026-01-09 06:37:00,9
Intel,nyjz26x,"MLID aka ""RTX 50 super will be released november 2025""",hardware,2026-01-09 06:53:37,5
Intel,nyknw8m,this just makes me like tom more,hardware,2026-01-09 10:35:57,5
Intel,nyjwmn8,"Replace that 60 with 30. Yes, people insisted we could not see more than 30 even while playing on 85hz CRTs.",hardware,2026-01-09 06:33:40,9
Intel,nyhcljr,"Meanwhile quake players kept dropping resolution to hit triple digit refresh rates :) But yeah, it's wild that 30 was the norm for so long on console",hardware,2026-01-08 22:00:09,6
Intel,nyko1gh,That's far from the worst prediction he's missed.  Remember SMT4 for future Zen architectures? 24 and 32 cores on desktop Zen4? L4 cache for Zen 4?,hardware,2026-01-09 10:37:14,12
Intel,nyrvn0z,Which Tom?,hardware,2026-01-10 12:08:40,1
Intel,nyjbd32,"console used to lock at 60, dont know why they choose to drop down to 30.",hardware,2026-01-09 04:06:56,5
Intel,nyjivip,"Because when the hardware cant handle all the sprites, it slows everything down including game logic which was tied to frame rate back then. It happens in plenty of hardware from snes to arcade cabinet.",hardware,2026-01-09 04:52:53,8
Intel,nyjwrq4,"also happened the other way round, older games would run faster than they should because they tied it to framerate rather than delta time. This bad practice was so common you can still find studios like bethesda do this.",hardware,2026-01-09 06:34:49,5
Intel,nz6q876,"for context, this G14 was the one that gave us [the first ever Geekbench scores for Panther Lake](https://www.reddit.com/r/hardware/comments/1ocaslx/panther_lake_geekbench_leak_its_good/) over on r/hardware",hardware,2026-01-12 16:33:10,39
Intel,nz6w6zw,"The price probably wasn't going to be low enough compared with other SKUs.  In the past, Intel skuss with Iris HD etc were so expensive and were only included in $2000 ultrabooks. This is probably going to be same.",hardware,2026-01-12 17:00:15,19
Intel,nz73dw5,When I first learned about I was confused why it existed. Not surprised Asus decided to cancel it.,hardware,2026-01-12 17:33:23,11
Intel,nz6r3wa,"It makes sense, a B390 only G14 kind of defeats the whole purpose of the Zephyrus, even as a base model.     Basically brings the GPU performance down to 3060 levels (actually a little worse than that), which is roughly 4 years ago at this point.",hardware,2026-01-12 16:37:10,38
Intel,nz6q33b,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-12 16:32:31,1
Intel,nzujefe,"Ugh this would have been such a cool business laptop option, I'm finding current-gen integrated graphics to not be enough to really handle Teams + external monitor + a presentation. Sounds like this would have hit the performance mark without having to pay through the nose.",hardware,2026-01-16 02:30:35,1
Intel,nz770nj,Itâ€™s also the one that didnâ€™t have a dedicated GPU in Geekbench,hardware,2026-01-12 17:49:53,15
Intel,nzcyo9s,"I don't think so, they're making it available on more than just the top 9 series, they also confirmed a custom chip for gaming handhelds which we'll prolly see at computex",hardware,2026-01-13 14:45:59,4
Intel,nz7invn,It probably made more sense as an low-end gaming Zephyrus when fitting it with 32GB LPDDR5x wasn't half the BOM,hardware,2026-01-12 18:42:14,18
Intel,nz7xzli,esp since on CES 2026 they launched an even more premium 14 inch laptop that actually does have the B390 (ExpertBook Ultra),hardware,2026-01-12 19:52:01,6
Intel,nz6vvy1,esp since Asus ended up launching the ExpertBook Ultra at CES  an even more premium 14 inch laptop compared to the G14 which does have a B390,hardware,2026-01-12 16:58:51,10
Intel,nz6ryqs,"But it actually can be used on battery unlike normal laptop gpus which make your laptop die in 5 seconds or performance is awful and it dies in 10 seconds, so what's the point of having a powerful GPU on a laptop if I got to plug in all the time it sucks.",hardware,2026-01-12 16:41:01,29
Intel,nz6tste,"A 3060 is still useable imo. If the efficiency is right, they can make usb-c powered gaming laptop and completely remove the dumb dc brick.",hardware,2026-01-12 16:49:18,17
Intel,nzcpz2s,"It would have made _some_ sense, but the Arc B390 mustâ€™ve been real good. But as you say, with the raise in RAM prices, you can forget about it.  I wonder whether Intel themselves would pair a mid-range Panther Lake with a top-notch iGPUâ€¦ It would make sense, since the high-end CPUs tend to have dGPUs.",hardware,2026-01-13 14:00:36,1
Intel,nz6tn0v,"For the Zephyrus you can also just disable the dGPU to game only on the iGPU, and the customers of the Zephyrus are minimum looking for a decently powerful dGPU",hardware,2026-01-12 16:48:35,19
Intel,nz6snj4,I hope their plugged out power scheme is permanent and doesn't vary based on application,hardware,2026-01-12 16:44:08,3
Intel,nzaau06,"> so what's the point of having a powerful GPU on a laptop if I got to plug in all the time  I have a laptop for travelling. I travel to places that have plugs, and I don't need to use a laptop while I am actually in transit.",hardware,2026-01-13 03:03:12,1
Intel,nz8xpz8,"It still pulls 60W (and apparently 80W, briefly) at full pelt...  That is to say an hour and a half battery life when gaming should be expected. It's not magic.",hardware,2026-01-12 22:40:05,0
Intel,nz71x4x,Usb c powered gaming laptops are more common now. The new ideapad 5 pro with panther lake and 5060 (combined 110w system power) uses usb c exclusively,hardware,2026-01-12 17:26:41,11
Intel,nz753k0,"> A 3060 is still useable   Nvidia seems to agree with you, which is why they are looking to put them back into production.  /I'm sorry, I couldn't resist.",hardware,2026-01-12 17:41:14,11
Intel,nz6wes7,"Useable, yes, but what customers of the Zephyrus are looking for, no.",hardware,2026-01-12 17:01:16,12
Intel,nz6vzj4,"Yeah, and it gets pretty good battery life on that and in power saving mode/60fps screen mode. Like 8 hours. Best I've had on a gaming laptop- not a high bar, but it's nice to be able to take it into the living room and use it light a normal laptop instead of it dying instantly.",hardware,2026-01-12 16:59:19,7
Intel,nz920o1,Still better than 45 minutes  Also it can obviously be put in a lower power mode to save battery while not hurting performance too drastically.,hardware,2026-01-12 23:01:47,2
Intel,nz6x4bl,"The G14 used to have gtx1650, so not sure what do you mean.",hardware,2026-01-12 17:04:35,0
Intel,nz724pz,Which was the 5050 of that time which is significantly more powerful than panther lake's igpu.,hardware,2026-01-12 17:27:38,9
Intel,ntyjuf7,Why does this need an article? It's a tweet by an official account praising their own product.,hardware,2025-12-14 10:28:41,118
Intel,ntynem1,"The B580 has 200W TDP, in a perfect world and TDP scales linearly, the B770 would be 50% faster, that would put it around the 5060Ti/9060XT.  If the price also scales linearly, that would be around 375â‚¬, seeing that the 9060XT is going for 350â‚¬ now, it's gonna be tough competition.",hardware,2025-12-14 11:02:45,44
Intel,ntypvez,Im really looking forward to panther lake X. 4-4-4 core configuration and Xe3 iGPU with sr-iov is perfect for running a Linux-Windows mixed vm environment without having to get a gaming laptop with a dedicated GPU for virtualisation.,hardware,2025-12-14 11:26:24,7
Intel,ntyixk6,I hope the Linux driver support and performance is good in these,hardware,2025-12-14 10:19:48,17
Intel,nu8l0vn,"Intel ARC needs to maintain their momentum. They have an excellent pricing strategy and genuinely compelling features, it's time they released a card that competes in the midrange. And no, I don't count the A770. As a B580 owner, increased ARC adoption rates will be sure to benefit all cards in the range, so I really hope that intel is committed for the long-haul here. They are not in the position to be burning consumers anymore",hardware,2025-12-15 22:51:09,4
Intel,nu1xwz6,"Releasing a GPU more than 1 year after the B580 came out seems weird to me. Unless this is a new architecture, or is using Intel's own process, and fabs.",hardware,2025-12-14 22:08:28,6
Intel,ntyjaqa,"4070 performance for $350-400, I'm calling it now.",hardware,2025-12-14 10:23:22,11
Intel,ntyxo3a,Hopefully they've seen Nvidia and AMD fuck things up by having two VRAM configurations and know not to do that.,hardware,2025-12-14 12:35:17,6
Intel,ntyfbh6,"Hello Revolutionary_Pain56! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-14 09:44:11,1
Intel,nu28x8z,They wouldnâ€™t need a B770 or mystery GPU if they actually released more than just a B50 to the masses.,hardware,2025-12-14 23:06:54,1
Intel,ntzwyfc,"I don't know what the driver situation is like a year later, but B580 was anywhere between a 4060ti and a 3060 (or less if the driver really choked), so comparing B770 to a single Nvidia point of reference probably isn't the whole story.  Intel has been selling a big chip with a lot of hardware relative to what they charge, so when the drivers work Battlemage can punch way above its price class. I expect the same this time.",hardware,2025-12-14 16:10:14,1
Intel,nu3z9nt,"It's been deleted, so it might even be inaccurate.",hardware,2025-12-15 05:31:41,8
Intel,nu1v2ky,Ad revenue.,hardware,2025-12-14 21:53:55,8
Intel,ntzk2x5,Trying to apply logic or rules to the internet is a waste of time.,hardware,2025-12-14 15:02:28,17
Intel,ntyxt68,"Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds. That however will mean a bigger die and viability might be questionable (considering they're already massive for the performance).",hardware,2025-12-14 12:36:26,30
Intel,ntzjh8i,"Price doesn't scale linearly because die sizes make defects scale quadratically. so pricing is the same, 2 50mm\^2 dies are cheaper than 1 100mm\^2 die     However in GPUs there is a fixed cost for every GPU so there is a sweet spot",hardware,2025-12-14 14:59:01,14
Intel,ntz8bta,"As always, TDP is a semi-arbitrary figure and has little to do with what the GPU requires.  Most GPU's of today have heavily inflated TDP's simply to try and juice benchmarks on review day as much as possible.",hardware,2025-12-14 13:51:23,3
Intel,nu40836,"The BMG-G31 is supposed to have 32 Xe cores in 8 render slices on a 256-bit memory bus, compared to the 20 Xe cores and 5 render slices on a 192-bit memory bus for the BMG-G21. Unless Battlemage is seriously memory bandwidth-limited, it should be almost 50% more performant.  The only question is die size. If it's 50% larger than the 270 mm^2 BMG-G21, that would exceed 400 mm^2. The GB203 in the RTX 5080 is 378 mm^2 for context.",hardware,2025-12-15 05:39:14,2
Intel,ntywdzf,With tdp of 300 w it better be RTx 5070 or 9070 territory for much low price,hardware,2025-12-14 12:24:39,-1
Intel,nvdrawd,Intel never confirmed SR-IOV on Panther Lake - did they?,hardware,2025-12-22 15:23:42,1
Intel,nu14qjw,"You can choose between high performance and crashes (xe) or low performance and stable (i915), and with Intel firing linux devs left and right I wouldn't expect much improvement any time soon.",hardware,2025-12-14 19:45:02,11
Intel,ntz5xy4,That would be an amazing value proposition.,hardware,2025-12-14 13:35:49,4
Intel,ntypyrq,Rtx 5070 16gb for 380$,hardware,2025-12-14 11:27:18,-1
Intel,nu2970r,Iâ€™d be happy if they didnâ€™t gate the Arc Pro B60 behind bad distributors.,hardware,2025-12-14 23:08:26,3
Intel,ntz0y7e,"So banking on the hope, that *everyone* ***else*** *somehow falls behind by accident*, only for Intel to succeed?  If that's their business-plan (looking at their foundry-woes, it seems it is), that's an awfully idiotic business-model.  ---- Last thing I heard, was redditors moaning about en masse that monopolies are bad. *Which one is it?!*",hardware,2025-12-14 13:00:48,-21
Intel,nu8khh7,"The B50 is not a gaming GPU and actually underperforms in gaming tasks compared the the B580. They need to have an actual range of cards, not just a budget option, and even more budget option, and a server/workstation GPU. The B770 is essential to compete in the midrange",hardware,2025-12-15 22:48:12,1
Intel,nu8kn1x,"A year later the drivers are fantastic, seriously not even a single hiccup. Been playing Hogwarts legacy at 4k 60fps with Xess Quality upscaling, and no frame gen.",hardware,2025-12-15 22:49:02,1
Intel,ntyyf0i,"> Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds.   The problem with this idea, is that this would cost them far more money, as you need more die space, which they already use relatively inefficiently compared to nVidia.  They can't really afford not to use every bit of die space they have for all that its worth.",hardware,2025-12-14 12:41:19,21
Intel,nu40pti,"Battlemage doesn't have the ability to add more Xe cores per render slice, this is something Intel has changed for Xe3. The BMG-G31 will have 128 ROPs, the same as an RX 9070 XT, or more than an RTX 5080.",hardware,2025-12-15 05:43:07,2
Intel,nvdwqwo,"afaik it works on every iGPU since skylake, but the driver is not in the mainline kernel",hardware,2025-12-22 15:51:22,1
Intel,nu2hk6w,I'm using an Arc A770 right now in Linux.  With i915 performance was unusably (for me) low.  With xe it's been fine.,hardware,2025-12-14 23:54:28,5
Intel,nu2tqb3,"the driver is already open source right? i think it will get better over time on virtue of being open source, but relying on intel to fix it now probably isnt gonna pan out.",hardware,2025-12-15 01:01:50,0
Intel,ntz7z9x,"Well it kinda has to be, the 4070 came out nearly three years ago.",hardware,2025-12-14 13:49:08,24
Intel,nu096mb,5060 performance for twice the price isn't a good deal.,hardware,2025-12-14 17:11:40,-4
Intel,ntz10sx,"I could see that. Nvidia really bailed out Intel by making the 5070 not much faster than the 4070 without using MFG to cheat lol   Edit: for all the Nvidiots downvoting, [the truth hurts](https://www.techspot.com/review/2960-nvidia-geforce-rtx-5070/#RT-1440p-png)",hardware,2025-12-14 13:01:20,4
Intel,ntz7opy,This seems an absurd overreaction. All I'm saying is they don't do a 5060ti or 9060xt situation where there's a 8 gig model and a 16 gig model.,hardware,2025-12-14 13:47:16,17
Intel,nubvjrs,"Itâ€™s not but the B50 is the only Arc Pro that isnâ€™t gated behind a bad vendor like Hydratech.    If they canâ€™t properly launch the B60, why should I trust Intel or itâ€™s partners with the B770 or some mystery GPU?",hardware,2025-12-16 13:27:34,1
Intel,nu1491m,5060 is not nearly as performant as the 4070,hardware,2025-12-14 19:42:34,19
Intel,nt8w4et,"Interesting results. If this is representative for consumer laptops, Panther Lake is a much bigger upgrade than most here, including me, expected. But it almost seems too good to be true somehow.",hardware,2025-12-10 06:01:38,24
Intel,nt7pdwj,is Geekbench a CPU or a GPU benchmark?,hardware,2025-12-10 01:17:56,13
Intel,nt7pbu4,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-12-10 01:17:35,1
Intel,ntafnry,How does this compare to the Snapdragon X2 Elite?,hardware,2025-12-10 13:56:04,1
Intel,nuahcx5,4 pcores  8ecores 4 lpcores..,hardware,2025-12-16 06:05:52,1
Intel,ntcj3az,"Geekbenchâ€™s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intelâ€™s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:16:10,1
Intel,ntbju2x,"Is Intel just ""squeezing the toothpaste"" again ? Even a low-frequency single-core 288V gets 2,700+ on Geekbench, while the 285H gets 2,600+ in single-core and 14,785 on multi-core. Therefore, TL;DR: I don't see Panther Lake being a huge improvement over the current Alder/Arrow Lake pairing. We will have to wait and see the power consumption, though.",hardware,2025-12-10 17:23:27,1
Intel,nte6d5x,"Iâ€™m sorry, but thatâ€™s awful? Only 9% better single core when it has a better node and a newer architecture? Compared to what Apple and Qualcomm achieve every year, thatâ€™s pathetic",hardware,2025-12-11 01:36:38,0
Intel,nt8wwpg,"Probably because GeekBench 6 only scales to a certain point, where more cores wonâ€™t help with improving performance compared to improving core IPC",hardware,2025-12-10 06:08:21,8
Intel,nt9ghvf,"Fr, I really need to get a new light laptop (bc my old one's hinge is broken), but starting to feel  like I'd be better off waiting for Panther Lake than compromising with a bulky gaming laptop....",hardware,2025-12-10 09:13:58,2
Intel,ntcj6hz,"Geekbenchâ€™s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intelâ€™s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:16:37,-2
Intel,nt7pvq6,cpu,hardware,2025-12-10 01:20:55,14
Intel,nt7t1it,"Probably one of the worst benchmarks out there for multicore tbh, Iâ€™d be more curious about the cb r24 scores",hardware,2025-12-10 01:40:02,16
Intel,ntdv8jz,"Panther Lake doesn't bring any major changes to the cores. It's mainly about bringing node shrink, redesigned SoC, and new iGPU.",hardware,2025-12-11 00:29:31,1
Intel,ntcpemm,"Itâ€™s obvious that this is the viewpoint of an outsider. Professionals would never look at it this way. Professionals first evaluate a processor based on its specifications, features, and process technology. Lunar Lake and Arrow Lake are *not* using some outdated process â€” they use TSMCâ€™s then-most-advanced N3B node, Intelâ€™s first time adopting it. Meanwhile, Panther Lake uses Intelâ€™s own **18A** process.  Based on the current benchmark results, Intelâ€™s 18A appears to outperform TSMCâ€™s N3B by at least the same margin that **Intel 4** trailed behind N3B â€” which is an astonishing result.  Every day you hear people saying how much TSMC has advanced, how far ahead its processes are, how â€œoutdatedâ€ Intelâ€™s nodes are, how AMDâ€™s processors using TSMC have excellent efficiency. These kinds of statements have been repeated endlessly over the past decade.  Yet today, Intel is using its newest process node to **clearly surpass** TSMCâ€™s top process from just one year ago.",hardware,2025-12-10 20:47:39,-1
Intel,ntep84w,"the real test will be how many watts the X9 388H needs to achieve its scores, because the 285HX needed like 90 watts to achieve its scores  so if the X9 could hit its scores while on its base TDP (65 watts) then thats a \~40% increase in efficiency, not bad",hardware,2025-12-11 03:32:35,4
Intel,ntfpawi,"But it does that while clocked almost 6% lower, so the IPC gain is actually decent. Especially considering most people expected Panther Lake to be a side grade because of the small architectural changes on the cores.",hardware,2025-12-11 08:27:12,2
Intel,nthte2d,"For the use cases of PTL, Geekbench (which is mostly consumer focused) is a good indicator.  It doesn't assume that its workloads are perfectly parallel, it assumes some threads are used more heavily than others, so its value in nT is influenced by its 1T.    If someone is using this for rendering or other highly parallelizable workloads they might want to look into a subtest or into an alternative benchmark, but for typical consumers it seems like Geekbench is a good approximation of their experience.",hardware,2025-12-11 17:03:33,2
Intel,ntckb4u,"Geekbenchâ€™s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intelâ€™s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",hardware,2025-12-10 20:22:19,1
Intel,ntgubsm,"Is macOS not an option? Because IMO, MacBook Air is *the* thin and light laptop to get, hands down.",hardware,2025-12-11 14:03:57,1
Intel,ntfhexr,"Couple of subtests leverage some new arm vector instructions and get huge scores, but those have limited influence to the overall score. Apple is better across the board, though the difference isnâ€™t as big as the overall score suggests.   One difference is that since geekbench is distributed as binary itâ€™s compiled more directly for apple architectures specifically while others use more generic targets. But that has very limited effect.",hardware,2025-12-11 07:10:24,2
Intel,nt7rc6b,But they have a gpu compute test too,hardware,2025-12-10 01:29:45,14
Intel,nt8452s,"Geekbench claims it's much more realistic than those multicore tests that scale nearly perfectly with tons of cores, and I think that's a fair take. It's not as if they didn't know how to create a benchmark that scales like other nT tests do, geekbench 5 nT does that.   I wouldn't call it worse, just different.",hardware,2025-12-10 02:45:40,29
Intel,ntfm9jb,Geekbench runs common workloads as they are commonly implemented. It gives you a score on how well a multi core implementation of that workload would actually run in that CPU.   I think that is far more useful than some perfectly parallel workload measuring max power and core count.,hardware,2025-12-11 07:57:01,5
Intel,ntco5ms,"I have carefully compared the various models across Geekbench, PassMark, and the differences between Meteor Lake, Arrow Lake, and Lunar Lake. If my judgment is correct, the theoretical peak performance of the 484 in Cinebench R23 should reach around **24,500**; the 285H scores **22,500**. Compared with the 285H, it should be easier for the 484 to achieve high scores because its power requirements are significantly lower than the previous generation built on TSMC N3B.  Its peak performance will not be extremely strong because the frequency is not high. IPC is likely improved by around **10â€“11%**, but clock speeds drop by about **6%**. Overall, that means single-core performance should only rise by **4â€“5%**.  The improvement will be most noticeable in Geekbench. Since PassMark single-core also shows gains, the IPC uplift and resulting single-core increase should be quite certain. If Geekbench were the only source, it would still be questionable, but PassMark is more solid and has higher reference value.  Overall, in terms of peak performance, the uplift is averageâ€”around **10%**, close to that figure.  However, the real key is the **efficiency gains**. I believe they will be excellent. Compared with the 285H, which requires **65 W** to reach **20,000** points in Cinebench R23, I estimate that the **388H** may only need **40â€“45 W**.  I also estimate that the Cinebench R24 score should fall around **1300â€“1400**. Compared with Qualcommâ€™s X Elite 2 at **1950**, there is still a significant gapâ€”but the two products differ drastically in scale.  Overall, Panther Lakeâ€™s greatest achievements lie in several aspects:  1. **Energy efficiency** â€” likely the best among all x86 products. 2. **Performance per mmÂ²** â€” excellent. For example, the 484: if you look at its die shot, the total area of the CPU (including the CPU tileâ€™s 4P and 4 LPE cores and all caches) is essentially equal to the die area of a traditional monolithic 8-core design. That means the 484 uses the same silicon resources as past 8-core chips, yet **no AMD mobile 8-core processor surpasses it**, either in raw performance or efficiency. 3. It also offers better performance-per-area than Qualcommâ€™s processors. The X Elite 2 has **18 cores**, including **12 â€œvery largeâ€ cores**â€”similar in size to Intel P-coresâ€”and **6 large cores**, each larger than Intelâ€™s E-cores. The die area of this chip is **2.5Ã— larger** than Panther Lake 484â€™s.",hardware,2025-12-10 20:41:27,-4
Intel,nthu15b,"I mean, it does bring SOME changes to the cores, both are a next generation, it just isn't a more radical change like will be happening with NVL.  A mid single digit improvement is still pretty decent.",hardware,2025-12-11 17:06:47,2
Intel,ntd3lso,">The benchmark is very friendly to ARM and least favorable to AMD.Â   How so?   >The only valid reference isÂ **same-generation, same-architecture comparisons**,  Geekbench is nice because it explicitly allows cross ISA comparisons. You don't have to take my word on it either, Intel and AMD themselves have used geekbench before to compare themselves to the ARM competition.   Same thing applies to spec and cinebench 2024.",hardware,2025-12-10 21:57:00,12
Intel,ntfgs1k,"What's with the Cinebench fascination? At any rate. Geekbench 6 runs a raytracing test, and the 388H leak shows it at 29700 points compared to a 285H scoring 25300 points. That would place the Cinebench R23 scores at about 20% higher for the 388H  https://browser.geekbench.com/v6/cpu/15500755  https://browser.geekbench.com/v6/cpu/15474224  At any rate, the reason Geekbench doesn't scale perfectly with more threads is because a lot of workloads hit scaling limits due to Amdahl's Law, or memory bandwidth limitations. This applies to SPECint and SPECfp results for multiple threads as well.",hardware,2025-12-11 07:04:32,2
Intel,ntsl9xc,My old 14900hx gets 35k multi core in cinebench r23,hardware,2025-12-13 10:15:38,1
Intel,ny53e0z,better go for cb2024 cuz r23 is being less relevant these days,hardware,2026-01-07 04:09:43,1
Intel,ntgxa5r,"lemme know once UTAU and Fighter Maker 2002 works on Arm macOS    (my point is that I work with a lot of old abandonware apps that barely even run on x86, so there's no chance in hell they gonna work on macOS)",hardware,2025-12-11 14:20:52,2
Intel,nt9ouc7,"Thus useless to compare high CPU core counts.  If you actually need more than 8 cores you also have workloads that scale much better than Geekbench 6. It's especially dumb to claim this CPU is close to a 16 core, 32 thread zen 5 cpu based on Geekbench...",hardware,2025-12-10 10:37:37,3
Intel,nvdnrp5,I have workloads that scale fine with 16 threads and would scale fine with 32. People who actually buy high end multicore CPUs have a use for them.,hardware,2025-12-22 15:05:12,1
Intel,nt8fz34,It runs for far too short a time to reflect accurate multi-core performance.  People don't get a multitude of cores to run a task for a few seconds.  They do it for tasks that take minutes or hours to complete.  I'd argue it spends too little time on single-core tests as well.  I don't trust it to provide any useful information about anything other than transient performance.,hardware,2025-12-10 04:00:09,-1
Intel,nt8esdi,I agree but the problem is it's being mindlessly used to compare MT scores as in this article.,hardware,2025-12-10 03:52:12,1
Intel,ntd4133,Bro there's no need to spam this same comment like 4x in the same post's comment section T-T,hardware,2025-12-10 21:59:01,5
Intel,ntduxlv,"Why's the scaling ""problematic""? Its nT scaling is by design because GB6 is trying to replicate common consumer workloads which are rarely embarrassingly parallel. If you wanna see how well nT scaling for rendering is, there's cinebench for that.",hardware,2025-12-11 00:27:43,5
Intel,ntwk3p9,"was your 35,000 score achieved with power consumption above 100W? Can you try it now at 80W and see how many points are left? Also, limit it to 40W and check if it can reach 20,000 points. Because I estimate that the 388H has a chance to hit 20,000 points at 40W.",hardware,2025-12-14 00:54:40,2
Intel,ntmv22s,"Ok, a simple no would have been fine.   Seems like those extremely old apps would run on any old POS x86 machine, if anything harder to run on modern hardware but hey what do I know. Best of luck.",hardware,2025-12-12 12:29:41,1
Intel,nt8x38v,"I mean, shortness is more of a problem if a device can cool itself properly or not rather than a problem of the CPU itself, unless said CPU in question is impossible to cool in that form factor",hardware,2025-12-10 06:09:56,12
Intel,nt9ufxf,Geekbench correlates with SPEC really well while taking a fraction of the time to run. Making it run for more minutes changes nothing,hardware,2025-12-10 11:28:32,4
Intel,nt8wp7g,"It provides useful information about the chip itself to real computer architecture enjoyers. Idk if gb6 changed it but geekbench has historically correlated with spec scores. Longer running programs like cinebench test the whole system including the thermal solution but geekbench gives a much better view into the pure performance of the cpu itself (and the associated memory system :/). Besides, you can always slap on a bigger cooler if thermals are that limiting.",hardware,2025-12-10 06:06:35,9
Intel,nt8mbai,"It's being used for comparison because that's what we have. AFAIK, this is the *only* 388H benchmark we have",hardware,2025-12-10 04:44:37,9
Intel,ntgnaao,That looks like an AI post to me,hardware,2025-12-11 13:22:24,2
Intel,nt9re78,It has been going hayway since SME just like GB5 had issues with AES Skewing results,hardware,2025-12-10 11:01:14,1
Intel,nt92449,Fair enough but I'd rather they kept something similar to GB5 multicore test in addition to their new 'more realistic' one.,hardware,2025-12-10 06:54:12,-2
Intel,nvfvk61,Incredible hardware news. Thanks for the share.,hardware,2025-12-22 21:50:44,19
Intel,nvl1r29,"This was sarcasm, by the way. A video from Usagi Electric on how computers count isn't hardware related but this is? OK mods.",hardware,2025-12-23 18:20:13,12
Intel,nu6hk69,"Frankly, I wouldn't buy one for gaming, though I must admit Battlemage is pretty sweet for video editors thanks to 10-bit AV1 and 4:4:4 chroma on the HEVC side + you also get two codec engines (at least on the B580 with the same G21 core).  For perspective, you'll have to move up to Nvidia GB203 (RTX5070Ti), or better, to get your hands on two or more NVENC engines for the same 10-bit AV1 + 4:4:4 H.265.  If I was a serious video editor, this is *the* graphics card I would get.",hardware,2025-12-15 16:37:33,46
Intel,nu6rlfl,The main bit that intrigues me about these ARC GPUs is their Linux gaming performance & how they compare to their windows performance.,hardware,2025-12-15 17:26:31,14
Intel,nue205x,"One interesting data point is he's testing with 7500f. We have no comparison with contemporaries or higher end CPU to examine CPU bottleneck, but it's a realistic scenario and system for the card.  Interesting how that 1gig made all the difference in TLOU2",hardware,2025-12-16 20:01:26,4
Intel,nu7ksue,Not in the same system the 1080 ti was in.,hardware,2025-12-15 19:47:27,7
Intel,nu6oprg,"idk why, intel gpu is so expensive in my country like bruh that gpu perform worse than cheaper nvidia/amd. those sucker trying to scam buyer just cause ""intel"" name in it.",hardware,2025-12-15 17:12:19,1
Intel,nu6jvho,Yeah intel's quick sync is very good at video editing and streaming as well. Even preferred over nvenc in streaming (no idea about video editing),hardware,2025-12-15 16:48:43,15
Intel,nuacl0f,"I would and I did (Intel B50 gpu).  So far, zero regrets and zero issues on linux.  Edit: Fedora for those that are curious.",hardware,2025-12-16 05:26:38,14
Intel,nugufgj,Rumour has it Linus torvalds uses an Intel card because he wanted something on a budget that could drive dual 6k screens.,hardware,2025-12-17 05:53:53,2
Intel,nu8m2qq,"For desktop use Intel on Linux is great, but gaming performance and compatibility is horrifically bad.",hardware,2025-12-15 22:57:03,0
Intel,nwpbxq3,Iceberg's intention always been to show realistic performance and average user would get with prices imaginable lol. Otherwise for highest possible performance people would prefer gn or hub.,hardware,2025-12-30 09:19:38,1
Intel,nu75g90,"it's the retailers, they don't sell well and need higher margins",hardware,2025-12-15 18:32:51,13
Intel,nwpc21t,"Yeah retailer margin and taxes made arc GPUs very less desirable, in my country b580 is close to rtx 5060 in price",hardware,2025-12-30 09:20:46,2
Intel,nuitvat,Its not a rumor lol he did a video with Linus tech tips and specifically requested they put a b580 in the PC they built him  [link to video](https://youtu.be/mfv0V1SxbNA?si=jT_3dFy1H40vrVjk),hardware,2025-12-17 15:13:42,7
Intel,nuc491o,"Performance is a little worse than on windows, but compatibility is not horrifically bad. It's pretty much the same as on windows.",hardware,2025-12-16 14:18:10,9
Intel,nuj4t5k,"I believe Linus Torvalds wanted an ARC Pro B50, but settled for the B580 because that's what LMG could get their hands on",hardware,2025-12-17 16:07:30,5
Intel,nuzrua9,can you imagine a bunch of nerds whispering about which graphics card an old man uses?,hardware,2025-12-20 06:16:26,1
Intel,nuf0jlu,"I'm curious as to how much worse. I've considered an upgrade to a B570 due to them being seen for Â£150 new, putting it into used RX 6600/6600 XT territory, but if the Linux performance of say a B580 on Linux falls closer to either or, then it's probably not a worthwhile choice over the used AMD options for me.",hardware,2025-12-16 22:58:15,3
Intel,nskpbhm,"TLDW:    GPU Models Tested: MSI Shadow 2X RTX 5050, Intel Arc B580 FE      16 games average:    1080P, High-Ultra Settings:     Native TAA: Arc B580 is 14% faster, 23% faster at 1% lows due to higher VRAM        DLSS 4 Quality vs XeSS Ultra Quality: Arc B580 is ~11% faster     DLSS 4 Quality XeSS Quality: Arc B580 is ~20% faster     DLSS 4 Balanced XeSS Balanced: Arc B580 is ~15% faster     DLSS 4 Performance vs XeSS Performance: Arc B580 is ~14% faster",hardware,2025-12-06 10:33:27,76
Intel,nskzrhs,"""There was a time, about a decade ago when the $250 price tag offered solid products, but the world has changed""  Yep, inflation. $250 in 2015 money is $342 in todays money. And you can get a very solid product at that price tier, the RX 9060 XT is $369 on Newegg.  GPU prices haven't gone up, you money is just worth way less.",hardware,2025-12-06 12:12:49,139
Intel,nsl4bqs,"5050 really has no right to exist at the price it does. B580 is obviously being sold at near cost or even a loss however, it's not exactly a fair comparison but that doesn't matter to consumers.  If you just want to game then I can't see any reason to consider anything else at this price point.",hardware,2025-12-06 12:50:37,48
Intel,nspebmq,I'd still probably go nvidia here as I don't trust intel's compatibility with older titles and the like.   Still it would probably be better to spend $20 more on a 9060 xt 8 gb or $50 more on a 5060 than either of these.,hardware,2025-12-07 03:40:56,8
Intel,nsm5bxh,"If UE5 games generally run this poor on Intel GPUs, there might be trouble ahead as there are lots of those games in the pipeline.  You still couldn't get ~~more~~ me to buy an Intel GPU, even if I was desperate for a cheap GPU right now. I'd just adjust my settings.",hardware,2025-12-06 16:32:46,8
Intel,nszc9eq,"The B580 is decent enough, but it might be better to just save a bit more and get a 16GB 9060 XT for $350 or something. That card is likely to last 10 years flat at this point, and it will definitely last at least 5.  And yes the 5050 is not good. Getting something with a half-decent iGPU would be a better use of your money at that point.",hardware,2025-12-08 18:55:08,1
Intel,nt0zpd3,The biggest issue is that he did not test PCIE 3.0 vs 4.0 vs 5.0. Those GPUs are very likely to go into budget builds or as upgrades to older motherboards like the B450.,hardware,2025-12-09 00:04:12,1
Intel,nsnsg7n,Imagine spending $250 on a GPU when you could literally just save $100 more for like a 100% percent more performance.,hardware,2025-12-06 21:49:06,-5
Intel,nskuu1l,Wait isn't xess a lower resolution per quality setting?,hardware,2025-12-06 11:27:34,37
Intel,nsuy6fa,"yeah there is a reason why there are 5050s for 210  the thing is a sub 200 dollar GPU, which matches it capability and vram well, its more or less the I want to step up from igpu deal",hardware,2025-12-08 00:59:57,2
Intel,nslxyj3,Maybe for the low end but high end I canâ€™t even buy a card at msrp outside of America.,hardware,2025-12-06 15:53:05,34
Intel,nspes5s,"*ignores that this is a 50 tier product and should be compared with the 950 and 1050*  This kinda of ""but but inflation"" virtue signalling I'd very unhelpful to these kinds of discussions. It's as if you're saying people should stop complaining gpus are several times more expensive than they used to be with the actual low end market completely destroyed.",hardware,2025-12-07 03:43:50,8
Intel,nsl2hua,Tech is supposed to beat inflation. Look at monitors or TVs or SSDs (before now) or CPUs or ....,hardware,2025-12-06 12:36:00,14
Intel,nt09q01,All the tech tubers are just turning into old men shouting at clouds. They will probably all be replaced by younger people living in the now soon enough.,hardware,2025-12-08 21:40:56,1
Intel,nslaf1q,"First of all 250 euros bought way more gpu in 2015 than 360 does today. And the lower end and midrange gpus were much less cut down vs the high end chipa today.  A 5050 sits where the 750ti did when the 980ti was out. Now you get entry level performance for mid end prices  Have wages actually increased that much? Because that is the only useful measure of ""inflation"". Everything else is just corporate profits   If prices for everything go up but wages don't then that leaves less money for frivolous shit like ram and storage and laptops and consoles, not more.  Even in my country where our wages are automatically indexed to match inflation, our purchasing power has dropped because the actual cost of living isn't properly represented in whichever calculation is used for the inflation number.  Houses have gone up by 100+ percent since 2015, rents have gone up by over 60 percent, grocery prices have more than doubled, utility prices have risen sharply, public transport has more than tripled in cost.  Minor expenses like clothing or a tv you buy every ten years have stayed flat, but that isnt what people are spending 80 percent of their income on.",hardware,2025-12-06 13:33:32,-8
Intel,nsl9m9r,"Shh, everyone knows that prices only go up on luxury goods due to evil corporations, after all how will people live without their computer not being 800% faster than last year?",hardware,2025-12-06 13:28:13,-6
Intel,nsmb10n,"> Yep, inflation. $250 in 2015 money is $342 in todays money.   People really need to stop using CPI. I can bet you that GPUs don't make it to the market basket. Yes, your money's value has fallen but not by that much.",hardware,2025-12-06 17:02:45,-8
Intel,nslqts2,You're getting downvoted for speaking the truth.  The RTX 5050 should be a $150-180 GPU for the price and value it offers but unfortunately people are gonna defend the price tag that the card was set for by Nvidia,hardware,2025-12-06 15:13:39,27
Intel,nspdyuf,"For a while you could get them for $229, which would be more acceptable vs a 5060 for $299, making it the same FPS/$. But the 5060 is actually the one on sale right now for only $30-$35 more. 30% faster for like 12% more money.",hardware,2025-12-07 03:38:38,1
Intel,nsn70s5,"I'd say that the cheapest new GPU that I'd blanket recommend with no ifs, buts and caveats is the 9060XT 16GB, everything below that either struggles with outright performance, VRAM or software issues like Arc.",hardware,2025-12-06 19:49:40,12
Intel,nsmn51w,"Intel checks all the right boxes on paper (generous VRAM, decent pricing compared to competitors, an alternative to the duopoly) but the recent CPU overhead stuff coupled with the crapshoot that is trying to play older games and it just isn't worth it",hardware,2025-12-06 18:06:54,7
Intel,nsl1d2d,"Yeah they cover this at the start of the video https://youtu.be/lLe5AP6igjw?t=229   XeSS 1.3 shifts everything down a tier, so their quality scaling ratio is everyone elses balanced ratio.  Older versions of XeSS match DLSS/FSR scaling ratios.",hardware,2025-12-06 12:26:39,38
Intel,nskx602,"I know quality is, not 100% sure about others. dlss quality preset uses higher resolution than XeSS and FSR quality presets",hardware,2025-12-06 11:49:24,1
Intel,nsxjyy3,Its fine for people who need a dGPU but not a beast for work. think stuff like CAD or Photoshop. It will also be fine for people who only play competitive multiplayer games.,hardware,2025-12-08 13:22:59,1
Intel,nsxk5xm,Nvidia cards are bellow MSRP here in eastern europe. AMD cards slightly above MSRP.,hardware,2025-12-08 13:24:15,2
Intel,nt0aiwr,"They are all selling below MSRP in the UK. Â£979 is MSRP for a 5080 and I can buy 3 in stock models for less than that price without much searching, at scan.co.uk.  If you are in South America its probably your countries insane import taxes, protecting their home grown GPU market lol.  29 upvotes from children who have not bothered to check or do any kind of reasoning.",hardware,2025-12-08 21:44:54,1
Intel,nsl44tr,"> Tech is supposed to beat inflation.  And it does, wtf are you trying to claim?  $100 CPUs these days run circles around 6700K which was the flagship in 2015. A B580 is faster than a GTX 980 Ti, which was the flagship card of 2015.",hardware,2025-12-06 12:49:07,75
Intel,nslgt6n,"It does. For the price of a 1993 CRT TV, you can get a flat-screen LED thrice the size and with 10 times the resolution.  SSDs? A 2 TB nvme is a fraction today than a 128 GB Sata one was a little over a decade ago.   What actually changed is inflation, and that the buying power of today's middle class person decreased significantly relative even to the 2000s.",hardware,2025-12-06 14:14:46,17
Intel,nssphno,Gpus are way more expensive to produce,hardware,2025-12-07 18:04:53,1
Intel,nsxk9lk,"> Tech is supposed to beat inflation.   It does, despite wafer prices increasing the last 10 years.",hardware,2025-12-08 13:24:53,1
Intel,nslrykc,Wrote a fucking who? The fact that TVs are cheaper in nominal terms than they were 15 years ago does not mean it has to be the same thing with every other tech product. TVs are not products manufactured necessarily on cutting-edge expensive nodes.,hardware,2025-12-06 15:20:04,0
Intel,nsridpx,"It does. For the same amount of money, you get way better GPU(unless your braindead thinks the gtx970 has same performance of 9060xt)",hardware,2025-12-07 14:17:24,0
Intel,nsmyw1o,"It does, but also, inflation has been extremely bad for 5 years.",hardware,2025-12-06 19:06:53,-1
Intel,nslef0x,"You are just making shit up at this point. NVIDIA GeForce GTX 760 (2013) release price: $250.   That was a shit card, arguably a worse product than the 9060 XT is today, when you compare it to contemporary rivals. How do i know it was shit? I had it.",hardware,2025-12-06 13:59:32,33
Intel,nsmzdz9,TSMC inflation is FAR higher than CPI. You are half right,hardware,2025-12-06 19:09:24,12
Intel,nsxnqe7,"according to US bureau of labour staticstics that measures the CPI it includes  all personal computers (desktops, laptops, tablets) and related equipment (printers, monitors, smartwatches, smartphones). It does not look at GPUs specifically, but the effect of that will be visible.",hardware,2025-12-08 13:46:23,2
Intel,nslsb0f,I doubt ppl are gonna defend the 5050 considering a 5060 or an 8GB 9060XT is not much more and a fair bit faster.,hardware,2025-12-06 15:22:04,12
Intel,nsuys01,"i mean, the 5050 off of amazon rn is 210, so it is getting there as a sub 200 dollar GPU for improving over iGPU right",hardware,2025-12-08 01:03:40,1
Intel,nsmjt4j,I pretty much but there's nothing else people *trust* in the category because apparently Arc cards are for professional nerds or something whereas I haven't had a real bad driver issue in over 2 years with my A770  People are also conditioned to fear older gen GPUs so 6xxx and 7xxx parts are sitting on shelves waiting for blowout discounts. People would still rather spend more on a basic nvidia from the 5000 series.,hardware,2025-12-06 17:50:02,0
Intel,nsokkai,B580 user here. I coupled it with a R5 5500 and as of the recent updates the card just seemed to run much better vs when I got it last July.  There was a video before which also revealed that the CPU overhead is now being addressed in subsequent updates.  https://youtu.be/gfqGqj2bFj8?si=PyAfB2NhqZKWWVXY  Iâ€™d say itâ€™s getting better and that Iâ€™d recommend it over a 5050 since the overhead is now fixed/negligible.,hardware,2025-12-07 00:31:34,10
Intel,nsxo5ua,Intel is in their second GPU generation. Its going to take a lot longer to catch up with the institutional knowledge and practical application in videogames that the others were developing for over 20 years. The CPU overheard was not an issue in Intel iGPUs and Alchemist because GPUs never got fast enough to matter. It is only now that they noticed that issue since the GPU is far enough to create it.,hardware,2025-12-08 13:48:58,1
Intel,nsq49gv,And that only in terms of native resolution and does not mean equal final image quality.,hardware,2025-12-07 06:54:10,7
Intel,nsljs69,\>dlss quality preset uses higher resolution than XeSS and FSR quality presets  Not exactly. FSR and DLSS are evenly matched in internals at all quality presets.,hardware,2025-12-06 14:32:33,22
Intel,nt3tjxg,Iâ€™m in Australia lol. The msrp for the 5090 is 1999 USD which translates to 3011 AUD The cheapest 5090 is 4800 AUD. Thatâ€™s not even close at all to the msrpâ€¦,hardware,2025-12-09 13:05:09,1
Intel,nsq2h5v,"It sure is funny every time all those 6700k/8700k era CPU's pop up on used parts sites or FB Marketplace and still expecting close to initial prices.  Who even buys them anymore? At least a Q6600 has retro value ,but those are just obsolete.",hardware,2025-12-07 06:37:50,8
Intel,nslou4e,"Not if you consider how the workloads being run on them have also changed. A GTX 970 ($450 inflation adjusted) would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  In other words, demand for performance has outstripped performance improvements, and those improvements are not felt as much.",hardware,2025-12-06 15:02:28,-9
Intel,nsm6adc,"A 100$ CPU in 2015 would easily run 2015 made software. A 100$ CPU in 2025 would barely run the electron JS slop. This includes Windows.  In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Yes yes yes it is very good on benchmarks but I don't stare at benchmarks all day. I use my computer for things you do at computer. Don't force my CPU to crunch how much digits of Pi it can compute.  A 980 Ti can easily run top 2015 games. Now? My laptop barely runs modern AAA games without looking a blurry mess. I simply can't fucking understand how you people look at the glorified motion blur and call ""yup it is the pinnacle of computer graphics"". How the fuck majority of modern AAA games look any better than RDR2 can anyone fucking tell me?",hardware,2025-12-06 16:37:49,-11
Intel,nstfa7l,"Why aren't you comparing relative buying power of 2014/2015 vs now then?  $650 got you what in 2014, a GTX 980TI?  $330 got you what in 2014? How close to the top end are both these things?    That's $900/$455 today, thereabouts.  What does $900 get you today?  Does that buy you anywhere near the top end?  And how does that product compare in relation to others above and below it?  Because the $330 product in question ($455 today) got you about ~75% to top end performance for ~half~ MSRP of the 980TI.  How does a $455 product of today square up relative to the top end?   Why don't we throw in a GTX 980TI vs a GTX 280 comparison while we're at it.  Make things really interesting.  I'll let you fill in those blanks (along with the $330 card in question) hoping you actually learn something in the process here.  The bar is very low, try not to trip.    The underlying point that user was making was pretty obvious if you read the comment they responded to.",hardware,2025-12-07 20:08:33,-7
Intel,nsm3fs2,"cmon man, give him some slack, he just made shit up cause it's convenience for his argument.",hardware,2025-12-06 16:22:46,16
Intel,nslu7ja,How is it comparable? The 9060 XT is a very good card for 250. Ideally it'd be around 200 or below but for 250 you get a card that's a bit overkill for even 1080 P gaming.,hardware,2025-12-06 15:32:39,-9
Intel,nsluf98,In quite a few countries the 9060 XT is at or below the RTX 5050s MSRP.,hardware,2025-12-06 15:33:51,21
Intel,nslugvd,Then why did the comment above mine get multiple downvotes? It's Reddit and that's how it goes unfortunately,hardware,2025-12-06 15:34:05,3
Intel,nsnx0y4,6x and 7x are priced far too high for old stock and are poor value compared to nvidias 50 series. They really haven't had a good price/performance low-mid end card since the 6700XT which are extinct at retail.,hardware,2025-12-06 22:14:23,5
Intel,nspdbpk,"I think they must have to go into every game, and adjust that t fix it, because it's not a universal fix it seems. Maybe per-game optimizations .",hardware,2025-12-07 03:34:19,4
Intel,nsxjt6i,yep. XeSS 1.3 is closer to DLSS 3 rather than DLSS 4 in terms of image quality. Its good enough to game on in my opinion.,hardware,2025-12-08 13:21:58,6
Intel,nsqehxx,"> Who even buys them anymore?  The best SKUs on sockets have always demanded a premium in the used market. Since that's where people upgrading old machines will go.  And many machines from OEMs are not readily upgradable with just new boards/CPUs combos. Since they use custom form factors etc. So it's either a in socket upgrade or replace the whole machine. The socket 6700k is on is also especially affected by the ""premium"" factor. Since there's no lower end SKU with 4C/8T. You either get the 6700/7700 variants or are stuck with lower thread count.",hardware,2025-12-07 08:33:16,10
Intel,nsm3au4,"> would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  I think your memory is impacted by the expectations at the time. And the problem of reviews often using older titles inflating numbers, ffs some are still benching with GTA V to this day.   The [970](https://tpucdn.com/review/nvidia-geforce-gtx-1060/images/witcher3_1920_1080.png) couldn't even get 60 fps in witcher 3. Which was released in 2015.  And the performance it got in Witcher 3. Was not much better than what the 5060 Ti got in [Black Myth Wukon](https://tpucdn.com/review/msi-geforce-rtx-5060-ti-gaming-16-gb/images/black-myth-wukong-1920-1080.png)  Which even including 2025 titles. Is one of the hardest/heaviest titles with the worst performance. You can expect much better performance in almost every title. Just like the 970 was doing better than it did in Witchers 3.   But to argue that we got a lot better performance back then in the games releasing at the time, that is just false.",hardware,2025-12-06 16:22:03,39
Intel,nsp8t2z,Yes 3.5GB of memory in 2015 was soooo much better than 12GB today /s,hardware,2025-12-07 03:04:36,3
Intel,nsm6g3n,"All the GPU makers are betting on you using DLSS/FSR/XeSS as part of your usage to play games. Maybe even frame generation along with Relex, and all the other tech they ship GPUs with. They used to only rely on you using regular AA techniques.   If you ignore all those options you have today, and pay like it's 2015, it might be worse a lot of the time. If you use those options, you're generally way ahead of where a GTX 970 would fall. So it depends if you're willing to adopt new rendering tech, or rejecting it.",hardware,2025-12-06 16:38:39,3
Intel,nsmzr6o,"No, not really, in 2015, you happily accepted 45 FPS on not the highest settings at 1080p",hardware,2025-12-06 19:11:17,3
Intel,nsn5d71,"This don't sounds like CPU problems at all, more like either Win11 is a vibe coded pile of bugs, or ACPI problems.",hardware,2025-12-06 19:40:49,6
Intel,nsmgtnv,"> Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Something is wrong. I keep reading people's experiences of stuff like this and I haven't experienced it, I'm not doubting it but I'm so curious as to what is wrong.  In particular I read a lot of people saying Windows Explorer takes forever to open etc",hardware,2025-12-06 17:34:05,8
Intel,nsmsf6b,"> In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  That's Windows for you.",hardware,2025-12-06 18:33:48,1
Intel,nsxkhp8,it does not matter how close to the top GPU is. its a completely useless comparison.,hardware,2025-12-08 13:26:19,2
Intel,nsm74h5,I'm not sure what you're saying. The 9060xt is $250 only in 2013 money. They are arguing it's better value than a GTX 760.,hardware,2025-12-06 16:42:14,1
Intel,nslzd1j,"Ppl up/downvote kinda randomly, doesn't really mean much post can go from +/-20 to the opposite real quick sometimes.  Anyways It's at +8 currently was at +something(2 maybe?) when i commented so who cares.",hardware,2025-12-06 16:00:40,5
Intel,nsxodfi,"Nvidia and AMD do a lot of per-game optimization in the driver as well. In some cases very brutally, for example Nvidia is known for grabbing all games DX12 drawcalls and rearranging them in driver because the way game handles it is inefficient.",hardware,2025-12-08 13:50:12,1
Intel,nsm6ry8,I remember upgrading to a 970 in 2016 and still being unable to max Witcher 3 at 1080p60 but got close enough,hardware,2025-12-06 16:40:22,3
Intel,nsxkob8,"funnily enough, GTA 5 Enhanced Edition can be quite a benchmark for ray tracing nowadays. But it took to this year for it to be released. I think we can consider it a testbed for whats going to be implemented in GTA 6.",hardware,2025-12-08 13:27:28,1
Intel,nsmnevw,"Bringing up 1080p no RT Wukong benchmarks sort of makes the point for me: the only way these cards look comparable is if we pretend features and standards are the exact same they were a decade ago.  High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen. RT was not a thing in 2015, now it is and Nvidia marketing really wants you to use it. It's like you're comparing Witcher 3 on Ultra settings to Wukong on Medium or High settings, and acting like it's apples to apples.  The moment you take modern displays and features (including DLSS to be fair) into account, it paints a picture where technology has moved on, developers and players would love to move on, and GPUs are struggling to make that jump.",hardware,2025-12-06 18:08:19,-10
Intel,nsxl3k9,"if you run out of memory today the game swaps textures and continues running, it just looks uglier.   If you run out of memory in 2015 it starts using the superslow 0.5 GB and everything breaks.",hardware,2025-12-08 13:30:09,2
Intel,nsxl8jt,Nvidia is certainly expecting DLSS+FG to be the typical use case. The vast majority of their benchmark and marketing material is with those two.,hardware,2025-12-08 13:31:02,1
Intel,nsmwsat,"I've got nothing against DLSS, I use it whenever I can, but sometimes it's just not enough to bridge that gap.  Another user brought up Witcher 3 and Wukong as an example of a graphically advanced 2015 game vs a graphically advanced 2025 game. The 970 would get 50+ fps on Ultra settings Witcher 3. Max out Wukong on a 5060 Ti and no amount of DLSS will make that card stop crying and screaming.",hardware,2025-12-06 18:56:07,-3
Intel,nsoze8r,"Like, Debian has no issues running on a n150 with multiple docker containers without instantly spiking the cpu to 100%.",hardware,2025-12-07 02:04:41,3
Intel,nsxmt3k,"I found a way to sort of kinda make file explorer slow. But its really a perfect storm thing. Have multiple screens, one of which is running in HDR and another in SDR. Have the file explorer tree open. Have a HDD, slower the better.  When you browse folders it refreshes the tree. When it refreshes the tree it asks connected devices if they are online, including the HDDs. Now move the window back and forth between your screens. When the explorer moves into HDR screen, it gets redrawn. Same when it moves to SDR screen. I suspect but cannot confirm there is a bug where the old instance is not cleaned correctly. So now when you browse it asks all devices if they are online 10 times. 100 times. At some point youll start noticing actual delays in opening folders.  Works even better if you havent restarted for a month.",hardware,2025-12-08 13:40:46,1
Intel,nsxn5gg,"the opening notepad thing, if you use taskbar it has a bad habit of not actually opening notepad until it finishes the online search for apps called notepad or whatever you typed. Disabling online search in start makes it fly really fast.",hardware,2025-12-08 13:42:53,1
Intel,nsm8g0h,The 9060 XT 8GB is currently retailing for 250$ in many areas. I'm saying that the 760 isn't an ARGUABLY worse product. It is a worse product for it's time straight up.,hardware,2025-12-06 16:49:09,6
Intel,nslzlxy,"Fair point, but isn't lower and competitive prices good for us?",hardware,2025-12-06 16:02:01,1
Intel,nsmqlcg,">  and acting like it's apples to apples.  Apples to apples would be comparing W3 performance for both cards.  Wukong even without RT is a CONSIDERABLY more advanced game graphically than original W3.   >High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen.   And? Better monitors showing up doesn't change the laws of physics and basic economics. It doesn't make scaling with die shrinks suddenly increase. With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.   And before you start harping on about die sizes. The die in the 5060 Ti is actually more expensive than the die used on the 970. Wafer price increases more than compensates for the size difference.",hardware,2025-12-06 18:24:26,15
Intel,nspgewd,"Looked it up. Wither 3 got 52 FPS at Ultra settings, no Nvidia Hairworks turned on, for a GTX 970. Wukong gets 42 FPS at the cinematic preset native resolution, which is actually intended for cinematics, but developers allow people to enable anyways. As Digital Foundry has said, they maybe shouldn't.  Gets over 70 FPS if you turn the preset down 1 notch to high. No upscaling, or frame generation, or hardware RT, which is like what Nvidia Hairworks was for Witcher 3. It's really not hard to get Wukong to run at 90 FPS on a 5060ti with some minor tweaks.",hardware,2025-12-07 03:54:39,7
Intel,nsxlsss,According to TPU review maxed out Wukong with DLSS got 42.3 fps. Not exactly the 50 fps you remmeberr for witcher but close. Heres a link to the review: https://www.techpowerup.com/review/black-myth-wukong-fps-performance-benchmark/5.html,hardware,2025-12-08 13:34:33,1
Intel,nsxop63,"That's very interesting thank you, I can definitely see why I haven't experienced it.  You genuinely wonder how Microsoft are testing these days.",hardware,2025-12-08 13:52:10,1
Intel,nsmvfxi,"> With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.  Except in practice from 2005 to 2015 you got considerably more advanced graphics *and* higher resolutions *and* generally higher framerates too. Now either you pay up or you gotta pick one.  As for the rest of your post, it's more of a digression. All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.",hardware,2025-12-06 18:49:17,-1
Intel,nsxm00p,"> As Digital Foundry has said, they maybe shouldn't.  hard disagree. As someone who does not have a lot of time for videogames and often end up playing older games with newer cards, those beyond high settings are great as it allows me to make use of my newer card and make the old game look better.",hardware,2025-12-08 13:35:48,1
Intel,nsrkwkw,"I disagree with comparing RT to Hairworks, when the visual impact as well as the emphasis put on it by Nvidia is so much bigger. I also disagree with using 1080p as a reference for Wukong, when high res and high refresh rate monitors are as cheap and plentiful as 1080p was back then.  Imagine you went back to 2015 and told the GTX 970 guy he's supposed to play his games at 2005 resolution and turn off antialiasing, how do you think he'd react?",hardware,2025-12-07 14:32:28,1
Intel,nsncxlc,">All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.  Why complain about something that there are valid reasons for lol",hardware,2025-12-06 20:22:05,5
Intel,nsyfiin,"It just makes such a small difference in UE5, it's really not worth losing 30% performance over for this engine. They would agree with you for a lot of other games, and Avatar Pandora kind of has a hidden setting, they'll maybe make available in menu at some point. Right now you need to modify a config file to enable it. Maybe they just need to wait until the final patch of a game to show those settings, years after launch, or just name them ""next gen"" or ""experimental"" with the setting below called ""ultra"".",hardware,2025-12-08 16:15:19,1
Intel,nss9lt6,"You can use DLSS and frame generation to play at higher resolutions. That's their intent. Especially UE5 games, because TSR was developed by Epic for a reason. The games on UE5 are really never intended to be run at a native resolution. I don't tell people to run UE5 at a 2013 resolution, but I also don't tell them to have the 2013 mindset that everything has to be run at native resolution, and that's the only way to play it. 1440p Balanced DLSS should give you around 50-60 fps without frame generation.",hardware,2025-12-07 16:44:57,2
Intel,nt9j82j,"The performance loss does not mater for future (re)plays.  The config settings are usually hidden because during testing there were instabilities found that they didnt think was worth fixing. There were some games that had settings beyond ultra with names like ""Extreme,"" ""Nightmare,"" or ""Insane"".",hardware,2025-12-10 09:42:20,1
Intel,nosu5u7,"3050 performance at half the power, can't argue with that.  I do wonder how long it will be before the low-end dGPU disappears entirely from laptops, suspect these new chips probably aint it due to price but it's presumably coming.",hardware,2025-11-14 12:51:04,51
Intel,noshwzt,"While Geekbench's OpenCL isnt super popular for GPU testing;  I still cant believe a mainstream integrated GPU is actually competing with laptop 3050Ti, and these results still aren't with final optimizations!  by the time Panther Lake comes out, Intel claiming that its as fast as laptop RTX 4050 might actually be true lol",hardware,2025-11-14 11:16:17,72
Intel,notzu22,Are these better than Ryzen igpu? How do they compare to RTX 4050 AND 5050 in non demanding game?,hardware,2025-11-14 16:35:58,8
Intel,notsbf4,"I'm excited sure but if the claims are true how can anyone here expected these to be reasonably priced and sell well? If it competes with a laptop dedicated GPU it will be priced 2-3 times that laptop. The 285H isn't even that popular and its expensive, hope the 385H is affordable AND the claims are true.",hardware,2025-11-14 15:58:55,4
Intel,noudeop,"The score is slightly above a GTX 1650 Super which in turn is slightly above a GTX 1060 which is barely matched by current most performing 128 bits iGPU (Lunar Lake / Strix Point)  Even if we expect 50% improvement compared to 890M, that's still half the performance level of a full power mobile RTX 3060. (roughly half a PS5 too)",hardware,2025-11-14 17:44:26,4
Intel,npgcjdt,This would still no where be close to M4/M5 in single core and GPU,hardware,2025-11-18 06:45:37,1
Intel,nvtf2fd,I sincerely hope Samsung will work on their camera and make it better in this version.,hardware,2025-12-25 03:06:03,1
Intel,nosfy6e,"Hello 6950! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-14 10:58:08,1
Intel,nouc8xh,"The problem with buying a device relying on Intel graphics drivers, is that you're getting a device relying on Intel graphics drivers.  I'd rather not.",hardware,2025-11-14 17:38:32,-4
Intel,nosxwa8,"iGPUs have already killed off the MX series, suppose itâ€™s a real possibility other low end dGPUs also get killed off",hardware,2025-11-14 13:14:59,43
Intel,note1cz,"More likely what is considered a low end dGPU will shift, just as a current low end dGPU would have been an high end model a decade ago.",hardware,2025-11-14 14:47:09,12
Intel,np3ssro,"iGPUs is why low end desktop parts dont exist anymore, its going to do the same for mobile parts.",hardware,2025-11-16 06:06:05,1
Intel,npclgdx,"It'll happen eventually but they have a few issues.  First is memory bandwidth, they'd need to put a wider bus on it to address which is costly.  Second is the memory itself, they need something with higher bandwidth (but that might be addressed with LPDDR6).  To address the bandwidth issue they often need to put on extra cache, the 12Xe3 core is basically maxing out the bandwidth of the 'standard' chip, and if they went to say 20Xe3 cores (which would be 4060+ level) then they would need a bigger memory bus and more cache (Strix Halo put 20mb of MALL cache to try and address, and a 256 bit bus) and that is more design work.  They'd also have an issue with the socket - the APU would be too big to fit into a standard socket and would need a custom one which means a custom motherboard which increases costs.  Another is heat - while more efficient overall, all the heat is in one spot so the computer needs to be thoughtful about moving that out.  A think that in 2028 big APU solutions can compete well against XX50 series and most XX60 cards.  It isn't happening before then however.",hardware,2025-11-17 17:36:51,1
Intel,nosr91b,Apple's M5 already beats the 3050Ti though.,hardware,2025-11-14 12:30:46,33
Intel,notfij9,I mean the early Iris pro onboard GPUâ€™s traded blows with gtx 650m at a lower power draw. Itâ€™s been done before. Still nice to see but nothing ground breaking,hardware,2025-11-14 14:54:51,10
Intel,nou9dky,We will have to wait for the actual laptops to release & see how they do in real games; performance in synthetic benchmarks does not always line up in real tests.,hardware,2025-11-14 17:23:51,10
Intel,nov4acg,PTL should have comparable or possibly even lower production costs than ARL. There's some room for optimism.,hardware,2025-11-14 19:59:19,7
Intel,noydico,this GPU is also coming into the 355H btw (more specifically there's a Ultra X7 version of the 355H with the full 12-core GPU)     and iirc last time the 255H was pretty popular,hardware,2025-11-15 09:37:36,2
Intel,npcp3di,"Way we should think about it in my mind is that it's got 12Xe3 cores, and 1 Xe2 core = 2 RDNA 3.5 cores.  Strix Halo has 40 RDNA 3.5 cores (though with a bigger bus and enhanced cache) which is kind of like 20 Xe2 cores (maybe a bit more), and Strix Halo basically ties with a 4060.  If you assume a 5-10% improvement from the architecture going Xe2 to Xe3 to offset the bus and cache (which isn't *as* limiting with only 60% of the hardware), then you're probably at about 60% of a 4060 power, which is more or less where this looks to fall.  4060 mobile passmark = \~17,400  4050 mobile passmark = \~14,300 (81% 4060)  60% 4060 mobile passmark = \~10,500  3050 mobile passmark = \~10,100  That puts it pretty much at a 3050.  With a little sprinkle of cutting edge upscaling and a bit of frame gen it'll be just fine for light titles, but won't be a dedicated monster.  It's just a good solid chip for light-weight long-battery general purpose use that can play some titles on the side (or play older titles with confidence).  I suspect it'll review pretty well, since the reviewers tend to be excessively games-focused and this will appeal, but it's not a games machine - it's a surprisingly zippy long-life chip that's a good fit for all-day laptops.  EDIT: 8060S (Strix Halo) is \~18,000 for reference.  EDIT 2: 890M (strix point's top end) about 8,100, so the PTL 12Xe chip will smoke that.  I'm excited about it, it looks like a great chip!",hardware,2025-11-17 17:54:36,0
Intel,nvy72ph,"I swear, the only issue i have with the book 5 is the camera, this shit doesn't deserve the book 5 hardware",hardware,2025-12-26 00:34:46,1
Intel,noui1dy,"It is not 2023 anymore. Intel Arc drivers work quite well, and Lunar Lake graphics are already pretty solid in the low power category. No need to keep repeating this nonsense when you haven't even used the devices.",hardware,2025-11-14 18:07:17,16
Intel,not3o9j,"Only sticking point seems to be cost I suppose, implementing a truly competitive iGPU looks like it will be expensive if Strix Halo is anything to go by.  I think the Intel/Nvidia partnership might at least come close to making it a reality to have a 50/60 tier integrated GPU in a laptop that isn't much more expensive than one with a dGPU currently... in theory it could/should be cheaper.",hardware,2025-11-14 13:49:22,10
Intel,nou51fd,Not really? Even optimistically speaking this won't be close to a 5050 laptop when accounting for bandwidth limitations in real world tasks that use the cpu and gpu at the same time.,hardware,2025-11-14 17:01:51,3
Intel,nou5hpq,"It's a good point. There really isn't any truly low end dGPU options out there now as far as I know, Nvidia abandoned the GT line and someone up there mentioned MX is dead etc.",hardware,2025-11-14 17:04:08,10
Intel,nouozrl,Iâ€™m assuming the caveat to posts like this is â€œrunning a version of windows/linuxâ€,hardware,2025-11-14 18:41:25,12
Intel,nossn02,"and the 780M when fed enough power also comes within range, and the 890M also beats it.",hardware,2025-11-14 12:40:39,22
Intel,np3szy6,"It could beat a 5090, it would still be useless until the bootloader is open.",hardware,2025-11-16 06:07:46,2
Intel,nougqlr,Wasn't the previous Intel igpu really good for games and efficient?,hardware,2025-11-14 18:00:43,9
Intel,nouo5af,"Even if what you said was true, I know enough about Intel to know that even if it worked great, they'd find a way to fuck it up eventually. That's what they do.",hardware,2025-11-14 18:37:14,-7
Intel,noteu83,">if Strix Halo is anything to go by.  I think Strix Halo skews how expensive a competitive iGPU could be. Strix Halo was very low volume and was also, imo, overbuilt. I think it's definitely possible to have a more cost competitive big-iGPU SoC",hardware,2025-11-14 14:51:22,14
Intel,np2vxz8,Prices for Strix Halo are rumored to fall in the new year; I'm curious to see if it ever reaches 1000-1500 dollar laptops like I've seen suggested it could. Seems like a ridiculous idea right now.,hardware,2025-11-16 02:14:32,1
Intel,nox0o6z,And in gaming.,hardware,2025-11-15 02:39:40,7
Intel,noxmkdx,I mean...technically Asahi Linux exists for Macs. Though not the M5,hardware,2025-11-15 05:15:54,3
Intel,nost857,Then there's the 8050S & 8060S,hardware,2025-11-14 12:44:41,12
Intel,np2p16a,"How so? It is not even beating, at best barely even with 3050 35W (non-Ti).",hardware,2025-11-16 01:33:18,1
Intel,npfpm2g,I'm sure thats the first thing that comes to mind when people purchase a thin and light.,hardware,2025-11-18 03:44:31,1
Intel,novhbul,"The Arc 140V like in Lunar Lake 268V? it's about 3050 Mobile (35W) sort of performance in real tests. There's a 140T in some chips too, I think it's less powerful than the V.",hardware,2025-11-14 21:08:26,6
Intel,noto02z,Strix point is the better comparison,hardware,2025-11-14 15:37:54,3
Intel,nou57r7,"I hope so. I was a little surprised by the price but as you say, they clearly have a market for that thing and it isn't particularly price sensitive.",hardware,2025-11-14 17:02:45,2
Intel,nosxd97,they are not really comparable to the traditional APUs,hardware,2025-11-14 13:11:43,18
Intel,not3ot0,These are 256bit bus devices and have even fatter GPUs .....,hardware,2025-11-14 13:49:27,18
Intel,npbumwg,"Wait, is the 780/880m close to the 3050? I had heard that it was close to the 1050 at some point... As an 780m owner, it sounds like I can suddenly play way more games than I imagined?",hardware,2025-11-17 15:24:12,1
Intel,novka84,yes that one,hardware,2025-11-14 21:23:45,2
Intel,notxkw1,"Right, but Strix Point suffers from the same problems that PTL-X will: 128b bus chocking the memory bandwidth.  If iGPUs want to truly replace entry level dGPU, there needs to be wider bus variants",hardware,2025-11-14 16:24:49,12
Intel,npcclrj,"Might seem so, but mobile 3050 is way slower than let's say 1660 laptop.  Still, you can play many games, just at lower res and settings. You can look up benchmarks/tests for handhelds with this gpu (or ryzen z1/z1 extreme)",hardware,2025-11-17 16:53:04,1
Intel,nour2eo,"No doubt , I meant in terms of availability",hardware,2025-11-14 18:51:36,1
Intel,nngoz2c,Why can't they have a low core count CPU but still keep the full iGPU?,hardware,2025-11-06 18:43:01,69
Intel,nngf3u0,"While this has pretty much been known for a while, you really do get the feeling when looking at these frequency and power stats that Intel hoped for more from 18A.  It's a good node and a good chipset, I think it's targeted and designed well and will do well in the market, but it doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.",hardware,2025-11-06 17:56:37,48
Intel,nnk8kqz,"6 different top models with just 100mhz between them is so stupid, that should be 1 or MAYBE 2 models for that core configuration  But just hope the 356H is cheap then",hardware,2025-11-07 07:43:58,10
Intel,nnku3ds,Is this desktop or laptop?,hardware,2025-11-07 11:19:09,3
Intel,nnglnmb,">16C (4P + 8E + 4LP)  I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  Two clusters make sense. What am I supposed to do with three?!  Unless I'm mistaken, AMD seems to be sticking with full fat Zen 5 cores in both Strix Halo and ""Fire Range,"" and frankly, it sounds like Intel is just trying to keep up in the core count race with its plethora of 'baby' cores.  Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.   I use my laptop for convenience and my desktop for compute heavy work.",hardware,2025-11-06 18:27:30,-6
Intel,nnhbq11,"16 cores, but only 4 of them will be fast. The rest will be slow cores.",hardware,2025-11-06 20:34:10,-2
Intel,nngan3d,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2025-11-06 17:35:20,1
Intel,nniwn7f,"Nice. My a770 still slaying 4k ultra gaming in 2025. No stutters no freezing just responsive beautiful gaming. Ofcourse itâ€™s paired with 14900k so my igpu technically has identical specs but I keep temps down this way. Also have an RTX 50 series. I love arc graphics. Bang for buck AT HIGHER RESOLUTIONS â€” thatâ€™s where itâ€™s at. And Iâ€™ve had 4090 (sold), amd 9070(sold). Never would I pay what they are asking for a 5090. Just because I wasnâ€™t 100 percent thrilled with price per FPS with the 4090. Once you have had all of them you will see Intel is the way to go with 4k gaming(unless you like paying thousands of dollars). Arc doesnâ€™t have shadows and artifacts . RTX does. Bad. Blurs my picture even on best settings possible.",hardware,2025-11-07 01:45:56,-6
Intel,nngrua7,Upselling,hardware,2025-11-06 18:56:23,45
Intel,nnh35ny,IMO it's probably not economical for them to do it.   Gaming laptops/handhelds are already niche enough as it is -- doesn't make sense for them to package both and for OEMs to carry both for just for a ~$1-200 delta between the units. Those who want the full GPU will just pay for it.,hardware,2025-11-06 19:51:36,16
Intel,nnntnit,"You gotta remember these CPUâ€™s donâ€™t have multi-threading. AMD gets 16 threads out of their 8 core z1 and z2 extreme but with these core ultraâ€™s you get one core and thatâ€™s it. A full on physical core is better than a virtual core, but the same amount of cores with multi-threading is much better than just a same core-count CPU without it. Intel are offering up to 16 cores which is 16 cores/16 threads, but generally should offer better multi-core performance vs an 8-core 16-thread. If anything itâ€™s nice Intel have the ability to improve core count offerings without significantly raising the power requirements.   I for one veered away from the MSI Claw 8 AI+ as a standalone PC solution because it lacked the multi-core performance I needed with it only having 8 physical cores and no multi-threading. A 16-core offering is exactly what I need.",hardware,2025-11-07 21:03:16,3
Intel,nnhhojx,Because nobody would buy it.,hardware,2025-11-06 21:03:48,5
Intel,no4wbjw,"yeah, sucks cause they said handhelds was a priority for them",hardware,2025-11-10 17:11:40,1
Intel,nngiuzv,"Not great, not terrible. In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.",hardware,2025-11-06 18:14:25,22
Intel,nngq1l8,According to jaykihn0 it's a heat issue: BSPD and transistor density is flattening the VF curve on the high-end. The ST power curve is substantially improved at lower frequencies over Lunar Lake and Arrow Lake.,hardware,2025-11-06 18:48:00,8
Intel,nngpld3,"If anything, seems strictly worse than N3, even compared to older iterations. That's not great for a node that was supposed to go toe to toe with N2. Especially if the production cost is more in line with the latter.",hardware,2025-11-06 18:45:52,9
Intel,nounnja,"18a is initially launching with its lower power variant, which is why the only nova cpus that will use it for the compute tile are the laptop ones. The higher power variant comes later",hardware,2025-11-14 18:34:48,1
Intel,nnitrfm,"Jaykihn, whose tweets these leaks are based on, says that the design characteristics cannot be extrapolated to the node itself.   So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.",hardware,2025-11-07 01:28:24,1
Intel,nngofbt,"> [â€¦] It doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.  Well, we already had rumors of Intel struggling to hit actual intended frequency-goals, no?  Though especially the actual performance-metrics of the node will be quite sobering I think.  The power-draw of these SKUs is supposed to be a TDP of *officially* 25W â€“ In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4) and 65/80W (4+4+4 and 4+8+4) respectively.  So it must be seen, if these parts are any more efficient in daily usage, or just a side-grade to Lunar Lake.",hardware,2025-11-06 18:40:28,-2
Intel,nngjg8l,"Panther Lake is pretty much a pipe cleaner, so it should still improve a bit.  Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD. If that is the case, it will only really be allowed to shine in high powered desktop and server chips, while power limited chips won't be able to reap as many of the benefits of 18A, since laptops tend to target the efficiency sweet spot, rather than clocks.  My understanding is that 18A will scale much better with extra power than N3 and N2. Once you pass the sweet spot, 10% extra performance on N2 might take 30% extra power, while 18A should continue to scale much more linearly for a good while longer.  So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones, especially in use cases where SRAM density is less of a concern. AFAIK Intel CPUs aren't as cache-dependent as AMD, so SRAM density might not be the highest priority, whereas slugging it out with AMD and trying to beat them on frequency might?  From what I've heard, Intel is still just about matching AMD in terms of IPC, while Arrow lake is very efficient, just not great for gaming. If Intel wants the CPU crown back, simply pushing more power could go a long way in clawing back the deficit, assuming 18A does indeed scale well with more power. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.",hardware,2025-11-06 18:17:12,-8
Intel,nnpnb6l,> 6 different top models with just 100mhz between them is so stupid  They've always done that because of the OEMs who like a million segments.,hardware,2025-11-08 03:41:25,3
Intel,nnkuisr,Laptop.,hardware,2025-11-07 11:22:50,4
Intel,nngwxp8,Since when is any user determining what cores they want to use and when?,hardware,2025-11-06 19:21:09,21
Intel,nngrg35,It makes sense to have three as the low-power cores prioritize keeping the power consumption low for battery life but to achieve this they aren't attached to the ring bus which hurts performance so 8 regular E cores are also used,hardware,2025-11-06 18:54:32,11
Intel,nnhqy1t,">I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  From a user's perspective, CPU's are a blackbox and the inner-working details are irrelevant. What matters is results. They want, typically, a balance of performance, battery life, heat / fan noise, and cost, placing greater emphasis on one of these categories over the others.  The idea is that P cores = performance optizied  E cores = Area optimized  LP-E Cores = power optimized.  The theory being to have different core types focus on different parts of PPA.  E cores and LP-E cores are the same microarchitecture. Just LP-E cores are off-ring so the ring (and rest of the cores) can be powered down in light-load scenarios.  Intel's biggest problem is that looking at each of the cores, the P cores perform their designated role the worst. 200% larger than E cores for \~10% more IPC and \~15% more clockspeed is a pretty rough trade off - and by the time you've loaded all P cores and need to load E cores too, that clockspeed difference diminishes hard.  Intel's better off (and all rumors point this way) to growing the E cores slightly and making that architecture the basis of the new P and E cores (like Zen vs Zen-C)",hardware,2025-11-06 21:48:50,11
Intel,nngrdc7,"> Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.  You are the niche use case. The average/median user is using H-class laptops.   These are meant to beat Strix Point and its refresh, which it will do and at better economics for Intel than Arrow Lake H.",hardware,2025-11-06 18:54:11,5
Intel,nnhcb0r,"And with all the issues Windows already has with the scheduler, I fully expect it to never be able to properly utilize the different cores. I expect a similar situation as with previous versions, where it's sometimes necessary to disable the slow cores to eliminate stuttering in games, because they sometimes offload tasks to them which slows the whole game down to a crawl.",hardware,2025-11-06 20:37:09,2
Intel,nngp9jv,"> I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  > Two clusters make sense. What am I supposed to do with three?!  Ain't these Low-Power Efficiency-Cores aren't even usable by the user anyway in the first place (and only through and for Windows' scheduler to maintain)? As I understand it, LPE-cores are essentially placebo-cores for marketing.",hardware,2025-11-06 18:44:22,-9
Intel,nnhcsuh,Darkmont should really not be that far off Cougar Cove. Certainly not *slow*.,hardware,2025-11-06 20:39:41,24
Intel,nnhjsmd,The E-Cores are plenty fast for most use-cases by now and there are 8 of them in there. Arenâ€˜t they even ahead of the skylake IPC by now? 8 will be doing a great deal with a good clock frequency.  Only the 4 LP-Cores are kind of weak. But they arenâ€˜t attached to the ring-bus and really only used for offloading low req Backgroundtasks and an idling machine. So that you can turn off the faster cores when they arenâ€˜t necessary.  But for performant gaming and productivity it is basically a 12 core machine.,hardware,2025-11-06 21:14:06,6
Intel,nngt3ry,I hope there's a 4+0+4+12 for handhelds but they might not consider adding a whole new tile configuration packaging line worth it over just giving the OEM a discount.,hardware,2025-11-06 19:02:26,11
Intel,nnpqhax,I find that weird. Who buys a 20 core CPU for GPU performance equal to like an RX 580?,hardware,2025-11-08 04:04:39,2
Intel,nnj0g84,"Yeah as of now only MSI looks committed to using Intel for their handheld, and not only are they not that successful but they had also branched out to AMD for another line of their gaming handheld.",hardware,2025-11-07 02:08:54,8
Intel,nnpmmi9,Gaming handhelds are niche but not gaming laptops (relative to gaming desktops). Steam hardware survey reveals a large number of mobile gpus very high up in the charts.,hardware,2025-11-08 03:36:31,1
Intel,nnhnhmd,Both Apple's M series and LNL use decent iGPUs for their 4+4 parts.,hardware,2025-11-06 21:31:54,15
Intel,nnnppbh,"This is a minority use case, but is a very valid one. I run a home server (/r/homelab) and could use some extra GPU power for video encoding and some basic AI stuff. Though this is just a home server so I donâ€™t want to put in a dedicated GPU due to the desire to not spend all of my discretionary income on my power bill. Right now I have a 9th gen i7 that is good enough for the basics of what I am doing, but Iâ€™ve been keeping my eye out for a replacement. Iâ€™m not alone in that need, but I do get this is a minority group. Iâ€™m sure there are other use cases though. Saying no one would buy it is wrong.",hardware,2025-11-07 20:42:57,1
Intel,nngm2dr,"IIRC Arrow Lake's compute tile is fabbed on N3B, which is pretty much TSMCs earliest N3 node for mass fabrication.  Its other tiles are on a mix of mature 5nm- and 7nm-class nodes.",hardware,2025-11-06 18:29:25,19
Intel,nnh1yr8,"> In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.  Arrow Lake was the pipe cleaner for 20A which in turn was deemed unnecessary to move into production because everything was going so well that they could use that groundwork to get a head start and move straight to 18A. That's right from Intel's mouth. Now the goalposts shift once again.",hardware,2025-11-06 19:45:51,20
Intel,nngr2a1,"> Not great, not terrible.  For what it's worth, I'm glad and somewhat relieved, that the never-ending charade of 18A (and ultimate sh!t-show it eventually again amounted to, after their blatant Vapor-ware 20A-stunt) finally is about to come to some lousy end, after years of constant shady re-schedules, even more bi-weekly Intel-lies and basically +2 Years of delays again.  So it will be a bit of â€¦ 'consolation' I guess, not having to constantly read that 18A-sh!t in every darn Intel-news.",hardware,2025-11-06 18:52:46,-9
Intel,nnh0tuv,"I'm curious to see the core's power curve itself. Or the core+ring, I forget what exactly Intel measures here for their software power counters.   Intel claims PTL has outright lower SoC power than LNL and ARL, so if the power savings are coming from better uncore design and not the node gains themselves....",hardware,2025-11-06 19:40:21,13
Intel,nnh18hs,">Â BSPD and transistor density is flattening the VF curve on the high-end   He doesn't make this claim. BSPD in particular was supposed to help the most at high voltage, and the density does not appear to meaningfully differ from N3, which it still regresses relative to.Â    Best case scenario, the node, for other reasons, can't hit as high voltages as N3, and the top of the curve is just capped. But you wouldn't expect thermals to be such an issue then.Â    Doesn't seem to be any evidence of the core-level VF curve benefiting from the node either.",hardware,2025-11-06 19:42:19,12
Intel,nniukba,"Jaykihn attributes only density to the supposed heat issues, not BSPD. Which makes sense because the process guy and Stephen Robinson both talked about having to space out the signal and power wires when working with BSPD.  If they prioritized density, then it is possible that frequency could not reach the theoretical maximum due to crosstalk or heat issues.",hardware,2025-11-07 01:33:21,2
Intel,nngraag,"18A has always been low density, but expected to compensate for it with very efficient frequency scaling past peak perf/W. It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Wait for the desktop/server chips before you call it. With fewer thermal constraints and much higher power budgets, they should be able to push well past the perf/W peak, where it should continue to scale well and for a long time before it starts hitting severely diminishing returns.  I'm not talking 300W monster CPUs, but the scaling between a 65W and 105W 18A CPU should be more significant than on current nodes, where you might gain like 10-15% additional performance from that 50% power bump. I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.",hardware,2025-11-06 18:53:47,9
Intel,nnklc8p,> So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.  Why would they suddenly decide to underclock it to such a degree?,hardware,2025-11-07 09:55:39,2
Intel,nngrcwg,"Tbh, LNL battery life (note: not the same thing as loaded efficiency) with -U/-P/-H perf levels and market reach would still be a very good thing. It's just a shame to see Intel finally straighten out their SoC architecture only to be hamstrung by a subpar node.",hardware,2025-11-06 18:54:08,9
Intel,nnhns9s,">In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4)      Intel's existing 2+8 parts have a PL2 of 57W, so seems to me OEMs are permitted to target the same PL2 they've been targeting on U series.  >and 65/80W (4+4+4 and 4+8+4) respectively.  Which is a fairly large drop compared to existing H series. ARL-H PL2 is up to 115W, PTL-H PL2 will be up to 65/80W",hardware,2025-11-06 21:33:20,7
Intel,nnh0cvv,">So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones,  NVL-S being external adds serious question marks to this.   I wonder if post NVL-S they go back to using further iterations of 18A though for even desktop, if they feel confident enough that they can hit high enough Fmax (even throwing away power and density) on those compute tiles.   Maybe for RZL or Titan Lake in 28' (going off memory for codenames lol).   >especially in use cases where SRAM density is less of a concern.  On paper, Intel has caught up to TSMC (even N2) in SRAM density.   >AFAIK Intel CPUs aren't as cache-dependent as AMD,  Generally speaking Intel uses way higher capacity caches than AMD, AMD uses smaller but faster caches.   >. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.  This seems *extreme*. Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?",hardware,2025-11-06 19:38:01,10
Intel,nnh6cvf,"I actually suspect otherwise - speculation on my part, but I suspect that they were running into unexpected process issues when they pumped the frequency and/or voltage higher so they had to scale back performance.  It almost feels like a process bug that they discovered when making the actual chips.  They've also talked about kind of weak yields on 18A on their latest earnings call.  That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  If they have a 'double upgrade' opportunity in both fixing the discovered 18A issues and implementing the originally planned 18AP improvements then it could end up being a somewhat bigger jump.  Don't think it'll be an N2 beater though.  18AP *might* beat the best of N3 in some applications, we'll see.  For that you'd want to look ahead to 14A where the CFO mentioned on the earnings call (and he was somewhat realistic in other areas so it's more believable here) that 14A has been a positive surprise and they're actually pretty excited about it.  It's possible that if Intel gets more experience on High-NA versus TSMC that they might start having an edge later on.  Maybe.",hardware,2025-11-06 20:07:18,10
Intel,nngqma7,"> Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD   And yet we see the exact opposite here. The leaker claims these go to to 65W, even 80W TDP. There's plenty of power to hit any ST boost the silicon can handle, yet it *regresses* vs N3B ARL. And that's with a year to refine the core as well.Â    > If that is the case, it will only really be allowed to shine in high powered desktop and server chips   Server chips are low voltage. Even lower typically than mobile chips. High-V only matters for client.Â    > My understanding is that 18A will scale much better with extra power than N3 and N2   There is no reason to believe that at this time. Notice that Intel themselves are using N2 for their next flagship silicon, including desktop.   Edit: Also, for the ""pipe cleaner"" rhetoric, remember how they cancelled 20A claiming 18A was doing so well and ahead of schedule? Even more obviously a lie now.",hardware,2025-11-06 18:50:42,14
Intel,nnj8gs7,Having to roll the dice on the scheduler doesn't make things better.,hardware,2025-11-07 02:56:58,5
Intel,no3fsuj,"Since release of Windows 7, which allowed users to easily set which cores are used on per application basis in first party software (task manager).",hardware,2025-11-10 12:23:22,1
Intel,nnh9r6a,"It's been a thing for a while as a means to circumvent the Windows kernel's shitty scheduling, especially on processors with asymmetric core or chiplet arrangements. Tbf, Windows has slowly begun to catch up and run cores a little more efficiently, but some people still swear by apps like Project Lasso for manually controlling their process affinity.",hardware,2025-11-06 20:24:17,-2
Intel,nnjjmma,"Users aren't ""determining"" anything because they don't have to.   On smartphones, they've virtually zero control over their own hardware.   And on the x86 side, I don't think people are buying big.LITTLE hardware in droves, and even if they are, I doubt the multiple core clusters are their deciding factor.   And if I had a big.BABY CPU, Iâ€™d definitely be tempted to play around with it, and yes, I donâ€™t see the big idea behind having three clusters, aside from marketing gimmicks and artificially inflated MT benchmark scores.  From what I've heard, Windows Scheduler has trouble dealing with just two clusters as it is. On Linux, the experience is, at best, tolerable.  At the very least, I've certain issues with Intel marketing these CPUs has having ""16 cores."" I know a lot of 'normies,' first hand, who have fallen prey to this deceptive marketing tactic.",hardware,2025-11-07 04:09:16,-4
Intel,nni9yge,"> E cores and LP-E cores are the same microarchitecture   In this case, they're even the same physical implementation. 100% identical to the compute cores. Also, for MTL/ARL they were neither power nor area optimized.",hardware,2025-11-06 23:31:35,5
Intel,nnhpwee,The average user is using U series,hardware,2025-11-06 21:43:43,8
Intel,nnh2hm0,">, which it will do and at better economics for Intel than Arrow Lake H.  Doesn't seem like this will be the case till the end of 26' though. At least from the earnings call a few weeks ago.",hardware,2025-11-06 19:48:23,3
Intel,nnlj5r4,"The average user runs apps like Teams, entirely off of the LPE core of Lunarlake and Pantherlake",hardware,2025-11-07 14:06:45,2
Intel,nngrbmr,"LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  4 Darkmont LPE, in therory, should be a significant improvement.",hardware,2025-11-06 18:53:58,14
Intel,nngrskq,"They are definitely more than usable if itâ€™s anything like LNL, which basically defaults to them.  If itâ€™s more like ARL or MTL, itâ€™s placebo except for S0 sleep.",hardware,2025-11-06 18:56:10,12
Intel,nngrybw,"The LP cores in Meteor Lake/Arrow Lake were too weak to be usable as multithreaded boosters, this isn't the case with Lunar Lake and Panther Lake will be no different",hardware,2025-11-06 18:56:56,11
Intel,nngqpfh,"Windows are supposed to be using these LPE cores for lighter task such as web browsing or Word documentation or idling so that the they won't have to activate the rest of the cluster -> Reduced power draw and therefore good battery life. Meteor Lake originated with these but they were so slow they were pretty useless outside of idling. Lunar Lake also had 4 LPE cores functionally and we see how good those cores are, and these Panther Lake are supposed to be built on that but with additional 8 E cores for people that really value multi-core performance and to also solve the one major weakness of Lunar Lake, if it is even that major in the first place.",hardware,2025-11-06 18:51:06,9
Intel,nnhryps,>Arenâ€˜t they even ahead of the skylake IPC by now  They're definitely ahead of Skylake now. Cougar Cove IPC vs Darkmont is gonna be like 10% better,hardware,2025-11-06 21:53:47,16
Intel,nni8z4w,> Only the 4 LP-Cores are kind of weak   Not for LNL or PTL. They're not crippled like the MTL/ARL cores. Should be full speed Darkmont. That's like 12th or even 13th gen big core.,hardware,2025-11-06 23:25:55,13
Intel,nnj1ufe,"The LPE cores being weak is probably the point anyway, or at least partially. It doesn't need to usurp an insane amount of wattage to get to full speed because it doesn't scale well, but it stays fast enough at lower wattage to default to that power profile -> Improved battery life.",hardware,2025-11-07 02:17:05,3
Intel,nnlhwmo,They are ahead of Raptor lake P cores and Zen 4 in IPC under 40W,hardware,2025-11-07 13:59:45,1
Intel,nngwvfw,Honestly I wonder if the 4 e-core cluster on the compute tile is outright more power efficient than the 4 p-cores.    I wish some reviewers would do power efficiency testing with different core count configurations enabled on LNL and ARL. Just for curiosities sake.,hardware,2025-11-06 19:20:53,13
Intel,nnihgw0,"At this point, I just hope they have a handheld chip with just 8 e cores (ensuring the CPU tile is as efficient as it gets) and a decent GPU. Maybe in time for an Intel + Nvidia chip, which could be a dream handheld chip.",hardware,2025-11-07 00:15:32,7
Intel,no19s52,Gaming laptop with iGPU should be niche. Most people just get Nvidia in their gaming laptops.,hardware,2025-11-10 01:40:12,1
Intel,nnin9fr,Apple gets their margins in upselling memory and storage they don't need to worry about the added complexity of more SKUs for the sake of price laddering,hardware,2025-11-07 00:48:54,15
Intel,nngr3we,"Perhaps fully mature was an overstatement, but it wasnâ€™t the first product on N3B, the node was already in HVM for over a year at that point IIRC.",hardware,2025-11-06 18:52:58,8
Intel,nnhhz5t,"We all know that's a lie and 20A was a disaster. Why repeat it now? 18A may not be perfect, but it exists and these chips are comming soon.",hardware,2025-11-06 21:05:14,16
Intel,nnkdpn4,What delays? 2025 node in 2025?,hardware,2025-11-07 08:35:46,0
Intel,nnhmjz8,">BSPD in particular was supposed to help the most at high voltage  Says who?  BSPD improves signal integrity, lowers IR Drop, and helps improve density, at the expense of heat dissipation.   Idk how a technology that is known to increase heat density was supposed to *improve* fMax",hardware,2025-11-06 21:27:24,4
Intel,nngweib,"> but expected to compensate for it with very efficient frequency scaling past peak perf/W   Quite frankly, the only people ""expecting"" that seem to be people on the internet unwilling to admit Intel underdelivered with 18A. There's been no objective reason to believe that was even a target for the node. If anything, the exact opposite. This was supposed to be the node where Intel focused more on low voltage for data center and AI.Â    > where you rarely push past the best perf/W due to thermal and battery constraints   That is not the case. ST turbo in -H series laptops has always been high, nearly on par with the desktop chips. At the power budgets being discussed, there's plenty available to hit whatever the silicon is capable of, and Intel's never been shy about pushing the limits of thermals.Â    And again, when you adjust for the power/thermal envelope, you *still* see a clock speed regression vs even ARL-H. The best possible outcome for Intel is actually that they can't hit the same peak voltage but look ok at low to mid.Â    > Wait for the desktop/server chips before you call it   Server is typically low voltage, or mid voltage at most. It's lower down than even laptop chips. And what desktop? Intel's using N2 for their flagship chips for NVL, which should really have been an obvious indicator for how 18A fairs.Â    > I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.   Intel had numbers in their white paper. BSPD, all else equal, was maybe a couple percent at high V. It's not going to produce dramatically different scaling, and again, we have direct evidence to the contrary here.Â    In general, Intel's had a lot of process features over the years that look a lot better on slides than they end up doing in silicon.",hardware,2025-11-06 19:18:34,17
Intel,nnguuaj,"> 18A has always been *Xyz* â€¦  There's always some excuse for Intel's stuff the last years, to accidentally NOT perform as expected, no?  > It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Hear, hear. Isn't that all to convenient â€¦  > Wait for the desktop/server chips before you call it.  â€¦ and when is that supposed to happen? Nova Lake will be TSMC's N2.  What *Desktop* CPU-line will be on Intel 18A anyway then?",hardware,2025-11-06 19:10:51,7
Intel,nnkoi67,Give reasons why you claim this is an 'underclock' that was done 'suddenly'.,hardware,2025-11-07 10:27:15,1
Intel,nngu3yg,"Of course, Lunar was quite competitive, yet it was mainly so on performance due to Intel basically sugar-coating the living benchmark-bar out of it via the on-package LPDDR5X-8533 RAM. That OpM for sure *majorly* polished its efficiency-metrics by a mile â€¦  Though looking back the recent years, that's how Intel always masked rather underwhelming progress on their process-technology â€“ Hiding the actual architectural inefficiencies and shortcomings behind a invisible *wall of obfuscation* by only ever deliberately bundling it with other stuff like newer PCi-Express versions of 4.0/5.0 or newer, faster, crazy high-clocking RAM-generations.  So Lunar Lake while very strong, was mainly so due to being propped by OpM, and TSMC's processes of course.",hardware,2025-11-06 19:07:17,-6
Intel,nnhp8md,"The article claims it's ""TDP"", which would typically refer to PL1. Might be leakers getting their terminology mixed up, but I wouldn't necessarily assume a large drop in power limits.",hardware,2025-11-06 21:40:24,4
Intel,nnkfy09,AFAIK Intel was at 165W in mobile back then â€¦,hardware,2025-11-07 08:59:02,0
Intel,nnh5e4g,"> Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?  [6%](https://web.archive.org/web/20240901235614/https://www.anandtech.com/show/18894/intel-details-powervia-tech-backside-power-on-schedule-for-2024/2)",hardware,2025-11-06 20:02:30,8
Intel,nnhouzs,"I wonder if the issues Intel's facing with 18A partially explain TSMC's very conservative approach to adopting ~~it~~ BSPD.  I image in reality, 18A (and AP's) fMax limits are almost entirely the reason for N2 in NVL-S",hardware,2025-11-06 21:38:35,6
Intel,nnh96c0,"> That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  There's no doubt that given enough resources they can fix the node to the point where it's good enough for HVM in high performance chips. It's what they did with 10nm after all but it took them several years to go from poorly yielding Cannon Lake/Ice Lake to okay Tiger Lake and good Alder Lake.  If we take the talk about 18A ""margins"" from the recent analysts call to be a euphemism for something like node yields and a stand in for when it will be ""fixed"" then we could be talking 2027 before 18A gets to that point. A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.",hardware,2025-11-06 20:21:25,10
Intel,nnh7vd8,"Yeah I know they talked about yield issues, but everything indicated that it was not a case of manufacturing defect density, but rather something else.   That would imply issues with hitting target frequencies, which as you say, they should be able to iron out in due time. That is as long as it isn't a fundamental issues, which I have a hard time believing, given how unconcerned they seemed about it.   I'm fairly confident it will be ironed out before Clearwater Forest hits mass production.",hardware,2025-11-06 20:14:54,0
Intel,nngu5ix,"There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery. It offers better voltage control and less current leakage, leading to higher efficiency at any given voltage, but especially past the point where current leakage starts becoming more of an issue, i.e. at very high frequency.  A pipe cleaner running at low frequencies is to be expected. Wait for the desktop chips and you will see P cores hitting well over 6+ GHz advertised all core boost, possibly 6.5 GHz. I wouldn't be surprised to see some users hitting 7 GHz stable.",hardware,2025-11-06 19:07:30,-2
Intel,nnlize0,The modern CPU cores schedule all cores equally with E core priority because nowadays the difference between E core and P core is less than between AMD X3D CCD and higher clocking CCD,hardware,2025-11-07 14:05:46,-1
Intel,nnhabba,"Windows Scheduler has come a long way but I mean come on, this is pretty pedantic. The commenter above is clearly not using a tool to assign cores to specific tasks they are just being annoying.",hardware,2025-11-06 20:27:06,7
Intel,nnh1new,4 skymont already seem pretty good in LNL,hardware,2025-11-06 19:44:19,8
Intel,nnh5cdj,> LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  Makes you think why Intel even went all the way to integrate those and waste precious die-space doing so â€¦,hardware,2025-11-06 20:02:16,2
Intel,nniocav,They could be useful if they could check your email and stuff while the laptop is effectively sleeping.,hardware,2025-11-07 00:55:19,1
Intel,nnhrk78,PLT's LP-E cores take after LNL. ARL/MTL's LP-E cores design is dead,hardware,2025-11-06 21:51:48,3
Intel,nnh5ttp,I hope those are powerful enough to eventually act as the low-power booster these were once supposed to.,hardware,2025-11-06 20:04:40,1
Intel,nnh666b,> Meteor Lake originated with these but they were so slow they were pretty useless outside of idling  The last info I had on the back of my mind was that these couldn't even be associated by the user and were basically reserved for Windows itself. Is that still the case now?,hardware,2025-11-06 20:06:23,1
Intel,nnj4lq1,"Yeah after being reminded that the LPE cores are also Darkmont like the E-Cores I looked up the layout in detail and the differences is simply the P and E cores are on a large Cluster with L3 Cache and a fast ring-bus, but the LPE cores are seperated from that Ring-bus and don't have any L3 Cache (just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster).     They are for sure also much more limited in power-budget and frequency, but they can be very fast in general. The main usage is being able to disable the large cluster in Idle or low cpu usage to heavily reduce power draw.     The communication in between the clusters is much slower. So anything that relies on synchronisation and communication between cores is only really fast (latency-wise) if it is kept within the same cluster.      The LPE cores therefore can be very fast actually, but only if the task stays within the cluster. Modern demanding games will therefore 99% soley run on the large cluster, because they would run slower if they spread across all of them. The small cluster could be used to offload background tasks to themselves (if the powerlimits aren't hit), which could at least improve 1% lows and fps stability.",hardware,2025-11-07 02:33:41,3
Intel,nngxyok,"It's not just more power efficient, but also much faster at low power which you kinda want since you'd be mostly GPU bottlenecked. The good thing is that they've made that part of their thread director tech.",hardware,2025-11-06 19:26:15,12
Intel,nnlikjx,Yes it is. Memory uses power and IPC of LPE should be past Zen 3,hardware,2025-11-07 14:03:29,2
Intel,nnkc1a7,Intel makes e-core only CPUs (N100 etc) but at this point they're based on several gen-old e-core design. They're good enough for what they are so they might not update them for a while still though.,hardware,2025-11-07 08:18:15,1
Intel,nnjcalp,"even 4 e-cores is enough, dont forget we use to pair up i7-7700K with GTX1080/1080Ti.  4 e cores + even stronger iGPU probably a better combo for handheld.",hardware,2025-11-07 03:21:21,8
Intel,nnoc1fy,Apple absolutely does price ladder their SoCs.,hardware,2025-11-07 22:41:38,1
Intel,nngy7u4,"True. But the node itself may be kinda mid. It seemed much, much more complex than N3E, and it's not as if the node was any sort of highlight in the products Apple used it in.   Going back to A17 reviews and such, there were serious questions presented about N3B vs N4P.",hardware,2025-11-06 19:27:29,14
Intel,nnhibo4,I'm fairly sure that's what u/ProfessionalPrincipa was also implying.,hardware,2025-11-06 21:06:56,6
Intel,nnhovl3,> We all know that's a lie and 20A was a disaster. Why repeat it now?   Probably in response to all the people assuming there must be some kind of upside.,hardware,2025-11-06 21:38:39,3
Intel,nnkl8kv,"It was supposed to be a 2024 node. And given the perf downgrade and admitted yield problems, seems more like they're only delivering the originally promised metrics in '26 or even '27.",hardware,2025-11-07 09:54:38,6
Intel,nnl32z1,"> What delays? 2025 node in 2025?  What delâ€” *Seriously now!?* Where you living under a rock the last years by any chance? o.O  18A is neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: *'cause Yields again!*) will be 1H26 and most likely even end of first half of 2026.  So it's a 2H24-node, which Intel is only able to offer actual volume basically +1â€“2 years later. That's called *Â»delayÂ«*.  **Edit:** And not to mention the actual massive performance- and metric-regression u/Exist50 pointed out already. 18A is basically 20A in disguise. Since 20A wasn't actually knifed, but just relabeled as ""18A"" instead.",hardware,2025-11-07 12:29:34,0
Intel,nnhohos,>Â Says who?   Intel. It was part of their own published results about PowerVia on the Intel 4 test chip. The gains range from ~negligible at low-V to around 6% at high-V.Â    Also explains why TSMC is not adopting it in the same way.,hardware,2025-11-06 21:36:46,11
Intel,nnkdym7,Nova Lake is full on N2?,hardware,2025-11-07 08:38:24,0
Intel,nnocn3l,"It's literally a regression from the prior gen, for a node that was supposed to be ""leadership"", mind you.Â    I don't know why it's hard to acknowledge that 18A is simply underperforming.",hardware,2025-11-07 22:45:03,1
Intel,nngzzny,"Nah, credit where credit is due. LNL made a ton of fundamental design changes that PTL should also benefit from. Yes, they also benefitted a lot from both having an actually decent node and on-package memory, and no, they're not on par with the likes of Apple or Qualcomm,Â but merely making ARL monolithic on N3 would not have delivered these gains.",hardware,2025-11-06 19:36:12,10
Intel,nnhs780,They definitely have their terminology mixed up. There's no chance PTL-H has an 65W/80W PL1,hardware,2025-11-06 21:54:56,7
Intel,nnpn3zz,Which was never the PL1 but rather the PL2.,hardware,2025-11-08 03:40:00,3
Intel,nnhxmr5,"Didn't TSMC delay their BSPD node too? Ik they changed it from appearing in N2P vs A16, but unsure if there was a timeline shift in that as well.   Honestly, what's going on with N2P/N2/A16 seems to be kinda weird, timeline wise.",hardware,2025-11-06 22:22:47,7
Intel,nnhappm,2027 just seems like hitting baseline parametric yields. Being able to get to an actual improvement in terms of performance is a whole other thing. Not an easy road.,hardware,2025-11-06 20:29:05,5
Intel,nnkqty9,">A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.  How it can be 24 product in 27 when it was 25 node in 25 with products on shelfs in january 26, rofl?",hardware,2025-11-07 10:49:31,0
Intel,nnhg56o,Intel themselves claim they won't be hitting industry acceptable yield till 2027 for 18A.,hardware,2025-11-06 20:56:18,3
Intel,nngyuum,"> There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery.Â    No, it's not. The main long term advantage of BSPD is to reduce the pressure on the metal layers as they do not scale like logic does. Better PnP is another advantage, but secondary. No one cares much about high-V these days.Â    > A pipe cleaner running at low frequencies is to be expected   As a reminder, ARL-20A was supposed to be the pipe cleaner, and they claimed they cancelled it because 18A was doing so well! PTL was supposed to be the volume product, a year after 18A was nominally supposed to be ready.Â    > Wait for the desktop chips   What desktop chips? PTL-S was cancelled long ago, and for NVL, they're moving the good compute dies (including desktop) back to TSMC on N2. A decision which should demonstrate the node gap quite clearly...   Also, ST boost for -H chips is in the same ballpark as desktop ones. They're already at the high end of the curve.",hardware,2025-11-06 19:30:37,10
Intel,nnoxxwo,"They aren't the same though, hence the roll of the dice to see if you actually get the performance you paid for.",hardware,2025-11-08 00:53:15,2
Intel,nnh7b83,Because they still improve battery life under very light loads.,hardware,2025-11-06 20:12:05,7
Intel,nnj1km0,Yeah I think you had to utilize tool such as Project Lasso. I tried to assess the LPE cores to HWINFO64 and it was rather painful to use so I understand why these cores are never used otherwise lol.,hardware,2025-11-07 02:15:28,1
Intel,nnj555b,"Yeah. The efficiency and overall battery life will probably comes down to how Windows manages threads, which seems to be pretty decent now with Lunar Lake seeing as they got pretty good battery life with the same design philosophy.",hardware,2025-11-07 02:36:58,2
Intel,nnkn219,"> just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster  It's also accessible by the large cluster, fyi.",hardware,2025-11-07 10:12:52,2
Intel,nnkd975,"For those games back in the day, yeah. But modern games can and do use more than 4 cores.   Depends how low end/power you want to go, of course. 4 cores + eGPU cuold be enough too depending on your games: [https://youtu.be/XCUKJ-AgGmY?t=278](https://youtu.be/XCUKJ-AgGmY?t=278)",hardware,2025-11-07 08:30:54,6
Intel,nnn8cyi,"The e-cores are unlikely to be hyperthteaded though. 8 e-cores gives you 8 threads just like a 7700K did, though none of them share cores. It feels like a sweet spot for a handheld in 2025 imho.   This is while AMD competitors use 8c/16t of full Zen. An 8 e-core solution would remain weaker but more than sufficient on the CPU side, while allowing enough power for what truly matters (a capable GPU). If paired with an Nvidia iGPU, that could be the perfect handheld chip, if their partnership produces one like that.",hardware,2025-11-07 19:12:43,3
Intel,nnhq813,"Those ""people"" are either bots or trolls. No sane person thinks Intel is crushing it with 18A.",hardware,2025-11-06 21:45:18,9
Intel,nnkrne6,"Whats the name of the close the gap initiative?  5 nodes in 4 years... announced in 2021  2021+4=2025  You can even read 3rd party articles from 2021 talking about 18A in 2025.  https://www.tomshardware.com/news/intel-process-packaging-roadmap-2025   >Intel didn't include it in the roadmap, but it already has its next-gen angstrom-class process in development. 'Intel 18A' is already planned for ""early 2025"" with enhancements to the RibbonFET transistors.",hardware,2025-11-07 10:57:07,1
Intel,nno64t2,">neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: 'cause Yields again!) will be 1H26 and most likely even end of first half of 2026.  How it aint 25 node when there will be products on shelves in january 26?",hardware,2025-11-07 22:08:46,2
Intel,nnljpf8,FMax at same power consumption is higher true. But the temperature is just too high. Both temperature and power consumption are important considerations,hardware,2025-11-07 14:09:47,2
Intel,nnklaca,"High end NVL is N2, low end is 18A. At least for compute dies.",hardware,2025-11-07 09:55:07,4
Intel,nnl268a,"That's what we know so far, yes. At least the performance-parts. The lower end is supposed to be 18A, I guess?",hardware,2025-11-07 12:23:00,0
Intel,nnhcrle,"> Nah, credit where credit is due.  I said verbatim, that Lunar Lake was quite competitive, and I meant it, unironically. It took long enough.  > LNL made a ton of fundamental design changes that PTL should also benefit from.  I haven't disputed that â€”  The modular concept was surely ground-breaking for Intel, and urgently needed!  Though, it's kind of ironic, how Intel all by itself proved themselves liars, when it took them basically +6 years since 2017, for eventually coming up with only a mere chiplet-copycat and their first true disintegrated chiplet-esque design â€¦  For a design, which *in their world-view*, Intel was working on their tiles-approach already since a decade, which is at least what they basically claimed when revealed by 2018 â€“ I called that bullsh!t the moment I first heard it. Surprise, surprise, they again straight-up *lied* about it â€¦  They most definitely did NOT have had worked on anything chiplets/tiles before, if it took them *this* long.  Just goes to show how arrogant Intel was back then, letting AMD cooking their Zen in complete silence since 2012/2013, for their later Ryzen. *Still boggles my mind, how Intel could let that happen* â€¦  > Yes, they also benefitted a lot from both having an actually decent node and on-package memory [â€¦]  In any case, we can't really deny the fact, that Intel basically cheated on Lunar Lake using OpM, eventually creating a halo-product, which ironically was quite sought after, but yet expensive asf to manufacture.  If AMD would've been to cheat like that (using OpM) in a mobile SKU, while dealing some marked cards in such a underhanded manner, it would've been ROFL-stomped Lunar Lake â€¦",hardware,2025-11-06 20:39:31,-2
Intel,nni6v1y,"Willing to believe that, though then I have to wonder why it would be so hard to cool at such a substantially reduced PL2. Also if/how they could pitch this as an HX replacement.",hardware,2025-11-06 23:13:41,3
Intel,nnq97pr,"Yes, I think it was 45- or 65W-parts PL1-wise. The spread was still obscene and outright insane.",hardware,2025-11-08 06:42:53,0
Intel,nnhgp5t,I'm excited for 18A-P. Intel subnode improvements have always seemed to bring decent uplifts (Intel 10SF being a notable example).,hardware,2025-11-06 20:58:58,2
Intel,noaeih3,"The difference is no longer stark. As I said, its now smaller than between 2 CCDs in the most popular CPU line ever made in actual raw performance. ie it's not something entirely unique at the scale the differences are",hardware,2025-11-11 14:51:59,1
Intel,nnheh7k,"How even, if these weren't even used with MTL!?",hardware,2025-11-06 20:48:06,-4
Intel,nnkfcmn,"Yup, pretty much paper-cores for marketing-reasons alone basically.",hardware,2025-11-07 08:52:54,1
Intel,nnlies1,We already have games tested on Skymont E cores. They are very fast,hardware,2025-11-07 14:02:35,4
Intel,nnoujai,"Yes we know modern games can use more cores, but having smaller amount of cores means saving power budget. Those power consumption saved are better off use to juice iGPU.   Lets not forget the E-Cores here are skymont, not skylake. Skymont is definitely more powerful cores. The exact same reason why i5-7600K is more power than i7-2600K.  given how iGPU for handheld are no way near GTX1080 yet. I think it is better to pump more juice for better iGPU.",hardware,2025-11-08 00:32:08,1
Intel,nnhveh1,Go to the intel stock subreddit lol,hardware,2025-11-06 22:11:05,6
Intel,nni71qc,"You'd be surprised how many people are willing to take Intel at their word on it.  Just look at previous threads here. Every time Intel posts nonsense slides, lot of people come crawling out of the woodwork.",hardware,2025-11-06 23:14:46,1
Intel,no3ffkc,"They are doing about as much as the ""sane"" people expected from 18A.",hardware,2025-11-10 12:20:33,0
Intel,nnku399,"> 5 nodes in 4 years... announced in 2021  It was supposed to start with Intel 7 in '21 and end with 18A readiness in H2'24.   https://img.trendforce.com/blog/wp-content/uploads/2023/10/17144859/intel-4-years-5-nodes.jpg  And now in the way end of '25, we're getting something more like what they originally promised for 20A, *if that*, and now they're saying yields won't be ""industry standard"" until as late as 2027. It's a disaster by any objective standards. Their failure with 18A literally got the CEO fired...",hardware,2025-11-07 11:19:07,6
Intel,nnpmsrt,"> How it aint 25 node when there will be products on shelves in january 26?  It seems, you don't actually grasp the concept of the term Â»differenceÂ«. *2025 and 2026 is actually NOT the same!*",hardware,2025-11-08 03:37:46,1
Intel,nnkm46i,so the igpu is propably tsmc too?  do we know if thats because of poor performence or poor manufacturing capazity?,hardware,2025-11-07 10:03:27,1
Intel,nni73m8,I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   I think they'll probably have ARL-R for HX,hardware,2025-11-06 23:15:04,3
Intel,nnqeasg,"Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage. Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.",hardware,2025-11-08 07:33:02,1
Intel,nnhrg1s,"The LP-E cores were a failure in MTL/ARL's design.  They're very useful in LNL and that trend follows with PTL/NVL.  They're definitely not just ""placebo cores"" - LNL uses them more often then the P cores and they can be activated and used in all core workloads as well.",hardware,2025-11-06 21:51:15,7
Intel,nnhp1ul,"They were, just not as often as Intel would have liked.",hardware,2025-11-06 21:39:29,3
Intel,nnkr81u,"For MTL and ARL, yes. For LNL and PTL though, I haven't test them out personally but seeing the battery results of LNL chips made me think that the LPE cores of those things seem to be legit.",hardware,2025-11-07 10:53:10,1
Intel,nnm0kgv,"Oh year, right, I think I missed (or forgot) that. Darkmont should be even better but I'm guessing not by that much.",hardware,2025-11-07 15:37:40,2
Intel,nnhvpi9,Kinda making my point.,hardware,2025-11-06 22:12:40,11
Intel,nnsiatm,"Bro xd  Node readiness and product readiness and product on shelves are 3 different date  Node must be ready before product, product must be ready before being on shelves.  Officially 18A was ready around Q2 25",hardware,2025-11-08 16:59:43,4
Intel,nnkmum2,"> so the igpu is propably tsmc too?  18AP, iirc. Good *enough* (or at least, assumed to be when originally planned) not to be worth the extra cost of TSMC.  > do we know if thats because of poor performence or poor manufacturing capazity?  Poor performance. Intel can't afford to be constantly a node behind. It's just too big of a gap.",hardware,2025-11-07 10:10:47,3
Intel,nni82fk,"> I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   But that's exactly it. They said 65W vs 80W was in relation to cooling challenges, but either is more than sufficient for max ST boost. Idk, maybe reading too much into too little information.Â    > I think they'll probably have ARL-R for HX   At one point it sounded like they wanted to pitch PTL as a partial replacement. Guess it's not good enough though.",hardware,2025-11-06 23:20:38,3
Intel,nnqhka1,"> Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage.  Of course, I already knew that. Binned Desktop.  > Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.  Well, up until 2018/2019/2020 Intel actually didn't really uncapped any of them and those parts were hard-limited to only pull their max 45â€“65W TDP.  Only later on when they were trying to hold pace with AMD, Intel insanely increased the TDP to pull to insane numbers (90W, 135W, 165W) â€¦ Other than that and prior, you had to resort to modded BIOS.  So until even Apple burned their thick skin on some i9 (**Cough* i9-9980HK in the 16"" MacBook Pro 2019), you didn't had such insane TDPs to begin with anyway (except for BIOS-mods).",hardware,2025-11-08 08:06:08,0
Intel,nnkfnwq,"As said, I owned a MTL-machine back then, and back then you couldn't even associated these cores after all, since you couldn't pin anything on them â€“ So yes, *back then*, these were placebo-cores and they were in fact pretty much 'on paper-cores for marketing-reason' and basically useless.  Seems Intel managed to improve their performance a lot and make them actually useful, which is good!",hardware,2025-11-07 08:56:07,0
Intel,nnl1o21,"Yes, MTL's LPE-cores were basically a dud, LNL was fairly workable and PTL will be hopefully potent.",hardware,2025-11-07 12:19:17,1
Intel,nnhvsow,haha,hardware,2025-11-06 22:13:07,7
Intel,nnxlj8j,> Officially 18A was ready around Q2 25  Which is still at least 4 quarters *past* its due-date actually â€¦,hardware,2025-11-09 14:16:26,1
Intel,nnqk8rl,"That's not strictly true, the (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  https://www.ultrabookreview.com/27215-asus-rog-g703gx-review/#a5  >That comes with a few drawbacks, though, and one of the most important is the noise development. Asus provides three different power modes for this laptop, which you can use to juggle between performance, thermals, and noise:  Silent â€“ CPU limited at 45 W and 4.2x multiplier, GPU limited at 140 W, fans only ramp up to about 40-43 dB in games; Balanced â€“ CPU limited at 90 W and 4.2x multiplier, GPU limited at 140 W; Turbo â€“ CPU limited at 200 W and 4.7x multiplier, GPU limited at 200 W, fans ramp up to 55-56 dB in games.  It was always up to the laptop OEM to configure the power limit and there was no hard cap enforced by Intel.",hardware,2025-11-08 08:33:24,1
Intel,nnndfa5,"The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, negating the entire point of them.  In addition, MTL/ARL had the LP-E cores on the SoC tile which made them functionally useless for full nT load.  LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks *can* stay entirely within the LP island, *and* they're on the same compute tile, so they're also used in full nT workloads.",hardware,2025-11-07 19:38:34,3
Intel,nnqndqo,"> That's not strictly true â€¦  Yes it actually is.   > The (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  Dude, you're basically confirming exactly what I wrote, by literally picking a **i9 9980HK**-equipped notebook of 2019, which is even exactly the very infamous SKU I was talking about â€¦  I was talking exactly on (among else) that very CPU, even Apple couldn't tame and had to undervolt (still without being able to prevent the later sh!t-storm) until eventually abandoning Intel altogether.  ---- All I'm saying is, all the years prior with only quad-core, Intel \*never\* (nor any OEMs for that matter) went above and beyond the official TDP â€“ The only lone exception from this, were those super-bulky Schenker Desktop-replacements.  That only suddenly changed by 2017â€“2019, when Intel suddenly increased core-count in mobile swiftly from Quad- to Hexa- and ultimately Octa-cores, while pushing the TDP in quick succession  into insane territory.  *That was the time, Asus took about a year to manage applying liquid-metal en masse, for Intel-notebooks!*  You are right with your sample here, yet you only confirm what I said before. Only past quad-cores it was.",hardware,2025-11-08 09:05:43,0
Intel,nnprgvn,"> The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  Do you happen to know, if that (ring-bus related) was the same principle on MTL?  > The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, **negating the entire point of them**.  Yeah, talking about throwing some completely untested sh!t onto the market, irregardless of the fact, if the product makes sense or not â€” A true classic, I'd say. Just Intel being Intel.  Since dropping those LPE-cores on MTL, would've actually made the mask and thus needed die-space *smaller*, which in turn would've actually *increased* their yields \*and\* in return profit-margins already â€¦  But muh, benchmark bars and ""AMD has moar corez!!"", I guess.  > LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks can stay entirely within the LP island, and they're on the same compute tile, so they're also used in full nT workloads.  Is there any greater penalty (latency or cache-flush-wise) for moving threads off the LP-island to P-cores?  *Thanks for the insides so far though!* I can't really see through anymore to all these constant arch-changes.",hardware,2025-11-08 04:12:06,1
Intel,nmflglf,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",hardware,2025-10-31 20:39:13,28
Intel,nmdl0rr,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,hardware,2025-10-31 14:32:50,57
Intel,nmm72sv,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,hardware,2025-11-01 23:31:51,2
Intel,nmf7n07,They said their high idle power is an architecture issue so they can't fix that,hardware,2025-10-31 19:25:22,16
Intel,nme3l5e,God forbid Intel supports Day 1 GPU drivers longer than 5 years,hardware,2025-10-31 16:03:57,-23
Intel,nmfdha7,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",hardware,2025-10-31 19:56:31,13
Intel,nmvlitt,So just like AMD then.,hardware,2025-11-03 13:41:12,2
Intel,nmfexoy,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",hardware,2025-10-31 20:04:15,26
Intel,nmhjwz4,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",hardware,2025-11-01 04:34:17,5
Intel,niqvrj6,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? â€¦Â but Xe3 plus will be enough to call it C series.",hardware,2025-10-10 09:51:49,29
Intel,nis4lzk,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",hardware,2025-10-10 14:45:18,12
Intel,nisn7g8,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,hardware,2025-10-10 16:15:59,5
Intel,niz80tx,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,hardware,2025-10-11 18:17:30,1
Intel,njgcnqq,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",hardware,2025-10-14 14:31:48,1
Intel,njxoogl,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",hardware,2025-10-17 07:35:47,1
Intel,nir35ti,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",hardware,2025-10-10 10:58:36,22
Intel,nir6blp,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",hardware,2025-10-10 11:23:30,14
Intel,nis0as0,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,hardware,2025-10-10 14:23:44,3
Intel,nisagox,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes XeÂ³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   XeÂ³ is a XeÂ³ based architecture like XeÂ³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. XeÂ³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",hardware,2025-10-10 15:13:44,3
Intel,njr6343,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",hardware,2025-10-16 06:29:51,1
Intel,nit88y1,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",hardware,2025-10-10 17:58:19,8
Intel,njxozxf,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,hardware,2025-10-17 07:39:00,1
Intel,niruidx,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,hardware,2025-10-10 13:53:56,4
Intel,nircpja,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,hardware,2025-10-10 12:08:58,30
Intel,nisb3yn,That's not what Peterson was talking about context wise when he addressed this in the video.,hardware,2025-10-10 15:16:53,3
Intel,nito8rq,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entryâ€‘level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",hardware,2025-10-10 19:19:56,-4
Intel,nis0ezd,[AMD RDNA 3.5â€™s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),hardware,2025-10-10 14:24:18,14
Intel,nirfv6f,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",hardware,2025-10-10 12:29:32,16
Intel,niw1ezc,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",hardware,2025-10-11 04:09:31,4
Intel,njr69nx,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",hardware,2025-10-16 06:31:38,1
Intel,nisec0d,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",hardware,2025-10-10 15:32:43,1
Intel,nizcijr,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,hardware,2025-10-11 18:41:45,1
Intel,nitxcqr,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",hardware,2025-10-10 20:07:36,1
Intel,njgdemn,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,hardware,2025-10-14 14:35:38,1
Intel,nitzoza,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",hardware,2025-10-10 20:19:35,1
Intel,nj6xna9,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,hardware,2025-10-13 00:16:37,2
Intel,nj7rqtc,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",hardware,2025-10-13 03:27:42,1
Intel,nifgt95,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",hardware,2025-10-08 14:41:36,38
Intel,nighktm,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",hardware,2025-10-08 17:39:48,30
Intel,nihvtct,MLID must have an aneurysm seeing the guy still employed at Intel,hardware,2025-10-08 21:50:09,14
Intel,nifmhnd,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",hardware,2025-10-08 15:09:23,31
Intel,nihklk5,Igpus not discrete gpus,hardware,2025-10-08 20:51:31,11
Intel,nigirli,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",hardware,2025-10-08 17:45:26,10
Intel,nii5519,Hahaha I can hear Tom saying â€œfucking Tom Peterson has been *lying* to youâ€ as you say that,hardware,2025-10-08 22:44:00,9
Intel,nifuaux,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,hardware,2025-10-08 15:47:29,10
Intel,niis0eh,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,hardware,2025-10-09 01:01:50,4
Intel,nigp77c,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,hardware,2025-10-08 18:16:46,13
Intel,nigl085,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",hardware,2025-10-08 17:56:04,0
Intel,nilqzn1,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,hardware,2025-10-09 14:41:05,4
Intel,nig3u1g,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,hardware,2025-10-08 16:33:52,17
Intel,nijp5fy,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,hardware,2025-10-09 04:46:48,5
Intel,nigpqtx,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",hardware,2025-10-08 18:19:29,9
Intel,nihx213,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",hardware,2025-10-08 21:56:59,6
Intel,nigh9jb,You stole what I was going to say... take my upvote.,hardware,2025-10-08 17:38:17,6
Intel,nijrll5,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",hardware,2025-10-09 05:07:37,8
Intel,nyxkxhx,"Intel GPUs have come a long way, and part of that journey was from A to B. The current lineup are th B series and are regarded as significantly better than the A series. I recommend B580 GPUs for gamers on a budget. It's a good card, and most of the a series issues are gone. Drivers work, upscaling happens, compatability is good.  tldr: Current Intel GPUs are fine, just avoid the A-series",buildapc,2026-01-11 06:49:31,8
Intel,nyxlbjn,"A580? Nah not that much, the RX 6600 and RTX 3050 are the same price and generally either have better better performance or better tech, both have better optimization, the Arc B580 though, absolutely as it has good drivers and is on the same level as a 4060 for a lower price",buildapc,2026-01-11 06:52:49,9
Intel,nyxou31,Look up the model on Gamer's Nexus. They seem to like them for the price point and pointed out that updates have fixed a lot of initial issues.,buildapc,2026-01-11 07:23:19,2
Intel,nyz9r5o,so will b570 be fine?,buildapc,2026-01-11 14:53:47,1
Intel,nyyoj98,"I wonder why no one ever recommends the B570. Sure, it might be less powerful than the B580, but I suppose that has to work better for certain CPUs that aren't that powerful",buildapc,2026-01-11 12:43:16,2
Intel,nz0htle,"Sure. It's about 10-15% slower than the B580, so take that into account when doing ""fps/$"" calculations in your head.  https://www.techpowerup.com/review/asrock-arc-b570-challenger/32.html",buildapc,2026-01-11 18:24:42,2
Intel,nz1l8yi,oh i deadass forgot that gpu existed ngl,buildapc,2026-01-11 21:22:02,1
Intel,o0iy8sl,How much is a B580?   https://www.techpowerup.com/review/gigabyte-geforce-rtx-5050-gaming-oc/33.html,buildapc,2026-01-19 18:58:32,1
Intel,o0izjp3,Id get the 5050,buildapc,2026-01-19 19:04:22,1
Intel,o0j1zg0,Idk if its still true but doesnt the Arc GPUs hace problems running old games because of drivers?,buildapc,2026-01-19 19:15:24,1
Intel,o0iyk32,"The B570 is fantastic value for $200. Slower than a 5050, but also like 20% cheaper, and with more VRAM in games that need it. If you're looking at the RTX 5050 though, you can just get a B580 for $250, which would be slightly faster. Similar performance to a 5050, but 12GB of VRAM. So while the trade off is worse raytracing and no DLSS, you do get XeSS in the fewer games that support it, and significantly more VRAM for games that need it (including possible future release)",buildapc,2026-01-19 18:59:55,0
Intel,o0iynvi,"B570 will have Intel's Ray Tracing solution, but if you want VR, Team Green is your option...Intel's VR support is very, very limited. And you appear to have no interest in Team Red's GPUs...",buildapc,2026-01-19 19:00:24,0
Intel,o0izaat,Same price as a 5060. Shouldnâ€™t I just get the RTX by that point? Trading off RTX features for higher VRAM amount. Hhmmmmâ€¦.,buildapc,2026-01-19 19:03:11,2
Intel,o0j3zvz,So many games arenâ€™t back compatible on ps5 so I mostly want old games (and vr) so that concerns me.  Games like the saboteur and gta 4 which I own on pc but arent on modern consoles,buildapc,2026-01-19 19:24:32,1
Intel,o0q59bi,"way much improved, DX9 and DX8 problems are fixed",buildapc,2026-01-20 19:57:54,1
Intel,o0izj7i,B580 can be found for the same as the 5060 ($300) so I run into that conundrum as well,buildapc,2026-01-19 19:04:18,1
Intel,o0j0thr,"Itâ€™s not that Iâ€™m not interested in red, I just couldnâ€™t find anything comparable in price. Arc b570 for 200 and b580 for 290. RTX 5050 for 240 and 5060 for 300. These are Amazon prices (new from manufacturer). Where does AMD fit in? Everyone online says 9060xt for budget but it comes in at a higher price than any of these cards",buildapc,2026-01-19 19:10:07,1
Intel,o0j0m0f,Everyone has ray tracing these days. RTX is a nvidia model designation.  Here's some charts to help. Ray tracing only performance: https://www.techpowerup.com/review/zotac-geforce-rtx-5060-solo-8-gb/35.html,buildapc,2026-01-19 19:09:12,1
Intel,o0j4mg5,Make sure to research well because i remember Arc drivers had problems with DX9 and couldnt run DX8 (but maybe it has been already fixed),buildapc,2026-01-19 19:27:25,1
Intel,o0j2j2t,"$300 for a B580? Jeez, they're readily available for $250 or less in the US. At that price, I wouldn't get a 5060 or B580, since the 9060XT 8GB is right around $300, even if the 9060XT 16GB is a significantly better option at a higher price.  Personally, I'd get the B570. The 5050 doesn't offer enough of a performance increase for me to personally justify the price increase. And if you buy it and install it quickly enough, it should come with Intel's Holiday Game Bundle. You'll have to install the card and log into Intel's software and all that before January 31st, but if you do, you get Battlefield 6 or Assassin's Creed Shadows or Civilization 7",buildapc,2026-01-19 19:17:53,1
Intel,o0j8vca,"It's because the AMD 9060 XT is 16GB and is faster than the B580 and 5060...   Have had the B580 LE since mid Dec '24 for my Ryzen 5 5600 build, and am very satisfied with the investment.",buildapc,2026-01-19 19:47:03,1
Intel,o0q6k9t,"People.. don't often realise that ""It's only $50 more"" is usually a dumb thing to say, because the whole point of a budget is that.. that's your limit, you can't go over it, doesn't matter if something that's ""just (x) amount more"" is better",buildapc,2026-01-20 20:04:03,1
Intel,o0j1026,I care less about raytracing and more about dlss and maaaaaaybe frame gen if I can stomach the input lag,buildapc,2026-01-19 19:10:56,1
Intel,o0j7k9n,"From the light reading Iâ€™ve done, B series cards are a lot better than A when it comes to that, but then again when it comes to old games, itâ€™s a case by case crapshoot. I remember back when my 1060 was new, I tried playing that mid fps game Area 51 and it wouldnâ€™t run.",buildapc,2026-01-19 19:41:05,1
Intel,o0j4llk,"How do I find better prices? Idk what readily available means. Looking online I couldnâ€™t find one for that cheap. $290 on amazon. I built my pc back in 2016 so Iâ€™m still an amateur at this stuff.  Edit: found a B580 for $260 on B&H. ðŸ¤” Only thing is I have an Amazon credit card that gives me 5% back. But still, seems like a good price.",buildapc,2026-01-19 19:27:19,1
Intel,o0q6362,"Living in Europe, I got a similar problem  \-Both B570 and RTX 5050 are almost the same price (20â‚¬ difference)   \-People suggest B580 instead because it's only (insert amount here more)   \-It's 300â‚¬, the same price as an RTX 5060   \-People suggest any AMD card instead   \-They just straight up do not exist where I live (only 9070 or 9070XT)",buildapc,2026-01-20 20:01:48,1
Intel,o0s1vjw,"Precisely why I ended up going with the 5050. Yes the 5060 is 50 more but if im asking for budget options, obviously I donâ€™t have the money or would prefer to spend it elsewhere. Otherwise Iâ€™d just get the 5070.",buildapc,2026-01-21 01:47:14,1
Intel,o0j7u1g,"Yeah, then better to get the RTX 5050 then",buildapc,2026-01-19 19:42:21,1
Intel,o0jakuc,"https://pcpartpicker.com/list/  Use this, and sort by cheapest",buildapc,2026-01-19 19:54:54,1
Intel,o125kdx,I'd get a 16gb 5060 ti 16gb but that price gap is just too much,buildapc,2026-01-22 15:19:08,1
Intel,o0l7spv,Thank you,buildapc,2026-01-20 01:48:08,1
Intel,o0l9v08,https://www.bhphotovideo.com/c/product/1874395-REG/acer_dp_z4bww_p01_nitro_oc_arc_b580.html,buildapc,2026-01-20 01:59:19,1
Intel,o0rbmt4,"You could spend all that on ram alone, honestly your system seems pretty balanced and not in dire need of an upgrade unless youâ€™ve got fuck you money",buildapc,2026-01-20 23:21:26,1
Intel,o0rcynh,I sold my 570x/5900x/32 GB CL16 for 250 flakes back then.,buildapc,2026-01-20 23:28:39,1
Intel,o0rc58e,Thatâ€™s not my system bro thatâ€™s what Iâ€™m eyeing,buildapc,2026-01-20 23:24:13,1
Intel,o0rch8a,"Ahh sorry I misread, itâ€™s definitely balanced. For the money could you get a 9060 xt instead of the b580? It performs slightly better and has more vram if you get the 16gb version",buildapc,2026-01-20 23:26:00,1
Intel,nx5hhax,It's a good gpu for its price. The driver issues that ruined A-series seem to be gone. Just be sure to enable ReBAR: BattleMage needs to to perform best,buildapc,2026-01-01 22:20:13,6
Intel,nx5gj70,[https://www.youtube.com/watch?v=00GmwHIJuJY](https://www.youtube.com/watch?v=00GmwHIJuJY)   [https://www.youtube.com/watch?v=npIpWFSfmv4](https://www.youtube.com/watch?v=npIpWFSfmv4)  Not sure what kind of options you have where you are. 3060 is pretty long in the tooth unless your getting a smoking deal. No low priced 9060s or or anything around where you are?,buildapc,2026-01-01 22:15:11,3
Intel,nx6wr9i,At 250 its a crazy good deal. Its not top of the line but its winning competition is the fact that it beats out every other card in its weight class at its price point.,buildapc,2026-01-02 03:18:01,2
Intel,nx7n3le,Itâ€™s great,buildapc,2026-01-02 06:21:14,2
Intel,nxbtc77,"In comparison to other GPUs within its range, what would it be comparable to in terms of performance?",buildapc,2026-01-02 21:45:50,1
Intel,nxbtinv,"In comparison to other GPUs within its range, what would it be comparable to in terms of performance? Especially if the driver issues are no longer present with the more recent updates",buildapc,2026-01-02 21:46:42,1
Intel,nxbt4xl,"A bit out of my price range, even for a used one. Iâ€™m only 19 and Iâ€™m building this by myself with my own money as my first ever build. From where Iâ€™m from, the 3060 12 gb is a bit manageable price-wise, I just wanted to check out other options.",buildapc,2026-01-02 21:44:51,1
Intel,nxo75v2,"i remember hearing that the overhead issues are less of/not an issue now, due to driver updates",buildapc,2026-01-04 18:49:23,1
Intel,nxbtrn7,[similar to a 3060ti](https://www.techpowerup.com/review/intel-arc-b580/32.html) but with newer features,buildapc,2026-01-02 21:47:56,2
Intel,nxbzpsr,Fair. If your on a tight budget. Consider some of the RDNA3 AMD cards as well. I mean if your looking at a 3 generation old 3060 anyway. Something like the 7600 XT 16gb might be on the market where you are at a good price. Its probably 20% or so faster then a 3060.,buildapc,2026-01-02 22:17:35,1
Intel,o0sqvwi,"Check your GPU utilization in the game. If it's near 100%, then a GPU upgrade is the best option. But, I encourage you not to buy the RTX 3050. Try to get at least a 3060.",buildapc,2026-01-21 04:15:05,9
Intel,o0spyy8,Intel has GPUs for a while they got a pretty good budget option too (just some problems with it though) called the Intel arc b580. Anyways upgrading to AM5 is not viable as its Ramageddon so ram prices are horrendous rn. I recommend checking out the 5600xt or 5800XT CPU. As for the GPU try looking for a 9060xt 8gb or 16gb it's a pretty budget option that'll do you well. If you have a budget of around 200-300 get rtx 3060. Before all this though what's your PSU wattage,buildapc,2026-01-21 04:09:10,4
Intel,o0sray4,"If you're open to getting used, might be able to upgrade both. Ryzen 5600 are ~100-120 USD used on Ebay or on Aliexpress (or look around for a local seller on Facebook Marketplace or /r/hardwareswap) and it drops right into your board to replace the 2600. Pair that with an Arc B580 (~250, maybe less used) and you've got a pretty solid setup.",buildapc,2026-01-21 04:17:52,3
Intel,o0sr7q9,Why not find a decent 5000 series cpu and thenulgrade the gpu?,buildapc,2026-01-21 04:17:17,3
Intel,o0srxnk,What's your budget?,buildapc,2026-01-21 04:22:05,1
Intel,o0sstpt,"What is you max budget? Anyway, since the RAM price is pretty fcked nowadays, I recommend you to stay on AM4 platform and just upgrade the CPU instead of straight away go to AM5 or Intel DDR5 equivalent  Unless you can only afford 3050 or 1660 Super, you'd better with something like 3060 12GB or something else",buildapc,2026-01-21 04:28:03,1
Intel,o0ssu8o,What speed is that RAM?,buildapc,2026-01-21 04:28:09,1
Intel,o0sunuq,"Keep the Ryzen 7 2700, B450 mb, 32g ram and get a GPU. You said the Intel B570 but for a little more you can get Intel B580 if your PSU and MB support it. You also said I5 14400 but then you would need another Motherboard.",buildapc,2026-01-21 04:40:28,1
Intel,o0sw0l8,"hey brother I have the same build (except I use Strix X470-I Gaming mobo and 16GB RAM). I am upgrading to a 5700x, a 9060 XT 16GB and 64GB of RAM.",buildapc,2026-01-21 04:49:44,1
Intel,o0syori,"Any 5000-series CPU would be a *massive* upgrade, howeverâ€¦ GPUs specifically will see massive price hikes & shortages in the next couple months. So it might actually be the last time you can snag a cheap used GPU.  Forget the 3050, a 3060Ti is an excellent value under $200. If you can find a 12GB version, youâ€™ll be especially glad you did. Iâ€™d recommend a 4060 for the same price range, but your motherboard is most likely PCI gen 3, so a 30-series card is your best bet. With the latest DLSS updates, Iâ€™d personally stick with RTX, especially with a lower-power card.",buildapc,2026-01-21 05:08:24,1
Intel,o0sthaj,That or finding the best priced used 30 series card.,buildapc,2026-01-21 04:32:29,3
Intel,o0sq4jb,600 watts,buildapc,2026-01-21 04:10:11,2
Intel,o0w6651,3060 ti is sub $200 used. Not a bad upgrade. He can get a used 6800 (non XT) for sub $300,buildapc,2026-01-21 17:48:45,1
Intel,o0sryit,Because I am dumb lol. This is the better option.,buildapc,2026-01-21 04:22:14,1
Intel,o0ssovk,250 or lower.,buildapc,2026-01-21 04:27:08,1
Intel,o0st5ye,250 or lower is my budget.,buildapc,2026-01-21 04:30:21,1
Intel,o0st2c1,3600 I think?,buildapc,2026-01-21 04:29:39,1
Intel,o0sq9v7,Yeah that's sufficient,buildapc,2026-01-21 04:11:08,2
Intel,o0st413,Used GPU for $150-180.  Used CPU for under $100.,buildapc,2026-01-21 04:29:59,2
Intel,o0ssu3s,Thank you! :D,buildapc,2026-01-21 04:28:08,1
Intel,o0mvz08,"RTX 5060 is a better choice for Arc Raiders than RX 9060xt 8GB, however, RX 9060xt 8GB is a better GPU on average.   So up to you, what do you prefer.",buildapc,2026-01-20 08:41:17,32
Intel,o0mwrxk,Why they downvote you? Lol,buildapc,2026-01-20 08:48:53,5
Intel,o0mvm0d,9060xt is comparable to the 5060ti so it really doesn't make sense as to why you should go 5060 unless it's for frame gen or DLSS4.5,buildapc,2026-01-20 08:37:51,11
Intel,o0pko9m,Do not buy an 8GB card in 2026. Just don't.,buildapc,2026-01-20 18:24:21,3
Intel,o0ono8g,Are the Intel cards available in your region? What's their price?,buildapc,2026-01-20 15:52:27,2
Intel,o0mvzzo,Rx 9060 any time,buildapc,2026-01-20 08:41:33,4
Intel,o0nw428,Avoid 8gigs 9060xt at all costs!,buildapc,2026-01-20 13:31:07,2
Intel,o0oo9xq,5060 is a bit better in Arc Raiders however I'm not sure it's worth going over your budget that much.,buildapc,2026-01-20 15:55:14,1
Intel,o0mwoy2,"FSR 4 is essentially unusable at 1080p, while DLSS 4.5 performs well. For that reason alone, Iâ€™d choose the 5060 if you can afford it.",buildapc,2026-01-20 08:48:07,1
Intel,o0mwh1y,"Hello, if the rx 9060xt  is cheaper get it  They are basically the same card.  Check out this benchmark:  https://youtu.be/xlbNsP5ySmA?si=XowPsOGMbrnOX1aD   Between the 9060xt 8 gb and the rtx 5060 you should allways get the cheaper one.",buildapc,2026-01-20 08:46:03,0
Intel,o0n7jz9,"If you plan on using Linux down the road, the RX 9060XT might be the better option.",buildapc,2026-01-20 10:30:10,1
Intel,o0nki3i,Where are u from ? They sell 9060xt 16gb for 380$ in iraq id assume the price is similar in such countries,buildapc,2026-01-20 12:16:22,1
Intel,o0mwkp8,5060 is faster for arc raiders natively and considering the feature advantage I would def go for it.  Although keep in mind if you go over the 8gb vram limit the 9060 xt will handle that better and is faster on average just not in arc raiders,buildapc,2026-01-20 08:47:01,0
Intel,o0mz5ji,Get the 5060! It has a better feature set like better upscaling and better Ray Tracing! You also don't have to deal with crappy AMD drivers which I have experienced and many AMD RX 9000 series users are also experiencing! You have better peace of mind with the Nvidia GPU!,buildapc,2026-01-20 09:11:22,-1
Intel,o0mwq90,"From what I've heard arc raiders heavily favors Nvidia side of things, look up some testing with the either card. If that's all you play then 5060 will be the better choice.",buildapc,2026-01-20 08:48:27,0
Intel,o0mwv7m,How much is a second hand 4060?,buildapc,2026-01-20 08:49:45,0
Intel,o0on4qj,"Go Nvidia but idk, is it possible for you to get 16gb? 8gb works perfectly fine in Arc Raiders, but 0 guarantee for other games, for example hogwarts or mafia are really struggling.  8gb can be a real gamble.",buildapc,2026-01-20 15:49:56,0
Intel,o0mxpsa,9060 XT 16gb,buildapc,2026-01-20 08:57:42,-2
Intel,o0mxb1l,B580,buildapc,2026-01-20 08:53:53,-1
Intel,o0mwu45,"The difference between them is minimal and switches depending on the game.  He should get whatever is cheaper, in this case the 9060xt*  https://youtu.be/xlbNsP5ySmA?si=XowPsOGMbrnOX1aD",buildapc,2026-01-20 08:49:28,17
Intel,o0n4ybl,Leaning towards the 5060 since I only see myself playing Arc for a while now. $50 difference to the 5060 is easier to stomach for me than the $200 for the 16gb 9060.,buildapc,2026-01-20 10:06:22,4
Intel,o0n691o,"Tell me about it. Probably the ""save up 200 and get the 16gb card"" crowd. Even if I do save it up, I really wouldn't want to spend 60% more for just a 20-30% performance difference :/",buildapc,2026-01-20 10:18:20,7
Intel,o0mwc34,Stop spreasing misinformation 9060xt 8gb compares to the rtx 5060 and you should allways get the rtx 5060 if it is at the same price or cheaper than the 9060xt 8gb.  The difference between them is minimal and fluctuates depending on the title ( amd or nvidia optimised )  https://youtu.be/xlbNsP5ySmA?si=XowPsOGMbrnOX1aD,buildapc,2026-01-20 08:44:45,-8
Intel,o0n52ku,"I'm seeing that the 5060 is better for arc raiders too, so leaning towards that now.",buildapc,2026-01-20 10:07:27,2
Intel,o0n5odd,"Thank you! That video was super helpful. Given he says the 9060 is only about 5% more powerful on average and the difference is more stark in Arc Raiders specifically, I'm more inclined towards the 5060 now. The $50 difference isn't too bad and I'm hoping to find better deals than $365.",buildapc,2026-01-20 10:13:05,2
Intel,o0nmvzj,"Yeah, that makes sense, but I just uninstalled bazzite a couple of weeks ago. It just wasn't for me. Found myself switching to windows more often than not.",buildapc,2026-01-20 12:33:12,2
Intel,o0nmz31,India. It was ~$380 a few weeks ago. I guess I missed the bus :/,buildapc,2026-01-20 12:33:47,3
Intel,o0n59ml,Thank you. I'm leaning towards the 5060 now.,buildapc,2026-01-20 10:09:16,1
Intel,o0n5xj5,"Thanks, I am leaning more towards the 5060 after seeing the replies here and considering the fact that it's the better gpu for the one game I play these days. I just wanted reassurance that the $200 difference for the 16gb model wasn't worth it, and I think I got it.",buildapc,2026-01-20 10:15:24,3
Intel,o0mzp5m,The 5060 is too slow for RT to even matter and the AMD driver thing is a complete myth in 2026. You might have had a point 10 years ago but you donâ€™t anymore.,buildapc,2026-01-20 09:16:33,4
Intel,o0n5buj,"It is pretty much all I play for now. And thanks, the 5060 looks like it's worth the extra 50 for me. Hopefully I can find better deals.",buildapc,2026-01-20 10:09:51,2
Intel,o0n57be,"Hard to find here, but they usually go for 250 or do. Used market is not the best here.",buildapc,2026-01-20 10:08:41,1
Intel,o0n5cpm,It's $200 more.,buildapc,2026-01-20 10:10:04,1
Intel,o0mxjou,Its worse than both at 1080p,buildapc,2026-01-20 08:56:07,1
Intel,o0mx7dn,">The difference between them is minimal and switches depending on the game.  True, but 9060xt 8GB is a bit better and cheaper.  >He should get whatever is cheaper, in this case the 5060  5060 cost 50usd more for OP.",buildapc,2026-01-20 08:52:58,10
Intel,o0o6tlp,"The 9060XT is the stronger card. At 1080p don't even think about the 16Gb. Traditionally, AMD updates their cards and the performance grows more and more over time. Personally, I'd get the AMD card. The ""benefits"" you get from Nvidia cards are useless at a 5060 level of performance. The pure raster advantage of the 9060XT will last you longer",buildapc,2026-01-20 14:29:18,2
Intel,o0s1fkb,Arc Raiders came out 3 months ago. You're going to own that GPU for years. Buying a GPU for one specific game seems like a bad idea.,buildapc,2026-01-21 01:44:42,1
Intel,o0nev09,You will massively regret buying an 8gb card if you ever want to use a higher resolution than 1080p. The 9060xt 16gb is worth it. Better to save up and put off the purchase than to buy something you will be disappointed with.,buildapc,2026-01-20 11:33:22,0
Intel,o0pl366,"There's a very good reason to not get an 8GB card though. It's just not worth it at this point in time. Its longevity is severely limited, do yourself a favour and just avoid any 8GB graphics cards.",buildapc,2026-01-20 18:26:14,3
Intel,o0nf7th,Its not a 20% perfomance increase. It is the difference between 10fps and 80fps when you go to 1440p or your 4k TV.  See https://youtu.be/7LhS0_ra9c4?si=z9VGzFIq3qbLEMc_,buildapc,2026-01-20 11:36:16,5
Intel,o0mwy6f,Seems I've been a victim of cherry picking then my bad I've just ran with it from what I've seen with most content creators. Still 9060xt is cheaper for them so they should get it,buildapc,2026-01-20 08:50:32,6
Intel,o0mzwga,"I'm surprised how close the 5060 is to the 9060xt, 8gb. The both play games well at 1080p so most people will be happy with either and yes vram an issue but I'd you don't play many AAA games this won't be an issue, we also can't ignore ram shortage this will affect consoles etc in the future.",buildapc,2026-01-20 09:18:29,0
Intel,o0n5sjw,"Im glad i could help, the advantage of going Nvidia is DLSS, it is superior to FSR and it is implemented in way more games.  It will serve you well, i also own it and i like it",buildapc,2026-01-20 10:14:10,1
Intel,o0nghl4,The 16 GB model will be the 5060 Ti if I am not wrong! The 5060 Ti is around 15-16% faster than the 5060 but if you can afford the 5060 Ti 16GB then I would suggest you to surely get it as it will be relevant for much longer in the future and Nvidia is going to stop production of the 16gb 5060 ti this year going from the rumours!,buildapc,2026-01-20 11:46:16,1
Intel,o0n06g6,No AMD drivers are still buggy! You can just check the r/AMDHelp and r/Radeon subreddits and there are multiple new posts everyday with complaints about driver timeouts or crashing or something else! I know from experience too! Just check the subreddits and you won't be disappointed with the number of complaints everyday!,buildapc,2026-01-20 09:21:11,-4
Intel,o0n66j8,"I couldnt play any God Of War games on my 2 different AMD cards because of awfull bugs which caused the fps to drop to 30 no matter the video settings ( rx 5700xt and 6650 xt) amd absolutely still has driver problems.  It is a common issue with amd card and god of war games ( you can look it up ), and it is insane that amd doesnt care enough to fix it. And there are more similar situations  Also in some games the 5060 can do rt, i use it in crysis 2 remastered and i get above 80 fps without upscalling, you can also use it in cyberpunk but dlss is required",buildapc,2026-01-20 10:17:43,-1
Intel,o0n83p4,250 compared to 350+ seems a decent deal tbh,buildapc,2026-01-20 10:35:08,1
Intel,o0nal89,Is it really? There's just a 50 dollar difference in my country.,buildapc,2026-01-20 10:57:08,1
Intel,o0mxm6b,How ?,buildapc,2026-01-20 08:56:46,2
Intel,o0mxagt,"My bad i misread the post, i tought the 5060 was the  cheaper one, he should get the 9060xt",buildapc,2026-01-20 08:53:45,4
Intel,o0nknfa,But did he ever mention wanting to upgrade to 1440p?,buildapc,2026-01-20 12:17:26,2
Intel,o0ziv3r,Okay what about a 7700 xt for ~$380? (Keep in mind the 9060 16gb is ~500 here).,buildapc,2026-01-22 03:54:01,1
Intel,o0mx2qa,"Oh so true i tought the 5060 was the cheaper one ,he should get the 9060xt",buildapc,2026-01-20 08:51:44,1
Intel,o0n014n,At 1080 both of them can play absolutely any AAA game without any problem ( medium is needed in indiana jones ) but the rest can run at high,buildapc,2026-01-20 09:19:46,1
Intel,o0n6q1c,"Yes, that still seems to be the consensus very much. Better upscaling and drivers.",buildapc,2026-01-20 10:22:37,2
Intel,o0nompx,"The 5060 Ti 16 GB is ~$630 here. Think the 9070 (non xt) would give better performance for that price range ($650 where I live). The pricing ladder in my market, I tell you! ðŸ˜…",buildapc,2026-01-20 12:45:01,1
Intel,o0n0jsr,I own one buddy. I have never had a single issue with my 9070xt. The only people posting to this subreddits are people having issues that the overwhelming majority of people do not have. Itâ€™s biased by the nature of what it is .   If amd gpus had driver issues you would see big publications talking about them but you donâ€™t. You are propagating a myth.  Youâ€™re also conveniently ignoring the disastrous 50 series launch drivers for NVIDIA.,buildapc,2026-01-20 09:24:45,3
Intel,o0n71rv,"Curious - in what kind of situations does the 5060 run into vram limitations in your experience with it? If I play something like Arc Raiders at 1080p at high (second highest), would I ""run out of vram?""   I'm afraid I still don't understand how that works entirely. Have you had such instances with the 5060?",buildapc,2026-01-20 10:25:37,2
Intel,o0n9jp2,"Yeah, but super hard/rare to come by. They get snagged up pretty fast too.",buildapc,2026-01-20 10:47:59,1
Intel,o0nee7u,"Yep. $200 difference between the cheapest variants of both here in India. If it were a $50 difference, I wouldn't have even made this post.",buildapc,2026-01-20 11:29:36,1
Intel,o0mxnu1,https://youtu.be/xlbNsP5ySmA?si=XowPsOGMbrnOX1aD  You can watch this video to convince yourself,buildapc,2026-01-20 08:57:12,1
Intel,o0np380,Yes he is interested in using his 4k TV at lower settings and framerate. It will not be playable with a 8gb gpu.,buildapc,2026-01-20 12:48:01,0
Intel,o0n77gz,"Yeah i also moved to nvidia even though it is not allways the best price for performance because my last 2 AMD cards ( 5700xt and 6650xt ) could not run God Of War at all no matter the video settings or resolution, the game kept dropping to 30 fps, and it is a common issue with Amd and god of war games and they still have not fixed it.  There are more issues like this, some are fixable if you google the solution, some are not and after the thing with god of war i decided to give up on amd for now, they also didnt add FSR4 to previous gen GPU s ( like the 7000 series) , while Nvidia gave DLSS 4.5 even to the first RTX cards.",buildapc,2026-01-20 10:27:04,1
Intel,o0nva4w,Then stick to 5060! For almost double the price it is not worth it!,buildapc,2026-01-20 13:26:17,1
Intel,o0n0ucf,You are welcome to make your assumptions and share your own experience but I can only speak based on my experience with them and what I observe from my reddit scrolls! I switched to the Green Side and won't be going for a new Radeon GPU anytime soon.,buildapc,2026-01-20 09:27:31,0
Intel,o0n7m73,"You can play Arc Raider at max settings and you wont run out of Vram, personally  i never ran out of vram because i dont allways play the latest games ( i play at 1080p )  In the Game Indiana Jones ( which is a very bad optimised game )you have to set the settings to medium to not run out of vram and i think in the new Doom you have to use high, thats it, in some future released games you might have to use high/medium instead of ultra.  When you run out of vram the game gets very bad frame drops.  I play Cyberpunk on a mix of high and very high i think and i dont have any issues with vram for example, same for BF6.  At 4k resolution the vram issues appear in newer titles",buildapc,2026-01-20 10:30:43,-1
Intel,o0nbkmg,At 1080p you will almost never run into a VRAM issue and Nvidia cards are also more efficient in VRAM utilisation! They are also much more power efficient i.e. they will consume less power for the same level of performance which saves some electricity while also giving your power supply more headroom!,buildapc,2026-01-20 11:05:47,-2
Intel,o0pxems,Can you go find a 5700xt and run that until you've saved for a  16GB card?  If you ever wanna move to the TV or even a better monitor 8GB is gonna be out gunned real fast.,buildapc,2026-01-20 19:21:26,0
Intel,o0nulfm,"Oh right yeah ðŸ˜…  I donâ€™t even own a TV, that wouldnâ€™t cross my mind.",buildapc,2026-01-20 13:22:16,1
Intel,o0qi7lp,"Haha what, more power efficient?",buildapc,2026-01-20 20:57:55,1
Intel,o0zin1x,Not available here. What about a 7700xt for ~$380 (vs 9060xt 16gb @~$500) or a 7600 xt for $270 vs (5060 @~$500)?  Used market is not ideal here. They're either overpriced or they get snapped up in minutes.,buildapc,2026-01-22 03:52:37,1
Intel,o0nxfgi,"Regardless, the 8gb models on some modern games run out of VRAM at 1080p. I assume most spend hundreds of euros to have a useful product for several years, not just a single.",buildapc,2026-01-20 13:38:35,3
Intel,o0ol6ja,"Yeah true, thatâ€™s why I grabbed a 5060 Ti 16 GB last month myself :â€™) But also I didnâ€™t wanna downgrade in vram from my 3060 12 GB.",buildapc,2026-01-20 15:40:47,1
Intel,o0zj6mo,"Sigh, how I wish I could get a 5060 ti. That's ~$600 here. How about a 7700xt for ~$380? (Keep in mind the 9060 16gb is $500 here)?",buildapc,2026-01-22 03:56:02,1
Intel,o110qw6,Well you could use Optiscaler to get FSR 4 on older Radeon cards and the 7700 XT comes with 12 GB of VRAM which is definitely better than 8 GB. I donâ€™t see why not.,buildapc,2026-01-22 11:18:12,1
Intel,nyk2zwg,">30c ish at idle to 60-80c under load.  That sounds normal my guy intel i9s are well known power hogs, the 200-300w that thing consumes will generate that kind of heat. Most people with that cpu run it with a 360mm aio to keep the temps in check.  Id suggest getting a 360mm rad.  [https://www.pcgamer.com/intel-core-i9-12900k-review-benchmarks-performance/](https://www.pcgamer.com/intel-core-i9-12900k-review-benchmarks-performance/)",buildapc,2026-01-09 07:27:00,9
Intel,nyk4qvy,"Those temperatures could be totally normal for the 12900kf / 240mm, depending on your room ambient temp and your case air flow. TjMax for that CPU is 100c as well, so running at 80c is totally safe and won't even throttle.",buildapc,2026-01-09 07:42:23,3
Intel,nyk22lk,"If the AIO screw has springs, you don't need to force it.. Check if paste covers whole surface and if you got the stock cooler try using it if not get a cheap cooler",buildapc,2026-01-09 07:18:55,1
Intel,nyk23cs,The pump is set at a certain speed. Can you go to bios and bump up the pump speed to check if that resolves anything?,buildapc,2026-01-09 07:19:06,1
Intel,nyk46lk,"Start with setting up your PL1/2 for the capacity of your AIO. If it is set to standard, 4096W, you are going to have problems cooling that thing. Start with 253/253 standard Intel would be 125/253/304A and keep lowering that until there is no more thermal throttling. After that you can fine tune with undervolt etc.  And ofcourse check AIO mounting, pump, fluid.",buildapc,2026-01-09 07:37:26,1
Intel,nyk88dk,For both 12th and 13th gen you can do the following:  - turn hyperthreading off - enable only 8 ecores - Set a negative voltage offset of - 0.050  Play games for a couple of hours if no crashes then decrease offset by another 0.025 (I.e. 0.075). And keep going till you find the lowest voltage without crashes. This will increase 1% lows tremendously in games too.   Once you're stable and if temps are still too high (thermal throttle) then you can reduce clock speed by 100hz and keep doing it till you're happy. Reducing by 200mhz doesn't hurt gaming performance.  Benchmark a CPU limited game (or make it CPU limited by lowering the resolution) before you start so you can see the gains.,buildapc,2026-01-09 08:13:22,1
Intel,nykdz7n,not a single mention of voltages?,buildapc,2026-01-09 09:05:28,1
Intel,nyk4j60,"I ordered a new case and a 360mm AIO which is supposed to be here tomorrow, since my current case doesn't support a 360 on the top, I plan to move everything over sometime this weekend. I was just wondering if there could be a different issue because the temps were rising instantly even with the AIO supposedly running.",buildapc,2026-01-09 07:40:30,0
Intel,nyk6ox6,"My ambient temperature is about 23c. This might be a dumb question, but do different CPUs have different safe temperatures? Everything I was reading was saying, generally, anything above 80-85c is risking permanent damage to the CPU.",buildapc,2026-01-09 07:59:35,0
Intel,nyk2hvo,Try mounting your AIO higher too.,buildapc,2026-01-09 07:22:36,1
Intel,nyk3yu1,"My pump control doesn't show up in the bios, saying it's at 0 rpm, but using corsair's software, it says it's running at about 2800 rpm with the extreme setting.",buildapc,2026-01-09 07:35:34,1
Intel,nyk84k2,"I'm not sure what PL1/2, 253/253 or 125/253/304A mean. I'll look up each of those unless you're willing to explain them in more detail.",buildapc,2026-01-09 08:12:24,1
Intel,nyka6tl,Okay thanks I'll give this a shot.,buildapc,2026-01-09 08:30:58,1
Intel,nyk68kd,"All cpus do that, remember electrical signals move at the speed of light but heat does not, so cpus can start doing work and heat up/heat down in nanoseconds.",buildapc,2026-01-09 07:55:36,2
Intel,nyk89gv,"They sure do, it used to change quite a bit with different CPU generations in the past. [Here is the official intel page for the 12900kf](https://www.intel.com/content/www/us/en/products/sku/134600/intel-core-i912900kf-processor-30m-cache-up-to-5-20-ghz/specifications.html), max temp of 100c is specified under ""package specification"" (TJUNCTION). You are definitely not risking any damage at 85c.",buildapc,2026-01-09 08:13:39,2
Intel,nyk5218,"I don't think the mount has springs, just a backplate for the motherboard and the standoffs that screw down the pump head. What do you mean mounting it higher? Higher in the case? I currently have it mounted at the top of my case.",buildapc,2026-01-09 07:45:08,1
Intel,nyk8ii4,"Do not know your mainboard, so canâ€™t help you. But Pl1/2 are the and short duration of max powerdraw so 253short 125W long with a max current draw of 304A. Basically the values you would need to cool.",buildapc,2026-01-09 08:15:55,1
Intel,nykgrws,Let us know results.,buildapc,2026-01-09 09:31:31,1
Intel,nyk7lpd,"Right, but I was expecting the AIO to keep the temperatures from jumping instantly to 90 or 100 at around 60% load.",buildapc,2026-01-09 08:07:43,1
Intel,nyk97ex,"Okay thanks, I'll do some research.  I have a MSI Z790 Gaming Pro WiFi motherboard.",buildapc,2026-01-09 08:22:05,1
Intel,nyqm5ak,"Thanks for the suggestion, I was able to get ARC Raiders running today for a couple of hours mostly below 80c (68-78ish) with a -0.100 offset with no stability problems so far. I haven't found the point where it becomes unstable, so I probably could decrease it more to get the temps even lower if needed.",buildapc,2026-01-10 05:29:36,1
Intel,nyqncbg,"My 14900k can do - 0.125 and stable. I'm sure yours can get there too, possibly beyond.",buildapc,2026-01-10 05:38:18,1
Intel,nza5qxr,"Do you happen to remember the windows stop code you were getting when yours  was unstable? I thought everything was running fine at -0.100, but I've had 3 CLOCK\_WATCHDOG\_TIMEOUT crashes when loading into a match of Arc Raiders and have since reduced the offset from -0.100, -0.090, -0.085, to now -0.080. Just concerned that I might be damaging my cpu.",buildapc,2026-01-13 02:35:38,1
Intel,o1i4wup,Id go for the 5700x over the 5600g. Even if you are paying a little more it'll last longer.,buildapc,2026-01-24 21:45:55,2
Intel,o1iily2,"If you are on tight budget, go for 200 usd option. 5700x build is at good price but it is a significant 100 usd more expensive.",buildapc,2026-01-24 22:52:15,2
Intel,o1j4279,That's what I ended up going with. The 5700x sold before I could jump on it.   The 5600g guy threw in a case and a mouse for $165 total. Got a CPU fan for $30 and I'm golden. Whole ass used system for under $200 (plus my old components).,buildapc,2026-01-25 00:43:25,1
Intel,o11ri3h,"The GPU will be a huge limiting factor, struggling with almost any modern game. Maybe some will run at 1080p low settings, but definitely not the newest titles. But otherwise it's fine.",buildapc,2026-01-22 14:08:52,9
Intel,o11ry18,Nothing wrong with using an older gpu and just waiting to upgrade 9600x is the same price iirc,buildapc,2026-01-22 14:11:10,5
Intel,o11tl1l,"8gb ram is pretty low.  I think you need 16 minimum, most aim for 32.  As others said the gpu is very old and will slow you down.",buildapc,2026-01-22 14:19:39,3
Intel,o11ve7n,Don't buy one 8gb stick of the the slowest ram available   I know its expensive but just wait rather that throwing that much at really poor ram,buildapc,2026-01-22 14:28:56,3
Intel,o11sl58,I wouldn't bother building until you have a decent GPU available. I have a laptop with a 980M and it has finally gotten to the point that it is no longer supported by some games at any setting.,buildapc,2026-01-22 14:14:30,2
Intel,o11tw10,"You are going to buy this? Why would you do that when you could get an older processor, mobo and ram so you can afford an actual usable gpu? No offense. This is just bizarre to me personally",buildapc,2026-01-22 14:21:14,2
Intel,o12pjja,"Go for AM4 or LGA1700 so you can use DDR4 instead of the insanely expensive DDR5. Then sell the stick you have now and reap the profits, or keep it for if you ever decide to upgrade in the future to a DDR5 platform. DDR4 is also still just a more stable technology in general in my experience. Older platform boards and chips will also be cheaper than AM5, while still enabling you to build a massively overpowered system for your use case. You can also put that money saved into other things in your life, or into a more powerful GPU like a B580 or 9060XT.  For example, instead of a Ryzen 7600, I'd recommend a 5700 for about the same price. Somewhat slower individual cores, but there are more of them (you're unlikely to notice the difference with the games you play), and your board and RAM will be a lot cheaper (you can also get much better memory than that for the price with DDR4, that DDR5 you have is bottom of the barrel stuff meant for office PCs, not for gaming). A nice Gen 4 SSD isn't too expensive still, and then you can use the difference for a better GPU, or as I mentioned before, to put to use elsewhere.  Also, if you use a good board with a second x16 PCIe slot, you can use that RX480 as a Lossless Scaling frame generation card. Or render the game on the 480 and use the 1050ti as a frame generation card.",buildapc,2026-01-22 16:50:00,2
Intel,o11vfoy,"Honestly, those boot times and random freezes scream failing HDD. Your old drive is likely on its last legs, and thatâ€™s probably whatâ€™s killing your internet connection too when the OS hangs.  About the new build: Going AM5 with the 7600 is a smart move. Itâ€™s a massive jump. But since youâ€™re buying DDR5, try to get a 6000MHz kit instead of 4800MHz. Ryzen 7000 series really needs that extra speed to shine, and 4800MHz will actually hold your CPU back more than you think.  Also, just a heads-up on the PSU: that CiT FX Pro is pretty bottom-tier. Since youâ€™re buying a nice ROG Strix board and a modern CPU, Iâ€™d spend a bit more on a reliable power supply (like a Corsair or Seasonic) just to be safe.  Swap to that RX 480 for nowâ€”itâ€™ll definitely beat your 1050 Ti while you save for a modern GPU. Once you install Windows on that NVMe SSD, the PC is going to feel like a rocket ship compared to your old OptiPlex",buildapc,2026-01-22 14:29:09,1
Intel,o1238iw,Nothing wrong with that at all.  A nice GPU upgrade that's reasonably priced would bea Radeon 5700XT.  Ebay has them for $150 or so and it's still a very capable card.  Don't get too worked up about biggest and baddest. Only thing beside the GPU I would focus on would be RAM I'd want at least 16 and probably 32,buildapc,2026-01-22 15:08:00,1
Intel,o124flp,she will game,buildapc,2026-01-22 15:13:44,1
Intel,o11wur2,"Iâ€™ve run every modern game on an i7-9700/RX570-8GB. Mostly to prove I can but arc raider, expedition 33, vein, Elden ring, and many others achieved 70-80 fps at 1080p",buildapc,2026-01-22 14:36:33,3
Intel,o11x1y9,"I've played most of the newest titles on it tbh, they do all work, just at very low settings but I get 60 fps. I play Arc raiders a lot and that works quite well and doesn't look too bad.",buildapc,2026-01-22 14:37:36,1
Intel,o11wfyf,Is the 9600x better?   yeah I'll upgrade the GPU end of this year but in my head its 420 pound for a new pc or 350 for a new graphic card.,buildapc,2026-01-22 14:34:24,2
Intel,o11vyxg,"Yeah I was thinking that, with the price of ram right now, I'm going to get 16gb and keep an eye out for cheaper sticks.",buildapc,2026-01-22 14:31:55,2
Intel,o12j2fq,"I disagree here. A good strategy / upgrade path I've used over the years is to sink around $1000 into a quality build minus graphics card, then 4/5 years later sink around $300/$400 into a decent current-gen graphics card. Then a few years later repeat the process (using the old graphics car with the new build.)   This obviously won't work if you want to play AAA titles on ultra settings, but for those of us without thousands to burn on gaming it's a good plan if you don't mind turning down settings a little.",buildapc,2026-01-22 16:21:18,2
Intel,o11xvhq,"Its just because I don't want to have to upgrade twice my gpu works fine on most things at low settings but the daily tasks like word and Discord it struggles on quite a bit. and my internet speed is stuck at 10mbs as the motherboard has a bug in it what intel never fixed.  I was going to upgrade the GPU later in the year anyways, this is like in the next month purchases. sorry didn't make that very clear.",buildapc,2026-01-22 14:41:46,2
Intel,o1208sm,"Sorry I didn't make it clear, my 9070 does have an ssd (swapped it over last year) but it didn't make a difference on boot times really at all. a bit annoyed as I went through the process of copying my data to it and I fucked it up and lost everything lol.     Good thinking, the PSU is just what I brought middle of last year for my current pc has the PSU in that one died and I had no money lol. I'll keep that noted thank you  Yeah the pc is going to be a bit WIP, my current pc is dying so just trying to scrap parts together in the next month before it dies. saying I only need to buy the motherboard and cpu now and I'll have a useable pc, the rest I can just purchase in the next couple of months.  Anything is better than a pc that randomly just kicks you off the internet and crashes on discord lol.  Thanks for the help tbh, I was a bit worried about if it would even be worth it.",buildapc,2026-01-22 14:53:26,2
Intel,o11x014,"Iâ€™ve run every modern game on an i7-9700/RX570-8GB. Mostly to prove I can but arc raider, expedition 33, vein, Elden ring, and many others achieved 70-80 fps at 1080p  Edit: not that that is apples to apples. But OP may just need a small upgrade.",buildapc,2026-01-22 14:37:19,3
Intel,o12homo,"Yes - I was going to reply that I am currently running Arc Raiders (such a great game!) on a GTX 960 on medium settings, and it runs perfectly with no issues.  Definitely limited in that you can't run 'ultra' settings, but to say ""struggling with almost any modern game"" is a bit of an overstatement.",buildapc,2026-01-22 16:15:07,1
Intel,o12mj74,7600 and 9600x are the same price so you might as well go for the 9600x unless you find a good deal or something or like a cheap 7500f,buildapc,2026-01-22 16:36:36,3
Intel,o12m6eu,"It's just one generation newer. Slightly better performance, but uses the same socket as the 7600. It the prices are close, definitely get it.",buildapc,2026-01-22 16:35:02,2
Intel,o13eyo9,"8 to 10 years between upgrades? Ouch. You are leaving a ton of performance on the table by doing that, especially when you are talking low-end GPUs which is what that price point is, these days. Still, you can only afford what you can afford. The key thing is that you are happy with what you have. If your method works for you and you're happy, then awesome!",buildapc,2026-01-22 18:43:36,2
Intel,o12qcnf,"For your use case, a good AM4 system with Zen 3 will last you for a long while. Especially if you can find an X3D for a reasonable price, but that's unlikely unless AMD starts selling them again.",buildapc,2026-01-22 16:53:39,1
Intel,o135d2t,Embark are the goats of optimization. The fact that the finals is playable on a ps4 is nuts,buildapc,2026-01-22 18:01:21,3
Intel,o13hkoz,"You're definitely not wrong! I'm a very patient gamer though, and most often games I am buying (unless multiplayer, which I rarely play anyway) are anywhere from 3-10 years old, so for me this method is perfect. I could spend more, but with 3 kids in college, 2 mortgages and lots of hobbies it just ain't gonna happen LOL. Cheers!",buildapc,2026-01-22 18:55:06,2
Intel,o18a4v1,"I mean it takes me 5 minutes longer than my friends to load in but it does work really well, runs better on my pc that fornite",buildapc,2026-01-23 12:52:15,2
Intel,o18ancv,Very true - same with phones I upgrade every 5 years to appreciate how fast the new tech can be.  And yeah my graphic card was old when I brought it 6 years ago. Iâ€™ll rather have a faster start up now tbh if I brought a graphic card it would be held back by my pc anyways,buildapc,2026-01-23 12:55:30,2
Intel,nw5kwmv,"If the B580 has good performance in the game that you are interested in then it can be a good option. However, in some games there are driver overhead issues. So you cant trust every benchmark with a 7800x3d/9800x3d to estimate the GPU performance. In some games performance is reduced with a Ryzen 5 5600 for example. If this has been fixed in your games (or you have a stronger CPU) then the b580 is a good choice.",buildapc,2025-12-27 07:01:10,2
Intel,nw5kpu1,The b580 is just the overall better card. Intel may be new to the dedicated graphics market but they have been making their own GPUs for almost 20 years at this point and have the experience of supporting them through driver updates for many years,buildapc,2025-12-27 06:59:28,1
Intel,nw5kzt8,b580 cause vram,buildapc,2025-12-27 07:01:58,1
Intel,nw5na11,"How much is the 8GB 9060 in your country?  But regardless, the B580 is better for simple gaming if you can pair it with a strong CPU.  The RTX 4060 is better outside of gaming, but also works better with a weaker resolution. DLSS also helps with higher resolutions.",buildapc,2025-12-27 07:23:15,1
Intel,nwgazqm,"Thanks everyone for the help, I really appreciate it!",buildapc,2025-12-28 23:44:04,1
Intel,nw5o3fl,> B580 is better for simple gaming if you can pair it with a strong CPU  i pair the b580 with the 225f which is an entry level cpu and they work well. gpu 100% and cpu 33% usually when gaming,buildapc,2025-12-27 07:31:05,1
Intel,nwgaunz,Sadly it costs $350 here. I considered that too but it's out of the budget. Btw thanks!,buildapc,2025-12-28 23:43:18,1
Intel,nw5obdk,"The 225F is a very current CPU, there are many people who will try to pair this with something like an i3-6100 or Ryzen 1400.",buildapc,2025-12-27 07:33:12,1
Intel,nz7be98,Used rtx 3060ti for 250 or less. Fantastic value card now a days.,buildapc,2026-01-12 18:09:33,3
Intel,nz798wq,"So the 9th gen is a bit of a weird CPU right now, you do have a couple of decent upgrade options, but also you cannot go past the 9th gen without a new motherboard. Also the 9th gen is quickly coming up on 6 years old now as well.   My affordable advice would be to grab a 9700k and a better CPU cooler, then you can start to worry about the GPU which is very PSU dependent anyways. Without knowing the PSU I cant really make a suggestion anyways.   But the truth is, I would almost start fresh. New mobo, cpu, PSU and GPU. It would be more pricy, but also a far better use of your money than dumping it into a system that will be too slow to run new games in just a year or two.",buildapc,2026-01-12 17:59:50,2
Intel,nz78fo5,"I mean, really both kinda. 6500 XT is a notorious shite GPU but the games you play are CPU bound so a new CPU would do wonders. Budget is needed here.",buildapc,2026-01-12 17:56:11,1
Intel,nz7kn04,"I have the ASRock H310CM-HDV with an i5-9400. The beta BIOS will add Smart Access Memory (a.k.a., resizable bar). This may improve GPU performance...depending on game.  An i9-9900 is in the $200+ range. An i7-9700 is in the $100+ range. The i9-9900's \~24% performance gain is questionable for \~100% increase in price. There are other options between your i3-9100f and the i7-9700...  Before dropping Benjamins on a new GPU, consider an overclock and undervolt of your 6500 XT:  [https://www.youtube.com/watch?v=jxSLrvoejhg](https://www.youtube.com/watch?v=jxSLrvoejhg)  If the OC/UV yields ""good enough"" improvements, then you might be able to bypass the premiums being demanded in the current PC component market.",buildapc,2026-01-12 18:50:55,1
Intel,nz7avun,"All the games you play avoid the problem of having a weak but modern gpu very well. Assuming you dont suddenly decide to play something more graphically intensive i reccomend upgrading your cpu seeing how it feels, then upgrading your gpu. The immediate option that comes to mind is the i5 9400f. It should be an approximate 25%-30% improvement in most of the games like cs2 due to the added cores, threads and clock speed compared to that fairly weak i3 cpu. Then you could do something like an RX 6600 for $120 if findable, if not I usually reccomend the 3060 12gb as a braindead midrange option still able to play anything, it's about 200.",buildapc,2026-01-12 18:07:14,1
Intel,nz7ayh1,Get a 12th or 13th Intel i5 CPU and a decent GPU like the 5060 Ti or the 9060XT,buildapc,2026-01-12 18:07:35,1
Intel,nz78jpw,"What is your budget for upgrades?  Your GPU is currently being held back by your platform. The 6500XT only has PCIe x4, and LGA 1151 only has PCIe 3.0. 3.0x4 bottlenecks the card pretty badly, going from on par with an RX 580 to worse than an RX 570. Getting onto a newer platform with PCIe 4.0 would improve both your CPU and GPU.",buildapc,2026-01-12 17:56:42,0
Intel,nz7k4tc,"What you could/should upgrade depends entirely on budget. The CPU is nearly the slowest 9th gen Intel chip, so an upgrade there would help a lot. The ""Best In Slot"" CPU is the i9-9900K but those go for nearly $200 on eBay. You can find an i7-9700 for half that and while it's 50% faster multicore, it's only 10% faster single core and most games just use one or two cores, but there are exceptions: research which games you play to check. Fortnite and Arc Raiders can be CPU bound, so the i7-9700/K might help you out a lot with those games, or really any open world, multi-player type game. The GPU is also the slowest of the 6000 series. As far as new GPUs go, the Intel Arc B580 12GB at $250-$300 or the Radeon RX 9060XT 16GB at $380-$400 are best bang for the buck on the cheaper side, your options really open up on the used market. Go to the [Techpowerup specs page](https://www.techpowerup.com/gpu-specs/radeon-rx-6500-xt.c3850) and check out the Relative Performance of your card with faster ones. I would upgrade to something at least 200% relative performance or higher if you can afford it. Between the GPU and CPU, I'd say the GPU upgrade should take priority over CPU. While a much newer GPU might be bottlenecked by the 9100F, its vastly superior performance will leave the 6500XT in the dust. GTX 1080ti (\~$150) RTX 2080ti (\~240) RTX 3060ti (\~$220-250) RX 6750XT ($280-350) RX 6800 (\~$300) RTX 3080 (\~$350) all examples of decent used options that are considerably more powerful than the 6500.",buildapc,2026-01-12 18:48:41,0
Intel,nz7aten,Adding a 9700K isnâ€™t gonna fix the fact the GPUs being bottlenecked by PCIe 3.0.  An R5 3600 and a cheap B550 board would probably cost not much more than an 9700K and improve the GPUs performance quite a bit. It would still work with the 16GB DDR4 they already have.,buildapc,2026-01-12 18:06:55,0
Intel,nz78qkh,I have seen many videos on how it is a bad card I havenâ€™t had no problems so far but definitely on my list,buildapc,2026-01-12 17:57:33,1
Intel,nz7da6d,LGA 1700 isnâ€™t worth recommending at all. DDR4 performance has aged terribly because newer games are more memory demanding. DDR4 boards have also gone up in price a lot.Â   HUB revisited the Ryzen 5000 series last week and included a 12400F on DDR4 and DDR5 for comparison (https://youtu.be/RijAyVshtok?si=NbuCRlSV4uhHOzlv) - the 12400F on DDR4 performs closer to an R5 5500 nowadays than the R5 5600 it used to compete with.,buildapc,2026-01-12 18:18:05,0
Intel,nz7etoy,"I have not seen anywhere near that level of bottleneck from even this level of pcie mismatch. Cool you have pet issues, but this is about the cheapest possible options for the platform (and use case) at hand. Not your personal advice segment with little but misremembered content to advise from.",buildapc,2026-01-12 18:25:03,0
Intel,nz7dvey,Dude have you seen the DDR5 prices lately or have you been living under the rock?,buildapc,2026-01-12 18:20:45,1
Intel,nz7id4q,https://www.tomshardware.com/news/rx-6500-xt-pcie-gen3-gen4-tested,buildapc,2026-01-12 18:40:55,2
Intel,nz7ia34,"What? AÂ lot of the review coverage for the 6500XT was about how bad the PCIe bottleneck is. The 6400 and 6500XT are unique in that theyâ€™re PCIe x4, even current lower-end cards like the Arc B570 and RTX 5050 are PCIe x8  https://www.techpowerup.com/review/amd-radeon-rx-6500-xt-pci-express-scaling/28.html TPU saw a 24% drop from 4.0 to 3.0 at 1080p  https://www.techspot.com/review/2398-amd-radeon-6500-xt/ Techspot saw a 22% drop in averages and 25% drop in lows from 4.0 to 3.0 at 1080pÂ   I wouldnâ€™t be surprised if itâ€™s even worse nowadays, games demand more VRAM now than they did 4 years ago. More VRAM the card doesnâ€™t have = higher PCIe bandwidth need = more bottleneck when you donâ€™t have bandwidth  A cheap upgrade like an R5 3600 + B550 board would not only improve CPU performance substantially coming from an old 4c4t CPU, but give the GPU the PCIe 4.0 it needs.  If you donâ€™t know what youâ€™re talking about, donâ€™t comment lol",buildapc,2026-01-12 18:40:32,1
Intel,nz7jl7h,"Yes, Iâ€™m not stupid.  You are far better off going for Zen 3 over anything on LGA 1700.Â   The R5 5600(X) performs as well with DDR4 as the 12400(F) does with DDR5.  Used they both go for ~$100-120, new both are ~$170 right now. AM4 boards tend to be cheaper and the RAM is the exact same.  Why would you spend more on LGA 1700 to get worse performance?",buildapc,2026-01-12 18:46:18,1
Intel,nz7iqr0,"My real-world experience with low-end cpu power and this mismatch on those titles is 10%. It isnt worse than having that i3 does to cpu-bound situations. Upgrading a bad platform is probabaly not his best price to dollar, but it is the nuanced situation he's in. if he can sell what he has for the price it goes for, going ryzen is always correct. However his cpu and all the others here are bad. If he can simply get a better part price on a single chip, he can get equivalent value from the upgrade compared to going ryzen.",buildapc,2026-01-12 18:42:36,1
Intel,nz7kl1z,You can get an i5 13600k with better performance than a 5800x. Also it is difficult to find the AM4 X3D CPUs these days so the intel route makes sense. He can later upgrade to a 14900k even which no AM4 CPU can match in terms of performance,buildapc,2026-01-12 18:50:40,0
Intel,nz7kgsl,"Werenâ€™t you recommending upgrading to a 9400F?  Sure, the 9100F might be the limiting factor right now, but youâ€™re still gonna hit the same PCIe bottleneck. Itâ€™s just gonna be more pronounced.",buildapc,2026-01-12 18:50:09,1
Intel,nz7nzri,"13th gen pricing is crap so Iâ€™ll use 14th gen for comparison, itâ€™s basically the same  14600KF is $245 new and $180-200 used. It has a 181W PL2 so you need a board with good power delivery to not throttle it, especially if you want an upgrade path. The cheapest DDR4 boards you can get right now that fit the bill are $140.  5800XT is $219 new and $150-170 used. AM4 boards are still cheap, thereâ€™s plenty of quality B550s for $100.  Going Intel over AMD would cost at least $60 extra and most of the performance gains you would see using DDR5 on the 14600KF would be wiped out using DDR4",buildapc,2026-01-12 19:05:55,0
Intel,nz7kpgs,"It'll then axtually be the bottleneck. When you get something with a normal pcie layout, possible even the same value as his current card, that utterly disappears. A 6600 would be a great upgrade from that position...",buildapc,2026-01-12 18:51:13,1
Intel,nz7pbw9,"There is a high-end bias with this platform, like any. It helps a lot to understand that budget systems fall within their spec, it is only the highest-end and lowest-end options of this era that stretch the pcie issue... there are cycles of alarmist narritives in all media. That one is old news that likely shouldnt have been printed imo.",buildapc,2026-01-12 19:12:06,0
Intel,nz7r3bk,Youâ€™re going to hit the large PCIe bottleneck the moment the CPU isnâ€™t terrible. Itâ€™s not gonna take much of a CPU upgrade for a GPU worse than an RX 570 to become the issue.,buildapc,2026-01-12 19:20:11,0
Intel,nz7r90m,Im pretty sure a 9400f isnt gonna cause an issue here man... ive used this platform before. Then his pc is worth something...,buildapc,2026-01-12 19:20:54,0
Intel,nz7rnwn,Youre giving me nothing against the clear narritives youre following over substance of experience with this shitty level of cpu performance.,buildapc,2026-01-12 19:22:49,0
Intel,nz7tdgt,"Do you respect at all the starting position he is in or are u just an advice salesman? Both of our ideas coexist. selling his current pc for more than its worth and just buying a dirt cheap 2nd gen ryzen with a sane gpu for the same money, then upgrading from there is also perfectly correct. That is a similar level of effort to a platform swap, or whatever you would prefer, which is also correct. The only bad advice here wastes more time and money than his original choice of platform already sealed. Also ur just wrong about pcie bottlenecks im pretty sure",buildapc,2026-01-12 19:30:40,0
Intel,nz7whko,"Iâ€™m making the reasonable assumption that the person running a 9100F + 6500XT in 2026 doesnâ€™t have a ton of money to spend on upgrades lol  Upgrading the current platform makes no sense without also upgrading the GPU. You canâ€™t get very far with CPU upgrades on an H310 board, anything higher than an i5 is gonna be throttled.Â   Swapping in a cheap PCIe 4.0 CPU + board like an R5 3600 + B550 will improve both the CPU and GPU performance while not spending a ton of money. It also opens the door to future upgrades to something like an R7 5800XT which will hold up far better with a GPU upgrade.Â   I gave you multiple reviews showing 20+% losses on PCIe 3.0. Thatâ€™s not opinion, thatâ€™s fact. That loss is going to be the same regardless of how good or bad your CPU is as long as the CPU isnâ€™t the bottleneck.",buildapc,2026-01-12 19:45:08,1
Intel,nz84eos,I genuinely wonder what u get living in the idealized world where pcie 3.0 sucks ass below the highest-end 4.0 products and visa-versa. My dad was rly into the ideas of mearsheimer and I luckily biocontained his ass before that fkers gone and made the meme rounds again. You listen to Armand Hammer?,buildapc,2026-01-12 20:21:52,1
Intel,nz7xfno,"Yea. Your advice is fine and wasnt the target of my statements, I added the last comment as proof I agree a platform swap is a totally good option. He doesnt even need to change his cooler for a 9400f and can then get a normal gpu for a similar value to his current one. This process is much simpler and has the potential to grant equal value if the chip is recieved for a low enough bargain, increasing the chances of finding a dreal given limited time. Do we disagree that we are both good at selling advice? I am still completely debating the validity of your sicituational understanding of pcie bottlenecks in relation to low cpu power circumstances, partially because of ur choice of sources, and ur disintrest in confronting that statement. His pc can gain value whilst spending little, my option is simply the lowest I coukd imagine.",buildapc,2026-01-12 19:49:28,0
Intel,o0bol6l,I'd keep an eye out for MSRP 5080 and use that 1k..  Should last a good long time.,buildapc,2026-01-18 17:35:55,12
Intel,o0byelm,5070ti for $750 would be a good choice,buildapc,2026-01-18 18:21:22,7
Intel,o0blyeq,"9070xt best bang for the buck, especially 1440p   at least in my country where it's 763 euros   while rtx 5070ti is 1000 euros",buildapc,2026-01-18 17:23:19,23
Intel,o0bswl1,"5070 Ti if you can find one, 5080 if you really can find the cheapest one and stretch your budget a bit.",buildapc,2026-01-18 17:56:05,5
Intel,o0bnhy4,5070ti and 9070xt are the best value for 1440p gaming. If you plan to do 4k or higher you're looking at 5080 or 5090 for the best experience.,buildapc,2026-01-18 17:30:40,6
Intel,o0c2rhp,Get yourself a GTX 650,buildapc,2026-01-18 18:41:02,3
Intel,o0clto8,"Honestly thereâ€™s no â€œwrongâ€ GPU to slot in with the rest of your build (although getting something lower tier than the options you stated would probably be a bit of a waste)  The 9800X3D gives you so much headroom that unless youâ€™re going for crazy high FPS @ 1080p (which youâ€™re not), it will be more than enough to pair with any GPU.   I think the 5070 Ti and 9070 XT are the most common â€œsweet spotâ€ for value/performance atm, but if you ever see a good deal on a 5080 then thatâ€™ll obviously be a bit better.   My main word of advice would be, donâ€™t stretch your GPU budget too far at the expense of the monitor. You donâ€™t have to go crazy or anything, but a quality monitor will improve your experience quite a bit more than a few % better performance from the GPU imo",buildapc,2026-01-18 20:11:47,3
Intel,o0byfr7,"Without productivity needs, it'll come down to how long you plan on keeping it. If you plan on keeping it for 5 years or less, the 9070 XT if the price is cheaper. If you plan on keeping it for more than 5 years, I'd lean towards the 5070 TI since Nvidia seems to have better long term support.",buildapc,2026-01-18 18:21:31,2
Intel,o0btn74,Nvidia way overpriced get the 9070xt,buildapc,2026-01-18 17:59:29,4
Intel,o0bnss9,5070ti or 5800 if budget is 1k ish.,buildapc,2026-01-18 17:32:08,4
Intel,o0bp5hz,"With a 9800X3D, anything less than a 5080 is just a waste of CPU speed.  Edit: for the people downvoting, here's the proof: https://www.techspot.com/review/3017-ryzen-9800x3d-vs-7600x-cpu-scaling/",buildapc,2026-01-18 17:38:36,3
Intel,o0bqy4a,I would suggest a 7800x3d with the money saved go 5070 ti or even 5080.  The new DLSS 4.5 for Nvidia cards is fantastic.,buildapc,2026-01-18 17:47:00,2
Intel,o0c1h70,"My only gripe with the 9070xt is that it's DDR6 vs the DDR7 on the 5000 series gpu. It upsets me because even my 3080 (5+ years old now) used GDDR6X. Though the AMD is more primed to save during the ram crisis, I would always want the fastest possible clock speeds to future proof (will help with the AI models of DLSS/FSR).  You probably will be fine with either, but that's what turned me off from AMD. Performance wise, AMD is a much better value, but this graph for CS2 also turned me off from getting AMD (note that most other games are comparable between 5070 ti and 9070 xt):  https://www.techpowerup.com/review/sapphire-radeon-rx-9070-xt-nitro/11.html",buildapc,2026-01-18 18:35:13,1
Intel,o0bmdli,5070ti donâ€™t let the AMD fanboys convince you otherwise,buildapc,2026-01-18 17:25:20,-6
Intel,o0bpi4i,Any advice on doing this? I browse buildapcsales pretty regularly but still have trouble catching the deals on time,buildapc,2026-01-18 17:40:15,6
Intel,o0guo03,"good luck catching 5080 at msrp, it's rarer than a unicorn at this point, also nvidia and amd announced a +150% price spike starting this february",buildapc,2026-01-19 12:50:35,1
Intel,o0bmcuc,"Thatâ€™s what Iâ€™m hearing for sure, I was nearly set on it a week ago because Iâ€™ve seen a lot of people say the raw power of the 9070xt might even be better than the 5070ti. Also seen some people say thatâ€™s not true though, and while bang for buck is important for me I would definitely pay a lil premium if it seemed worth it",buildapc,2026-01-18 17:25:14,7
Intel,o0c4jx5,My man! Lmao,buildapc,2026-01-18 18:49:08,2
Intel,o0guxry,based,buildapc,2026-01-19 12:52:25,1
Intel,o0guxen,"thing is the reliability of the 50 series concerns me, lately nvidia is having drivers issues plus the well known power connector love for catching fire",buildapc,2026-01-19 12:52:21,2
Intel,o0cmn0g,"I wouldnâ€™t say itâ€™s a â€œwasteâ€ per se - the 9800X3D outpaces basically every GPU on the market rn (as long as itâ€™s not 1080p). So yea in most circumstances you wonâ€™t be stretching it to the limit, but that means you donâ€™t have to think about upgrading it for a longgggg time - and every build always has some limiting factor. With a 9800X3D, itâ€™s just never the CPU lol",buildapc,2026-01-18 20:15:47,6
Intel,o0bpq0a,"Iâ€™ve wondered about this myself, and the longevity would be pretty great",buildapc,2026-01-18 17:41:17,3
Intel,o0brb6h,"Already got the 9800x3d, I realize I may have gone overkill lol. Probably gonna save for 5080 a this point fuck it",buildapc,2026-01-18 17:48:41,3
Intel,o0dymei,"thing is 50 series cards, especially the 70, 80 and 90, like to catch fire cause of the new connector",buildapc,2026-01-19 00:19:46,1
Intel,o0bmmtf,What would you say it has over the 9070xt? The goal here is to only let facts convince me lol,buildapc,2026-01-18 17:26:34,4
Intel,o0bsuwd,I haven't had much luck with 5080s but recently we tracked down a 5070 ti for my buddy at a local Walmart at MRSP check your local walmarts and best buys and set up some sort of stock alert for nvidia's website and founders cards restocks. In my experience reddit is too slow because by the time you see it they're gone.,buildapc,2026-01-18 17:55:52,5
Intel,o0ctbru,If you live near a microcenter I'd regularly monitor their site.Â  There are also a few discords you can join for free that track stock.,buildapc,2026-01-18 20:48:18,3
Intel,o0bxgdn,My friend upgraded from a 1080ti to a 5080 just before the holidays last year and he ended up having to sign up for some kinda stock alert system with MSI directly. Worth checking into if thatâ€™s what youâ€™re after. General brick and mortar retailers are kinda hit or miss when it comes to personalized alerts for GPUâ€™s IME. (Microcenter being the obv exception.),buildapc,2026-01-18 18:17:03,2
Intel,o0d2fca,"Hotstock.io app, I grabbed a 5080 FE from nvidia 2 weeks ago.",buildapc,2026-01-18 21:40:06,1
Intel,o0guq60,nvidia and amd announced a +150% price spike starting this february,buildapc,2026-01-19 12:51:00,1
Intel,o0bmpjl,"raw power they are basically equivalent   in some titles one it's better than the other, in some they are equal  as the 9070xt costs less i went for that myself  i will not spend 300 euros more for basically 1 to 5% more performance in some scenarios while i'll have same or less in others  if they are equivalent in price in your region go for your personal preference    or if the 5070ti is cheaper go for it",buildapc,2026-01-18 17:26:55,7
Intel,o0cy2wl,"Hardware unboxed did a comparison in raw and upscaled performance for both the 9070xt and 5070ti. Im very happy with my 5070ti I got at msrp a week ago, especially since 150 bucks wouldnt make a difference to me in 4 or 5 years.",buildapc,2026-01-18 21:14:55,2
Intel,o0c09lx,"Well, you have a fantastic CPU there.  Yeah, a 5080 will be a perfect pairing and gives you a very high-end PC.  Have fun!",buildapc,2026-01-18 18:29:46,1
Intel,o0ea1fn,I got a FE5090 and immediately undervolted it - no problems in 6 mos of ownership so far.  The plug on the FE model is supposed to be different and less susceptible.   Hope so.,buildapc,2026-01-19 01:21:22,1
Intel,o0boryu,"Better tech.. Dlss 4.5, frame gen, better drivers, slightly higher base frame rate. Mainly dlss 4.5 and frame gen. Iâ€™d love to go w AMD (I do when it comes to cpu)but they just arenâ€™t at the same level in terms of gpu.",buildapc,2026-01-18 17:36:49,3
Intel,o0bqht9,"Basically the Nvidia feature suite. FSR4 is good but not as good as DLSS4 and now we have DLSS 4.5 which is another step up.   DLSS frame generation is also better than the counterpart from FSR but many people may not want to use frame generation at all.  If you play any ray traced title, although 9079XT does run these games pretty much as fast as 5070TI, the image quality will be worse than 5070TI because AMD just has way worse ray reconstruction (even the new FSR ray construction exclusive to the new CoD game is still a lot worse than the Nvidia counterpart). 9070XT also performs worse than 5070TI in path traced game.  But I am not sure about things like driver stability. I know it is something AMD had been accused a lot in the past but from my own experience the driver for GeForce 50 series at June/July last year was downright horrible, like games literally freeze at unacceptable occurrences. It has been largely fixed at this point but some of the issue remains. Like forcing anisotropic filtering on still causes artifacts in many games",buildapc,2026-01-18 17:44:53,3
Intel,o0bqpqz,"I donâ€™t know a ton about the current market but I just got a 5070ti a week ago at microcenter and both were at msrp but my understanding is for the extra 150$ (at msrp) youâ€™re paying the nvidia tax ofcourse but other than that slightly better dlss support in games and driver support over the years based off what Iâ€™ve read, then frame gen which you certainly donâ€™t need but for me it means I can stretch the life of the card for 2-3 years longer and jump up to 4k if I want to. Native performance is basically identical so really you canâ€™t go wrong between saving money and frame gen Iâ€™d say it depends how long and how hard youâ€™re going to be pushing the card.",buildapc,2026-01-18 17:45:55,2
Intel,o0bti4u,"Makes sense, appreciate the advice my friend!",buildapc,2026-01-18 17:58:50,3
Intel,o0dw1qe,I do and they have some just above MSRP! Hopefully they still have them by the time I get my tax refund. Cheers!,buildapc,2026-01-19 00:05:58,1
Intel,o0bmxio,Gotcha. I appreciate the insight!,buildapc,2026-01-18 17:27:59,4
Intel,o0gu0rq,"5070 ti msrp good catch mate, sadly in my country cheapest one 1k euro  honestly i wanted to stay nvidia but i didn't have 1k plus to throw around like that, also if I had gone with the 5070, Iâ€™d have needed to change the power supply as mine doesn't have the new power connector",buildapc,2026-01-19 12:46:13,1
Intel,o0gsla1,"well bro, thing is when a 3k dollar product reliability is subject to dumb luck let's just say it's not worth that much then",buildapc,2026-01-19 12:36:08,1
Intel,o0e27xp,"I grabbed a 5080 Zotac SOLID CORE right after Christmas and they sold out not long after. I'd keep an eye out for a proper $999 price, hopefully they will restock with no hike.",buildapc,2026-01-19 00:39:01,2
Intel,o0dyxd8,I got my 5080 at MSRP at microcenter! The SFF wind force card. Gigabyte. Love it!,buildapc,2026-01-19 00:21:24,1
Intel,o0bnqqs,"no prob, happy to help, don't listen to fanboys that tell you that ""no you have to take this one because is clearly much better"" always go with an objective mind, look at price to performance and at offered features",buildapc,2026-01-18 17:31:51,3
Intel,o0btw2j,"Keep in mind that if you dont go with nvidia you lose dlss and mfg, which is miles ahead of fsr. While might be almost equal in power id say its worth it because of these free frames with almost zero image quality lost.",buildapc,2026-01-18 18:00:36,5
Intel,o0bykns,Please donâ€™t let this man convince you to get a 9070xt over a 5070ti you will regret it. Frame gen DLSS and Ray tracing are very good now. AMD cards are cope city.,buildapc,2026-01-18 18:22:08,7
Intel,o0j9uc6,"Yeah thats rough, honestly the new amd cards are very valid, Im just an nvidia guy and spending the extra 150 over the 9070xt wouldnt make a difference to me in 4 or 5 years when I upgrade again.",buildapc,2026-01-19 19:51:32,1
Intel,o0e9cz6,Good shout. I gotta wait til I get that refund anyway so fingers crossed,buildapc,2026-01-19 01:17:31,1
Intel,o0c0i86,"Donâ€™t worry lol, if anything Iâ€™m now convinced I should just go big and get a 5080",buildapc,2026-01-18 18:30:50,4
Intel,o0dxw2l,"bro if you like having a fire hazard in your house plus fake frames go ahead, i'm offering him a realistic choice, if he wanted a budget component he could've gone with the intel arc 580 16gb, a monster of a card for like 300 euros  Nvidia is overpriced and dangerous   Amd is powerhungry and not great for productivity but it's really good for raster performance, and if he plays arc raiders fake frames will only increase latency and worsen his game performance   Intel has questionable drivers because it's a new lineup",buildapc,2026-01-19 00:15:52,-2
Intel,o0c9658,This is the way,buildapc,2026-01-18 19:10:34,3
Intel,o0dxmek,be aware that 50 series card all have the new nvidia power connector that has a big passion of catching fire,buildapc,2026-01-19 00:14:29,-2
Intel,o0fsrua,"AMD is fine but its not better and definitely not at the same price point. 9070xt is worth considering for $560 if 5070ti is $750 but Id still probably rather get the 5070ti. If theyre closer in price its not really a contest outside specific use-cases like games without DLSS/RT, Call of Duty, or if your OS is linux.",buildapc,2026-01-19 07:17:21,0
Intel,o0dxq1z,"Iâ€™ve been worried about that too, what can one even do about this",buildapc,2026-01-19 00:15:01,2
Intel,o0gsyar,"look mate, in my country the cheapeast 5070ti costs around 1k euro, the 9070xt costs 700ish, cheapest one around 730  you'll excuse me if i don't spend 300 euros more to have a pratically equal product but with the added possibility of burning my house down  cheapest 5080 costs 1.4k and with more risk of catching fire  stop being a fanboy and start being objective, big companies don't care about us consumers but  for the moment the amd card is the better deal if you have a price difference >70 dollars - 60 euros  or if you just want a safe card that doesn't burn your house and you don't need it for productivity",buildapc,2026-01-19 12:38:46,0
Intel,o0dz4lc,"i heard some producers have given color coded cables with their new shipments of 50 series card so if the connector is not perfectly in you will see colors, but i heard that even fully inserted cables sometimes melt, especially with 80 and 90 cards  beside this, you can only pray god  edit: man the fanboys of nvidia are angry, they don't understand that having a fire hazard in the house it's not worth the 1k plus they spend, also they don't understand that defending amd or nvidia will not get you any award, it's not like jensen and his cousin (ceo of amd) will come to your house and give you a free top of the line gpu",buildapc,2026-01-19 00:22:28,0
Intel,nvursni,r/homelab,buildapc,2025-12-25 10:48:55,1
Intel,o06qmo8,"Depends on the title.  A little bit in most, none in others, a lot on a handful.",buildapc,2026-01-17 22:36:02,8
Intel,o06tidq,Don't worry about it too much. The RTX 5080 is still going to give you a massive performance boost.,buildapc,2026-01-17 22:50:40,3
Intel,o06wtpf,1080p yes 1440p competitive settings- yes 1440p max graphics- sometimes 4k no,buildapc,2026-01-17 23:07:17,3
Intel,o06wp6q,"Anyone saying â€œlittle to notâ€ is misleading you.  If you are comparing to something like a 14700K, the 12400 is a big bottleneck, even at 1440p and 4K.    As soon as you turn on upscaling like DLSS, youâ€™re making yourself more CPU limited.  DLSS Quality at 1440p would be somewhere around 980p, so take a look at 1080p benchmarks for an idea of the potential performance difference.",buildapc,2026-01-17 23:06:38,3
Intel,o06rl43,"Just do it.  The longer you wait, the higher the prices go.  You can think about the CPU upgrade later when you save some money.",buildapc,2026-01-17 22:40:48,2
Intel,o06rrou,"No way to calculate it  Once you get the GPU, check GPU usage at an unlocked framerate and youâ€™ll see how much performance you could be potentially missing out on  Iâ€™d be curious on the data and Iâ€™m sure others would be, if you update the post after getting the GPU",buildapc,2026-01-17 22:41:45,2
Intel,o06svg0,Like any bottleneck it's a variable that will go back and forth between the CPU and GPU primarily.,buildapc,2026-01-17 22:47:24,2
Intel,o06to9v,"What motherboard model do you have?  LGA1700 socket included the 13th and 14th gen Intel CPUs, and most motherboards support upgrading to them with a BIOS update.",buildapc,2026-01-17 22:51:31,2
Intel,o06wbw7,You will be somewhat cpu limited in a lot of games.That will be the case even with a 5070ti. I would try and upgrade to 14gen i5.,buildapc,2026-01-17 23:04:46,2
Intel,o08ivy1,"It will bottleneck heavily at 1440p, I had experience with 9070xt (which is worse than 5080) with 12400f",buildapc,2026-01-18 04:24:17,2
Intel,o084mf6,"Not worth getting a 14600K / KF and a 5070 Ti? The 5080 tends not to be worth it for price to performance when compared to the 5070 Ti.  There were some 5070 Ti models for Â£750 last week, but looks like theyâ€™re going up in price now, but that CPU and GPU combo total price should be about the same price as a 5080",buildapc,2026-01-18 02:59:54,1
Intel,o0884vx,"I'm using a i5 12400f with a 5070ti so almost the same as you, i did overclock and undervolt mine so might be even somewhat closer in performance 3075mhz with 0.925mv to you.   I do have 2k and 4k texture mods installed and multiple other mods so maybe the fps i get may not be what you see on vanilla cyberpunk and even slightly higher for vanilla.   Yes you will still be cpu limited in cyberpunk for me 1440p no ray tracing or PT i get 70-100fps the 70fps is the extreme dips and this is with every setting maxed except for crowd density at low   I also pulled up intel present mon and took a look at cpu busy and gpu busy this tells me how quickly one is sending information to the other, generally you want the cpu busy and gpu busy to be the same so that one is sending and completing the instructions at the same time and speed.   So with my hardware/mods and settings my cpu busy was 8ms and gpu busy was 6.5ms so my gpu was definitely waiting on my cpu for a bit.   With pathtracing on i dropped down to about 50-60fps.   Keep in mind though this is with quite a few texture mods and them being either 2k or 4k and also this is with preset M.   I'm currently getting the i5 14600kf arriving in 1-2 days since i want better shader compilation times as well since dear lord it takes a while for some.  Arc raiders while i haven't played the game i'm pretty sure as like any other live service online fps game will be quite cpu intensive rather than gpu intensive based on their style of graphics and how it's done so you should be limited by the i5 12400 as well as for how much sorry i don't have any data for you.",buildapc,2026-01-18 03:19:53,1
Intel,o0exx2d,"A bit in RDR2, a ton in cyberpunk. No idea about ARC.",buildapc,2026-01-19 03:32:08,1
Intel,o0n70o9,"Hi op, I recently upgraded to a 5070ti and also have a 12400f.  Im on 3200mhz of ddr4 ram and playing at 1440p as well.  In some games like Arc Raiders, God of War, and Control, everything is perfect. GPU utilization is 99%.  However, im seeing a number of issues in several games. For instance, CP2077 has my GPU utilization at 65-70% and at ultra preset, no RT or dlss, im getting 80fps, while I see others with this gpu get 120. I know its a cpu botleneck bec when I leave the city and go to open areas, the utililization jumps to 99% and im getting like 120-130 at times even.  Likewise, several games like Space Marine 2, Outer Worlds 2, Spider Man Remastered, and more all sit at like 70-85% GPU utilization. Delta Force is at 59% (still 144fps tho) , Helldivers 2 sitting at 50% GPU utilization and a terrible 60 fps average. Starfield bounces from 60%-80% in the Jameson area and and gives me 60fps on average. Some drops to 55 fps too.  Whats more, DLSS options make no difference at all because of the bottleneck.  I can go from DLAA to DLSS Ultra performance without gaining any extra fps. This is a massive bad point considering how nice DLSS is these days.  Even among the games that give me 99% gpu utilization, Arc Raiders is the only that gives me DLSS performance gains. That game is just beautiful, giving me 100+ at cinematic settings, native res.   But for the other games, I literally have a 13600kf ready to be opened and installed (waiting on the cooler).  With a 5080, you are def going to see some bottleneck. So def consider a 13600kf/14600kf (they both identical in perf so get whichever is cheaper)",buildapc,2026-01-20 10:25:19,1
Intel,o06qzcs,"not much, The more graphic intense the game the more the PC relies on the GPU and not CPU. You will be fine.",buildapc,2026-01-17 22:37:47,0
Intel,o06rayc,"since you're a 1440p gamer, the GPU does majority of the work, so you'll be fine.",buildapc,2026-01-17 22:39:24,0
Intel,o06q5u8,probably not by much,buildapc,2026-01-17 22:33:38,0
Intel,o06qrva,Oh yes I'll add some titles to my post thank you,buildapc,2026-01-17 22:36:45,1
Intel,o08bd4b,Would you say a 5070ti should also be considered or not?,buildapc,2026-01-18 03:38:19,1
Intel,o06y595,"Yeah I am looking at my current options of upgrading to something like a 14600kf, which I think would solve my problems? I could always upgrade to that after a few weeks or months if I find the bottleneck too hard. Has the issue with the Intel cpu's later gens been fixed? I remember there was some bios updates for them but I don't really hear much anymore",buildapc,2026-01-17 23:14:11,1
Intel,o06u1tb,I have the B660 AORUS MASTER Ddr4,buildapc,2026-01-17 22:53:26,2
Intel,o0898kq,"Yeah the 5070ti I was looking at (msi gaming trio) is around Â£920, compared to like Â£1240 of the Rtx 5080. I'm just paranoid if I didn't go for that extra bump that I'd regret it later. If I went for the 5070ti, I could definitely afford the new cpu (like a 14700kf or such) in one go too.",buildapc,2026-01-18 03:26:07,1
Intel,o08bavn,"This is a really helpful, thank you so much. Can I ask, with your current combo how is your general performance in whichever games you play please? I am considering dropping to a 5070ti and a cpu upgrade instead.  As for arc raiders performance, on the max settings, it's still capping out my fps with dlss balanced at around 60 (although I'm using fsr 2x fg), as it has global illumination and stuff so I do think while there are moments I'm cpu limited there's also very intense graphical stuff, but if I upgraded gpu it would definitely be held back more by the cpu",buildapc,2026-01-18 03:37:57,1
Intel,o0nfjlo,"Thank you so much, this is incredibly useful. Where I am you actually can't get 13600s (or any 13th gen), so I was thinking to do as you said, and get a 14600k, or 14700k, though the latter is like 50% more for hardly any gain.  But thank you so much, esp for the arc raiders ones as I always felt it might be cpu limiting but it's good to know it's more gpu.  (also this is completely irrelevant, but have you tried the frame gen in anything yet? I've tried things like fsr frame gen and lossless scaling and typically don't experience much issues with them or mind, and I can only imagine how good nvidias one will be)",buildapc,2026-01-20 11:38:55,1
Intel,o07lpkj,Its the 3rd best GPU on the market with a 4 year old budget CPU. There is absolutely going to be a noticeable bottleneck in many games,buildapc,2026-01-18 01:18:06,1
Intel,o06wwbn,"So yes, your board would support up to 14th gen Intel CPUs, and looks to have a high quality VRM, so shouldn't struggle at all with even the highest end CPUs.  I'd suggest grabbing the GPU and seeing how it performs in the games you play, and looking into upgrading the CPU if some aren't meeting your expectations.   The 14400 isn't much of an upgrade over the 12400, but you can currently get a a 14600kf for ~$220 US, which is a notable upgrade over both the 12400 and 12600.",buildapc,2026-01-17 23:07:39,3
Intel,o08cie2,"Pricey that one, Iâ€™ve been using the cheaper Gigabyte Windforce 5070 Ti with no issues, sure thereâ€™s no fancy lighting but it seems dead quiet. Running it with a 13900K and never had any performance hiccups with Red Dead 2 and Cyberpunk at 1440p Ultrawide.  Amazon have the MSI GAMING Trio 5070 Ti for Â£840 atm and some cheaper on their Used Like New option. Youâ€™d still be looking at like Â£400 more for a 15% - 20% performance increase.  I guess if go all out for the 5080 and do end up needing to upgrade your CPU, then you can get a few hundred if you sell your 3080 and use the funds from that.",buildapc,2026-01-18 03:45:00,1
Intel,o08grb3,"Games i play or played, final fantasy 7 rebirth, Expedition 33, kcd2, alan wake 2, the last of us part 1, space marine 2 all of these is with quality setting and preset M   Final fantasy 7 rebirth 66% resolution scaling (this game doesn't have balanced weirdly so you're only able to set 33, 50, 66 and 100%) at 66% i'm roughly at 115-130fps. (Also do note i have some 4k texture mods installed).   E33 is about the same quality setting at 90-100fps no frame gen.   kcd2 unfortunately i haven't had time to play it so i can only give you starting area fps which is about 100-120fps, it'll probably tank like 15-25fps once you get to kuttenberg with the amount of npcs they have.   alan wake 2 is about 80-95fps in the nursing home, i don't have a save to load into the more intensive area which is about the forest area near the start of the game.   The last of us part 1 is 90-105fps.   Space marine 2 is where you will struggle abit with the i5 12400, during small fights and running you will be about 80-90fps but once you start going into the big battles you will drop to about 60-70fps.   Oh for when BF6 was in the testing phase i was getting about 85-100fps during that play session not sure if this helps with anything for you.",buildapc,2026-01-18 04:10:36,1
Intel,o0ntbi2,"Yeah, id get the 14600k then. It's more than fine for a 5080/5070ti.  Yes, Arc runs perfectly as is.  And yes, frame gen has been incredible, honestly. While the messaging about how it's useless when you're below 60fps is accurate, it is actually beautiful if you like having high fps.  For example, in Outer Worlds 2, my FPS in the town area drops to 54-57 fps. However, multi frame gen, gives me a very comfortable high refresh experience. Likewise frame gen in arc raiders, space marine 2, and indiana jones feel really good.  I also have the Lossless scaling app and honestly, Nvidia frame gen is several times better. I remember testing out lossless scaling in several games and the only game where it felt nice was in Kingdom Come Deliverance 1. However, in games like helldivers, the input lag was very noticable even if you had 60fps base.  Here's another amazing part about 5000 series. Smooth motion. I discovered it only recently and if you weren't aware, its the driver side implementation of frame gen. With the nvidia app, I can turn it on in almost any game, including old ones like control and games like Enshrouded which dont come with any frame gen.  let me tell you, even this driver level frame gen is so much better than lossless scaling. I was recording benchmarks with my 12400f yesterday just so that I can have a comparison when I make the switch and something funny happened. I was playing helldivers and was wondering why I was getting 120 fps. I had remembered it being 60fps earlier. For a moment, I thought my CPU had become unthrottled or that a mew patch had droppeed with massive fps improvements. However, I suddenly realized that I had simply enabled smooth motion earlier. It's legit like native 120 fps and I didnt even realize it.  Of course, dont' expect it to make 60fps feel nice if you only have 30fps (Thankfully, that will never happen right now with this tier of cards as even Path tracing on cp2077 gives me like 55fps). However, who knows what magic nvidia will cook up next. Dynamic frame gen might be super interesting to try in spring as well.  I was this close to actually getting a 9070xt. However, after experiencing frame gen, both native and driver level, and now the new and improved dlss4.5, im so glad I trusted my gut and went Nvidia. It came at a massive price here in India as well (like $965) but when im dropping big money on a card, Id rather get the best feature set than save what is a negligble amount in the context of the  4-5 years I will be using the card.      I am only envious of the dudes with DDR5 memory because that adds a good 20% extra performance in a ton of games apparently. But I guess it's not good to be too greedy haha.",buildapc,2026-01-20 13:14:40,1
Intel,o06xrtk,"Thank you so much, after your comment I did come to think that upgrading down the intel line was still an option (I obv didn't do it before because of the issue, and in my mind currently I'd just ruled out cpu upgrades because of the whole ddr5 prices, and forget about 14th gen Intel)  I am in the UK so prices are a little more, but I could definitely go for like a 14600 or such. Is there much difference between the like 4600kf and like 14700? I will be upgrading my cpu so I should probably go for a higher than 850w if I plan to upgrade my cpu given I just saw the 14600kf is a 125w",buildapc,2026-01-17 23:12:11,1
Intel,o07z7s0,"The ""f"" indicator on the CPUs means that it's a version with the internal graphics chip disabled, which is generally not a problem if you're going to have a dedicated GPU, and shouldn't effect performance.  The ""k"" indicator means that the CPU has a higher max power draw, and that it's unlocked for overclocking. To overclock, you also need a Z-series motherboard. The higher max power draw does mean that even with your B-series board, the ""k"" CPU will be able to run slightly faster than the ""non-k"" variant of the same CPU, usually translating to around ~5% more performance in real world scenarios.   The 14700k(f) is 0-10% faster at most vs the 14600k(f) in gaming workloads, depending on the game, so the 14700 non-k would be even less of a difference vs the 14600k(f). There's generally no reason to go for a non-k variant over the k variant if it's not cheaper, and the same goes for the f CPUs. In the UK, it looks like the 14600 non-k is not available anymore from major retailers, and the 14600kf is about Â£20 cheaper than the 14600k, so probably worth saving the money with the f model in your case.  I usually use this as a quick reference for power supply requirements with CPU + GPU combinations: https://www.msi.com/blog/recommended-psu-table  You can see that for an Intel i7 CPU with the RTX 5080, they indicate that an 850w PSU should be fine if you aren't looking to overclock, so as long as your PSU is a good quality 850w, I wouldn't worry about the power draw.   On the other hand, the 5080 uses Nvidia's new 12pin HPWR connector, which requires an (included) adapter to be used with older PSUs. This connector has a pretty bad reputation for pulling power unevenly and *melting* if the connections are not secure, so most people advise to use power supplies with native 12pin HPWR connectors if possible, to avoid extra points of failure. If you're careful (or if your PSU has these connectors natively - it's usually indicated on the box as ATX 3.0 or ATX 3.1), then it *shouldn't* be an issue, but these connectors are very finicky, so people are usually quite cautious about them.   The only other thing to note is that DDR4 does tend to effect the performance of these CPUs vs DDR5, which can range quite a bit depending on the game, with a few edge-cases actually running better on good quality DDR4 (games that prefer memory latency vs speed). This also effects 12th and 13th gen CPUs, so the performance differences between CPUs will still be the same in a relative sense, when both are using DDR4.",buildapc,2026-01-18 02:30:20,1
Intel,o08asy3,"Thank you so, so sooo much <3 you are literally the best person ever, thank you  As for F, looking on the sites in the UK they generally seem to be around the same price as non F, so in that case I think I will grab a none F just in case of like, a gpu issue. Similar with K, the K are only marginally more expensive so I would definitely go with that over a none k.  As for 14700k vs 14600k, the 14700k is about Â£100 more than the 600, does that justify the price enough? The only other thing I can think of is for the 14600k the only version in stock for me is the F version (the others are all out of stock), so it's around Â£200 for 14600kf and like Â£320 for the 14700k.  Pins wise, yes I've done alot of very panicked research into the old vs new connectors, the melting and such. I was planning to upgrade my psu with the gpu (I currently have an 850w psu, I was going to get a corsair 850w at 3.1 for like Â£140) to get the at 3.1 ones with the native cable, and I was going to plug that thing in *very* firmly.  As for ram yeah I'm just using some corsair vengeance ram, but I can't remember the specific one/name. It's 2x8, and 3200mhz  Once again thank you so much",buildapc,2026-01-18 03:35:06,1
Intel,nz9lefm,you must have some huge hands judging from the apparent size of that box,pcmasterrace,2026-01-13 00:45:35,2
Intel,nzakbj4,"Holy shit my hands look like a giant Hold on ill get you a pic with me actually holding the BOX, not the gpu.  https://preview.redd.it/kkndv19xh1dg1.jpeg?width=4000&format=pjpg&auto=webp&s=924d6b75377cecc6a98ac1bae1a765b4349dc025",pcmasterrace,2026-01-13 03:55:11,3
Intel,nziazoy,"yes, but choose the b580 instead (though it has 12gb of VRAM), also id recommend the rx 9060xt 16gb",pcmasterrace,2026-01-14 08:44:59,8
Intel,nzib09a,Get the B580 in my opinion,pcmasterrace,2026-01-14 08:45:09,6
Intel,nzkvgwx,Had the a770 LE loved it! Wasnâ€™t very powerful considering the specs but it was nice to see Intel actually putting effort in the Gpu space. I would personally go with the B580,pcmasterrace,2026-01-14 18:04:10,2
Intel,nzoamdv,Yup. Also does media processing very very good.,pcmasterrace,2026-01-15 04:38:15,1
Intel,o19b2m6,"Neat, such a shame motherboard manufacturers really dont like to give secondary PCIe slots x8 lanes.  This bodes well for intel card users that might upgrade in the future, me personally, I'd like to run this one in a dual setup with a 5070ti or 5080, if those ever become cheap again (lol, probably not).  Cool stuff mate.",pcmasterrace,2026-01-23 16:00:55,28
Intel,o19ct2x,"I currently run 2 gpus (6800xt and 3060 12G). I use the AMD for most things and the 3060 for image and video generation in comfy ui. I also have 5 displays so both cards have displays hooked up. I tried Ubuntu a few months back and both x11 and Wayland had major issues with the dual gpu setup. Iâ€™m currently planning to put the comfy ui stuff on my server (strix halo) and remove the NVidia card to try Bazzite, but thatâ€™s still a bit of a ways off.  Power consumption isnâ€™t great. It uses a lot of power sitting on the windows desktop (~400W) and about another 150W in Ubuntu on the desktop.   My recommendation would be if youâ€™re going to use dual gpu for anything, stick to one brand and make sure your motherboard PCIE supports it correctly.",pcmasterrace,2026-01-23 16:08:44,9
Intel,o19rkxs,"I feel you on the bus bandwidth issue. There are so many old cards like RAID controllers and 10GB+ networking gear that use PCI-E 8x. I'd rather have a second 16x or 8x slot than a bunch of m.2 slots.   I wouldnt even mind paying a bit more for extra IO, but it feels like the prosumer workstation tier died off years ago. You either spend hundreds on a motherboard or you get whatever $99 board is on sale.",pcmasterrace,2026-01-23 17:15:23,3
Intel,o19za5g,"I'm running an rx7800xt and a b570 (previously an acr a310 eco as secondary) and i gotta say the performance increase was phenomenal, my total power draw is actually down while still playing on ultra settings, and temps are stable.",pcmasterrace,2026-01-23 17:51:12,3
Intel,o1a6ty6,"throwing in that the taichi aqua z890 is excellent for 2 cards, both at pcie5 8x",pcmasterrace,2026-01-23 18:24:51,3
Intel,o19edn5,"I have a X870 TUF Gaming and it has 2 16 PCIe slots, one is PCIe5, the other is PCIe4. Still AM5 though and it was around 230 bucks.  So you don't need to spend 1500 bucks on a motherboard.  Edit: I just checked on Amazon and the Taichi is 300 bucks. Where did you get that $1500 price from?",pcmasterrace,2026-01-23 16:15:45,2
Intel,o1a1onv,"One interesting thing to note is that even if you have enough PCIE 8/16x slots, if your CPU canâ€™t handle that many lanes then the chipset will take the rest, and that will reduce speeds a good bit. Even if it doesnâ€™t have to bifurcate the lanes. Itâ€™s ok for storage, but not ideal for GPUs. Like for example an Intel 12700k has 20 PCIe lanes that it can support directly, so after that, everything else is done on the chipset.",pcmasterrace,2026-01-23 18:01:58,1
Intel,o1ae16x,"Just a thought, you can use that Intel card as your dedicated frame gen card with the Lossless Scaling application on Steam. I haven't done it myself but I've seen several videos testing it and it looks like it works great!",pcmasterrace,2026-01-23 18:56:57,1
Intel,o1asxbt,"I've run dual gpu (albeit same brand) on multiple systems, and the result is mixed. In some games like E33 where parry timing is crucial, the moderate frame gains can be overshadowed by the perceptible latency. And god forbid you alt/tab, sometimes the frame gen would freeze on a certain frame and can only recover by turning scaling off and back on. For gaming it wasn't as game changing as I hope it would've been.   Though I should mention dual gpu for creative softwares is definitely a nice gain, I've set the windows graphics settings to use 1 gpu for premiere and 1 gpu for photoshop/lightroom, and it does speed up the workflow for both.",pcmasterrace,2026-01-23 20:06:30,1
Intel,o1au86l,"Do you currently have your monitors plugged into your main gpu? As far as I know, for the LS setup, you want to have displays connected to your secondary gpu at all times and set your games to run off the main.  This skips the need for main gpu having to transfer data to secondary and back.",pcmasterrace,2026-01-23 20:12:40,1
Intel,o1b63e8,"Yup, pretty much my experience as well. I have a 3080 and a spare 2060.  I actually managed to get to a net positive outcome rendering the hard frames at 1080p and outputting through the 2060, but the amount of unnecessarily fiddling that requires is absolutely not worth the somewhat better framerate and painfully obvious worse latency.  My 2060 is back to collecting dust.  note: I had zero driver issues mixing different generations",pcmasterrace,2026-01-23 21:08:50,1
Intel,o1c6zd1,"I hadn't heard of this, but also interesting that I have a proper X8/X8 board (Gigabyte X870E Aorus Master X3D). The other question is does Lossless Scaling work with the integrated GPU on the CPU like the Ryzen's typically have?",pcmasterrace,2026-01-24 00:14:21,1
Intel,o197zd3,What an absolute stupid idea. If this is/was a thing there would be many many dual GPU systems reusing old GPUs around.  Surprise! There are none.,pcmasterrace,2026-01-23 15:47:06,-46
Intel,o19bn6e,Totally agree.,pcmasterrace,2026-01-23 16:03:28,6
Intel,o1ci37j,"From what I know it is not a problem coming from the motherboard. This CPU only has 24 PCIe lanes, it is far from enough to get 2 PCI ports at 16x and a nvme at 4x. The only way was to use the discontinued HEDT platform was offering that, with chipset like X299 adapted to i9-10980XE and its 48 lanes.  If you still need some additional PCIe lanes, you can use a Xeon W9 on W790 platform and you will have 112 PCIe lanes 5.0...",pcmasterrace,2026-01-24 01:15:38,2
Intel,o1a0gks,"From testing different set ups, it is better to use different manufacturers unless you're using two gpus from the same generation (3090+3060, 6800+6600, etc). I've tested a few set up of just amd and Nvidia, and it only works if it's the same generation. You can't reliably run a 30 and 10 series. You can't run a 7000 with a rx580. ... Well you can, but you may have to mess around with older drivers and such.",pcmasterrace,2026-01-23 17:56:31,4
Intel,o19ejdo,"Thank you for advise. I chose two different brands because most of the materials about dual gpu contain a combination of two different brands. Some even say that in this way you can use the advantages of both brands of video cards. For example, good performance from AMD and better codec from Nvidia or Intel.",pcmasterrace,2026-01-23 16:16:28,1
Intel,o1ahaqy,"You dont have enough lanes to run 2 16x slots on consumer CPUs anyway. And with 16+8 you wouldnt have enough for a single NVME drive.  Most of the older ""slower"" expansion cards run fine on the chipset slots.",pcmasterrace,2026-01-23 19:11:57,2
Intel,o19f533,"Sorry, I forgot to convert my currency to dollars.  *going to fix it.",pcmasterrace,2026-01-23 16:19:10,4
Intel,o19sb7h,Look carefully at the slots. The lower 16x slot isnt physically 16x. It has the 16x plastic connector but it only has 4x pins.,pcmasterrace,2026-01-23 17:18:53,4
Intel,o1ahdan,"Please read the post first)  >The whole frame generation idea(Lossless Scaling) slammed into the x4 bus bottleneck. Some say x4 is ""enough for video transfer"" comparing to capture cards that mostly use x4... that's BS, deception, scam! From 160 clean FPS on RX, I dropped to \~82 FPS with half-second perceptible lag (it feels it).",pcmasterrace,2026-01-23 19:12:17,1
Intel,o19a0k0,"Username checks out, theres an entire discord community dedicated to it. But making shit up on the internet is easier i guess",pcmasterrace,2026-01-23 15:56:12,15
Intel,o19aidf,Can you read?,pcmasterrace,2026-01-23 15:58:24,6
Intel,o19zoor,"There's literally an entire subreddit for this, where we share testing data, help each other with settings, there's a discord, the program is on steam. Just because you live under a rock doesn't mean you can try to invalidate facts with your uneducated feelings.",pcmasterrace,2026-01-23 17:53:01,3
Intel,o19d2gc,"Update us on how the x8 PCIe dual slot goes mate, I'd like to see if its worth it for me as well.",pcmasterrace,2026-01-23 16:09:53,4
Intel,o1b6s9v,That makes sense too. I doubt there is much support for dual drivers from a single manufacturer.,pcmasterrace,2026-01-23 21:12:07,1
Intel,o1arff7,"Yep, it's a shame that Threadripper didn't just stay at the high end level, but it went full pro and out of reach entirely.",pcmasterrace,2026-01-23 19:59:24,3
Intel,o1av023,"The bandwidth is there to use older gen PCI-E 8x cards, its just cheaper not to provide the physical PCI-E 8x traces.   My old Highpoint 2720 SAS controller or Intel 10GBE cards definitely take a hit running at 4x. I guess depreciation of older gen stuff is inevitable but I wish somebody offered a secondary 8x slot for a decent price.",pcmasterrace,2026-01-23 20:16:20,1
Intel,o1aiqbu,"I skimmed it, no one is going to read all that. Everything I saw was focused on encoding.  Lossless Scaling 2x frame gen works great for me with no perceptible lag in any game I tried.",pcmasterrace,2026-01-23 19:18:40,-1
Intel,o1bzfai,Can you share your Lossless Scaling settings?,pcmasterrace,2026-01-23 23:33:29,1
Intel,o1ed25z,"Lmao that dude is an ignorant fuck. For the record, I have the same issues as you. 3070 running on an x4 bus, if I try to use lossless scaling it halves my fps, introduces lag, screen tearing, makes it unplayable.",pcmasterrace,2026-01-24 09:16:10,3
Intel,o16x91o,"Forza Horizon has always been very well optimized, hopefully that doesn't change this time.",pcmasterrace,2026-01-23 06:03:05,13
Intel,o16saox,thats really reasonable,pcmasterrace,2026-01-23 05:26:05,9
Intel,o16hk03,At least it's not the UE5 engine.,pcmasterrace,2026-01-23 04:14:40,3
Intel,o177vj5,forza is one of the best optimized games ever made,pcmasterrace,2026-01-23 07:30:47,3
Intel,o16h4tw,"Not bad really. But then again, what do they consider playable?",pcmasterrace,2026-01-23 04:12:05,4
Intel,o16la15,"I mean that seems pretty fair. I'd say you can get a pretty good idea of what they consider ""minimum"" once they release the recommended system requirements. If it jumps all the way up to like a 3080 or something then you know their idea of ""minimum"" is barely playable.",pcmasterrace,2026-01-23 04:38:29,2
Intel,o172ew4,"Considering theyre requiring a 16xx gpu...the game isnt going to use rtx or lumen for the lighting by default. Those two things alone will kill frames. Also consider how many special effects they are expecting to go off at a time compared to something like a fps...smoke...thats about it. Alos, since its a racing game and the expectation is that people are going to go by everything at 150mph, they dont have to be tedious about texture sizes too much on anything other than cars.   Sure they can add settings (and probably will) for all that, but its a 16xx with fsr will run this well enough at 1080p. When you look at most games, the reason their minimum requirements are anything more than a 10xx or 16xx is because theyre using rtx and lumen by default. Also when you look at most games recommended setting, its usually 1440p with rtx and that does require quite a bit better card. But as far as straight raster at 1080p, a 1080ti is still (rip driver support) more than capable...especially since you can run fsr3 on it if you need a fps boost.  So tldr, its probably not that its optimized as much as they arent requiring rtx for lighting.",pcmasterrace,2026-01-23 06:44:25,1
Intel,o181a2q,im wondering if i can play fh6 on my current setup with close to or as same settings and fps and fh5 i have 5600x and rx6700xt build and i get 140 fps 1080p on high/ultra settings,pcmasterrace,2026-01-23 11:51:31,1
Intel,o19rfb6,what about the maximum ultra settings requirements?,pcmasterrace,2026-01-23 17:14:39,1
Intel,o1arln0,It has to run on the Xbox series S so it's never gonna be too hard to run,pcmasterrace,2026-01-23 20:00:11,1
Intel,o172izr,">GRAPHICS: Nvidia GTX 1650  **""My pledge is eternal service""** \- Demetrian ""i just found all the 40k warhammers"" Titus",pcmasterrace,2026-01-23 06:45:20,1
Intel,o16oecp,I can run FH4 on my i7 4th gen. I'm going to try to run FH6 and see.,pcmasterrace,2026-01-23 04:59:06,0
Intel,o16hffh,720p at 30 fps is my guess.,pcmasterrace,2026-01-23 04:13:53,8
Intel,o16j3wo,"my guess would be something like 1080p30 medium or high, maybe 1080p60 low",pcmasterrace,2026-01-23 04:24:28,2
Intel,o172rzt,"Unless recommended is 1440p with rtx, then a 3080 is a reasonable card, turn off rtx and run in 1080p, i could see a 1080ti running at 60fps...especially with fsr3 or xess.",pcmasterrace,2026-01-23 06:47:24,1
Intel,o1dab5w,"The recommended settings could very well have ray tracing in mind, so that is not necessarily true.  FH5 has ray tracing (but only in the garage) so I'd imagine fh6 will have it out in the open world",pcmasterrace,2026-01-24 04:03:53,1
Intel,o19rsyu,"ray tracing was already in FH5, but in a very sterilised form that barely had an effect on performance",pcmasterrace,2026-01-23 17:16:26,1
Intel,o1asrox,that's true of all the poorly optimized games we've been getting too (besides the sony exclusives),pcmasterrace,2026-01-23 20:05:45,1
Intel,o16lh6v,"Look at Mr optimistic over here! Lol.   You're probably right, I'm just jaded.",pcmasterrace,2026-01-23 04:39:46,1
Intel,o1awzq3,Iirc it was only in photomode,pcmasterrace,2026-01-23 20:25:44,1
Intel,o16mcuh,It's based on how the previous two Forza games ran on their given minimum requirements (which are very similar).,pcmasterrace,2026-01-23 04:45:32,1
Intel,o11aqvf,Really nice budget build. Solid cable management. Picture taking skills need improvement. But you get a 9/10 because you installed the mobo speaker. Nice job.,pcmasterrace,2026-01-22 12:32:25,3
Intel,o11azmd,"Oh and that front fan, might want to put that closer to the top where it can blow by the ram and board. Down there itâ€™s just feeding the PSU and maybe slightly the GPU. Put it near the middle to the top.",pcmasterrace,2026-01-22 12:34:04,1
Intel,o11bilt,"Clean build. Next upgrade: GPU, then monitor refresh rate.",pcmasterrace,2026-01-22 12:37:37,1
Intel,o11brd2,Slick! B580 is a solid choice too. Next I think some RAM heatsinks would do good for your aestetics.,pcmasterrace,2026-01-22 12:39:13,1
Intel,o12frp7,It will run a lot better if you get a different cpu.  The G series cpus have like no L3 cache.  Sell the G and get an X,pcmasterrace,2026-01-22 16:06:28,1
Intel,o13dpg5,Nice.  Good job.,pcmasterrace,2026-01-22 18:38:03,1
Intel,o11jwzq,"Oh i didntÂ´t realize that!, i will change the fan position, its a very good tip, thank you",pcmasterrace,2026-01-22 13:28:21,1
Intel,o11k5p9,"I think i will hold to this until ram goes down, then migrate to an am5 system. But if that takes long, maybe a 9070 XT would be nice, also upgrading from 1080p to 1440p",pcmasterrace,2026-01-22 13:29:43,1
Intel,o11ksxz,"i was doubting between B580 or 9060 XT (16GB), i chose the Intel GPU cause it was less money, so i could upgrade my motherboard, cpu fan and PSU and get a more solid base. I may upgrade the GPU later, but it all depends of how the shortage develops. About the heatsinks, do they sell those separately? i didnÂ´t know that",pcmasterrace,2026-01-22 13:33:17,1
Intel,o1491zv,"that is exactly what i am going to do, i got a Ryzen 7 5700X for like 135 euros, i expect to recover part of that selling the 5600G",pcmasterrace,2026-01-22 21:01:27,2
Intel,o12a5jp,Good choice,pcmasterrace,2026-01-22 15:41:00,1
Intel,o1158kp,why we buying a 9070xt for cs2 exclusively,pcmasterrace,2026-01-22 11:53:26,169
Intel,o113s3i,I see no point in buying 5070 over 9070xt   5070ti vs 9070xt yeah maybe,pcmasterrace,2026-01-22 11:42:32,620
Intel,o1150si,"Cs? You can buy whatever you want, even some other cards. Id get the cheaper one",pcmasterrace,2026-01-22 11:51:52,67
Intel,o113c35,Buy whatever is cheaper,pcmasterrace,2026-01-22 11:39:04,103
Intel,o11253i,"i am a 9070xt owner. i delibrately went amd this generation to see whether the drivers always were broken like some claimed or it is just made up by nvdia shills. and my experience so far been it been running pretty smoothly except some couple bugs with deselecting auto update not working and some instability with vr on some drivers. so for me it is not as broken as some claims, it is really just couple drivers out of many that cause issue. can avoid most issues if don't update every driver version.",pcmasterrace,2026-01-22 11:29:34,59
Intel,o113u6u,cant think of one reason to go with the 5070 if it is compared to a 9070xt.  The 9070xt is the clear winner in that comparison if they are priced similarly,pcmasterrace,2026-01-22 11:42:59,55
Intel,o114pqb,"What driver issues lol, just get the 9070xt.",pcmasterrace,2026-01-22 11:49:35,46
Intel,o12d7s7,Pick the one that DOESN'T use 12VHPWR connector,pcmasterrace,2026-01-22 15:54:56,6
Intel,o1133w9,"I have both. When it comes to production work loads Nvidia is still king, however, if you are just concerned about gaming, go with AMD.",pcmasterrace,2026-01-22 11:37:18,11
Intel,o113lsm,"Between those two cards I would definitely go with the 9070xt especially if you only play CS2.    The main benefit of the 5070 is the quality of DLSS, but the 12gb are such a negative, that I would go with the 9070xt especially since CS2 runs good enough to not need any upscaling.    For very popular games like CS2 AMD drivers are not an issue, if you play some very niche games or use some CAD software then Nvidia might be the better choice.   Regardless of which one you choose you should buy as soon as possible as prices are increasing very quickly.",pcmasterrace,2026-01-22 11:41:11,30
Intel,o1246nj,"You only play cs2? Respectfully, how tf is this even a question. You're never ever going to be take to take advantage of 400fps, or 430fps for the nvidia cars. N e v e r.   Get the cheaper one and be done with it.",pcmasterrace,2026-01-22 15:12:33,3
Intel,o115xa3,"I do not understand the other comments, the 5070 is for your usecase likely better since you are only playing CS2, which neither needs more then 12GB VRAM or equal. The used value of the 5070 will likely also hold better then the 9070XT.  According your benchmark above you have 10% more FPS with the 5070 in the game you \*\*only play\*\*.",pcmasterrace,2026-01-22 11:58:27,11
Intel,o112w5k,"As a 5070 owner who recently upgraded to a 9070 XT, Iâ€™d highly recommend the 9070 XT. It has more VRAM, more overall performance (9070 XT is roughly equal to, though ever so slightly weaker than the 5070 Ti), and stronger native processing. The 5070 does have DLSS 4 and 4.5 (which are more widely available than FSR 4), but thereâ€™s not much else to sway me back.",pcmasterrace,2026-01-22 11:35:36,30
Intel,o116itt,AMD is the way to go if youâ€™re at all interested in moving away from Windows towards Linux at any stage.,pcmasterrace,2026-01-22 12:02:49,2
Intel,o117apc,"If you currently use, or plan to use, Linux then go with the 9070XT, if not then it pretty much comes down to a bunch of personal preference questions, I especially don't think that extra 4gb of vram is likely to do much for CS2, though I could be wrong as I haven't played since it replaced CS:GO.  That said, I have a personal hatred for Nvidia so I would suggest the 9070XT if you were to directly ask me :P",pcmasterrace,2026-01-22 12:08:28,2
Intel,o11ie4s,I have a rx 7800 xt and this numbers must be completely wrong,pcmasterrace,2026-01-22 13:19:46,2
Intel,o12lpdi,Get a 1440p monitor,pcmasterrace,2026-01-22 16:32:57,2
Intel,o13jqfm,You only play cs2?   What ever is cheaper dude. You will be maxing any monitor you own with both of those cards lol,pcmasterrace,2026-01-22 19:04:39,2
Intel,o14q9wn,Keep in mind CS is also CPU-bound game and in 1080p you better have a good cpu to get high fps. Yes at 1080p the 5070 looks better. At 2k you will see the 9070xt performs better.,pcmasterrace,2026-01-22 22:25:17,2
Intel,o15arwh,Cs2 1080p is not a good benchmark tool,pcmasterrace,2026-01-23 00:14:31,2
Intel,o15d2qi,"CS player here. TLDR:  * If you play on 4:3 aspect ratio, **it literally doesn't matter pick the cheaper card** * If you play on 16:9 or 16:10, then go for the 9070XT.   Explanation: (I'm assuming you have a good CPU and a 144hz monitor at least.)  In terms of raw raster power, which is mostly what is required in a badly optimized game like CS2, the 9070XT shines. You will notice a consistently ""smoother"" gameplay when gaming at higher resolutions since those 1% lows will hit hard and often ingame. In my experience the frequent drops in fps are mostly what affects performance and responsiveness, and why CS2 feels sluggish and inconsistent sometimes.       If however you're gaming at 4:3 the extra VRAM will not matter as you will consistently have more than 144 fps in any scenario, which matches your monitor's refresh rate (or 240 fps if you have a good cpu and a higher refresh monitor). So 5070 and 9070XT will have the same performance at 4:3 aspect ratio.  For what it's worth, I have the 5070 and my main game is CS2, no issues whatsoever at 4:3 1280x960. I also play other games at 1440p most at Max/High settings with no issues whatsoever. The 5070 is more than enough for your needs imo.",pcmasterrace,2026-01-23 00:26:35,2
Intel,o15dl99,"If it was my choice I'd pick 9070xt, because [fuck nvidia](https://youtu.be/iYWzMvlj2RQ), but from what I know for your needs both cards will work",pcmasterrace,2026-01-23 00:29:16,2
Intel,o15exy3,If the price is the same go for the 9070xt,pcmasterrace,2026-01-23 00:36:22,2
Intel,o15fskc,"Intel is a new discrete card vendor, AMD's driver issue story is a decade old, even then it was overstated (I ran a HD7870 in 2022 still). Sure there were issues, but so had nvidia,like how vr was a stuttery mess for like a year recently. If it's not a typo and you mean 5070 not ti, it's not even up to debate I believe, AMD all the way. Solid cards, more vram, ray tracing has closed significantly, and linux support is night and day, I switched on nvidia recently which had some annoyances, since going 9070XT it's pretty much plug and play gaming. I don't think dlss is worth the hype, sure it can be better than TAA, but only because TAA is dogshit, and yeah nvidia is kind of a step ahead, but AMD does keep up. I picked a 4070 before this thinking dlss and rt and reflex is worth it, and felt disappointed by the end, sure they are alright, but not a big deal at all, or something I'd use in the first place given the choice.",pcmasterrace,2026-01-23 00:40:52,2
Intel,o15hyzc,"9070xt, incase if u wanna switch to linux.  As AMD works out of the box with Vulkan on linux.",pcmasterrace,2026-01-23 00:52:28,2
Intel,o15isgz,"that 12gb vram wont cut it  trust me, new unoptimize slop keeps pumping out of the UE oven and you need those extra vram",pcmasterrace,2026-01-23 00:56:56,2
Intel,o15x2lf,how are people picking a 5070 over a 9070 XT? Like the benchmark show everything is better on a 9070 even one that isnâ€™t an XT. Are people just at blind to being fanboy of Nvidia?,pcmasterrace,2026-01-23 02:17:17,2
Intel,o165si5,">only play CS2  buy a GPU that starts with RX, RTX, or hell, even GTX for as cheap as possible on Facebook marketplace or something. You do not need that hardware",pcmasterrace,2026-01-23 03:05:09,2
Intel,o16i3ja,Outdated benches old drivers,pcmasterrace,2026-01-23 04:18:05,2
Intel,o11p82z,No brainer: 9070 XT is somewhere btw 5080 and 5070 Ti. And cheaper as a 5070 Ti.,pcmasterrace,2026-01-22 13:56:59,5
Intel,o11men0,Save yourself from your PC burning itself up and get a 9070 XT  12VHPWR is a legitimate fire hazard at this point,pcmasterrace,2026-01-22 13:42:00,3
Intel,o12j8bj,"I noticed only CS2 the 9070xt doesn't get good performance game needs an update for it, but everything else it's beyond the 5070, in Doom Eternal it manages to overtake everything.  https://preview.redd.it/yd96g6hdfxeg1.png?width=1080&format=png&auto=webp&s=b2b8045e41167900a8e34c2b5675595943991e95",pcmasterrace,2026-01-22 16:22:01,2
Intel,o1124lo,9070xt to boycott nvidia.,pcmasterrace,2026-01-22 11:29:28,8
Intel,o113270,even if you have a 1440p 360hz a 5070 should be fine.,pcmasterrace,2026-01-22 11:36:55,2
Intel,o114py1,I bought the 9070xt after my 3080. It was vastly cheaper compared to a 5070ti and has better performance.  I highly recommend it.,pcmasterrace,2026-01-22 11:49:37,3
Intel,o119zzz,https://preview.redd.it/uck3cx9i9weg1.png?width=3088&format=png&auto=webp&s=2fbf18fc990f05563ff3c41c64f62a0883612b31,pcmasterrace,2026-01-22 12:27:21,2
Intel,o1148mk,"I remember 1.5 years ago I was in a similar position choosing between 7900 GRE and 4070 Super for primarily CS2 and secondary GTA V.  The comparison videos I saw pointed hard towards 4070 Super in CS2 in terms of - FPS, temperature and power consumption. Made the choice easy for me. For GTA, it was very similar so didnâ€™t make a difference. I recommend checking YouTube benchmarks for the exact game you want to use the GPU for, donâ€™t go with what a generalistic FPS comparison across games or with what other users tell you (because they might be playing very different games than you).  (I play 1440p 165 Hz with G-sync on, meaning locked to 158 FPS).",pcmasterrace,2026-01-22 11:46:02,1
Intel,o115sg4,The answer is staring at you.,pcmasterrace,2026-01-22 11:57:28,1
Intel,o115wdy,"Had/have both. The 5070 was fine, but as soon as I found a 9070XT at a great price in March of last year, I sold that Zotac 5070 Solid to a friend.  Hate to say it, but ultimately, I looked at the plugs, kept seeing issues with users having that cursed 12VHPWR fail. I also considered that even with a SFX power supply (my case is a T1 v2.5), there was still plenty of open connections to feed the extra 8+2 ports of the particular ASUS Prime OC 9070XT 16GB that I swapped to. Cable routing was more inconvenient.  Sold it to a friend that didn't care about all that. He got a card right after his older card died, and I was able to move onto the card I really was targeting (bought the 5070 in February).  If my purpose/wishes required a 5080 or even 5090, then I might have taken my chances long-term on relying on that 12VHPWR as a necessary evil/risk. Seeing as the 9070XT can compete with the 5070TI, but use the more proven power connector design, I would choose the more reliable option.",pcmasterrace,2026-01-22 11:58:15,1
Intel,o1168ct,Between these two cards the 9070xt for me. More VRAM and nvidia don't really compete with this card without dlss. I recently purchased a 5070ti because I wanted the dlss and 16gb for work but with your stated use case buy the 9070,pcmasterrace,2026-01-22 12:00:41,1
Intel,o117bpm,Buy both,pcmasterrace,2026-01-22 12:08:39,1
Intel,o117lpl,Xt,pcmasterrace,2026-01-22 12:10:37,1
Intel,o117mlp,9070 for if you want to possibly dual boot windows with Linux/bazzite. 5070 for everything else,pcmasterrace,2026-01-22 12:10:48,1
Intel,o117pwl,Wait for Super cards /s,pcmasterrace,2026-01-22 12:11:28,1
Intel,o117tos,Do you have a monitor this would make a difference at?,pcmasterrace,2026-01-22 12:12:14,1
Intel,o1184a4,>The price is the same btw  I find it borderline impossible but if price is indeed the same then pick way better card which is 9070 XT.  5070 Ti for the same price of course but not 5070 non-Ti.,pcmasterrace,2026-01-22 12:14:18,1
Intel,o1188my,"Depends on what you want? For gaming only take the AMD. If you want to do something else that requires Cuda cores, take the Nvidia. It's that simple.  I personally took 5070ti, because I need it for CFD, but otherwise I would have taken 9070 XT Nitro+",pcmasterrace,2026-01-22 12:15:09,1
Intel,o118o97,"Are you intending to play at max settings? Do you have an AM5 X3D CPU?   Because realistically, there shouldn't be any difference between the two graphics cards for CS2 otherwise, as you'll be entirely CPU limited.   Benchmarks like TPU use max settings to show the scaling between graphics cards, which is good for benchmarks as otherwise they would just be measuring the CPU, but is effectively turning CS2 into a synthetic test.",pcmasterrace,2026-01-22 12:18:11,1
Intel,o119fzj,"Neither one is needed for CS2. Whatevery you have now, is probably still fine for CS2.",pcmasterrace,2026-01-22 12:23:34,1
Intel,o119k2m,Only reason i went for the 5070 was cause i had an G-sync screen. Other than that iâ€™d buy the cheapest.,pcmasterrace,2026-01-22 12:24:21,1
Intel,o119n7j,Depends on fsr4 support.,pcmasterrace,2026-01-22 12:24:57,1
Intel,o119t0p,If you don't play VR get AMD,pcmasterrace,2026-01-22 12:26:03,1
Intel,o11a2vs,With lossless scaling there are very few games that raw GPU power isn't the best option. That duck  ðŸ¦† destroyed Jensen and his ridiculous leather jackets. Want 20x frame gen? ðŸ˜‚,pcmasterrace,2026-01-22 12:27:54,1
Intel,o11awqw,The more you buy the more you save.,pcmasterrace,2026-01-22 12:33:32,1
Intel,o11axw5,"This post makes zero fucking sense, you had problems with intel arc drivers so you donâ€™t want a 9070xt?",pcmasterrace,2026-01-22 12:33:45,1
Intel,o11be0x,If you only play cs2 you definitely dont need a 9070xt,pcmasterrace,2026-01-22 12:36:45,1
Intel,o11bmcd,You need to decide yourself. Do you really plan on playing no other game?  Sure 440 vs 400 seems obvious if you need that level of performance.,pcmasterrace,2026-01-22 12:38:18,1
Intel,o11bml3,"9070XT doesn't burn its connectors, at least I haven't seen one yet.",pcmasterrace,2026-01-22 12:38:20,1
Intel,o11bn2p,same price ? get 16pin 9070xt.,pcmasterrace,2026-01-22 12:38:26,1
Intel,o11bt9i,Always nvidia,pcmasterrace,2026-01-22 12:39:34,1
Intel,o11c0a3,Imo 9070xt from one of the board partners that doesn't use the 12 pin that could melt sooner or later,pcmasterrace,2026-01-22 12:40:50,1
Intel,o11cacz,"i would go for the 5070. It has the better feature set, if u are interested to use them in other games beside cs. If you just want to play cs, go for the amd one and enjoy a few more bucks in your wallet.",pcmasterrace,2026-01-22 12:42:40,1
Intel,o11ci1p,"9070xt is far superior to vanilla 5070 and more future proof due to the extra ram  5070ti on the other hand is the slightly superior card, but if the price difference between it and a 9070xt is more than, let's say 100 bucks, not worth it imo.",pcmasterrace,2026-01-22 12:44:02,1
Intel,o11d193,"If they are the same price or in a similar priceclass I'd go 9070XT it is comparative to a 5070ti, I primarily play RPG's on 1440p and I get around 140fps on games like KCD2.  If a 5070 is much cheaper you may wanna consider it depending on your budget, but you get a comparatively worse GPU with 12GB Vram.  That said if you can get a 5070ti for the same price or at least within 100-150 bucks of a 9070XT i'd consider that.  But between the 5070 and 9070XT, 9070XT all the way.",pcmasterrace,2026-01-22 12:47:26,1
Intel,o11d5bl,395fps?,pcmasterrace,2026-01-22 12:48:08,1
Intel,o11d6hb,I've got a 9070xt and I couldn't be happier. I am happy that I picked it up a month ago though before the price jumped over $100 more,pcmasterrace,2026-01-22 12:48:21,1
Intel,o11d9e8,6000 go for broke...,pcmasterrace,2026-01-22 12:48:52,1
Intel,o11dija,9070xt,pcmasterrace,2026-01-22 12:50:30,1
Intel,o11eak3,Get nvidia bruh just be done with it cpus itâ€™s a f different story,pcmasterrace,2026-01-22 12:55:23,1
Intel,o11ek5h,Do you actually play at 1080p or a higher resolution? 1080 is really bad at comparing GPU's.,pcmasterrace,2026-01-22 12:57:02,1
Intel,o11exq6,"The RX 9070 XT is faster than even the RTX 5070 Ti in CS2 with current day drivers. If that's all you play and you don't do any other work with your GPU, buy it",pcmasterrace,2026-01-22 12:59:20,1
Intel,o11f4zh,"If you going to buy 5070, i would recommend to buy the TI version instead of the regular one. If you are on a tight spot with budget, go with the 9070 XT, while the 5070 has 12 Gigs, the 9070 XT has 16 Gigs. YES the 5070 is also a bit cheap but at that point i would rather go with the 9070 XT.",pcmasterrace,2026-01-22 13:00:34,1
Intel,o11fa0x,"If you genuinely only play CS2, it doesnâ€™t matter which one you get. I doubt you will notice the extra FPS if you donâ€™t run the counter. The extra 4GB would definitely be useful for down the road or other games and I have not had any driver issues with mine, but cannot guarantee your experience will be the same.   If you do choose Nvidia, I would suggest the ti for the extra VRAM. Understand as well that some people do still have driver issues. Due to the nature of the internet and fanboyism, I really couldnâ€™t tell you if it is actors frequent or not. It may have to do with specific settings or configurations. But positives. The game you claim you play exclusively is optimized for Nvidia as you can see by the performance boost. Nvidia software and features are indeed more robust and proven. They are at least one generation ahead of AMD with RT, PT and upscaling tech. Donâ€™t bother with frame generation in CS2 it adds input lag.   In short go with the one that fits your budget that you have enough confidence in. It will be in your system. Most people here will pick one brand or the other and try to say it is the only choice. That isnâ€™t true. Both have pluses and minuses, you need to decide which one is more positive to you.",pcmasterrace,2026-01-22 13:01:25,1
Intel,o11fuhk,"Walmart still has the PNY 5070â€™s at MSRP and some locations (US) have been clearance sales at an even bigger discount. I have no idea what AMD pricing looks like - last batch of Red I tempted my patience and fate with  was a pair of 6970â€™s in crossfire.   I would say get what makes the most logical, supportive, and financial sense for you, OP. Green has my vote tho.",pcmasterrace,2026-01-22 13:04:54,1
Intel,o11gcdl,But it doesnâ€™t seem to be better according to that benchmark? The 5070 seems to get 30 more FPS. If you only play CS2 and that game performs better on The 5070 it seems like a no brainer.,pcmasterrace,2026-01-22 13:07:51,1
Intel,o11h13s,"Both options are kinda overkill if you'll only going to be playing CS2 on it, unless you want to play at 4K or something. That said, the better option is whichever you find cheaper, which is most likely going to be the 9070XT. At the same price, I'd go with the 9070 XT.  Both GPUs perform at a similar level, with variations depending on the game. In general, the 9070 XT performs slightly better, sitting in between the RTX 5070 and the RTX 5070 Ti. But there are specific cases in which the 9070 XT can fall below both, and others where it can rise above both.  The main actually notable advantage of the 9070 XT is the higher VRAM capacity.  The main actually notable advantage of the RTX 5070 is DLSS being better than FSR.  AMD drivers haven't been a concern for this generation, and in fact, Nvidia drivers have done a lot worse since the beginning of last year.",pcmasterrace,2026-01-22 13:11:54,1
Intel,o11h6k5,You need 400 fps for cs2?,pcmasterrace,2026-01-22 13:12:47,1
Intel,o11hdyq,"Depends on pricing the 5070 has better performance on paper but getting one at MSRP is kinda hard, the 9070xt is a good card with solid performance and is easier to get at MSRP (at least near me).",pcmasterrace,2026-01-22 13:13:59,1
Intel,o11hi7z,"If on Linux and it is the same price, go with AMD. But always look at price and go with the cheaper option",pcmasterrace,2026-01-22 13:14:40,1
Intel,o11ho1q,"If theyre the same price I would grab the 9070 just because I dont trust that particular websites benchmarks.Â    But if you do and the game you play does much better with the NVidia card just buy that one! Just make sure you check more sources, just in case. If they play similar I would go for the one with more vram",pcmasterrace,2026-01-22 13:15:35,1
Intel,o11hqy5,"Different perf, different price. Depend on your budget.",pcmasterrace,2026-01-22 13:16:03,1
Intel,o11hyiw,"In my experience, 9070xt is really good performance. But AMD Adrenalin soft is hell, drivers are bugged, it bothered me so much.   So I bought a brand new 5070 Ti the same price I paid for 9070XT day one in March and sold my 9070xt for 700â‚¬",pcmasterrace,2026-01-22 13:17:17,1
Intel,o11i68g,Trust me get that 9070xt if you got the money the drivers aint bad anymore and watch a gaming conparison video between them,pcmasterrace,2026-01-22 13:18:31,1
Intel,o11itv1,"I see it this way:   Have an amd CPU? Get the 9070xt.   Have a Intel CPU? Get the 9070xt.   I do not have any first hand experience with the 5070ti. But I now have 6 friends in my gaming group that are running 9070xts, myself included, and we are all very happy with them. All of us also have ryzen x3d cpu's of one generation or another, and 3 of us come from Intel/Nvidia rigs before this.   I can say one thing: power consumption is way, way, way down and those 1% lows are oh so sweet and good.   I will probably get a 9850x3d and maybe that new phantomlink board and GPU. Idk. It's that or a couple of guns with the coming tax money.",pcmasterrace,2026-01-22 13:22:15,1
Intel,o11iz6g,"You get more, than 360fps, so for cs it does not matter, for other games (if you decide to play them) the 9070xt is better, also for the future 12gb vram could become a problem soonerâ€¦",pcmasterrace,2026-01-22 13:23:06,1
Intel,o11j6e5,"if youâ€™re exclusively only playing CS, get whichever is cheaper. if you want a better all around card, get the 9070xt",pcmasterrace,2026-01-22 13:24:12,1
Intel,o11jhco,If youâ€™re just playing cs2 then i suggest buying the cheapest gpu and the rest of the money goes to hacks just like everyone else,pcmasterrace,2026-01-22 13:25:54,1
Intel,o11jv3d,amd 16 GB cards. no brainer if you play on 1080p and competitive games it's the best value.,pcmasterrace,2026-01-22 13:28:03,1
Intel,o11kb2f,"The 9070XT is better than the 5070 To on average, often getting within a few fps of a 5080.  And compared to Nvidia cards right now, AMD can often be had much cheaper.",pcmasterrace,2026-01-22 13:30:32,1
Intel,o11kixe,If u are a happy nvidia user rn and donâ€™t want issues with drivers go with 5070 trust meâ€¦ i also heard a lot of good things about amd gpus and went for rx 5700 in my previous pc and the drivers and especially amd adrenaline overlay were driving me insaneâ€¦ luckily i switched to rtx 5070 now and it works just as i remember nvidia things work - just perfect,pcmasterrace,2026-01-22 13:31:45,1
Intel,o11kjnd,I have the 9070xt and I love it. My last card was a gtx1080.   I'm all about value and for me the 9070xt was Â£150-Â£200 cheaper than the equivalent Nvidia card. If it had been the other way them I would have got the Nvidia card. Prices are all over the place ATM though so check around for deals. My card has gone from Â£550-Â£650 since I got it.,pcmasterrace,2026-01-22 13:31:52,1
Intel,o11ktie,"Buy the 5070 for CS2 at 1080p if that's your focus, and accept the trade-offs if and when they become an issue later on.  Personally, I got the 9070xt because I play at 4k mostly and thus need the horsepower/vram, and the 5070ti was 20% more at the time.  Both will be good - the 9070xt is going to be better bang for the buck broadly, but certain applications/usages still favour nvidia.",pcmasterrace,2026-01-22 13:33:23,1
Intel,o11l4u5,Always buy the biggest GPU you can afford.,pcmasterrace,2026-01-22 13:35:06,1
Intel,o11lai2,At 1080p it won't make a difference. You could buy a 7900XTX and it would probably be your best bet.,pcmasterrace,2026-01-22 13:35:58,1
Intel,o11m7ns,Buy one with 8Pin connectors so you can safely use your PC without burning your house down.,pcmasterrace,2026-01-22 13:40:57,1
Intel,o11mr65,"5070 if you do more than just gaming, for example video editing or blender. if you only do gaming and wont do any productivity in future then go with 9070xt",pcmasterrace,2026-01-22 13:43:53,1
Intel,o11mvj0,Have a 9070 XT. Working great for me.,pcmasterrace,2026-01-22 13:44:32,1
Intel,o11og2p,DLSS beats FSR and itâ€™s more universally adopted,pcmasterrace,2026-01-22 13:52:54,1
Intel,o11ospq,"CS2 is a game that favours Nvidia cards, but in general for gaming - 9070XT is definitely better choice than 5070",pcmasterrace,2026-01-22 13:54:44,1
Intel,o11oufi,Those frame rates are so absurdly high to me. Iâ€™m happily on a 60Hz crt ðŸ˜ƒ,pcmasterrace,2026-01-22 13:54:59,1
Intel,o11pnhj,"9070 XT is at the level of a 5070 TI But you are saying the price is the same as the 5070?  9070 XT is a no brainer here, not even close.",pcmasterrace,2026-01-22 13:59:14,1
Intel,o11pv9b,Go with AMD. Because depending on microslops future you might change to Linux and AMD works fÃ¶awlessly most of the time on it other than nopevidia.,pcmasterrace,2026-01-22 14:00:21,1
Intel,o11rm3i,Remember..vram exists and the more vram you got the better it is unless you only care about fps against its competitors.  The 9070xt has 16gb of vram vs 5070 having 12gb of vram.,pcmasterrace,2026-01-22 14:09:27,1
Intel,o11s5z9,I choose the one that doesn't design shitty power management or hate it's consumers. But you do you...,pcmasterrace,2026-01-22 14:12:19,1
Intel,o11s6cj,"Tbh, if you only play cs2 then the best option is to buy a mid range card like a 3060 and the rest of your money to upgrade the cpu, or even upgrade your monitor if you already have a good cpu",pcmasterrace,2026-01-22 14:12:23,1
Intel,o11sx6x,"A cs player I know who plays esea main currently uses 9070xt, it's a good GPU for cs",pcmasterrace,2026-01-22 14:16:13,1
Intel,o11t8ck,"the 5070s are objectively worse in every way, don't even think about it and go buy the 9070XT.",pcmasterrace,2026-01-22 14:17:49,1
Intel,o11tf02,You want a 9070 XT for only CS2 at 1080p? ðŸ˜‘,pcmasterrace,2026-01-22 14:18:47,1
Intel,o11tptf,"if you ever plan on going Linux, right now AMD GPU's are the choice. While NVIDIA GPU's are playable in linux, windows still wins at the FPS.",pcmasterrace,2026-01-22 14:20:20,1
Intel,o11tspj,Either one seems super overkill for CS2 but do your thing,pcmasterrace,2026-01-22 14:20:45,1
Intel,o11u3t1,"It's 4 fps difference, just go Nvidia, you said had problems before with the other.",pcmasterrace,2026-01-22 14:22:20,1
Intel,o11v2h8,get the 9070 so you could have the chance to switch to steam os,pcmasterrace,2026-01-22 14:27:13,1
Intel,o11w81r,you have to compare the 9070XT with the 5070 Ti  12GB VRam is bad in 2026,pcmasterrace,2026-01-22 14:33:15,1
Intel,o11wjgw,Can someone tell me why the 3090 with 24GB is better than a 5070 with 12GB?,pcmasterrace,2026-01-22 14:34:54,1
Intel,o11wkqi,"Unpopular opinion, especially from someone that usually chooses AMD, RTX 4070. It's still good. You can get it with an 8 pin power connector. 200 watts. There's some that are short. And there's some that have all of these. Great card to do a 5L SFF.",pcmasterrace,2026-01-22 14:35:05,1
Intel,o11wrvy,"9070xt over 5070, 5070ti over 9070xt but the 300-500â‚¬ higher Ti.. is it really worth it? If you want ray tracing and dlss then maybe if you have extra cash for it, but since you play CS2 those won't even matter. I have 9070xt and I have no regrets buying it.",pcmasterrace,2026-01-22 14:36:07,1
Intel,o11xgkx,"9070xt.  Why care about dlss4.5 when the 9070xt can bruteforce the difference? And it has more vram, it's a no brainer at around the same price.  With that being said, buy a 2060 or something like that, chances are that it will be more than enough for cs2.   In my experience, the whole thing of bad AMD drivers must be either in the past, or the cope of a bunch of famboys, it just works as good as nvidia, in my case, it's even better, as the nvidia driver is a pos and generates some stutters in SteamVR for no reason",pcmasterrace,2026-01-22 14:39:39,1
Intel,o11xstq,9070xt unless you need cuda cores. More VRAM is going to be important going forward,pcmasterrace,2026-01-22 14:41:23,1
Intel,o11xxdt,Well I was contemplating the same thing. But walmart has 5070 for $400 new. Honestly love the the upgrade to 5070.,pcmasterrace,2026-01-22 14:42:01,1
Intel,o11yf4f,Get amd and go linux.i got 5060 but i wish i got amd for that linux perfomance.i am running nobaru on my card it runes fine but some title i lose 10% perfomance,pcmasterrace,2026-01-22 14:44:27,1
Intel,o11yh1r,"Hi guys, which page is that?",pcmasterrace,2026-01-22 14:44:44,1
Intel,o11zcr7,If you are playing 1080p then GPU isnâ€™t going to change thaaaat much,pcmasterrace,2026-01-22 14:49:05,1
Intel,o121mqk,You play CS on 1080p. The choice is basically irrelevant,pcmasterrace,2026-01-22 15:00:08,1
Intel,o121rtz,"I also was trapped between a 9070xt and a 5070. I could get them each for around 550â‚¬, but decided to go with the 5070. The reason was that the 9070xt takes a lot more power and I only have a 600w PSU. I would have needed to buy a new PSU for the 9070xt or needed to underclock it, which would has lost its purpose. I also plan to use it for different productive application where the 5070 Performance better.",pcmasterrace,2026-01-22 15:00:50,1
Intel,o121w92,You dont need either for cs,pcmasterrace,2026-01-22 15:01:27,1
Intel,o121wk6,for cs2?   cringe,pcmasterrace,2026-01-22 15:01:29,1
Intel,o122m2f,I have the 9070XT and have over 200FPS on 1440p in CS,pcmasterrace,2026-01-22 15:04:57,1
Intel,o123ee8,9070xt. Itâ€™s not even debatable. If you found a 5070ti for $750-800 Iâ€™d go w that,pcmasterrace,2026-01-22 15:08:46,1
Intel,o1245cz,"To be honest, an RX 480 could run CS2 fine  But if you insist: hereâ€™s a very biased answer based solely on vibes and nvidia hate:  Get the 9070 XT",pcmasterrace,2026-01-22 15:12:23,1
Intel,o1245ql,Iâ€™ve got a 7800xt and Iâ€™m getting over 500 fps in Fortnite,pcmasterrace,2026-01-22 15:12:26,1
Intel,o1255ut,i have 9070xt. itâ€™s a great card and runs lots of things 4k60+fps but i have so many driver issues with amd i think all th time about returning. i will keep trying to solve it though,pcmasterrace,2026-01-22 15:17:12,1
Intel,o125ezc,All these cards will perform perfectly fine for competitive gaming as long as itâ€™s not 4k max settings just get what suits ur budget and donâ€™t buy into the war between amd and navidia as these people are just gonna back whatever card they own XD Im running a amd card and Iâ€™ve never had an issue or a bottleneck so Im sure anything in ur budget will do fine (considering ur talking about the cards in the attached image),pcmasterrace,2026-01-22 15:18:25,1
Intel,o125mez,AMD drivers are fine. If you only play cs2 then both of these cards are overkill.,pcmasterrace,2026-01-22 15:19:25,1
Intel,o127i95,"If it's similar pricing/performance for your budget always get the Nvidia model for dlss, unless you need it for Linux.",pcmasterrace,2026-01-22 15:28:31,1
Intel,o127vez,"both gpus are the ""same"" bc they are competing in the same range. So get what you want, i think theres not much difference in performance. Also, the 9070 and 9070XT both have 16gbvram and the 5070 12gbvram/5070TI 16gbvram.",pcmasterrace,2026-01-22 15:30:15,1
Intel,o128rxw,But cs2 is cpu heavy game,pcmasterrace,2026-01-22 15:34:35,1
Intel,o129dow,"If you mostly play ""competitive"" games 100% get the 9070xt. Its a way better card for that, i would only recomend the 5070 if you're interested in the nvidia tech but those are completely irrelevant in games like csgo",pcmasterrace,2026-01-22 15:37:28,1
Intel,o12b4qd,nvidia shits on gamers heads.     and i choose based on that.,pcmasterrace,2026-01-22 15:45:27,1
Intel,o12bbyh,"9070 XT has better performance at everything except ray tracing.  If you don't play tons of games that require that, I believe it is the better option.",pcmasterrace,2026-01-22 15:46:23,1
Intel,o12bovp,Buy a 1080ti. Best GPU,pcmasterrace,2026-01-22 15:48:01,1
Intel,o12cq5q,"Even 9070 is better than 5070, XT is no brainer choice if you stuck in between these two.",pcmasterrace,2026-01-22 15:52:42,1
Intel,o12ev2k,4070 tiâ€™,pcmasterrace,2026-01-22 16:02:18,1
Intel,o12ggr5,I was making a similar choice between the 5060ti 16 GB and the 9060 XT 16 GB. I ended up going with the 5060 ti for CUDA compatibility for AI vision training and inference. If I only planned to use GPU for gaming I wouldve gone with the 9060 XT.,pcmasterrace,2026-01-22 16:09:38,1
Intel,o12gick,9070xt >> 5070 in EVERYTHING   also  9070 non xt still >> 5070 in EVERYTHING,pcmasterrace,2026-01-22 16:09:50,1
Intel,o12hf8v,Complete waste of money to play cs. You can buy something much cheaper.,pcmasterrace,2026-01-22 16:13:57,1
Intel,o12iczn,I would aim for more vram.,pcmasterrace,2026-01-22 16:18:09,1
Intel,o12iffb,Depends entirely on your budget. If you exclusively play CS2 and can't imagine playing any highly demanding games then you can judt go with a cheaper option. But if you ever eventually wanna play Marvel Rivals or GTA 6 then go with the 9070XT,pcmasterrace,2026-01-22 16:18:27,1
Intel,o12imyg,If you only play CS2 then save yourself a small fortune and get an older card?,pcmasterrace,2026-01-22 16:19:24,1
Intel,o12k3b0,I would personally choose the 9070xt since Ray Tracing and DLSS4.5 are not going to gain you much with the lower tier Nvidia GPU.  I'm saying this as an Nvidia user. Only get Nvidia if you can afford the 5070ti or better.,pcmasterrace,2026-01-22 16:25:48,1
Intel,o12kniv,"I was in the same boat as you this summer, except the 9070XT was about â‚¬100 more expensive than the 5070. I got the 5070 and don't regret it, it's a great card for the money (in today's market), but were they the same price, I'd have gotten the 9070XT.",pcmasterrace,2026-01-22 16:28:17,1
Intel,o12kt1l,9070 XT easily.,pcmasterrace,2026-01-22 16:28:58,1
Intel,o12ms9p,Support the card that doesnt use that connector that people post daily pictures of being a melty mess.,pcmasterrace,2026-01-22 16:37:42,1
Intel,o12mxt7,Kinda wasting the card using it at 1080p but 9070XT no question,pcmasterrace,2026-01-22 16:38:23,1
Intel,o12oc5a,Whichever has the best performance at your preferred price range.,pcmasterrace,2026-01-22 16:44:37,1
Intel,o12pnbf,"Truly,  are you really going to notice the difference?  Be honest, are you?",pcmasterrace,2026-01-22 16:50:28,1
Intel,o12pyuh,5070 Ti > 9070 XT > 5070,pcmasterrace,2026-01-22 16:51:55,1
Intel,o12qxjz,Glad to see my 6900xt still up on the list going strong,pcmasterrace,2026-01-22 16:56:14,1
Intel,o12qzov,"Rx 9070xt, its not even a question. The 9070xt rivals with the 5070ti, whilst the direct comparison to the 5070 is the 9070 non xt. Its a no brainer, it doesnt even matter if you have dllss4 or fsr4, the raw power and extra vram makes up for any upscaling you could put in there",pcmasterrace,2026-01-22 16:56:30,1
Intel,o12r6ti,"Not sure why anyone would ever buy AMD. If you're ever going to play any other games, DLSS and other technologies are light years ahead of anything AMD put out. I'd only ever recommend their CPUs.",pcmasterrace,2026-01-22 16:57:23,1
Intel,o12sk74,AMD - screw Nvidia and their AI bs,pcmasterrace,2026-01-22 17:03:39,1
Intel,o12stu3,I have a 9070XT and Iâ€™m happy with it.,pcmasterrace,2026-01-22 17:04:54,1
Intel,o12t4ke,9070xt gang +1,pcmasterrace,2026-01-22 17:06:17,1
Intel,o12txee,9070XT for sure but youâ€™re kinda wasting money if all you do is play CS on your computer. Could be a 2080 and youâ€™d still be fine.,pcmasterrace,2026-01-22 17:09:58,1
Intel,o12u0ff,9070 xt,pcmasterrace,2026-01-22 17:10:21,1
Intel,o12v5uc,"toss a coin, pick the cheaper one or pick the one you like the looks of better. they are so close, that one in will perform better in some games, the other in others.",pcmasterrace,2026-01-22 17:15:34,1
Intel,o12vmvv,"If all you play is CS2, get a 9060 or something... It's just wasted money buying a high price bracket card for this game, especially since the benchmarks are probably all on Ultra settings but nobody uses those for competitive.",pcmasterrace,2026-01-22 17:17:42,1
Intel,o12wi9y,"Good lord, there are a lot of DLSS haters in this thread, lmao.",pcmasterrace,2026-01-22 17:21:38,1
Intel,o12wrub,"It's been asked a billion times, why not search?",pcmasterrace,2026-01-22 17:22:50,1
Intel,o12wwbs,"If you're truly only playing cs2, just get the 5070 since it's cheaper.  I just got the 9070 XT myself and it's an awesome card if you plan to play other games.",pcmasterrace,2026-01-22 17:23:24,1
Intel,o12x6px,Never giving Nvidia my money ever again I don't care if I can't play GTA 6 on brain bleed mode it's not worth it. Besides they've made it very clear they don't give a fuck about home computer users.,pcmasterrace,2026-01-22 17:24:42,1
Intel,o12y11h,I just recently bought the 9070xt after years of Nvidia only and was also thinking about the 5070 and i just can tell you that i don't regret it at all and i'm more then happy with my choice.,pcmasterrace,2026-01-22 17:28:33,1
Intel,o12ybv0,Get a 5070ti 5070 is poopy,pcmasterrace,2026-01-22 17:29:56,1
Intel,o12yv5n,Buying a new GPU just to play a old FPS at min spec...,pcmasterrace,2026-01-22 17:32:21,1
Intel,o12ywi2,5070 any day of the week.,pcmasterrace,2026-01-22 17:32:32,1
Intel,o12z77h,If you only play CS you dont you can get away with a way cheaper GPU,pcmasterrace,2026-01-22 17:33:53,1
Intel,o12zfex,Are you going linux or staying windows?,pcmasterrace,2026-01-22 17:34:55,1
Intel,o12zkkm,Saving more money and go for the 5070 ti. Bought it a while ago. It's beast + DLSS 4.5 is a huge improvement.  Never know if in the future you are going to try a more demanding game or not. Just future proofing a little with current price of VGA,pcmasterrace,2026-01-22 17:35:34,1
Intel,o130io8,"Just buy whichever is cheaper, youre only playing CS2.",pcmasterrace,2026-01-22 17:39:52,1
Intel,o1328kw,Iâ€™ve always went Nvidia because my GPUs get recycled into my media server and until recently AMD just didnâ€™t have the support that Nvidia does..  in the last year or so Intel and AMD GPUs seem to have much better support..  if Iâ€™m just gaming and on a budget I donâ€™t think I could pass up the value of AMDs stuff.,pcmasterrace,2026-01-22 17:47:35,1
Intel,o1330dm,"If you're only playing CS 2 at 1080p you might as well save money and get a RX 9060XT or 5060Ti (16 GB models)  If you really want to play big games that are coming out in the next few years at high settings, like The Witcher 4 and GTA VI, I suggest the 9070XT.",pcmasterrace,2026-01-22 17:50:59,1
Intel,o134349,Honestly no matter which way you go your gonna have driver issues I have a 9070xt and have minimal issues if any with it. But you have to also think about the connector as well do you want the hassle of your cable nuking your card or not,pcmasterrace,2026-01-22 17:55:44,1
Intel,o1349za,9070xt with a set of standard power plugs because i refuse to have the fire hazard that is 12v2x6 in my house. canâ€™t really bring myself to support nvidia until they address that issue in a meaningful way,pcmasterrace,2026-01-22 17:56:34,1
Intel,o134wnj,"9070 xt is better, but depends on your budget really. Bith would be good",pcmasterrace,2026-01-22 17:59:20,1
Intel,o136c6t,For CS2 at 1080p just get like a used 1080 itâ€™ll do wonders,pcmasterrace,2026-01-22 18:05:43,1
Intel,o138k5n,"Nvidia are where it's at. It's just fact if you look at the software and AI development. They're ahead of AMD and it doesn't matter what mental contortions AMD fans do. I don't give a shit about brands I give a shit about reality of features and that's why I say Nvidia is where it's at currently for GPUs just as AMD are the clear winners in the CPU space. 5070Ti is more comparable (and better) than a 9070xt though, 5070 it becomes more questionable.  Who cares for your use case though it's absolutely ridiculous.",pcmasterrace,2026-01-22 18:15:34,1
Intel,o138zty,Reddit AMDcirclejerk will always tell you AMD. It's a cult.,pcmasterrace,2026-01-22 18:17:28,1
Intel,o1393jy,Iâ€™ll sell you my old shitty card that still clocks 180fps on CS2 if you want.  If youâ€™re only playing CS2 you barely need to think about what GPU to buy lol.,pcmasterrace,2026-01-22 18:17:55,1
Intel,o139ilv,Get a 9070xt I got mines at micro center for 599 msrp,pcmasterrace,2026-01-22 18:19:44,1
Intel,o139ssm,Ich hab the 5070 and switched to the 9070xt. It was a nice improvent,pcmasterrace,2026-01-22 18:20:59,1
Intel,o139xp6,"If itâ€™s for CS2 only then neither, you should be looking raw horsepower which the 7900XTX has in abundance over both of those cards.",pcmasterrace,2026-01-22 18:21:35,1
Intel,o13afhr,"I bought a 5070 FE used from a good friend who used it one week and decided he wanted something more powerful. He ended up with one of those Doom 5080s and sold me the 5070 below msrp and ebay prices for used. I love it, it is defiantly the nicest card I've had the chance to own.",pcmasterrace,2026-01-22 18:23:45,1
Intel,o13aq56,Whatever you get cheaper and/or faster (from a good source),pcmasterrace,2026-01-22 18:25:03,1
Intel,o13b02h,Id always default to Nvidia. Rock solid.,pcmasterrace,2026-01-22 18:26:13,1
Intel,o13e2e6,"If performance in a single game is so important, you should probably look up performance on the latest drivers as things change over time and reviews aren't updated to reflect this.  This is exactly the reason why it's a bad idea to based comparisons on a single game.  The 9070 XT has higher overall performance as reviews show, which means it's entirely possible a driver or game update pushes it's performance above the 5070.  Plus I don't know a single person that ended up only playing a single game on their GPU over it's life.",pcmasterrace,2026-01-22 18:39:38,1
Intel,o13fsy8,9070 plain. XT gives you 10% more performance for 20% more money and 30% more power draw. Bad deal for a marginal improvement.,pcmasterrace,2026-01-22 18:47:18,1
Intel,o13gr8c,Damn I have finally reached the point where my card isnâ€™t on these charts. Guess itâ€™s time to make a new build!,pcmasterrace,2026-01-22 18:51:31,1
Intel,o13ij08,"It seems like both those cards are overkill for what you're doing, why not a 9060XT 16GB or the equivalent nvidia?",pcmasterrace,2026-01-22 18:59:16,1
Intel,o13jh87,"If you're truly only going to play cs2, the 9070xt is only 5% faster at 1440p according to hardware unboxed's testing with a 9800x3d. With any other CPU, they'll perform identically.   The 9070 XT is the better card, but not for cs2. I don't get why everyone's missing that point in the comments",pcmasterrace,2026-01-22 19:03:31,1
Intel,o13lgly,"For  same price 9070xt, it's a no brainer, more rasterization power and more VRAM, it competes with the 5070ti. CS2 probably needs some sort of update, those numbers don't add up. Drivers in AMD are pretty stable just make sure to use DDU to uninstall any nvidia driver in safe mode or flat out format your hard drive (that's what i did).",pcmasterrace,2026-01-22 19:12:25,1
Intel,o13sgpv,"I used to play CS2 before I got BF6... It's great for it, on a 300hz 1440p screen getting about 450fps. Never had a driver issue at all on CS2, it will be more than fine.",pcmasterrace,2026-01-22 19:44:14,1
Intel,o13up7x,"just get whatever one you have the best deal on, everyone crashing out over dlss vs fsr but that doesn't even matter when you have 400fps at native res",pcmasterrace,2026-01-22 19:54:24,1
Intel,o13x2zo,"I am a 5070 user and I really enjoy perks like first access to the new upscaling models -- DLSS 4.5 -- and (the previously exclusive to NVIDIA GPUs) in-home game streaming with Moonlight. That said, I also really find myself wishing that I had 4 more GB of VRAM. You need to be careful not to saturate it because it will bork up your framerate and you will need to reset your game to fix it.",pcmasterrace,2026-01-22 20:05:25,1
Intel,o13y7aq,"I have a 4070 and it works great for cs2 at 1440p 240hz. the 5070 is a little better, so it's very good. if you have like a 360hz+ monitor than I'd say 9070xt, it definitely has much more raw performance than the 5070, and that's exactly what you want for a competitive shooter. If you want to save some money, or don't need that kind of power, 5070 is just fine",pcmasterrace,2026-01-22 20:10:38,1
Intel,o13zjap,"If you're looking just to have a nice card, Nvidia has been stocking the the 5070 founders on their site for $550. The 9070xt is $200 more, so it's not a comparison. the 5070ti though? I'd probably do the 9070xt because the games you play can nearly run on integrated graphics.",pcmasterrace,2026-01-22 20:16:47,1
Intel,o140up1,"Iâ€™m going to say this, AMD drivers have been a little rough for the past 6 months. 25.10.1, 25.11.1, 25.12.1 and the yesterday unveiled 26.1.1 all have issues in some games on windows. Iâ€™m talking driver timeouts, lower utilization, wrong clock speeds. Itâ€™s been rough, but I wonâ€™t fully blame AMD. The fact of the matter is I get 50-60 fps more in a lot of titles on Linux and zero crashes. My SO has a 5070 and on most of the games we play she makes 50~ fps less than me (sometimes even more). But on windows that can drop to 30 or even 20. Regardless the 9070xt is more powerful. Make sure to reinstall windows fresh, and update to 26.1.1. So far on this new driver Iâ€™ve had zero crashes on windows.",pcmasterrace,2026-01-22 20:22:55,1
Intel,o141ejh,2060 super. Best card ever made,pcmasterrace,2026-01-22 20:25:28,1
Intel,o141pn9,5070 ti small price jump huge upside,pcmasterrace,2026-01-22 20:26:55,1
Intel,o142ddg,9070 xt is better than the 5070 and when ocd can get near the 5080,pcmasterrace,2026-01-22 20:29:59,1
Intel,o142oko,"Get a 16GB card, whatever you do",pcmasterrace,2026-01-22 20:31:26,1
Intel,o143y17,"5070 Ti OC, the best compromise in my opinion",pcmasterrace,2026-01-22 20:37:24,1
Intel,o144zjl,9070XT,pcmasterrace,2026-01-22 20:42:19,1
Intel,o147rj1,"5070, obviously. But Iâ€™d go for the e 5070 Ti.",pcmasterrace,2026-01-22 20:55:22,1
Intel,o149bm5,If you play CS only go nvidia for multiple reasons like nvidia reflex etc.,pcmasterrace,2026-01-22 21:02:42,1
Intel,o14cl2r,If your playing csgo Iâ€™m assuming your playing g at 1080p or 1440p. Why you need such a high end card?   Go get a used 3060-3070. $250 eBay.,pcmasterrace,2026-01-22 21:18:12,1
Intel,o14h803,9070 XT v 5070 Ti  9070 v 5070  Those are the GPUs to compare,pcmasterrace,2026-01-22 21:40:27,1
Intel,o14i7ne,I got some bad news for you. You arent getting 300fps in MHW lol.  Pay for VRAM not the name...  Edit: Ahhh missed the edition vs actual game... reguardless. Buy the VRAM not the name. And consider whatever is your weakest link behind the GPU. No sense in buying a 5090 if you loooove your 60hz 720p monitor,pcmasterrace,2026-01-22 21:45:15,1
Intel,o14juue,12GB VRAM is the minimum for 1440P. Get a GPU with 16GB to be safe,pcmasterrace,2026-01-22 21:53:11,1
Intel,o14ms76,"If you're doing Windows, Nvidia, and if you're doing Linux, AMD.",pcmasterrace,2026-01-22 22:07:41,1
Intel,o14nb09,"5070 all day, or 5070 Ti for the extra few bucks. No driver headaches, no issues, just works, and works much better.",pcmasterrace,2026-01-22 22:10:19,1
Intel,o14qals,I'll pick the 9070xt over the 50xx every day of the week.  I would much rather have the battle tested 8 pin power connectors any day over a 10%-25% FPS gain (that I literally cannot perceive).,pcmasterrace,2026-01-22 22:25:23,1
Intel,o14svf6,"I bought the 9070xt a month ago. Got a good price ($560). Sold my 3070 for $250 so net cost was pretty low. Benchmarks are full 2x improvement pretty much. I have no complaints although make sure you safe mode ddu. First time I didnâ€™t do safe mode and stuff was crashing constantly.   I was researching constantly and it had the best ROI at the time.   If you want to do any AI at any point, or a bunch of encoding, maybe go Nvidia. Latter maybe a point of argument but it always outperforms AMD for me",pcmasterrace,2026-01-22 22:40:19,1
Intel,o14ubfm,9070xt i think the drivers been good lately,pcmasterrace,2026-01-22 22:48:33,1
Intel,o14uen8,"So happy to see my 3090 is still competitive (kind of). Raw power is better with the 9070xt, but software features makes the 5070 an easy choice.",pcmasterrace,2026-01-22 22:49:02,1
Intel,o14vgxy,9070xt all day. Imo. I had the same debate almost a year ago and ended up getting a 7900xtx for $700. I still think i made the right decision.,pcmasterrace,2026-01-22 22:54:39,1
Intel,o14yr1x,I would not buy a 12GB Gpu in 2026.     So its either 5070 Ti or 9070 XT,pcmasterrace,2026-01-22 23:11:41,1
Intel,o152zc8,I would go with the 9070 non XT if it saves you more money since the XT isnt worth it,pcmasterrace,2026-01-22 23:33:34,1
Intel,o15d5c9,"If all you're playing is CS2 at 1080p, either card is probably overkill.  You'd be fine with a 9060 XT (even 8GB), and it would be a whole lot cheaper.  Or if you want to stick to Nvidia, a 5060 or 5060 Ti.  Unless you're playing at elite levels of competition, the difference between 300fps and 400fps is pretty academic.",pcmasterrace,2026-01-23 00:26:57,1
Intel,o15nfxs,i vote 16GB vram. but seems like you are updating often so maybe donâ€™t  have to worry about longevity.  but counterpoint if you want good drivers i think nvidia is better supporter of drivers for their cards.,pcmasterrace,2026-01-23 01:23:02,1
Intel,o15p9g8,With the recent update in Nvidias DLSS i would get the 5070ti for msrp. if you still can.   Or its the 9070xt for sure.,pcmasterrace,2026-01-23 01:33:26,1
Intel,o15pc5q,Never be dumb enough to buy amd,pcmasterrace,2026-01-23 01:33:52,1
Intel,o15w3hg,Glad I got my 4080s for retail before the 50 cards came out,pcmasterrace,2026-01-23 02:11:55,1
Intel,o15yjw9,If you donâ€™t have a 9800X3D it wonâ€™t even matter which gpu you get,pcmasterrace,2026-01-23 02:25:27,1
Intel,o161gmt,Depends on how hard you are running your pc.. my 3080 hasn't had any issues for me..,pcmasterrace,2026-01-23 02:41:28,1
Intel,o166lcc,nvidia for dlss,pcmasterrace,2026-01-23 03:09:42,1
Intel,o166srp,dont forget about the nvidia rtx 4090 64gb titan card ai slop edition,pcmasterrace,2026-01-23 03:10:54,1
Intel,o168t4s,If it's 5070 ti then 100% worth it. 5070 over 9070xt is only worth it if you need ray tracing or you work with 3D software like blender or any video editing software...even obs. Nvidia is much better in performance for those.,pcmasterrace,2026-01-23 03:22:28,1
Intel,o16dsqh,Get the GPU that is more optimized for your game.,pcmasterrace,2026-01-23 03:51:44,1
Intel,o16qajf,If you only ever plan on playing counter strike then yes 5070 is what you want if however you thinking about playing any other game the 9070 / 9070xt is worth it depending on pricethe 9070 is slightly faster the 9070xt much more so its just not at counter strike.,pcmasterrace,2026-01-23 05:12:02,1
Intel,o16r981,Why does one need 400fps in cs2?? If that's all i played I'd still rock a 1080,pcmasterrace,2026-01-23 05:18:41,1
Intel,o16scwn,"From personal experience, the 5070 is the better esports card. 9070XT is the better AAA card.",pcmasterrace,2026-01-23 05:26:32,1
Intel,o16tl99,5070 TI,pcmasterrace,2026-01-23 05:35:28,1
Intel,o16v1wk,Either get 5070ti or 9070xt just look at games you wanna play and compare fos,pcmasterrace,2026-01-23 05:46:15,1
Intel,o170ksl,"What PSU are you using?  The 9070xt draws 400w at peak so if you are getting a 12hpwr connector card, make sure to get the voltage protector board by TG or Aqua.   My research took me from the 5070 to the XT to the 5070ti.  1. Ram on the first jump 2. Power on the second jump. The AMD does draw 50 to 60% more, putting the connector into a higher risk category for burn out. I was also limited by the PSU and didn't want to undervolt.  If you get a 12hpvwr over voltage protection board, and you have the PSU bandwidth. The 9070xt is an easy choice. As others have said, Linux.....and SteamOS.  Good luck!",pcmasterrace,2026-01-23 06:29:21,1
Intel,o172c2w,"I was deciding between these two, then got a 5070Ti for almost the same price on some luck, and haven't regretted anything. I thought I was gonna switch to a ""red"" GPU, but this is my 3rd Nvidia GPU and haven't been let down even once yet (knock on wood)",pcmasterrace,2026-01-23 06:43:47,1
Intel,o172gu6,"Get whatever is cheaper, which probably would be 9070. What problems are you expecting playing only cs2? Besides, AMD driver issues is now thing of the past - thing of the present is both AMD and NVidia driver issues. And AMD and Intel CPU microcode issues. And SSDs randomly dying because of shitty OS update.  Everything's shit, and also expensive. So, pick whatever is cheaper, you'd have some issues either way. Or not. But vendors do not matter much.",pcmasterrace,2026-01-23 06:44:51,1
Intel,o179ada,"CS2's optimization isn't particularly friendly to AMD graphics cards. I recommend sticking with NVIDIA cards instead.  But, both of these cards offer more than enough performance. It ultimately comes down to whether you have productivity needs. If you require model training and CUDA acceleration, then the 5070 is undoubtedly the better choice.",pcmasterrace,2026-01-23 07:43:19,1
Intel,o17cylv,"Honestly, if the 5070 plays the games you like better then get a 5070  but if it's pretty close get the 9070xt as it will be overall better in most games",pcmasterrace,2026-01-23 08:16:18,1
Intel,o17og7z,Who told you that 9070XT would have more driver issues? I have come across far more from my customers using nvidia cards now with severe issues,pcmasterrace,2026-01-23 10:03:14,1
Intel,o17wilb,Go with a 9070xt. I have mine since they release and no issue. For the driver part the issue was with older generation,pcmasterrace,2026-01-23 11:13:53,1
Intel,o188sex,DLSS > FSR,pcmasterrace,2026-01-23 12:43:49,1
Intel,o18vyk2,"Unless itâ€™s the Ti, get the 9070xt. 5070 ti is superior though",pcmasterrace,2026-01-23 14:50:05,1
Intel,o1d4ciq,"For that price/performance range, just get a 9070xt. It's way better than 5070 except with maybe ray tracing. If you can find a 5070ti at or near MSRP, though, get that.",pcmasterrace,2026-01-24 03:26:40,1
Intel,o1es4kn,9070xt. Apples to apples is 9070 vs 5070 where the 9070 is still the better gpu.,pcmasterrace,2026-01-24 11:33:36,1
Intel,o1ipk0c,"I went 5070 BCS cheaper and dlss 4.5 is a better upscaler with wider game support on launch, whereas fsr 4 support gets added late.",pcmasterrace,2026-01-24 23:28:01,1
Intel,o1ivcae,"If you only play cs2 why get such a good gpu? I mean it'll crush it for sure! Also 9070xt for if you want to play any other games that aren't multiplayer, other wise 5070",pcmasterrace,2026-01-24 23:58:18,1
Intel,o1kbh6d,"I just bought a 9070xt and I was tossing up between it and the 5070ti and I'm so glad I went 9070xt  paired with 9800x3d I get 600+ fps at all times on CS (low/medium graphics)  the 9070xt's closest competitor is the 5070ti which is like 1% better unless you care about ray tracing and all that, but it's also $500 AUD more expensive than the 9070xt  I think the 9070xt is the best price-to-performance card you could buy right now (at least in Aus)",pcmasterrace,2026-01-25 04:51:43,1
Intel,o114eey,"wtf are these benchmarks results? Outperformed by a 2 generation old 6900XT, what a joke.  Get the damn 5070Ti",pcmasterrace,2026-01-22 11:47:14,-1
Intel,o117bvp,"The 5070 performs a lot better with Ray-tracing. It has CUDA technology that makes it a lot easier to dabble in AI generation, if you wanna do that.  9070 XT has more Vram though, and will last longer when you consider running the newest games. Although it's the weaker option for ray tracing.  I have the 9070xt and I'm very happy with the choice,  but setting up Stable Diffusion was a bit of a hassle for me.  So there's pros and cons on both sides.  If one is significantly cheaper, I would go with that. They're both great options.",pcmasterrace,2026-01-22 12:08:41,1
Intel,o11af4r,For cs2 no other gpu than 5090.,pcmasterrace,2026-01-22 12:30:12,1
Intel,o11ikdj,"Dude you play cs2 on 1080p. like seriously , any potato gpu will get you over 200 fps. are you a pro?  get the cheapest GPU.    also, nvidia drivers have been a lot worse than AMD for years now.",pcmasterrace,2026-01-22 13:20:46,1
Intel,o114f41,"i also had ro decide betwen thise 2, went for the 5070because i got doom tda for free with it, others wise i would have gone for the 9070xt, 16gb is pretty nice",pcmasterrace,2026-01-22 11:47:23,1
Intel,o116eu3,"I know this wasn't your question, but have you thought of going with the 9060xt? It offers 16gb vram (there is also the 8gb version but I wouldn't get it for an upgrade) and if you're only playing cs you'll get good performance anyway  Edit: I'm only mentioning it as an option because it's considerably cheaper and I believe it would be enough for your use case. If you have the budget and want to go for one of these anyway for the higher performance, I'd go with the 9070xt",pcmasterrace,2026-01-22 12:02:00,1
Intel,o117b5b,"screw Nvidia get the 9070xt. I've had mine since launch day (3090ti owner before that) and have had almost no issues beyond the first few weeks when games/drivers were stabilizing to the new hardware. Cost to performance ratio is crazy good, I just wish there was a higher vram option since I do a lot of vrchat.",pcmasterrace,2026-01-22 12:08:33,1
Intel,o117j5a,For same price 9070 XT is EASY choice. Just reinstall Windows when you upgrade (even if you pick 5070 for some reason) so you completely delete old drivers.,pcmasterrace,2026-01-22 12:10:06,1
Intel,o11kpei,Multi-player games either one will do. If you play AAA games. Having nvidia MFG can help make games feel smooth with higher frame rates. But 9070xt is no slouch. You be happy with either one.,pcmasterrace,2026-01-22 13:32:46,1
Intel,o11ov9v,Normally Iâ€™d recommend the 9070 xt but for your use case the 5070 is the better buy. Just get the cheapest option of the 2.Â   Also maybe check the performance of a 5060 ti/ 9060 xt and save yourself some money.,pcmasterrace,2026-01-22 13:55:07,1
Intel,o11qs44,For real? the 9070 XT is as of now equal or faster than the 5070 ti...,pcmasterrace,2026-01-22 14:05:08,1
Intel,o11ii3b,"Right?? Everyone arguing that one card is â€˜betterâ€™ than the other but we are genuinely talking about 400fps vs 430fps. Wtf is even the point of the whole discussion, this dude probably has a 165Hz monitor or something along the lines.",pcmasterrace,2026-01-22 13:20:23,95
Intel,o13h0fy,I have a friend who exclusively plays CS on a 1080p 144hz monitor and he just made a completely new build with a 4070 but refuses to get a new monitor lol. So why the fuck spend the money you idiot?!,pcmasterrace,2026-01-22 18:52:40,3
Intel,o13os15,Because 300 FPS isn't enough apparently,pcmasterrace,2026-01-22 19:27:24,4
Intel,o11j52o,is like buying a ferrari to go around your plastic pool    gen z is lost,pcmasterrace,2026-01-22 13:24:00,6
Intel,o11d3b0,5070 is cheaper.,pcmasterrace,2026-01-22 12:47:47,55
Intel,o11b2rx,Even then it would need to be within 100$.  They are identical in performance for most games,pcmasterrace,2026-01-22 12:34:39,2
Intel,o11ofb0,What if you're specifically using it for AI?,pcmasterrace,2026-01-22 13:52:48,1
Intel,o11q3tf,5070 is NVIDIA,pcmasterrace,2026-01-22 14:01:38,1
Intel,o12jrv2,and if he ever decides to go over to Linux - cause we all now microslop is killing windows - amd has 100% better driver support,pcmasterrace,2026-01-22 16:24:24,1
Intel,o13ipav,"9070 xt is only 5% faster in cs2 and it's that's the main (maybe even only) game OP plays, why spend extra?",pcmasterrace,2026-01-22 19:00:03,1
Intel,o13xxra,Dlss is better than fsr   Dynamic frame gen also seems pretty cool,pcmasterrace,2026-01-22 20:09:23,1
Intel,o14sz8d,Took the words right out my mouth. 5070ti are pretty much non existent now so get the 9070xt,pcmasterrace,2026-01-22 22:41:02,1
Intel,o1fi1dt,Raytracing is a point of he likes that.,pcmasterrace,2026-01-24 14:29:38,1
Intel,o14ygom,"Shit, my 6600XT runs it flawlessly.",pcmasterrace,2026-01-22 23:10:11,1
Intel,o1196t2,GT 710 it is,pcmasterrace,2026-01-22 12:21:47,102
Intel,o1146n0,"the drivers were ass once, but that was ages ago and they have been pretty good ever since then",pcmasterrace,2026-01-22 11:45:37,58
Intel,o112ykz,"I had the same problems, I disabled auto updating with the help of youtube video and now I did not across any  problems in 7 months.",pcmasterrace,2026-01-22 11:36:08,5
Intel,o13jq4e,"I think the driver issue comes from people switching from nvidia to AMD. Nvidia drivers behave like malware when you try to uninstall it, leaving  behind files that conflict with hardware from the competition. a lot of people don't know about this, so they don't use DDU (or use it wrong) or flat out nuke the OS.",pcmasterrace,2026-01-22 19:04:37,3
Intel,o119oxw,"Oh god, during the Vega days and then the 5000 series days. I ended up returning my launch 5700xt because the drivers were so broken it was just crash after crash after black screen.   I haven't went back to AMD GPUs since but I assume they have gotten miles better.",pcmasterrace,2026-01-22 12:25:16,4
Intel,o13rck5,"Just bought a 9070xt and handed down my 3090. I was very worried about AMD driver issues, but it's been solid for me. I'm actually impressed by the metrics and monitoring in Adrenaline.  With Nvidia, I was stuck on a very old driver because newer drivers weren't stable for a couple of my primary games. I went AMD instead of 5070TI because it was a $150 USD price difference.",pcmasterrace,2026-01-22 19:39:10,2
Intel,o11wo45,"Same here, and itâ€™s most been great apart from one big ***   Indiana Jones Hidden Circle had several game breaking bugs annoying late in the game.   Other than this, which is very frustrating as I was 30 hours into the game and literally cannot progress past the crash, it is great",pcmasterrace,2026-01-22 14:35:34,1
Intel,o125k6i,Also went AMD this time. Drivers have not been an issue and the software auto detected which driver i needed and installed it right away. I had to do nothing.,pcmasterrace,2026-01-22 15:19:06,1
Intel,o12cli0,Drivers are still sour with older cards but the last few years it's been pretty solid for their newer ones.,pcmasterrace,2026-01-22 15:52:07,1
Intel,o12o28f,"I have also switched to amd after using nvidia since 2013, i did had windows update corrupt/unninstall my video drivers twice, but again, i just did DDU and installed the card, maybe it was exclusively windows fault",pcmasterrace,2026-01-22 16:43:23,1
Intel,o12xlpw,"I upgraded my 980ti to a 7900xtx in 2023. I've had 1 issue since then. It was an issue with DirectX12 in WoW and PuBG. The weird part was Dx12 worked in retail wow but was having issues in classic wow, so I think it was more a Blizzard implementation issue than an AMD issue. As for PuBG, idk. Other than that, my experience with AMD has been good. (I had an HD 6950 and a 9800 Pro back in the day, too, and I didn't really have issues either).",pcmasterrace,2026-01-22 17:26:35,1
Intel,o13gk5o,"i dont know if this i just me but so far ive only had shit experiences with AMD, tons of driver crashes, games dont support fsr (not amds fault but still a gripe)",pcmasterrace,2026-01-22 18:50:39,1
Intel,o15s3sv,also a 9070xt owner. had a few weird stability issues in the beginning but updating fixed them all and irs been smooth sailing after the first month or so. bought it pretty much at launch so some bumps are to be expected. my model is also pretty quiet even on load which is nice.,pcmasterrace,2026-01-23 01:49:36,1
Intel,o177wy4,"Funny I had the exact opposite experience coming from 5 years of a 3070 undervolted  First driver I installed in December crashed all the time , I had to find a stable one , but since then, it has been as it was on Nvidia with Shadow Play / Recording while gaming which was relatively stable but sometimes for some reason just didnt resume working after Hibernation.",pcmasterrace,2026-01-23 07:31:08,1
Intel,o18miv9,"In my recent experience, it is Nvidia drivers that are terrible. Over the last 4 years, I've never had any driver issues with my 5700xt or 7800xt, but my daughter has had driver issues with both her 2070 and 4070.Â  She still can't get open GL to work on the 4070.Â  Everytime I find a way to fix it, it seems to break itself the next day.",pcmasterrace,2026-01-23 14:01:55,1
Intel,o1179fo,Go look through the amd sub itâ€™s not nvidia shills trashing amd drivers.  There are so many posts about driver timeouts and black screens still.  The new thing is to buy an open box 9070xt and realizing why it was an open box.,pcmasterrace,2026-01-22 12:08:12,1
Intel,o11ddnu,He clearly says that he only plays CS2. And the chart clearly shows that the 5070 is better than the 9070XT in that particular game,pcmasterrace,2026-01-22 12:49:38,35
Intel,o11qf1d,70% of gamers upscale.  Why the fuck is everyone in this subreddit pretending  raw raster is only thing that matters,pcmasterrace,2026-01-22 14:03:14,0
Intel,o11cd55,"If anything, Nvidia drivers have been ***A LOT*** worse than AMD the last year or so",pcmasterrace,2026-01-22 12:43:10,24
Intel,o11ih8a,"I've never had a single crash when I was running a 3070. But ever since I installed a 9070 xtx, I've had constant driver time outs and crashes.   I regret going AMD.",pcmasterrace,2026-01-22 13:20:15,5
Intel,o11er85,But he only plays CS2 and the chart shows the 5070 outperforming the 9070xt.,pcmasterrace,2026-01-22 12:58:15,15
Intel,o119slm,"Plus if OP wants to go Linux, it's better with AMD.",pcmasterrace,2026-01-22 12:25:59,14
Intel,o11jyx2,The chart clearly shows the 5070 performing better in cs2 so your comment makes no sense.,pcmasterrace,2026-01-22 13:28:39,2
Intel,o11qlkc,How is 12 GB of vram a downside ?  Unless you play 4k its not an issue anywhere,pcmasterrace,2026-01-22 14:04:11,2
Intel,o115tq7,"Bruh, â€œa 5070 owner who recently upgraded to a 9070 xtâ€ is crazy.",pcmasterrace,2026-01-22 11:57:44,109
Intel,o1137sq,"In my experience, at this point, if a game doesn't support FSR4, it doesn't need it.",pcmasterrace,2026-01-22 11:38:08,20
Intel,o116wt9,\>It has more VRAM  very important in cs2,pcmasterrace,2026-01-22 12:05:40,10
Intel,o11exzv,The difference in VRAM is irrelevant on CS2.,pcmasterrace,2026-01-22 12:59:23,3
Intel,o11gkbc,Why if this person plays one specific game? I just cannot see a reason for them to get the worse performing card because in this particular title it does seem like the 5070 out performs the 9070xt.,pcmasterrace,2026-01-22 13:09:10,3
Intel,o11n33b,Only affects 5080s and 5090s,pcmasterrace,2026-01-22 13:45:39,2
Intel,o115hrq,I have a 7900 xtx but both companies are just different sides of the same shit coin. The CEOs are literally cousins.,pcmasterrace,2026-01-22 11:55:18,28
Intel,o11a2et,"More performance, less money.",pcmasterrace,2026-01-22 12:27:48,1
Intel,o1187nf,I have 280hz,pcmasterrace,2026-01-22 12:14:57,1
Intel,o11ksyr,"Also u get dlss, framegen which is just better in nvidia and better performance in like video editing apps etc",pcmasterrace,2026-01-22 13:33:18,1
Intel,o11td4j,Wait hold up..... I only read the title and didn't know it was for cs2 specifically..... bro its overkill if you're buying a 9070xt or 5070 for cs2 if thats only the one game you play. You can get an rx580 that will do the job or at most a 6600xt since you wont be needing 4k for cs2. 1080p with maximum fps with those cards and anything higher is just a waste of that gpus potential and money unless you play other games at maximum graphics. Plus we need to know what monitor you use too. Do you have a 1080p monitor? A HD 2k monitor? Or a 4k monitor?,pcmasterrace,2026-01-22 14:18:31,1
Intel,o124cpn,Not saying to get this card just saying that anything above it Iâ€™m sure u wonâ€™t bottleneck unless ur playing some 4k game at max graphics,pcmasterrace,2026-01-22 15:13:21,1
Intel,o13hbba,People are telling me gpu doesnt matter but i have a 1050ti. Most people in this thread dont know what they are talking about. 1050ti is bottlnecking it so bad.,pcmasterrace,2026-01-22 18:53:59,1
Intel,o13ixq3,I want a good gpu that will last me years. I am not getting am5 ddr5 ram for 500â‚¬. I am keeping 5800x3d,pcmasterrace,2026-01-22 19:01:05,1
Intel,o14z084,CS2 supports Anti lag 2 on the radeon.,pcmasterrace,2026-01-22 23:13:01,1
Intel,o14yy8r,That graph clearly says CS2 not CS go....,pcmasterrace,2026-01-22 23:12:43,1
Intel,o17ojwf,Customers? Whats your job,pcmasterrace,2026-01-23 10:04:11,1
Intel,o17orp1,Omg ur the dude with the arctic p14 thats so cool to see you under my post. I also bought them cuz of your post. The goat fans,pcmasterrace,2026-01-23 10:06:11,1
Intel,o1kbqf4,"I also have not done any tuning to this card, only thing I did was turn off zero rpm and adjust the fan curve. these cards do like undervolting aswell which I would do if my temps were higher at no cost to performance",pcmasterrace,2026-01-25 04:53:19,1
Intel,o1356h7,"I think this bench is a little bit dubious there shouldn't be such a massive difference between cards in CS2, it's usually extremely CPU bottlenecked even at 1080p. With a 7800XT and 9800X3D i get way more than 300 avg. 750ish on the benchmark map, \~450-550 avg in game depending on the map. ""TPU Custom scene"" i really wonder what that is.",pcmasterrace,2026-01-22 18:00:32,1
Intel,o16i9wm,I was back when the 9070xt was released and cs2 has abysmal performance. They later patched it so now it's good.,pcmasterrace,2026-01-23 04:19:11,1
Intel,o11alqw,What? The 9070 XT will easily beat a 5070 in RT outside of Path Tracing (if you're able to fit it into the 5070s VRAM limits). The 9070 XT is straight up better outside of productivity and some Nvidia QOL features.,pcmasterrace,2026-01-22 12:31:26,7
Intel,o119aqi,Aren't there already scripts that remove these cleanly?,pcmasterrace,2026-01-22 12:22:33,1
Intel,o11rffh,Best part is further down they said their monitor doesn't even have a refresh rate to match that.,pcmasterrace,2026-01-22 14:08:30,34
Intel,o11zwjn,"Even if you have a 500hz monitor CS2 1% lows fucking suck, the game drops 1% lows significantly if utility like smokes or molotovs are used, guns firing huge 1% low dips, random bug that will make your game run at 200fps so you need to restart CS2.",pcmasterrace,2026-01-22 14:51:46,5
Intel,o11npuw,Overzealous buying has been a thing in every generation. The grandpa brought a sports car to go 40km/hr to the shops twice a week.,pcmasterrace,2026-01-22 13:49:02,8
Intel,o147yta,What does this have to do with gen z? Grumpy old geezer lol,pcmasterrace,2026-01-22 20:56:19,2
Intel,o11f033,so is the 9070,pcmasterrace,2026-01-22 12:59:44,136
Intel,o13ceem,Op said they cost the same for him. In that case there really isn't much of a reason to get the 5070.,pcmasterrace,2026-01-22 18:32:20,4
Intel,o11gouw,With 4 gb less of vram.,pcmasterrace,2026-01-22 13:09:55,8
Intel,o125ocd,Isnâ€™t the 5070 only 12gb while the 9070 is 16?,pcmasterrace,2026-01-22 15:19:41,1
Intel,o12j2z2,Thatâ€™s it.,pcmasterrace,2026-01-22 16:21:22,1
Intel,o12vvg0,12gb vs 16gb.  I paid 549 for my 9070xt.,pcmasterrace,2026-01-22 17:18:47,1
Intel,o1398zz,Got my asrock 9070xt for $500 two months ago.,pcmasterrace,2026-01-22 18:18:34,1
Intel,o11e0k5,Yeah. If he cares about cuda or some path tracing I would go 5070ti otherwise 9070xt all the way,pcmasterrace,2026-01-22 12:53:42,6
Intel,o145xjz,Op is isnt,pcmasterrace,2026-01-22 20:46:48,1
Intel,o1ficpg,Raytracing needs more vram there are already games that makes it unplayable on 5070 12gb like indiana  Edit 9070xt wins vs 5070 12gb in RAYTRACING it loses in PATHTRACING       Just to be clear,pcmasterrace,2026-01-24 14:31:22,1
Intel,o1fj4vj,Also from what i see in benchmarks 9070xt does better in rt than 5070 12 gb,pcmasterrace,2026-01-24 14:35:44,1
Intel,o119r88,This guy gets it  ![gif](giphy|GLSwDJn7WvY9SaSRYV),pcmasterrace,2026-01-22 12:25:43,33
Intel,o11bsxz,And then you stuck at driver checking when installing proxmox on your repurpose gaming PC since GT710 is too damn old even for proxmox..,pcmasterrace,2026-01-22 12:39:30,1
Intel,o117bew,"And Nvidia drivers are ass now, but nobody ever talks about that.",pcmasterrace,2026-01-22 12:08:36,46
Intel,o12a5nh,"just to give perspective. I had a 390 from 2016 to 2023. I've never had any issues except knowing if the driver sided AA is working or not (which was a special usecase that didn't matter much). Now I have a 6800xt. 0 issues.  Driver issues are vastly exaggerated online and the myth of amd having bad drivers is almost 15 years old now. They were bad but as you said, ages ago. Today nvidia has way more issues with their ai vibe coding (which amd also will have). Stick to the rule as always: Don't update on release day, wait a month at least unless you really have to.",pcmasterrace,2026-01-22 15:41:01,5
Intel,o11771p,"More importantly, initial drivers for the 5000 series were complete shit. Took months before nvidia improved that.",pcmasterrace,2026-01-22 12:07:44,6
Intel,o130x77,"The peak ""AMD drivers bad"" era was genuinely pushing a decade ago at this point. There were some decently widespread issues with year 1 RDNA2 iirc, I had a day 1 6900XT and had pretty much zero issue personally but I know that's anecdotal. Outside that I feel like AMD driver issues really started tapering off after Polaris launch. I never had any driver issues I couldn't fix with a google search and like 15 minutes effort my entire time with an RX480 8GB and even having to do that was extremely rare.  I'll actually say my HD6950 was annoying to troubleshoot and was a headache at times but also that's a GPU from 2010/2011 and iirc literally part of the first generation of AMD GPUs after the ATI branding got killed. And the entire reason I swapped to it was I also had driver issues with my Fermi architecture Nvidia GPUs so I figured I'd try a Radeon card. Both brands I was having issues with in that era tbh.",pcmasterrace,2026-01-22 17:41:41,1
Intel,o140sqj,The drivers were ass before most of the people in this forum were out of diapers and at that time Omega drivers were the preferred option.,pcmasterrace,2026-01-22 20:22:40,1
Intel,o12y4aq,Yup windows fucks shit up.,pcmasterrace,2026-01-22 17:28:58,2
Intel,o11hr27,Weren't there rumours that the reason some people had really bad issues compared to others was because there were some bad batches of the cards themselves? (It was a big proportion of the launch cards if I remember correctly). Can't make claims on the validity but if it was that many it's possible that and would never admit it because they'd have to replace so many,pcmasterrace,2026-01-22 13:16:04,8
Intel,o11ao39,Weird. I bought a 5700XT at launch and never had those issues. Kept it running all the way until last month when I upgraded to a 9070XT.,pcmasterrace,2026-01-22 12:31:53,2
Intel,o13a6ew,"I dunno about that, searched timeout on the AMD reddit and it returned 1 result in the last 9 years.  I'm sure there are more but you are absolutely misrepresenting the frequency.  The AMD sub actually allows users to post their issues.  On the Nvidia sub, you aren't allowed to outside the one megathread they have for it and even then the mods aggressively delete anything not positive of Nvidia there.    Your open box example is just you trying to craft a story to fit your narrative.  If open box = bad drivers (when it could be dozens of other things), you fail to realize there are in general more open box Nvidia products for sale than AMD (simply through virtual of them selling more cards).  By your nonsense logic, Nvidia's drivers much be bad.",pcmasterrace,2026-01-22 18:22:38,8
Intel,o11ac2v,"Yes, because no one is gonna post that their drivers are working fine and without issues. You're always gonna see complaint posts because it's why people will post there. For advice.  The Nvidia sub doesn't have it because there's a master list of all issues already available and the fixes for them. AMD doesn't have such a master list on their sub. Also, a lot of issues are labelled as driver faults when they're actually ram instability and CPU issues instead. People don't typically test things themselves.",pcmasterrace,2026-01-22 12:29:38,6
Intel,o12chl7,"go to the nvidia sub and they have their own set of problems, go to the ferrari sub and they have problems, go to the banana's sub and they report issues with banana's     all those subs have ppl posting problems.. nobody goes there to say they have 0 problems. you only read complaints on forums.",pcmasterrace,2026-01-22 15:51:38,2
Intel,o142u76,Do you mean amdhelp? Itâ€™s a subreddit for AMD Cards and CPUs having issues. Yes people are having issues on there so they post there. I had a problem with a new gigabyte 5070 where it kept black screening and had no where to go. Remember 572.83? Pepperidge farm remembers. I see melting connector post here everyday that must explain all the returned nvidia cards. See how cherry picking works?,pcmasterrace,2026-01-22 20:32:11,2
Intel,o11dj5b,"1080 > 3070ti > 9070xt here. Never had so many driver crashes before.  Itâ€™s not unusable, and Iâ€™m still relatively happy(would just get a 5070ti if I could go back in time). The card performs well, but I get a crash like once every few weeks.",pcmasterrace,2026-01-22 12:50:37,3
Intel,o1h64vo,Ur getting downvoted cuz ur right lol,pcmasterrace,2026-01-24 19:04:54,1
Intel,o13aou8,"If he only plays CS2, he doesn't need either unless he's pro (and even then it's still questionable).  The game doesn't need anywhere near this level of power.  Mind you, most people will not play one single game for the entire period they expect to keep a GPU.   This is why basing the decision solely on CS2 performance is silly.",pcmasterrace,2026-01-22 18:24:53,4
Intel,o12x5ru,"The day he steps outside his bubble, he will appreciate going with the card that is a tier above. If the 2 are similar performance, but one plays WAY faster in every other game.... why not buy the tier up, if for nothing more than the longevity and resale down the road?",pcmasterrace,2026-01-22 17:24:35,5
Intel,o11r4fa,because fsr4 is good enough and frame gen sucks even tho jensen will give you 20x frame gen next year and tell you it looks amazing,pcmasterrace,2026-01-22 14:06:55,6
Intel,o13bvdf,"First, your 70% figure is entirely made-up.  70% of gamers don't even have cards that can accelerate AI upscaling.  Second, the 9700 XT has higher base performance.  The need to upscale is less.  Third, the 9700 XT has more memory, giving the card more longevity.",pcmasterrace,2026-01-22 18:30:00,2
Intel,o11dklg,"To be completely honest I have 9070xt since week after release.   I've had a 1 situation when Windows decided to update Radeon drivers to a Radeon PRO drivers, I was shocked when I saw blue accents in adrenalin instead of red ones.",pcmasterrace,2026-01-22 12:50:52,9
Intel,o134rs5,"I could be misinformed but I think Nvidia did clean their act up in the final quarter of 2025 basically. But absolutely I think some people just glossed over that Nvidia had genuinely terrible drivers fairly consistently for the bulk of 2025. With some of the highlights being the early Blackwell drivers being unstable so the pro strat was using pre-launch drivers from iirc December 2024 that didn't support multi frame gen and maybe DLSS4 in general.  That one specific driver that would set custom fan curves to 0RPM at all temps and cooked multiple GPUs to the point of needing to be RMA'd. Which while it did get hotfixed like a full day later, that was A. still long enough we saw multiple reports of broken cards and B. they weirdly didn't force the update or heavily promote on social media or anywhere to tell people to do the update. So if you were out of loop which some people were, we still saw a few more cooked GPUs drip feed in days after it was fixed.  And the Doom the Dark Ages ""Game Ready Drivers"" that literally in the patch notes first block of text ""Documented crashes when playing Doom the Dark Ages"". The game ready drivers literally weren't game ready.  And just in general stretches of weeks to months of bad drivers consecutively that led to people basically constantly having to check in on which specific driver versions were ""safe"" to update to and when to stay on version for long periods of time before updating. It was at least half of 2025 if not three quarters of it that Nvidia had questionable drivers. Meanwhile AMD's drivers were probably the most stable they'd been in a while for the bulk of 2025, including the launch day drivers for RDNA4 were weirdly just good. They did have the advantage of the delayed RDNA4 launch but day 1 9070 and 9070XT drivers seemed to just kinda work.",pcmasterrace,2026-01-22 17:58:44,2
Intel,o12c5g1,Had the same problem.   to fix the constant driver time-outs I had to disable the igpu.,pcmasterrace,2026-01-22 15:50:07,5
Intel,o11kqdx,Theyâ€™re also all above 300 fps and heâ€™s only playing 1080p. Will that much fps make any real difference considering the monitors at theat resolution? OP can save a good amount of money if they just get a cheaper card like the 7800 XT.,pcmasterrace,2026-01-22 13:32:54,3
Intel,o11690h,"Screams ""i don't do my research"".",pcmasterrace,2026-01-22 12:00:50,52
Intel,o115zac,"If you don't mind tinkering with things, you can usually enable it even if not officially supported. I just started using optiscaler but I recently got a 9060xt. With a 9070 you're probably right that it's not really needed, especially where it's not already supported",pcmasterrace,2026-01-22 11:58:51,5
Intel,o116d2l,Odd take.,pcmasterrace,2026-01-22 12:01:39,5
Intel,o13dk02,"I mean, what does a 5070 have over a 9070 xt in CS2 then?",pcmasterrace,2026-01-22 18:37:23,1
Intel,o1507qf,"I mean, unless theyâ€™re planning on only playing CS2 on their PC and nothing else for the rest of their lives, the 9070xt would probably be a better option overall. For me, the better graphics on a lot of things and stronger performance are more important than an extra 36 fps on a card that already gets almost 400 fps.  Also, I saw that a driver update in July allowed the 9070 XT to beat the 5070 in 1440p, but I donâ€™t know when these specific benchmarks were taken, since Iâ€™m seeing several different ones. Does anyone know where this benchmark chart is from?",pcmasterrace,2026-01-22 23:19:15,1
Intel,o119j8e,"Wtf is this question then? You can go with 300â‚¬ cards and get above 300fps at all times.  If you want to be free in the future, you're forced to go AMD anyway, since NV refuses to patch to Linux.",pcmasterrace,2026-01-22 12:24:12,2
Intel,o11atf0,"Note how even the lowest card on that graph is higher fps than your monitor? Get the cheapest one that does your max refresh.Â    Of course that graph is also 1080p, so if youve got a different resolution you should use that as your basis. But most modern cards will crush CS2. Its not a very hardware intensive game.",pcmasterrace,2026-01-22 12:32:54,2
Intel,o149ov9,"Fair, but I'm not sure that a 9060XT wouldn't last for several years. Personally I'm hoping gpu prices come down a bit in the near future so I can grab a 9060XT myself. I'm also staying on AM4. In either case, GLHF!",pcmasterrace,2026-01-22 21:04:27,1
Intel,o151sx1,Ok cs2. Same shit. OP must have 500hz monitor.,pcmasterrace,2026-01-22 23:27:25,1
Intel,o17ttkg,"yeah my job/business is well, gaming PCs   though I'm liquidating all of the new in box stuff and quitting it all",pcmasterrace,2026-01-23 10:51:12,1
Intel,o11z2t2,"you are mostly right. any frame above your max refresh rate is a waste of electricity.   however, if you are getting 200 fps on a 100hz monitor, thats not exactly a waste. monitor displays the most recently produced frame at any given moment. that means the higher your fps, more responsive the game feels.   can most of the people can tell the difference? probably not. I cant tell myself. Still i lock my fps double the amount of refresh rate. fps dips during gaming. its nice to have some headroom even if you cant tell. at least for competetive fps games.",pcmasterrace,2026-01-22 14:47:43,7
Intel,o189r35,"Cause back in the day when we played TFC/CS we barely made 30 fps    Even for today standards , a hair above your monitor refresh rate is enough    buying a 5070/9070 for 1080p cs is straight up retarded",pcmasterrace,2026-01-23 12:49:55,1
Intel,o11gwya,"There is no reason to buy a 9070 over a 5070 given the minuscule performance difference but big software features quality difference, so it remains the same.  9070XT is a complete performance tier above. But also a whole price tier above.  Itâ€™s all about budget.  I personally think that unfortunately for AMD if loking for a 9070XT might as well get a 5070ti If looking for a 9070 better get a 5070",pcmasterrace,2026-01-22 13:11:13,29
Intel,o14dj93,Still more expensive than the 5070,pcmasterrace,2026-01-22 21:22:42,1
Intel,o17nc6c,Well i didnt reply to OP,pcmasterrace,2026-01-23 09:52:59,1
Intel,o1279r6,Which i dont even need,pcmasterrace,2026-01-22 15:27:23,1
Intel,o127m8p,"Perhaps. I am fine on 8gb, 12gb is an extra 4 from 8, and i am on 1440p. It just dependa on wether your situation requires the extra ram or not.",pcmasterrace,2026-01-22 15:29:02,1
Intel,o17nae1,Dont need the extra 4. Enjoy american prices =) cheapest 9070xt is 695 EURO where i live.,pcmasterrace,2026-01-23 09:52:31,1
Intel,o17nbhu,Enjoy american prices. Cheapest 695 EURO here,pcmasterrace,2026-01-23 09:52:48,1
Intel,o1fj9ro,Thats like me saying all Cards are bad becouse you cant get 100fps in city skyline 2. Take a normal game. I had no trouble with RT with my 4070 in most games i played.,pcmasterrace,2026-01-24 14:36:30,1
Intel,o1flxle,All the benchmarks and people i see says 5070 is superior to the 9070 in ray tracing scenarios but without RT 9070 is clearly better.,pcmasterrace,2026-01-24 14:50:55,1
Intel,o11wtv5,"I'm a simple man. I see our Olympic Hero, I up-vote our Olympic Hero.",pcmasterrace,2026-01-22 14:36:25,2
Intel,o11k4eh,You must be new here,pcmasterrace,2026-01-22 13:29:30,13
Intel,o11usdu,Nvidia drivers have been dogshit for a decade,pcmasterrace,2026-01-22 14:25:48,3
Intel,o155tpt,"Between Nvidia, AMD, and Reddit... its Reddit most likely to do me dirty.   The tribalism is just so intense that so many lie. Or it is user error and they have a habit of pointing the finger when they mess something up.   Idk. My hardware from both Nvidia and AMD runs just fine with updated drivers. If anything, AMD additional bloat software has caused me issues, but I uninstall it and use lossless scaling. I've had just a couple older games that didn't run on my AMD, that did on my Nvidia, but that's rare.   Get them all and see for yourself. Corporate tribalism is kind of sad.",pcmasterrace,2026-01-22 23:48:23,2
Intel,o12btfb,"i had a 5700 for 4 years and didnt realy have any issues, the drivers worked fine for me the whole time i had the card",pcmasterrace,2026-01-22 15:48:35,2
Intel,o13h3df,"Driver issues are exaggerated because there's an amdhelp sub but not an Nvidiahelp sub, and posts about issues get removed from the nvidia sub...   Of course when there's a dedicated public help sub you see more issues...",pcmasterrace,2026-01-22 18:53:01,1
Intel,o11ka2e,That could be true. I think I remember seeing something similar.,pcmasterrace,2026-01-22 13:30:23,5
Intel,o11bqo7,You got lucky then my friend.   Driver issues were crazy. One google search even today confirms that I wasnâ€™t just hallucinating it.,pcmasterrace,2026-01-22 12:39:05,3
Intel,o1h5s6w,"Hey man, I don't want to call you a liar but I think you're either misinformed or just straight up wrong.   I don't have a dog in this game but I just went to the radeon Sub and I typed in ""timeout"" and I got a massive amount of search results back in the last week alone. Maybe it's different if you go to the AMD sub, but if you go to the Radeon sub it's very different.   Again driver issues can happen to anyone. I'm not going to claim otherwise but to say that there isn't no driver issues being reported by AMD people is kind of insane. I go to both subs. I don't really care about whatever team green or red or whatever. It's all bullshit. But the people that complain about drivers are always the people with the cards, not the opposite side as you claimed",pcmasterrace,2026-01-24 19:03:21,1
Intel,o1h61ny,It's also because it's on the radeon sub.,pcmasterrace,2026-01-24 19:04:31,1
Intel,o11v8zb,9080xt wow great gpu,pcmasterrace,2026-01-22 14:28:10,6
Intel,o11w7em,"Fsr4 support is still so thin on the ground though. They'll be up to a couple hundred titles by now (I hope, last confirmed number I saw was less than 100) and that is a fraction of the games that support dlss4 through driver override.",pcmasterrace,2026-01-22 14:33:09,6
Intel,o11x02i,Fsr4 is supported in 10x less games than NVIDIA.  DLSS is supported day 1 in basically any game.  2x frame gen is all you need for basically anything with a 5070,pcmasterrace,2026-01-22 14:37:20,5
Intel,o12jvy8,And give you 8gb ram plus upscaling to called it even,pcmasterrace,2026-01-22 16:24:54,1
Intel,o12zg7q,"Frame gen is not that bad, I was against it previously but after trying it - I don't hate it and my use case is worst case scenario because I'm playing around 40-70 fps, lmfao",pcmasterrace,2026-01-22 17:35:01,1
Intel,o136hv5,Yup. That's the only issue with AMD cards. You have to tinker with windows settings to turn off auto updates or else it fucks your drivers. That's more of a Windows issue than AMD.,pcmasterrace,2026-01-22 18:06:25,1
Intel,o117ksn,"Screams ""I only listen to redditors"".  I swear you could inverse reddit advice for PC building and come out on top 99% of the time.",pcmasterrace,2026-01-22 12:10:26,32
Intel,o1186d6,"I agree, but sometimes you read reviews and see benchmarks of a GPU, think it will meet your usecase and then it doesn't. Then you got that annoying niggle in the back of your mind, that you messed up, and now are you going to live with it, or take a small hit and get what you actually should have.  To give one example, would be people claiming card can play 1440P/4K, when they can probably play those res at Mid setting to an acceptable FPS, or only on older games. And example would be all the people claiming a B580/9060XT 16GB does well at 1440P, when they honestly don't, unless you are playing some older games/less intensives games (Or want to play 1440p with a mix of low-high setting and get a barely stable 60 FPS on more intensives games with upscaler).  or the people who loved to claim a 4070TI was a 4K card. New builder may also get confused with benchmarkers using a 7800X3D/9800X3D to test a budget and mid tier card, not understanding the CPU is helping it's performance and a mid tier CPU will produce different results. Games also get patches which cause worse performance. You could end up watching an old benchmark, from maybe even 3-9 months ago, and get a false idea of what the GPU can do in the game you plan to mostly play. E.G: Tekken 8 released and the min GPU to play FSR 2 Ultra performance low/mid was a 1050TI, after a few patches (\~9 months) it was a 1660TI, nowadays 18+ months on, it can barely run on a 2060S at min setting. This was across the board, a 4070 was producing worse results at older settings and you had to crank down the res and/or setting. ETC ETC. There's a lot of reason why people end up slightly upgrading after they bought, even if they have done the research.  A lot of people on this sub, also don't help, people have mad copium and love to inflate the performance of their GPUs. It still isn't uncommong to hear, Oh yeah my 3600/3060 crushes \*insert new modern intensive game\* (e.g: MHW), when they really do not. Not a diss to those will older hardware, but stuff like that circulating only helps to confuse people.",pcmasterrace,2026-01-22 12:14:43,3
Intel,o13ebz1,Slightly better performance per the chart above + cheaper.,pcmasterrace,2026-01-22 18:40:47,2
Intel,o16uwr6,"It seems like thatâ€™s the case.  â€œ I only play CS2.â€ doesnâ€™t really leave much room for interpretation.  Like I would certainly get the 9070 XT over the 5070 in almost every situation but if this is the specific scenario, yeah, I would get the 5070.",pcmasterrace,2026-01-23 05:45:11,1
Intel,o12eu8a,"usually if im dropping $$ on a card, and have a crap monitor, the next upgrade would be the monitor anyway...  since monitors aren't having ram shortage problems getting the card asap makes sense.   for CS2 though, that's questionable, lol.",pcmasterrace,2026-01-22 16:02:12,3
Intel,o145rnp,But 430 makes no difference compared to 400 on a 165hz monitor,pcmasterrace,2026-01-22 20:46:02,3
Intel,o11pgah,"Except for the obvious the price.  OP has stated that in his area the price of the 9070 XT is the same as the 5070, should be a no brainer.  In most regions the 5070 TI is often 30% more expensive.  Unfortunately for AMD, no matter how much value they can pump out, it seems you'll always have some one saying get Nvidia.",pcmasterrace,2026-01-22 13:58:10,117
Intel,o11n0gi,5070 ti is like $300 more in my region compared to a 9070xt. It's a no brainer for me to go with the 9070xt in that sense.,pcmasterrace,2026-01-22 13:45:15,31
Intel,o11u39g,Well as of today in my country the 9070xt is cheaper than 5070. So......,pcmasterrace,2026-01-22 14:22:15,5
Intel,o11icj1,I mean its main advantage vs 5070ti is that its a decent bit cheaper for very minimal performance diff,pcmasterrace,2026-01-22 13:19:30,16
Intel,o11iwyv,"""big software features quality difference"" is a complete bullshit spewed by nvidia fanboys.    Lets translate that = DLSS looks better than FSR. THE END    which btw the dude plays cs2 in 1080p and will never, ever need that. In a realistic scenario a 5070 or 9070 will also do 1440p ultra with no need for upscaling.",pcmasterrace,2026-01-22 13:22:45,15
Intel,o11lj3r,"The vram will be more and more of a problem with time, if the bubble pops.",pcmasterrace,2026-01-22 13:37:17,3
Intel,o11mbx7,"The cheapest 5070Ti is 32% more expensive than the cheapest 9070XT, for a minor performance increase. I'd say that you might as well get a 9070XT if you are looking for a 5070Ti",pcmasterrace,2026-01-22 13:41:35,4
Intel,o12oet9,There is no big software quality difference though.,pcmasterrace,2026-01-22 16:44:57,1
Intel,o12pk48,"It depends to be honest. In general I would agree with you, but I was also able to find my 9070xt for 200 bucks less than the cheapest 5070ti",pcmasterrace,2026-01-22 16:50:05,1
Intel,o12shi9,"The 9070 and 5070 are essentially the same in performance tho, and why I prefer the AMD card is not having to deal with the 12vHpwr and having 4 gb more vram, also mutch better Linux support.   It's a hard pick tbh, but I value this more than the Cuda cores, ai performance and other Nvidia things, of which I only can miss dlss, since having both main competitor's software available is a win, but I trade core stuff. It's a tough choice",pcmasterrace,2026-01-22 17:03:19,1
Intel,o134rjb,5070 ti is absolutely not the same price tier as a 9070xt. Itâ€™s like a $200-300 difference.,pcmasterrace,2026-01-22 17:58:43,1
Intel,o13b84a,9070 does have more Vram. Pretty good reason imo,pcmasterrace,2026-01-22 18:27:12,1
Intel,o15ez2e,I am so happy with my 5070. The dlss features are absolutely top notch.,pcmasterrace,2026-01-23 00:36:33,1
Intel,o17o95u,5070ti AND 5060ti is also disappearing from the shelves. It has completely disappeared from my supply source . 9070xt is still plenty available,pcmasterrace,2026-01-23 10:01:25,1
Intel,o11lgoh,"He might want to get a 4k monitor eventually, though.",pcmasterrace,2026-01-22 13:36:55,3
Intel,o12auur,Those extra gb makes the card more future proof,pcmasterrace,2026-01-22 15:44:12,1
Intel,o1fjywj,Did i say anything about rt in your 4070? Its not like 8gb isnt enough to run rt i just said some of the games show that 12gb might not be enought to run rt,pcmasterrace,2026-01-24 14:40:23,1
Intel,o1fovyg,Well maybe someone is lying to you or i m stupid but man i dont know just a randome benchmark i found on youtube tells diffrent story https://youtu.be/_PbwQQyhjYM?si=XR2J_VNN1IZZz0w0  In worst scenario here 9070xt gets same fps as 5070 12gb and in others 9070xt wins,pcmasterrace,2026-01-24 15:06:23,1
Intel,o11x8ru,"Yup, but I still see people using the ""but drivers"" argument in favor of Nvidia.",pcmasterrace,2026-01-22 14:38:34,7
Intel,o17vnpo,"I've had both. I don't give one single shit about any company, and I don't like any of them more than the rest. I usually buy AMD because they have better value for the money. That's it.",pcmasterrace,2026-01-23 11:06:40,1
Intel,o16e9t5,"Just to add some balance: Had to toss my 5700xt last year because I constantly got critical display driver errors. All monitors shutting down and then the PC rebooted itself, for weeks, with X different drivers I tested. DDU, clean Windows install, tested the card in my server, tested another GPU in my main PC. Googled for hours, tried so much, then I took the L and went Team Green. Plus I have two friends with 6000s AMD cards who constantly have/had issues, so personally I just canâ€˜t tell friends anymore to buy AMD GPUs.",pcmasterrace,2026-01-23 03:54:38,1
Intel,o11dkxu,Must have I guess. Best card I ever owned by a long shot.,pcmasterrace,2026-01-22 12:50:56,3
Intel,o1hkqnt,">Again driver issues can happen to anyone. I'm not going to claim otherwise but to say that there isn't no driver issues being reported by AMD people is kind of insane.  No one is saying that.  What was said is that the other user was vastly over-representing the issue.  In principle, you appear to entirely agree with me, driver issues happen regardless of brand.  >I don't have a dog in this game but I just went to the radeon Sub and I typed in ""timeout"" and I got a massive amount of search results back in the last week alone.  Using the exact same criteria as you, with the search term set to ""timeout"" and the time constrained to 1 week the number of results in total is 10:  [https://imgur.com/a/e8u6lXk](https://imgur.com/a/e8u6lXk)  Looking into each of those instances, one of them is a user who moved from Nvidia to AMD because their Nvidia card was giving them timeout issues that going AMD actually resolved, 2 are users experiencing driver timeouts, and 2 are game specific driver timeouts (which is a game issue).  1 of those was a double post from user Fit-Masterpiece-9930 (also a new account that's only commented on that issue, which makes it's potentially suspect).  1 is a laptop GPU whose driver support is on the laptop vendor and not AMD.  Plus, again it's a new account that only has a single comment.  1 is not even a driver timeout issue but the very same tazman137 from this thread making an anti-AMD comment again.  Coincidence?  I think not.  His post history is full of anti-AMD stuff (even their CPUs, which are widely regarded as the best).  This is just a reminder that search results are not equal to legimate claims.  You have to verify the voracity of the posts and their posters.  As always, consider the source.  With the above in mind, where exactly is this ""massive"" amount?  It seems to be that cursory investigation proves my point quite clearly.  Basically your comment boils down to you going to a different subreddit and getting different results from the one I searched.  You didn't once consider that ""hey he's not a liar, we just searched two different places"" and you didn't actually look into those individual reddit posts.  You say you don't have a dog in this game or don't want to call me a liar but then proceed to disprove yourself that very same sentence twice.  You do call me a liar and you clearly do have a dog in this game (otherwise you wouldn't be posting) so spare me the false courtesy.",pcmasterrace,2026-01-24 20:10:17,1
Intel,o13b30g,timetraveler,pcmasterrace,2026-01-22 18:26:35,1
Intel,o11x72x,"Every new games supports dlss out of the gate, and their backlog is so much bigger.   But i guess you gotta be a drone fanboy and just repeat FAKE FRAMES, PRICE TO PERFORMANCE , RAW POWER",pcmasterrace,2026-01-22 14:38:20,3
Intel,o132rjw,It had 100 back in September  its around 400 now,pcmasterrace,2026-01-22 17:49:54,1
Intel,o11at43,All the redditors who said wait for the 50 Supers xd,pcmasterrace,2026-01-22 12:32:50,12
Intel,o11a7ct,">annoying niggle in the back of your mind  Sure, the 9070XT is the better card but dropping $700+ for 10% more performance is just dumb no matter how you look at it. Itâ€™s just spending for the sake of spending.",pcmasterrace,2026-01-22 12:28:45,7
Intel,o153wzm,"Really? Where I live, theyâ€™re roughly the same price (or they were, until the 9070xt microcenter deal expired). Honestly, Iâ€™d love to know why the 5070 beat the 9070 XT here, because the 9070 XT averages around 10% more powerful than the 5070 in the benchmarks Iâ€™m seeing, (though it does shift up and down a lot, so Iâ€™m truly curious why).",pcmasterrace,2026-01-22 23:38:25,1
Intel,o1700le,"I guess a better way of putting it is that I donâ€™t think OP should be basing such an expensive decision on the single game theyâ€™re currently playing, especially when the 9070 XT outperforms the 5070 in so many other non-shooter games. Ultimately, itâ€™s up to them, but I would recommend that they consider whether or not theyâ€™ll ever want to play any other games.",pcmasterrace,2026-01-23 06:24:48,1
Intel,o174phk,"Kinda, tho to be pedantic, if you lock the framerate to 165, one will use a smidge less power.",pcmasterrace,2026-01-23 07:03:18,1
Intel,o12bugw,"I mean Iâ€™m pretty pro-AMD but if you can get the Nvidia equivalent and have better software support and features and hardware for not much more itâ€™s a no brainer. I had the choice of 9060XT, 5070, and RX 9070 and I got the 5070 cause I donâ€™t wanna question if a game will support certain features. I donâ€™t wanna question if I need to use optiscaler for upscaling, or question if theyâ€™ll treat my card like the $1000 7900XTX and make it obsolete after a year or two. All those things I have to question buying into AMD, but not Nvidia and for most enthusiasts itâ€™s worth the premium.   Hell, I had people saying to buy a 9060XT with worse RT, Software, less raster by like 35% over a 5070 for nothing but 4GB VRAM extra. Thatâ€™s a lot of compromise IMHO",pcmasterrace,2026-01-22 15:48:43,5
Intel,o12rka1,But the features!!!,pcmasterrace,2026-01-22 16:59:05,1
Intel,o14bbe2,I have exactly the same price difference and got a Sapphire pulse one. No regrets.,pcmasterrace,2026-01-22 21:12:14,2
Intel,o11u71e,At that point obviously 9070XT,pcmasterrace,2026-01-22 14:22:47,6
Intel,o11lugm,DLSS 4 getting csgo to 999 fps bro,pcmasterrace,2026-01-22 13:38:58,3
Intel,o11k0pl,"Longer software support as well, Nvidia wonâ€™t drop support after 2 generations given their track record",pcmasterrace,2026-01-22 13:28:55,6
Intel,o11mid3,"-DLSS SR looks better than FSR.  -DLSS frame gen works better than FSR frame gen.  -Reflex is better than AMDâ€™s antilag (and much more widely available)  -DLSS has support in MUCH more games than FSR, Specially DlSS 4 vs FSR 4.  -Path tracing is feasible in RTX GPUs that are 4070ti and above when using DlSS upscaling + frame gen while on AMD Path tracing is not really feasible even using upscaling and frame gen, the performance hit of path tracing is still to much for AMD even on a 9070XT.  -Ray reconstruction straight up doesnâ€™t has an alternative on AMD, they donâ€™t have something like it out  yet.  -power consumption efficiency on Nvidia is better.  -Day one driver support for new games is higher in Nvidia.  -Nvidia is still better for streaming.  Thereâ€™s white a few things.  No fanboyism here, just a ton of facts that you ignored",pcmasterrace,2026-01-22 13:42:33,6
Intel,o11r5mg,"Lol no, no it's not. The stability of the drivers + large advantage in ray tracing and AI capability + longterm support are all huge advantages for Nvidia currently. I own both a 9070 xt and a 5070 ti, theyre both htrat cards, but if I could only have one for the next 5 years it would be the RTX card easily.",pcmasterrace,2026-01-22 14:07:05,-1
Intel,o11n0p7,"Thatâ€™s why the fact that Nvidia announced RTX neural compression wich has been shown to reduce VRAM usage UP TO 90% (Wich even if it ends up being 40%-50% is. Massive) coming soon 1 year ago, while AMD still relies on high VRAM wich we know they wonâ€™t be able to support with the new ram prices, will make Nvidia age much better.",pcmasterrace,2026-01-22 13:45:17,1
Intel,o14uoop,"Depends on the market. Here where I'm at, 5070 is 40% cheaper than a 9070xt. And the 5070ti is 10% cheaper. It all depends where you're located. I hate these ""what should I buy"" threads on reddit where the location is obfuscated.",pcmasterrace,2026-01-22 22:50:31,1
Intel,o14b8kv,"While I agree itâ€™s a valid point in favor of the 9070, honestly at their performance tier, the number of games where, at the resolution this GPUs are aimed for wich is 1440P and if triple A, upscaled 1440P. 16 is an advantage over 12 in very very small amount of games.  And seeing the RAM situation I donâ€™t expect vram demands to increase at a normal pace during some years, Iâ€™m expecting stagnantion since, even if AMD says so, I highly doubt theyâ€™ll be able to mantain prices.  Maybe for half a year but eventually theyâ€™ll have to buy more Vram and prices are through the roof. High vram GPUs will be very expensive for some years now.  Whether nvidia or AMD both will be very expensive. So I wouldnâ€™t be surprised if both corporations focused on low VRAM GPUs.  Wich means stagnantion.  What do you think?",pcmasterrace,2026-01-22 21:11:52,1
Intel,o12ix9v,Never been an issue,pcmasterrace,2026-01-22 16:20:40,1
Intel,o1fl8s4,What are you on about ? You make no sense at all and were did you get 8GB from none of the cards we talk about has that. 4070 and 5070 has both enough to play RT in most games atm thats what im saying and the 9070xt does not and that would be a point to go with nvidia over the amd.  Why did you get defensive over that and started talking crazyðŸ¤£,pcmasterrace,2026-01-24 14:47:16,1
Intel,o1hlco4,"I think you clearly have an agenda since you're taking it so personally. You said ""i see one result on 9 years"". I check i see 9+ recently lol  But alright have fun with whatever agenda ur repping",pcmasterrace,2026-01-24 20:13:10,1
Intel,o12c1f8,All the ones that said wait for supersâ€¦ but now theyâ€™ll be ultra overpriced thanks to ai.  Canâ€™t make shit up,pcmasterrace,2026-01-22 15:49:36,1
Intel,o11duwl,The difference isn't $700 though. I have a friend who constantly buys and sells shit on marketplace. You're assuming he bought them both new and threw the old in the trash.,pcmasterrace,2026-01-22 12:52:42,5
Intel,o11dccl,"I'll be honest mate, I agree with you. I personally only think an upgrade is worth it when you get a 20%+ performance increase. Like I've myself have battled with getting a 7800X3D when they go on sale (best sale I saw was \~$250 about 6ish months ago, was so hard not to pull the trigger), since the performance uplift from a 7600X, in CPU intensive games I play (strat games) isn't enough. Sadly I have an asrock board, pre-9XXXX release, and yeah with their issues with a 9800X3D, it isn't an option for me...  But you can sell a brand new GPU, esp a 5070, for a minor lose (ESP right now). And get a 9070XT. It isn't dropping $700+. It's dropping the difference of what you got used for your old GPU and the price of the new one. That could be $0-100, to get the GPU you wish you had got from the start. Which isn't too bad to many. AMD card also sell for cheaper used, it isn't that hard to sell a used Nvidia card and get an AMD card from the same gen, used, for almost the same price. Not long ago, I sold a 4070TI and got a Used 7900XTX and had some money to spare/made a profit. I wanted to play 4K and the 4070TI wasn't cutting it. I also got an amazing deal on a 2 week old 7900XTX, seller wanted better RT performance for Cyberpunk, and got a 9070XT.",pcmasterrace,2026-01-22 12:49:24,1
Intel,o170k9g,"Sure and I totally agree but they seem quite clear about just playing this game and I think something to highlight would be that they can save a bit of money by getting the 5070. Itâ€™s also going to be viable for other games of course it will, more than that they seem to be playing at 1080p for this particular game so I would suspect that they probably have a 1080p monitor and at that resolution you really donâ€™t need a 9070 XT. I donâ€™t know if CS2 is going to get updated with this, but Nvidia is also doing their reflex 2 framewarp which no one seems to know when itâ€™s coming out, but if you are a competitive gamer, itâ€™s probably a big deal. Honestly, Iâ€™m fascinated by the whole argument because it seems ridiculous to me to buy any of these modern midrange cards in order to play one game, but if thatâ€™s what OP wants to do, I guess it is.   They seemingly donâ€™t show any interest in other games if they did the 9070 XT is obvious the better choice but thatâ€™s still overkill for a 1080p monitor where I do think that a 5070 is more reasonably specced with 12 gigs of VRAM going very far at that resolution and the performance being respectable. You should likely be able to play everything at like ultra settings and have a good experience on a 1080p monitor.",pcmasterrace,2026-01-23 06:29:14,1
Intel,o12daa5,"I don't disagree that Nvidia has better features, but we've reached a point where AMD is catching up, which over a year ago wasn't even close.  FSR 3 was below DLSS 2, while FSR4 is somewhere between DLSS 3 and 4.  You get AMD if in your region you want a midrange card that's the best bang for buck possible, if it's priced correctly.  I disagree with the 7900 XTX being obsolete, you bought it before FSR 4 was a thing, in my opinion it still performs exactly the same. Perhaps some expected it to get a better upscaler, but even this is speculation.  Both Nvidia and AMD have been known to release new features only for their new cards. However AMD open source more, if it wasn't for them we'd still be paying for modules in monitors for G-Sync.",pcmasterrace,2026-01-22 15:55:14,5
Intel,o138f7l,I had a bunch of issues driver wise and compatibility wise with my 3070ti but no issues with my 6800xt.   I actually prefer the software and puts out for modifying the settings for each game.   I also donâ€™t like ray tracing or frame generation on either platform though. Iâ€™d rather have the 100+fps,pcmasterrace,2026-01-22 18:14:58,2
Intel,o1h4q63,Nvidia has better features but a **9070 xt** at same price as 5070 is no brainer. Its legit a tier up.,pcmasterrace,2026-01-24 18:58:44,1
Intel,o11ucvy,"And yet AMD has better price to performance ratio in almost all games. While I agree nvidias AI features are better, the difference is not as big as it used to be. FSR4 is vastly superior to FSR3. Antilag (2) is also a very good option to Reflex, I've actually noticed a bigger difference than I did with reflex, but it does not work properly in every game, so some tradeoff there.   FSR does have less support in new/old games, but any major new release always has FSR in my experience.   Day one driver support being better on Nvidia isn't really relevant when they still have a lot of issues with games not fully utilizing the nvidia cards, and this has a big problem this last or two year for nvidia. You simply don't need day one driver support on AMD for most games. There's been numerous games where the 50XX series has had worse performance than the 40XX series in new games even with day 1 drivers.   So while I agree somewhat that Nvidia is higher quality in general, it's a lot more nuanced than you make it seem",pcmasterrace,2026-01-22 14:23:36,5
Intel,o11nuis,"Few yeara Ago I brought my first nvidia gpu (3080) because of promises of great tech. Only nvidia tech Iâ€™ve used is nvnec, others are really gimmicks- frame gen increases input lag, DLSS I have never needed as I get decent fps without it, I tried it with space marine 2 but didnâ€™t like the result. Second time I wanted to try dlss was with EFT but that game just has bad implementation of it. Also atleast on Linux amd drivers seems more stable, Iâ€™ve had issues after kernel update with nvidia.",pcmasterrace,2026-01-22 13:49:44,10
Intel,o123znb,"He plays cs2 , only thing thats worth mentioning there is reflex , which is considered superior over antilag , but its not really a dealbreaker .  Op should focus on an X3D chip and a good monitor , + decent gpu , personaly i own a 9070xt and r5 7600 , and im happy with it , also CS2 only basicly , going with anything higher gpu wise is not worth it , cpu is what matters a lot in cs , expecialy X3D chips .",pcmasterrace,2026-01-22 15:11:37,4
Intel,o11sxzn,"It's literally pure fanboysim, 95% is DLSS/RT which either isn't used by the niche in case of DLSS or shouldn't be used at all (RT) due to complete FPS crippling",pcmasterrace,2026-01-22 14:16:20,8
Intel,o11pfny,And you don't even mention NVENC,pcmasterrace,2026-01-22 13:58:04,5
Intel,o12dzlc,Now repeat after me:  NATIVE GAMING IS SUPERIOR TO AI GENERATED SLOP,pcmasterrace,2026-01-22 15:58:22,4
Intel,o11tzoe,"Don't forget broadcast, super useful and I am not even a streamer, just use it for my work computer",pcmasterrace,2026-01-22 14:21:45,1
Intel,o13xbqb,This is a good list. Thank you for making me feel better about my 5070.,pcmasterrace,2026-01-22 20:06:33,1
Intel,o1637mh,"There's also driver support to think about. Nvidia supports their cards for WAY longer than AMD does. AMD sometimes seems to randomly remove driver updates for certain cards.  I know AMD has an appealing price point and feature set on paper, but in real world testing I'd say Nvidia is the winner. Even if it looks like you'll get a bit worse performance, the security of knowing your card will be supported for years and years is worth a lot to me.",pcmasterrace,2026-01-23 02:50:52,1
Intel,o11sa2c,"your fanboysim goes great lenghts son, good job here a pat on the back by the company that couldn't give two shits about you",pcmasterrace,2026-01-22 14:12:55,0
Intel,o11nx6k,"Do you believe every single tech you see will be mainstream?  As a person with a masters on computer engineering, It will be incredibly hard for this to make it to mainstream.  And even if it does, they would become such a ridiculous monopoly they would give it to amd as well (which is something they already do)  At that point, because this tech exists, we will cram so much stuff into the games that the vram usage will go up again, I guarantee it.",pcmasterrace,2026-01-22 13:50:08,4
Intel,o14j4qm,"Well just saying that there is a reason, not no reason at all like you said to choose 9070 over 5070.",pcmasterrace,2026-01-22 21:49:42,1
Intel,o1fnt11,I m trying to explain to you that some of new games need more vram to run rt   8gb was here for scale mostly ok for 1080p  12gb is fine for 1440p but when you ad rt its needs a lot more i doubt new games will be using less and less vram  Also wdym by 9070xt is not enough for rt? Amd rt hardware got a lot better with 9000 series and 9070xt gets you more fps than 5070 12gb in rt just go and check youtube benchmarks,pcmasterrace,2026-01-24 15:00:45,1
Intel,o1hlyvr,"If the points I've made are wrong, then prove it with words.  Not logical fallacies and ad hominem distractions.",pcmasterrace,2026-01-24 20:16:05,1
Intel,o11kdfc,"A brand new 5070 goes for about $600, so Iâ€™m the best case youâ€™ll get $500 on the used market if you get lucky with Facebook marketplace goblins. So, $200 and a piece of your sanity for marginal uplift is still kind of a shit deal even before you consider the value of your time.",pcmasterrace,2026-01-22 13:30:54,2
Intel,o173hxq,"They said that the price is the same in the last sentence of the post, by the way. Thatâ€™s why Iâ€™m arguing for the 9070 xt. Being overspecced isnâ€™t a hindrance to gameplay, but being underspecced in the future will absolutely be.",pcmasterrace,2026-01-23 06:53:14,1
Intel,o16zor2,"Just in regards to fsr vs dlss, imo its less about the quality of the upscalers and more about adoption rate. The amount of games that natively support fsr vs dlss is like 10 to 1 in my experience. It does seem to be getting better, but my point is the fact of fsr becoming much better is moot when adoption rate is so low.",pcmasterrace,2026-01-23 06:22:11,1
Intel,o13v7rx,"So the only game he plays is cs2? Is that really a thing? I get having a favorite game you sink most of your time into but yall are talking like he will never boot another title, period",pcmasterrace,2026-01-22 19:56:47,1
Intel,o11t6l7,Literally every game i have installed right now has RT and I notice a lot how much worse the visual fidelity is without it.  Donâ€™t extend what you care about to wha others use/need,pcmasterrace,2026-01-22 14:17:34,0
Intel,o11ttpk,Not relevant as twitch added av1 support nvidia no longer has encode lead.,pcmasterrace,2026-01-22 14:20:54,4
Intel,o11qpp3,also no longer relevant,pcmasterrace,2026-01-22 14:04:47,2
Intel,o11wa4h,"Lmao friend, I have an Arc B580, a 9070 xt, and a 5070 ti, and the next GPU I buy will be whatever Intel cooks up for Xe3. Your fanboy nonsense has no power here, I'm just a pc building enthusiast who doesn't care about your tribal GPU wars. Also, if you think AMD cares about you anymore than Nvidia does, you're a fool lol.",pcmasterrace,2026-01-22 14:33:33,4
Intel,o11swi6,"What do you mean â€œbelieve it will become mainstreamâ€  Im not talking about some university study publishing their findings in some cool tech feature that could change stuff etcâ€¦ and opening a fund raiser to start working on it.  Iâ€™m talking about Nvidiaâ€™s GeForce department publishing their demo of it in 2025 CES and announcing it as coming soon to RTX GPUs in fact it was supposed to be launched for the 5xxx series but seems itâ€™s still not ready.  If anything they have more reason now than ever to invest on it, since with current ram prices, keeping high VRAM GPUs at prices that have any chance to sell in the consumer market seems impossible.  A feature that makes 8GB GPUs still feasible in 2027 is something they and every likely to focus on, since ram prices wonâ€™t go down any time soon",pcmasterrace,2026-01-22 14:16:08,2
Intel,o14c8tt,Beverly a product for future promises. Thatâ€™s a big no no,pcmasterrace,2026-01-22 21:16:36,1
Intel,o1hmd35,"The difference in time was 3 minutes based on timestamps. Does it take more than 3 minutes to read that?  Alright. Again clearly you want to rep some agenda, I've clearly walked into it  my bad. Have fun.",pcmasterrace,2026-01-24 20:17:56,1
Intel,o1218w1,Why are you assuming he can't buy a 9070xt used on marketplace?,pcmasterrace,2026-01-22 14:58:16,1
Intel,o171k0t,"From what I can tell most new games support it, but titles that have been released already often do not.  That said, I'm someone who uses upscaling when a game is too demanding. I fully believe Native is the way to go, but I do see the value for upscaling, especially when framerate goes below an acceptable rate.  What I mean is, I expect a 4080s/5070 ti/9070 XT to run most titles already out there at max settings without upscalers. 4k might have more outliers, and there is of course path tracing, but in general I don't think it's a big issue.",pcmasterrace,2026-01-23 06:37:21,1
Intel,o12pz1m,Fucking finally lol,pcmasterrace,2026-01-22 16:51:56,4
Intel,o11tr1m,"Mainstream in this context means it will be usable in games not designed to work with it.  Also an early version has been released with support for other gpus, you can read more about it here:  https://github.com/NVIDIA-RTX/RTXNTC",pcmasterrace,2026-01-22 14:20:31,2
Intel,o12carr,"I would assume supply constraints. Being that theyâ€™re, a year old at most, not many will exist on the used market.",pcmasterrace,2026-01-22 15:50:47,2
Intel,o12pahp,"quick look on ebay shows there's a fair amount, at least in the uk, but they're either used and super close to the price of a new one or never used and the same price as a new one. either way it's not worth it and better to just get a new one from a proper retailer so you'd get warranty",pcmasterrace,2026-01-22 16:48:53,1
Intel,o0te6dr,Personally I'd take the 1st one. Performance between the 8700f and 14400f is pretty much the same for games and I'd rather have a cut down 7700xt than a 5060 as it should pull comfortably ahead in most titles and the extra vram is cool. It's in a better position to upgrade once prices stop being fucky as well,pcmasterrace,2026-01-21 07:13:05,3
Intel,o0te07g,the B580 oneo,pcmasterrace,2026-01-21 07:11:33,1
Intel,o0tqb5e,"Usually if the 1st attribute in the list is **windows** , it's a really bad omen to me. 1st it should never be in priority spot, 2nd I suspect they charge extra 50-100$ included in the price for the clean windows (they used 5$ OEM key) and I could purchase the PC laptop way cheaper without the windows,",pcmasterrace,2026-01-21 09:06:46,1
Intel,o0u3hvt,as far as i know all newly purchased laptops come with windows out of the box,pcmasterrace,2026-01-21 11:08:48,1
Intel,o0rawu6,What next gen path tracing does that game use? ðŸ˜…   And why is it in Lego game? ðŸ˜…,pcmasterrace,2026-01-20 23:17:33,338
Intel,o0rex5g,">Next year you prolly gonna need a 4080 to play a remaster or snake or some like that  But look at it this way: soon, your GPU will no longer appear in the system requirements!",pcmasterrace,2026-01-20 23:39:26,219
Intel,o0rbige,"At first I too wondered why a Lego Batman game of all things would have such insane system requirements. But apparently, it's UE5, so there's the answer to that.",pcmasterrace,2026-01-20 23:20:47,159
Intel,o0reta1,\*laughs in EVGA 1080\*,pcmasterrace,2026-01-20 23:38:50,60
Intel,o0rh9dg,Ive got a 6800xt. Cutting it close...,pcmasterrace,2026-01-20 23:52:17,19
Intel,o0rf8qi,![gif](giphy|GJVpbMjfT2Ftm),pcmasterrace,2026-01-20 23:41:13,10
Intel,o0rdtij,Nah i've got the super i'm still good,pcmasterrace,2026-01-20 23:33:22,15
Intel,o0tbbnq,We are truly cooked once the 4090 starts appearing in the system requirements,pcmasterrace,2026-01-21 06:48:09,8
Intel,o0rfwiu,Me who's gpu isn't as powerful as the one in the minimum requirements,pcmasterrace,2026-01-20 23:44:51,11
Intel,o0rg4q6,I was surprised to see my 7700XT in the recommended requirements for Indiana Jones last year.,pcmasterrace,2026-01-20 23:46:06,6
Intel,o0rhtnu,You ought to have seen the uproar about the massive requirements of Oblivion in 2006. Folk screaming from the rooftops how lazy developers stopped optimizing games in the 1990s with PCs being so powerful since.  Nothing changes.,pcmasterrace,2026-01-20 23:55:21,12
Intel,o0rtp4r,I blame Ray tracing. My 1080 TI holds up incredibly well at 1080 P without Ray tracing.,pcmasterrace,2026-01-21 01:00:21,3
Intel,o0tf3tm,well...   https://preview.redd.it/k532p4szlneg1.png?width=331&format=png&auto=webp&s=0e1474a83e9ae4316d7c6e3b59c8915cb04f98b1,pcmasterrace,2026-01-21 07:21:28,4
Intel,o0sy5tv,UE5 SLOP,pcmasterrace,2026-01-21 05:04:39,3
Intel,o0sr8uw,Me whose just glad my CPU is still technically above minimum requirements:  ![gif](giphy|s9Y0czwWdTtB7U6d5I),pcmasterrace,2026-01-21 04:17:29,2
Intel,o0syecu,The 3080 isnâ€™t even that old especially with the memory shortage.,pcmasterrace,2026-01-21 05:06:19,2
Intel,o0t6the,"me, a 1060 owner, reading this post   ![gif](giphy|11rxTjOcUlCScw)",pcmasterrace,2026-01-21 06:10:22,2
Intel,o0rjx6y,"So because the game has requirements like this it means it isn't optimised, you know you can't use the same stuff forever",pcmasterrace,2026-01-21 00:06:43,3
Intel,o0s8mjt,I never pay attention to system requirements.  And at this point those might have been written up by AI lol.,pcmasterrace,2026-01-21 02:25:33,2
Intel,o0spxd8,"Just repasted my cpu and rebuilt my whole PC for maintenance and cleaning the other day  Ryzen 2600x Sapphire nitro+ 5700xt SE  Running borderlands 3 at 1440 ~100 fps  I seriously don't understand what's up with these crazy ""requirements"" nowadays",pcmasterrace,2026-01-21 04:08:52,2
Intel,o0revc2,Hopefully my 5600X can hold out for awhile longer. Wish I could find a 5700X3D though.,pcmasterrace,2026-01-20 23:39:09,1
Intel,o0rg7r1,Bro my old 1070 appeared so far back lmao,pcmasterrace,2026-01-20 23:46:34,1
Intel,o0rhz0b,"It happens to us all, just at different times.  These are some times tho.",pcmasterrace,2026-01-20 23:56:08,1
Intel,o0rjqeg,My arc a770 16gb is minimum?!? I dont have $1500 for a new card dammit.,pcmasterrace,2026-01-21 00:05:42,1
Intel,o0rk3p6,looks like the only way to play this game is gonna be on consoles..,pcmasterrace,2026-01-21 00:07:43,1
Intel,o0rluht,You know system requirement lists aren't exhaustive and are often formed from the hardware the developer had on hand to test and are guidelines.,pcmasterrace,2026-01-21 00:17:11,1
Intel,o0rm8e8,"all this for a lego game, i hope it looks better than GTA 6 then",pcmasterrace,2026-01-21 00:19:17,1
Intel,o0rn5y1,What the actual fuck,pcmasterrace,2026-01-21 00:24:18,1
Intel,o0rnznk,"That's the recommended tho. Not the minimum. People did this with JWE3 as well, freaking out over the recommended specs and not looking at min at all.",pcmasterrace,2026-01-21 00:28:48,1
Intel,o0ropa4,Iâ€™m finally upgrading from a 2060. Thing was a workhorse. I was able to keep 60-70 fps in most games at 1440 with some custom graphics lowered on almost all modern games.,pcmasterrace,2026-01-21 00:32:39,1
Intel,o0rpnjx,They have to render each brick for it. I all seriousness they made Lego batman on the PSP what are they doing to increase the spec requirements that much,pcmasterrace,2026-01-21 00:37:49,1
Intel,o0rqrsj,I saw 9600 under CPU and thought *what* game needs THAT as a minimum?? then realized it's a *K*,pcmasterrace,2026-01-21 00:43:55,1
Intel,o0rrlet,GeForce now is an optionâ€¦,pcmasterrace,2026-01-21 00:48:28,1
Intel,o0rsn0h,"You'll be fine, just lower a couple settings and use framegen to round the edges.",pcmasterrace,2026-01-21 00:54:22,1
Intel,o0rv95j,Every game company is unintentionally or intentionally helping hardware companies with their unoptimized game... You want to play stronghold remastered on your 2070ti? Too bad buddy think again when you have a 4070ti! Basically just a monopoly over our wallets,pcmasterrace,2026-01-21 01:09:14,1
Intel,o0rvovr,"Kept seeing 1660ti, knew it was my time.",pcmasterrace,2026-01-21 01:11:46,1
Intel,o0ryi0t,my 2070 is still holding strong. In it for the long run,pcmasterrace,2026-01-21 01:27:47,1
Intel,o0s0slr,At least your GPU is the recommended system requirements instead of the minimum requirements so it could run somewhat ok for you. *cries in GTX 1080*,pcmasterrace,2026-01-21 01:41:01,1
Intel,o0s1nve,"I'm one gen behind those, but I mostly just play Xenotilt and Demons Tilt, and other pinball games (and few shmups).  Edit- maybe not a full gen. Rocking a 5500 Radeon Xt 8gb oc. Pretty happy with it for what I play though.",pcmasterrace,2026-01-21 01:46:01,1
Intel,o0s4f1s,This was just a placeholder btw confirmed by the devs via discord,pcmasterrace,2026-01-21 02:01:41,1
Intel,o0s7d1e,7700x and 32 GB of DDR5 ðŸ’”,pcmasterrace,2026-01-21 02:18:27,1
Intel,o0sjhn0,I wonder how long will my 4080 super appear on the list,pcmasterrace,2026-01-21 03:28:52,1
Intel,o0slvlb,are you gonna play gta 6 RT or what,pcmasterrace,2026-01-21 03:43:24,1
Intel,o0sn6yn,I read somewhere that a developer or rep said specs are more of a placeholder for steam but most games come in hot at launch so who knows,pcmasterrace,2026-01-21 03:51:28,1
Intel,o0sogbx,Vote with your wallet,pcmasterrace,2026-01-21 03:59:22,1
Intel,o0stale,I played for years with systems that weren't even in the minimum requirements section.,pcmasterrace,2026-01-21 04:31:14,1
Intel,o0sxm4b,"Why are you looking at recommended specs and use the word ""need"" to upgrade? Games are shit but 3080 is still insanely good today",pcmasterrace,2026-01-21 05:00:48,1
Intel,o0sxoql,"I know I'm always cooked, because my GPU is a laptop 1050.",pcmasterrace,2026-01-21 05:01:18,1
Intel,o0sz2qp,Companies are going to have to scale those requirements down or they're going to be losing sales.,pcmasterrace,2026-01-21 05:11:12,1
Intel,o0t2pb1,"People who play Lego Whatever, you exist! I've found you! My life is complete!",pcmasterrace,2026-01-21 05:37:52,1
Intel,o0t2pwe,"I just bought an RX 9060XT for future proofing after Doom the dark ages required more than my RTX 2060, at this pace my new GPU will appear in system requirements by next year.",pcmasterrace,2026-01-21 05:37:59,1
Intel,o0t47ba,People say ram is crazy expensive but used ddr4 ram is still cheap af. You can get 32gb under 100â‚¬.   People always just want the newest and hottest shit.,pcmasterrace,2026-01-21 05:49:22,1
Intel,o0t55oc,Why optimize when you can just vibe-code?,pcmasterrace,2026-01-21 05:56:49,1
Intel,o0t5s77,"I am hoping that one possible positive (?) from the RAM shortage is that developers will release games like this, less people buy due to requirements, forcing developers to better optimise their games.  Itâ€™s unlikely but fingers are crossedâ€¦â€¦",pcmasterrace,2026-01-21 06:01:53,1
Intel,o0t6wiw,*stares at my 3090*,pcmasterrace,2026-01-21 06:11:03,1
Intel,o0t7wsb,PSX games worked with 2MB of RAM. PS2 has 32MB of RAM.  Game devs have gotten too lazy.,pcmasterrace,2026-01-21 06:19:20,1
Intel,o0t8res,You can usually run games at quite a bit under minimum requirements. It's not something i'd worry about.,pcmasterrace,2026-01-21 06:26:20,1
Intel,o0tddmv,"I will do you one better.  In the future, games will require that your GPU supports frame generation.",pcmasterrace,2026-01-21 07:06:00,1
Intel,o0tekfq,This requirement is most likely with raytracing turned on. Turning it on results in these high requirements for even the most basic looking games.,pcmasterrace,2026-01-21 07:16:34,1
Intel,o0tew73,canâ€˜t even blame people with not enough spare money switching to console at this point.,pcmasterrace,2026-01-21 07:19:31,1
Intel,o0tf8nd,Not really. My old GPU wasn't even stated in minimum system requirements. Still I could play new games for like another 7 years.,pcmasterrace,2026-01-21 07:22:42,1
Intel,o0tk8gn,"a 2070 is the requirement.. 3080 is recommended. it seems pretty reasonable to me, given the most recent releases.",pcmasterrace,2026-01-21 08:08:49,1
Intel,o0toznx,Y'all remember when lego games were actually optimized?,pcmasterrace,2026-01-21 08:54:03,1
Intel,o0tqfs0,Games need to chill out.,pcmasterrace,2026-01-21 09:08:02,1
Intel,o0ttlk8,"I replaced my 5700XT recently, glad I did. Let's hope the replacement lasts.",pcmasterrace,2026-01-21 09:38:53,1
Intel,o0tynh0,Imagine not even making minimum requirement list,pcmasterrace,2026-01-21 10:26:04,1
Intel,o0ugx1w,"Im not saying this is good, just stating the potential reasons. The game uses UE5, which is already heavy out the gate, and those minimum specs line up with the PS5 and XSX.   So, given that the XSS is a thing lower than minimum, it will be viable for 30 fps at 1080p. I'd imagine.   Either the consoles are 1080p@60, and they're just using that as PC min spec, or they just chucked the equivalents as min spec even if they aren't the minimum to run it.",pcmasterrace,2026-01-21 12:48:22,1
Intel,o0uo6pv,Nothing is as bad as the AI sys requirements for indiana jones or whatever the fuck game that was. ðŸ¤£,pcmasterrace,2026-01-21 13:32:18,1
Intel,o0upc4s,"The game dev was definitely like ""this runs on my system buttery smooth, it will work everywhere else fine""  Meanwhile the Dev's pc is a workstation with four 4090 stack with liquid cooling",pcmasterrace,2026-01-21 13:38:47,1
Intel,o0uzx0z,I remember when my GPU fell below the system requirements..... (GTX 770). Felt bad....,pcmasterrace,2026-01-21 14:34:54,1
Intel,o0v0jt9,My PC doesn't even meet recommended specs... FOR A LEGO GAME,pcmasterrace,2026-01-21 14:38:08,1
Intel,o0vbtwh,"Even since 2020, am looking for a title that is worth playing, let alone upgrading....",pcmasterrace,2026-01-21 15:32:32,1
Intel,o0ves7l,Are you *complaining that your GPU is the recommended GPU* for this game?,pcmasterrace,2026-01-21 15:46:07,1
Intel,o0vlh2q,Wow why does it need a 2070? Would a 1080 run it?,pcmasterrace,2026-01-21 16:16:03,1
Intel,o0vpp1j,Crypto Miner with some Lego gameplayâ„¢,pcmasterrace,2026-01-21 16:35:05,1
Intel,o0w03m6,"I know the A770 isnt exactly a beast, but seeing a 16G VRAM card in the minimum specs is fucking wild.",pcmasterrace,2026-01-21 17:21:51,1
Intel,o0wqvbs,"I never see my specs recommended now im worried that I bought bad parts =( why wont any game recommend my system specs?  /s  Fr though, the recommended specs never make sense. They never give a target or configuration for the minimum or recommended, so it ends up being meaningless. I always thought recommended specs should be controlled like nutritional facts on food are.  For example:  Cyberpunk 2077  Minimum (720p, 30fps, lowest settings, Fsr 3.1 performance) GTX 1030 Xxx Xxxxx Xxxx  Recommended (4k, 240fps, maximum settings, DLSS 4.5 performance, FG 2x, pathtracing enabled) RTX 5080 Xxx Xxx Xxxx",pcmasterrace,2026-01-21 19:19:36,1
Intel,o0x2804,"I mean itâ€™s game dependent.  Iâ€™ve seen my 4090 as recommendedâ€¦ But it was path traced Indiana Jones, so fair enough.  If I saw my 4090 being necessary for a Borderlands game THEN I would be worriedâ€¦ ohâ€¦ waitâ€¦.",pcmasterrace,2026-01-21 20:11:15,1
Intel,o0x8dkj,Bro am still on the i5 12400f,pcmasterrace,2026-01-21 20:39:15,1
Intel,o0ylbvz,"Yeah, my 3080Ti is starting to creep into recommended hardware.",pcmasterrace,2026-01-22 00:42:05,1
Intel,o0yom39,Lego builders journey or something like that with RT on looks really freaking good for a kids game.,pcmasterrace,2026-01-22 01:00:09,1
Intel,o1027hp,better than mine lmao,pcmasterrace,2026-01-22 06:09:16,1
Intel,o10z1d4,9600K + 2070S + 16GB DDR4 super was my old PCâ€¦,pcmasterrace,2026-01-22 11:03:48,1
Intel,o12j7gc,I'll be worried when my GPU starts showing up in the requirements :D,pcmasterrace,2026-01-22 16:21:54,1
Intel,o16x2qc,Glad I finally upgraded my CPU after seeing 9600K in that ðŸ˜¬,pcmasterrace,2026-01-23 06:01:43,1
Intel,o1c9h31,2070 as a minimum is ridiculous,pcmasterrace,2026-01-24 00:27:49,1
Intel,o1jlogs,Ya...that happened about 2 weeks ago with my 590px &6700xt....sigh,pcmasterrace,2026-01-25 02:20:04,1
Intel,o0tpb26,shit take,pcmasterrace,2026-01-21 08:57:04,1
Intel,o0rds1l,"It's a nearly 8-year-old GPU. The last Lego Batman game (2014) also had 8-year-old GPUs as its minimum specs (The NVIDIA 7600GS and ATI Radeon 1950, both released in 2006).   The recommended specs are slightly more generous than the last Lego Batman game: recommended was 4 to 5-year-old GPU's (GTX 480 and Radeon 5850), while the 3080 is nearly 6 years old.   You being uncomfortable with the passage of time has nothing to do with optimization. It's been more than a decade since the last game. Graphics improve. Not just the actual effects, but also the amount of stuff that can be rendered at once in a map.",pcmasterrace,2026-01-20 23:33:09,-17
Intel,o0sil8m,another unoptimized slop,pcmasterrace,2026-01-21 03:23:26,0
Intel,o0sckv9,"I have the explanation.  You know how everyone was begging for a return of AA games, right? Everyone was fed up of only AAAs with inflated budgets and indies? People wanted mid-budget ""AA"" games more often?   That's where Unreal Engine 5 comes in. The Monkey's Paw curls a finger.   It allows teams to make games much more quickly, and gives lower budget games the opportunity to have a much greater scope both in terms of graphics and content (e.g. Clair Obscur Expedition 33). Also just because it's Lego doesn't mean it's not going to look gorgeous. If this has Lumen, this is going to look better than Arkham Knight in terms of lighting.   But UE5 is notoriously tougher to run. That's why they don't want to underbelly how intensive the game is. Odds are, it should run fine on a lower end card. But they don't want to be complained at if it doesn't.",pcmasterrace,2026-01-21 02:48:05,-1
Intel,o0rq838,NOOOOOOOO,pcmasterrace,2026-01-21 00:40:55,0
Intel,o0s7o0t,Minimum 3080 wow is it crysis,pcmasterrace,2026-01-21 02:20:10,0
Intel,o0rf9qa,The 3080 is 6 years old and the equivalent of a 5060 Ti which is a current gen mid range GPU. For such a card to appear in the recommended requirements is very reasonable if you ask me,pcmasterrace,2026-01-20 23:41:22,-18
Intel,o0rmh3i,With requirements like that I better feel like I'm in Legoland watching a moving diorama,pcmasterrace,2026-01-21 00:20:35,81
Intel,o0rj1b7,Cant remember if it was in the Skywalker Saga Lego game but that shit looked amazing for it being Lego.,pcmasterrace,2026-01-21 00:01:55,25
Intel,o0t4iud,cant be rt or pt judging by how they put 3080 and 6800xt which are no where near comparable in that.  the game is just that fucking horridly made it seems...,pcmasterrace,2026-01-21 05:51:49,6
Intel,o0s0abf,But it will be *under* the system requirementsâ€¦,pcmasterrace,2026-01-21 01:38:07,12
Intel,o0rj3wk,His won't but then mine will :(,pcmasterrace,2026-01-21 00:02:18,23
Intel,o0xpo4u,And then it'll hit minimum sys req and that's the end of it.,pcmasterrace,2026-01-21 21:58:01,1
Intel,o0rjhil,UE5 does not like my 9070 I can tell you that much,pcmasterrace,2026-01-21 00:04:22,19
Intel,o0rhme6,I guess the question is if the recommended spec is for 4k ultra at 120fps. Then that makes more sense.,pcmasterrace,2026-01-20 23:54:15,21
Intel,o0sbe5b,Yeah Unoptimized Engine 5 is the worst.,pcmasterrace,2026-01-21 02:41:22,8
Intel,o0tkcia,UE5 will bring a 5080 to its knees if it does on of those close ups of a face. Â What the fuck is up with that engine,pcmasterrace,2026-01-21 08:09:53,2
Intel,o0t1rh5,Why does a Lego game need ue5?,pcmasterrace,2026-01-21 05:30:51,0
Intel,o0rukrc,"As long as RT isn't required you're golden...even so, the GOAT lives on.",pcmasterrace,2026-01-21 01:05:24,11
Intel,o0rn6p6,I still have a 1060. Basically playing PowerPoint slides at this point,pcmasterrace,2026-01-21 00:24:25,8
Intel,o0s90t8,Apparently Iâ€™m out. Still rocking the regular RX5700 (and Ryzen 3600),pcmasterrace,2026-01-21 02:27:47,7
Intel,o0rr3ds,"6700XT here, my heart sank when I saw the 700XT part",pcmasterrace,2026-01-21 00:45:41,3
Intel,o0t7mtt,Yeah I got the same card. It really isn't cutting it for the latest single player games I've noticed. Such a shame. It would've gotten more longevity if they back ported FSR4,pcmasterrace,2026-01-21 06:17:02,1
Intel,o0ruof0,I9 7920x still rocks though.,pcmasterrace,2026-01-21 01:05:58,2
Intel,o0z58oh,*Cries in 1060*,pcmasterrace,2026-01-22 02:34:25,1
Intel,o0rprxb,It's time brother. I went from a 2060 to a used 3080 to a 5070 in the past 5 or so months. Massive difference.,pcmasterrace,2026-01-21 00:38:29,1
Intel,o0rvor4,"Indiana Jones was two years ago  We're all gettin old. But honestly Indiana Jones looks fuckin incredible and it uses RT for everything so it makes sense that the reqs would be kinda high. New DOOM game is also like that, and it runs crazy smooth.",pcmasterrace,2026-01-21 01:11:45,1
Intel,o0rvycg,Games stop being optimized when they can't run on the PC I built 5 years ago.,pcmasterrace,2026-01-21 01:13:16,13
Intel,o0rn8s1,"Or when a graphics card started to appear in required specs.   Or when EverQuest released an expansion with a graphics upgrade, that recommended 256 RAM. Which found a lovely Windows bug, where versions older than XP could get lost in the RAM and freeze. Forcing the user base to need to upgrade windows",pcmasterrace,2026-01-21 00:24:43,8
Intel,o0rojr0,"To be fair Oblivion had a bunch of seriously stupid things going on, like massive texture files for random little prop rocks and the like. That sort of thing is very sloppy and it's not the kind of optimisation that costs anything to do. Bethesda's reputation isn't unearned.",pcmasterrace,2026-01-21 00:31:49,6
Intel,o0rq2kd,For a bit of context on this. I had a 2 year old ATI X800 XT that couldn't run Oblivion properly. Todays equivalent would be a 4090 being only able to run a game on potato settings.,pcmasterrace,2026-01-21 00:40:05,4
Intel,o0s3u6g,"Yeah, pretty sure most games from 2006 would struggle with a GPU from 2000. The 3080 is over 5 years old now. I just replaced my 3080 a few weeks ago with a 9070 XT.",pcmasterrace,2026-01-21 01:58:24,3
Intel,o0s7ems,"I remember that, and bought a 7900gt for $330 to max it out, show me a top tear graphics card for the current equivalent price of $500 that can play the current oblivion remaster at max graphics.",pcmasterrace,2026-01-21 02:18:42,1
Intel,o0sbcxq,This is a lego game tho that doesn't look any better than previous lego games but requires 4 times the hardware power to do so,pcmasterrace,2026-01-21 02:41:10,-1
Intel,o0rq28o,"I found one for 220 a week ago, but went for am5 instead. Should've bought it just to flip ðŸ’€",pcmasterrace,2026-01-21 00:40:02,1
Intel,o0rkqsx,2070 is listed as minimum. 3080 is listed as recommended.,pcmasterrace,2026-01-21 00:11:11,3
Intel,o0t4hby,Bud. I got my gf 64 gb 16x4 for 100 cad.  32 gb is 200 cad rn it's sad  EDIT that 64 gb was last summer,pcmasterrace,2026-01-21 05:51:29,1
Intel,o0rgxfz,8 year old GPUs now are much closer to modern GPUs than 8 year old GPUs were 8 years ago.,pcmasterrace,2026-01-20 23:50:30,14
Intel,o0rf68q,"Yeah no, this is a bad optimisation, a Lego game should run on a toaster. This is UE5 doing what it does best",pcmasterrace,2026-01-20 23:40:50,15
Intel,o0rf7db,"The problem is that how many factors of 2x power increase GPUs launched in that time, and also look at the CPU minimum spec which is really high compared to the old one. The older one could run on really old CPUs. The new one needs something released relatively recently. Not that this is an unfair ask but it's still high as a minimum.",pcmasterrace,2026-01-20 23:41:01,3
Intel,o0rvbtb,"This a terrible comparison in so many ways. Much akin to people comparing gpu/cpu prices in the 00s to today.   Look, I don't expect the gtx 1080 to be running this game at ultra, but I kinda thought stuff like it would at least reach minimum requirements.Â    RTX 3080 recommended is crazy. Even the 5060 ti and rx 9060 XT are slightly slower.",pcmasterrace,2026-01-21 01:09:39,2
Intel,o0szzlf,A nearly 8 year old gpu that is more powerful than the 5060ti 16gb which isnâ€™t even a year oldâ€¦..,pcmasterrace,2026-01-21 05:17:48,1
Intel,o0rmhkx,"It has everything to do with optimization though, since lego games don't do bleeding edge technology graphics, besides it's 6yo high-end GPU. It's still better than 5060ti which is new gen",pcmasterrace,2026-01-21 00:20:39,0
Intel,o0rhzaj,"Itâ€™s closer to the 5070 lol, especially the 12gb version which is only 10% or so slower than the 5070, much faster than a 5060ti.",pcmasterrace,2026-01-20 23:56:10,6
Intel,o0rjv7d,Same with Lego Horizon on the PS5.,pcmasterrace,2026-01-21 00:06:25,10
Intel,o0ve6tg,"Dw, mine will too ðŸ˜†",pcmasterrace,2026-01-21 15:43:26,2
Intel,o0rlybg,Ue5 is so bad on amd cards.,pcmasterrace,2026-01-21 00:17:46,9
Intel,o0t9v1z,Doing fine for me,pcmasterrace,2026-01-21 06:35:38,1
Intel,o0rmqho,"it still doesnt make sense that way in my opinion, its a frigging lego game",pcmasterrace,2026-01-21 00:21:59,16
Intel,o0tg5xi,"> We truly need to call bad craftmanship out more.  People really want 'Indy' or 'AA' games while demanding a degree of technical excellence that even most AAA productions can't afford...  E33 made an excellent compromise to deliver on its aesthetic.  You can't do this on the same budget with a custom or older engine, or at the very least take a huge risk that it just won't work out.  It takes much bigger studios to maintainwell optimised engines with modern feature sets that are suitable for general game development, like id with its id tech engine or Ubisoft's Anvil engine.",pcmasterrace,2026-01-21 07:31:12,5
Intel,o0teih2,"Comparing a 2015 game to a 2025 game is kind of weird man idk.     Clair obscure runs decently on modern hardware, I've seen people with budget rtx4050 laptops running it.",pcmasterrace,2026-01-21 07:16:05,4
Intel,o0vawer,"In which sense does Clair Obscur look like it should scale down nicely? It features all the standard photorealistic graphics and characters you'd expect from a high-end 2025 release, with a large amount of natural environments. I would expect it to scale down as well as, say, MGS Delta.",pcmasterrace,2026-01-21 15:28:13,2
Intel,o0w9k9c,RT isn't required in 99.9%+ of available games. I'll survive without the remainder. Those devs are choosing to make their games exclusive to a small portion of the market.,pcmasterrace,2026-01-21 18:03:32,1
Intel,o0ru2fg,I'm here with you brother... 1070 but it's almost the same at this time,pcmasterrace,2026-01-21 01:02:27,4
Intel,o0slr7z,My 1050 is rocks notepad pretty hard.,pcmasterrace,2026-01-21 03:42:40,3
Intel,o0wfxvt,"I'm on a 6700xt and 3600x, I fear I'm not too far behind",pcmasterrace,2026-01-21 18:31:27,1
Intel,o0tswee,It would have gotten more longevity if the way developers use UE5 wasn't an abomination.,pcmasterrace,2026-01-21 09:32:04,1
Intel,o0tcclb,"saying Indiana Jones release was ""two years ago"" when it's 1 year and a month is diabolical",pcmasterrace,2026-01-21 06:57:01,7
Intel,o0rzmie,Damn. Was it really two years?,pcmasterrace,2026-01-21 01:34:17,2
Intel,o0rwl19,"My kid's 2070 is *outraged*, *outraged, I tell you* but the kid just drops to medium or uses DLSS and he's happy.  Probably throw a 9060XT at him later this year.",pcmasterrace,2026-01-21 01:16:54,3
Intel,o0tgvci,"I remember buying Command and Conquer Generals only to find that I couldn't run it on my still fairly new PC. It often only took around 2-3 years for hardware to start becoming seriously obsolete.  Generals released in 2003 and required a DirectX 8.1-capable GPU with at least 32 MB VRAM. This ment it needed an Radeon 8500 or GeForce 4 from late 2001/early 2002. Radeon 7000 and GeForce 3 GPUs from 2001 were already outdated.  Those GPUs released at $299 MSRP. Adjusted for inflation, that's almost $550 today. So depending on how you want to compare prices, that's roughly on par with requiring an RTX 4060 or RTX 4070 Super for a game released in 2025.  The fact that people now believe that games requiring 7 year old hardware is a sign of declining optimisation is kind of hilarious.",pcmasterrace,2026-01-21 07:37:45,5
Intel,o0rneua,"Oh heck, don't remind me about Evercrack. I was on Windows 2000 at the time with 512 MB, so didn't see that particular issue!",pcmasterrace,2026-01-21 00:25:39,2
Intel,o0rqaj0,I've got the original big box PC version of Everquest pre expansion and the list of recommended GPUs on the back is absolutely wild. Shit you've never heard of. Diamonds and Orchids and Matroxx this that and whatever.,pcmasterrace,2026-01-21 00:41:17,1
Intel,o0rw6ls,"dudes who be yappin about ""games aren't optimized anymore!"" don't know shit about ""unoptimized"" games. GPUs from 3+ years ago are still completely viable today but back then a GPU that's a year old would barely be able to play anything new",pcmasterrace,2026-01-21 01:14:36,5
Intel,o0s4ni4,"I was running a Radeon 9700 when Oblivion dropped, bought it in late 2002. It was the king of the hill, I had it overclocked slightly over 9700 Pro spec.  Oblivion *killed it*. Even on ""Medium"". Even at 1024x768. FPS was below 20 at all times and dropped into single figures when refraction shaders were used.  That'd be like an AAA game today running badly on an RTX 4080 or 4090 from 2022.",pcmasterrace,2026-01-21 02:03:01,3
Intel,o0szufv,Gpu improvements have heavily stagnated. The 3080 is still stronger than the 5060ti 16gb and even trails the 4070 from last gen.,pcmasterrace,2026-01-21 05:16:46,1
Intel,o0t77ea,you couldnt play games maxed out on $500 graphics cards even in 2005 without framerate dips.  as for now. a $500 5060ti 16gb running dlss 4.5 at balanced 1440p looks good and hits 60+fps.,pcmasterrace,2026-01-21 06:13:30,2
Intel,o0ssam5,"It's a LEGO game now set in a larger open world and the visuals absolutely look more detailed than the last Star Wars LEGO game, which was set in smaller linear environments. Open world games just take more to render then closed environments do.   Like, I dunno what else to tell you. The 3080 is almost six years old. It's aging and it's perfectly natural that older GPUs move down into the ""recommended"" spectrum.",pcmasterrace,2026-01-21 04:24:29,-3
Intel,o0rkutf,Thank u i was panicking. Still not great,pcmasterrace,2026-01-21 00:11:48,1
Intel,o0rtkwr,"tbh for real, a lego game shouldn't be demanding this much lol. ue5's wid for this",pcmasterrace,2026-01-21 00:59:42,0
Intel,o0rj5qs,What would you say is closest to a 3080ti?,pcmasterrace,2026-01-21 00:02:36,1
Intel,o0rkz30,The 4070 super is about the same,pcmasterrace,2026-01-21 00:12:26,0
Intel,o0rlsmt,"This isn't something subjective, it's verifiable. According to TPU, the 5060 Ti is only 11% slower while the 5070 is 15% faster.   The requirements talk about the 10gb variant so that's the one I'm considering.   Iceberg Tech also made a video comparing the 3080 12gb and the 5060 Ti and they seemed to be close, so the 10gb variant would be closer.   I'm sure there are more benchmarks you can lookup to verify the performance of the cards and reevaluate your understanding based on the results",pcmasterrace,2026-01-21 00:16:55,-2
Intel,o0rvflj,thought you were talkin about the Forza Horizon 4 expansion for a sec,pcmasterrace,2026-01-21 01:10:16,9
Intel,o0rogr6,Playing Horizon with my son right now and it looks amazing.,pcmasterrace,2026-01-21 00:31:22,2
Intel,o0sqi9r,"Me, personally, have a much better time with ue5 games than I did on the 4070.  Less stuttery",pcmasterrace,2026-01-21 04:12:40,6
Intel,o0txbkv,UE5 is bad on all cards,pcmasterrace,2026-01-21 10:13:50,2
Intel,o0t1i2k,Arena Breakout Infinite is UE5. It runs in 3440x1440 at over 200fps on my 7900XTX. It's buttery smooth so to say.   UE5 runs fine on AMD cards.,pcmasterrace,2026-01-21 05:28:53,3
Intel,o0wpicy,"Seriously. When folks do these comparisons of older games vs. newer games, it's always games that at the time they released were technical achievements from some of the biggest studios pushing the boundaries of what was possible at the time.   That often had not only outsized budgets, but development timelines relative to what a smaller team can afford.  It's like uh, yeah, the game looks good because they put an immense amount of time and labor into bespoke assets and toolkits to achieve a specific aesthetic within the boundaries of that period's hardware. (and oftentimes not even within those boundaries, but pushing beyond them like Arkham Knight)",pcmasterrace,2026-01-21 19:13:30,3
Intel,o0wzqwb,Truth. I had no issues when I had a GTX 1080 last year.,pcmasterrace,2026-01-21 19:59:52,2
Intel,o0wu8za,The fact you can play the majority of modern games on 10 year old hardware seems to say the opposite of the point you're making.,pcmasterrace,2026-01-21 19:34:52,1
Intel,o0t2fe2,Yeah but you need RTX and more CUDA cores to get all the best AI copilot features,pcmasterrace,2026-01-21 05:35:50,1
Intel,o0uin2h,Big part of it as well. Everything is UE5 slop nowadays.,pcmasterrace,2026-01-21 12:59:15,1
Intel,o0sbdzb,I know the feel. I have a xeon w 2145. It's basically the i7 7820x. Stick rocks and better than my previous i7 3770.,pcmasterrace,2026-01-21 02:41:20,1
Intel,o0rzrfz,"Unless it's for medical reasons, 3 years unemployment is wild. Like brotherman, mcdonalds and Walmart are always hiring. A job is a job. Even a demeaning one. I work at a grocery store.",pcmasterrace,2026-01-21 01:35:05,3
Intel,o0s6do3,"Just barely. It released in December 2024, so yes but no.",pcmasterrace,2026-01-21 02:12:56,6
Intel,o0u8cdy,"I played it on a 6800xt 1440p no upscaling and it ran great, so you missed out.",pcmasterrace,2026-01-21 11:47:59,3
Intel,o0uvtei,"RT capable cards have existed for eight years, if your PC rig doesn't have one it might be time for an upgrade ngl",pcmasterrace,2026-01-21 14:13:34,2
Intel,o0t6xep,the 9060xt is pretty similar speed as it lacks dlss and fsr3 looks bad (unless the game supports fsr4). 9070 or 5070 are upgrades though,pcmasterrace,2026-01-21 06:11:16,0
Intel,o0u93qb,"And before Steam and indie games really took off we had a period of publishers dismissing all PC gamers as pirates. That era was full of just *atrocious* ports from consoles, if the publishers even bothered at all.Â    It was pretty much just Valve and Blizzard holding the PC flag for a while, and Bioware was at least good about adapting their UIs for PC (tactical mode in DAO, for example).",pcmasterrace,2026-01-21 11:53:38,2
Intel,o0roykh,"I had ME at the time, and after adding ram to my system, I was soft locking about every 30-45 min.",pcmasterrace,2026-01-21 00:34:03,1
Intel,o0tgjsl,"Not to mention that a lot of the stutters of the Oblivion remaster are not because of UE5, but because the original Oblivion code still stutters on hardware from over 15 years later.  Asset streaming in open worlds is a hard problem. We live in a blessed time to treat the occasional shader compilation stutter in UE5 games a as a major issue.",pcmasterrace,2026-01-21 07:34:47,3
Intel,o0s71he,I had a vanilla GeForce 6800 which I was able to unmask some extra pixel pipelines for extra performance. Not sure if I ever played Oblivion on that though because I played it a couple years after release. I might've played it with my next build with the GeForce 9800 gt.,pcmasterrace,2026-01-21 02:16:40,2
Intel,o0szko3,It doesnâ€™t matter how old the 3080 is. Itâ€™s not a weak gpu by any means. Itâ€™s like 10% stronger than the 5060ti 16gb and only slightly trails the 4070 for comparison.,pcmasterrace,2026-01-21 05:14:47,2
Intel,o0rtra0,Eh I blame developers just as much,pcmasterrace,2026-01-21 01:00:41,0
Intel,o0rmkqr,"12gb 3080 and 3080ti are essentially the same performance, so the closest equivalent would be a base 4070 and 4070 super or 5070",pcmasterrace,2026-01-21 00:21:07,2
Intel,o0rn9q5,"Yeah there are a lot more advantages to the 3080(like its near 2x bus), and there are 2 variants of the 5060ti... don't know ""iceberg tech""... so wont speak to that.  When looking at either. We see that when in RT, the 3080 just smacks the crap out of the 5060ti 8Gb, and makes the 5060ti 16GB do a very good impression of the french. Only winning when it can leverage the 16GB Vram buffer, which is predominantly 4k, which is not a great res for that GPU anyway.",pcmasterrace,2026-01-21 00:24:52,2
Intel,o0rn503,"Tech power up isnâ€™t accurate down to the perctanges like that, itâ€™s more of a rough idea on where the perf of a card stacks up.   In real world benchmarks and game testing at 1440p/4K the 3080 is much closer to the 5070.   You even said that the 3080ti is closest to the 4070 super, which is correct. The 12gb 3080 is essentially the same thing as a 3080ti in perf, and a 4070 super is around 5070 perf too, so you basically agree with me already.",pcmasterrace,2026-01-21 00:24:09,1
Intel,o0s7zya,haha. Lego Horizon Adventures to fully clarify!,pcmasterrace,2026-01-21 02:22:03,3
Intel,o0sqozz,"Probably able to just brute force it a bit more, I found I had a better time when I went with the x3d chip from my 12th gen i5. The stutters hit less frequently recovered much faster than the gpu upgrade",pcmasterrace,2026-01-21 04:13:50,1
Intel,o0u3u8p,bad developers are bad on all cards* ftfy,pcmasterrace,2026-01-21 11:11:44,1
Intel,o0w0qoc,I mean 7900 XTX is still tied for like 8th best amongst non-commercial cards.  Its kind of a beast.,pcmasterrace,2026-01-21 17:24:41,1
Intel,o0t27ld,"There are definitely examples of ue5 that run fine on amd cards, but there are a lot that donâ€™t.",pcmasterrace,2026-01-21 05:34:13,1
Intel,o0yo7id,Worse than a game at max settings? Well yeah. A game from 2014 at lowest also looked worse than a maxed out 2004 game. Maxed out 1440p half life 2 vs lowest settings 720p shadow of Mordor (which was considered a good looking game on releaseâ€¦not so much later). Half life 2 clears.  Itâ€™s been like that for a very long time. Basically ever since graphics settings.,pcmasterrace,2026-01-22 00:57:53,1
Intel,o0wylh1,I think we're saying the same thing? Only a small number of recent games require RT.,pcmasterrace,2026-01-21 19:54:41,1
Intel,o0ucpgf,"And the physical hardware was so low quality.  GPU didn't even need a 12VHWPR connector to frequently fail within 2-3 years, because capacitors, coolers, PSUs, and the general build quality were so bad. [This thing was a $299 card in 2002](https://www.techpowerup.com/gpu-specs/geforce4-ti-4400.c180), and it's not like high-end cards were built much better. Even on a purely physical level, it's awesome how much better built a [modern equivalent like the 5060 is.](https://www.techpowerup.com/gpu-specs/geforce-rtx-5060.c4219)  I lost 2 GPUs to those dreaded flickering polygons (afaik indicating that some memory cell or data channel had broken) just in the 3-4 or so years I was playing World of Warcraft.  Not to mention the buzzing audio because most cables and speakers broke easily. I had to weigh down the cable of my speakers to make up for a partial cable break constantly, and had buzzy sound every summer when temperatures went up.",pcmasterrace,2026-01-21 12:19:56,2
Intel,o0vedp8,Whoever said it's *weak*? It's now a mid-range GPU. It is *literally the recommended GPU* for this game.,pcmasterrace,2026-01-21 15:44:18,1
Intel,o0rr75m,Thank you for the insight ðŸ¤,pcmasterrace,2026-01-21 00:46:17,1
Intel,o0rt02w,Is HUB a big enough channel for ya? Their review puts the the 5060 Ti 11.5% behind the 3080 12gb at 1440p so within 10% of a 3080 10gb. I would not call that a smacking.   I'm not saying that the 5060 Ti is a good card or that the 3080 10gb is bad. All the comparisons I'm saying indicate that they're reasonably close to each other and that's about it,pcmasterrace,2026-01-21 00:56:25,0
Intel,o0rryte,"The post specifically mentions the 10gb variant of the 3080 though. HUB's 5060 Ti puts it 10% slower than a 4070/3080 12gb at 1440p as well, and the 10gb variant is a few percent slower. Am I missing something?  https://preview.redd.it/pvqh1o7aoleg1.png?width=1763&format=png&auto=webp&s=ed5bda7f6f8891119c5126bc371b1930bca8bf04",pcmasterrace,2026-01-21 00:50:34,1
Intel,o0ss6c5,"Makes sense, I have a fairly mid range CPU, maybe I'll upgrade it, but to be honest, I don't want to till the next generation of CPUs hit",pcmasterrace,2026-01-21 04:23:41,1
Intel,o0vd4yi,"Oh yeh it usually runs bad, but this is Id's engine which runs amazing on amd cards generally. They did black magic to get it running smoothly. Obviously I couldn't use the path tracing mode but who cares.",pcmasterrace,2026-01-21 15:38:38,2
Intel,o0vdgye,"That's funny, because of course both Indiana Jones and Doom TDA at max settings run *much* better on your GPU than Control does. Because they have 'forced RT' so the games are actually built to *properly utilize* it, unlike Control which has the nice non-forced RT everyone loves - because it's bolted onto the existing engine and runs like crap. Just like *every* game that has RT bolted on runs like crap when it's enabled.",pcmasterrace,2026-01-21 15:40:11,2
Intel,o0uhod9,"I had one of the earlier fan cooled GPUs, and it had the bearing in the fan fail. It rattled for a bit, then the card died.   I remember all the issues with audio drivers, and it being generally recommended to have a separate sound card to get better stability for games",pcmasterrace,2026-01-21 12:53:12,2
Intel,o0vf8n6,What I meant was the guy I was replying to says itâ€™s perfectly natural for a 6 year old gpu to be seen in the recommended. Kinda insinuating that the 3080 is an older less powerful card that should be relegated to this status. Despite it being stronger than the less than a year old 5060ti,pcmasterrace,2026-01-21 15:48:07,-1
Intel,o0sflta,"Again, what 5060ti...  You used a source. I used the exact same source. When you actually dive into the specific results, the ""12%"" average. Turns out its a lot more than 12% but occasionally the 3080 didn't run a vram parasite. You just have to open up the techpowerup results to see the details.",pcmasterrace,2026-01-21 03:05:37,1
Intel,o0rvpqp,Where my bro the 3070 ti? He falls right above the 7700 XT in 1080p.,pcmasterrace,2026-01-21 01:11:54,1
Intel,o0ssf1o,9800s only recently came out and honestly even the 5800x3d is still a premium chip. You wouldnâ€™t be putting yourself in a bad position if you grabbed any x3d chip,pcmasterrace,2026-01-21 04:25:19,2
Intel,o0viifn,"Sure, but also it *is* an older less powerful card. It's simply relegated from a flagship status to midrange - while 5060Ti although more recent was of course midrange to begin with.",pcmasterrace,2026-01-21 16:02:45,1
Intel,o0t2uzq,You can't just grab a CPU and call it a day. Moving from LGA 1700 to AM4 would be monumentally stupid. He can already upgrade to CPUs equal to the 5800X3D on the existing platform.,pcmasterrace,2026-01-21 05:39:04,2
Intel,o0t3r93,"Of course he can, I was just on the same train of thought of I upgraded to an x3d chip.",pcmasterrace,2026-01-21 05:45:55,2
Intel,o0ta593,You upgraded to AM5 which makes sense. The 5800X3D isn't an upgrade over the best Intel CPUs.,pcmasterrace,2026-01-21 06:38:03,2
Intel,nx0v4hv,"CPUs don't support resizable BAR, devices do. Arc cards don't require resizable BAR it's just expected to be used.",pcmasterrace,2026-01-01 03:06:37,2
Intel,nx0ze2r,"Rebar isn't needed for Arc gpus to function, but it is needed for them to function as intended. I believe Arc does need a motherboard with a UEFI bios, but could be wrong on that.",pcmasterrace,2026-01-01 03:35:19,2
Intel,o0ccybu,That actually brings up a good point. How's the price & availability of the Intel GPU's?,pcmasterrace,2026-01-18 19:28:48,788
Intel,o0cp93j,"Yeah, I have a 5090.  Optiplex 5090.",pcmasterrace,2026-01-18 20:28:36,274
Intel,o0ccriv,lol i have 2 580s,pcmasterrace,2026-01-18 19:27:52,90
Intel,o0cmg7d,nah just ask him what color his graphic centre is /j,pcmasterrace,2026-01-18 20:14:53,20
Intel,o0cbzzi,GTX 480 and RX 480,pcmasterrace,2026-01-18 19:24:12,34
Intel,o0fso4l,Meanwhile in Russia...  https://preview.redd.it/kshjflwab9eg1.jpeg?width=1280&format=pjpg&auto=webp&s=b868e7a5f35ed44778fb7e1eeede67093b0515bd  [https://en.wikipedia.org/wiki/KR580VM80A](https://en.wikipedia.org/wiki/KR580VM80A),pcmasterrace,2026-01-19 07:16:26,11
Intel,o0cpnyb,Rx580 on top,pcmasterrace,2026-01-18 20:30:34,41
Intel,o0d3r82,I'd be impressed if it's a 3GB GTX 580 Classified.,pcmasterrace,2026-01-18 21:46:26,8
Intel,o0dbla7,Wrong! My friend has the Arc A580,pcmasterrace,2026-01-18 22:23:05,6
Intel,o0dt870,RX 580 one of the GOAT,pcmasterrace,2026-01-18 23:50:53,12
Intel,o0d0pc5,I loved my GTX 580.,pcmasterrace,2026-01-18 21:30:22,5
Intel,o0ccxs9,And there is also Intel A580,pcmasterrace,2026-01-18 19:28:44,18
Intel,o0cibl0,"Lol, I went from an rx580 to a B580",pcmasterrace,2026-01-18 19:54:48,7
Intel,o0g149z,Or he's dislexic and actually meant 5080,pcmasterrace,2026-01-19 08:32:47,4
Intel,o0d0c3o,"Rx580 is a great piece of technology. Still have one in one of my PCs, it handled all I needed with ease.",pcmasterrace,2026-01-18 21:28:09,9
Intel,o0d5ako,R9 280 is like a 580 right? It's just cooler?  Has more numbers does bigger morez,pcmasterrace,2026-01-18 21:53:39,4
Intel,o0gd6wn,Yeah I got the 9700â€¦  â€¦ati radeon 9700,pcmasterrace,2026-01-19 10:26:53,5
Intel,o0glurf,"I have a 1060 gtx, i cant play the oblivion remake ðŸ˜ž",pcmasterrace,2026-01-19 11:43:31,3
Intel,o0cq1kl,Intel gpus are real?,pcmasterrace,2026-01-18 20:32:22,8
Intel,o0cq0c5,Ran a couple 580's in sli for the first rig I built.,pcmasterrace,2026-01-18 20:32:12,2
Intel,o0d1rwh,You are goddamn right,pcmasterrace,2026-01-18 21:36:44,2
Intel,o0dweq8,Intel 486 vs AMD 486 vs Cyrix 486,pcmasterrace,2026-01-19 00:07:54,2
Intel,o0eb0la,"Nah, he's got a 5800. A lot of people with money don't know what they've got. They just go to the store, get a prebuild and they're all set up.",pcmasterrace,2026-01-19 01:27:00,2
Intel,o0hmeuq,"If i buy the Arc, i can complete the holy trinity",pcmasterrace,2026-01-19 15:24:16,2
Intel,o0cjgck,RTX 580?,pcmasterrace,2026-01-18 20:00:16,2
Intel,o0ee42v,I would initially think that he has a 5080 lol,pcmasterrace,2026-01-19 01:44:37,2
Intel,o0cm1e2,"Reminds me of [""There are that many digits in graphics cards now? 'RTX'???""](https://youtu.be/3RF5AdkQ9nI?si=mm4TORijj2QkGdoK&t=32s)",pcmasterrace,2026-01-18 20:12:51,1
Intel,o0cqipm,Hercules Monochrome. 2megs,pcmasterrace,2026-01-18 20:34:40,1
Intel,o0djedq,The GOAT,pcmasterrace,2026-01-18 23:00:27,1
Intel,o0do2xu,"It seems they took the idea from bad children's film studios, gave it a similar name and questionable design, perfect for confusing parents.",pcmasterrace,2026-01-18 23:23:59,1
Intel,o0e4jg2,"5080 has ""something 580""",pcmasterrace,2026-01-19 00:51:02,1
Intel,o0ebjxm,"Mmm Thermi, the way its meant to be grilled!",pcmasterrace,2026-01-19 01:30:05,1
Intel,o0ff8oh,Pair it with a 7700 something,pcmasterrace,2026-01-19 05:27:58,1
Intel,o0fpnk1,My unc has a 280 ðŸ˜­,pcmasterrace,2026-01-19 06:50:50,1
Intel,o0fz7n0,I had rx570 when it first launched and ngl.. 8 GB was great,pcmasterrace,2026-01-19 08:15:03,1
Intel,o0gcvdn,"A gtx 580 was the first gaming end card I got. Before that I had some amd office card in my family's pc, and then a gt 8500 in my college pc.  Man did I love this card. It played everything I threw at it on ultra/high. I may not have paired it with the best cpu, probably a mid-range one, but It ran until I replaced it with a gtx 970.  Back then I was snobbish and I just kept this card in storage, then threw out all my old pc parts when moving house. Only nowadays do I realized that I could have kept it and used it for an XP retro build. What a dumbass I was.",pcmasterrace,2026-01-19 10:23:54,1
Intel,o0gfe5w,"RX580, the ~50$ card that refuses to die and that we donâ€™t want to see go.",pcmasterrace,2026-01-19 10:47:01,1
Intel,o0grby2,"We also have i7 7700 from intel and two from AMD, CPU 7700 and  GPU RX 7700",pcmasterrace,2026-01-19 12:26:49,1
Intel,o0gtxd5,The rx580 is realistic to still be using. Thatâ€™s some value per $ especially over time.,pcmasterrace,2026-01-19 12:45:34,1
Intel,o0inz5z,"I was interested in these Intel GPUs until I read that they need to do optimizations for each game, this is not sustainable.",pcmasterrace,2026-01-19 18:13:38,1
Intel,o18w1me,If I did builds with Intel CPUs I would definitely consider an ARC. They seem very decent cards for the price.,pcmasterrace,2026-01-23 14:50:30,1
Intel,o0ctjk8,"I mean the Rx 580 is a $30-40 GPU in 2026 as it's an 11 year old architecture and no longer supported and missing features to play current AAA games, it's also a pretty slow card.",pcmasterrace,2026-01-18 20:49:28,0
Intel,o0cl7n8,why is there such a low creativity in gpu and cpu names/numbers ?,pcmasterrace,2026-01-18 20:08:49,0
Intel,o0d207u,Nowadays id assume 5080. Most people arent buying decade or older hardware or intel for pcs. They buy whatever the hell best uy has for them,pcmasterrace,2026-01-18 21:37:58,-1
Intel,o0cde1v,So 480?,pcmasterrace,2026-01-18 19:30:55,-2
Intel,o0ck7ww,Kinda OK. Drivers are better but still not perfect. The B580 is competent but not fast enough to utilize all 12 GBs of VRAM. It is great if you use Linux or simply don't want to buy Nvidia.,pcmasterrace,2026-01-18 20:03:59,475
Intel,o0d2di6,Rebar is required.  You should be using Rebar anyways though.,pcmasterrace,2026-01-18 21:39:50,12
Intel,o0d9h9j,They were going for 320$ CAD (about 230$ USD) here in Canada for black friday and boxing day with a free game. If you compare it to equivalent cards they're about 100$ cheaper while having more VRAM and readily available here.,pcmasterrace,2026-01-18 22:13:36,8
Intel,o0cldrs,"A380 user here. Very damn cheap gpu, rivals price of RX580. Going strong, even runs some AAA titles fine (Cities: Skylines II, got to 160k cims till my cpu gave up. GPU can go further, but idk abt other AAA thats new). Ran Ace Combat 7 on it, max settings and it was perfectly fine. Drivers are amazing under linux, not so much about windows ones",pcmasterrace,2026-01-18 20:09:38,6
Intel,o0d54m4,Bad in south America,pcmasterrace,2026-01-18 21:52:52,2
Intel,o0e9qvp,"Hey for $260, the Arc B580 has some killer performance. XeSS (Intel's upscaler) is improving and I was able to play games at 4K 40-50FPS on Ultra settings for Forza Horizon 5, Doom the Dark Ages, Helldivers 2, and Space Marine 2 paired with an i5-13400F.",pcmasterrace,2026-01-19 01:19:42,2
Intel,o0fpppz,"In my country, B580 is the best thing I could get for the money and it's not even close.",pcmasterrace,2026-01-19 06:51:20,2
Intel,o0noyu8,It's gotten much better. I could get one for MSRP.,pcmasterrace,2026-01-20 12:47:14,2
Intel,o0cq6hf,"how ironic.  now spit it out, which one?  https://preview.redd.it/szmltn1i46eg1.png?width=794&format=png&auto=webp&s=ffb01eef59392b03c0fe1442ca62553e986aa472",pcmasterrace,2026-01-18 20:33:03,109
Intel,o0cwwo5,"Same. I have 2 GTX580 and sold my third one because it was ""slightly"" broken.",pcmasterrace,2026-01-18 21:08:18,10
Intel,o0he5dm,"didn't know 1160 was a thing, probably missed that generation",pcmasterrace,2026-01-19 14:43:22,2
Intel,o0erq04,Me offering my friend my 6600  What he thinks I'm offering: RX 6600XT  What he probably thinks is realistic: i5 6600k  What it actually is: Core 2 Duo e6600,pcmasterrace,2026-01-19 02:57:37,40
Intel,o0d1xfq,I ride until it dies,pcmasterrace,2026-01-18 21:37:34,21
Intel,o0fbbuv,"For sure, have mine as a backup if the need arises I have plenty of games that I can have fun with. A damn work horse thatâ€™s for sure.",pcmasterrace,2026-01-19 04:59:35,1
Intel,o0cpgdy,"There is, but I think the B580 is significantly more popular, which actually makes me happy that Intel GPUs became an actual decent choice.",pcmasterrace,2026-01-18 20:29:33,31
Intel,o0icom7,More like the 8GB R9 290X and R9 390X with half the bus width.,pcmasterrace,2026-01-19 17:22:53,2
Intel,o0mlgpl,Micro center in my state has been having consistent stock of them for months it seems like no one ever buys any,pcmasterrace,2026-01-20 07:06:04,1
Intel,o0cqfkq,I think they're just folklore,pcmasterrace,2026-01-18 20:34:15,2
Intel,o0ftico,"Wasn't my first rig, but trolling through my old photos I found this one from 2011.  https://preview.redd.it/ga7bsvd8c9eg1.jpeg?width=959&format=pjpg&auto=webp&s=d45dd8860184363507bcb946cf139f55ed17d238",pcmasterrace,2026-01-19 07:23:49,2
Intel,o0cpit7,RT 5010 2GB DDR4,pcmasterrace,2026-01-18 20:29:53,3
Intel,o0d4iip,RTX 5080,pcmasterrace,2026-01-18 21:49:58,1
Intel,o0dj3wi,Lmao,pcmasterrace,2026-01-18 22:58:58,3
Intel,o0d0m6b,man i lost testosterone just by clicking on that link..,pcmasterrace,2026-01-18 21:29:48,3
Intel,o0ggya2,"it ztill runs most competitive and not too demanding games fine in 1080p, it even gets around 100fps in cs2 at 1440p, love this card",pcmasterrace,2026-01-19 11:00:56,2
Intel,o0cm7zn,"I agree, but the last thing I want  is Snapdragon 8 Elite Gen 5 kinda situation",pcmasterrace,2026-01-18 20:13:45,13
Intel,o0cvvfs,Because being clear > creativity,pcmasterrace,2026-01-18 21:02:44,3
Intel,o0hnopj,"alright, you want creativity?   take my Intelvida Ryarc Ryzen 4070 Super Ti XT XTX Titan Jigawatt edition",pcmasterrace,2026-01-19 15:30:16,1
Intel,o0cp9mh,"Wdym not fast enough to utilize 12GB VRAM? I literally use mine for 4K gaming. On Cities Skylines I, with a city with 60 000 population I use 9.7gb VRAM and 20-60 FPS depending on where in the map you look.",pcmasterrace,2026-01-18 20:28:40,259
Intel,o0clp3b,Do ARC GPUs work well on Linux? I didn't know.,pcmasterrace,2026-01-18 20:11:09,15
Intel,o0h2x02,"More VRAM than you need is better than running out of VRAM. The GPU itself is somewhere in the neighborhood of 4060/4060Ti territory, so it's certainly capable of outrunning 8GB, which would result in a horrible, stuttery mess",pcmasterrace,2026-01-19 13:42:24,6
Intel,o0cxywk,Feel like it's probably the best option at that price if you're scared of used GPUs at least (not that there much competition),pcmasterrace,2026-01-18 21:14:15,3
Intel,o0i3oi5,The opposite of everything you just said is true. Drivers are stable now. It saturates 12 GB of VRAM if playing at higher resolutions. And it actually is not great for Linux if you intend to game.   Intel has good compute for like hosting an LLM on Linux but similar to Nvidia has a pretty substantial performance hit on Linux for gaming. Not quite as bad as Nvidia but nowhere near as plug-and-play as AMD on Linux.,pcmasterrace,2026-01-19 16:42:10,3
Intel,o0dowyb,"Damn I would've thought drivers were even worse on Linux, is it really good for that OS? Just got my B580 and been thinking of changing to Linux 'cause of all the Ai nonsense on Windows",pcmasterrace,2026-01-18 23:28:19,2
Intel,o0if66g,I was getting above 11gb usage on Spiderman Remastered. Probably the only game I've seen it that high though.,pcmasterrace,2026-01-19 17:34:15,2
Intel,o0dgkjl,Arc STILL has driver issues??? Damn and I almost bought a b580...,pcmasterrace,2026-01-18 22:46:41,3
Intel,o0j595a,Borderlands 4 had way worse performance on all of my friends laptops wich were way more expensive than my pc mainly due to vram limitations also at least in my experience drivers and game compatability is great even though I run an and cpu,pcmasterrace,2026-01-19 19:30:19,1
Intel,o0j5svj,Almost every modern cpu that you would pair with a intel b580 has rebar,pcmasterrace,2026-01-19 19:32:52,1
Intel,o0dtrgg,Why? I live in Argentina and have seen them,pcmasterrace,2026-01-18 23:53:46,1
Intel,o0cuxhu,"Funnily enough the GTX580 was the first GPU to really be used in ML/AI training, back in 2012. 2x GTX580s was what the AlexNet model was trained on. AlexNet was widely regarded as the first truly successful deep learning model.",pcmasterrace,2026-01-18 20:57:17,73
Intel,o0e8yu9,"oh you know which 580s i had, let's just say it's better than an xbox 360 ðŸ˜Ž (I don't use them, but when I came back to PC in like 2012 was using them, and am a tech hoarder)",pcmasterrace,2026-01-19 01:15:16,2
Intel,o0d60oc,"Oh, so you're the asshole who sold me the ""slightly broken"" one that got hit by a semi truck at highway speeds  /j",pcmasterrace,2026-01-18 21:57:05,9
Intel,o0d8mam,slightly you say?,pcmasterrace,2026-01-18 22:09:34,2
Intel,o0hiw1i,"was a limited run, only sold from nigerian vendors",pcmasterrace,2026-01-19 15:07:11,3
Intel,o0jbxwq,I still have a GeForce 6600 GT in a tote somewhere.,pcmasterrace,2026-01-19 20:01:11,2
Intel,o0j7rqq,yeeah 580 on top!!!,pcmasterrace,2026-01-19 19:42:03,1
Intel,o0kwr55,"I had your exact configuration until I inherited my little brother's 3060 ti. Not to worry, the 580 still does work in my other brothers computer.",pcmasterrace,2026-01-20 00:47:08,1
Intel,o0fbpv8,That card aged like the finest wine for sure. I've still got my Red Devil there just in case.,pcmasterrace,2026-01-19 05:02:20,4
Intel,o0dlsn8,"And we hope that trend continues, because competition in this case is good.",pcmasterrace,2026-01-18 23:12:30,8
Intel,o0cppvh,"Yeah, I am still happy on A770!",pcmasterrace,2026-01-18 20:30:49,6
Intel,o0eybxo,"lol, they are readily available compared to most other cards.",pcmasterrace,2026-01-19 03:34:37,6
Intel,o0gxsqh,https://preview.redd.it/53vvdbkm2beg1.jpeg?width=1840&format=pjpg&auto=webp&s=512f7fbc438741fd52f4f8966c8a1abb57de88de  My humble rig. Bought the 2nd 580 a year after building the rest of the computer,pcmasterrace,2026-01-19 13:11:23,3
Intel,o0daiks,"Testosterone is nonexistent in this community, why else do you think it's filled with furries and femboys?",pcmasterrace,2026-01-18 22:18:10,3
Intel,o0cqu0m,RX 9090XTX XT OC Stealth 5.0,pcmasterrace,2026-01-18 20:36:08,9
Intel,o0d5ziu,"Third parties do it all already. Zotac amp extreme infinity, gigabyte eagle oc ice sff, powercolor hellhound and red devil",pcmasterrace,2026-01-18 21:56:56,2
Intel,o0hiuz9,"i mean there is room for confusion still, this is just a meme but come on, specially with amd gpus and cpus",pcmasterrace,2026-01-19 15:07:03,1
Intel,o0dc4ob,"haha I use the B580 for cities skylines at 4k as well, would never have expected anyone else to be doing what I am.",pcmasterrace,2026-01-18 22:25:36,135
Intel,o0elhtl,"also, secondary note, if anyone out there *isn't* using Loading Screen Mod/FPS booster in CS1 they need to get on that, even if they are only interested in playing entirely vanilla (y tho)",pcmasterrace,2026-01-19 02:24:55,8
Intel,o0faekq,I'd say that game is an exception. It *will* eat up everything you got and it *will* ask for more.,pcmasterrace,2026-01-19 04:53:09,4
Intel,o0j2smx,"Sounds amazing tbh for a card of that price  Also, XeSS 2 has been surprisingly good for me, I imagine it's even better on an Arc card",pcmasterrace,2026-01-19 19:19:05,2
Intel,o0ehvuo,"modded/assets or no?  also, what's your ram, because in my experience ram is the bottleneck, especially with loading time.",pcmasterrace,2026-01-19 02:05:25,1
Intel,o12wqam,mmm 20 fps huh,pcmasterrace,2026-01-22 17:22:38,0
Intel,o0cr6r0,"As you said yourself, you are not using all 12 GBs at once.",pcmasterrace,2026-01-18 20:37:51,-27
Intel,o0g57yl,You kind of proved their point. Even while playing an intensive game at 4k you only use 10gb.,pcmasterrace,2026-01-19 09:11:28,-5
Intel,o0crte6,Both Intel and AMD support Linux very well. It it only Nvidia who is troublesome in that matter.,pcmasterrace,2026-01-18 20:40:49,55
Intel,o0cqv5n,"Yeah. Since the drivers are open source unlike nvidias ITS AS gÅ‚Ã³d AS amd  Edit - Fuck autocorrect  *its as good as amd, but their drivers are still less mature",pcmasterrace,2026-01-18 20:36:17,1
Intel,o0i4ct2,"This guy is wrong. Intel has a substantial 10-20% performance hit on Linux. Not quite as horrible as Nvidia but nowhere near as good as AMD.   It is true Intel has open source drivers which is nice, but itâ€™s still not a â€œgoodâ€ choice for Linux gaming.   Intel does not invest into Linux gaming at all. They even said this blatantly in a recent interview with Digital Foundry.   If you genuinely care about having a good experience on Linux with no headaches, you need AMD sadly.",pcmasterrace,2026-01-19 16:45:11,3
Intel,o0gbuhc,"On Linux it is mostly Nvidia who is troublesome when it comes to drivers. It is generally better now than before, but getting Nvidia drivers is still a pain in the ass if you want legacy drivers.   Arc and Radeon run great on Linux but game compatability is still not perfect. But that is not a driver issue but a game issue.   Most developers don't release Linux versions still so you need a specialised translation layer between Linux and the game. The funny thing is that even while using the translation layer some games run better on Linux than on Windows.",pcmasterrace,2026-01-19 10:14:28,1
Intel,o0i6d6h,"On windows, no. The opposite of everything that guy said is true.",pcmasterrace,2026-01-19 16:54:14,3
Intel,o0iblt7,"I've had a b580 basically since launch, and the only driver issue I had was from the latest one where intel screwed up vram overclocking. I expect it to be fixed soon anyways because there have been very consistant driver updates.",pcmasterrace,2026-01-19 17:18:03,3
Intel,o0e0nct,"Me too, is there is something more than the B580? Is the only one here, no A380, no A770 for example",pcmasterrace,2026-01-19 00:30:33,1
Intel,o0d8l4w,the beginning of the end,pcmasterrace,2026-01-18 22:09:25,52
Intel,o0dtwnz,"So it's all your fault, you're the origin of the slop.  /Jk",pcmasterrace,2026-01-18 23:54:33,12
Intel,o0dajkr,Yeah. I had a good EVGA 580 that was missing the original fan shroud and fan that I got very cheap and then I got a very cheap 580 that had a dead memory chip which was somehow revived by the guy I got it from with a slight BIOS modification. I then switched the coolers and sold the worst of both worlds as part of a small lot with a bunch of other old cards.,pcmasterrace,2026-01-18 22:18:17,4
Intel,o0g8zm6,Epic,pcmasterrace,2026-01-19 09:47:33,1
Intel,o0cysn7,[XFX AMD Radeonâ„¢ RX 580 GTS XXX Edition 8GB](https://www.xfxforce.com/gpus/amd-radeon-tm-rx-580-gts-xxx-edition-8gb-4),pcmasterrace,2026-01-18 21:18:51,6
Intel,o0dkfg9,"Yooo!!! Thats awesome! I also play on highest settings, I am really happy with my GPU performance (im upgrading from a GTX1650 laptop). Which CPU do you play with?",pcmasterrace,2026-01-18 23:05:42,29
Intel,o0feze5,"I play vanilla and don't play with mods lmao. I have some DLC's that's all.  I have 32GB 6400MT/s RAM, when playing I noticed I was using around 12gb of RAM. Windows idling uses around 8gb.",pcmasterrace,2026-01-19 05:26:00,2
Intel,o0k378d,Since i play on 1080p and i don t have access to dlss i don t use any kind of upscaling but i have to say that for anti aliasing xess is far better than fsr and especially tsr,pcmasterrace,2026-01-19 22:11:58,1
Intel,o0crks5,"You want it to max the 12GB? I can't just tell it to do that, I need to load enough stuff in the game so it maxes the VRAM.  I don't understand your logic. VRAM is like RAM, more chrome tabs, more RAM. So more of the map being rendered/more stuff in the game, more VRAM. Where did you hear otherwise?",pcmasterrace,2026-01-18 20:39:46,33
Intel,o0dbw9y,"Most graphics cards go from 8 GB of VRAM to 12 GB, with very few offering an intermediate amount. If he is using 9.7 GB, that means that an 8 GB card would be too little, making the upgrade to 12 GB necessary.",pcmasterrace,2026-01-18 22:24:30,2
Intel,o0cs90l,"Cause I didn't load enough game at once. Holy shit it's like saying ""ah your pc is too weak and cannot use all 64 gigs at once"" while you just started windows. Like ofc you didn't open anything that will take up RAM. If I loaded up a city with 100 000 population I might max the VRAM because I am loading even more things up. Omg this has to be ragebait",pcmasterrace,2026-01-18 20:42:47,32
Intel,o0cshhu,Yeah but they still used more than 8gbs of vram which is most likely how much would have been put in the card if not 12gbs,pcmasterrace,2026-01-18 20:43:55,7
Intel,o0g6q6a,No. Read my other comments I'm not saying this for  a 82736th time.,pcmasterrace,2026-01-19 09:25:55,1
Intel,o0d91r3,"Well, and also the mobile Intel parts arenâ€™t great for Linux performance from what Iâ€™ve heard. Certainly not particularly relevant to this discussion but just good to know.",pcmasterrace,2026-01-18 22:11:36,5
Intel,o0eurq5,"nvidia on linux user here. it's not as bad as it used to be, there are even distros that install the drivers for you out of the box",pcmasterrace,2026-01-19 03:13:56,1
Intel,o0dhfl8,so don't use linux,pcmasterrace,2026-01-18 22:50:50,-6
Intel,o0dg9e0,"""as gÅ‚Ã³d as"" lmao",pcmasterrace,2026-01-18 22:45:12,1
Intel,o0j5kyz,Yea the guy above you has litterally no idea what he is talking about I had 1 bug and that a meaningless bug in deadlock a game in closed beta,pcmasterrace,2026-01-19 19:31:50,1
Intel,o0e6t0l,"Yeah but they're irrelevant, haven't seen many gpus like the 4070 or something anyways because they are a generation older and weren't very transcendental",pcmasterrace,2026-01-19 01:03:15,1
Intel,o0eqjgs,"I mean, you're not entirely wrong. We researchers have been working in the field for quite a long time. I've been doing machine learning since mid 2010s. I still remember trying to run early artificial neural networks on a GTX 960 4GB edition. For better and worse, we developed the mathematics and the software frameworks for this.  The use of GPUs in high performance computation is much older than most PC users/gamers realise",pcmasterrace,2026-01-19 02:51:25,14
Intel,o0e52zw,"This, kids, is why ""I can fix her!"" never applies to partners, cars, and e-waste lots on eBay.",pcmasterrace,2026-01-19 00:53:55,3
Intel,o0df8ma,lol,pcmasterrace,2026-01-18 22:40:27,2
Intel,o0d0eas,Tragic that XFX wasn't making ATi cards back in the day or we could have had an **XFX X1900 XTX XXX** Edition.,pcmasterrace,2026-01-18 21:28:29,5
Intel,o0ei7l9,"imho gpu doesn't matter much for CS, what matters is ram and cpu.  edit: lotta poorly informed people here. I played CS, modded, on a laptop without a dedicated GPU. Ran fine because I had a decent CPU and lots of ram.",pcmasterrace,2026-01-19 02:07:08,6
Intel,o0kz5g9,You can still probably get some notable FPS gains on your bigger cities using   [loading screen mod](https://steamcommunity.com/sharedfiles/filedetails/?id=2858591409)  and   [FPS booster](https://steamcommunity.com/sharedfiles/filedetails/?id=2105755179)  it's just nice having a bit more responsiveness,pcmasterrace,2026-01-20 01:00:15,2
Intel,o0k8ojj,"Don't sleep on xess 2, as I said it's been very good in terms of quality for me  I guess at 1080 you don't need it, but if some game can't write reach 60 or whatever your target is, definitely try it on ultra quality",pcmasterrace,2026-01-19 22:39:10,2
Intel,o0ct5yt,I donâ€™t understand what these guys donâ€™t understand.   Itâ€™s like theyâ€™re purposely misreading your comment.,pcmasterrace,2026-01-18 20:47:28,21
Intel,o0dtd2l,Bud wants a 9.6GB GPU to ensure 100% usage.,pcmasterrace,2026-01-18 23:51:36,6
Intel,o0cv5bz,"The B570 has 10GBs of VRAM. If the user could live with a lower FPS, then a B570 could have been enough.",pcmasterrace,2026-01-18 20:58:33,-1
Intel,o0g7eyn,"Ok but your comment is still missing the point. Say someone said ""this car can't go faster than 120km/hr"" your reply is basically ""wdym it can't go over 120km/hr? I literally drive mine on the autobahn. I get 100km/hr"". See the issue?",pcmasterrace,2026-01-19 09:32:36,-6
Intel,o0dayyw,"Well that is mostly because their are mobile parts, so it will always be unfair to compare them to desktop parts.",pcmasterrace,2026-01-18 22:20:12,9
Intel,o0e7agl,"That's was I was saying, the availability is bad. It was bad even at launch",pcmasterrace,2026-01-19 01:05:57,1
Intel,o0jbm2b,"Wasn't the GeForce 8000 series the first to come out with CUDA support?  At least as far as gaming cards, I'm not real familiar with Quadro stuff.  I had an 8800GT back in '07 and I remember talk about how GPUs would one day be useful for so much more than just gaming.",pcmasterrace,2026-01-19 19:59:40,3
Intel,o0kna9z,>The use of GPUs in high performance computation is much older than most PC users/gamers realise  Nvidia saw this coming and that's why they're now worth 5 trillion dollars.  Gamers now get the scraps,pcmasterrace,2026-01-19 23:56:40,3
Intel,o0e7l6x,"4 of the 5 other cards were good, just kinda meh for me as a collector. I definitely sold them for a fair price as well. I just put in the half dead 580 and a dead 8800GTX. I think the other cards were GTX465, GTX 460, Radeon HD5850 and a 560ti.   I think some Chinese guy bought them.",pcmasterrace,2026-01-19 01:07:35,2
Intel,o0fvtqe,"fuck does it apply to then, ships?",pcmasterrace,2026-01-19 07:44:25,1
Intel,o18ul0t,At least it applied to the Major in Ghost In The Shell. ðŸ˜ðŸ˜‰,pcmasterrace,2026-01-23 14:43:14,1
Intel,o0d2do5,I have an XFX SWFT 210 RX 7600,pcmasterrace,2026-01-18 21:39:51,1
Intel,o0gu7nf,"You are right, those downvotes are undeserved",pcmasterrace,2026-01-19 12:47:30,12
Intel,o0h41n3,"Idk why you got downvoted but yes, you're correct. Cities Skylines will run fine on any CPU with iGPU from 2018 that has at least 4 cores. They works fabulous with Ryzens with 3D cache.",pcmasterrace,2026-01-19 13:48:52,8
Intel,o0ic3rd,"Honestly, my mind went Computer Science, then Counter Strike, then I re-added the context of the previous comments and got it.",pcmasterrace,2026-01-19 17:20:18,5
Intel,o0m9prd,Ok I'll give it a look thanks!,pcmasterrace,2026-01-20 05:31:53,2
Intel,o0ctrs0,"I'm getting ragebaited rn ðŸ˜­  VRAM works just like RAM.  If you have a computer that has 32 gigs of RAM, maybe you need to open 1000 chrome tabs to use all 32 gigs of RAM.  If I have a game where more population of the city means more things to be loaded. It means that more population eats up more VRAM. I had a city with 60K pop, means it eats up 9.7gigs of VRAM when I am loading up that size city. If the city was let's say 100K, it's gonna be bigger which means more things to load which means more VRAM used which means potentially maxxing out the VRAM of my GPU.  Also if you didn't know this, maxxing out the VRAM of a GPU is not preferable as it brings stutters. That's why 8gigs of VRAM is not seen as enough these days. Because if I played my game on a 5050, it would start stuttering because I am maxxing the VRAM on that GPU.  Sigh",pcmasterrace,2026-01-18 20:50:43,12
Intel,o0cvy92,"Bait or mental retardation, call it",pcmasterrace,2026-01-18 21:03:13,3
Intel,o0cszpi,But he didnâ€™t say it utilizes 12 GB of VRAM.,pcmasterrace,2026-01-18 20:46:34,5
Intel,o0cwln1,"Sure but they don't sell 9.7gb cards, as far as I know it currently goes 0.5, 1, 2, 3, 3.5, 6, 8, 12, 16, 32, 48.",pcmasterrace,2026-01-18 21:06:31,1
Intel,o0dglmw,"The game uses 9.7 GB which is more than 8 GB, am I correct? How many graphics cards with 9.7 GB of VRAM do you know,? Asking that, because 9 GB in this case is too little and 10 GB would be underutilized according to your impeccable logicðŸ¤£",pcmasterrace,2026-01-18 22:46:49,0
Intel,o0d97pd,"Dont overthink it, its just classic internet people confidently asserting stuff they have no understanding of",pcmasterrace,2026-01-18 22:12:24,5
Intel,o0ehagx,"Not necessarily. While it's not common, there are games and settings that will appreciate the additional 2 GB of VRAM the B580 offers, and the B580 will have enough power play at these settings at 60+ FPS.  TLOU2 at 1080p Very High is an example of this. It needs [at least 11 to run smoothly](https://youtu.be/1miEsftnFI4?t=447) (the 1080 Ti has stable but slow frametimes), and the B580 can get an [average of 80 FPS natively.](https://youtu.be/lLe5AP6igjw?t=611)",pcmasterrace,2026-01-19 02:02:03,3
Intel,o0g7ziv,Out of all the comments you chose that one?? I understand that the GPU has to be strong because I can't be just using 12gb VRAM at 5fps. But guess what? I use 10gb VRAM at 30fps and I have some breathing room of 2gb because if you didn't know if you max out the VRAM in a GPU then your game starts stuttering. Like bro the b580 has a great amount of vram. The RTX 5060 is stronger but it would perform worse in my game since it only has 8 GB so idk what your end goal is here. To prove that the b580 only needs 8?,pcmasterrace,2026-01-19 09:38:05,6
Intel,o0dbgc3,"Well, yes, but comparatively to other mobile parts like Z2 extreme chips they have performance issues on Linux.  Thatâ€™s kind of unfortunate I think because Intel makes some really killer mobile chips that I would love to use but I donâ€™t want to use Windows on these mobile devices preferably.",pcmasterrace,2026-01-18 22:22:26,2
Intel,o0e8k40,"Still who would buy them? I prefer the 9060xt for just a bit more to get something far better, I just don't see the appeal",pcmasterrace,2026-01-19 01:12:58,1
Intel,o0jevud,"I'm not sure which one is the first one to come out with CUDA support, but the GTX580 was the first one people actually managed to run deep learning models on successfully  It's worth bearing in mind that pre 2012, the mathematics, datasets and architectures of neural networks hadn't really developed enough to take full advantage of GPUs  The software frameworks were also very limited back then. It's not just CUDA you need. It's a bunch of CUDA accelerated frameworks like CuBLAS, CuRand. CuDNN etc.  Nvidia has put a lot of work into developing all of those frameworks over the last decade and a half",pcmasterrace,2026-01-19 20:14:57,4
Intel,o0d2rzi,"I guess it even longer. It's actually ""XFX SPEEDSTER SWFT 210 AMD Radeon RX 7600 Core Edition"" Holy shit",pcmasterrace,2026-01-18 21:41:49,3
Intel,o0cxmpy,"The point is that the actual speed of a B580 is more likely to be the bottleneck than its amount of VRAM.   Let say I have a big car. The car's trunk has room for 1000 bricks but the car's engine can only do 100 HP. The car can indeed carry 1000 bricks at once but it won't be able to drive very fast with 1000 bricks in the trunk. If you are very patient in trafic you could tolerate driving with all 1000 bricks at once, but most of us would rather carry fewer bricks and drive faster.  That it the same situation as with the B580 (but depends on the game). You could have a very large world with many building and people all loaded at once, but the GPU is not fast enough to also get you above 60 FPS.  If you can tolerate driving a big car very slowly, then good for you. Patience is a virtue after all.",pcmasterrace,2026-01-18 21:12:14,3
Intel,o0cxkci,"The original comment says that the card has issues with utilizing the full 12 GB of VRAM, then this guy commented on it saying ""what do you mean it's not fast enough to use the 12 GB of VRAM?"" Then proceeded to provide an example if it only using 9.7 GB of VRAM... He could have easily provided an example of another game that uses the full 12 GB of VRAM.... Instead he provided an example of it using 9.7gb VRAM. Maybe I'm confused but it sounded like he was trying to provide an example to counter the original claim of it not using the full 12 GB of VRAM. Which he should have provided an example that used closer to the full 12 GB of VRAM, he could have used a different game that was more graphically intense to do so",pcmasterrace,2026-01-18 21:11:53,-3
Intel,o0dk54v,"holy moly its like 9.7 is the theoretical limit the game can use. No, it will use more the more I play. Thats why having 12GB is great cause you have a bit of breathing rooom.",pcmasterrace,2026-01-18 23:04:17,1
Intel,o0dvop6,"One skill I'm really proud of in myself is my IRL ability to admit when I don't know something. Although I rarely actually say that on the Internet, because if I don't know something, I just don't leave a comment.",pcmasterrace,2026-01-19 00:04:01,2
Intel,o0g9zyg,"My point is you made a dumb comment. I would say 10gb would probably be best to make it a bit cheaper, since 95% of people getting such a budget card are not running 4k games that will utilise 12gb. also  >Out of all the comments you chose that one??  yeah. I chose that one, because it was the first one you made in the chain, and the one I first specifically replied to. Am I supposed to dig up random ones instead?",pcmasterrace,2026-01-19 09:57:11,-5
Intel,o0e8v7g,"I wanted an A310 for an old PC that I have, way better than a Radeon RX 550. Sadly, that won't happen",pcmasterrace,2026-01-19 01:14:42,1
Intel,o0djpum,"wowie this is the second car analogy. But actually damn your analogy is so much better than the last guy's. Yes this makes a lot of sense I understand!!! And yes I can tolerate driving a big car slowly, just like how I said, 20-60 FPS while using 9.7GB in 4K which is still impressive imo. But if it was a 5050 and I maxxed the VRAM, then my game would have bad 1% lows and start stuttering a lot, forcing me to turn down the graphics or resolution. So I think 12GB VRAM is perfect for the B580, 8GB is too little in my opinion. As well because I do use more than 8GB.",pcmasterrace,2026-01-18 23:02:05,1
Intel,o0dit5b,mmmm maybe the secret is using premium instead of regular maybe thatll make my VRAM go up! :D,pcmasterrace,2026-01-18 22:57:27,1
Intel,o0cy6nc,â€œMaybe Iâ€™m confusedâ€  Yes.,pcmasterrace,2026-01-18 21:15:32,7
Intel,o0dyxhr,"I was just playing into their comment, obviously having a buffer is nice but even if 9.7gb was the maximum the game could use, there isn't a card that exists with that configuration it's either 8gb or 12gb",pcmasterrace,2026-01-19 00:21:25,1
Intel,o0goi57,"Ok and if I played my game for a bit longer than I would use even more than 9.7gb and my game will stutter if I had 10gb VRAM. But what's nice is the B580 has 12GB and I still have a decent amount of breathing headroom. Btw cities Skylines came out in 2015, what if I want to play an even more modern and unoptimized game? That 12gb VRAM is amazing for that situation. I really don't understand what you want, I personally bought mine for 300 euros and you can't get 12gb VRAM anywhere else for that price. Also outside of gaming, productivity work eats up a lot of VRAM and this makes the b580 a good option for people who build a pc for work.",pcmasterrace,2026-01-19 12:04:58,5
Intel,o0e9df8,Maybe look for something similar that's used (with some luck) idk what else you could do on that situation,pcmasterrace,2026-01-19 01:17:35,1
Intel,o0cycgq,Like I said maybe I'm confused,pcmasterrace,2026-01-18 21:16:25,-2
Intel,o0gpmz6,My main point here is that your original comment was poorly worded and pretty dumb.,pcmasterrace,2026-01-19 12:13:52,-5
Intel,o0e9rs8,"Is a PC that has ocasional use, not the main. I wanted it low power too to not have to change the PSU, the Arc A310 was perfect for it.",pcmasterrace,2026-01-19 01:19:51,1
Intel,nwywe0i,You can look up compatibility on the PC builder site.   I got a Z790 for 125$   I got the I5-14600Kf for 150$   It holds up really nice.  BE REALLY CAREFUL the Intel GPUs need REBAR so keep your eyes peeled for that.,pcmasterrace,2025-12-31 20:02:48,2
Intel,nwyvx1g,"Must be nice to have a MC, my closest is a few thousand miles away.",pcmasterrace,2025-12-31 20:00:16,1
Intel,nwz9s45,intel 13th and 14th gen have cpu failures so only buy if you are willing to risk that,pcmasterrace,2025-12-31 21:15:39,0
Intel,nwyxczb,Totally forgot about the rebar. Thanks for the reminder. My current cpu and mobo support rebar and I have it activated. Definitely something Iâ€™ll have to consider when I upgrade to ddr5,pcmasterrace,2025-12-31 20:08:03,1
Intel,nwyxm93,Would 12600kf for 100 be good?,pcmasterrace,2025-12-31 20:09:27,1
Intel,nwyxsvs,A few thousand miles? Sorry that sucks. Their bundles/combos on parts are better than sex sometimes.,pcmasterrace,2025-12-31 20:10:26,2
Intel,nwyxsdq,I mean.. CPUs are pretty far ahead of GPUs at least in terms of gaming.   Unless you planned on running 1440 with everything on ultra the B580 is great on most games. Youâ€™ll never feel like you are forced to run a game on low graphics.,pcmasterrace,2025-12-31 20:10:22,1
Intel,nwyy97j,If you plan on investing in Intel GPUs I would press you to get the newest gen.   Intel ARC B580 which is an entry Intel GPU didnâ€™t play nice with older Intel Chips   12600 would hold up just fine.,pcmasterrace,2025-12-31 20:12:54,1
Intel,nwyyd72,Thanks thereâ€™s also the brand new core ultra 5 225f with bf6 for 150 on amazon. Would ultra core be a better fit?,pcmasterrace,2025-12-31 20:13:30,1
Intel,nwyyobw,If you donâ€™t plan on upgrading itâ€™s fine but if Intel releases a newer card that more powerful it might get bottlenecked.,pcmasterrace,2025-12-31 20:15:12,1
Intel,nwyyr3l,Dang maybe am5 cpu is better,pcmasterrace,2025-12-31 20:15:37,1
Intel,nwz119d,Iâ€™d say to go AMD.   Intel royally fucked up in that race but since they did have a major price drop because of it I scooped one up cheap.,pcmasterrace,2025-12-31 20:27:59,1
Intel,o0hinmz,It doesnâ€™t look built. Itâ€™s still in boxes,pcmasterrace,2026-01-19 15:06:03,1
Intel,o0hjrxw,"Better to use two ram sticks, for dual channel, supposed to be faster. But should work fine like this too.",pcmasterrace,2026-01-19 15:11:32,1
Intel,o0i3j63,"This is blue! Proud to see how far you've come in the pc world Quick question, I thought you were going with the 6600",pcmasterrace,2026-01-19 16:41:30,1
Intel,o0hjaim,"Oh, these are just my parts, I did not put a pic of it being built, sorry",pcmasterrace,2026-01-19 15:09:09,1
Intel,o0i3oy5,I wouldn't be surprised if people start doing this for the shortage lmao,pcmasterrace,2026-01-19 16:42:13,1
Intel,o0j13qq,"Micro Center bundle, only 1 stick... I asked for 2 8s but they said it would ruin the deal...",pcmasterrace,2026-01-19 19:11:24,1
Intel,o0j0xv4,"Then, I found the Arc B570 for cheaper + its a newer and faster GPU",pcmasterrace,2026-01-19 19:10:40,1
Intel,o0jmlfk,"Intel doesn't have the most stable drivers compared to the AMD and Nvidia, and their dx12 support for flight sims might actually make it worse than the 6600",pcmasterrace,2026-01-19 20:51:18,0
Intel,o0rbv0y,"Intel Arc drivers + CS is a rough combo. Arc has gotten better, but esports titles are still hit or miss. FPS swings are kinda expected",pcmasterrace,2026-01-20 23:22:41,1
Intel,o0rs9zd,"Monitor?  A bog standard (cheap) 1080p monitor may not go over 60 hz,",pcmasterrace,2026-01-21 00:52:20,1
Intel,o0t7vmk,yeah might as well sell the whole pc and try to build a different one idk if the cpus bottlenecking or if its the gpu,pcmasterrace,2026-01-21 06:19:05,1
Intel,o0t7ueg,i think he has a 180hz acer monitor,pcmasterrace,2026-01-21 06:18:47,1
Intel,o0gker5,That's about what you can expect in that game,pcmasterrace,2026-01-19 11:31:22,11
Intel,o0hcobf,"you are playing at fhd on a 4k gpu, up the resolution and shift more of the work onto the gpu, not the cpu. at fhd its ur cpu doing most of the work. you need to match ur monitor to ur rig.",pcmasterrace,2026-01-19 14:35:47,3
Intel,o0ht7w6,Playing at 1080p with a 5080 is kinda strange. You're gonna be CPU bottlenecked and only using 2/3 of your GPU. Would be better off with 5070 and beefier CPU at that resolution.,pcmasterrace,2026-01-19 15:55:23,3
Intel,o0gpy4p,"In warzone thatâ€™s just your cpu limit. Donâ€™t know about arc raiders but 200fps sounds realistic on a 14700k. The NVIDIA drivers and DLSS overhead is a real issue in cpu limited scenarios and games, often resulting in a 5-10% additional performance hit compared to similar performaning amd cards. (One of the reasons why a 9070xt matches a 5080 in battlefield 6 when tested correctly)   You likely canâ€™t do much besides increasing visual settings. This wonâ€™t make your fps go higher but at 1080p youâ€™ll be cpu bound anyways so you can at least enjoy better visuals xD   Or get a 9800x3d for a nice 30-40% gaming performance improvement",pcmasterrace,2026-01-19 12:16:18,6
Intel,o0gljne,What resolution are you playing at?,pcmasterrace,2026-01-19 11:40:59,2
Intel,o0h9fzo,Play on 1440p instead of 1080p.  That processor is not for 1080p gaming.,pcmasterrace,2026-01-19 14:18:45,2
Intel,o0hqzqu,Why do you use dlss performance on a fullhd? Just render game in native...,pcmasterrace,2026-01-19 15:45:24,2
Intel,o0kaxen,if you cant hold 200 set it to 180 capped or 165 set monitor hz rate to 165 sweet spot is 165,pcmasterrace,2026-01-19 22:50:39,2
Intel,o0lbkk2,i have drops down to 200 on rebirth with a tuned 14700k and a 5090 since the last update lmao. same fps on 2k and 4k.,pcmasterrace,2026-01-20 02:08:32,2
Intel,o0gl4kf,I get 100 fps on a 5600x 7700 xt....,pcmasterrace,2026-01-19 11:37:30,2
Intel,o0gjnc3,Yes BIOs is to newest available Fw.,pcmasterrace,2026-01-19 11:24:49,1
Intel,o0ydflk,You need an X3D CPU if you want 200+ fps in 1080p. Your CPU is more suited for singleplayer games with RT at high resolutions.  Intel CPUs are garbage for competitive games.,pcmasterrace,2026-01-21 23:59:31,1
Intel,o0glnwx,If you want more go x3d,pcmasterrace,2026-01-19 11:41:57,1
Intel,o0glnoy,FULL HD,pcmasterrace,2026-01-19 11:41:54,-2
Intel,o0gqiq7,"That's not how it works. Increasing resolution doesn't inherently take load off the CPU. What actually reduces CPU load is lowering the number of frames the system is trying to produce. When you raise the resolution and the GPU can't render as many frames, CPU load will drop as a side effect, but that's due to the reduced framerate, not the resolution increase itself.  If you cap the FPS to, say, 60 at both 1080p and 4K (assuming the GPU can handle both), the CPU workload will be the same. The CPU's job is tied to how many frames it has to prepare, not the resolution those frames are rendered at. When you're CPU limited, it means the processor is already producing as many frames as it can. Raising the resolution won't magically allow it to produce more frames, it just risks shifting you into a GPU bottleneck instead. That can make utilisation graphs look ""healthier"", but it's impossible for it to increase FPS, since you were already at the limit.",pcmasterrace,2026-01-19 12:20:40,2
Intel,o0gw4i5,"Yeah that looks about right, at least for arc raiders that's right where it should be with dlss, sitting a bit below 200",pcmasterrace,2026-01-19 13:00:21,3
Intel,nyiyx49,I'll believe them (either Intel or AMD) when I see the benchmarks.  Until then this is all just pointless noise.,AMD,2026-01-09 02:57:02,213
Intel,nyj1h7b,well yeah you can't compare them because strix halo is on a signficantly larger die wheras panther lake is more comparable to something like the hx370.   If amd is able to get strix halo at a competitive price then sure it will compete but the issue is that with such a large die I don't think it is possible for them to compete in price with panther lake,AMD,2026-01-09 03:11:08,60
Intel,nyj2ydj,Iâ€™ll never understand why AMD is not committing to design RDNA4 based APUs and at this point I just take RDNA 3.5 as a joke because they canâ€™t even support FSR4 on it officially nor the RX 7000 cards.  Itâ€™s like they are losing on purpose,AMD,2026-01-09 03:19:12,62
Intel,nyja8ig,"AMD has this â€œitâ€™s good enough for a while and weâ€™ll release something great that people will forget this happenedâ€  Vega lasted in mobile for nearly 5 years and got RDNA2 designs. Now, itâ€™s RDNA3.5 being built for mobile platform and betting on that to be good enough until RDNA5/UDNA bridge die designs releases (unverified rumor)  AMD also has this weird obsession with competitor naming. Sure, itâ€™s meant to confuse buyers but itâ€™s hurting them than helping, maybe it does help in terms of inventory.  Theyâ€™re not intel-like of stagnation. Theyâ€™re competing but not for us in the consumer market and weâ€™re just getting scraps until enterprise trend die down (currently AI trend/bubble).  Well, itâ€™s understandable as Zen designs are really focused in Epyc and scale down to Ryzen SKUs.  And the 400 series is a bad refresh when Ryzen 6000 mobile is the definitive refresh they have done, Zen 3+ and move to RDNA2. AMD couldâ€™ve done similar commitment but itâ€™s not currently.  Also, AMD forgor Strix Halo laptops are still nowhere to be found aside from 1 or 2",AMD,2026-01-09 04:00:20,11
Intel,nyj02vs,Amd really doesnâ€™t gaf about anything other than data centre these days,AMD,2026-01-09 03:03:22,11
Intel,nyjy5yc,Idgaf when Strix Halo products are nowhere to be seen (notebooks),AMD,2026-01-09 06:46:12,5
Intel,nykgnvm,"Except that the B390 will be far more common as it will be seen in far more laptops. Yes, the 8060S & 8050s can be found in some laptops, but for the laptops you'll find in places like Currys, Best Buy or Mediamarkt, the B390 will be the most powerful iGPU you'll likely find & it'll happily outdo a Radeon 890M",AMD,2026-01-09 09:30:29,6
Intel,nyj9p6p,"AMD is at the point where Intel was before they went down the route and are recovering, history repeats before it's too late.  Not going to believe either until we get actual benchmarks and results.",AMD,2026-01-09 03:57:09,8
Intel,nz0u8e4,Meanwhile AMD keeps putting out new chips with years old GPUs.,AMD,2026-01-11 19:18:48,3
Intel,nyjg6co,Core Ultra 2 is already a better mobile soc.  I don't know why AMD thinks 12-16 cores is more important than battery life when it comes to laptops.,AMD,2026-01-09 04:35:45,7
Intel,nyjsbo1,It does sound complacent but ultimately the proof is in the pudding.,AMD,2026-01-09 05:59:29,2
Intel,nyku0xe,"If it is not even fair to compare (because Strix Halo is WAY more watts) then why is AMD comparing them? B390 will exist, Strix Halo virtually does not in laptops.",AMD,2026-01-09 11:27:40,2
Intel,nynggco,"AMD is playing the same intel book a few years ago. Except now instead of 14nm+++++++, it is RDNA 3.5555555.",AMD,2026-01-09 19:23:52,2
Intel,nykbcoh,The Intel igpu has a better upscaler by far. FSR3.1 is a third class competitor in comparison. People have been crying out for AMD to release FSR4 for RDNA3.5 but AMD has some seriously stupid execs in charge.,AMD,2026-01-09 08:41:31,4
Intel,nykofoz,Lot of markets and Intel did bribe the oems for decades and still do,AMD,2026-01-09 10:40:44,2
Intel,nyjsl7o,Benchmarks first,AMD,2026-01-09 06:01:32,1
Intel,nyn2qhh,Beware hubris.,AMD,2026-01-09 18:22:54,1
Intel,nynbvp5,"There are already benchmarks, look them up.",AMD,2026-01-09 19:03:00,1
Intel,nynoj5f,Don't they already have an integrated GPU that's on par with an RTX 4060? According to Framework?,AMD,2026-01-09 20:00:51,1
Intel,nyorjcc,"strix halo is nice and all, but too prohibitively expensive to be considered for many people  arc b390/b370 will be available in much cheaper products for which amd doesn't have a proper answer to atm. amd's next lineup can't be lazy if they want to stay competitive",AMD,2026-01-09 23:05:05,1
Intel,nyqdhsx,"The GPU doesn't matter if you don't have proper drivers and they are so far behind still Intel.   Great progress, but the drivers are still going to be the thing that makes people say no.   If intel keeps on chugging away and they work with all the DirectX games backwards and going forward.   I'm talking past DirectX games, you can't just worry about the new games there's games that are older that don't run well.   It took AMD many many years to get decent drivers, Intel I don't know if they're just focusing on hardware and not the drivers, but that so far is what's been holding it back.   Hopefully they can release a true dedicated GPU back in rival something that's out there at a much better price that will bring at least some competition back until the AI scam is over.",AMD,2026-01-10 04:30:09,1
Intel,nzbi19w,Panther Lake is using a superior process technology. So they are right. But it doesn't matter as customer will choose what's better. But until AMD has something out that uses 2nm then yes they will be behind probably,AMD,2026-01-13 08:12:42,1
Intel,nzgbs70,I hope Intel stays competitive and AMD also brings its best to the table.,AMD,2026-01-14 00:33:01,1
Intel,nyj94g8,They should be worried about DLSS 4.5 though. Fix stuttering on FSR 4 and improve image quality,AMD,2026-01-09 03:53:50,2
Intel,nyj4pf7,I just wish Intel would make a very cut down panther lake offering to be the successor to the N1xx/N3xx line of efficient chips that have found their way into mini PCs.,AMD,2026-01-09 03:28:57,1
Intel,nyoak0m,amd unfazed? I bet they are talking big shit again then will get absolutely demolished (as it happened with vega too),AMD,2026-01-09 21:42:52,1
Intel,nyjappo,"Yeah it's old architechture, that's the point, Intel moved to the lastest node and barely manages to eek out a win. A win is a win nonetheless, but AMD still have plenty to dials to turn up.",AMD,2026-01-09 04:03:08,-6
Intel,nyju3k9,"I don't understand the fuss about iGPUs? Like why do they assume the average Joe would care about an IGPU at all? That's maybe 5% of the market and even then...most of them would get a dGPU anyways.  And apart from the GPU, what's special about the CPU? Combining (Lunar Lake) efficiency with (Arrow Lake) power? Sorry but my Ryzen AI 7 350 does that already. The top of the line x9 388h is about ~10% faster in single core aka the only thing that matters and will probably be in 2500â‚¬+ laptops whereas my 7 350 is in 500â‚¬ laptops.  I tried a 285h laptop besides the AI 7 350 and not only did it run hotter and less efficient, it also felt less snappier.    And the AI 7 350 was designed as a Lunar Lake competitor anyways so it was never worse in efficiency and ahead of Arrow Lakes like the 255h in that regard.  So I don't see why anything should really change...?",AMD,2026-01-09 06:13:30,-11
Intel,nyj661m,"AMD doesn't want to bring RDNA 4 to APUs, so as not to give FSR4 to users other than those with dedicated GPUs.",AMD,2026-01-09 03:37:14,-7
Intel,nyjyeyc,a brand new product on a newer node is better than an older product on an older node?! who knew?,AMD,2026-01-09 06:48:14,-2
Intel,nyj7q2f,"And the price. I used to be an AMD fan, but as soon as competition with Intel was gone, AMD raised their prices and now act as if they believe they are a luxury brand. Hope Intel gets back into the game and if Intel can slash their prices it might end-up being the right choice.  At the same price, Intel is dead on arrival. At a serious discount they will take the place of AMD. No one is buying Strix Halo for handhelds, it is too expensive.",AMD,2026-01-09 03:45:51,87
Intel,nyjgluo,Plenty of folks tested it at CES.  Intel was confident enough to let reporters run benchmarks and it's basically around 4050 level.  You should be able to run most games at 1080p at medium-high settings in an Ultrabook form factor.,AMD,2026-01-09 04:38:25,19
Intel,nylsap7,"Both are right/wrong.  Intel made their comparisons to Strix point because theyâ€™re in the same power class. Panther lake is much faster than Strix Point at the same power level (according to Intel, AMD doesnâ€™t deny that) at 45W.  AMD says it doesnâ€™t matter because their Strix halo (up to 120W) is faster which is pretty obvious.   Itâ€™s not technically lying, AMD is just referencing an entirely different class of product.",AMD,2026-01-09 14:52:54,4
Intel,nyj2b71,"Well lunar lake is a monster and competes directly with the z2e both on performance and efficiency, so no reason to think panther lake will be worse.  Even if it falls short of Intels claims it will still be the leader until next year.",AMD,2026-01-09 03:15:43,22
Intel,nyjcak9,The noise is doing a great job advertising for them. A war between them with fighting words will get them tons of free advertising.,AMD,2026-01-09 04:12:29,1
Intel,nylt6fh,And a much higher power budget.  AMD says they win because their 120W chip is faster than Intels 45W chip.   No surprise to anyone.,AMD,2026-01-09 14:57:05,25
Intel,nyjcmeq,"I generally agree but i suspect that the 388h is using a much larger gpu than people suspect. I think its probably ~165mm2 in size, not 55mm2. i suspect the 55mm2 die varient is for the 4xe version, and the 12xe version is 3x that size.  I also dont understand why strix halo is so expensive. It would be interesting to see bom and packaging costs.",AMD,2026-01-09 04:14:25,7
Intel,nyje36x,"It sounds like RDNA4 just doesn't scale at all. All the rumors point to them going straight from RDNA3.5 to RDNA5 in APUs, just skipping RDNA4 all together.",AMD,2026-01-09 04:23:13,38
Intel,nyjuufi,RDNA 3.5 was the only reason I didnâ€™t invest in a STRIX HALO mini PC. The price is too much for outdated unsupported tech.,AMD,2026-01-09 06:19:22,18
Intel,nyji62l,"Might be the same reason they stuck with Vega for so long in APUs.  At the current available desktop memory (DDR4 at the time) an architecture change wouldn't have made a huge difference.    Once DDR5 came out for laptops, we finally saw RDNA 2+ APUs (Ryzen 6000 APUs).  I'd bet once DDR6 starts appearing on laptops we'll get a similar iGPU architecture leap.",AMD,2026-01-09 04:48:17,10
Intel,nynizjw,"AMD probably just didn't bother making a new APU design when they didn't have new CPU core to go with it. Medusa Halo is rumored for 2027 with Zen 6 and UDNA/RDNA5, so the Point version will likely release then too.",AMD,2026-01-09 19:35:27,1
Intel,nyiu18v,AMD has and always will be their own worst enemy,AMD,2026-01-09 02:31:09,80
Intel,nyiyjxo,"It's not like they aren't developing something this whole time, releases are planned many years in advanced. Intel will have some rope and then will get inevitably leap frogged",AMD,2026-01-09 02:55:05,8
Intel,nyj83ic,It's not like Intel isn't doing the same. Panther Lake and ARC are holdovers of things developed under Gelsinger.,AMD,2026-01-09 03:47:56,5
Intel,nyj28hs,"> AMD is going to f--- around and let Intel catch up, in CPUs and GPUs.  this is what we actually need: competition. AMD kicked intels butt, now intel is kicking back. it's a win for us either way.",AMD,2026-01-09 03:15:19,11
Intel,nyiwa7d,I hope Intel will catch up and encourage AMD to compete. Having cleat leader in CPUs or GPUs is bad for consumers.,AMD,2026-01-09 02:43:07,10
Intel,nyjspa3,"I mean, we know that AMD is innovating. They literally showed Zen 6 at CES. Its just not ready yet for mobile, and Intel caught up. Same thing happened with Alder Lake, where intel released that before Zen 4 was ready.",AMD,2026-01-09 06:02:25,4
Intel,nykmhz9,"So AMD having much faster iGPUs for decade or more did not do much.  But now Intel rolling out something at unknown price/power package will absolutely decimate AMD.  Regardless of what will happen, ""AMD's fault"" indeed. (amazing silicon designers and experts at everything posting for free on reddit have convinced me)",AMD,2026-01-09 10:23:35,4
Intel,nynjyg2,"Laptops haven't really been AMD's focus, and apart from Zen1, AMD's focus has been mostly on data center, with desktop being the natural offshoot.",AMD,2026-01-09 19:39:58,1
Intel,nyjozjz,Yeah they ain't immune to being complacent.  And bad press doesn't make Intel stay bad.,AMD,2026-01-09 05:34:41,0
Intel,nzb2682,"News at 10: ""Companies prioritise profits""",AMD,2026-01-13 05:54:06,1
Intel,nynkzx7,"so either AMD doesn't have the capacity to produce them, or OEMs aren't interested, neither option is a compelling reason for AMD to focus on mobile",AMD,2026-01-09 19:44:43,1
Intel,nyjj3pe,"Weâ€™d wish they were, but theyâ€™re not. Intel was struggling on all fronts due to their fabs. Amd is actually moving super fast in data centre so both epyc and instinct which is where they believe their money will be. They just donâ€™t care to do anything in the consumer market.",AMD,2026-01-09 04:54:22,6
Intel,nynm27a,"Halo is so much faster that the upscaler difference doesn't matter at all. Of course, it is probably also bigger.",AMD,2026-01-09 19:49:33,0
Intel,nyoktyw,"Yes, Strix Halo",AMD,2026-01-09 22:31:24,1
Intel,nyjieva,And having fsr4 supported in mobile at all,AMD,2026-01-09 04:49:53,5
Intel,nzat1bf,FSR 4+ may be great but game support (number of titles + GPUs supported) is embarrassingly low,AMD,2026-01-13 04:48:21,1
Intel,nyjm8fi,Dlss 4.5 not that great in my opinion. It fixes some ghosting but creates more shimmering because it has so much sharpening. I had to dial back to 4.0.,AMD,2026-01-09 05:15:20,0
Intel,nyj9k89,wildcat lake.  only issue it seems to be using 2 P cores and 4 LPE cores instead of E + LPE,AMD,2026-01-09 03:56:23,3
Intel,nyjicwx,70% faster being â€œbarely eke out a winâ€? Go ask why amd is stuck with 18 month old architecture despite intel managing to replace arrow lake after 12?,AMD,2026-01-09 04:49:30,12
Intel,nyjvty1,"Youâ€™ve got it backwards. The majority of laptops use IGPUs. IGPUs being as powerful as integrated graphics allows for cheaper thinner devices that are more power efficient. The entire intel CPU/IGPU performs on par with a 4050 at 60w at only 45w. When you factor in the 10-15w the CPU takes with the 4050 and youâ€™re looking at similar performance at like half the power.   Being power efficient opens up a lot of form factors to be able to game with such as thin and light laptops, tablets, or gaming handhelds",AMD,2026-01-09 06:27:13,10
Intel,nyl2kgg,"> I used to be an AMD fan, but as soon as competition with Intel was gone, AMD raised their prices and now act as if they believe they are a luxury brand.   That's why it's silly to be a ""fan"" or ""supporter"" of one company or the other.  They don't care about you, they care about making money and when they have a dominant position they will exploit it.  > At the same price, Intel is dead on arrival. At a serious discount they will take the place of AMD. No one is buying Strix Halo for handhelds, it is too expensive.  Outside of that one device (Ayaneo maybe?), you're right.  But now AMD is also releasing an 8-core version of Strix Halo with the full 40 GPU CUs, which should be cheaper.  I expect that we'll see that in more handhelds at the high end.  Realistically speaking, it's easy to make the case that on a 7""-9"" screen the 40 CUs is way overkill.  There's still room for a middle ground that Intel could easily fill.",AMD,2026-01-09 12:30:34,34
Intel,nyksz11,"Absolutely consumers win when competition is hot, AMD has a bit too much of a lead ATM so they are cashing in and getting lazy. That said I am glad they are having their day, only because a few years ago they were on the brink of bankruptcy and I really want to see them on a fairly level playing field with Intel... If we end up with 2 juggernauts training blows, having big resesrch budgets, etc we'll get lots of innovation and competitive pricing.",AMD,2026-01-09 11:19:09,10
Intel,nykkoyj,"You had to shovel $1k for a 8 core CPU for about a decade, before AMD came.  So ""it just hiked the price"" is BS.  AMD cannot keep prices low while TSMC, effective monopolist, keeps posting record profits quarter after quarter.",AMD,2026-01-09 10:07:29,40
Intel,nyjf6qu,AMD is basically just waiting for their chance to do the bad things. They are a corporation after all.,AMD,2026-01-09 04:29:52,33
Intel,nyo271t,"Help us Cyrix, you are our only hope...",AMD,2026-01-09 21:04:11,3
Intel,nykfx0k,AMD has always been like this. The OG Athlon FX line from ~22 years ago were $1000 CPUs.,AMD,2026-01-09 09:23:31,-3
Intel,nyjjlvs,Wasn't it equivalent more or less to the 4050m as it was power limited to 30 watts?,AMD,2026-01-09 04:57:40,13
Intel,nykm4lu,"It's the price of the final product that will matter.  And given that Intel has lion's share of the mobile market, I don't see why the would not ask outrageous $$$ for it.  It is ""impressive"" only in the ""for iGPU"" context.   Based on the benches shown, laptops were consuming around 60W.  While AMD""s 370 HX has been shown to be able to game at below 20W, so uh.  Let's bait for wenchmarks in any case.",AMD,2026-01-09 10:20:17,5
Intel,nyl1xad,"Sure, sure.  I'm going to wait for proper benchmarks done under lab conditions and documented by more than ""Intel let me run this game with the FPS counter on.""  I mean, the general impressions for Intel are quite positive and if they're true then I hope it spurs AMD to do more.  I'm just not going to blindly accept ""first impressions"" as a replacement for proper testing.",AMD,2026-01-09 12:26:10,1
Intel,nym6wt0,*With 64gigs of ram at 9600mhz,AMD,2026-01-09 16:00:23,0
Intel,nzialor,"Tbf, it's like dgpu winner is claimed by who has the strongest one, so in that way it's kinda fair.  But how's the availability? Is halo in laptops actually? What's the pricing?  And what's the bang per buck on point and this?",AMD,2026-01-14 08:41:12,1
Intel,nyoht8c,"https://x.com/jaykihn0/status/1812898063502938260/photo/1  ""PTL-H 12Xe pictured."" so according to that the 55mm2 die variant is xe12",AMD,2026-01-09 22:16:37,4
Intel,nynhnmj,"12 Xe cores is 60% of the 20 cores in the B580 and that's 272mmÂ², but of course that also has GDDR memory controllers, and such that aren't needed on a GPU chiplet, but it's likely that the die for the top SKU is quite a bit bigger than 55mmÂ², I'd say between 90 and 130 mmÂ².",AMD,2026-01-09 19:29:20,3
Intel,nykps9b,">I think its probably ~165mm2 in size, not 55mm2.   Which might open an ""dGPU sized iGPUs"" race.  NV could be the main victim here Surely AMD can oversize its iGPUs too.  I actually thought that AMD was forced to do so, by Filthy Green's GPP effectively banning AMD dGPUs. Typing this from G15 AMD Advantage Edition TUF.",AMD,2026-01-09 10:52:22,1
Intel,nyjhzne,What matters is the intel chip regardless of actual die size runs on quad channel LPDDR memory instead of the octa channel of strix halo and is fitting into mid and small size laptops 15-45w.,AMD,2026-01-09 04:47:09,-9
Intel,nykcsfk,Then how is the exynos 2600 using rdna4 fron samsung if it doesn't scale?,AMD,2026-01-09 08:54:32,11
Intel,nyktb13,RDNA5 doesn't exist. The actual name for the next generation architecture is UDNA1.,AMD,2026-01-09 11:21:54,-4
Intel,nzasd0f,"Same here. No new FSR tech and ROCm was just as poor. It works now but Vulkan is often better.. Very disappointing. For AI, NVidia is so far ahead.",AMD,2026-01-13 04:43:51,1
Intel,nym1e1u,If intel can extract more out of LPDDR5x with B390 then I don't see how AMD can't. Just too stingy to give more die area to cache?,AMD,2026-01-09 15:35:23,3
Intel,nyjxf3k,So basically RDNA4 is just another RDNA1,AMD,2026-01-09 06:40:05,1
Intel,nytzgu6,Kepler said in another subreddit that Medusa Premium and Halo is launching in 2028. You're only getting the crappy RDNA3.5 iGPUs for the third time.,AMD,2026-01-10 18:54:53,1
Intel,nyj1177,"Intel: but the enemy of my enemy, is my friend.  Intel ðŸ¤ AMD  we're cooked guys /s",AMD,2026-01-09 03:08:41,23
Intel,nykbk7y,"Yeah the people saying AMD is stagnating are just wrong. AMD is kicking all kinds of ass... They just don't care much for the consumer market currently.    The other issue is that there's no point releasing a new line of products when no one can afford anything because nand flash is so expensive.    Companies CAN afford this because they need to ride the ai wave, but consumers can't because the average PC cost almost doubled.",AMD,2026-01-09 08:43:25,3
Intel,nyjeicz,"Wow! Excellent. Hopefully we'll see the products coming to market soon, and hopefully the 2 P cores won't matter as much since we're seeing a major lithography improvement. Intel is really impressing me lately.",AMD,2026-01-09 04:25:42,2
Intel,nyltrkg,This. Plus even lunar lake outperformed Strix Point in many scenarios already. Intel is at least one generation ahead here,AMD,2026-01-09 14:59:54,2
Intel,nyk0tw7,"It's not the 4050 at 60W. The laptop they compared only allows for 30W to the 4050. Nvidia's specs for the 4050 is 35W minimum, so I don't know how Dell even got to 30W. Below a certain wattage, gpu performance decreases exponentially because a minimum level of power is required to even have the gpu turned on.   Panther Lake is built on Intel 18A, which is supposed to be much better than the 'ancient' TSMC 5nm the 4050 is built on. The 4050's cpu is also Arrow Lake, which is less efficient than Lunar Lake. Again, that skews the agenda.   You can already game on thin and light devices with discrete graphics. Laptops like Asus's G14 is only 3.3lb, but sports a 4060 which is like twice as fast as intel's new igpu. The dgpu turns itself off when on battery, and the integrated graphics takes over. Anything more intensive should be used with a charger plugged in.   In short, paying for a big igpu doesn't make much sense for anyone interested in performance. And gaming handhelds? Does anyone really care about those useless bricks for investment into integrated graphics? It's not like the cost of Panther Lake is going to be cheap when its laptops start at $1300. With that kind of money, you can get 2025 Asus Zephyrus G14 with a 5060 and blow its shit out the water. Or for those on a budget, 5050 laptops have been seen for $600.   Integrated graphics have come so far, pairing Intel's newest 18A Panther Lake with an RTX 4050 could still make a lot of sense.",AMD,2026-01-09 07:08:24,-2
Intel,nyjxwil,"Ehh not the mayority but *all* computers use iGPUs. The thing is, for the average Joe aka 95% of the market, there won't be a difference in the usage between an Intel Iris or RTX 5090 dGPU. And the efficiency would only come into place if they would game on battery (who does that anyways) or create/edit videos (again, virtually no one would do that without a dGPU). And even then, having your laptop drained in 2 hours 15 minutes instead of 2 hours is not ""gamechanging""   So it does not affect the efficiency at all during webbrowsing, watching videos, creating documents etc.  Thats also the reason why Intel has the non X 5,7,9 which will properly be by far the more demanded version as, again, the average Joe does not care the slightest about iGPU.  And apart from the iGPU, PTL is just a tiny step up from the Ultra 200 series...",AMD,2026-01-09 06:44:02,-3
Intel,nyqzsy8,"It's so weird that people treat their computer parts with a cultish following. Most of the people I know don't think about their cards at all and are just happy to play whatever games.   Super weird to be ""team red"" or ""team green"".   I can't imagine describing myself as a ""my computer chip manufacturer fan"". Cringe lmao.",AMD,2026-01-10 07:19:21,2
Intel,o0v4u9k,"How are you determining that AMD has a ""lead""?  In terms of Marketshare, Intel absolutely dominates x86, especially mobility (laptops), and if you walk into a Best Buy and ask 10 random customers ""Would you ever consider buying an AMD laptop?"", five of them would ask ""What's AMD?"", another three would say ""Isn't that a budget brand?"" (Their ""awareness"" of the zeitgeist of PC hardware is stuck in 2005), and maybe, and that's a big maybe, two out of ten would have a favorable opinion of AMD hardware so long as they've been paying attention for the last few years.  In terms of budget and expenditure, in 2025, Intel spent $17 billion in R&D versus AMD's $7.4 billion, outsizing it by a large amout. Total sales for 2025 are projected at approximately $53 billion for Intel and $33 billion for AMD.  I see this all the time and I've seen it for the past 8 years.... the only place that AMD has a ""lead"" is in the mind of PC hardware enthusiasts.... because it's not in the numbers as seen above (sales, expenditure, market share, etc), and it's not in the ""mindshare"" of your average consumer.  Hypothetically speaking, we could say that in a duopoly, as with Intel/AMD in x86, the BEST situation a consumer could hope for would be an even 50%/50% split in marketshare.... this would bring about the fiercest competition and would hopefully lower prices, increase innovation, etc. (and yet another reason why any fanboy who wants their favored company to dominate is LITERALLY cheering against their own interests as a consumer) Even with AMD's seriously impressive turnaround, their capture of marketshare, their ability to compete with two of the largest companies in the world while having considerably fewer resources, they still have a very, very, long way to go before approaching 50% of the x86 market across all segments.  In fact, to get any closer to that idealistic 50/50 split, AMD would have to continue winning and Intel losing for for many more years.  In other words, fears that AMD is ""becoming what Intel used to be"" and ""getting lazy"" are not an accurate reflection of the reality.",AMD,2026-01-21 14:59:24,1
Intel,nykyb9y,"What no AMD deserves much worse. How can a company fuck up so much and still survive. I would rather good competition rather than competing for the sake of competing. Marketing is bad, products are bad and they keep shooting themselves in the foot. Id rather Qualcomm or some other ARM company compete with X86. Its ARM or RISCV time to shine.",AMD,2026-01-09 12:00:44,-14
Intel,nylsh5z,Ever head a look at their profit margins?,AMD,2026-01-09 14:53:45,2
Intel,nykktsq,"Bullshit.  For starters, pricing is not a ""bad thing"".  Bad thing is, pick any piece from blue/filthy green's arsenals:  1) Strongarming OEMs 2) Strongarming Journalists 3) Proprietary standards",AMD,2026-01-09 10:08:42,14
Intel,nylsknw,Waiting? Theyâ€™ve been busy doing that for years now,AMD,2026-01-09 14:54:13,0
Intel,nyosqz5,"I WISH they were still around. I think their IP got sold to Via, who's not doing anything with it.  At this point, we might only get competition in the desktop x86 space if the government forces Intel and AMD to license x86 and x86-64 to some other chip designer (like Qualcomm or Mediatek) or Windows on Arm and Linux on Arm start getting wide application support, including office software and games.",AMD,2026-01-09 23:11:26,2
Intel,nykpg6r,"""Being good"" was never about price.",AMD,2026-01-09 10:49:30,1
Intel,nyjpruz,"They said it rivals a 4050 at 60W. The 4050 maxes out at 100W on paper but it's actually at 80W that it hits its peak performance. a 60W 4050 is about 85% of it's max performance.  So performance wise panther lake should be about on par with a full powered 3050Ti laptop.  That plus more advanced ray-tracing cores, it's running doom dark ages really well, AMD is still stuck at RDNA 3.5 and ray-traced games suck on the 890M.  I wish intel released a 24 Xe Core Variant with a 256-bit bus, double the cores and bandwidth. That would compete with the 5060/5070 laptop GPUs.",AMD,2026-01-09 05:40:22,16
Intel,nyl1feq,"Not synthetic benchmarks, we want to see benchmarks in games.",AMD,2026-01-09 12:22:47,7
Intel,nyld8l0,"You're either intentionally mis-stating this, or truthfully aren't aware, BUT, you can game sub 20w on any igpu. What actually matters is the performance scaling.  Also, just to clarify, while the 890m CAN game between 6-20w its performance is essentially identical to the 780m, z1e, etc. It only gets impressive at power draws 30+ (signed, a very happy 7840u handheld owner)  So, what we need to know is how well the new Panther Lake chips scale",AMD,2026-01-09 13:35:33,1
Intel,nyjhpru,Lunar lake came out after strix point. It was squarely a competitor to the 890m. Amd just officially released the cut down strix point as Z2E later.,AMD,2026-01-09 04:45:20,-5
Intel,nzinuqp,"No, not really. Making up a ""winner"" is stupid and nothing but fanboy behavior.  A faster dGPU is generally better because youâ€™re generally not power limited on a desktop. It doesnâ€™t really matter whether or not you have a 5060 or 5090 or whatever.  In mobile systems itâ€™s a huge difference.  There are entirely different power classes that donâ€™t compete with each other. A thin and light notebook with a 15W CPU cannot have a 100W CPU in it.  Panther lake aims towards unplugged performance which is the 45W power class and the same as Strix Point.  Strix Halo is a much higher power class that requires a laptop to be plugged in permanently for the full performance.  Itâ€™s for mobile desktops that are usually plugged in but can be mobile for some time with heavily degraded performance.  Itâ€™s an entirely different class of product and you wonâ€™t find (many) devices where Strix Halo and Strix Point/Panther Lake compete with each other.",AMD,2026-01-14 10:47:20,1
Intel,nyojqy6,"Yeah, i am gonna contend that either that is wrong, and is the 4xe version, or that intel basically lied on their benchmarks.  If none of those two things are true, Intel's new graphics architecture will absolutely dominate in the next round of discrete graphics GPUs.  with B580 intel needed \~80% more silicon to match nvidia performance. Now they need \~10-20% less die area. meaning their performance per transistor basically doubled gen/gen. which is unheard of. Even maxwell (largest architectural uplift in the history of GPUs in the last 10 years) did not achieve anything close to that. And it was a massive overhaul with huge changes.  So . . . there is something big i am missing . . . or intel is going to dominate in all things graphics going forward.",AMD,2026-01-09 22:26:03,1
Intel,nyjiq2t,"Let's not perpetuate this ""octa channel"" DDR 5 nonsense; it's a quad channel chip, and the Intel one is a dual channel",AMD,2026-01-09 04:51:54,14
Intel,nyk3wyi,"If you're going to be pedantic about it at least be correct please, they both use 16b LPDDR channels so the actual counts are 8 channels for Pantherlake and 16 for Strix Halo.  You're fighting a losing battle either way, the industry has long since settled on 64b as the standard channel width for marketing, independent of the actual number of address/command buses.",AMD,2026-01-09 07:35:06,1
Intel,nyki3az,Where did you find information of it being rdna4?,AMD,2026-01-09 09:43:42,7
Intel,nykhvc9,It's a custom implementation. IIRC it's not even RDNA4 but some Samsung derivative that probably has a ton of changes in silicon design to drive power down.  The short story is that AMD didn't bother to do low power optimizations in the architecture and silicon design. RDNA5 should change that.,AMD,2026-01-09 09:41:40,7
Intel,nynjaao,"Mark Cerny talked about RDNA5, AMD's leaked documents have talked about both UDNA and RDNA5",AMD,2026-01-09 19:36:51,7
Intel,nykyia5,This is nonsense RDNA5 does exist. I used to work there.,AMD,2026-01-09 12:02:08,-4
Intel,nyu9kyt,"Kepler's track record with AMD stuff isn't great, but everything is possible",AMD,2026-01-10 19:43:39,1
Intel,nyj1emc,">we're cooked guys /s  No sarcasm there lol  All tech companies are colluding right now, seeing as american business laws don't matter anymore",AMD,2026-01-09 03:10:44,13
Intel,nyj8sm2,"Yes, a frienemy.",AMD,2026-01-09 03:51:55,0
Intel,nyleo84,"Oh, you didn't say AMD was ""slacking off"" and letting intel ""catch up"". Figures.",AMD,2026-01-09 13:43:25,3
Intel,nylti7r,But thatâ€™s also more because of the increased demand from AI than anything else.,AMD,2026-01-09 14:58:39,1
Intel,nyjxwti,"It's 6C/6T and 2x Xe3, so don't expect a whole lot of performance. This is Intel's Mendocino.",AMD,2026-01-09 06:44:06,1
Intel,nylwgde,The slide specifically says 60w sustained for the 4050. I couldnâ€™t find your claimed 30w anywhere. If Iâ€™m wrong Iâ€™d be interested to see where you got the 30w number from because that would be shady by Intel,AMD,2026-01-09 15:12:36,2
Intel,nyp8y9d,"I posted elsewhere about the Framework Desktop with the Ryzen AI 385 and 32GB of RAM.  That's a pretty sensible config for a small gaming device, though it has 32 CUs instead of the full 40.  Still, that puts it ahead of anything in it's class other than the 395+.",AMD,2026-01-10 00:37:45,6
Intel,nym6sbh,"Yes, have you compared it to that of the competitors?  In general, pricing is not the issue to me.   Dirty play like blackmailing OEMs, proprietary standards and other misuse of the dominant market position is. (on top of being illegal)",AMD,2026-01-09 15:59:50,17
Intel,nytz6q1,"Let me guess, you're too young to recognize he's talking about before your time.",AMD,2026-01-10 18:53:34,8
Intel,nylxgon,"Yeah while AMD can't even announce their product AT A CONSUMER ELECTRONICS SHOW! and instead ONLY TALK about AI and government work......   AMD sucks just as bad as Nvidia just as bad as Intel, it's just a constant moving circle jerk as to whom is the least evil.",AMD,2026-01-09 15:17:16,11
Intel,nyr90di,How about this one: strongarming game developers into NOT including DLSS?,AMD,2026-01-10 08:43:32,0
Intel,nytewb4,"Nobody is interested in x86, otherwise Via would have been bought up.  We are in the age of the cloud and all software is custom made. Hence RISC+",AMD,2026-01-10 17:18:43,2
Intel,nyk5oi5,"Also Intel has XeSS which is very helpful for handhelds since they can't manage higher wattages, although not all games provide XeSS as an option",AMD,2026-01-09 07:50:42,7
Intel,nykdl7b,Is there really a 4050 or are you referring to the 4050m even when you don't add the m?,AMD,2026-01-09 09:01:51,2
Intel,nyobdr7,"they said 60W, but if you look at the laptop they used, it's 30W, probably 60W whole system",AMD,2026-01-09 21:46:41,1
Intel,nysouji,Nvidia paid intel 5B dollars ....Read whatever u can ... But it was to stop intel giving high power gpu to mainstream... .,AMD,2026-01-10 15:11:53,1
Intel,nylhxw8,"Perf + price + (to a lesser extent, but it still matters) power consumption together is what matter.   None of the 3 is decisive on its own.",AMD,2026-01-09 14:00:38,3
Intel,nylu1zs,"I think youâ€™re getting downvoted because it was sold by reviewers as a Z1E competitor as thatâ€™s what was available in regular devices.  Hx370 was only used in niche manufacturers, like GPD only when it launched.   Youâ€™re right though, it was supposed to be a competitor for hx370, but was held back by drivers and other things until mid to late 2025, which corresponded to Z2E (cut down hx370) release.  Fast forward to now, and it competes/beats both",AMD,2026-01-09 15:01:17,1
Intel,nyplsu3,"i mean you can legit put it over the intel provided slides and its pretty much a dead on match. the PCH is smaller in the presentation photos so they can make it look pretty, but the real chip is an exact match to that leak https://cdn.videocardz.com/1/2025/05/INTEL-PANTHER-LAKE-DEMO-1200x675.jpg  ~~this generation is seeing quite a significant leap forward in manufacturing technology (Gate All-Around/RibobnFET & Backside power delivery/PowerVia) these usually do result in big gains and that does make it a bit harder to compare to prior nodes. not to mention~~ **[GPU is TSMC N3E still quite a bit denser than N4 class tho]** its not the exact same uarch as B580, while still a derivative of battlemage, there does seem to be some (rather significant) improvements between xe2 and xe3 https://gamersnexus.net/gpus/intels-new-gpu-xe3-architecture-changes-handheld-gaming-cpus-xess3   but where a lot of the fps gain will be from is N-Frame and Pixel generation intel want to promote those numbers over native performance. AMD cant do with RDNA 3.5. id expect 8060s to be a much more powerful igpu but it is lacking what is essentially lossy compression for realtime graphics. that is a pretty big deal and i do think it will be what causes Strix halo to be a product that just ages poorly, costs too much for what it is really (308mm^2 io/igpu chiplet cant be cheap on 4nm) its really amds pipe cleaner for future packing tech.  people said they same when the zen chiplets were rumored to be the size they are. you save a lot of area not needing memory controller on that chip would be my guess. d2d bonding is very space efficient compared  for some perspective strix halo igpu block + media engine block is about 120mm^2 on N4P(143.7216MTr/mm^2) of the iod, the rest is i/o and the npu.  the 12 Xe3 chip is 55mm^2 on N3E(216MTr/mm^2) (so we could napkin approximate about 80mm^2 if it was on N4P)",AMD,2026-01-10 01:48:30,6
Intel,nyjjq7p,"Technicality is technicality, if the channel width is cut down by half but channel number is doubled then they still doubled the memory channel. People just need to know memory channels are not all equally wide just because thatâ€™s all they know being PCMR enthusiasts.",AMD,2026-01-09 04:58:28,-1
Intel,nykilu3,"https://www.thelec.kr/news/articleView.html?idxno=50232. Seems your out of the loop, you think amd can't scale rdna4 but samsung can?",AMD,2026-01-09 09:48:29,10
Intel,nykicmf,"Rdna4 is objectively faster than rdna3/rdna3.5 at the same clock, power' cu count and bandwidth  something 100% desirable for apus. Stop making excuses for amd and their bad decisions. Everything b your claiming samsung did for rdna4 to scale is something amd could have done aswell and has done as amd has made changes to rdna3(rdna3.5) for apu specifically the same can be done for rdna4.",AMD,2026-01-09 09:46:05,3
Intel,nyui8sr,Sure. weâ€™ll see,AMD,2026-01-10 20:27:04,1
Intel,nyj73k6,Have the business laws mattered since the dawn of post-dialup internet?  I don't think they have. Where's our fucking bell-style breakup? 41 years ago was the last *real* monopoly breakup... and they let it come right back.  EU does half measures and they don't come to the rest of the world. It's a travesty that we don't have nationwide GDPR or force allow sideloading on ios.,AMD,2026-01-09 03:42:31,4
Intel,nyjduwz,An enerend of sorts,AMD,2026-01-09 04:21:49,0
Intel,nylwn40,"Sure. But they're still not stagnating. Since December 2023, when Mi300X and Mi300A were released, they released Mi325x, Mi350x, Mi355x and soon, Mi400x.    The latest gen is running HBM3e and 3nm CDNA4. Those are some immensely advanced products.    On the Epyc side, they've got the 9965, a 192 core 384 thread monster that Intel can't even attempt to compete with.    Intel hasn't advanced in server stuff at the time either. Their top SKU was 18c in Haswell, and that hasn't moved until like Cooper lake? So from 2014 until 2020, they haven't moved an inch in server space either.    AMD has gone from 32 cores first gen in 2017 to 6 times that in 2024.    It's honestly not even comparable. AMD advanced more every generation than Intel did from Haswell to Kaby lake at the very least.",AMD,2026-01-09 15:13:28,2
Intel,nylaft6,"The modern Atom is fine by me, the N100 had more performance than a 6500t so this one should have more than enough compute for many different use cases whilst retaining low load efficiency. This product could potentially obliterate even the newest and best SBCs for home lab use cases, even regarding efficiency.",AMD,2026-01-09 13:19:41,1
Intel,nymqm53,"[Intel Performance Index](https://edc.intel.com/content/www/us/en/products/performance/benchmarks/intel-core-ultra-processors-series-3_1/) Search 4050. [Dell 14 Premium is this laptop, with a TGP of 30W](https://www.dell.com/en-us/shop/dell-laptops/dell-14-premium-laptop/spd/dell-da14250-laptop/useda14250hcto01#customization-anchor)  [In PCWorld's test, they got 48 fps for Cyberpunk](https://www.youtube.com/watch?v=NdLYuQQPo5c). My 4060 gets 73 fps using 60W using high settings and 2880x1800 DLSS instead of XeSS. That's a game where Intel gpus performs well above average. A 4060 optimus laptop uses around 3.5W an hour at idle without the screen turned on. With the screen and igpu powering it, it's about 8W. Having discrete graphics in modern systems doesn't really impact battery life anymore.   So yeah, Intel was intentionally being misleading, hoping people wouldn't actually bother to check their figures. Panther Lake's massive igpu still doesn't make sense for anyone who cares about performance. Maybe a little bit for battery life, if it's more efficient to drive high resolution displays, despite its large size being wasteful. Most igpus go into office pcs. In terms of gamers, Steam's hardware survey suggest that desktops and gaming laptops with dgpu are the biggest share.",AMD,2026-01-09 17:28:27,2
Intel,nym76rq,"Yes, I did. AMD could hold prices low, they choose not to.",AMD,2026-01-09 16:01:38,-4
Intel,nzqu8wh,What AMD did a decade ago has no bearing on how they operate today. Corporations have to alter their behavior quarterly in order to maximize their legal obligation to constantly increase shareholder profits.,AMD,2026-01-15 15:45:18,0
Intel,nymy6xm,9850X3D lol gottem,AMD,2026-01-09 18:02:35,7
Intel,nyp6syu,you didn't just try to suggest that CES is for....consumers..... did you.... seriously?,AMD,2026-01-10 00:26:18,0
Intel,nyrhvdt,"Ahaha, lovely lie. And even if true, how would that change a lit of ""bad things"" lol.",AMD,2026-01-10 10:06:50,1
Intel,nytjk6m,"Seems like things are going that way. I guess all we can do is wait and see if the Arm takeover gets so complete that Intel and AMD have to join in, and then suddenly have to compete with Qualcomm and Mediatek.  If that ever happens, hopefully we'll see more competition.",AMD,2026-01-10 17:41:05,1
Intel,nykh31e,linux and optiscaler is the way,AMD,2026-01-09 09:34:24,2
Intel,nylckas,"4050m, although it really wouldn't matter either was as the 4060 and 4060m are functionally identical in regards to performance (+5-7% for desktop) so if there were a full size 4050 we'd expect it to be the same or even less of a difference",AMD,2026-01-09 13:31:44,1
Intel,nyljjoq,"That's all fair, I'm looking at it from a handheld perspective. Performance at power draws that are actually feasible in handhelds has been stagnant since handhelds have really gotten popular. That is, it has if you want more than an hour of battery life",AMD,2026-01-09 14:08:59,0
Intel,nylvmjs,"Actually, the supposed driver issue was only a MSI Claw specific issue and not a general lunar lake issue.  https://www.notebookcheck.net/Intel-Lunar-Lake-iGPU-analysis-Arc-Graphics-140V-is-faster-and-more-efficient-than-Radeon-890M.894167.0.html  Here's a review from September 2024 using a LNL Zenbook S14 with 28w TDP. It had no issues generally outperforming the HX370 in the Zenbook S16. As usual the PCMR-esque dominated crowd on here paid no attention to laptops (which is the real life volume) and only looked at some handheld (which is a niche irl) so they thought that supposed ""lunar lake issue"" was widespread.",AMD,2026-01-09 15:08:44,3
Intel,nyjl1zy,"Each DIMM of DDR5 has 64 bits total of bus width, same as DDR4, 3, 2, and 1. And I do understand what you're talking about (not to mention that Strix Halo can't even take SODIMMs), but nobody else talks like that. When you call it an ""octa-channel"" chip, what people read is that it has as much bandwidth as a Threadripper Pro, because that is how [AMD is marketing those chips themselves](https://www.amd.com/en/products/processors/workstations/ryzen-threadripper.html).",AMD,2026-01-09 05:07:19,8
Intel,nyko7q2,"Yeah, and AMD says Strix Halo has four channels in their customer facing spec as well, because they have 128b and 256b buses respectively. They use the 64b channel convention as it is a customer facing spec, that doesn't meant they actually have that many channels in hardware.   The Pantherlake datasheet isn't public yet, but you can see plainly in the [actual spec sheet](https://edc.intel.com/content/www/us/en/design/products-and-solutions/processors-and-chipsets/core-ultra-200h-and-200u-series-processors-datasheet-volume-1-of-2/memory-controller-mc/) for Arrowlake H that it supports 8 channels of LPDDR5X (additional [spec](https://edc.intel.com/content/www/us/en/design/products-and-solutions/processors-and-chipsets/core-ultra-200h-and-200u-series-processors-datasheet-volume-1-of-2/supported-memory-modules-and-devices/) for channel width). Pantherlake will be the same.   The equivalent AMD doc is not available for Strix Halo but you can see the 16x16b spec quoted by Chips and Cheese [here](https://chipsandcheese.com/p/evaluating-the-infinity-cache-in#:~:text=Strix%20Halo%20has%2016%20memory%20controllers%20and%20CS%20instances%2C%20each%20handling%20a%2016%2Dbit%20LPDDR5X%20channel).  You cannot gang these channels into a dual channel mode, that is not how modern memory works, and there is no allowance in the LPDDR5 spec for 64b channels. The 16b channels have separate command/address buses and burst for a sufficient length (32n) to fill a cache line with each access.  To be clear I think standardising on 64b ""channels"" for marketing specifications is a good thing, it allows quick mental calculation of memory bandwidth without having to get into the nitty gritty. But if you're going to be pedantic and use the actual channel count, it's best to be correct.",AMD,2026-01-09 10:38:46,5
Intel,nykiuny,"Im not original guy you responded to I just wanted to know, becouse I couldnt find it on google. thx",AMD,2026-01-09 09:50:43,9
Intel,nykjg5k,"I'm not making excuses just explaining the rationale, which I don't agree with BTW.       Yes I know AMD are some lazy mofos. RDNA 3.5 till 2029 for iGPU is cheapo strategy as usual.",AMD,2026-01-09 09:56:09,7
Intel,nyli3fq,https://en.wikipedia.org/wiki/Hyperbole,AMD,2026-01-09 14:01:27,3
Intel,nym2zdk,">It's honestly not even comparable. AMD advanced more every generation than Intel did from Haswell to Kaby lake at the very least.  >Intel hasn't advanced in server stuff at the time either. Their top SKU was 18c in Haswell, and that hasn't moved until like Cooper lake? So from 2014 until 2020, they haven't moved an inch in server space either.  Since we're talking about the server side now, Haswell-EP went from 18 cores maximum to 22 core Broadwell-EP to 28 cores on Skylake-SP. Cascade Lake-AP (rare bespoke sku) went up to 56 cores per socket. ""Haven't moved an inch"" is inaccurate.",AMD,2026-01-09 15:42:41,3
Intel,nym7k8z,"In CPUs theyâ€™re losing market share to arm, the Datacenter GPUs are mostly bought by companies who canâ€™t afford NVIDIA",AMD,2026-01-09 16:03:20,0
Intel,nymvxz5,"I think you were looking at the old core ultra series 1 testing not the current CES testing. For their claim they used the following settings:   Intel B390: Processor: Intel Core Ultra X9 388H (Panther Lake) PL1=45W; tested in Intel reference platform; Memory: 32GB LPDDR5 9600; Storage: Samsung PM9A1 512GB; Display Resolution: 2880x1800; OS: Windows 11 26200.6725; Graphics Driver: Intel Arc Graphics Pre-Production driver; NPU Driver: Pre-Production driver; BIOS: Pre-Production BIOS; Power Plan set to Balanced, Power Mode set to ""Best Performance"".  NVIDIA RTX 4050: Processor: Intel Core Ultra 7 255H (Arrow Lake); tested in Dell 14 Premium with Nvidia GeForce RTX 4050; Memory: 32GB LPDDR5 8400; Storage: Samsung 9100 Pro 1 TB; Display Resolution: 2k IPS; OS: Windows 11 26200.7171; Graphics Driver(s): dGPU: 32.0.15.8180 (GeForce 581.80) & iGPU: 32.0.101.8250; NPU Driver: 32.0.100.4404; BIOS: v1.4.0; Power Plan set to Balanced, Power Mode set to ""Best Performance""; Dell Optimized = Ultra Performance. Battery Size: 68Whr",AMD,2026-01-09 17:52:34,2
Intel,nyma4vw,"Why would AMD ""hold prices low""?  Gross margins are below 50% (48, as in 2022), while NV has it at 70%.  We know they are worse in PC/GPU market and better in datacenter.",AMD,2026-01-09 16:14:51,11
Intel,nypf46n,What did you think the acronym CES stands for?,AMD,2026-01-10 01:11:10,8
Intel,nyrmvf6,It would further validate what HisDivineOrder said which is that AMD is just another corporation.  Which they are.,AMD,2026-01-10 10:52:27,0
Intel,nyl953n,"Really wish Optiscaler had a better installer, something akin to Reshade. The whole manual process for each game makes it annoying to use.",AMD,2026-01-09 13:12:04,3
Intel,nyks16k,Intel still takes a heavy penalty on Linux in graphics vs. AMD. Hopefully that improves as well.,AMD,2026-01-09 11:11:24,6
Intel,nyjobcx,Well thereâ€™s more than one type of memory ðŸ¤·â€â™‚ï¸ PCMR crowd just defaults to DDR DIMMs but the world of mobile is mostly LPDDR from phones tablets to handhelds and most small laptops,AMD,2026-01-09 05:29:47,3
Intel,nynuctv,"Nope, I was looking right at the current testing. I do have to make a correction though: Panther Lake's cpu is built on Intel 18A, and the gpu is built on TSMC N3E  Let's summarize. In a head to head battle, Intel claims the 45W Panther Lake Core 388H with its ""massive graphics"" is 10% faster than a 30W 4050 paired with a 30W Arrow Lake 255H. Panther Lake's cpu is built on the most advanced silicon process node 18A, designed to compete against TSMC's N2 (2nm) which is set to release in products in the second half of 2026. Panther Lake's gpu is built on TSMC N3E, a significantly more efficient N3. The 4050 is built on a custom 2020 TSMC 5nm variant, and Arrow Lake is built on TSMC N3+6nm. Arrow Lake is designed for specifically for high power use vs the low Lunar Lake and Panther Lake.   The future of integrated graphics is truly bright. I can see it being exactly where it is now. Vital for battery life in office laptops and actual gaming laptops with discrete graphics. Big igpus? Mostly irrelevant and a waste of money.",AMD,2026-01-09 20:27:44,2
Intel,nyqxy0s,"Yes acronym has ""consumers"" in the name, but it's not directed or intended for consumers, it's intended for the big industry, the maker's manufacturers, the ones creating services, and the subtle parts of the distributors and such, it was and has NEVER been intended for the end users, the broad consumers.  Maybe bloody well look up what CES is and what it's for before asking a silly question.",AMD,2026-01-10 07:03:08,3
Intel,nys7tqn,"No, it would not. There is a difference between a shoplifter and a serial killer, even though both are criminals.  Filthy Green plays in a league of pieces of shit of its own.",AMD,2026-01-10 13:35:08,0
Intel,nynpovt,True but you put command once in your game and you forget about it,AMD,2026-01-09 20:06:13,0
Intel,nysa4r8,"This is so delusional  You're talking about AMD that made Int8 version of FSR4, which is THE hardest part, and then keeps it away from users to sell more RDNA4 cards.",AMD,2026-01-10 13:48:55,-1
Intel,nyov5va,"Missing the point. It's about accessibility and ease of use not how often you need to do it. If a tool to bring similar functionality as Nvidia isn't at a similar level of accessible and easy to use as the manufacturer apps, then it's relegated to enthusiasts only.   Reshade is one of the most popular modding tools for post-processing shaders because it's so easy to install, use, and manage for multiple games on the same system.",AMD,2026-01-09 23:24:12,1
Intel,nytis7z,"AMD had no reasons for such lock-in, it makes sense only for companies dominating the market, to push people to refresh.  I have not seen palatable proof that FSR4 could be ""easily backported"" but isn't.",AMD,2026-01-10 17:37:25,1
Intel,nx5elyy,Already seen card prices here in Canada jump 10-15% since last night. It's insane.   Now is not the time to build.,AMD,2026-01-01 22:05:04,4
Intel,nxlg7un,"Hi, I was wondering if there's any reason to worry if my 7800X3D sometimes spikes for 1-2 seconds to 100Â°C while gaming and then goes back to the usual temp. I have noticed the highest temp recorded by HWiNFO at one point was 104Â°, though I never noticed it on the OSD while in a game and never noticed a performance drop. Is there a problem with the cooling or something that could damage my CPU or is it just a sensor bug/issue?",AMD,2026-01-04 08:46:39,3
Intel,nx583ui,"If you're looking to do a PC build...just don't.  If you NEED to do one, do it right now. It's not getting any cheaper this year.",AMD,2026-01-01 21:31:55,4
Intel,nx5jal3,"Here's a dumb question that would be absolutely ridiculed if I dared to create a whole thread around it.  Is there any truth to my hypothesis that Play Station PC ports are likely to be relatively well-optimized for AMD GPUs, given that the Play Station 5 itself is indeed some variant of RDNA? I recently got a 9070xt and have been overall very impressed, but its achilles heel seems to be ray tracing. This isn't exactly surprising to me, as I researched my GPU options to death before buying one, and the general consensus is that Nvidia is stronger in the ray tracing department. But if I were to boot up, say, Ratchet and Clank Rift Apart, a game that supports ray tracing at 60 fps on the base Play Station 5, could I expect it to perform better than a similarly demanding game that wasn't particularly optimized for AMD hardware?  It's largely hypothetical question, as I already own the GPU, am satisfied with the GPU, and of course did my due diligence before buying the GPU so I would know exactly what to expect. But I just haven't really heard much discussion of what, if any, overlap we get optimization-wise for games that were optimized first and foremost for the AMD-based Play Station 5.",AMD,2026-01-01 22:29:53,2
Intel,nx6hz7y,"Thinking about doing a platform upgrade from a 5800X3D to a 9800X3D, how much of an improvement will I see with my RX 7900 XTX?   Obviously I know that DDR5 is priced high now but I think it's only going to get worse if I wait. I live near a Microcenter as well so I'll be doing one of their combos with the CPU, Mobo, and RAM.",AMD,2026-01-02 01:47:36,2
Intel,nx8k7tp,"Early 2025 I was thinking about upgrading to AM5 but there's no way that's happening, I only got a sapphire nitro+ 9060 XT 16gb on Black Friday.  Current setup is Ryzen 3600 on Gigabyte b450 Elite v1, 16gb ram 3200, 9060 XT. My question is, would an upgrade to 5800X make sense? It costs 165 euros where I am and it's the only upgrade I can make that I see. I play games like Helldivers 2, BF6 nowadays. Also I play on 1080p.   Thank you.",AMD,2026-01-02 11:26:25,2
Intel,nxim4kb,"Hi all  I'm about to give my water-cooled 6950xt to my brother as I picked up a 9070xt.  As I've got to.out the og heatsink back on I'd like to.replace the pads ofc. Does anyone know the sizes needed.  I'd also throw a kryonaut grizzly bear pad on the GPU, would this be a 1mm pad?  I'd like to get this right as he's on a 5700XT so it will be a good upgrade for him.  Many thanks.",AMD,2026-01-03 22:08:10,2
Intel,nzdyg7l,"Is PowerColor a good brand of GPUs?  Iâ€™m planning on upgrading my gpu from my almost 7 year old nvidia rtx 2060 to an PowerColor rx 7800xt Red Devil, and am bit worried if theyâ€™re a reputable brand.   Was holding off the upgrade due to not wanting to chase percentages, and now that I fully embraced Linux (Fedora 43 KDE) I wanted to get something that has better compatibility with the OS as I did encounter some issues due to NVidia drivers.  Edit: forgot to mention that I have a compatible system with 600w power supply",AMD,2026-01-13 17:45:26,2
Intel,nxaeni9,"I could use some suggestions on upgrading a desktop box my son built for me in 2013. It was used for my graphic arts business (Adobe Suite) and has performed admirably for the last 12 years. It's running Windows 10 and most of the patches will not install. It can't be upgraded to Windows 11, and while I realize that every MS upgrade I ever did in the past caused major mayhem, I probably should go ahead and do it before it quits running altogether.  Below is a list of what he ordered and put in it.   What should I order that will swap out and last me another 5-10 years? I just used this for work and internet. No games.  â€¢ MB Gigabyte|GA-970A-UD3P AM3+R   â€¢ VGA Sapphire|100365BF4L R9 270 2GD5   â€¢ PSU Roswell|RX850-S-B 850W RT (has been replaced)   â€¢ CPU AMD|8-Core FX-8350 4.0G 8M R   â€¢ SSD 256G|Samsung MZ-7PD256BW R   â€¢ MEM 8Gx2|Corsair CMZ16GX3M2A1600C9  It also has a DVD RW Drive and I added a 12TB WD Hard drive   I'm sure most of you folks can look at that list and quickly see what I need to change. I'm thinking CPU, Motherboard and RAM? Thanks for your expertise.",AMD,2026-01-02 17:44:40,1
Intel,nxc2q2i,"I typically wouldn't do a pre-built but considering I can get my hands on this right now if I wanted and the prices of things going up, would this be worth grabbing?  $1,649.99 AMD Ryzen 7 9800X3D, AMD Radeon RX 9070XT 16GB, 32GB DDR5 RGB,2TB NVMe SSD  [https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5](https://www.bestbuy.com/product/ibuypower-slate-gaming-desktop-pc-amd-ryzen-7-9800x3d-amd-radeon-rx-9070xt-16gb-32gb-ddr5-rgb2tb-nvme-ssd-black/J3R75JYGZ5)  Thank for the input in advance!",AMD,2026-01-02 22:33:02,1
Intel,nxeipp1,"Quick sanity check: Am I right to say that there are no new production of X570 boards at the moment, and therefore I should just sit tight with my Asus X470 Stix-F board until the RAMmegeddon eases before moving up to AM5/AM6?",AMD,2026-01-03 07:51:43,1
Intel,nxfttws,"Bonjour, j'ai un vieux pc qui a malheureusement commencÃ© Ã  rendre l'Ã¢me fin 2025 et je dois donc me dÃ©pÃªcher d'en racheter un avant que les prix deviennent exorbitants. Je recherche un Pc fixe (si possible prÃ©montÃ© Ã©tant donnÃ© que je suis peu douÃ© lÃ  dessus) pouvant faire tourner les jeux d'aujourd'hui (E33, Dlc Baldur's Gate etc...) et si possible ceux de demain.   J'ai un budget correct (1200 euros max) et je risque pas de faire grand chose Ã  part jouer dessus.    Merci d'avance pour vos avis !",AMD,2026-01-03 14:01:31,1
Intel,nxqoxma,"I just installed my new RX 9070 XT today, replacing my RTX 3060 Ti, and after getting the new drivers set up and the old ones gotten rid of, i'm having an issue of intermittent audio crackling. Is there a know simple fix for this?",AMD,2026-01-05 01:59:57,1
Intel,nxu74dg,what are the best settings for my rx 9070 xt steel legend on adrenalin? should I prioritize lower temps or higher performance? and will the performance between settings be negligible playing in 3440x1440p? I'm currently running the default option under Performance>Tuning,AMD,2026-01-05 16:15:52,1
Intel,nxurns3,"When I'm playing a game, my screen suddenly goes black and I have no way to shut down my PC; I have to restart the power supply. Does anyone have any solutions, please?",AMD,2026-01-05 17:50:56,1
Intel,nxv130z,"Are there plans for chipset refresh for Zen 6 or 7 or there will be only firmware and BIOS  updates  for existing ones? I heard Zen 6 should have better memory controller , with higher 1:1 RAM speed support (perhaps 8000MT/s + ) etc. , but of course still same AM5 socket.",AMD,2026-01-05 18:33:10,1
Intel,nxv5q21,"Hey guys.  Whats the best way to get a smooth 60fps lock on a 120hz display?  I use MSI Afterburner and the adrenaline app, neither felt as smooth as native 60hz.  On nvidia i used the half vsync feature and that worked for me but AMD doesn't have an equivalent option.",AMD,2026-01-05 18:53:49,1
Intel,nxzp3f0,"weird issue as off 2 days ago: RX 7700 XT with 25.12.1 driver on W10 - when powering on the system, the secondary screen (HDMI) is not receiving any signal until the HDMI cable is unplugged and plugged back in. No recent updates installed.",AMD,2026-01-06 11:41:16,1
Intel,ny1f10g,"7800x3d SUSPICIOUSLY LOW TEMPERATURES   I just finished building my computer and tested it in two games, at 2k resolution and the highest settings: The Last of Us Part Two and Battlefield 6. My 7800x3d is showing temperatures below 50 degrees Celsius, even though I'd read on forums that it can get hot. I checked it on the cooling display, HWMonitor and in MSI Afterburner. Is it possible for air cooling to be this efficient, or do I need to configure something in the BIOS to get the processor to run at full performance? Bf6 runs with 180fps and TLOU have around 100fps.  I have rtx 5070 and 32gb ddr5. Cooler: Phantom spirit Evo vision with stock paste.",AMD,2026-01-06 17:15:25,1
Intel,ny4j9i2,"New to AMD and plan to keep the same cpu cooler, I have a NH-D15. I bought this cooler back in 2021-22. Would I need a new mounting bracket to accommodate this change?   I have upgraded to 7 9800X3D, Mobo is a Tuf gaming B650E-E if this information is needed. Any help appreciated!",AMD,2026-01-07 02:15:55,1
Intel,ny70dtx,"Hi there, hope everyone is doing fine and started new year on a good note :)      Recently became the proud owner of a 9070 xt 16GB Ram - Hellhound specifically (https://www.powercolor.com/product-detail214.htm)  I just want to double - check that my AMD adrenaline edition settings are correct - What do I need selected for maximum gfx quality?     Thank you !",AMD,2026-01-07 13:16:10,1
Intel,nyc3nmx,"looking for help understanding core parking on the 9950X3D, does it outright disable the other cores while gaming? or do other applications running use the non X3D cores?",AMD,2026-01-08 03:51:56,1
Intel,nyci0og,"I'm building my first ever AMD PC, and my second ever PC (My old one had a 2080 super, 10th gen i9 and sadly died a few months ago). I did not know that you were supposed to buy certain ram depending on what CPU/motherboard you used. I'm either going to be buying a 9850X3D or a 9800X3D, and the motherboard I have currently is the MSI MAG B850 TOMAHAWK MAX WIFI ATX AM5 Motherboard. My ram is the G.Skill Trident Z5 RGB 32 GB (2 x 16 GB) DDR5-7200 CL34 Memory. Should I return the ram and get the AMD EXPO equivalent? Will I lose performance if I keep it? Will I lose stability if I keep it? Will it even work properly?  Some extra info:   I can afford to return it and buy the equivalent for an extra $100 or so. My GPU is the Sapphire Pulse 9070 XT, I'll be gaming at 1440p, my I have an WD\_BLACK SN8100 2 TB SSD, and a Corsair RM850x (2024) 850 W Fully Modular ATX Power Supply.",AMD,2026-01-08 05:22:26,1
Intel,nygdcim,"Is a reasonable upgrade for my system possible?    Hello there,  i would like to know if there is any reasonable upgrade possible on my AM4 System...   I would like to play Call of Duty Warzone on a 1080p Monitor with 180 fps but ALSO use ~ 500 tabs at the same time.  Currently my System runs the 500 tabs but only gets ~ 120 fps in cod.   Since AM5 is very expensive currently due to RAM prices, i do not see any reasonable chances for a Upgrade and therefore am looking for advice :)    My current System:  AMD Ryzen 9 5900X - 12x3.7GHz  => OVERLOCKED at 4.7GHz with 1.304v (undervolted for that speed -> Temps below 80Â°C)   32GB DDR4 3600MHz Team Group T-Force Vulcan Z - DDR4 (2x16GB) => UPGRADED to 64GB (4x16GB)   AMD Radeon RX 7900 GRE 16GB => slight OC possible BUT Temps tend to go above 80Â°C, at higher OC even above 90Â°C... (possibly i could add more cooling to the Tower?)   * Systemtreff Gaming Mid Tower AirForce GT1   * Systemtreff ITS-Raven - Prozessor - LuftkÃ¼hler  * Gigabyte B550 Gaming X V2 - AM4  * 850W MSI MAG A850GL PCIE5 80+ Gold  => UPGRADE MSI MPG A850G  * 1TB M.2 SSD (NVMe) MSI Spatium M450 PCIe 4.0  * 1TB M.2 SSD (NVMe) MSI Spatium M450 PCIe 4.0   In the future i might want to play COD2026, which could receive a huge engine upgrade... and i also will run ~500 tabs at the same time.",AMD,2026-01-08 19:24:33,1
Intel,nyj771y,"Hi there, hope everyone is doing fine and started new year on a good note :)  Recently became the proud owner of a 9070 xt 16GB Ram - Hellhound specifically ([https://www.powercolor.com/product-detail214.htm](https://www.powercolor.com/product-detail214.htm))  I just want to double - check that my AMD adrenaline edition settings are correct - What do I need selected for maximum gfx quality?  Thank you !",AMD,2026-01-09 03:43:02,1
Intel,nykrihq,"im running a 5600x currently, what would be the best cpu to upgrade to on am4 for gaming except for the 5800x3d?",AMD,2026-01-09 11:07:02,1
Intel,nyld64a,"What does the 18th byte do?  On my system it changes on a daily basis. Display port radeon software rx 580  Also Current Link Settings - 2.7 Gbps x 4. Seems I have a bandwith issue, should be more as i have a DP 1.2 standard gpu port cable etc  ""BestViewOption""=hex:00,00,00,00,00,00,00,00,03,00,00,00,01,00,00,00,08,89,ff,ff,00,00,00,00,00,00,00,00  ""BestViewOption""=hex:00,00,00,00,00,00,00,00,03,00,00,00,01,00,00,00,08,80,ff,ff,00,00,00,00,00,00,00,00",AMD,2026-01-09 13:35:10,1
Intel,nylqaba,"AM4 CPU Compatibility question.     I currently have an HP system with a Ryzen 2700.   I'm thinking about picking up an AM4 motherboard for a ""new"" build. To be precise, an ASRock B550M-ITX/AC.  I can get a Ryzen 2600 on the cheap and swap out the 2700 into the new mobo. That way I can hand the HP system to my wife as an upgrade.  But. According to the ASRock lists. These older CPU's are not compatible. Just the 3000 series and up.  My question is, what makes these older CPU's incompatible on the same socket? I see some Chinese boards that support the 1000 to 5000 series Ryzens.  Right now, I can get the ASRock new for a decent price. Given the DDR5 debacle, I still have enough DDR4 sticks laying around that makes sticking to AM4 an easy , affordable choice.",AMD,2026-01-09 14:43:10,1
Intel,nymut0x,"HELP - GPU not detected after Ubuntu boot repair and CSM toggle  SYSTEM SPECS CPU: AMD Ryzen 5 9600X GPU: AMD Radeon RX 9070 XT Mobo: AsRock B650M PG Riptide Main Storage: Crucial T500 M.2 NVMe (Windows 11) Secondary Storage: ADATA SATA SSD (Old Ubuntu install)  THE ISSUE My PC was working fine until I tried booting into an old Ubuntu installation on my secondary ADATA SATA SSD. Now my RX 9070 XT is not detected at all in BIOS or Windows Device Manager, and I only get display output from the motherboard.  WHAT HAPPENED To see the old SATA SSD in the boot menu I had to enable CSM in the BIOS. Booting into that drive resulted in a black screen. I then used a Live USB to run the boot-repair utility with the recommended repair settings. This seems to have installed the GRUB partition onto my Crucial T500 M.2 drive instead of the SATA drive. Now I can boot into both OSs, but the GPU is completely invisible to the system.  CURRENT STATUS Windows Device Manager only shows the Integrated Graphics. When I try to install AMD drivers, the installer fails because it cannot detect the GPU. I was briefly able to get graphics output from my GPU by unplugging the PC, flipping the PSU switch to off, and holding the power button to empty the capacitors. I then booted it up with the HDMI cable plugged into my GPU and I saw the asrock logo but it was stuck there for 2 minutes and I impatiently turned it off. I'm also considering trying this again and letting it run it's course  PLAN AND QUESTIONS I am planning to disconnect the SATA SSD and try to wipe the Ubuntu boot entries from the M.2 drive to see if the GPU reappears. Has anyone experienced a Linux bootloader repair or CSM toggle hiding a GPU from the BIOS? Specifically, could the Crucial T500 and ADATA drive conflict be causing PCIe initialization issues after the CSM change? Should I try clearing the CMOS first? Any help would be greatly appreciated. Thanks!",AMD,2026-01-09 17:47:28,1
Intel,nyp2whf,"I can buy a Ryzen 7 5700 (without X, the one that is 5700g without a built-in graphics chip) for 120 euros (\~$140) or 5800x for 188 euros (\~$218). Is it worth the extra? My GPU has 16GB, so PCIe shouldn't have too much of an impact.",AMD,2026-01-10 00:05:44,1
Intel,nz62itx,"So I have a question guess I want a 2nd opinion, with the new information of AMD potentially bringing back some old AM4 chips.   I recently bought a ryzen 5800x for my little brother to upgrade his 3600 thing is my brother is currently at the military until May so his upgrade isn't super urgent. Should I return it and potentially hold out on the chance that AMD brings back the 5800x3d?!",AMD,2026-01-12 14:39:27,1
Intel,nzq963u,# OpenGL to DX12 Wrapper on Windows? Do any of you guys know a program that wraps OpenGL API to DX11 or 12? Something like Dgvoodoo2.,AMD,2026-01-15 14:01:35,1
Intel,nzrzrx7,I have seen alot of posts about changing the Curve Optimizer to All cores -30 or -20 and it gives you same the performance and lower temps?  is -20 or -30 better?  and how lower does the temp go?,AMD,2026-01-15 18:51:03,1
Intel,nzssz2e,"Hello guys, hope u're doing well today.   I currently have a perfectly working system, Ryzen 7 9800x3d, X870E aorus pro ice, and 2x16gb trident z5 royal neo 8000mhz cl38.   got a pair of corsair vengeance rgb cudimms, 2x24gb 8000mt cl38, as an upgrade.   after I got them I found about the cudimms and the ""incompatibility"" with ryzen which makes them run as a normal udimms in ""bypass"" mode.   searched a lot on the internet about performance or benchmarks, couldn't find any.   since I'm thinking of returning them if they don't work, and I can't if I open them, I'm looking for someone that has tried cudimms on 9800x3d and could share his experience, that would be awesome and I'll be really thankful.",AMD,2026-01-15 21:05:23,1
Intel,o007h0r,"Question about dealing with overheating  Playing a game I'm getting 95ÂºC on my GPU and the performance is tanking. I know it's overheating. However, when I stop playing, the temperatures go down, I try playing again an hour later, now I'm getting 75ÂºC and the performance is just as bad as before. Has it not really cooled down somewhere inside?   It's a 5 year old 6900XT and I'm trying to see if I can reapply the thermal past, but until there I'm trying to see if there's anything else I can do to keep it from getting to the point of overheating, and I don't know what to do to really reset the test except waiting for the next day before I try again.",AMD,2026-01-16 22:23:58,1
Intel,o03mdoz,"Hi all,  Trying to use the curve optimiser for my 7800x3d so going Advanced CPU configuration>AMD Overclocking>Precision Boost Overdrive then setting it to advanced to use the curve optimiser. However every time I go back into the BIOS it's changed from advanced to enabled.  Does this mean it's not saving my settings or is it just a visual error - wondered if anyone else has had this?  The other reason I ask is ran three cpu cinebench tests with the curve optimiser set to -15, -20 and -25 and all returned results within 0.1% of each other and same temps too so doesn't seem to be working as I expected.  I'm pretty new to this so any help is appreciated thanks :-)",AMD,2026-01-17 13:18:40,1
Intel,o073hc7,"I have been looking at the user manuals for motherboards like H13DSG-OM which support eight MI300X Instinct GPUs.  *None* of them explain whether the midplane (like BPN-GPU-GP801) **must** be fully populated with eight GPUs, or if it will boot and operate with fewer GPUs installed (like four, two, or only one).  Does anyone know if any of the OAM motherboards (of any manufacturer, not just Supermicro) for MI300X support such partial installs?",AMD,2026-01-17 23:41:57,1
Intel,o075i10,"I have a 9800x3d , is it worth having a negative curve of -30 and +200 on core clock for games?   I tried cpu intensive games and I didnt notice really any fps difference at all",AMD,2026-01-17 23:52:44,1
Intel,o08w39x,"PLEASE HELP. I just finished a build for my friend and I'm getting frequent no display out on power on. This is only fixed by a restart but I'm PASSING all my POST lights. The only thing I can think of would be a bios update but I'm not sure, these are all brand new parts.",AMD,2026-01-18 05:58:32,1
Intel,o0dmvvr,"AMD Ryzen 9 9950x  ASUS Strix X670E  my current clock speed is over 5500 MHz (showing in NZXT cam, but Task Manager shows slightly lower), default is 4300MHz. Power hovers around 40-60W while browsing.  i have never touched anything regarding overclocking so dont know whats happening, didnt realize how long this has been happening.  I just tried disabling PBO in bios but that didnt change anything.  Any suggestions? Finding conflicting information online",AMD,2026-01-18 23:18:01,1
Intel,o0l1emh,"# Is the 6000 series cards stuck with 25.3.1 forever now for stable VR?    I keep trying the updated software for my 6800XT, and every time it still has the stuttering/ghosting issue that doesn't occur with the 25.3.1. So I keep rolling back (properly with ddu) but now games aren't just whining about the old drivers (ninja gaiden) but Battlefield 6 flat out won't allow me to play (without some registry hack) So with AMD not doing any work on this card's drivers anymore, does that also include this issue that is THIS old being completely ignored?",AMD,2026-01-20 01:12:52,1
Intel,o0n18nb,"Should I disable my iGPU?  I have a 9600X and 9060XT and wondering if there is any benefit to leaving the iGPU active. I noticed that it allocates 512 MiB of system ram as vram, but I wonder if I could miss out on something if I disable it.",AMD,2026-01-20 09:31:19,1
Intel,o0q9hqx,"Hi everyone, the refresh rate of my Lenovo Yoga Slim 7 13ACN5 drops suddenly when I connect it with the Gigabyte M027Q28G. I can't change the refresh rate back then in that case.  I use my laptop basically as a desktop so I only use the display of the Gigabyte monitor. The issue only happens when I connect my laptop with the Gigabyte M027Q28G, when I try to connect my laptop to other monitors the issue doesn't even appear and the refresh rate is stable throughout the entire usage of the monitor.  When my twin brother hooks up his laptop to the monitor he doesn't suffer from the refresh rate drop issue at all so the issue only appears when I hook up my laptop to our Gigabyte M027Q28G.  When I'm using the monitor I connect it with a HDMI cable to my laptop through the Baseus Joystar 7-in-1 USB-C Hub as my laptop has only USB-C ports so no HDMI port or a DisplayPort port at all.  I've tried the following things in order to try solve the mentioned issue:  * Reinstall the GPU drivers of my laptop. * Pull the HDMI cable out of the HDMI port on my USB-C hub and then plug it in again.  Regrettably these things I tried didn't solve the issue so now my question is how I can solve the refresh rate drop issue when my laptop is connected to the Gigabyte M027Q28G.  Thanks for your help in advance lastly! :)",AMD,2026-01-20 20:17:48,1
Intel,o18qds7,My amd icon in system trey has a ! mark next to it i don't have any notifications i restarted and it wont go away,AMD,2026-01-23 14:21:49,1
Intel,o1g8osf,"**Is it worth waiting for the 9950x3d-2?**  My current system is a ryzen 5950x w/ 64Gib RAM on an x370 mainboard (Asus Crosshair VI Hero).   The mainboard is 7 years old (bought back then with a ryzen 1800x) and I really want to replace these components in my main PC.  The sooner I am thinking, the better, since I am using it 99% for work (Software development, virtual machines, local LLMs, databases, etc.) and everything needs to work - always.  Now the question is: Wait out the upcoming 9950x3d2? Or just go for the existing one?  I would pair the processor with a Crosshair X870E Hero.  Is it worth the wait considering my main use for the rig? My feeling says ""no"", but I am curious what others think about this :-)",AMD,2026-01-24 16:39:29,1
Intel,nxdpvtk,"https://i.redd.it/n3zqy5xj72bg1.gif  Apparently my driver stopped updating nearly 2 years ago, and I was never concerned about it. Do I need to do some ridiculous workaround here?",AMD,2026-01-03 04:11:23,0
Intel,nxmzstu,"Hi. HWiNFO is a very reliable monitoring tool, so unless there is a known open issue regarding sensors for your CPU SKU, I'd trust these temp readings.  I don't use a Ryzen 7 7800X3D, but the maximum operating temperature (Tjmax) for the 7800X3D is 89Â°C. If you're seeing temperatures over 100Â°C, that's likely a cooling problem that could damage your chip over time. I'd probably check my cooling system and setup if I were you. That said, another 7800X3D user might think differently, so maybe there is nothing to worry about.  Based on my experience, CPU temps over 100 Â°C usually indicate poor thermal management or inadequate cooling.",AMD,2026-01-04 15:31:19,1
Intel,nxtun1i,"check if memory and/or fabric clocks spike at the same time (max values basically), if they do it is a sensor bug.",AMD,2026-01-05 15:16:47,1
Intel,nx5b3qi,You still can build a PC as long as you know where to get the parts you need at a price you can afford despite the crappy RAM and GPU prices.,AMD,2026-01-01 21:47:15,2
Intel,nx8r8gw,"China stolen Samsung DRAM tech, this year we may have influx of chinese cheap RAM from CXMT to save us",AMD,2026-01-02 12:24:29,1
Intel,nx92ypu,"> how much of an improvement will I see with my RX 7900 XTX?  Up to 50% but this is assuming heavily CPU bottlenecked games (stuff like Battlefield 6, Factorio, Stellaris etc). Less than 15% in a standard AAA grade single player title if you play at 1440p. 0% if you play at 4k.   There's no single metric here as it really depends on a game. If you love 4X games like Stellaris I would upgrade. If you prefer Silent Hill or Alan Wake 2 I wouldn't.",AMD,2026-01-02 13:44:35,2
Intel,nx92d32,"It actually might make sense considering you are playing CPU heavy games at a relatively low res. I would also check if 5700X is available since it's pretty much the same thing as 5800X, except often a bit cheaper.   I see techspot actually tested BF6:  https://www.techspot.com/review/3043-battlefield-6-cpu-benchmark/#2025-10-15-image-png  3600 got 62 fps 1% lows and 86 averages whereas 5800X reached 80 fps 1% lows and 108 averages. So theoretically up to 30% better. Still, in both cases it's playable, fps dropping to 62 probably won't kill you.",AMD,2026-01-02 13:40:59,2
Intel,nxq9rvb,It can vary model to model. I watch search for the sku you purchased and if you can't find it try contacting the manufacture to see if they can tell you. EVGA used to be good about providing this info but it really depends.,AMD,2026-01-05 00:39:44,1
Intel,nxav8dz,"CPU, motherboard and RAM yes. AM5 and DDR5 are the newest.  I don't like windows 11 and I will keep using windows 10 for as long as I can. Unless you absolutely need to upgrade I wouldn't bother.",AMD,2026-01-02 19:00:42,0
Intel,nxev4pq,"According to the review on that site it comes with 5200 MT/s RAM which is not ideal. 6000 matches the memory controller's speed so that's what I would recommend. It's not a big issue, only a small performance difference. Other than that it looks like a solid setup.",AMD,2026-01-03 09:40:03,1
Intel,nyidpo3,"The ram by itself is fine, though you'll probably need to manually set it to something like 6000 CL32. You can get the expo sticks, but the only thing that'd really change for you is the preprogrammed profile to allow you to get it with just a single setting.  The 7200 should run out of the box, but the CPU will switch into a different memory mode where it runs its memory controller at half clocks that lowers performance until about DDR5 8000, so that's why you'd go to the 6000 instead",AMD,2026-01-09 01:04:11,2
Intel,nyv1n08,good point,AMD,2026-01-10 22:03:25,1
Intel,nxn91rb,"it's not constantly hitting 100 Â°C so idk what to think of it, hasn't happened today yet and I've been monitoring it so maybe it's nothing",AMD,2026-01-04 16:15:19,1
Intel,nxaknxe,"Thank you for your reply!  Price difference for me between the 5700x and the 5800x is 10 euros so cost isn't something to consider in my case. I'll look up thermals to see if it makes a difference. The benchmark you linked is so helpful for my purposes, kudos!",AMD,2026-01-02 18:12:20,1
Intel,nxrzsmj,"Thanks bud. It's the actual AMD card branded 6950xt, some people mis name it as a founders edition. I'll.try reach out to AMD today.",AMD,2026-01-05 06:52:53,1
Intel,nxngkvz,"It's good that it doesn't happen constantly, but even if those readings occur occasionally or intermittently, it's generally not a good sign.  However, if it hasn't happened again today, and you're under similar or identical workloads to when you had those readings, you probably have nothing to worry about. It could just be a few inaccurate readings.  Continue to monitor your CPU temperatures and, if you notice occasional readings over TJmax again, it's worth checking your current thermal management (thermal paste, contacts, etc.) and cooling setup (fans, AIOs, and/or liquid cooling). Prevention is better than cure.",AMD,2026-01-04 16:50:14,1
Intel,nvc9b2o,"The Radeon RX 9060 XT offers the highest raw frame rates at 1080p, outperforming the competition by roughly 4-5% on average.  The RTX 5060 provides nearly identical performance but adds the advantage of DLSS 4 for superior upscaling and image quality.  While the Intel Arc B580 is the slowest card, its 12 GB of VRAM allows it to handle Ultra settings that cause the 8 GB cards to stutter.  Ultimately, the video recommends the 16 GB version of the RX 9060 XT as the best long-term choice for modern gaming.",AMD,2025-12-22 08:32:19,147
Intel,nvcj3xg,Had to sell the 6600 XT and went for the 9060 XT 16GB to play at 1440p. Iâ€™m loving it,AMD,2025-12-22 10:10:46,36
Intel,nvim4sd,I got my 8GB 5060Ti open box excellent BestBuy for $309. It was brand new.,AMD,2025-12-23 09:07:15,5
Intel,nvgivw5,"Bought a 9060XT 8GB for 247e (renewed on Amazon, Black Friday stuff) and sold the temporary 4060 non-TI 8GB for 220e on marketplace. Good deal...",AMD,2025-12-23 00:02:30,3
Intel,nvotif9,Personally out of the 3 I'd pick the 5060. Transformer model is but better than FSR4 at 1080p,AMD,2025-12-24 08:41:46,3
Intel,nvahjmg,Only compares 8GB cards from teams red and green since itâ€™s only considering <$300.,AMD,2025-12-22 00:43:27,7
Intel,nw2aruf,ðŸ˜®ðŸ«³ðŸ¿,AMD,2025-12-26 18:41:22,1
Intel,nwnskte,"I found an openbox 9060 XT 16GB at Microcenter for $305 and jumped on it. Very impressed so far, especially with undervolting.       I have the Powercolor Reaper model and it is legitimately impressive that they were able to make it that small.",AMD,2025-12-30 02:33:44,1
Intel,nvbruur,"I feel like the HUB guys are going too hard with their VRAM crusade. Why recommend a GPU that's slower now just because it might be faster in the future?   A slight downgrade in render resolution or texture quality is hardly even noticeable, and with looming shortages I feel like most studios are going to start optimizing for lower VRAM further reducing the long term disadvantage of 8GB GPUs.",AMD,2025-12-22 05:48:05,-14
Intel,nvcdgdm,"The real answer, buy a used 2080ti. Usable VRAM, DLSS4, it still is 250W so it can run on most PSUs.  It is the most balanced option if you can't afford a 9060XT 16GB.",AMD,2025-12-22 09:14:12,-15
Intel,nve7fwf,"all of them are power hungry junk, where are good cards?",AMD,2025-12-22 16:44:49,-9
Intel,nvm6i3z,real hero here,AMD,2025-12-23 21:49:44,4
Intel,nwe5oim,"Intel is on the right path, but they need to start using 384-bit memory interfaces on 12GB cards instead of the 192-bit memory interface they used on this card.",AMD,2025-12-28 17:23:33,1
Intel,o0nywwz,Does the 9060xt with fsr have higher performance than 5060 oc edition No Ti with dlss? And by how much?,AMD,2026-01-20 13:46:49,1
Intel,nvm9ob0,"The 5060 will crush, without less than a 40 percent difference, from dlls alone. Then add frame gen. WOW I can't believe you can get away with this.",AMD,2025-12-23 22:06:17,-12
Intel,nvftrl1,5060 then cuz way better in AI  5% performance cut to gain 2x-3x AI speed,AMD,2025-12-22 21:41:30,-33
Intel,nvfjawi,What processor are you using with 9060 xt?  Is it the same as you were using with 6600 xt?,AMD,2025-12-22 20:46:22,10
Intel,nvjicw9,"Can you tell me how well it runs games at 1440p? Have you played some of the demanding ones like Black Myth Wukong, Stalker 2, etc? Do you play at medium? high? I assume FSR is always on.   And also what's your target FPS? Would really appreciate the feedback, because I have the same card and I'm thinking on switching to 1440p but I don't know what monitor would be good refreshrate-wise",AMD,2025-12-23 13:36:03,1
Intel,o18s8r6,"Honestly? Pretty decent deal for a 8GB model, considering how the 16GB cost around 450$ these days, almost double the price.",AMD,2026-01-23 14:31:18,1
Intel,nvb3bfo,"Well yeah, the cheapest RX 9060 XT 16GB is [$380](https://pcpartpicker.com/products/video-card/#c=596&sort=price&page=1&P=11811160064,51539607552) and the cheapest RTX 5060 TI 16GB is [$430](https://pcpartpicker.com/products/video-card/#sort=price&P=11811160064,51539607552&c=593). When you're comparing $300 GPUs, you're not going to bring up a GPU that's nearly another hundred dollar.",AMD,2025-12-22 02:55:15,48
Intel,nvcb05n,"That would be a completely new phenomenon if you look at the past. Sure, some (probably indie) studio will optimize their games, but they would have done so already because they care about their customers.  Nothing will change with the current devs or tech, it's just a temporary issue that memory is that expensive. The prices will be lower in 2027, or we'll get used to it and buy more expensive stuff.",AMD,2025-12-22 08:49:20,9
Intel,nvda1uj,"6 ish year old product that is out of warranty from some rando, is not exactly comparable here and definitely not a ""real answer""",AMD,2025-12-22 13:46:25,18
Intel,nvd5m8s,"Dunno why you're being downvoted, the 2080 Ti is still very good value for the price and often has good OC headroom. Beats 5060 in most cases and you're right about 11GB being decent",AMD,2025-12-22 13:18:38,-1
Intel,nvckhpi,The real answer is to stop being cheap and spend money on your hobbies.,AMD,2025-12-22 10:24:05,-26
Intel,nvgc2fa,you tell us,AMD,2025-12-22 23:21:59,1
Intel,nvhwm4g,Why do power requirements matter?   Electricity costs pennies,AMD,2025-12-23 05:17:09,0
Intel,nwno1my,So many think memory bandwidth matters more than it does. The 5060 ti has less bandwidth than the B580. Architecture matters a lot.  More bandwidth would do next to nothing for it.,AMD,2025-12-30 02:09:09,1
Intel,nvr8r0s,AMD cards have upscaling and framegen as well...,AMD,2025-12-24 18:35:16,4
Intel,nvgtg9q,"Can you elaborate what do you mean by ""AI speed""?",AMD,2025-12-23 01:04:59,29
Intel,nvlqflt,No. Dlss and frame gen is much less impactful in terms of performance boost at the low end and the latency is more noticeable. Itâ€™s also half the vram.,AMD,2025-12-23 20:25:01,9
Intel,nvm9h9y,WOW 25 so far for the TRUTH. HUB and fooling now.,AMD,2025-12-23 22:05:16,-4
Intel,nvfl8ph,"Yes, same processor, 5600x",AMD,2025-12-22 20:56:36,7
Intel,o18rwrw,"I only play Space Engineers, Cyberpunk 2077, Helldivers 2 and pre-2020 videogames, new games are meh for me.  All of them at high with ultra textures and a few middle settings at things i never notice mid-game at 1440p 27"" IPS monitor, 60fps (don't care for more, maybe one day i'll try 75fps).   Runs very cool and quiet, never hitting above 65Â°C on closed room 20Â°C ambient temperature, my case sounds the same as a 20"" metal fan at speed 1, which is the same white noise that helps me sleep at night, an upgrade compared to my old RX 6600 during the summer lol.  Not sure if this helped or way too late, just wanted to comment anyways.",AMD,2026-01-23 14:29:36,1
Intel,nvjk7kj,"yeah seriously, here b580 is noticably cheaper for example.",AMD,2025-12-23 13:47:03,4
Intel,nvcwed4,"Yeah, I just wanted to point it out because thereâ€™s people like me to whom prices in dollars means nothing (or who donâ€™t read the title) and then waste time watching an irrelevant video (though I skipped to the conclusion so not that much time).ðŸ™‚",AMD,2025-12-22 12:11:20,-8
Intel,nvdtbxt,"Point being? If a cap blows because it's old any repair shop can fix it, If it's a fan dying you can fix that yourself.  On the other side there's not much the warranty can do for running out of VRAM.",AMD,2025-12-22 15:34:11,-7
Intel,nvcpxpj,"In this economy? It doesn't make any sense to not keep perfectly usable hardware that does the job just fine out of a landfill.   A 2080ti or a 3070 or AMD equal is more than enough performance for most people. Easily, and is way better bang for your buck.",AMD,2025-12-22 11:15:29,10
Intel,nvgxp18,"In a time of global economic uncertainty, it's a horrendous time to overspend on hobbies.",AMD,2025-12-23 01:31:13,3
Intel,nvhzp95,"heat, noise, size, messy cables",AMD,2025-12-23 05:41:18,1
Intel,nvuuu5m,"Correct, but they do not have commercial dlss support. How many games do you not have the ability and ww do?   Thanks for the dowmvotes nvidia. Amd brainwashed.   Just to let you know: you have all been played. Look closer.",AMD,2025-12-25 11:21:47,-2
Intel,nw6d88g,"They're seemingly referring to the speed of running LLMs locally using that GPU, unless I'm also out of the loop. A good sub to look into that stuff would be /r/LocalLLM   I wouldn't recommend doing that with a 5060 but the 16gb version must be the best choice in that price range and would handle the very small models easily and the small ones with a little slowness.",AMD,2025-12-27 11:35:46,1
Intel,nvcxw7p,"News flash: We're always in ""this economy"". I know someone who works at a fucking McDonalds, has a kid, and spends more money on his hobbies than you do.",AMD,2025-12-22 12:23:17,-11
Intel,nvlzofj,nothing global about it,AMD,2025-12-23 21:14:23,-2
Intel,nvipuzv,"So you prefer low power for lower heat and smaller size.   I'm not space conscious, so those things don't matter.   What's with messy cables? The PC sits under the table, so it also doesn't matter how ugly it is.",AMD,2025-12-23 09:44:14,2
Intel,nvw2mmm,"Well actually people have been modding games to put FSR where neither AMD or NVIDIA added official support.   Pretty much every game had amd nvidia and even intel upscaling these days.   In fact, when i still had my 3080ti, i was able to use AMDâ€™s framegen in many games (cyberpunk, dying light, talos principle) because NVIDIA didnt provide any option for 3000 series.   I still bought nvidia because amd doesnt offer any cards at 5080 level, so no brainwashing here. Youâ€™re completely uneducated blinded by consumerism",AMD,2025-12-25 16:47:51,3
Intel,nvimsbd,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-23 09:13:38,1
Intel,nveay56,They sound irresponsible.,AMD,2025-12-22 17:02:16,10
Intel,nvfpbcp,"If you spend more on your hobbies than the things you actually need, you might be financially irresponsible",AMD,2025-12-22 21:18:17,7
Intel,nvj2hzu,"the unhinged extra power cables attached to the card itself, it makes everything harder to handle  and heat isn't just about the size of the case, it's noise and comfort of the room  and no, AC doesn't solve that as it's another source of noise that is even worse than the PC itself, only to be used when the weather is too bad to live otherwise",AMD,2025-12-23 11:42:04,0
Intel,nvwscdp,"Oh yeah? And who do you think had went through hundreds of accounts taking about that mod?   Came out DEC 22 2023 I remember the day I went from 22fps in Alan wake 2 to 50 (3070). I posted in this site non stop ban after ban just to try and get you this information. Go on the forums you will pick me out if you look back.   I, in all seriousness, would not be surprised if you know about that mod from ME.   Therefore, I am not blinded. I simply understand that an entire company propped up by manipulation on social media (and GPU) should not exist, and a real competitor would be in their place. There I have just demonstrated that not only am I not blinded, it might be you. Good on you for knowing about that mod (serious).   I will tell you another secret, maybe not meant for you. If you don't mind using dlss and mfg? 5060ti 16gb all day for 1440p or lower. Not only is it hundreds of fps, it has valuable vram on it that will see the card rise in price since it's discontinued.   Hows that for an uneducated prediction?",AMD,2025-12-25 19:18:39,-2
Intel,nvjm540,Gotcha.  What's your preferred card?,AMD,2025-12-23 13:58:18,1
Intel,nvkhap4,from this generation that'd be RTX Pro 4000 Blackwell generation SFF with replaced cooler  personally I own a passive A2000 SFF that replaced my modded 1650 (KalmX was released too late),AMD,2025-12-23 16:39:40,2
Intel,nvkshx0,"Hopefully I am wrong but there is no aftermarket cooler for the RTX Pro 4000 SFF, right ?  https://n3rdware.com/gpu-coolers",AMD,2025-12-23 17:35:10,1
Intel,nvkw08y,"unfortunately no, nothing ready to use that I know of  if you have access to measuring equipment machining a shim isn't even that expensive, haven't seen any publicly available projects for it yet",AMD,2025-12-23 17:52:24,1
Intel,nvkx5mu,"Hmm that sounds tricky.  Iâ€™m thinking about getting PCI express extensor and a GPU holder to be able to use it with my MS-A2, keeping the GPU externally til the n3rdware cooler is available.",AMD,2025-12-23 17:57:58,1
Intel,ntamglk,"From r/radeon   * Ray Caching: Only available in Warhammer40K today, more games next year. * Ray Reconstruction: Only available in Black Ops 7 today with more games next year. * AI Frame Gen: Available in Black Ops 7 today with 40 games by end of 2025.",AMD,2025-12-10 14:35:10,103
Intel,ntak2ov,It's almost 2026 and AMD keeps reinstalling the AMD Install Manager that I do not want and have to keep manually uninstalling. Stop this AMD.,AMD,2025-12-10 14:21:39,303
Intel,ntam2kl,What is fsr redstone? and which games use it?,AMD,2025-12-10 14:32:55,85
Intel,ntbp2n9,I just tested the release on four machines (76X&78XT/78X3D&79XTX/97X&9070XT/75F&76XT). Every system still suffers from crashing drivers when hardware-accelerated apps are used (Chrome/Discord/etc.).  Please fix. :),AMD,2025-12-10 17:49:10,24
Intel,ntak8pe,"I got a notification for the update in AMD Adrenalin Edition, but it does not appear in the actual install manager lol",AMD,2025-12-10 14:22:35,49
Intel,ntanibq,so can I open adrenalin on this one with a rdna 2 igpu and rdna3 gpu or is it still broken like the last version,AMD,2025-12-10 14:41:05,19
Intel,ntalx8c,<--- Int8 rdna2 enjoyer,AMD,2025-12-10 14:32:04,83
Intel,ntaottt,did they fix enhanced sync and noise suppression yet,AMD,2025-12-10 14:48:21,37
Intel,ntayll4,Did this driver fix purple visual glitches with the RX 7700 XT? It's a known bug that appeared after the driver 25.4.1,AMD,2025-12-10 15:39:11,14
Intel,ntav0ai,"The ignorance by amd of Rx 7000 users is astounding tbh, but this is 2025 AMD not prior AMD where they would try to appease a larger user base.  It's going to make me rethink my loyalty for future gfx purchases",AMD,2025-12-10 15:20:57,49
Intel,ntam4ms,So we cant test path tracing performance yet on Cyberpunk? Lol,AMD,2025-12-10 14:33:14,33
Intel,ntbddly,"This is a very underwhelming update for RDNA 4 users I get that this technology needs to mature, but they should already be at a point where the implementation is across more wide array of games. My fallen RDNA 2 and RDNA 3 brothers will be remembered. The only reason AMD gpus are still relevant rn is price, nvidia tax is crazy. GG",AMD,2025-12-10 16:51:29,11
Intel,ntbtbc6,"Thanks for nothing again, AMD.  Signed, 7900 XTX user.",AMD,2025-12-10 18:09:31,27
Intel,ntanq1w,So is there any point to installing this if I'm on RDNA2 and don't have any of the issues that they fixed?,AMD,2025-12-10 14:42:16,19
Intel,ntboygm,This is the worst driver amd made 9060xt for me. 2 games instantly crashes. Indiana jones and silent hill 2. With this driver if you enable ray tracing game hangs and give error.i already report bugs in 25.12.1 and same with 25.11.1 and amd does nothing. every ray tracing titles works ok with 25.10.1 driver and this is bad. amd does not listen users anymore. anyone has any crashes happen like me?thanks...,AMD,2025-12-10 17:48:37,9
Intel,ntcc5fr,AMD Software still crashes randomly,AMD,2025-12-10 19:40:52,8
Intel,ntan1w5,Nothing on Oblivion Remastered crashing? Intermittent application crash or driver timeout on 9000 series when playing Battlefield 6?,AMD,2025-12-10 14:38:30,15
Intel,nti2mdh,#AmdNeverAgain Give Fsr4 on rdna3,AMD,2025-12-11 17:48:52,7
Intel,ntamhuj,Pretty dissapointing ngl,AMD,2025-12-10 14:35:22,23
Intel,ntba2eq,New update new problems,AMD,2025-12-10 16:35:17,5
Intel,ntayron,"The adrenalin app just auto updated my 9070xt mid game, now my screen is black with no signal output to my monitor but my music is still playing lol. I waited for 10mins then I had to hard restart my computer for it to say the update failed",AMD,2025-12-10 15:40:02,11
Intel,ntb58wr,Should I get the RTX 5070 ti or 5080 at msrp? I am currently selling my XTX after radio silent news about FSR 4 int 8 on it.,AMD,2025-12-10 16:11:45,22
Intel,ntak0ko,Everything is RDNA 4 exclusive? awesome /s  RIP finewine.,AMD,2025-12-10 14:21:19,59
Intel,ntb9myj,Please add the broken noise suppression to â€œKnown Issuesâ€.,AMD,2025-12-10 16:33:11,5
Intel,ntbbh4c,"If  this driver update keeps crashing my gpu im not leaving 25.9.2 for a while, im also starting to think about selling my gpu and get nvidea, and really black ops 7 why not a real game like cyberpunk i dont want to waste 70 euro for fifa with guns",AMD,2025-12-10 16:42:08,5
Intel,ntbih3y,"Can confirm on my 9060XT that Silent Hill 2 is still crashing and Avatar Frontiers of Pandora currently has a bug when FSR4 is enabled where the entire screen starts flashing like a strobe light, shadowy areas seem to trigger it. This is with both games fully patched & up to date.",AMD,2025-12-10 17:16:44,5
Intel,ntbq2n7,"Let me see - all the new ""Features"" will be available for Cyberpunk 2077 in at least 1 year time and ONLY with RDNA4 ??",AMD,2025-12-10 17:53:59,6
Intel,ntcf8iq,AMD NoiseSuppresion still broken. Since September!,AMD,2025-12-10 19:56:49,6
Intel,ntedkus,"Are pink artifacts fixed on RX 7700 xt, anyone ? It was bugged in 25.11.1 driver last month.",AMD,2025-12-11 02:20:15,4
Intel,ntb4cu0,Whereâ€˜s support for 7000 series? Wtf is this dead meat,AMD,2025-12-10 16:07:26,13
Intel,ntaofpr,Iâ€™m on a 6000 card is there literally no reason for me to download this,AMD,2025-12-10 14:46:12,20
Intel,ntasl1x,"all i want is to be able to capture clips in my games but for whatever reason amd either doesnt understand im in the game, recognizes the game wrong (battlefield 6 shows as elder scrolls online which i dont even have).",AMD,2025-12-10 15:08:16,4
Intel,ntdy3yt,It's december and still no FSR4 for vulkan.,AMD,2025-12-11 00:46:14,4
Intel,ntf1v9l,25.11.1 was dog water driver timeout city for me I'm just gonna assume this new one will also be the same.,AMD,2025-12-11 04:59:50,4
Intel,ntaql28,Is this worth updating to from 25.11.1  Is it more stable?,AMD,2025-12-10 14:57:42,6
Intel,ntc0jb7,"I had to downgrade to 25.9.1 to have some level of stability, can somebody confirm that the new driver is safe to upgrade to without it messing stuff up?",AMD,2025-12-10 18:44:11,8
Intel,ntawnis,Still no fsr 4 support for rdna3 ðŸ™„,AMD,2025-12-10 15:29:24,9
Intel,ntb91tu,"Guys calm down. RDNA3 being moved to maintenance mode is part of their new strategy, no longer ""Fine Wine"", the new approach is Stale Ale. That way their products remain DOA after launch and people won't keep them very long.",AMD,2025-12-10 16:30:16,13
Intel,ntatk4y,idk why I find it so funny that a specific Roblox game got called out in the patch notes,AMD,2025-12-10 15:13:24,3
Intel,ntavzqu,Did they fixed the amd noise supression not turning on?,AMD,2025-12-10 15:26:00,3
Intel,ntbglg2,"Anyone know why Cronos: The New Dawn has been showing [""FSR 4""](https://i.ibb.co/nqW2VMng/Cronos-The-New-Dawn-2025-12-04-02-28.png) for me on a 7900 XT for a few weeks? At first it was 3.1.  I know it can be modded in but this is on a new Windows 11 install and I haven't done any modding.",AMD,2025-12-10 17:07:23,3
Intel,ntbm1c9,"Looks like new chipset drivers, too.",AMD,2025-12-10 17:34:16,3
Intel,ntc41oy,"I thought the application freeze fix might have stopped monster hunter wilds from crashing on me but nope still does it (DXGI_ERROR_DEVICE_REMOVED,)",AMD,2025-12-10 19:00:46,3
Intel,ntcb4ur,/u/amd_vik are you aware of assetq corsa evo vr not working on AMD cards since 25.9.1 ? It displays the left and right eyes out of alignment and therefore fails to show a cohesive single image.,AMD,2025-12-10 19:35:42,3
Intel,ntcgpo5,so no fsr4 support on Vulcan still? this is getting ridiculous,AMD,2025-12-10 20:04:11,3
Intel,ntdhmve,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.Â   Thank fucking god.,AMD,2025-12-10 23:10:51,3
Intel,ntdxxbg,Still experiencing 100% gpu usage almost constantly as soon as you boot up BF6 on newer drivers after 25.10.1 and higher temps in general  I'm locking my FPS to 144 but the older drivers is showing better overall temps and less gpu usage for me ðŸ¤”  [https://imgur.com/a/ctbMCx7](https://imgur.com/a/ctbMCx7),AMD,2025-12-11 00:45:09,3
Intel,ntf7tyk,BF6 crashing after a few minutes in game with that driver on a 6800xt,AMD,2025-12-11 05:46:22,3
Intel,nth0425,"ever since 25.9.2 still same bug is present even now and now it causes even more problems because ML based FSR and FG fails when it happens: Adrenalin app just shuts down randomly even when idle, no errors, no driver timeout, no dx12 trimeout, just adrenalin itself gets shut down in random times. why wont you guys do something about it finally? Seriously its been so long now... im on 9070XT Steel Legend Dark Edition from ASRock, 80% of your users or more report the same issue FIX IT for the love of GOD. I tried everything hoping its on my side but windows reinstall, DDU and AMD cleanup app and fresh driver install nothing helped its still here",AMD,2025-12-11 14:36:29,3
Intel,ntsip7g,"Both 25.12.1 and 25.11.1 drivers have the same bug on RX 9060 XT. When my screen goes blank and later i wake up screen, i have two mouse cursors on the screen, until i launch some app and then will second, fake cursor disappear.",AMD,2025-12-13 09:49:20,3
Intel,o14ytde,"u/AMD_Vik Hey there! Just want to let you know that I've seen a lot of people have problems lately (including myself) with Direct-to-Display/Directmode implementations for DisplayPort/HDMI wired VR Headsets. DirectMode cannot enable reliably even on SteamVR native HMDs, and the only way to get them running right now is by installing a older driver version, enabling directmode there, and then updating to a newer version with the directmode already set -> so directmode itself still works fine, but there seem to be problems toggling into this. I also saw this happening with other VR compositors like PimaxPlay though radeon users are generally not as common there.  Happens with every driver released after and including 25.4.1 on my machine, and driver older than 25.3.1 can toggle into Directmode just fine. It is still broken as of 25.12.1 and 26.1.1  at first I thought this was a somewhat individual issue but as I've looked into it I saw more and more people have that problem with RDNA3 and RDNA4 GPUs across the board for basically any VR headset that uses Directmode.  I remember you mentioning on the AMD forums once that you want to be hit up about VR related issues - but really I didn't know how else I would reach out to you.",AMD,2026-01-22 23:12:00,3
Intel,ntaw89a,I hope this fixes the many crashes I've had since the last update...,AMD,2025-12-10 15:27:13,5
Intel,ntc9i8c,"Still enjoying the piss out of the 7900XTX on 25.9.2. It chews through everything I throw at it at the settings I choose, don't care about new driver releases unless a new game I want to play doesn't play well on whatever driver I currently have installed.",AMD,2025-12-10 19:27:35,6
Intel,ntb1a2q,"Even tho I have a 9070xt this is still so underwhelmingâ€¦ We waited 6 months and got basically nothing yet. Sorry for all rdna2, 3 users.  Fun fact: Its been years now that the adrenaline software cant be opened, the only fix ist to press win+p and select only main monitor. Than start it, than swap monitor profile againâ€¦   Definetly buying nvidia next time, not supporting this big company anymore, which is behind in every aspect. Image you just want to play alan wake 2 (looks beautiful).",AMD,2025-12-10 15:52:19,8
Intel,ntaqa2o,"ass. no support for rdna2/3, no new features for rdna2/3, rdna4 have only one game that support all of that, redstone framegen almost identical to 3.1 framegen, frame pacing still there.",AMD,2025-12-10 14:56:05,12
Intel,ntazsry,hardware unboxed tested it and frame facing is broken when amd frame gen is on sadly,AMD,2025-12-10 15:45:05,4
Intel,ntaqj5o,So the HDMI crashing issues should be fixed in this version yes?,AMD,2025-12-10 14:57:25,2
Intel,ntayomq,Any news on fixing the gpu vram leak issue on bf6? Sorry Iâ€™m lazing not reading the patch notes,AMD,2025-12-10 15:39:36,2
Intel,ntb8vq9,25.12.1 does not even install on my Minipc (780M) + 6650XT eGPU Setup.   I thought I might fix 25.11.X not opening in an eGPU Setup.   Guess I will be running 25.9.2 for another few Months.  God why something always break? I thought it would be better going all AMD for the eGPU setup.,AMD,2025-12-10 16:29:25,2
Intel,ntbdrmk,"Yeah I'll still be with 25.9.1 until the texture flickering is fixed in BF6, also instant replay just didn't work in 25.11.1 for me.",AMD,2025-12-10 16:53:24,2
Intel,ntbt5fb,Will this help Warzone not look so blurry on 7900xt? Game is unplayable,AMD,2025-12-10 18:08:44,2
Intel,ntc1hhd,So there seem to be two links - going through support>picking GPU(9070XT in this case) downloads the 25.21.1 win 11-b.exe file meanwhile going from this release note article it downloads the win11-c.exe . Any difference?,AMD,2025-12-10 18:48:42,2
Intel,ntc4frb,"im using 6800xt the driver page has the win11-a version and article have win11-c version. which one should i choose i really dont know and this ""different builds"" confusing a lot of people",AMD,2025-12-10 19:02:40,2
Intel,ntcb9km,"Genuine question, why all the hype and rush to release this today when it has just two games to showcase the benefits?",AMD,2025-12-10 19:36:22,2
Intel,ntcl3bs,Jesus how long has that Cyberpunk Pathtracing crash been in the known issues. It feels like it's been more then half a year.,AMD,2025-12-10 20:26:14,2
Intel,ntcrk7m,Installed with no issues,AMD,2025-12-10 20:58:08,2
Intel,ntdoxgx,"I canâ€™t play Warzone because I canâ€™t update my bios, there doesnâ€™t seem to be a recent bios update available for my Acer Nitro 5, using Adrenaline. Anyone know if this will help?",AMD,2025-12-10 23:53:00,2
Intel,ntfskqf,Doesn't look like they fixed the bug with Enhanced Sync not working properly with Freesync.,AMD,2025-12-11 09:00:35,2
Intel,ntgkfzg,Any fix planned for 9070 users who cant enable Hardware Lumen on Oblivion Remastered? Game crashes as soon as we turn on the option.,AMD,2025-12-11 13:04:42,2
Intel,nthjjtu,Still no fix for Battlefield 6 for those with AMD 6800M GPU. I swear my next setup is going away from AMD if this is not resolved anytime soon.,AMD,2025-12-11 16:15:50,2
Intel,ntkinfb,u/AMD_Vik      In 2022 AMD made changes to OpenGL Driver. So since 2022 the extension gl\_ati\_fragment\_shader is missing in the driver. It cause problems in older games like Call of Duty 1 from year 2003. Stutter on some maps and broken water rendering because the games can't use the extension anymore.     Our Community is waiting since 3 years for a fix.,AMD,2025-12-12 01:38:24,2
Intel,ntn9tly,in black ops 7 only 25.9.2 driver work better even new 25.12.1 much worse fps drops,AMD,2025-12-12 14:01:19,2
Intel,ntp4ou0,Very unstable for me (7900XTX). Driver keeps crashing even when I'm just watching videos. Reverting to 25.11.1,AMD,2025-12-12 19:39:09,2
Intel,ntq9ysh,i just had to roll back to 25.9.2 because 25.12.1 kept crashing my system with poe2   even GGG straight up said don't use 25.10-25.12,AMD,2025-12-12 23:26:01,2
Intel,ntsszh2,"After observing you guys for a few days xD, 25.12.1 was installed along with new chipset driver on my system.  To my surprise, unlike previous 25.11.1, Adrenalin interface now runs properly with igpu enabled.  I need to test it out with real games, but for now, I've dodged instant roll back.  FYI, If you're using two or more GPUs, including igpu, on a single system with muti-monitor. Download the C package(1.65GB one including rdna1&2+3&4).",AMD,2025-12-13 11:33:00,2
Intel,nttlrcj,"NoUnfortunately, they don't work (( Random crashes remained + In some games, the inability to use frame generation through drivers was added (( Sad ( R5 3600 32gb ram Rx 7700 xt ) Rolled back to 25.9.1 everything works with it",AMD,2025-12-13 15:00:42,2
Intel,ntyj52n,"I had a very weird issue:     My PC would just crash when i did an Windows Defender Scan (only Full Scan, it worked fine with QuickScan or other programms like Malewarebytes) like the power was cut. I did a number of things even rollback the chipset driver but that didn't help. Then i rolled back to 25.9.1 + the newest chipset driver and everything worked fine again.   In case somebody had a similiar issue",AMD,2025-12-14 10:21:51,2
Intel,ntz35tj,"Anyone having problem with AMD overlay with this update? Somehow not showing at any game even if enabled, if I click to different monitor, it shows up. But when I click back to the game it disappear again.",AMD,2025-12-14 13:16:47,2
Intel,ntzh5jv,AMD Wattman settings don't apply for the first time they're set. They have to be changed and applied to a different setting and then to the desired one back and forth to get them to work. I use wattman to set my custom fan curve and it's been glitchy since 25.11.1.,AMD,2025-12-14 14:45:37,2
Intel,nu4w43f,Should I download the new driver version if I have 6800XT? There is nothing in the patch notes about this series... And if yes - why?,AMD,2025-12-15 10:40:37,2
Intel,nu8xc58,"Error code 182 for my AMD Radeonâ„¢ 780M integrated GPU on my Ryzen 7 8854HS CPU.  All other driver updates before 25.12.1 worked fine on my Lenovo Legion Slim 5 Gen 9, but this one says my GPU is incompatible, even though AMD's driver download page is providing [this download link](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe) to the installer:  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-8000-series/amd-ryzen-7-8845hs.html",AMD,2025-12-16 00:02:21,2
Intel,nuazy8e,7000 Series web browser glitch? and sound glitch? huh,AMD,2025-12-16 09:03:04,2
Intel,nwzds1t,"So I upgraded on a 6800xt and lost a lot of features like video recording, screenshotting, custom game profiles, and hotkeys. Is that intended?  Crazy to be missing hardware supported features",AMD,2025-12-31 21:37:22,2
Intel,ny008s3,"AMD sucks: FSR 4 is locked to RDNA 4, while NVIDIAâ€™s DLSS 4.5 runs even on RTX 20-series GPUs. My next GPU will be NVIDIA only, and I advise everyone against buying AMD. Itâ€™s a greedy company with no respect for customers â€” you buy a graphics card last year, and the next year itâ€™s already outdated",AMD,2026-01-06 13:00:32,2
Intel,ntb2bag,"Is it safe to update, 25.9.1 is stable for my 9070XT and causes zero crashes with the timeout bullshit from clock speeds going to 3300+ MHz",AMD,2025-12-10 15:57:21,4
Intel,ntapbf5,What does fsr Redstone means ?,AMD,2025-12-10 14:50:59,2
Intel,ntb3ek3,"Is this driver more stable than 25.11.1 it was causing driver time outs and i even got a blue screen. I rolled back to 25.9,2 and now im scared to update to this one lol",AMD,2025-12-10 16:02:45,4
Intel,ntc136m,These comments are all over the place is it better than 25.11.1 or not? ðŸ˜‚ðŸ˜‚,AMD,2025-12-10 18:46:48,2
Intel,ntc1b9d,"So in short, still no support for 7000/6000 series, yipee",AMD,2025-12-10 18:47:54,3
Intel,ntfncpt,"Idk what happened but after this update my game crashed and then my PC crashed and when I turned it back on AMD Adrenaline disappeared from my PC, completely gone. What did you do lol.",AMD,2025-12-11 08:07:39,3
Intel,ntavbyt,For Sale: 7900 XTX - $50 OBO  I know these are no longer desirable due to being left in the dust by AMD after only a few months of real support but hopefully it will be at least a good paper weight for someone.,AMD,2025-12-10 15:22:38,3
Intel,ntb0knq,So now driver frame gen is gone? Unless the game specifically supports it? And the overlay as well? Both are completely gone now after the update...,AMD,2025-12-10 15:48:53,2
Intel,ntb1r9y,What about the fixes for the 7900xtx crashing all the time?,AMD,2025-12-10 15:54:38,2
Intel,ntim4nj,"Â«#AmdNeverAgainâ€ Whereâ€™s the Christmas gift in the form of FSR 4 for RDNA 3? In the new 2026 year, it might be time to think about switching to Nvidia.",AMD,2025-12-11 19:23:42,2
Intel,ntbtv4u,"Toujours pas de FSR4 pour les sÃ©ries 7000 ? Câ€™est mort. Pour ma part, je nâ€™achÃ¨terai plus de cartes AMD. Si Nvidia continue Ã  proposer son DLSS pour les anciennes cartes, alors mon choix est fait.",AMD,2025-12-10 18:12:11,3
Intel,ntaitkk,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 14:14:29,1
Intel,ntaqe06,"Downloads ""whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe"" for 9070XT, ""whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe"" for 5700 XT  What does it mean?",AMD,2025-12-10 14:56:40,1
Intel,ntatgqs,It took a while for DLSS 4 to get implemented in a good way on 40 series cards too but a version made it there. Give it time. Now if they can just start prodding developers to incorporate that as well itâ€™ll be worth it. Not enough games yet but hereâ€™s hoping!,AMD,2025-12-10 15:12:54,1
Intel,ntau6wp,Any update the in fact that and adrenaline software is not working when second monitor is connected? Especially using iGPU for second monitor ?,AMD,2025-12-10 15:16:42,1
Intel,ntauct1,Omg I think they fixed the LG oled tv reboot bug,AMD,2025-12-10 15:17:33,1
Intel,ntb6asm,Should i install it directly or should I use AMD cleanup utility first?,AMD,2025-12-10 16:16:51,1
Intel,ntb80pv,some one have problem with instaling?,AMD,2025-12-10 16:25:13,1
Intel,ntb8lei,Does this fix the driver timeouts that were happening with Edge? I had to revert the November update because of that problem,AMD,2025-12-10 16:28:00,1
Intel,ntb9m3i,Any fix or still need iGPU disabled for 7000 and 9000 cards?,AMD,2025-12-10 16:33:04,1
Intel,ntb9mdl,The update is still not showing up in install manager,AMD,2025-12-10 16:33:06,1
Intel,ntbb7t2,Honestly this software was the bane of my card for the longest time. Not having it anymore stopped so many weird bugs and crashes.,AMD,2025-12-10 16:40:52,1
Intel,ntbbs61,Does AMD's Instant Replay record still bug out?,AMD,2025-12-10 16:43:39,1
Intel,ntbd79c,Anyone tried the new fsr redstone yet? I am hoping for a big improvement over the old fsr,AMD,2025-12-10 16:50:36,1
Intel,ntbif1z,do you guys remove the old drivers before you install new ones? or just install ontop,AMD,2025-12-10 17:16:28,1
Intel,ntbp8r8,The path tracing crash STILL on Cyberpunk is absolutely wild to me. Finally AMD has a card capable of playable raytracing but we can't use it on the 'Crysis' of modern times to even test it out.,AMD,2025-12-10 17:49:59,1
Intel,ntbtqi7,Adrenalin doesn't show this update for me yet lol,AMD,2025-12-10 18:11:33,1
Intel,ntbw2oz,"> Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Glad for this, it was annoying that we were stuck in 25.9.2",AMD,2025-12-10 18:22:46,1
Intel,ntc4icg,Are the issues with SecondLife fixed? Last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:03:00,1
Intel,ntc4u1h,9060 non XT 8GB can do the math 7900 XTX Nintendont,AMD,2025-12-10 19:04:36,1
Intel,ntcbbg1,"Iâ€™m at work, so I cannot check for myself: does this fix the constant crashing in Oblivion when hardware lumen is turned on?",AMD,2025-12-10 19:36:37,1
Intel,ntcdlil,Has anyone tested Marvel Rivals on 25.12.1 version of the driver? The only stable driver that works without crashing on that game is the 25.8.1 version.,AMD,2025-12-10 19:48:14,1
Intel,ntchv47,Adrenalin Panel not showing bug still presentâ€¦ :-((((,AMD,2025-12-10 20:10:02,1
Intel,ntcyy65,Am I the only one who doesn't have the new option in the drivers for frame generation with a 9070 XT?  https://i.redd.it/7r86hslx3g6g1.gif,AMD,2025-12-10 21:34:34,1
Intel,ntd63ea,"why are there 2 versions, b and c, 1.65Gb and 991Mb, release notes and through the support page, and is it stable or shall i just keep 25.8.1 as any other seems to crash call of duty, regular, other games seem fine,  ryzen 9 7950x3d/rx7900xtx",AMD,2025-12-10 22:09:16,1
Intel,ntdfkjs,Did it fix the god of war 2018 checkered shadows?,AMD,2025-12-10 22:59:20,1
Intel,ntdjg2j,"I spent all this time with 25.9.2 on my 9060xt because the following ones were disgusting to me, I will give this new update a chance and let's hope everything improves a little!!",AMD,2025-12-10 23:21:02,1
Intel,nteannh,Arc raiders crashes are fixed or not?,AMD,2025-12-11 02:02:44,1
Intel,ntek6ze,"Makes my 9070 XT to constant run on 100% load in bf6 no matter if i play or sit in the menu. Cause device hung, graphic glitches and high temps.   Same with all drivers above 25.9.   25.9.1 works flawless with no errors and the load varies depending on the scenery as it should.",AMD,2025-12-11 03:00:09,1
Intel,nteshht,Noticing in Hogwarts Legacy with the new FSR and FG enabled over a period of like 30 seconds my 9070XT will go from \~250W used and 200 FPS and then drop down to say 120W used and 90 FPS and then after a short period go back up again. With FG disable it stays consistent 140 FPS-ish,AMD,2025-12-11 03:54:05,1
Intel,ntfd9ta,"FYI for ""Driver Only"" guys, 25.12.1 still have an issue to install this option.  l've open ticket to support team for last 2 versions. but I can't follow their request to observe the issue.  Don't know how long to keep using extracted file method. lol  Will see how 25.12.1 ""driver only"" perform.",AMD,2025-12-11 06:33:01,1
Intel,ntfu45p,oh nice! they fixed the FSR4 Quality Presets artifact issue,AMD,2025-12-11 09:16:24,1
Intel,ntg07nz,"When AMD finished Orange, Yellow Green, PurpleStone, can we unlock FSR Infinity?",AMD,2025-12-11 10:18:18,1
Intel,ntgsz5x,"Is it worth updating to this latest driver? I am not planning to use frame gen, is the image quality better or are there any fps improvements in games?",AMD,2025-12-11 13:56:05,1
Intel,nti5u4n,"Updated to 25.12.1 now, before I was on 25.8.1, have a Rx 6800 XT and a Ryzen 7 7700X. Also updated my Chipset-Driver today. Haven't testet much yet, played now for like 1 hour Space Marines 2, watched some Youtube vids since I updated. So far looks ok. Only thing that worried me first was that I found in my Reliability History, 2 critical entries of LiveKernelEvents of code 1a8. But these were written down by Windows on the time, while I was updating my driver. We will see, if anything happens I will keep you updated.",AMD,2025-12-11 18:04:23,1
Intel,ntigzns,"Despite the device ID-based driver update blocking set in August, it has worked until now. The windows tried to install some driver on the 6700XT just now, and unfortunately, it also replaced the software itself somehow. threw an error message too.  Manual update would not go through unless i removed the driver update block.   What a sad situation.",AMD,2025-12-11 18:58:17,1
Intel,ntnwqsn,"Anyone else has problems with CS2/Fortnite? Started happening after i updated drivers to 25.11 My whole PC would randomly freeze for like a minute with the ""AMD software detected that a driver timeout has occurred"" error. Once the PC unfreezes i must kill the game from task manager.",AMD,2025-12-12 16:00:37,1
Intel,nto9zy8,Does it fix the arc raiders dxgi crash of the previous driver?,AMD,2025-12-12 17:05:14,1
Intel,ntpp1wj,"How do I downgrade from this driver?   Iâ€™ve tried four different older drivers and all of them give me error 182 â€“ GPU is not supported (RX 9070 XT) during install.   Iâ€™ve already used DDU and the AMD Cleanup Utility, but the only driver I can install successfully is 25.12.1.",AMD,2025-12-12 21:27:18,1
Intel,ntqd768,pc started to crash 7900xtx... reverted to 25.11.1,AMD,2025-12-12 23:45:59,1
Intel,ntw41n8,Hi me and other people I know. Also forums and Facebook pages . Have had an issue with the frame gen after 25.9.2 . When they released new features we have all had issues where its drops fps and is completely unplayable. Has this been fixed in 25.12.1 I have 7900 xtx 7800x3d. Friend has 9070xt 9800x3d Both have issues. And im on windows 10 he's on windows 11. I used ddu and tried all settings on frame gen and other settings to fix it. Not to mention the drivers where stutters and lower fps without frame gen. Thanks,AMD,2025-12-13 23:14:13,1
Intel,ntwqp19,"When I enable V-Sync in the game, I experience lag; it only runs smoothly with V-Sync enabled when I also activate the performance overlay. This problem has existed since driver version 25.11.1.",AMD,2025-12-14 01:37:28,1
Intel,ntytwdj,"I have a second card from amd. And both cards have driver problems. Now I have an rx 9070 xt oc. I don't do any undervolting. Everything is at factory settings including the bios. I had 4 driver failures in 7 hours. What good is FSR if the driver doesn't work? It would be good if you finally solved this problem. I can stand it for a while, but if it continues like this, I'm leaving AMD.",AMD,2025-12-14 12:03:18,1
Intel,nu0egj3,"Wish they would acknowledge the bug where turning on GPU scaling and integer scaling adds more input delay, so for example the mouse movement will feel sluggish.  Been having this issue for 3 months now since nya bought a a 9070 XT",AMD,2025-12-14 17:38:40,1
Intel,nu0h263,"On the RX 7600S graphics card, Adrenalin does not launch at all, and during installation it removed the driver PCIVEN_1022&DEV_15E2&SUBSYS_15131043&REV_60.",AMD,2025-12-14 17:51:39,1
Intel,nu2b8e5,"How are those with a Cezanne CPU supposed to install this?  Selecting the 5750G from the drivers download page offers 25.21.1, yet none of the 3 variants of the installer support it.  * whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe (Vega, supposedly?) - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe (combined? ""Systems with RDNA series graphics products"") - nope  Each of them return Error 182.  Even the minimal web installer, amd-software-adrenalin-edition-25.12.1-minimalsetup-251207_web.exe, only offers 25.8.1.  VEN_1002&DEV_1638 is nowhere to be found in the .inf for any of the 25.21.1 variants.",AMD,2025-12-14 23:19:40,1
Intel,nu3psfe,I'm still hesistant to upgrade on this driver until they resolve these driver timeouts hell I'm even on 25.9.1 still experiences time to time TDR's.,AMD,2025-12-15 04:22:56,1
Intel,nu4kg83,"Is it worth for my 9060XT to go from 25.11 to the latest, Im having some problems where ghost of tsushima crashes.",AMD,2025-12-15 08:42:46,1
Intel,nu51zrq,Driver is making valorant run like crap for me idk why .,AMD,2025-12-15 11:34:40,1
Intel,nv057ke,im using an rx6600 and up until today i was fine avg 200fps on r6 today the game says its at 22 usaeg when it avgs 1-4 and now it has major fps drops/tears,AMD,2025-12-20 08:25:06,1
Intel,nv2wvfi,This driver constantly crashes call of duty for me. Whatever windows update installs(which seems to be 25.10. something) is the most stable there is. 9070XT.,AMD,2025-12-20 19:41:18,1
Intel,nvtvg24,Anyone see if this fixes the issue of the graphics sliders not working at all and being stuck?,AMD,2025-12-25 05:17:11,1
Intel,nw7hqmm,Still crashes. They will never fix it. Just buy nvidia or intel.,AMD,2025-12-27 16:03:55,1
Intel,nw7j7uy,"u/AMD_Vik It says ""Intermittent application freeze when using the in-game Radeonâ„¢ Overlay."" in fixed issues but I've actually had my whole system lock up because of what seemed to be adrenalin having issues with the performance overlay....  I noticed one thing that pointed towards the overlay specifically: I was going through Adrenalin and when I was on the recording tab I switched to performance; it seemed like Adrenalin froze so I clicked Smart Tech. to see if it would respond.  Initially it didn't, before eventually swapping to the smart technology screen. I then went back to the record tab and tried again: same results.  That's about all I've got for specific steps. I closed Adrenalin and went back to doing whatever and I noticed my fans turned on and like two minutes later when I went to close my browser my cursor stopped before I got to the corner of my screen and I needed to hard power down my system.  Not sure if this is at all related to that issue. But i had it happen on the last driver as well, and came here trying to see if there was a known issue...",AMD,2025-12-27 16:11:24,1
Intel,nxehl3e,Any chance to support VR HP reverb G2 (WMR) 60hz mode with Oasis driver? I'm locked in win10.,AMD,2026-01-03 07:42:05,1
Intel,nxhczy3,"Indiana Jones crashing every 5 minutes, cant complete the game. Its just freezes and the PC barely responsive with these Timeout messages.  9070 with 5800x3d",AMD,2026-01-03 18:31:17,1
Intel,nymudet,"czeÅ›Ä‡, jest sen instalacji jak uÅ¼ywam 7900xtx i nie uÅ¼ywam Å¼adnych wspomagaczy ?",AMD,2026-01-09 17:45:31,1
Intel,nzbmbw2,"Hi devs!   I would like to bring again to your attenction this thread: [AMD Software: Adrenalin Edition 25.9.2 Release Notes : r/Amd](https://www.reddit.com/r/Amd/comments/1nk9qgo/comment/nfb2o2j/)  Is there any chance you can bring us 60Hz mode for ex WMR drivers, now working directly in steamVR with Oasis Drivers?",AMD,2026-01-13 08:53:49,1
Intel,o0k4npb,So uh... I just had an application crash on Blue Gate while playing Arc Raiders... I don't think it is fixed...    9070xt with a 7800x3D,AMD,2026-01-19 22:19:09,1
Intel,ntakvhf,FSR Redstone support? Will my minecraft machine run faster now?,AMD,2025-12-10 14:26:09,1
Intel,ntap40e,Gonna be able to play modern titles on my HD5750 thanks to redstone !,AMD,2025-12-10 14:49:52,1
Intel,ntaqgkm,"Ray Caching in 40K?  Not sure how they got this to work on the tabletop in real life but sounds awesome  In all seriousness there are a large number of games in the Warhammer 40K universe, any chance they are saying which one?  Space Marine 2 Darktide Battlesector   Etc",AMD,2025-12-10 14:57:02,49
Intel,ntavwoz,"so pretty much nothing for today, shrug...",AMD,2025-12-10 15:25:34,11
Intel,ntapnnp,Is there a partial list of the 40 games with the new frame gen? Is it something different from the fg we already have?,AMD,2025-12-10 14:52:46,8
Intel,ntam71a,they wont,AMD,2025-12-10 14:33:37,57
Intel,ntf1fzq,"It's so annoying.  I would keep it if it didn't constantly pop up trying to get me to install ""AMD Chat"" and ""AMD Privacy View"".  I don't want your shovelware AMD, take a hint.",AMD,2025-12-11 04:56:41,22
Intel,ntbob9s,"There should be an option during install to exclude it, it can't be that hard to do. Same as you, u/MihawkBeatsRoger , I also uninstall it afterwards.       Notifying u/AMD_Vik",AMD,2025-12-10 17:45:29,17
Intel,ntasnol,"This.   Why I took it out are my own reasons and quite frankly, irrelevant. It's my PC and I don't want it. So please AMD, listen to me and keep it off.",AMD,2025-12-10 15:08:39,22
Intel,ntb2rjk,"Focus on serious matters, this is a joke. If you do not want it feel free to install the driver only version, and be happy u have that choice. If you want the full features of adrenalin, well install manager is one of them.",AMD,2025-12-10 15:59:33,-1
Intel,ntapviv,It's a rebranding of the entire FSR ecosystem. What's new today is machine learning enhanced frame generation for RDNA4 cards. You can enable it in the driver for any game with FSR 3.1.4 or newer.,AMD,2025-12-10 14:53:56,131
Intel,ntb2cv7,It adds denoising for Path tracing. In theory it should look way better now,AMD,2025-12-10 15:57:34,7
Intel,ntap7sr,All the games that don't use bluestone,AMD,2025-12-10 14:50:26,25
Intel,ntbfl85,"Only one , the new call of duty ATM. So if you enjoy shitty games , have at it",AMD,2025-12-10 17:02:21,4
Intel,ntcmrfi,9070xt i see brave or discord freezing and lagging when watching a YouTube video still. I dont understand how hardware acceleration bug hasn't been fixed yet. Wtf are they doing.,AMD,2025-12-10 20:34:27,14
Intel,ntcuvcq,Yup same here. Had to roll back to October to fix again,AMD,2025-12-10 21:14:37,8
Intel,ntem0sw,25.9.1 works on my 9070 XT. Everything after that is a mess for me,AMD,2025-12-11 03:11:43,6
Intel,ntfe4wd,Tagging u/AMD_Vik  so they are aware of the issues.       I encountered the same problems on my 6800xt. Figma on chrome is causing random BOSD. The system will just restart without notice. Every single web app seems unstable on my system and memory usage is all over the place. Rolling back to 25.9.1 doesn't fix everything but it eliminates 70% of the issues..,AMD,2025-12-11 06:40:43,4
Intel,ntf2h1v,Oh well. :/  Funny thing is I rebooted my PC again for a Windows update. The first thing that greeted me after opening a web browser was the driver giving up the ghost.  On 25.11.1.,AMD,2025-12-11 05:04:26,2
Intel,nvesl6s,Ive been wondering what this seemingly random crashing has been. Thanks for this comment!,AMD,2025-12-22 18:30:18,2
Intel,ntgomie,"9800x3d, 6950xt, no issue with either chrome or discord or firefox with hardware accelerated set",AMD,2025-12-11 13:30:30,1
Intel,ntkqfzv,"Me too.  Installed 25.12.1, whenever I use YouTube in Full Screen, the whole system freezes, while the sound is still audible, then I have to hard-restart my PC. Happened three times, decided to downgrade to 25.11.1 again.",AMD,2025-12-12 02:24:44,1
Intel,ntalgrc,"Same, and I'm still on the October drivers",AMD,2025-12-10 14:29:29,19
Intel,ntaosq5,You can download it from the website. The app release notification always lags behind the site. This is nothing new.,AMD,2025-12-10 14:48:11,8
Intel,ntaon6n,"This should be fixed, I'm not sure why it was omitted from the release notes",AMD,2025-12-10 14:47:21,28
Intel,ntbfx3u,<--- inte 8 rdna3 enjoyer,AMD,2025-12-10 17:04:01,37
Intel,ntbh75k,"How do I set this up, can't find any info",AMD,2025-12-10 17:10:24,1
Intel,ntaukfz,"I can't speak on enhanced sync, but noise suppression is still busted and not working =/",AMD,2025-12-10 15:18:39,17
Intel,ntarxtz,"I'm piggybacking, because I need that info too",AMD,2025-12-10 15:04:52,4
Intel,nwscpi6,"I can't seem to keep framerates under control in a lot of games, generally smaller simpler games, with the new 9070xt. Enhanced sync, vsync, chill, boost, whatever I do I'm still wondering why my pc is at 100% gpu, 600fps, and 300w power draw playing something like minecraft or geometry dash.  Even with a 144hz display. I'd be happy locked at 60 even.",AMD,2025-12-30 19:52:48,1
Intel,ntbjznn,"Been using it for a few hours with the 7900XTX, so far so good.   Hopefully it's 100% fixed.",AMD,2025-12-10 17:24:13,7
Intel,ntbqeln,I hope they fixed it. I will test it now,AMD,2025-12-10 17:55:35,3
Intel,nteci65,"Did the typical test that I usually do and it didn't show up for me and I'm on the RX 7700XT as well. So hopefully, it's fixed.",AMD,2025-12-11 02:13:47,2
Intel,ntb5cx9,"AMD stopped giving a shit about it's fans once the company was saved and they started raking in the money. The change in tone was clear as day. That said, I'll still buy their GPUs because I hate Nvidia far more and I don't see that changing.",AMD,2025-12-10 16:12:17,26
Intel,ntch9q3,"yeah my next one will be Nvidia, better features, better performance espacialy with RT/PT   And apperently longer support... and AMD cards in a simmilar performacne bracket don't even cost THAT much less sooo.... jeah I am mad aswell",AMD,2025-12-10 20:07:01,15
Intel,ntk9244,"I agree. AMD has shown poor support for 7000 series owners. If there was a FSR4 int8 leak, AMD should officially release FSR4 for 7000 series owners.  I bought my 7800xt only 2 years ago before RDNA4 cards came out.  Nvidia provides DLSS4 upscaling to their older generations like rtx3000 series",AMD,2025-12-12 00:39:32,3
Intel,nw3y7cj,"Your system is almost exactly like mine, did you also have crashing problems while having the Xbox Gamebar DVR feature turned on? I would have constant driver timeouts until I turned it off.",AMD,2025-12-27 00:12:39,1
Intel,ntaoqmx,"If you're referring to the app crashes with RTPT reflections enabled, we're working with CDPR on a fix",AMD,2025-12-10 14:47:52,60
Intel,ntcukk4,Signed /another 7900xtx user,AMD,2025-12-10 21:13:09,17
Intel,nu3780v,"I came over from NVDA last March, bought a 7900xtx, RMAd it a few weeks ago due to pink/purple pixelation that would randomly happen. Now it's non stop driver timeouts and random performance issues every time I boot my PC or games. I am never buying another AMD card. I'd rather get ripped off by NVDA and not have constant headaches.",AMD,2025-12-15 02:23:41,1
Intel,nugitp7,"Which driver are you currently on? I'm just curious; personally, I'm on 25.9.2, and surprisingly, I have 0 problems, unlike with previous versions. Should I try 25.12.1?",AMD,2025-12-17 04:25:49,1
Intel,ntapar1,"Nope. Generally if the driver does not massively increases performance in some game, or you don't have any issues or the issue you have isn't fixed, then it's not worth updating, unless there is some new feature you want.    I reverted back to 25.9.1 (from the top of my head) because with any newer driver BF6 crashes randomly, and neither DICE nor AMD seem to give a damn about it.    And before someone asks, I tried any other fix on the internet for Battlefield and nothing else worked.",AMD,2025-12-10 14:50:52,19
Intel,nte7fbw,Same here. Anything above 25.9.2 crashes ray tracing games like Silent Hill  2 and Oblivion Remastered.   Ihr never had a more crash prone GPU than the 9070XT.,AMD,2025-12-11 01:43:09,3
Intel,ntc8k09,"Try this, taken from another comment branch https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 19:22:53,1
Intel,ntissbw,Might potentially be fixed by a recent Windows update?  24H2 (and an earlier mini-patch that included this) apparently resolved a lot of crashing for folks.  See [here](https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/),AMD,2025-12-11 19:57:19,2
Intel,ntaohu1,"Yeah I was really hoping they'd have got buy in from a decent number of devs with updates to big RT showcase games like Indiana Jones, Alan Wake 2, Cyberpunk, etc. But Black Ops 7 and Warhammer 40K... and that's it (for the RT features)?",AMD,2025-12-10 14:46:32,9
Intel,ntb73f1,5070ti fs  basically a better 9070xt,AMD,2025-12-10 16:20:42,13
Intel,ntcro7s,"Iâ€™d wait on 5080ti with more VRAM but these are going to be obscenely expensive knowing nvidia + current RAM prices. Both 5070ti or 5080 are more of a sidegrade than upgrade, not worth the hassle IMO.",AMD,2025-12-10 20:58:42,2
Intel,nted84w,Get a 5070ti. I never thought I would say that. But this is what is is.. 9 months after release and the drivers are still D.S.,AMD,2025-12-11 02:18:08,2
Intel,ntb6h5d,What about a secondhand 5070ti?,AMD,2025-12-10 16:17:43,3
Intel,ntbx8qa,"I mean, I wouldn't get either. 5070 TI is a sidegrade from the XTX, and 5080 is only slightly better. DLSS and RT would be the only reason.",AMD,2025-12-10 18:28:25,2
Intel,ntbzcmw,Sidegrading for an upscaler sounds like a joke.,AMD,2025-12-10 18:38:32,2
Intel,ntam4ba,"I think Linux developers are doing some experiments As of now, FSR 4 (FidelityFX Super Resolution 4) does not officially support RDNA 2 or RDNA 3 GPUs, even on Linux. However, thanks to Develerâ€™s work on VKD3D-Proton 3.0, there is partial and unofficial support for RDNA 3 under specific conditions.  RDNA 3: Partial Support via Develerâ€™s VKD3D-Proton  - Develerâ€™s VKD3D-Proton 3.0 includes support for FP8 (8-bit floating point), which is required for FSR 4. - This means RDNA 3 GPUs (like RX 7600, 7900 XT/XTX) can run FSR 4 in some games via Proton, even though AMD doesnâ€™t officially enable it. - Global override toggles in AMDâ€™s 25.9.1 driver can bypass the FSR 4 whitelist, allowing it to run in FSR 3.1-compatible games.  I hope they succed it will be a slap in the face.",AMD,2025-12-10 14:33:11,27
Intel,ntakqc7,This has been announced for months.,AMD,2025-12-10 14:25:21,28
Intel,ntazxem,Yeah AMD refusing to port features to any card released before the 9 series makes supporting them really hard.,AMD,2025-12-10 15:45:42,8
Intel,ntakt3q,Say thanks they haven't demoted 7000 series to only game drivers,AMD,2025-12-10 14:25:47,9
Intel,ntamwc6,"Your best case is your RX 7900 turning into Balsamico, whatever that means.",AMD,2025-12-10 14:37:39,1
Intel,nte0i5m,"Its because RDNA 4 added hardware that 3 and 2 don't have. Now before I get kicked to death by angry people, there is a version of FSR Redstone that uses and INT8 path that is compatible and will work on 2 and 3, however that has not been launched today and AMD have not confirmed it will be.   That isn't to say they won't do it, but right now it's not been announced. Perhaps there will be enough noise to get AMD to change their mind or it might be that they want to get it out on their latest cards first before complicating matters with older RDNA support.  Only time will tell",AMD,2025-12-11 01:00:33,1
Intel,ntaoydb,"Bro the AI accelerators completely got revamped, upscaling technique isn't usually the indicator for 'fine wine', it is when non-upscaling raw performance numbers improve.",AMD,2025-12-10 14:49:01,1
Intel,ntf8us3,Same boat here. Tired of trying.,AMD,2025-12-11 05:54:50,1
Intel,nte6mdt,Thanks for testing. Have you perhaps tested Oblivion Remastered?,AMD,2025-12-11 01:38:14,1
Intel,ntf8lpl,Finally fixed! It's a christmas miracle!!!,AMD,2025-12-11 05:52:44,6
Intel,ntf2wry,I regret getting this 7800xt,AMD,2025-12-11 05:07:41,2
Intel,ntazn6o,Any card released prior to the 9 series.  Amd could give 2 shits as they chase the AI bubble (jokes on them if I was an exec I'd double down on the consumer market to insulate from the impending bubble burst),AMD,2025-12-10 15:44:19,18
Intel,ntaw82d,sadly,AMD,2025-12-10 15:27:12,6
Intel,ntedpok,"Yep, I go back between 23.9.1 and 25.9.2. I couldn't be happier.",AMD,2025-12-11 02:21:03,2
Intel,ntnjpo8,"If it's any consolation, I was on an NVidia card for 2+ years where I wasn't getting the DLSS updates. Then they actively removed features when they went to the NVIDIA app.  Looking at AMD's roadmap, RDNA4 looks like a stopgap anyway until RDNA5 (prob will be called UDNA?) comes out. So in another year and a half I'll be in the same situation with my 9060XT.",AMD,2025-12-12 14:55:34,2
Intel,ntbhjqe,"Use OBS, replay buffer",AMD,2025-12-10 17:12:09,3
Intel,ntg1daf,Was just thinking of giving a shot for Indiana Jones and the Great Circle - I guess not anymore since FSR4 doesn't work with it..,AMD,2025-12-11 10:29:39,2
Intel,nuji470,"That was a terrible driver for me also. New one has been night and day improvement, give it a shot.",AMD,2025-12-17 17:12:44,1
Intel,ntasabf,"Microsoft had bugs also causing hanging crashes. Everyone loves to blame GPU drivers immediately, but check this out:  https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",AMD,2025-12-10 15:06:41,22
Intel,ntars3b,I also want to know this.,AMD,2025-12-10 15:04:00,3
Intel,ntasr85,"I'm wondering the same thing, 25.11.1 is still the most stable for me!",AMD,2025-12-10 15:09:10,3
Intel,ntbzovt,Wondering too. I bumped back down from 25.11.1 because it was unstable on my machine.,AMD,2025-12-10 18:40:09,2
Intel,ntf7xwz,Stay on 25.11.1 if you are on RDNA 1 or 2,AMD,2025-12-11 05:47:16,2
Intel,ntgynxw,squash hard-to-find sharp reach memorize fade husky divide subsequent plough   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:28:31,1
Intel,ntlwtqy,"~~It messed up my audio, now everything sounds 8-bit. If you're on RDNA4, avoid this update.~~  EDIT: it's not the drivers, after much tinkering I was about to deduce that it was my monitor. So it should be ok to update",AMD,2025-12-12 07:15:44,1
Intel,ntelprh,I can't use anything above 25.9.1 on my 9070 XT,AMD,2025-12-11 03:09:46,2
Intel,nth7dkb,forget it they gave u the middle finger move on fuck both amd and nvidia,AMD,2025-12-11 15:15:41,3
Intel,ntb6lhi,wonâ€™t happen,AMD,2025-12-10 16:18:18,2
Intel,ntbt1wv,"Fine wine is only a thing for very few and specifics types of wine, typical wine still goes bad over time.",AMD,2025-12-10 18:08:16,5
Intel,ntbeh2e,What is the source for this or is it trust me bro?,AMD,2025-12-10 16:56:49,4
Intel,ntecwqr,They should just remove this feature. It never worked from day 1..,AMD,2025-12-11 02:16:12,1
Intel,ntbm91h,what is the difference,AMD,2025-12-10 17:35:20,1
Intel,ntcnmrm,"Can you tell me if this is also applicable to 25.12.1? There are several (frustratingly unlisted) VR-specific fixes aligned, one of them closely relates to what you've just described",AMD,2025-12-10 20:38:48,2
Intel,ntekn1e,Same here. 25.9.1 makes my problems go away,AMD,2025-12-11 03:02:56,1
Intel,ntfr6xt,Same for my 9070 XT. Device hung error,AMD,2025-12-11 08:46:36,3
Intel,nu0gqvz,"Thanks for reporting, had that once with 25.11.1 + 9070XT (W10) before reverting to 25.9.1 (since then, it never reappeared).",AMD,2025-12-14 17:50:07,1
Intel,o155b38,Thank you for reaching out.   That's really weird - I don't suppose you have any links to posts about this for us to skim through?  I'll follow up with my colleagues about this tomorrow,AMD,2026-01-22 23:45:38,2
Intel,ntoma7o,Do you get a firmware update pop up? Is this one?  https://i.redd.it/w46j86mnct6g1.gif,AMD,2025-12-12 18:06:40,1
Intel,ntmvi3j,"I'm familiar with this impacting United Offensive, I don't believe we're reintroducing this old vendor specific extension, however. I do have a ticket for the performance issues though; I don't believe this is related to the missing extension.",AMD,2025-12-12 12:32:54,2
Intel,nu65nki,"Tested for 2 days(1day and 22hrs uptime)  No crash, No BSOD for me so far. Nothing strange.  MS Edge, Google Chrome video playback, youtube...etc all play nice while gaming on main monitor.  Diablo 4, MSFS 2024, Doom dark age, Forever winter(UE5), Witchfire(UE4)...etc All run fine.  Lossless scaling runs fine on spicy vids to all of the above games xD  HWinfo64 and MSI Afterburner, RTSS all run as they should.  (Win11 25H2 uptodate, X670E, igpu(98x3d)+7900xtx+6400 3gpus, 2 monitors, hybrid mode)  Edit) rx 6800 + r7 7700x on win11 25H2, X670E, Single monitor, igpu-disabled -> runs fine.  rx 6700xt + i7 8700k on win11 25H2, Z370, Single monitor, igpu-disabled -> seems good.",AMD,2025-12-15 15:39:46,1
Intel,nuizppc,"25.11.1 had pink artifacts glitch on chromium browsers with 7700 xt but i installed 25.12.1 yesterday and no issue so far, i did not see artifact pink glitches or sound issue so far ?",AMD,2025-12-17 15:42:58,3
Intel,ntemk82,My 9070 XT hate every driver above 25.9.1,AMD,2025-12-11 03:15:11,1
Intel,ntcw96j,I updated to this driver and immediately got a BSOD. Rolled back to October 25.10.2 again,AMD,2025-12-10 21:21:24,5
Intel,ntgzi2n,offer steep theory scale straight obtainable physical ad hoc selective test   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-12-11 14:33:06,1
Intel,ntc7ylk,thats what I wonder too! Is it more stable??,AMD,2025-12-10 19:19:58,3
Intel,ntb2i9s,haha r u fr,AMD,2025-12-10 15:58:18,8
Intel,nteeogf,la mÃªme. C'est scandaleux,AMD,2025-12-11 02:26:49,2
Intel,ntbcqw3,"Means that they've created separate driver packages tailored for the specific gens (A rdna1/2, B for RDNA3/4, C - combined fat package that contains both drivers for systems that might have both gens on the same machine (igpu + dgpu) )",AMD,2025-12-10 16:48:24,4
Intel,ntb2cwp,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-12-10 15:57:35,1
Intel,ntcubg5,"Cleanup utility first, always!",AMD,2025-12-10 21:11:55,3
Intel,ntbx46o,I also wanna know best way updating drivers. DDU kinda annoying but maybe must be done i don't know,AMD,2025-12-10 18:27:48,1
Intel,nte7te5,It's doing it there too.,AMD,2025-12-11 01:45:34,1
Intel,ntcm462,Never seen any crashes on it with latest driver prior to today 9070xt w11,AMD,2025-12-10 20:31:13,1
Intel,ntctang,"Just tested it tonight, and for me it's working fine, 9060xt here, windows 11 with the latest update, although i play with the ""medium"" preset which disables ""lumen"", can't say it might work for you but you can try it if it still crashes constantly",AMD,2025-12-10 21:06:50,1
Intel,ntdql6b,Might want to check [https://www.reddit.com/r/radeon/comments/1pjeonb/fyi\_fsr\_ml\_framegen\_requires\_windows\_11/](https://www.reddit.com/r/radeon/comments/1pjeonb/fyi_fsr_ml_framegen_requires_windows_11/) :|,AMD,2025-12-11 00:02:35,2
Intel,ntltbvr,nope. I still crash,AMD,2025-12-12 06:44:56,1
Intel,nujivd3,"I fixed my arc raider crashes (mostly in blue gate map load) by running DDU, installing 25.10.1 (down from 25.11.1), and deleting shader caches (dont know if the shader cache delete helped or not). I upgraded to the newest drivers the day after they released and haven't had a single crash since in arc raiders, including w overlay.",AMD,2025-12-17 17:16:27,1
Intel,nts13sv,"for me, DDU in safe mode, disconnect internet, install 25.9.1 fine for me(9060XT).  I've tried 25.10/ 25.11 and revert back to 25.9.1 with this way. Now observing 25.12",AMD,2025-12-13 06:52:17,2
Intel,nujhigl,Yes. I downgraded from 25.11.1 because of the crashing. Now been on 25.12.1 all week and havent had any issues come up. You also get proper fsr4 upscaling now.,AMD,2025-12-17 17:09:46,2
Intel,nv4w9xy,"I just want to say I think I found the culprit. It also happens on the winupdate one too, because it started crashing all the time.  Core clock boosts itself WAY past what it is declared on the card(I got a Sapphire 9070XT Nitro, supposed to be 3060MHz). Here's the moment before it crashes to a black screen:  [afterburner screenshot](https://i.ibb.co/3YpJFtzM/Screenshot-2025-12-21-030537.png)  The dip in clocks is the moment it crashes. As you can see, it is running well above boost clocks. Hence, freezing in a few minutes, proceeded by a black screen, and a crash. The ups and downs are from me alt tabbing in the graphs, by  the way.   This is with core clock -200mhz applied in Afterburner and no crashes, boosts to just above declared boost clocks. Here the dips in up and down on power are probably me toying around how much exactly -mhz is needed.  [afterburner -200mhz](https://i.ibb.co/YTQfGJtc/11111.png)  All of the crashing behavior so far is replicable in COD, CS2, Cronos New Dawn.  u/AMD_Vik",AMD,2025-12-21 02:43:00,3
Intel,nw7st51,thanks for reaching out - funny timing; I noted that on the internal ticket for this issue yesterday having seen other accounts of end users noting this issue persists even with 25.12.1. Perhaps the fix aligned to that point release somehow slipped.,AMD,2025-12-27 16:59:39,2
Intel,ntbhx4e,I still have my 5670,AMD,2025-12-10 17:14:01,2
Intel,ntazsxe,Itâ€™s for Darktide apparently,AMD,2025-12-10 15:45:06,23
Intel,ntauw3f,any game with fsr 3.1 fg also has the new fg since drivers override it. itâ€™s also why they stopped versioning fsr. any game with fsr 3.1 should just automatically have any new version of fsr when the drivers update,AMD,2025-12-10 15:20:21,11
Intel,nthk5bz,I forgoed any amd software entirely  Use more clock tool  10x better with 0% of the bloat   ^^ helped me get my 4th in world furmark score (7900xtx user),AMD,2025-12-11 16:18:42,2
Intel,ntbig0n,If you want to be in control of whatâ€™s on your computer then Windows is not the OS for you,AMD,2025-12-10 17:16:36,18
Intel,ntbjons,"Dumbest take one can have, since installing only the driver won't let you manage the settings at all.  Which has nothing to do with this useless launcher no one wants or needs.",AMD,2025-12-10 17:22:43,16
Intel,nteh1sb,Found the install manager dev lol,AMD,2025-12-11 02:41:05,3
Intel,ntaqzke,Thanks.,AMD,2025-12-10 14:59:49,15
Intel,ntb7o1t,Unfortunately Redstone FG is bugged with poor frame pacing,AMD,2025-12-10 16:23:28,19
Intel,ntaqis1,Nice to see the innovation continuing on,AMD,2025-12-10 14:57:21,18
Intel,ntbic15,But only on the 9060 and 9070 right?,AMD,2025-12-10 17:16:03,1
Intel,nte60vn,I remember this mentioned since the  GCN 1.0 days. Lol,AMD,2025-12-11 01:34:30,8
Intel,ntfp000,"On my end, the driver crashes. Most of the time it manages to recover (sometimes it will crash a few more times before stabilising). Sometimes it doesn't recover (leaving only 1 of my monitors working), so I had to reboot. Then after rebooting, strong chance it'll crash again the moment I open my browser.",AMD,2025-12-11 08:24:09,3
Intel,nzzy88u,Oh thank god it's not just me.,AMD,2026-01-16 21:39:12,1
Intel,nuur9u6,"My experience with switching to amd was so smooth and perfect until 25.9.1. Everything after that just caused stutter issues in games, programs randomly crashing, drivers crashing completely causing my pc to reboot, this is so sad i hope they fix this soon and bring back a stable version asap. Rolling back to 25.9.1 now aswell until that happens.",AMD,2025-12-19 12:46:06,3
Intel,ntwnl8a,"\+1 on this. Most games crashed drivers with any newer drivers except 25.9.1, but poe2 i cant play with vulkan or Directx 12 only with Dx11",AMD,2025-12-14 01:17:11,2
Intel,nv8ptlv,pÅ™esnÄ› zustÃ¡vÃ¡m na 25.9.1 vÅ¡echno jinÃ© crash,AMD,2025-12-21 18:59:03,1
Intel,nw3xh0a,"I had been having the absolute worst time with drivers when I first bought my 7600XT, but finally found stability with 25.8.1 (and turning the Xbox Gamebar DVR off...) but I'm so paranoid now to update my drivers again. The only reason I decided to check on updates now though is a sudden appearance of my screen flashing black at random times.",AMD,2025-12-27 00:08:20,1
Intel,nth6kuu,Are you able to tell us what the error code is on the BSOD? I don't suppose you have a kernel memory dmp pertaining to one of these failures over at      C:\Windows\MEMORY.DMP,AMD,2025-12-11 15:11:30,5
Intel,ntaoqu8,Yeah same,AMD,2025-12-10 14:47:53,2
Intel,ntapwco,"Remember when you could click ""Check for Update"" inside the AMD Software and if there was an update, it would download and install it for you?  Glad they fixed that awful experience, and we have the Installation Manager now.",AMD,2025-12-10 14:54:03,27
Intel,ntap5oq,Thanks will give it a try after I finish work,AMD,2025-12-10 14:50:07,9
Intel,ntchncg,"Wait, AMD Customer Support told me that 2 monitors connected to iGPU and dGPU has never been officially supported and that this configurations breaks performanceâ€¦ so they told me bullshit?",AMD,2025-12-10 20:08:56,1
Intel,nte3vcl,Any update on three Oblivion Remastered and Silent Hill  2 Remake crashes? A lot of us are still with the September drivers because of them.,AMD,2025-12-11 01:21:16,1
Intel,ntcb9cq,<--- Ditto,AMD,2025-12-10 19:36:20,5
Intel,ntbpv70,Optiscaler lets you inject it. Do not use in multiplayer games though.,AMD,2025-12-10 17:53:00,3
Intel,ntauof3,it cannot possibly be this difficult to fix when thereâ€™s already community workarounds,AMD,2025-12-10 15:19:15,10
Intel,ntb6tpy,both are still broken somehow,AMD,2025-12-10 16:19:24,1
Intel,nwsjipr,running at 600 fps with vsync on means that somethingâ€™s terribly wrong with something in your software thatâ€™s breaking vsync. thatâ€™s definitely not normal,AMD,2025-12-30 20:25:38,1
Intel,ntcnloa,I did some testing AND as far as I can tell I do think it's actually fixed finally,AMD,2025-12-10 20:38:39,5
Intel,ntbd1ml,I would continue buying their GPUs if they gave me something to buy.  The XTX has no upgrade path on RDNA4.,AMD,2025-12-10 16:49:51,20
Intel,nteixfg,"I had Nvidia for years, the main reason I switched was that the drivers went to shit last year. I'm just sick of them in general, too. The 7800 XT I bought has been one of the most trouble free cards I ever had, aside from Adrenalin randomly closing in certain versions.",AMD,2025-12-11 02:52:24,2
Intel,ntdc84n,"If I could get my hands on a 5070 Ti Iâ€™d happily switch. AMD likes to take advantage of the underdog, for-the-people image whenever itâ€™s convenient but theyâ€™ll just as quickly throw us under the bus and fuck us raw once theyâ€™ve got the bag.  Is Nvidia a gang of greedy fucks? Sure. But at least the bullshitâ€™s right out front where you can get a good strong whiff of it. You know what youâ€™re in for.",AMD,2025-12-10 22:41:15,3
Intel,ntm5vgi,"I purchased a 7700 XT and a 7600 8gb I'm March and while I'm satisfied with performance, it would definitely be awesome to have FSR 4 on both cards as FSR 3 and 2.2 (overwatch )leave alot to be desired",AMD,2025-12-12 08:42:08,2
Intel,ntapkhc,It's been so long bro :( Hopefully the fix comes with ray regeneration support?,AMD,2025-12-10 14:52:19,24
Intel,ntbfm3b,"Hey Vik, is there any info for FSR4 Vulkan support?  It's quite sad to see that there still isn't support for it as it has been 9 months by now since the release of the 90 series  Also is there any info about the EAC issue with Star Citizen and the latest drivers?",AMD,2025-12-10 17:02:29,15
Intel,ntc52w2,"Amd Noise Supression doesn't work, when I try to turn it on, nothing happens, but in 25.9.1 it works",AMD,2025-12-10 19:05:49,8
Intel,ntcc596,"Hey amd\_vik is amd Aware of the 1 year on going Darktide issues with amd  ( GPU , and specially X3D cpus ? ), and that even the Dev of Darktide ( Fatshark ) seemingly gets ghosted by amd ?  heres some more info specially first links includes a few Dev comments  [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)  [https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f](https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f)  [https://forums.fatsharkgames.com/c/darktide/performance-feedback/97](https://forums.fatsharkgames.com/c/darktide/performance-feedback/97)",AMD,2025-12-10 19:40:50,7
Intel,ntdbffr,"Vik, weren't you on holiday leave? xd",AMD,2025-12-10 22:37:02,4
Intel,ntc7hrz,Any fixes for the SecondLife issues we've had the last few months? last driver that didn't break textures was 25.9.1,AMD,2025-12-10 19:17:39,2
Intel,ntbwr0w,Will this update fix some of the artifacting Iâ€™m seeing in cyberpunk with fsr enabled?,AMD,2025-12-10 18:26:01,1
Intel,ntcji37,Also getting driver timeouts in Cyberpunk with RDNA3 with raster or RT. I did not have these problems with my RDNA2 card.,AMD,2025-12-10 20:18:15,1
Intel,ntcuy8r,"The AMD FSR ML-based Frame Generation option in the Radeon panel disappears in Windows 10.  So I have a question: Is ML-based Frame Generation no longer usable in Windows 10? This option is available in Windows 11, but not in Windows 10.",AMD,2025-12-10 21:15:00,1
Intel,ntdcmj2,Can I join if mine's just an XT?,AMD,2025-12-10 22:43:23,1
Intel,ntawh67,What GPU are you using?,AMD,2025-12-10 15:28:30,2
Intel,ntfkwf8,Try reinstalling Windows. That fixed it for me.,AMD,2025-12-11 07:43:52,1
Intel,nte7o94,"This doesn't work. We are talking about games that crash with or without it, the only difference being the older AMD driver working.",AMD,2025-12-11 01:44:41,4
Intel,ntc9ed0,I already install the latest update before update drivers its not update related. Vulkan driver is the problem in indina jones and silent hill 2 after windows update 25.11.1 not crashing ray tracing enabled but in 25.12.1 its broken again. So driver is the problem...,AMD,2025-12-10 19:27:04,2
Intel,ntamwm4,"They said earlier in 2025 they were working on FSR 4 support for RDNA 3, and then it leaked in September with the INT8 version...",AMD,2025-12-10 14:37:41,11
Intel,ntal44u,"They might as well have lol, they aint getting no new features",AMD,2025-12-10 14:27:29,15
Intel,ntbssdw,They also promised features to the few of us who bought 7900 XTX. Good luck defending them when it's your turn to be disappointed.,AMD,2025-12-10 18:06:58,2
Intel,ntbim9d,I expected them not to abandon their king card lmfao. Who does that,AMD,2025-12-10 17:17:26,2
Intel,ntar1pe,"Not really, they teased the possibility of including other architectures.",AMD,2025-12-10 15:00:06,2
Intel,ntimm5h,Maybe next time you should read the whole thread before replying.,AMD,2025-12-11 19:26:09,1
Intel,ntaqnxp,"It's also related to getting new features in generations other than just the latest one, ""bro"".",AMD,2025-12-10 14:58:07,1
Intel,nthyzs0,"I have the 7800 xt hellhound i F love it, tbh i care less about this redstone thing but its frustrating why a 2 year old lineup is abandoned all of a sudden",AMD,2025-12-11 17:30:59,2
Intel,ntbkahh,"> I'd double down on the consumer market to insulate from the impending bubble burst  If that bubble bursts nobody is going to have much money to spare for consumer goods. That bubble bursting will tank the entire economy along with it.  *Long* term that might work out better, though.",AMD,2025-12-10 17:25:41,3
Intel,ntbdi9g,further reminder amd is not your friend sadly,AMD,2025-12-10 16:52:07,9
Intel,nth472g,Same for me but Doom Eternal. I play at 4k and it needs upscaling at that res.,AMD,2025-12-11 14:58:42,1
Intel,ntfl1sl,What if I'm on RDNA 4?,AMD,2025-12-11 07:45:19,1
Intel,ntheoob,Yeah there are no good choices,AMD,2025-12-11 15:52:11,1
Intel,ntbmlpj,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html),AMD,2025-12-10 17:37:04,1
Intel,ntcu71s,Adrenalin is for GPUs.   Chipset is for CPU & mobo.,AMD,2025-12-10 21:11:19,1
Intel,o15856l,"Most of the conversations happened across various discord servers, so I cannot directly link them, but I also found a few posts here on reddit that seem to point towards the same direction which roughly match with the timeframe.   [https://www.reddit.com/r/Vive/comments/1nl0540/direct\_mode\_not\_working/](https://www.reddit.com/r/Vive/comments/1nl0540/direct_mode_not_working/)  [https://www.reddit.com/r/AMDHelp/comments/1ljezuf/steamvr\_crashing\_on\_amd\_drivers\_after\_2451\_direct/](https://www.reddit.com/r/AMDHelp/comments/1ljezuf/steamvr_crashing_on_amd_drivers_after_2451_direct/)  It's a bit odd because if you set the flag beforehand with a older version and then update to newer versions, Directmode still works fine, but if you fully remove the drivers with AMD's driver removal utility or Display Driver uninstaller (which propably deletes some cached driver file for directmode, is my guess), Directmode disables fully and can't turn on anymore at all.      As far as headsets are concerned, I know at least Valve's implementation seems to suffer from this (I know about cases involving the Valve Index, Bigscreen Beyond and Vive Pro 1, all using Valve's implementation), but somebody in the pimax subreddit community discord seemed to have the same problem, which he also was only able to fix by rolling back the drivers.  I could propably reach out to a few more affected people over discord and encourage them to get active here, if you'd like.  But I do think that it is a fairly reproducable problem, at least on my end, it happened across two different systems.",AMD,2026-01-23 00:00:34,3
Intel,o16r403,"Hi - I am having this problem. I wanted to try the special ROCM driver so dutifully did a clean install as suggested.  Tried to launch SteamVR and it errored. Realised headset had reverted to being a monitor, running at low resolution (less than recommended) Increased the resolution to recommended (combined resolution of the two panels), and SteamVR started detecting the headset - though promoted to enable direct mode. Pressing the button to enable it restarts SteamVR, but then prompts again, and HMD is still a monitor.   Interestingly reverting to the RDNA 4 release driver, it instantly worked - Direct Mode was enabled without me having to toggle.  So it is like SteamVR is successfully toggling something, but the newer AMD drivers ignore whatever it is doing/don't act on it. As soon as older driver is installed, it does act on it, and the HMD disappears from Display Settings/stops being treated as a monitor.  This wouldn't affect the majority of SteamVR users who are streaming to a Quest, and it wouldn't impact people who upgrade to a newer driver without a cleanup.  It would affect people doing fresh installs with a recent driver, and people who do clean driver installs/use DDU.",AMD,2026-01-23 05:17:40,1
Intel,o178o4d,"I installed the latest driver from a couple of days ago, and my Beyond is a monitor again ðŸ˜­   I didn't explicitly choose to do a clean install, but maybe because I chose to install the AI Package, it decided to do a clean install for me automatically?",AMD,2026-01-23 07:37:51,1
Intel,ntpptsg,Yes thatâ€™s the one. I have no idea where to turn lol,AMD,2025-12-12 21:31:29,2
Intel,ntnglvb,Sad news. Nvidia still supporting old extensions.,AMD,2025-12-12 14:38:54,2
Intel,ntbuxj0,"Hey OP â€” Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2025-12-10 18:17:20,1
Intel,ntdymrv,oh wow i haven't had any issues yet but that doesnt mean much. 25.11.1 i didnt have issues for a week or so.,AMD,2025-12-11 00:49:22,1
Intel,ntcmz77,"Interesting, Iâ€™ll test it today. I was crashing non stop on 25.11.1 so hopefully this update fixes it",AMD,2025-12-10 20:35:33,1
Intel,ntlyzyv,Same,AMD,2025-12-12 07:35:41,1
Intel,nv973go,"I have the same model GPU inconsequentially boosting well above the advertised clocks (nearly 3.4GHz) in both windows 10, 11 and fedora 43 with no issues.  This has been discussed several times on this community; whilst the clock behavior may surface other issues or instabilities on the system, it's not in itself the cause of problems.",AMD,2025-12-21 20:28:00,3
Intel,nw7vn10,"I actually have one more potentially related thing for you!   During the game I tried to turn the overlay on using my hotkey. Noticed it didn't. Since I've seen this before (we can call this a ""soft lock"") I tried to open the full screen experience with the hotkey. Which brought up my mouse (was using a controller in game before pressing the keys) but I could not move it...  My workaround has been: ctrl+alt+esc to task manager, tab to the search bar, type ""radeon"" and force kill the host service.  The instance I reported before this was a ""hard lock"" that I've noticed while trying to use my browser over a borderless game running, before this time where it was when the gpu wasn't under any actual load as far as I knew.  Glad to hear it's a known issue and not my hardware though... Thanks for getting back to me!",AMD,2025-12-27 17:14:03,1
Intel,nxaf1nm,Which driver version DOESNT have this issue?   I've tried going back all the way to .10 and it's all having the issue...,AMD,2026-01-02 17:46:29,1
Intel,nylsudn,"Good morning, when will the new AMD Software driver be available?",AMD,2026-01-09 14:55:31,1
Intel,ntc5yzi,Literally the one game I don't play lol,AMD,2025-12-10 19:10:11,10
Intel,ntb8cnz,"Yay, I own that one",AMD,2025-12-10 16:26:50,3
Intel,ntestwe,"It's not even out for Darktide yet either. Fatshark clarified that it's experimental and needs more work, so it's not in the live build",AMD,2025-12-11 03:56:23,3
Intel,ntazo47,So it's under the umbrella of the fsr4 override if I understood this correctly. For the fsr2 and 3.0 games I can use optiscaler right? Sorry I just bought a 9070xt coming from nvidia so I need to get used to these things.,AMD,2025-12-10 15:44:27,3
Intel,ntifqj7,I used to do that but a few games can use the FSR4 in driver upgrade.  The enhanced sync was nice too when it worked.,AMD,2025-12-11 18:52:16,2
Intel,ntbjqio,"Unfortunately I play games and run software that require Windows so I have it on a separate drive. When I do switch to it (and I update the driver to take advantage of new features), this shit typically happens along with a slew of forced updates.  You are right though, I do primarily run CachyOS.",AMD,2025-12-10 17:22:58,16
Intel,nte7ly0,found the linux user,AMD,2025-12-11 01:44:17,4
Intel,ntd5qr8,You're talking nonsense.  Engineer managing 2k endpoints and several hundred servers.,AMD,2025-12-10 22:07:29,4
Intel,ntcmjlp,Wasn't the dude's claim it has been always bugged with AMD,AMD,2025-12-10 20:33:21,6
Intel,ntctlcs,ðŸŒðŸ‘¨â€ðŸš€ðŸ”«ðŸ‘¨â€ðŸš€,AMD,2025-12-10 21:08:21,1
Intel,ntasjqd,It's barely an improvement.,AMD,2025-12-10 15:08:04,12
Intel,ntcmoxo,It's branding,AMD,2025-12-10 20:34:05,1
Intel,ntbkqjy,"Yes, RDNA4 refers to the RX9000 series.",AMD,2025-12-10 17:27:52,3
Intel,o012dhm,Disable mpo,AMD,2026-01-17 01:17:11,1
Intel,ntp6j29,I have one of these captures if you want it (error code 0x00000119). I've been having a TON of driver timeouts and BSOD for the past couple of driver versions and I've had to roll back to October to resolve them. Seems like any app that has hardware acceleration enabled causes it and exasperated when viewing the system via RDP.,AMD,2025-12-12 19:48:47,1
Intel,ntb0ccn,"Uninstalling the install manager brings back the ""check for updates"" functionality until you update again (and have to re-uninstall the install manager)",AMD,2025-12-10 15:47:46,12
Intel,ntap8zv,Let us know how it goes!,AMD,2025-12-10 14:50:37,9
Intel,ntci6s3,"I don't know how much of an impact this could have on perf since it's not something I've measured. I personally wouldn't do this, though. With a dGPU installed I keep iGFX off.",AMD,2025-12-10 20:11:40,4
Intel,nted5dt,"performance wise it should only be a couple frames of latency, when doing rendering on dgpu and going out through igpu it'll just copy over the frame buffers.   Main impact is on pcie bandwidth as it'll use up quite a lot there, and to a smaller degree RAM load, so you definitely don't want to run some other dynamic load on the igpu when gaming to overwhelm its pcie link. I think on 7000/9000 it's x8 so it may be fine? But I'm really not sure could be x4 too",AMD,2025-12-11 02:17:41,1
Intel,nth79az,"We're tracking a failure in silent hill 2 remake, I believe a fix is aligned to a future release. I'll need to check in with oblivion remastered",AMD,2025-12-11 15:15:04,3
Intel,ntdvql1,"Do you have to do that convoluted setup and download the drivers from Limewire, or has Optiscaler wrapped it in to their application?",AMD,2025-12-11 00:32:26,2
Intel,ntbvuyt,"So, no official release... ;(",AMD,2025-12-10 18:21:45,1
Intel,nte1rh2,Any tutorial for a noob on RDNA2?,AMD,2025-12-11 01:08:16,1
Intel,ntbfsb9,what workaround?,AMD,2025-12-10 17:03:21,5
Intel,ntbgebv,"Same issue with fsr4 on rdna1-3.   It shouldn't be this difficult, it's in a perfectly working state made possible by like one guy's few days worth of work.   And yet AMD just doesn't do it...",AMD,2025-12-10 17:06:24,2
Intel,ntbmhrj,FUG,AMD,2025-12-10 17:36:32,1
Intel,nwtucuk,"Oh, definitely not normal for sure... but I have this issue on multiple games and I did not have this issue on the 6080 it replaced. This seems to only be impacting my 9070.",AMD,2025-12-31 00:22:00,1
Intel,ntczm93,"Such a relief, but i am also annoyed because they are ignoring 7000 series... I can literally use FSR 4.0.2 on my 7700XT and it is WAY better than FSR 3.1....",AMD,2025-12-10 21:37:51,2
Intel,ntcztos,I hope it is fixed for me as well ðŸ˜­ðŸ™. Thanks for the info.,AMD,2025-12-10 21:38:50,2
Intel,ntchg2c,yep would have upgraded but with an XTX.... you can cut your vram in 2/3 and have less Raster performance for a good upscaler and better RT performance it's such a stupid fucking problem....,AMD,2025-12-10 20:07:57,4
Intel,ntaq6sy,"That's not something I'm privy to, but it could be worth reaching out to them to request looking into if they're not already.",AMD,2025-12-10 14:55:36,36
Intel,ntbho9v,"I'm not privy to any of the FSR stuff - that's a different team to mine. I can pass on the feedback.  The Star Citizen EAC issue should be addressed, please let me know how it is.",AMD,2025-12-10 17:12:47,24
Intel,nte0zy9,i still am!   so many fixed issues out of the release notes that I felt the need to stick around and help clear things up in the communities I frequent. I'll go back into hiding again soon,AMD,2025-12-11 01:03:33,5
Intel,ntciqi1,"I've seen something like this over at OCUK Forums but weren't given enough data to work with. We've attempted to reproduce a corruption issue but apparently we've not been successful.  Can you give me a step by step breakdown on how to hit this, as well as a clear depiction of the issue?",AMD,2025-12-10 20:14:24,5
Intel,ntqc750,"No, XT peasants needs to form their own group.",AMD,2025-12-12 23:39:51,2
Intel,ntcxhw3,6800XT.,AMD,2025-12-10 21:27:27,7
Intel,ntanrvb,Some of their marketing said they would like to get it working if possible.,AMD,2025-12-10 14:42:33,11
Intel,ntarc4r,"There are already third party options, but it would be nice to see if Steam Machine drives INT8 FS4 support since it runs on RDNA 3 tech. Let's see what happens in 2026.",AMD,2025-12-10 15:01:37,3
Intel,ntbkv5b,Yeah there are going to be serious consequences as major retirement funds have invested in all these AI stocks because they have made so much money.,AMD,2025-12-10 17:28:30,3
Intel,ntflk2b,"Give it a try, for my 6800xt it's crashing in almost all games...  ![gif](giphy|QMHoU66sBXqqLqYvGO)",AMD,2025-12-11 07:50:12,1
Intel,o17bv58,"Scrap that!   I ran SteamVR which caused Steam to crash but restarting it as prompted, my Beyond 2E disappeared as a monitor again - weird!",AMD,2026-01-23 08:06:21,2
Intel,ntsqrgy,"Sorry, out of curiosity, if you close it, it won't let you play? What do you get? Could you send me a photo so I can understand?",AMD,2025-12-13 11:11:24,1
Intel,ntnluq2,"I agree. Please can you raise a ticket requesting support for this over at our GPUOpen and ask other end users and developers to upvote it and leave a comment registering their interest? (please share a link to it here if you do) https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues  As far as I'm aware, the impacted titles are: IL-2 Sturmovik: 1946, Neverwinter Nights Diamond Edition and Call of Duty. If there are any others, I would really appreciate you letting us know.  E: I believe it's posted here: https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues/80",AMD,2025-12-12 15:06:46,2
Intel,nthltl0,"Just an update - I ended up running DDU and re-installing the latest update and now things are pretty stable, no driver timeouts from hardware accelerated apps either. Could be something to do with the architecture change between driver packages - but doing a complete removal between updates seems required now.",AMD,2025-12-11 16:26:46,1
Intel,ntcop7s,I never seen 1 crash on 25.11.1 although I did use the preview update for windows 11 last week which fixed some amd gpu related crashing and that solved my arc raiders random crashing,AMD,2025-12-10 20:44:09,1
Intel,nv9cafd,"My apologies then - it seems latest driver on Windows seems to be the source of issues then, seems more people have issues posting on /r/AMDHelp , also with 9070XT's. Seems all device hung errors and timeouts recently posted are with 25.12.1. I had no issues on cachyOS (Hyprland) running CS2 too, latest amdgpu.",AMD,2025-12-21 20:55:29,1
Intel,nxbs3rv,I believe this was introduced with the 25.20 driver branch. it shouldn't be present in 25.9.1/2,AMD,2026-01-02 21:39:50,2
Intel,nym5qu9,"I think our SVP noted in a recent interview it'll be later in Jan, the date they provided was the 21st, though I'd treat this as a tentative timeline just in case anything crops up",AMD,2026-01-09 15:55:10,6
Intel,ntb0k7p,"yes, 3.1 is where AMD adopted the same modular approach as nvidia so any game at fsr 3.1 or above just runs at whatever latest fsr version your driver supports, which is currently 4 although now the versions aren't numbered anymore",AMD,2025-12-10 15:48:50,6
Intel,ntcauqa,Hell yeah ðŸ‘ðŸ»   Impressive you can run that on a 5x86,AMD,2025-12-10 19:34:18,4
Intel,ntbryby,"Since you're already an advanced user, perhaps you could block it from installing by selectively blocking AMD in your hosts or pi-hole? It's not a dumb solution, but it's better than having to deal with push-installs.",AMD,2025-12-10 18:02:57,3
Intel,nthi3lk,I might be an ass but Iâ€™m not wrong,AMD,2025-12-11 16:08:50,2
Intel,nthi8dc,Sorry  If youâ€™re a **consumer** and want to be in control of whatâ€™s on your computer then Windows is not the OS for you,AMD,2025-12-11 16:09:28,2
Intel,ntfql24,"Yes, If you mean the bad frame pacing when fps is lower.  I still opt to spent 1-200 hrs of my gaming session with FSR 3 frame gen, 7900xtx.  It's not that bad when the output is close enough to monitor max hz, similar to what hardware unboxed did in thier test.  The generated frame still comes out too early but it has to wait for the monitor's nest refresh which is consistent.",AMD,2025-12-11 08:40:22,1
Intel,ntc2hr1,ty,AMD,2025-12-10 18:53:26,1
Intel,ntpa4lm,can you run analyze -v in windbg or fire it over to me via your preferred file sharing method?  I personally like to use https://send.vis.ee,AMD,2025-12-12 20:07:35,2
Intel,ntb1b5l,"u/amd_vik it sounds like this person doesnt want the manager to install again, but I am pretty sure you can do custom option to uncheck it. If you do express of course it will put it back sschuler.",AMD,2025-12-10 15:52:28,8
Intel,ntcew16,Can confirm this issue is fixed for me on 9800x3d + 9070xt (I had this issue on 25.11.1 and reverted to 25.10.2 until today) ðŸ‘,AMD,2025-12-10 19:55:01,9
Intel,ntb65up,"Seems to be working fine, though when I was installing the driver my igpu showed up separately from the dgpu in the installer with a download link. But when re-running it they both show under 25.12.1  Should I be installing some separate older driver for it to keep things like hw accel working or was that just some hiccup?",AMD,2025-12-10 16:16:11,6
Intel,ntaufrk,Oh great will also test after work itâ€™s been headache since last driver update,AMD,2025-12-10 15:17:58,5
Intel,nthzjga,Thank you for taking the time to respond. This has been very frustrating.,AMD,2025-12-11 17:33:43,2
Intel,ntlgdax,"I'm sorry to comment directly to you here. Do you have any report about monster hunter wilds performance drops in recent GPU drivers ?    I'm using 9070xt.    I have to use version 25.3.1 to play wilds with no stutters, anything newer gives a lot of stutters in many places.",AMD,2025-12-12 05:02:06,1
Intel,ntjjshb,"Yeah you still have to download it on your own, the creator of Optiscaler already said they aren't going to bundle it probably due to the whole legality around it.",AMD,2025-12-11 22:14:22,1
Intel,ntbpcf0,"i saw a post that detailed how to essentially replace noise suppresion with the working version in newer drivers, you can probably find it here somewhere",AMD,2025-12-10 17:50:29,3
Intel,nwtxl54,yeah somethingâ€™s definitely wrong. iâ€™m assuming youâ€™ve already tried ddu?,AMD,2025-12-31 00:39:33,1
Intel,ntbytau,Thank you for this! been waiting for a fix with Star citizen.,AMD,2025-12-10 18:35:58,7
Intel,ntcdxlc,Yeah SC seems to be working for now.,AMD,2025-12-10 19:49:59,6
Intel,ntcib7n,"Bonjour, pour le moment sur Star citizen le problÃ¨me avec EAC fonctionne pour la 7900XT. Merci d avoir rÃ©glÃ© le problÃ¨me. Bonne fÃªtes de fin d'annÃ©e.",AMD,2025-12-10 20:12:16,2
Intel,ntcro8s,That's good to hear. What about Noise Suppression not working since 25.9.2?,AMD,2025-12-10 20:58:42,1
Intel,ntcnbes,Hmm let me try. So pretty much having installed the latest driver (25.12.1) I just open SecondLife. I look closely at my avatar/character and my skin looks like this  [https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4](https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4) (excuse my outfit but just easier to show)  this is how it's supposed to look and also does on 25.9.1 [https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4](https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4)  I've heard that this doesn't occur on linux but only windows (But I don't have linux so can't say for sure)  I think you need PBR / Materials or some reflection on your skin to see the issue.   If you fly up to around 2000+ meters above ground it becomes easier to see  These are my settings [https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png](https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png)  I have an rx 7900XTX,AMD,2025-12-10 20:37:14,3
Intel,ntcnscv,"Hello! I am actually one of the developers on the client team for Second Life, and I have been trying to figure out how to get in touch - we have found at least one nasty bug on some of the Strix Halo chips with the current drivers.  Can you send me a message here so we can exchange emails?",AMD,2025-12-10 20:39:36,3
Intel,nu85qao,ðŸ˜­ðŸ˜­ðŸ˜­ðŸ˜­,AMD,2025-12-15 21:31:35,1
Intel,ntdy4eq,"I had a similar issue with my 6800xt and the other thing that helped was to sit it to fullscreen or borderless and swap back and forth. Now I'm only playing in fullscreen (which is annoying), but it doesnt crash anymore.",AMD,2025-12-11 00:46:19,1
Intel,ntf5uuk,I have the same card and exactly the same problem. Can't install newer drivers or BF6 just constantly crashes.  I'm on 25.10.2 tho,AMD,2025-12-11 05:30:42,1
Intel,ntapogt,"And it is, and they did, we have the leaked int8 version from September... Just needs official driver implementation now.",AMD,2025-12-10 14:52:54,5
Intel,ntu98tq,"Before the Black Ops 7 (which I donâ€™t own) integration to Warzone, I could click off it & carry on. But since the integration it just closes the game.",AMD,2025-12-13 17:06:00,2
Intel,ntudqmy,"Yes, i have created this github issue.",AMD,2025-12-13 17:29:47,3
Intel,nva2mbl,"If those failures are avoided by clock limiting the board, the problem area could be a different domain entirely (CPU, memory, power, etc.).  The linux remark is interesting, it kind of calls back to similar failures with NV31 in certain apps like Helldivers 2; we had a little internal discussuon about how the amdgpu kernel driver managed to mostly avoid such issues, though I dont recall the outcome.  If you get the opportunity, I'd recommend a suite of system integrity routines as a sanity check; please take a look at [one of my older posts](https://old.reddit.com/r/Amd/comments/1l9ox9r/amd_software_adrenalin_edition_2562_optional/nn3yuay/) for some background.",AMD,2025-12-21 23:17:59,2
Intel,o12cvk0,issue persists in the newest 26.1.1 update....,AMD,2026-01-22 15:53:23,1
Intel,nym5z84,OK thanks.,AMD,2026-01-09 15:56:13,1
Intel,ntb3jrw,"They aren't numbered in the sense of like 4.0.2 or like there won't be an ""fsr 5""? Thank you very much btw, very helpful info!",AMD,2025-12-10 16:03:28,3
Intel,ntdazyo,Like a charm. :D,AMD,2025-12-10 22:34:45,1
Intel,ntbte7r,"I probably could, but AMD (and any other company, really) should be following the users preference anyways. It is a band aid fix and doesn't solve the problem.  Not a bad idea though.",AMD,2025-12-10 18:09:54,4
Intel,ntwpskf,I've been using computers since dos 3.  You're a spanner.  I'm sure MacOS is soooooo much more open.,AMD,2025-12-14 01:31:33,3
Intel,ntpczrx,Here you go: [https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw](https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw)  I did run in windbg but I have no idea how to save the output unless you just want a copy + paste of it here haha,AMD,2025-12-12 20:22:52,1
Intel,ntbtmtr,"Thank you for the idea, I just tried a custom install during an update, was given 2 choices (update/dont update driver and install/dont install privacy view). After installing drivers, step 2/2 was installing the install manager.    After updating through adrenaline using the custom option, I attempted reinstalling again using the auto-detect, custom install. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.   Installing via the WHQL package, custom install follows the same steps as above. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.",AMD,2025-12-10 18:11:03,1
Intel,ntci8tp,Appreciate the feedback,AMD,2025-12-10 20:11:57,6
Intel,ntb8e8t,Thank you for confirming.  That interesting though. I think the most seamless way to support products from both branches is to use the AMD auto detect tool. Can you tell me how the iGPU is represented in Windows' Device Manager?,AMD,2025-12-10 16:27:02,7
Intel,ntbkinx,"[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html)  https://i.redd.it/3vxsa8yave6g1.gif  If you suspect the installation is incorrect, download the package that includes the IGPU driver using the link provided above. The basic version does not include the IGPU driver, but provides a separate download option during installation.  Anyway, it seems like a lot of bugs have been fixed in this version.",AMD,2025-12-10 17:26:48,1
Intel,ntbxdgl,"if you can find it, you will be the goat",AMD,2025-12-10 18:29:03,4
Intel,nwv35gt,"This isn't every game, this is only some games. Not all games have a native vsync option either. That being said, from what I can find, this is a known issue.  https://steamcommunity.com/discussions/forum/1/601900047372731730/  https://www.wumeicn.com/screen-tearing-fix-for-rx-9070xt-and-freesync/",AMD,2025-12-31 04:49:26,1
Intel,ntciae3,Thank you for letting us know ðŸ‘,AMD,2025-12-10 20:12:10,7
Intel,ntcoi7s,appreciate the info. I'll ask our technicians to check in with the settings you've provided,AMD,2025-12-10 20:43:12,4
Intel,ntfvp58,I can confirm there is no issue in linux. A windows version running under proton in linux has no issues as well.   In the video there is flickering on head and body. I see only flickering on the head (when running it on the windows pc)  But my body has no layers attached - the body in the video usually comes with layers. But all heads have multiple transparent layers. The problem occurs even when that layers are not in use and are fully transparent.   Probably related.,AMD,2025-12-11 09:32:57,1
Intel,nte0wcf,"Hey there, thank you for reaching out!  I don't suppose it would be possible for one of our devrel folks to contact you via a linden lab email address like business@lindenlab.com?",AMD,2025-12-11 01:02:56,3
Intel,ntuvq16,"So if you click dismiss, the game closes, did I understand correctly? It doesn't let you enter the COD HQ ? I'm telling you this because I too should update the bios, in fact it happens to me too, but I click dismiss and it lets me play anyway.",AMD,2025-12-13 19:03:40,1
Intel,nva7np2,"for CS2, it was the newest driver that caused crashes exclusively, but on that driver I also got stronger boosts off the bat, hence it crashed faster. Now on 25.10.1(from windows update), COD still crashes with a black screen then tab to desktop with a driver timeout detected. Looking at afterburner(just using it to monitor clocks, no OC/UV applied or anything) the moment the GPU touches 3300+ I get thrown to the desktop. Can't even finish the training course even with ""speedrun strats"" before it crashes. It boosts [momentarily to 3300+](https://i.ibb.co/bgLFC0dp/coreclockcrash.png) and I get a screen freeze, crash, and sent to desktop with a driver timeout.   [These](https://send.vis.ee/download/103635cf66bdb907/#t2lRq409eeNwv6AaafhKJA) are both my crash report submissions. I'd go tomorrow over the stress tests, but I have managed to complete Time Spy/Steel Nomad without issues. And like I said, my system has has 0 issues before on a 2080ti.",AMD,2025-12-21 23:47:10,1
Intel,nvccr7w,"FYI, I passed [everything.](https://imgur.com/a/WyB9FeE)  This leaves the driver only. I made sure windows update didn't download its own driver this time, installed 25.12.1, still getting driver timeouts and crashes in games. I don't know what to tell you. Memtest86 also passed without any issues.",AMD,2025-12-22 09:07:04,1
Intel,o12dr67,that's... unexpected. Can you tell me what hardware this is with?,AMD,2026-01-22 15:57:19,1
Intel,ntb5lb9,"there won't be an ""fsr 5"" because any game with fsr implemented from here on out should, in theory, be compatible with every future version of fsr made, so numbering them isn't as meaningful. they're probably just going to stick with unofficial codenames like redstone for diffrentiation. Nvidia still uses versioning for DLSS despite it using the same system because it's good for marketing and diffrentiation so I'm not sure that dumping the version numbers is a wise decision but it also makes sense",AMD,2025-12-10 16:13:25,5
Intel,ntbuj2m,"I agree with you wholeheartedly, but super users do what they do best - sudo that shit. x)",AMD,2025-12-10 18:15:24,2
Intel,ntgknre,"I've never had AMD Chat or Privacy View force install, I hate they show up in the available software to install when updating, but I just dont click to install them lol, just update the gpu/chipset drivers",AMD,2025-12-11 13:06:05,1
Intel,ntpzp27,I guess a snippet of the faulting component from the output would work.  This is a minidump. Do you have a kernel memory dump>?,AMD,2025-12-12 22:25:23,2
Intel,ntqn26t,"sorry i missed this, seems it had expired. maybe someone downloaded it before i did?",AMD,2025-12-13 00:46:30,2
Intel,ntbdwdm,"Right now in devmgr with re-running the driver installer from the site things look like this https://u.numerlor.me/2faMBA . I also remembered adrenalin has full driver details and everything looks fine there https://u.numerlor.me/w1Snxw https://u.numerlor.me/EOclpA so I think it was just the installer being a bit confused.  Compared to the installer on the first screenshot, when doing the actual update (from inside adrenalin) the Radeon Graphics was a separate item, and had a ""Download driver"" or something along those with the link I mentioned",AMD,2025-12-10 16:54:02,4
Intel,ntbapq8,What about the combined exe? It's still available? That will install both gen but was bugged with control panel disappearing on previous driver.  The combined exe is around 1.6GB.,AMD,2025-12-10 16:38:26,3
Intel,ntc59vr,This might be it? Worth a shot I suppose.  Edit: This worked for me on the latest driver  [https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2025-12-10 19:06:45,4
Intel,nwvgoe4,"i donâ€™t have this issue in any of the same games, but i have no idea what could be causing it in your setup and not mine though",AMD,2025-12-31 06:30:39,1
Intel,o12hpew,"can you explain how one would reproduce this corruption who has never used second life and has no $ to spend in game? I am trying to reproduce the corruption you are describing but it seems that it has to do with in-game purchases or ""face layers"". can you explain how to apply these layers to the player?",AMD,2026-01-22 16:15:12,1
Intel,ntgiwq2,geenz@ but yes,AMD,2025-12-11 12:54:38,2
Intel,nvf2a9r,any news? SL are not updating their customers with anything constructive and it is affecting most of us.,AMD,2025-12-22 19:18:19,1
Intel,ntvblbb,"Hmm, when I can, Iâ€™ll have another look! Thanks!",AMD,2025-12-13 20:31:34,2
Intel,nvd9inn,"I see. Is this specific to CS2 or does it occur with other apps on your end?  We're presently tracking and working on TDRs in that game specifically, though I'm kind of worried in a way that clock limiting works around this failure.",AMD,2025-12-22 13:43:12,2
Intel,o131pj2,7900 xt!  I did a DDU and installed the newest driver too so I feel like it isn't carried over unless it was something from my settings ...,AMD,2026-01-22 17:45:13,1
Intel,ntr6yhj,"I do not, only the minidump but I've uploaded it again here [https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN\_IxLREw](https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN_IxLREw)",AMD,2025-12-13 02:56:08,1
Intel,ntbd7hc,"Yes it should be fixed under that scenario, and the combined package is linked on the release notes:  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html  https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe  Kind of guessing here but I believe the '-c' towards the end of the file name denotes a combined package spanning RDNA support.",AMD,2025-12-10 16:50:38,6
Intel,nthusz8,This worked for me btw - did it a few days ago before these drivers dropped. When I update I'll be using the same method.,AMD,2025-12-11 17:10:36,3
Intel,nwwqpp5,"Are you running 4k in freesync on a 9000 series card?  I'm going by the radeon performance metric overlay saying minecraft/etc is using 300w power.  UE5 games are fine, games with an internal frame cap don't have an issue (well, they have their own frame pacing issues but that's not this).  I can always tell when framerate is going nuts because I can hear the squealing in my speakers when the gpu is at 100%. It's especially bad in menu's. If I turn off features/settings that improve quality or try a lower in game resolution, it gets much worse.",AMD,2025-12-31 13:15:38,1
Intel,nth5y4y,thanks a bunch. I'll pass this on to my ISV contact and see where we get with that.,AMD,2025-12-11 15:08:11,3
Intel,nvfir9a,You can find it here [https://github.com/secondlife/viewer/issues/5048](https://github.com/secondlife/viewer/issues/5048),AMD,2025-12-22 20:43:28,1
Intel,nub8ufp,news ?,AMD,2025-12-16 10:31:04,1
Intel,nve66vp,"COD is the greatest offender - I can't even get through the training course for Zombies without a black screen>driver timeout message, even if I try to speedrun it in a way (because I've attempted it so many times) it is inevitable it's going to crash, that one crashes with this [error](https://i.ibb.co/KjxynXH5/image.png).  Again, NO OC is applied. Other than the ram running at 2666, which as stated with both mem tests successful and went through both by Karhu's test and Memtest, have no issues. Including no issues with my previous GPU,2080ti, again. CS, I can't even start a match with friends because it'll inevitably crash randomly, sometimes it is within 5-10 mins, sometimes it is near instant in a couple of minutes. Tried everything from 25.12.2 to 25.9.1. PSU is a RM1000e, using the 12pin cable natively from the PSU. It is all the way in, this PSU I specifically even got for this GPU as I didn't want to use an adapter to power the card from all the experiences I've read with the 12pin + adapters.  Here is also a [video](https://www.youtube.com/watch?v=cSkaI6WSfJY) of it happening.",AMD,2025-12-22 16:38:34,1
Intel,o133wr0,"Okay, this is going to be tricky. I was under the impression this was completely eliminated, as we can no longer hit this internally.  Assuming that only your mouse input is blocked, I'll need your help capturing a usermode dmp of the RadeonSoftware.exe process via task manager.  This will involve setting some keys in windows registry. Are you comfortable with this?",AMD,2026-01-22 17:54:58,1
Intel,ntvi492,"huh, that's odd. Do you have any larger files over at       C:\Windows\LiveKernelReports\WATCHDOG\",AMD,2025-12-13 21:08:32,2
Intel,ntbt0lr,"Installed the c one. And seems to be working fine. 780M and 6800 here. Still when selecting a specific GPU for a specific app, both energy saver and performance show 6800. This bug has been forever. And it's probably just a registry key when the driver install. Win11.",AMD,2025-12-10 18:08:05,4
Intel,nx0meck,"iâ€™m using a 1440p freesync monitor, i basically always have fps counter on in all of my games so i can verify that vsync always works. even works without the freesync monitor. frame rate only ever goes uncapped when i disable vsync. is it only an issue at 4k?",AMD,2026-01-01 02:07:41,1
Intel,nwkq3wh,No updates there,AMD,2025-12-29 17:14:59,1
Intel,o13ecrh,"at this point you kind of need to edit the windows registry to make windows usable: so I'm mostly familiar with the process.     though I'm a bit worried about the ""no longer hit this internally"" part since this has been a thing for quite some time now...",AMD,2026-01-22 18:40:53,1
Intel,nuyojnk,I do actually have one in there that's 17MB from a BSOD yesterday caused by the AMD driver,AMD,2025-12-20 01:45:48,1
Intel,nwyqsn1,it's tagged as a milestone for feb,AMD,2025-12-31 19:33:08,1
Intel,o13jkce,"it was newly introduced in the 25.20 branch, eliminated at the tail end of that branch's production lifespan (25.12.1), though it seemed to still repro for several people, including AMDers. We've never observed it with 25.30 release candidate builds, which is why I'm surprised.  [Following this resource](https://learn.microsoft.com/en-us/windows/win32/wer/collecting-user-mode-dumps), can you set the DumpType DWORD at       HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\Windows Error Reporting\LocalDumps\  ...and set its value to 2, leading to the generation of full usermode dmps.  This is a non invasive configuration for the most part, and isn't something you should have to revert, but you can also save this location in the registry editor for ease of access.  When that's defined, you can reboot the system.  The next time you reproduce the issue with Adrenalin In-game Overlay hogging mouse input, see if you can pull up task manager (ctrl shift escape). The 'cursor' will be on the process list header, if you hit tab twice, it should focus on the hamburger menu element. from there you can arrow down to the Details header, and hit space or enter.   With details open, you can hit tab a few more times to cycle back to the process list elements. if you start typing 'RadeonSoftware.exe' (should just need to key in ""rad""), it'll pull that process element into view. I'm hoping you have a context menu key rather than a function button on your keyboard, if you hit that, you should be able to arrow down to the 'Create memeory dump file' option.  When you have that in hand, zip it up and fire it over to me via a method of your choice. I'm partial to https://send.vis.ee  hit me up if you need a hand with whatever",AMD,2026-01-22 19:03:54,1
Intel,nonhqm9,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.Â   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",AMD,2025-11-13 16:12:15,80
Intel,nonf76t,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",AMD,2025-11-13 15:59:52,125
Intel,nonf5bq,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",AMD,2025-11-13 15:59:37,73
Intel,nooaz8h,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,AMD,2025-11-13 18:34:35,18
Intel,nonkrkq,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,AMD,2025-11-13 16:27:06,13
Intel,nonnn81,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too ðŸ˜¿.",AMD,2025-11-13 16:41:13,10
Intel,nonfuov,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,AMD,2025-11-13 16:03:01,17
Intel,noob2qb,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? Iâ€™ve spent 1 entire afternoon try every solutions given by Google but today the problem is still thereâ€¦,AMD,2025-11-13 18:35:02,8
Intel,nopg6ma,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",AMD,2025-11-13 21:59:54,8
Intel,nonp8tv,So does this mean Arc Raiders will stop randomly crashing in Windows?,AMD,2025-11-13 16:49:00,10
Intel,nonw7rh,Just installed these zero issues so far!,AMD,2025-11-13 17:23:17,5
Intel,nondz23,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",AMD,2025-11-13 15:53:59,25
Intel,nonlldq,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,AMD,2025-11-13 16:31:10,5
Intel,noofqtg,There was a long delay with the blank screen. Made me a bit nervous,AMD,2025-11-13 18:57:20,4
Intel,noo2zob,At this point i'm sure that cyberpunk will never be fixed.,AMD,2025-11-13 17:56:32,10
Intel,noolxx3,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,AMD,2025-11-13 19:27:59,6
Intel,nonj6l9,No fix for being unable to enable Noise Suppression...,AMD,2025-11-13 16:19:20,7
Intel,nooktgl,When does Linux get this,AMD,2025-11-13 19:22:26,3
Intel,nop2o04,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",AMD,2025-11-13 20:52:28,3
Intel,noqem0g,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",AMD,2025-11-14 01:15:45,3
Intel,noqnucr,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",AMD,2025-11-14 02:10:59,3
Intel,nou4y1d,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,AMD,2025-11-14 17:01:23,3
Intel,nouw9o1,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",AMD,2025-11-14 19:17:52,3
Intel,novj51b,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",AMD,2025-11-14 21:17:52,3
Intel,npexfdr,Windows update keeps trying to update my driver.,AMD,2025-11-18 00:54:41,3
Intel,noo4qjo,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,AMD,2025-11-13 18:05:01,4
Intel,noniqz3,No FSR4 on RDNA3 no care,AMD,2025-11-13 16:17:12,15
Intel,noo25hd,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",AMD,2025-11-13 17:52:27,5
Intel,nonthc8,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,AMD,2025-11-13 17:09:50,2
Intel,nooud97,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,AMD,2025-11-13 20:10:18,2
Intel,nortjvj,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,AMD,2025-11-14 07:15:55,2
Intel,nos3s8s,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",AMD,2025-11-14 08:57:09,2
Intel,not49x9,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",AMD,2025-11-14 13:52:51,2
Intel,noux5p5,This driver was way better than the version before it(for me at least).,AMD,2025-11-14 19:22:23,2
Intel,novpivg,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",AMD,2025-11-14 21:50:55,2
Intel,np4ombz,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",AMD,2025-11-16 11:19:30,2
Intel,npp1qov,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,AMD,2025-11-19 16:55:57,2
Intel,nqawzsb,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",AMD,2025-11-23 03:42:39,2
Intel,npaw51d,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",AMD,2025-11-17 11:47:34,3
Intel,nondc4t,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",AMD,2025-11-13 15:50:55,8
Intel,nonmrak,"Brooooo, they didnâ€˜t fix the flickering in BF6 when recordingâ€¦",AMD,2025-11-13 16:36:52,2
Intel,nonvub9,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.Â  * Fucking LOL.,AMD,2025-11-13 17:21:27,2
Intel,nonmi72,25.10.2 completely broke vsync... not even a mention about this in the notes?,AMD,2025-11-13 16:35:38,2
Intel,noncnxo,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2025-11-13 15:47:39,1
Intel,nonguv3,There is new AFMF features too.,AMD,2025-11-13 16:07:56,1
Intel,nonmglo,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,AMD,2025-11-13 16:35:26,1
Intel,nonn4xw,bf6 fps drop fixed?,AMD,2025-11-13 16:38:44,1
Intel,nonvhb6,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,AMD,2025-11-13 17:19:40,1
Intel,noo456j,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",AMD,2025-11-13 18:02:08,1
Intel,noo651n,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",AMD,2025-11-13 18:11:49,1
Intel,nooad23,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",AMD,2025-11-13 18:31:40,1
Intel,nooeeia,How is the driver ? 7700 XT here.,AMD,2025-11-13 18:50:53,1
Intel,noojnun,Finally a potential fix for CPU metrics? Look forward to seeing if itâ€™s true!,AMD,2025-11-13 19:16:39,1
Intel,noovdps,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,AMD,2025-11-13 20:15:26,1
Intel,noozjd6,do yall use ddu for every driver or do yall just update it with the app?,AMD,2025-11-13 20:36:35,1
Intel,noozq5o,"New AMD update ðŸ‘ðŸ‘ðŸ‘ðŸ‘, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",AMD,2025-11-13 20:37:32,1
Intel,nop06vu,I just canâ€™t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video Iâ€™m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,AMD,2025-11-13 20:39:55,1
Intel,nop4b7m,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",AMD,2025-11-13 21:00:46,1
Intel,nopfrqo,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,AMD,2025-11-13 21:57:50,1
Intel,nopilp6,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,AMD,2025-11-13 22:12:30,1
Intel,noplmto,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,AMD,2025-11-13 22:28:42,1
Intel,nopnmjz,Think this broke Vulkan in POE2,AMD,2025-11-13 22:39:24,1
Intel,noqjzdo,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",AMD,2025-11-14 01:48:12,1
Intel,nor6g8r,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,AMD,2025-11-14 04:06:42,1
Intel,nor7il2,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,AMD,2025-11-14 04:14:01,1
Intel,nor9p0f,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,AMD,2025-11-14 04:29:34,1
Intel,nord0sz,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,AMD,2025-11-14 04:54:17,1
Intel,norxf8j,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",AMD,2025-11-14 07:53:31,1
Intel,nos6z6k,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",AMD,2025-11-14 09:29:57,1
Intel,nos7i23,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:35:25,1
Intel,nos7vbg,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",AMD,2025-11-14 09:39:11,1
Intel,nosa7uh,subtract strong cats brave outgoing husky coordinated important rustic juggle   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,AMD,2025-11-14 10:02:49,1
Intel,nosrlfs,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,AMD,2025-11-14 12:33:14,1
Intel,nosysjm,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",AMD,2025-11-14 13:20:27,1
Intel,not149u,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",AMD,2025-11-14 13:34:35,1
Intel,notb7lg,I'm glad the CPU metrics are showing again,AMD,2025-11-14 14:31:49,1
Intel,notcd57,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",AMD,2025-11-14 14:38:07,1
Intel,notlcun,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",AMD,2025-11-14 15:24:40,1
Intel,notm5ep,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),AMD,2025-11-14 15:28:36,1
Intel,notufou,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,AMD,2025-11-14 16:09:14,1
Intel,noue3ki,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",AMD,2025-11-14 17:47:54,1
Intel,nouooi9,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",AMD,2025-11-14 18:39:53,1
Intel,noutw0a,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 ðŸ˜…   What a fucking joke",AMD,2025-11-14 19:05:43,1
Intel,nov8foi,Shits been crashing my system since the update :( sapphire 7900xt,AMD,2025-11-14 20:21:15,1
Intel,novg42t,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",AMD,2025-11-14 21:02:02,1
Intel,nowdvrw,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,AMD,2025-11-15 00:14:02,1
Intel,noydj17,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",AMD,2025-11-15 09:37:48,1
Intel,noypu29,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",AMD,2025-11-15 11:42:58,1
Intel,noyv323,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",AMD,2025-11-15 12:27:57,1
Intel,nozb3zp,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",AMD,2025-11-15 14:16:00,1
Intel,noze8xv,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,AMD,2025-11-15 14:34:56,1
Intel,nozoxq5,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",AMD,2025-11-15 15:35:11,1
Intel,nozv077,Ð£ Ð¼ÐµÐ½Ñ ÐœÑƒÐ»ÑŒÑ‚Ð¸Ð¼ÐµÐ´Ð¸Ð° ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»Ð»ÐµÑ€ Ð²Ñ‹Ð´Ð°ÐµÑ‚ Ð¾ÑˆÐ¸Ð±ÐºÑƒ. Ð”Ð»Ñ ÑÑ‚Ð¾Ð³Ð¾ ÑƒÑÑ‚Ñ€Ð¾Ð¹ÑÑ‚Ð²Ð° Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ñ‹Ðµ Ð´Ñ€Ð°Ð¹Ð²ÐµÑ€Ñ‹. (ÐšÐ¾Ð´ 28),AMD,2025-11-15 16:06:54,1
Intel,np0n0ro,Noise Suppression still broken. 3rd release without that functionality in a row.,AMD,2025-11-15 18:33:00,1
Intel,np0qihb,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalaciÃ³n del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",AMD,2025-11-15 18:50:19,1
Intel,np0sz88,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",AMD,2025-11-15 19:02:53,1
Intel,np2gy28,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",AMD,2025-11-16 00:44:12,1
Intel,np2igku,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,AMD,2025-11-16 00:53:10,1
Intel,np2iy25,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",AMD,2025-11-16 00:56:07,1
Intel,np2n7ns,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",AMD,2025-11-16 01:22:23,1
Intel,np2rc23,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,AMD,2025-11-16 01:46:41,1
Intel,np3zqgd,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me ðŸ™",AMD,2025-11-16 07:09:06,1
Intel,np4btup,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",AMD,2025-11-16 09:09:18,1
Intel,np4c4bj,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",AMD,2025-11-16 09:12:17,1
Intel,np59xsp,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,AMD,2025-11-16 14:05:46,1
Intel,np5tc80,Still not working AMD NOISE S,AMD,2025-11-16 15:57:27,1
Intel,np5w51d,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,AMD,2025-11-16 16:12:01,1
Intel,np6sb4d,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",AMD,2025-11-16 18:55:53,1
Intel,np75mw5,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,AMD,2025-11-16 20:02:08,1
Intel,np7fiy7,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,AMD,2025-11-16 20:52:53,1
Intel,np9tmrb,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",AMD,2025-11-17 05:29:10,1
Intel,npa497n,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",AMD,2025-11-17 07:05:30,1
Intel,npbc7th,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",AMD,2025-11-17 13:41:36,1
Intel,npbdww5,"Unfortunately, version 25.11.1 does not start with Windows.",AMD,2025-11-17 13:51:35,1
Intel,npcr8ua,Is AMD going to come up with another driver soon?,AMD,2025-11-17 18:04:51,1
Intel,npd465l,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",AMD,2025-11-17 19:07:02,1
Intel,npeqls5,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,AMD,2025-11-18 00:15:16,1
Intel,npgfe4k,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly ðŸ˜ž.",AMD,2025-11-18 07:12:00,1
Intel,npgq8pe,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",AMD,2025-11-18 09:03:29,1
Intel,npgujea,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",AMD,2025-11-18 09:49:30,1
Intel,nph1gio,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,AMD,2025-11-18 10:58:38,1
Intel,nphl085,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,AMD,2025-11-18 13:22:24,1
Intel,npikkr4,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",AMD,2025-11-18 16:26:09,1
Intel,npnxcnt,getting bsod randomly since 25.9.1 sad..,AMD,2025-11-19 13:20:00,1
Intel,npowfg1,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",AMD,2025-11-19 16:29:43,1
Intel,npwkypv,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",AMD,2025-11-20 20:27:52,1
Intel,nq842b2,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",AMD,2025-11-22 17:50:50,1
Intel,nq9u9z3,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",AMD,2025-11-22 23:33:52,1
Intel,nqwbryc,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,AMD,2025-11-26 15:56:06,1
Intel,ns8k1w2,"Sorry but for me the drive give me crash pop up message every time i boot up my pc. Also just right now i got a freeze, black screen to all my monitors.",AMD,2025-12-04 12:42:18,1
Intel,ns9soky,The worst driver this year so far,AMD,2025-12-04 16:45:18,1
Intel,nscxupo,"Still havent fixed the noise cancellation lmao, guess its another month+ of old version :) Thanks amd, truly doing wonders.",AMD,2025-12-05 02:44:53,1
Intel,nsgsekn,CS2 crashing with driver timeout after tabbing out or watching streams on 2nd screen 7900xtx,AMD,2025-12-05 18:29:05,1
Intel,nsqr7j8,"When is 25.12.1 coming out? I have read only bad things about 25.11.1 here, so I wanted to skip this one.",AMD,2025-12-07 10:40:19,1
Intel,nonf78x,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",AMD,2025-11-13 15:59:52,0
Intel,nond6d4,So no redstone yet,AMD,2025-11-13 15:50:09,1
Intel,nonqjy0,FSR AI frame gen??? Didnâ€™t they say thatâ€™d it would also have a driver toggle?,AMD,2025-11-13 16:55:25,1
Intel,nonv0vm,Did AMD ever add support for Cronos?,AMD,2025-11-13 17:17:25,1
Intel,nonxx39,Well Star Citizen will load now!  Now some longer term testing....,AMD,2025-11-13 17:31:41,1
Intel,nonw8zf,Anybody tried this with Anno 117 yet? Iâ€™m hoping it helps performance,AMD,2025-11-13 17:23:27,0
Intel,nooyqhv,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,AMD,2025-11-13 20:32:28,0
Intel,nooyuwp,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,AMD,2025-11-13 20:33:06,0
Intel,noqrxh3,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,AMD,2025-11-14 02:35:08,0
Intel,nozwu6t,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,AMD,2025-11-15 16:16:36,0
Intel,np07ekg,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,AMD,2025-11-15 17:12:12,0
Intel,noni2qa,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,AMD,2025-11-13 16:13:55,-4
Intel,noqc54j,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,AMD,2025-11-14 01:00:45,-1
Intel,np4dff7,"Here we go again, jetzt stÃ¼rzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen auÃŸer XMP war aktiviert, dann stÃ¼rzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das Ã¼bernehmen mÃ¼sst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team GrÃ¼n nicht.",AMD,2025-11-16 09:25:48,-1
Intel,nonpv4u,Yeah same here LG c5 42inch ðŸ˜°,AMD,2025-11-13 16:52:03,21
Intel,noockre,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesnâ€™t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",AMD,2025-11-13 18:42:09,17
Intel,noprnhq,"I have this but on display port, HDMI works fine",AMD,2025-11-13 23:01:25,7
Intel,nonyety,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",AMD,2025-11-13 17:34:06,11
Intel,nopqt8d,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",AMD,2025-11-13 22:56:47,2
Intel,nonu691,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,AMD,2025-11-13 17:13:13,2
Intel,nop2vm5,I have the same issue with display port but itâ€™s okay with hdmi :/,AMD,2025-11-13 20:53:31,1
Intel,nq0dwdl,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",AMD,2025-11-21 12:22:58,1
Intel,nonpu8n,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",AMD,2025-11-13 16:51:56,81
Intel,nonjytd,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,AMD,2025-11-13 16:23:11,26
Intel,noo9nj4,"V25.10.2  hereâ€¦ I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",AMD,2025-11-13 18:28:20,3
Intel,not85q8,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",AMD,2025-11-14 14:14:59,4
Intel,nonkdfa,combined again it looks like ðŸ¤·â€â™‚ï¸,AMD,2025-11-13 16:25:10,1
Intel,nongchq,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,AMD,2025-11-13 16:05:27,101
Intel,nono7wt,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",AMD,2025-11-13 16:44:00,2
Intel,nonhdck,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",AMD,2025-11-13 16:10:27,5
Intel,np0qz7g,You try install last chipset driver ?,AMD,2025-11-15 18:52:41,1
Intel,nongxu7,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",AMD,2025-11-13 16:08:21,1
Intel,nonrxcq,So it's the driver that's why that happens ðŸ˜¡ and it's not fixed?,AMD,2025-11-13 17:02:09,0
Intel,noogyei,Thank you for your service,AMD,2025-11-13 19:03:14,8
Intel,nopxjjg,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",AMD,2025-11-13 23:35:26,7
Intel,nov7gjn,Any update mate?,AMD,2025-11-14 20:16:03,1
Intel,nosfu5h,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",AMD,2025-11-14 10:57:06,0
Intel,nonw38z,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",AMD,2025-11-13 17:22:40,6
Intel,npdh2mf,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",AMD,2025-11-17 20:11:40,3
Intel,np42etk,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",AMD,2025-11-16 07:35:21,2
Intel,noroh5d,"I'm the opposite, IÂ just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",AMD,2025-11-14 06:29:26,1
Intel,nonifp9,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDUâ€™d it again to go back to 25.9.2 since games were stuttering.",AMD,2025-11-13 16:15:40,11
Intel,noozgtx,Same.,AMD,2025-11-13 20:36:13,3
Intel,nop6flo,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",AMD,2025-11-13 21:11:26,3
Intel,noxqsoq,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",AMD,2025-11-15 05:51:40,1
Intel,nosbqvm,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,AMD,2025-11-14 10:17:54,1
Intel,nozhfiv,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,AMD,2025-11-15 14:53:42,1
Intel,nopl6z7,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",AMD,2025-11-13 22:26:21,1
Intel,noppntf,If it still crashes set RTX Global Illumination to Static.,AMD,2025-11-13 22:50:26,6
Intel,nor7jw2,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",AMD,2025-11-14 04:14:16,2
Intel,nonlw78,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",AMD,2025-11-13 16:32:40,12
Intel,notyc45,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as theyâ€™ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since theyâ€™re much deeper in engine code/inputs.",AMD,2025-11-14 16:28:32,2
Intel,noocnzc,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it ðŸ¤“",AMD,2025-11-13 18:42:35,1
Intel,noo416z,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",AMD,2025-11-13 18:01:35,1
Intel,not2qjr,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",AMD,2025-11-14 13:44:02,2
Intel,nopbmoh,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,AMD,2025-11-13 21:37:14,2
Intel,nosw536,Ugh,AMD,2025-11-14 13:03:59,2
Intel,nooumki,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",AMD,2025-11-13 20:11:38,3
Intel,noqgvkg,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",AMD,2025-11-14 01:29:31,1
Intel,nos3g9h,"Linux doesnâ€™t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, thatâ€™s when you get driver updates, and theyâ€™re completely different from windows branch.",AMD,2025-11-14 08:53:47,1
Intel,noqn7n2,just uninstall it I prefer manual check myself.,AMD,2025-11-14 02:07:15,2
Intel,nor7u07,So AMDs default driver overclocks and doesnâ€™t reflect that in the values?,AMD,2025-11-14 04:16:14,1
Intel,nqsncxf,Same issues here i underclocked it but this new update just made it worse,AMD,2025-11-26 00:03:57,1
Intel,np5tu2z,ok it is still crashing ... complete reboot :(,AMD,2025-11-16 16:00:03,1
Intel,nq4e73q,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",AMD,2025-11-22 01:28:20,1
Intel,nope0rx,Okay.,AMD,2025-11-13 21:49:03,1
Intel,nonl1up,Iâ€™m hoping Valveâ€™s new steam machine will push them on that since itâ€™s RDNA3 based.,AMD,2025-11-13 16:28:30,18
Intel,noukbhw,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,AMD,2025-11-14 18:18:29,3
Intel,nooggfu,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",AMD,2025-11-13 19:00:46,4
Intel,noozx8g,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,AMD,2025-11-13 20:38:33,1
Intel,noptibm,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",AMD,2025-11-13 23:12:01,1
Intel,noo53y9,welcome to amd,AMD,2025-11-13 18:06:50,0
Intel,nonvvaf,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,AMD,2025-11-13 17:21:35,1
Intel,nooca2m,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",AMD,2025-11-13 18:40:45,1
Intel,np1vdc1,Same. Never even had Ryzen master installed.,AMD,2025-11-15 22:35:21,2
Intel,npiam42,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",AMD,2025-11-18 15:37:33,1
Intel,nowsbia,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,AMD,2025-11-15 01:45:44,1
Intel,nonegtb,What is redstone?,AMD,2025-11-13 15:56:21,5
Intel,nonnq47,What's weird is Black Ops 7 has ray regeneration.,AMD,2025-11-13 16:41:37,6
Intel,none418,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",AMD,2025-11-13 15:54:38,3
Intel,nontlx6,vsync issue fixed with win 11 KB5068861 update.,AMD,2025-11-13 17:10:27,13
Intel,nonxa48,had no issues with vsync on 25.10.2,AMD,2025-11-13 17:28:33,3
Intel,nons4sz,works fine for me,AMD,2025-11-13 17:03:10,6
Intel,noorn1m,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,AMD,2025-11-13 19:56:34,1
Intel,nopcb8w,"That it did, lol. My only complaint.",AMD,2025-11-13 21:40:35,1
Intel,noqh6ym,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",AMD,2025-11-14 01:31:26,0
Intel,nonl36f,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,AMD,2025-11-13 16:28:41,3
Intel,noovbth,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,AMD,2025-11-13 20:15:10,2
Intel,nonpyiq,"Fps drop over time? That's a game issue, it's got a memory leak",AMD,2025-11-13 16:52:31,4
Intel,nopz2ou,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",AMD,2025-11-13 23:44:30,1
Intel,nov7ewn,Iâ€™d settle for bf6 going one entire game without drivers crashing the game and freezing pc,AMD,2025-11-14 20:15:48,1
Intel,noorxgl,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",AMD,2025-11-13 19:58:00,1
Intel,nov7k59,Crashes?,AMD,2025-11-14 20:16:34,1
Intel,nowyxe0,I have this problem in all games.,AMD,2025-11-15 02:28:26,1
Intel,nprco16,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",AMD,2025-11-20 00:04:27,1
Intel,not2cbd,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",AMD,2025-11-14 13:41:44,1
Intel,np3fwq0,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",AMD,2025-11-16 04:25:12,1
Intel,norugaj,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",AMD,2025-11-14 07:24:32,3
Intel,nopmfkv,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,AMD,2025-11-13 22:33:01,2
Intel,noscuea,Epic version runs just fine.,AMD,2025-11-14 10:28:45,3
Intel,not9drm,Cyberpunk GOG last version patch runs fine on this driver.,AMD,2025-11-14 14:21:42,1
Intel,nosnl0p,"Hey there, can you give an example of how this looks now versus how it's supposed to?",AMD,2025-11-14 12:03:38,2
Intel,noso7o5,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,AMD,2025-11-14 12:08:26,1
Intel,nou0ebb,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,AMD,2025-11-14 16:38:48,1
Intel,nox9yy0,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,AMD,2025-11-15 03:41:47,2
Intel,nou7nae,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,AMD,2025-11-14 17:15:04,2
Intel,noypui8,"The game is booting, this message was for the 25.10 they just didn't removed it",AMD,2025-11-15 11:43:04,2
Intel,noza0c5,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,AMD,2025-11-15 14:09:14,1
Intel,np31dy5,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,AMD,2025-11-16 02:48:35,1
Intel,npgrqyr,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",AMD,2025-11-18 09:19:37,1
Intel,nqit1yt,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",AMD,2025-11-24 12:56:50,2
Intel,nsv6cts,"Good call, it caused nothing but problems for me and pretty severe. Were talking driver timeouts with black screens and even a couple bluescreens.",AMD,2025-12-08 01:50:11,1
Intel,nonny1j,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",AMD,2025-11-13 16:42:40,5
Intel,nopggve,My 9070 xt crushes while I try to use fsr 4 on new drivers,AMD,2025-11-13 22:01:20,1
Intel,noo04cb,Why don't you try it and let us know if you can. Would be helpful for lots of us,AMD,2025-11-13 17:42:31,1
Intel,nont8g8,It's in Redstone. Still not out yet,AMD,2025-11-13 17:08:37,3
Intel,nopd6c2,Didn't work for me...,AMD,2025-11-13 21:44:51,1
Intel,not23h8,Wait until you see how much your browser's cache is churning...,AMD,2025-11-14 13:40:17,2
Intel,notlyfp,Why cant you use Adrenalin? I'm using it on 25.9.1,AMD,2025-11-14 15:27:40,1
Intel,nq0kohy,I just received a windows extension update for my LG monitor. If you can boot up go check.,AMD,2025-11-21 13:08:34,1
Intel,nopw101,The last time I had this problem it was a RAM issue.,AMD,2025-11-13 23:26:32,4
Intel,npd560g,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,AMD,2025-11-17 19:11:57,1
Intel,norotfv,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",AMD,2025-11-14 06:32:31,1
Intel,nood411,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,AMD,2025-11-13 18:44:41,10
Intel,noo4uio,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",AMD,2025-11-13 18:05:33,4
Intel,nooyzy7,Do u reintall already up to date chipset drivers?,AMD,2025-11-13 20:33:49,1
Intel,norplxi,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,AMD,2025-11-14 06:39:36,1
Intel,nonuzmx,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",AMD,2025-11-13 17:17:15,3
Intel,nonxvx2,doing so (separation) will create a freak out shitstorm part 2.,AMD,2025-11-13 17:31:31,12
Intel,nonzgmu,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,AMD,2025-11-13 17:39:17,16
Intel,nonz6zk,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),AMD,2025-11-13 17:37:57,11
Intel,nons9ct,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,AMD,2025-11-13 17:03:49,17
Intel,nooofaj,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",AMD,2025-11-13 19:40:29,2
Intel,nony71m,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,AMD,2025-11-13 17:33:02,16
Intel,nonscqs,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,AMD,2025-11-13 17:04:17,1
Intel,nonuj7z,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,AMD,2025-11-13 17:15:00,21
Intel,nonzcwc,Thank you for communicating,AMD,2025-11-13 17:38:47,10
Intel,nononki,Unfortunately happens to me too. So for me itâ€™s a big issue as I canâ€™t update to this driver until it is fixed ðŸ˜°,AMD,2025-11-13 16:46:06,4
Intel,nooyj1v,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.Â  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.Â  Issue goes away using a non 4k 240hz display.Â Â    I believe this system crash is deeply related to DSC on Windows.Â  I only got these two PC bsods when I bought a 4k 240hz display.Â  Returned a monitor (bad oled) and the issue went away.Â  Got a new oled a few weeks ago and now I have these bsods again.Â Â    Never had a bsod before I got these 4k 240hz displays.Â  Fresh Windows 11 installs too between both PCs and between my first and second oled.Â  Systems are both solid and stable.Â Â    Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20Â  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.Â  Hopefully someone else had experience with them on 4k 240hz.",AMD,2025-11-13 20:31:24,5
Intel,nons1mi,Thank you AMD my bad for getting upset,AMD,2025-11-13 17:02:44,5
Intel,nongngq,Thank you.,AMD,2025-11-13 16:06:56,2
Intel,noobm6s,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why itâ€™s failing. Would be cool to see the technical details if thatâ€™s possible. (Iâ€™m actually more interested now on why itâ€™s not working vs just getting it fixed).,AMD,2025-11-13 18:37:36,2
Intel,nop1khf,Thank you!,AMD,2025-11-13 20:46:53,2
Intel,nop0wol,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,AMD,2025-11-13 20:43:34,1
Intel,nonlavb,Redstone when?,AMD,2025-11-13 16:29:44,0
Intel,nonhqde,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",AMD,2025-11-13 16:12:13,15
Intel,nonlm2a,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,AMD,2025-11-13 16:31:16,10
Intel,nonz8d7,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",AMD,2025-11-13 17:38:09,6
Intel,nonjrum,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",AMD,2025-11-13 16:22:14,3
Intel,nonovmq,Non pc monitor tvs are sometimes cheaper especially for larger sizes. Iâ€™m on lg c5 oled 42inch and it only has hdmiâ€¦,AMD,2025-11-13 16:47:11,3
Intel,noo0hf6,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",AMD,2025-11-13 17:44:18,3
Intel,nonq4py,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",AMD,2025-11-13 16:53:21,2
Intel,noolj45,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",AMD,2025-11-13 19:25:57,1
Intel,noqqvuj,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",AMD,2025-11-14 02:28:55,1
Intel,nosnlnp,> Are y'all playing on televisions?  Do you guys not have phones?,AMD,2025-11-14 12:03:46,0
Intel,nonhcwn,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,AMD,2025-11-13 16:10:23,2
Intel,noqf5pn,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,AMD,2025-11-14 01:19:04,5
Intel,noqno3l,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",AMD,2025-11-14 02:09:58,1
Intel,nov6ye9,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",AMD,2025-11-14 20:13:22,1
Intel,noyds7c,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",AMD,2025-11-15 09:40:31,1
Intel,nsoev4p,Why does it seem like driver quality/support has gotten substantially worse this past decade? Are we running out of skilled software engineers or is hardware just getting too out of hand?,AMD,2025-12-06 23:58:22,1
Intel,noshsep,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,AMD,2025-11-14 11:15:06,4
Intel,nonx5ls,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",AMD,2025-11-13 17:27:56,1
Intel,np52n5a,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",AMD,2025-11-16 13:17:38,3
Intel,np1d4kt,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",AMD,2025-11-15 20:53:05,2
Intel,nonis5q,OK thought I was the only one. 25.10 is bad bad,AMD,2025-11-13 16:17:22,4
Intel,nood354,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",AMD,2025-11-13 18:44:34,1
Intel,nos072w,Thanks for testing it,AMD,2025-11-14 08:20:41,1
Intel,np22kzb,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",AMD,2025-11-15 23:18:12,1
Intel,nosh56b,I thought FSR 4 was only on RDNA 4? ðŸ¤”,AMD,2025-11-14 11:09:11,1
Intel,nozuikm,My thoughts exactly. Thanks.,AMD,2025-11-15 16:04:18,1
Intel,nopzlun,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,AMD,2025-11-13 23:47:39,2
Intel,noq7kwh,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,AMD,2025-11-14 00:33:58,2
Intel,not1lyv,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,AMD,2025-11-14 13:37:28,1
Intel,noojjne,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",AMD,2025-11-13 19:16:05,1
Intel,nooj67g,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",AMD,2025-11-13 19:14:14,2
Intel,nopbvvl,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,AMD,2025-11-13 21:38:30,5
Intel,nouxgnr,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,AMD,2025-11-14 19:23:57,1
Intel,norm1yc,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",AMD,2025-11-14 06:08:00,3
Intel,npawrxf,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",AMD,2025-11-17 11:52:53,1
Intel,noo4anu,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,AMD,2025-11-13 18:02:52,8
Intel,nosbylr,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",AMD,2025-11-14 10:20:00,0
Intel,nphuo0h,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game ðŸ‘€?,AMD,2025-11-18 14:14:35,1
Intel,noojmmw,Fun fact - i am dual booting and on Linux this bug is not existent...:)),AMD,2025-11-13 19:16:29,2
Intel,noockb2,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",AMD,2025-11-13 18:42:05,2
Intel,nqw6fd5,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,AMD,2025-11-26 15:29:31,1
Intel,noznq2r,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",AMD,2025-11-15 15:28:41,1
Intel,nongh5i,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,AMD,2025-11-13 16:06:05,7
Intel,noozl1i,It's a thing you can search for on Google,AMD,2025-11-13 20:36:49,1
Intel,nonsdkw,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,AMD,2025-11-13 17:04:24,3
Intel,nonm0k8,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",AMD,2025-11-13 16:33:16,1
Intel,nonxcza,ahh i'm on Win 10 so probably why I didn't see it.,AMD,2025-11-13 17:28:57,2
Intel,noorjtx,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,AMD,2025-11-13 19:56:07,2
Intel,nonn11b,"Yes, but was it in the previous WHQL driver ? I'm not sure.",AMD,2025-11-13 16:38:12,1
Intel,nons5g1,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),AMD,2025-11-13 17:03:16,2
Intel,nos7pyk,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,AMD,2025-11-14 09:37:40,1
Intel,nqrxf23,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",AMD,2025-11-25 21:39:49,1
Intel,nov8osy,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",AMD,2025-11-14 20:22:35,1
Intel,np3gi86,Either launch with curseforge or rollback,AMD,2025-11-16 04:29:38,1
Intel,nopmt4r,"Damn, didnâ€™t work for me last driver either. I can get FSR4 to work in other games just not BF6",AMD,2025-11-13 22:35:02,1
Intel,nr5w3fb,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,AMD,2025-11-28 03:55:10,1
Intel,noxy5g3,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",AMD,2025-11-15 07:00:01,1
Intel,noz0zh9,You 100 procent sure on this?,AMD,2025-11-15 13:12:06,1
Intel,np59d5s,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,AMD,2025-11-16 14:02:08,1
Intel,np32vom,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",AMD,2025-11-16 02:57:54,2
Intel,npgs1he,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic cardðŸ¤¡,AMD,2025-11-18 09:22:45,1
Intel,nswxbvi,"I had randomly black screens with 24.2.1, this was annoying as hell. Had to DDU the Driver and went back to 23.11.1, after this everything was fine.",AMD,2025-12-08 10:15:23,1
Intel,nonplo5,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,AMD,2025-11-13 16:50:46,1
Intel,noolo6b,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",AMD,2025-11-13 19:26:39,2
Intel,nov5qbd,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,AMD,2025-11-14 20:06:52,1
Intel,nq588dp,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,AMD,2025-11-22 04:54:10,1
Intel,noo85c3,They do not.,AMD,2025-11-13 18:21:19,5
Intel,np5srze,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",AMD,2025-11-16 15:54:29,1
Intel,noo8tps,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,AMD,2025-11-13 18:24:30,11
Intel,noo2nnu,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,AMD,2025-11-13 17:54:54,9
Intel,noolzkz,"AND is taking away one additional driver feature per day, you say?",AMD,2025-11-13 19:28:13,1
Intel,nooy45h,"Yes, Iâ€™m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select â€œGPUâ€ you get a file that has a different dimension from the one you download if you choose â€œCPUâ€. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with â€œminimal_install), but Adrenalin App does not open.",AMD,2025-11-13 20:29:17,4
Intel,noo1i55,Thank you for explaining it before the rage baiters go nuts.,AMD,2025-11-13 17:49:17,2
Intel,noo3cx3,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,AMD,2025-11-13 17:58:18,46
Intel,nooncln,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",AMD,2025-11-13 19:35:04,3
Intel,noo0xcy,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",AMD,2025-11-13 17:46:27,13
Intel,nopu61n,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",AMD,2025-11-13 23:15:46,5
Intel,nopc45s,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",AMD,2025-11-13 21:39:38,2
Intel,noo3fsu,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,AMD,2025-11-13 17:58:41,25
Intel,npp1edb,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,AMD,2025-11-19 16:54:16,2
Intel,nopvrx5,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,AMD,2025-11-13 23:25:04,3
Intel,noo53xx,Already launched in COD 7,AMD,2025-11-13 18:06:50,3
Intel,nonp7d7,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so youâ€™re saying i shoulf switch to hdmi?",AMD,2025-11-13 16:48:48,3
Intel,nonwqs3,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,AMD,2025-11-13 17:25:53,1
Intel,norbib0,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,AMD,2025-11-14 04:42:59,1
Intel,noni0s3,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",AMD,2025-11-13 16:13:39,1
Intel,nor1k1x,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,AMD,2025-11-14 03:34:28,1
Intel,nonzc4h,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",AMD,2025-11-13 17:38:40,2
Intel,noshb1m,With the compiled leaked DLL you can use it on RDNA3 as well.,AMD,2025-11-14 11:10:42,1
Intel,nosbtoj,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",AMD,2025-11-14 10:18:39,2
Intel,notnotg,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,AMD,2025-11-14 15:36:20,1
Intel,noqg8tt,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),AMD,2025-11-14 01:25:43,4
Intel,noonewp,Thank you! Exciting keen to see what itâ€™s like,AMD,2025-11-13 19:35:23,1
Intel,noosgem,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",AMD,2025-11-13 20:00:38,2
Intel,nopjngc,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,AMD,2025-11-13 22:18:03,2
Intel,nphu5po,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",AMD,2025-11-18 14:11:53,1
Intel,npb27so,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,AMD,2025-11-17 12:35:58,1
Intel,nopnm90,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,AMD,2025-11-13 22:39:21,1
Intel,nqw82e8,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",AMD,2025-11-26 15:37:49,2
Intel,nozpqvb,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",AMD,2025-11-15 15:39:28,1
Intel,nonmi38,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",AMD,2025-11-13 16:35:37,4
Intel,nony81v,lmao chill out dude go touch some grass,AMD,2025-11-13 17:33:11,6
Intel,nonozwd,Could be grounds for lawsuitâ€¦ Thatâ€™s funny!,AMD,2025-11-13 16:47:47,4
Intel,norvwn6,Because of MPO.,AMD,2025-11-14 07:38:45,5
Intel,noq77oq,yeah same with 25.11.1 25.9.2 works for me,AMD,2025-11-14 00:31:52,1
Intel,nonv3ns,"25.10.2 was the previous WHQL, so also yes :P",AMD,2025-11-13 17:17:48,2
Intel,noo30h0,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",AMD,2025-11-13 17:56:38,1
Intel,noxixmd,Which driver version and does it still crashing?,AMD,2025-11-15 04:47:44,1
Intel,nopn1gw,OK I will install it now and test it and get back to you. Give me 10 mins.,AMD,2025-11-13 22:36:16,2
Intel,noppoge,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,AMD,2025-11-13 22:50:32,2
Intel,nr7hjjv,I'll work with the engineer from that ticket check if that issue has somehow regressed.,AMD,2025-11-28 12:30:29,3
Intel,nrptkfo,We've not been able to reproduce this internally so far. Can you remind me which GPU (was this a 7900XTX?) + connectivity method you're using?,AMD,2025-12-01 14:48:30,2
Intel,noztt6w,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,AMD,2025-11-15 16:00:37,2
Intel,np0adh6,"Yup just need to say ""No""",AMD,2025-11-15 17:28:00,2
Intel,npgto9y,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",AMD,2025-11-18 09:40:23,1
Intel,nonps7q,I don't see how it would work on 23.9.1 lol,AMD,2025-11-13 16:51:40,-1
Intel,nov73co,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",AMD,2025-11-14 20:14:06,1
Intel,noprr9f,I did it this morning before the new driver and confirm chipset drivers were untouched,AMD,2025-11-13 23:02:01,3
Intel,noooxx5,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and donâ€™t use the latest drivers. At least AMD owned up to it so I canâ€™t be too upset but hopefully they really do fix this soon as new users may not understand whatâ€™s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly itâ€™s stable for them and they donâ€™t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs donâ€™t always have a DP connector at all.,AMD,2025-11-13 19:43:06,3
Intel,noo4q8p,"ah, that explains it. Thanks. :)",AMD,2025-11-13 18:04:59,1
Intel,nooab1c,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named â€œminimal installâ€). Obviously Iâ€™m referring to AMD driver download page.",AMD,2025-11-13 18:31:24,1
Intel,nop73kl,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",AMD,2025-11-13 21:14:50,1
Intel,novl7li,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",AMD,2025-11-14 21:28:30,1
Intel,noo4i0q,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",AMD,2025-11-13 18:03:52,22
Intel,noo7r27,What about Noise Suppression not working since 25.9.2?,AMD,2025-11-13 18:19:27,7
Intel,np8f5i6,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,AMD,2025-11-17 00:06:17,2
Intel,nopub91,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",AMD,2025-11-13 23:16:35,1
Intel,noo3ufw,Hell yeah ðŸ™‚ amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,AMD,2025-11-13 18:00:40,8
Intel,norjsvf,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",AMD,2025-11-14 05:48:36,2
Intel,nonrmjz,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",AMD,2025-11-13 17:00:38,9
Intel,noo81ru,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,AMD,2025-11-13 18:20:51,6
Intel,nosmcf6,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",AMD,2025-11-14 11:53:57,3
Intel,noniq65,That was my very first actual driver issue I experienced with AMD.,AMD,2025-11-13 16:17:06,3
Intel,noshl11,Oh that's nice! I'll look into it when I get the chance.,AMD,2025-11-14 11:13:15,1
Intel,nosh6j0,Cool. Thank you,AMD,2025-11-14 11:09:32,1
Intel,notszvs,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience ðŸ˜–,AMD,2025-11-14 16:02:13,1
Intel,not1h8l,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,AMD,2025-11-14 13:36:42,2
Intel,npb8iqb,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",AMD,2025-11-17 13:18:49,1
Intel,npbldor,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",AMD,2025-11-17 14:34:50,1
Intel,noprwpb,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,AMD,2025-11-13 23:02:52,2
Intel,nonmz2f,"Fair enough, and yeah sooner the better for all of us",AMD,2025-11-13 16:37:55,0
Intel,noofit0,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,AMD,2025-11-13 18:56:16,-1
Intel,nozg3tu,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",AMD,2025-11-15 14:45:59,1
Intel,nopq3va,Fingers crossed,AMD,2025-11-13 22:52:53,1
Intel,nsxlbh0,"Thanks for attempting to retest.  It's a 7900XTX with an Index connected via DisplayPort. I am on the latest 25.11.1 driver.  I run a monitor at 4k 120Hz 10bpc with HDR Off, which uses DSC, as my main and only display. I tried disabling DSC in the monitor settings which runs at 4k 120Hz 8bpc with HDR Off but I don't think I noticed a change in latency. I thought that DSC on and off on two different devices might contribute to the problem but I'm not sure.   I have also tried running the Index under a RX480 on another PC and I fairly certain the latency looks different under 90Hz and looks similar under 120Hz. Can't play much to test though as an RX480 runs the Index at a very blurry setting. Getting around to doing this test is what took me so long to reply.",AMD,2025-12-08 13:31:32,2
Intel,ntkkg7z,Were you able to find the issue?,AMD,2025-12-12 01:49:15,1
Intel,np0lx2u,"Allright ty, will Install new, any differences in performance?",AMD,2025-11-15 18:27:26,1
Intel,nphlmf1,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",AMD,2025-11-18 13:25:51,1
Intel,nonq3uo,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,AMD,2025-11-13 16:53:14,2
Intel,npb8jiw,Thank you for this. This was very helpful. Got adrenaline working fine now.,AMD,2025-11-17 13:18:58,2
Intel,noot79m,"I wish my LG C4 42"" had a display port. Its my primary monitor.",AMD,2025-11-13 20:04:24,3
Intel,nop8j9i,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man itâ€™s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for meâ€¦ and have zero time to reinstall Windows.",AMD,2025-11-13 21:21:55,1
Intel,nqeioib,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",AMD,2025-11-23 19:07:08,1
Intel,npaqybw,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",AMD,2025-11-17 11:00:13,5
Intel,nopyh74,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",AMD,2025-11-13 23:40:57,4
Intel,notchza,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,AMD,2025-11-14 14:38:51,2
Intel,nosoenw,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",AMD,2025-11-14 12:09:54,1
Intel,notd4le,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",AMD,2025-11-14 14:42:17,1
Intel,notu48n,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,AMD,2025-11-14 16:07:41,1
Intel,npblkwc,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,AMD,2025-11-17 14:35:55,1
Intel,nphlnml,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,AMD,2025-11-18 13:26:02,1
Intel,noru29k,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,AMD,2025-11-14 07:20:44,1
Intel,np08w4v,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",AMD,2025-11-15 17:20:05,1
Intel,ntmuect,We've still not been able to reproduce this unfortunately. I'll need to check in when I'm back at work next year,AMD,2025-12-12 12:24:51,3
Intel,np0tp7f,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,AMD,2025-11-15 19:06:34,2
Intel,nphr5q3,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",AMD,2025-11-18 13:55:53,1
Intel,nopc4t4,No you can't.,AMD,2025-11-13 21:39:44,1
Intel,nonrg54,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",AMD,2025-11-13 16:59:46,0
Intel,npbfbpp,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",AMD,2025-11-17 13:59:52,1
Intel,noq4fcn,"They are TV's, not pc monitors. Buy the right tool for the job",AMD,2025-11-14 00:15:47,-2
Intel,noxv18g,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",AMD,2025-11-15 06:30:19,1
Intel,np729v3,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",AMD,2025-11-16 19:45:29,1
Intel,npiownv,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",AMD,2025-11-18 16:47:14,1
Intel,nrkoujc,since last BF6 Update i had zero crashes also on 25.11.1,AMD,2025-11-30 18:13:02,2
Intel,noruco5,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",AMD,2025-11-14 07:23:31,2
Intel,np08z2a,What about 25.11.1?,AMD,2025-11-15 17:20:29,1
Intel,npkeuqy,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,AMD,2025-11-18 21:52:44,1
Intel,nopdsez,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,AMD,2025-11-13 21:47:54,3
Intel,noo8n6z,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",AMD,2025-11-13 18:23:39,2
Intel,nonsm12,"Look online for fsr 4 on 6000 and 5000 series, you will understand,Â    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",AMD,2025-11-13 17:05:33,2
Intel,np73g8a,Did you reboot after setting that key? Is the display with chrome still only partially updating?,AMD,2025-11-16 19:51:24,2
Intel,norvx55,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",AMD,2025-11-14 07:38:54,1
Intel,npkhv83,thank you,AMD,2025-11-18 22:08:07,1
Intel,nopey1i,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",AMD,2025-11-13 21:53:39,1
Intel,noozt1l,"Not a typo, I was asking about something else and he missed my point...",AMD,2025-11-13 20:37:57,2
Intel,norw6su,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",AMD,2025-11-14 07:41:30,2
Intel,nopm704,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",AMD,2025-11-13 22:31:45,2
Intel,nopq646,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",AMD,2025-11-13 22:53:13,0
Intel,mz2hn4c,"What a disgusting build, I love it",AMD,2025-06-21 23:44:28,164
Intel,mz2c56w,the content we crave,AMD,2025-06-21 23:11:17,81
Intel,mz2taf0,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",AMD,2025-06-22 00:56:32,49
Intel,mz35qhi,What GPU are you using in your build?  All of them,AMD,2025-06-22 02:15:29,16
Intel,mz34fmt,you're one hell of a doctor. mad setup!,AMD,2025-06-22 02:07:07,4
Intel,mz38u8t,The amount of blaspheming on display is worthy of praise.,AMD,2025-06-22 02:35:37,4
Intel,mz4f388,Brother collecting them like infinity stones lmao,AMD,2025-06-22 08:29:44,4
Intel,mz4ibrt,I'm sure those GPUs fight each others at night,AMD,2025-06-22 09:02:18,3
Intel,mz4o6eq,Bro unlocked the forbidden RGB gpus combo,AMD,2025-06-22 10:01:39,4
Intel,mz3lb45,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,AMD,2025-06-22 04:02:59,3
Intel,mz419ab,What the fuck,AMD,2025-06-22 06:15:48,3
Intel,mz520aa,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,AMD,2025-06-22 12:03:18,3
Intel,mz8w6af,Yuck,AMD,2025-06-23 00:36:46,3
Intel,mz3q5i1,Wait until you discover lossless scaling,AMD,2025-06-22 04:40:21,2
Intel,mz4pnpm,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,AMD,2025-06-22 10:16:23,2
Intel,mz4vx72,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",AMD,2025-06-22 11:13:47,2
Intel,mz57f8x,Now you just need to buy one of those ARM workstations to get the quad setup,AMD,2025-06-22 12:42:21,2
Intel,mz5dj5p,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,AMD,2025-06-22 13:22:04,2
Intel,mz65vu4,Love it lol. How do the fucking drivers work? Haha,AMD,2025-06-22 15:55:37,2
Intel,mz6knzs,What an amazing build,AMD,2025-06-22 17:11:07,2
Intel,mza30vq,wtf is that build man xdd bro collected all the infinity stones of gpu world.,AMD,2025-06-23 05:11:08,2
Intel,mzdg22n,Youâ€™re a psychopath. I love it,AMD,2025-06-23 18:23:11,2
Intel,mzeff3z,This gpu looks clean asfðŸ˜­,AMD,2025-06-23 21:12:27,2
Intel,mzf9oh7,The only setup where RGB gives more performance. :D,AMD,2025-06-23 23:54:00,2
Intel,mzgj5a3,Now you need a dual cpu mobo.,AMD,2025-06-24 04:36:20,2
Intel,mzjl4ek,Placona! I've been happy with a 6700xt for years.,AMD,2025-06-24 17:04:15,2
Intel,ng0v4qd,absolute cinema,AMD,2025-09-24 21:52:34,2
Intel,mzaqf4v,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",AMD,2025-06-23 08:51:27,1
Intel,mz3qf7i,"Brawndo has electrolytes, that's what plants crave!",AMD,2025-06-22 04:42:29,46
Intel,mz3a7jh,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",AMD,2025-06-22 02:44:38,16
Intel,mz3f8hm,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",AMD,2025-06-22 03:18:58,3
Intel,n031c2v,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",AMD,2025-06-27 15:50:28,1
Intel,mz3fahp,Team RGB,AMD,2025-06-22 03:19:20,16
Intel,mz775k1,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",AMD,2025-06-22 19:03:06,4
Intel,mz3q4dh,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here:Â https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",AMD,2025-06-22 04:40:06,13
Intel,mz5nt69,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",AMD,2025-06-22 14:21:37,3
Intel,mz4qjhz,"OpenCL works on all of them at once, and is just as fast as CUDA!",AMD,2025-06-22 10:25:02,3
Intel,mz5onps,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",AMD,2025-06-22 14:26:11,2
Intel,mz5oxpc,Start here with FluidX3D:Â https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md ðŸ––,AMD,2025-06-22 14:27:41,2
Intel,mz737je,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",AMD,2025-06-22 18:42:52,2
Intel,mzavujs,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",AMD,2025-06-23 09:45:37,1
Intel,mz3m009,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (Iâ€™m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),AMD,2025-06-22 04:08:09,9
Intel,mz57a7w,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,AMD,2025-06-22 12:41:24,4
Intel,mz3lspz,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,AMD,2025-06-22 04:06:39,4
Intel,mz3kt6w,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",AMD,2025-06-22 03:59:14,3
Intel,mz3l3jt,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",AMD,2025-06-22 04:01:24,8
Intel,mz3qt8d,Thank you so much for the very detailed response!,AMD,2025-06-22 04:45:35,3
Intel,mz5oyvv,Well worth it!,AMD,2025-06-22 14:27:51,3
Intel,mz5zat7,Thank you my man!! Looking forward to run some tests once I get home.,AMD,2025-06-22 15:21:59,2
Intel,mz74o6f,That's awesome!,AMD,2025-06-22 18:50:23,2
Intel,mzbns72,"Yes, but SLI is a bad description for it.",AMD,2025-06-23 13:13:43,1
Intel,mz3s5tj,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",AMD,2025-06-22 04:56:27,17
Intel,mz4kejl,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",AMD,2025-06-22 09:23:30,8
Intel,mz64tvp,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",AMD,2025-06-22 15:50:15,4
Intel,mz3smwy,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",AMD,2025-06-22 05:00:24,3
Intel,mz40qgf,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",AMD,2025-06-22 06:11:00,3
Intel,mz56bwd,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",AMD,2025-06-22 12:34:46,5
Intel,mz4wpgy,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",AMD,2025-06-22 11:20:29,5
Intel,mzffsev,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",AMD,2025-06-24 00:29:44,2
Intel,mz4ih7t,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  â€‹â€‹â€‹  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",AMD,2025-06-22 09:03:49,1
Intel,mz4olvb,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",AMD,2025-06-22 10:05:55,2
Intel,mz4mwra,Why are you connecting the monitor to the gpu and not the mobo?,AMD,2025-06-22 09:49:01,0
Intel,mzeajzd,"ðŸ‘   thanks for the info, this'll definitely come in handy eventually.",AMD,2025-06-23 20:49:01,1
Intel,mz4oaqj,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,AMD,2025-06-22 10:02:50,2
Intel,mzehy8b,No worries mate. Good luck,AMD,2025-06-23 21:25:07,2
Intel,mz4zjpa,"For some reason I switched up, connecting to the gpu is the way to go. I derped",AMD,2025-06-22 11:44:11,3
Intel,nlb3nwr,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-10-25 13:40:37,1
Intel,ms76zj5,It's alive. Rejoice.,AMD,2025-05-14 01:54:03,3
Intel,ms6f1il,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2025-05-13 23:11:19,1
Intel,m84i6ct,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",AMD,2025-01-20 06:59:20,19
Intel,m84uer1,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",AMD,2025-01-20 09:01:59,15
Intel,m8861s4,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,AMD,2025-01-20 20:45:52,6
Intel,m80r0p3,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,AMD,2025-01-19 18:16:28,34
Intel,m8efiwt,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,AMD,2025-01-21 19:23:32,2
Intel,m84nhes,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,AMD,2025-01-20 07:50:12,2
Intel,m83he9u,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",AMD,2025-01-20 02:32:38,-8
Intel,m862icn,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",AMD,2025-01-20 14:58:27,7
Intel,m84neo0,I'm fairly sure they use dxvk for d3d9 to 11.,AMD,2025-01-20 07:49:28,5
Intel,m872p8h,Could just be a cache issue,AMD,2025-01-20 17:49:03,2
Intel,m8c5h0v,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,AMD,2025-01-21 12:24:17,1
Intel,m85qkad,Glad you brought up Nvidia as I didnâ€™t know this had improved until the testing around Arc showed it had gone.,AMD,2025-01-20 13:49:31,3
Intel,m80ufhx,"According to the graphs, AMD has slightly less overhead than NVIDIA.",AMD,2025-01-19 18:32:18,79
Intel,m8290el,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",AMD,2025-01-19 22:33:50,13
Intel,m874iee,"Lowest with DX11 and older, but not with the newer APIs",AMD,2025-01-20 17:56:51,1
Intel,m81i5d3,And when is the last time HUB did a dedicated video showing the improvement in overhead?,AMD,2025-01-19 20:25:39,0
Intel,m873isl,or it's just a cache/memory access issue,AMD,2025-01-20 17:52:35,1
Intel,m83l8d5,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",AMD,2025-01-20 02:54:04,25
Intel,m83sg28,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",AMD,2025-01-20 03:39:34,15
Intel,m83s1d0,"Intel uses software translation for DX11 and lower, so it does matter for them.",AMD,2025-01-20 03:36:52,0
Intel,m82afin,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",AMD,2025-01-19 22:40:55,-17
Intel,m82o5am,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",AMD,2025-01-19 23:53:09,0
Intel,m862pny,That's not true. Intel's issue is being too verbose in commands/calls.,AMD,2025-01-20 14:59:30,0
Intel,m83h5jp,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",AMD,2025-01-20 02:31:29,-16
Intel,m83sird,HUB used DX12 games that also showed the issue.  It's something else.,AMD,2025-01-20 03:40:04,5
Intel,m87xk13,"The comment to which I am replying is talking about nVidia, not Intel.",AMD,2025-01-20 20:07:14,6
Intel,m84dadg,"Iâ€™m pretty sure HUB doesnâ€™t like Nvidia *or* AMD. Theyâ€™re calling it how it is, these parts are too damn expensive.",AMD,2025-01-20 06:15:54,9
Intel,m83slz3,That's actually... just worse news.,AMD,2025-01-20 03:40:39,3
Intel,lfjff1l,I always dreamt of the day APUs become power houses.,AMD,2024-07-29 19:57:14,57
Intel,lfj5g73,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",AMD,2024-07-29 19:03:41,19
Intel,lfltm14,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",AMD,2024-07-30 05:13:45,2
Intel,lfqfwra,Damn Why is AMD even involved in iGPU,AMD,2024-07-30 23:50:46,1
Intel,lfjm4t2,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",AMD,2024-07-29 20:32:18,-15
Intel,lfjhomu,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",AMD,2024-07-29 20:09:09,49
Intel,lfjtsec,almost there,AMD,2024-07-29 21:13:13,3
Intel,lfkaj8b,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",AMD,2024-07-29 22:50:53,1
Intel,lfkuvgo,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",AMD,2024-07-30 00:57:59,0
Intel,lfkjnlw,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,AMD,2024-07-29 23:47:05,-2
Intel,lfjfk07,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",AMD,2024-07-29 19:57:57,22
Intel,lfkemqm,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,AMD,2024-07-29 23:15:53,2
Intel,lfjlhvn,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",AMD,2024-07-29 20:28:55,3
Intel,lgze3vw,"It depends on what your goals are for a laptop.Â  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!Â Â  I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.Â  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",AMD,2024-08-07 18:47:35,1
Intel,lfjrf1q,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",AMD,2024-07-29 21:00:13,1
Intel,lfjr0pr,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",AMD,2024-07-29 20:58:06,-7
Intel,lfjet3n,yes its so bad. better go buy some steam deck or ally x,AMD,2024-07-29 19:54:02,-10
Intel,lfjomos,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,AMD,2024-07-29 20:45:29,12
Intel,lfji4cg,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",AMD,2024-07-29 20:11:25,17
Intel,lfk18sm,How are they going to feed all those CUs? Quad-channel LPDDR5X?,AMD,2024-07-29 21:55:13,5
Intel,lfkuy27,That's considerably faster than an XSX.,AMD,2024-07-30 00:58:27,2
Intel,lfkvkit,>That's tapping on 4070/7800 levels of performance.  What is?,AMD,2024-07-30 01:02:29,3
Intel,lfmp8zh,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",AMD,2024-07-30 10:56:08,3
Intel,lfjj0he,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",AMD,2024-07-29 20:15:59,5
Intel,lfm3fxr,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,AMD,2024-07-30 06:54:17,1
Intel,lfkw2is,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,AMD,2024-07-30 01:05:44,2
Intel,lflubg9,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",AMD,2024-07-30 05:20:30,2
Intel,lfjw9yq,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",AMD,2024-07-29 21:27:05,4
Intel,lfkbfbe,It's called satire. You're just salty because you're the butt of the joke.,AMD,2024-07-29 22:56:19,-3
Intel,lfkw8g2,throw it in the next steamdeck and Iâ€™ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,AMD,2024-07-30 01:06:50,5
Intel,lflsl6l,Praying the blade16 gets it.,AMD,2024-07-30 05:04:09,1
Intel,lfk3os9,"This is the rumor, if youâ€™re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",AMD,2024-07-29 22:09:30,12
Intel,lfk4vp7,256 bit bus + infinity cache.,AMD,2024-07-29 22:16:36,11
Intel,lfkfxeg,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,AMD,2024-07-29 23:23:53,2
Intel,lfl3c3y,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",AMD,2024-07-30 01:53:05,1
Intel,lfl04sh,"Yes, itâ€™s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",AMD,2024-07-30 01:32:08,1
Intel,lfovbfq,The rumored 40CU strix halo chip. Not the actual chips released this week.,AMD,2024-07-30 18:37:40,1
Intel,lfkzt9q,7500mhz ram and the 780m,AMD,2024-07-30 01:30:05,2
Intel,lflujq4,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",AMD,2024-07-30 05:22:43,2
Intel,lfm7511,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",AMD,2024-07-30 07:34:59,1
Intel,lfk4w6h,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",AMD,2024-07-29 22:16:41,9
Intel,lfkvrtv,Literally where did you see 40-60% uplift at half the power?,AMD,2024-07-30 01:03:49,5
Intel,lfnnej3,> 40-60% performance uplift at half the power  Source?,AMD,2024-07-30 14:48:25,1
Intel,lfm3q9d,"i chuckled, then again im not a fanboy of anything",AMD,2024-07-30 06:57:22,-1
Intel,lflvl1g,Dont expect 40CUs in a handheld anytime soon,AMD,2024-07-30 05:32:53,8
Intel,lfmyyqu,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",AMD,2024-07-30 12:16:43,1
Intel,lg35wq0,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.Â  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.Â  The system should have an actual TRUE quadcore memory setup.Â  Many of these systems have currently (and will absolutely continue to have)Â dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.Â  This matches my system that runs a 780m with 7500mhz lpddr5x.Â  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.Â  This will continue to hamstring these APUs regardless of how many cu they throw at em.",AMD,2024-08-02 03:44:51,1
Intel,lgyqo0o,"â€œAbsolute monsterâ€? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.Â   Â AMD has a long way to go before claiming â€œMonsterâ€ class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet arenâ€™t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Maxâ€¦ let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",AMD,2024-08-07 16:49:14,1
Intel,lfp60n3,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,AMD,2024-07-30 19:33:48,1
Intel,lfql0n0,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. Thatâ€™s far from desktop 4070, never mind 4080.",AMD,2024-07-31 00:22:30,4
Intel,lfo4zrj,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,AMD,2024-07-30 16:22:11,1
Intel,lfoeo9v,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,AMD,2024-07-30 17:12:32,0
Intel,lukc8v1,">AMD has a long way to go before claiming â€œMonsterâ€ class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",AMD,2024-10-30 18:32:02,1
Intel,lukp0ww,"APU is one of AMDâ€™s biggest markets. You are kidding if you think they donâ€™t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they donâ€™t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Appleâ€™s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games arenâ€™t the only thing APUs are used for so PS5 isnâ€™t wholey in the conversation. PS5 also costs monthly to play online and their games arenâ€™t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",AMD,2024-10-30 19:35:13,1
Intel,lukywwo,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Appleâ€™s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",AMD,2024-10-30 20:22:39,1
Intel,ldaak7j,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2024-07-15 13:10:50,1
Intel,leiilpv,"Hey OP â€” PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",AMD,2024-07-23 08:23:24,1
Intel,lekd2f5,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,AMD,2024-07-23 16:24:13,30
Intel,lejyiil,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",AMD,2024-07-23 15:07:15,19
Intel,lelur0p,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",AMD,2024-07-23 21:04:22,7
Intel,lek4mor,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,AMD,2024-07-23 15:39:41,2
Intel,leouddh,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",AMD,2024-07-24 11:04:39,1
Intel,lep6hwc,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",AMD,2024-07-24 12:39:31,1
Intel,leufb7c,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",AMD,2024-07-25 09:17:02,1
Intel,lehh8b4,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-07-23 02:41:24,1
Intel,len76ez,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,AMD,2024-07-24 01:57:07,1
Intel,lelfwyp,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,AMD,2024-07-23 19:47:16,0
Intel,lelodyi,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",AMD,2024-07-23 20:31:10,0
Intel,leki2kn,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",AMD,2024-07-23 16:50:30,4
Intel,lemusx8,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",AMD,2024-07-24 00:37:13,1
Intel,lenkqpy,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,AMD,2024-07-24 03:30:22,0
Intel,lem1iup,"Installs beta software, proceeds to complain about it",AMD,2024-07-23 21:41:28,1
Intel,lenbfz4,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,AMD,2024-07-24 02:25:00,1
Intel,lem77tu,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",AMD,2024-07-23 22:13:57,0
Intel,lelhk36,What Ghost of Tsushima issue?,AMD,2024-07-23 19:55:44,1
Intel,lelridi,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",AMD,2024-07-23 20:47:19,8
Intel,lf385p0,"This was a graphics card, not a â€˜GPUâ€™ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",AMD,2024-07-26 20:25:40,1
Intel,leorvpo,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",AMD,2024-07-24 10:41:40,5
Intel,lelhp6y,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,AMD,2024-07-23 19:56:28,0
Intel,lem0nam,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",AMD,2024-07-23 21:36:35,-3
Intel,lf3gd3s,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",AMD,2024-07-26 21:11:19,0
Intel,lf88lah,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,AMD,2024-07-27 19:04:01,0
Intel,lezwia9,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",AMD,2024-07-26 06:45:51,1
Intel,lem6kr4,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,AMD,2024-07-23 22:10:14,9
Intel,lf1fo06,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,AMD,2024-07-26 14:36:17,2
Intel,lenktr1,The documentation for it would still be in their archives,AMD,2024-07-24 03:31:01,-2
Intel,lep98lz,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",AMD,2024-07-24 12:57:51,5
Intel,ky7tcb2,"Pretty annoying how everything follows the same linear fps/price curve, thereâ€™s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",AMD,2024-04-05 19:25:59,21
Intel,ky7p0fb,Wish Arc cards were better. They look so pretty in comparison to their peers,AMD,2024-04-05 19:01:17,11
Intel,ky7t8hc,Thats actually a pretty solid and accurate breakdown.,AMD,2024-04-05 19:25:23,4
Intel,ky7m91o,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,AMD,2024-04-05 18:45:54,11
Intel,kyooqk9,3080 still looking good too,AMD,2024-04-08 22:34:34,2
Intel,kyakde9,What they have peaceful then 4k series?,AMD,2024-04-06 07:27:42,1
Intel,kyjljxe,Just get a 4090. I will never regret getting mine.,AMD,2024-04-07 23:42:07,1
Intel,kys0jes,i miss old good times where radeon HD 7970 as best single core card cost around 400$,AMD,2024-04-09 15:02:55,1
Intel,kzdsbrd,"Damn, the A770 is still so uncompetitive...",AMD,2024-04-13 13:49:40,1
Intel,kybklob,"It's like the free market priced cards according to their relative performance. How weird, right?",AMD,2024-04-06 13:42:41,1
Intel,kyjjx67,How is that possibly annoying,AMD,2024-04-07 23:31:52,0
Intel,kya236v,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,AMD,2024-04-06 04:17:14,2
Intel,kyaw0hp,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",AMD,2024-04-06 09:51:52,1
Intel,kybpb3p,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",AMD,2024-04-06 14:15:00,2
Intel,kygdnfc,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,AMD,2024-04-07 11:17:10,1
Intel,kys12cm,8gb perfectly fine today :),AMD,2024-04-09 15:06:00,1
Intel,l9ad3sk,"Ah yes sure, now where did I leave my 1500 euros?",AMD,2024-06-19 10:11:00,2
Intel,kybkrrc,"I donâ€™t mind free markets, Iâ€™m just saying the state of the market is less fun now than it used to be.",AMD,2024-04-06 13:43:53,9
Intel,kymgwzk,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,AMD,2024-04-08 14:36:56,1
Intel,kya4qoq,"Yeah, i like the black super series.",AMD,2024-04-06 04:40:54,1
Intel,kyw7k0z,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",AMD,2024-04-10 08:27:23,0
Intel,kybtcsj,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",AMD,2024-04-06 14:41:11,2
Intel,kxhli0e,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,AMD,2024-04-01 02:17:59,223
Intel,kxl9t8e,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",AMD,2024-04-01 19:43:02,24
Intel,kxiush3,"""The ability to â€œturn it off and on againâ€ should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",AMD,2024-04-01 10:12:15,110
Intel,kxrny0e,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",AMD,2024-04-02 22:36:02,17
Intel,kxkeqm3,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",AMD,2024-04-01 16:50:42,29
Intel,kxhn7gu,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",AMD,2024-04-01 02:30:21,119
Intel,kxi9i5m,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",AMD,2024-04-01 05:48:52,67
Intel,kxpi7rl,"Yo, I saw the title and thought this gotta be Gnif2.",AMD,2024-04-02 15:15:20,6
Intel,kxhii78,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",AMD,2024-04-01 01:56:41,35
Intel,kxisjb3,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",AMD,2024-04-01 09:45:49,35
Intel,kxhfw6h,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",AMD,2024-04-01 01:38:50,62
Intel,kxiukyk,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",AMD,2024-04-01 10:09:50,14
Intel,kxiah6c,"Iâ€™ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, itâ€™s usually a day late and a dollar short. Blender on Linux still canâ€™t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that thereâ€™s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but weâ€™ll have to wait and see.",AMD,2024-04-01 05:59:50,24
Intel,kxlnigb,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*â„¢ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",AMD,2024-04-01 20:59:38,22
Intel,ky0wzku,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",AMD,2024-04-04 15:28:04,4
Intel,ky567n0,Long but worth it read; Well Done!,AMD,2024-04-05 08:38:06,4
Intel,kxnqc72,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",AMD,2024-04-02 05:31:11,3
Intel,ky1f7to,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,AMD,2024-04-04 17:07:58,3
Intel,l012ykv,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",AMD,2024-04-17 19:05:55,3
Intel,kxitz3a,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",AMD,2024-04-01 10:02:50,24
Intel,kxmpmyk,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,AMD,2024-04-02 00:54:21,5
Intel,kxp7mvs,Iâ€™ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,AMD,2024-04-02 14:13:09,6
Intel,kxq8p0p,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",AMD,2024-04-02 17:41:45,5
Intel,kxr0ydr,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,AMD,2024-04-02 20:16:04,5
Intel,kxtpd72,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",AMD,2024-04-03 08:19:54,5
Intel,kxj7ncd,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",AMD,2024-04-01 12:18:50,12
Intel,kxirbw1,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100Âºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",AMD,2024-04-01 09:31:11,16
Intel,kxnysdb,Lmao as a recent AMD intern I feel this in my bones. I still canâ€™t fathom just how little effort is put into software stability these days.,AMD,2024-04-02 07:08:39,4
Intel,kxi4dih,100% all of this...  Love looking glass by the by,AMD,2024-04-01 04:54:44,7
Intel,kxt140w,How does say VMware handle this? Does it kind of just restart shit as needed?,AMD,2024-04-03 04:01:28,2
Intel,kxibc53,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",AMD,2024-04-01 06:09:51,18
Intel,kxizp6h,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",AMD,2024-04-01 11:05:58,5
Intel,kxju0p0,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",AMD,2024-04-01 14:52:01,3
Intel,kxjywwd,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,AMD,2024-04-01 15:20:47,3
Intel,kxkj3fj,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",AMD,2024-04-01 17:15:05,3
Intel,kxilacf,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",AMD,2024-04-01 08:13:50,2
Intel,kxikwgx,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",AMD,2024-04-01 08:08:54,1
Intel,kxnag16,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",AMD,2024-04-02 03:12:09,1
Intel,kxqkz3h,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",AMD,2024-04-02 18:48:54,1
Intel,kxk4suo,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",AMD,2024-04-01 15:54:31,0
Intel,kxjykgb,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,AMD,2024-04-01 15:18:47,0
Intel,kxnctg8,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",AMD,2024-04-02 03:30:01,0
Intel,kxierbw,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",AMD,2024-04-01 06:50:41,-8
Intel,kxxhwq9,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",AMD,2024-04-03 23:01:13,-1
Intel,kxip0e1,TL;DR. **PEBKAC**.,AMD,2024-04-01 09:01:51,-23
Intel,kxk9iir,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-01 16:21:24,-2
Intel,kxksj8e,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",AMD,2024-04-01 18:06:47,-3
Intel,kxo5btd,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,AMD,2024-04-02 08:32:08,-5
Intel,kxiw3lo,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",AMD,2024-04-01 10:27:10,44
Intel,ky1fyc2,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  ðŸ˜¤ðŸ˜­,AMD,2024-04-04 17:12:00,3
Intel,kxjwsde,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",AMD,2024-04-01 15:08:22,28
Intel,kxte67y,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,AMD,2024-04-03 06:04:47,2
Intel,kxkf630,"Thanks mate I appreciate it, glad to see you here :)",AMD,2024-04-01 16:53:06,15
Intel,kxtip4r,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",AMD,2024-04-03 06:57:08,5
Intel,ll8wytp,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",AMD,2024-09-03 02:42:30,1
Intel,kxhow6p,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",AMD,2024-04-01 02:42:51,34
Intel,kxhpa3h,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",AMD,2024-04-01 02:45:39,12
Intel,kxjf8yq,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,AMD,2024-04-01 13:17:38,13
Intel,kxpa05g,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,AMD,2024-04-02 14:27:21,-3
Intel,kxiv9ac,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",AMD,2024-04-01 10:17:32,18
Intel,kxp8y84,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,AMD,2024-04-02 14:21:05,9
Intel,kxjfdjy,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",AMD,2024-04-01 13:18:34,5
Intel,kxj3tba,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",AMD,2024-04-01 11:45:39,9
Intel,kxjhcp0,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",AMD,2024-04-01 13:32:25,4
Intel,kxjknpx,"Excellent post, very informative. Would take issue with this though:Â Â  Â    ""Speaking of VRAM, The drivers use VRAM less efficiently. Look atÂ any side-by-side comparison between games on YouTubeÂ between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.Â    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?Â    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.Â    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?Â    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",AMD,2024-04-01 13:54:35,3
Intel,kxtwy1v,"Funny, I saw the title and thought the same too!",AMD,2024-04-03 09:54:20,5
Intel,kxhlmwx,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",AMD,2024-04-01 02:18:57,26
Intel,kxn102r,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",AMD,2024-04-02 02:07:08,-1
Intel,kxnsbw0,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,AMD,2024-04-02 05:52:30,9
Intel,kxjj86s,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",AMD,2024-04-01 13:45:07,9
Intel,kxjs7vy,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",AMD,2024-04-01 14:41:18,-4
Intel,kxi3d8c,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",AMD,2024-04-01 04:44:52,12
Intel,kxvte63,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",AMD,2024-04-03 17:32:25,3
Intel,kxmufyt,ursohot !  back to discord rants...,AMD,2024-04-02 01:24:48,-5
Intel,kxix377,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,AMD,2024-04-01 10:38:16,24
Intel,kxmy36x,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocksÂ EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",AMD,2024-04-02 01:48:12,7
Intel,kxjbu8k,"I dunno man. Iâ€™ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I havenâ€™t used anything by team green lately, so it may all be the same shit , different pile.",AMD,2024-04-01 12:52:07,5
Intel,kxlfj2c,Lol same with me tbh I haven't had any problems ðŸ˜‚ but I guess some do idk ðŸ¤·. I have crashed less with AMD than my old  Nvidia card.,AMD,2024-04-01 20:14:49,3
Intel,kxnky9y,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",AMD,2024-04-02 04:38:17,0
Intel,kxj2kjm,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me Â£540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent Â£200 more on a second hand 3090 instead of this.",AMD,2024-04-01 11:34:09,4
Intel,kxta6ee,"It doesn't handle it, it has the same issue.",AMD,2024-04-03 05:22:41,2
Intel,kxj4eg4,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),AMD,2024-04-01 11:50:55,11
Intel,kxj38ou,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",AMD,2024-04-01 11:40:25,5
Intel,kxiu2ph,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",AMD,2024-04-01 10:03:58,4
Intel,kxidqq0,Me neither. I use a RX580 8GB since launch and not a single problem.,AMD,2024-04-01 06:38:22,3
Intel,kxie3oi,Because they're talking absolute rubbish that's why.,AMD,2024-04-01 06:42:43,-17
Intel,kxj72uk,You are one of the lucky ones!,AMD,2024-04-01 12:14:06,8
Intel,kxue41z,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",AMD,2024-04-03 12:32:07,2
Intel,kximvz5,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",AMD,2024-04-01 08:34:35,12
Intel,kxjkdyv,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",AMD,2024-04-01 13:52:49,2
Intel,kxi3fxr,lol your flair is Please search before asking,AMD,2024-04-01 04:45:36,-3
Intel,kyy38w2,"Hey OP â€” Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-10 17:04:31,1
Intel,kxipuql,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,AMD,2024-04-01 09:12:36,-1
Intel,kxt2f9e,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,AMD,2024-04-03 04:12:16,1
Intel,kxiexwv,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",AMD,2024-04-01 06:52:56,30
Intel,kxxifs5,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",AMD,2024-04-03 23:04:27,4
Intel,kxkxwhq,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,AMD,2024-04-01 18:36:38,5
Intel,kxo5nh7,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,AMD,2024-04-02 08:36:20,7
Intel,kxmvpp1,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",AMD,2024-04-02 01:33:01,47
Intel,ky1ipao,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",AMD,2024-04-04 17:26:58,2
Intel,kxkcepy,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",AMD,2024-04-01 16:37:46,29
Intel,kxs8nai,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",AMD,2024-04-03 00:45:36,5
Intel,kxk4crx,"""NVIDIA, it just works""",AMD,2024-04-01 15:51:58,14
Intel,kxncqt4,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,AMD,2024-04-02 03:29:27,4
Intel,kxof5tw,What is the AMD Vanguard?,AMD,2024-04-02 10:31:39,8
Intel,kxtr5do,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",AMD,2024-04-03 08:42:33,8
Intel,kxnum1q,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,AMD,2024-04-02 06:18:22,6
Intel,kxjkmnv,You misspelled $2.3T market cap....,AMD,2024-04-01 13:54:24,10
Intel,kxjp8qb,"Okay yeah fair enough, hadn't considered this. Removed it from my post",AMD,2024-04-01 14:23:19,1
Intel,kxxn4fl,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",AMD,2024-04-03 23:33:02,2
Intel,kxpe18q,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",AMD,2024-04-02 14:51:06,2
Intel,kxlmn5s,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",AMD,2024-04-01 20:54:42,1
Intel,kxjv1e3,This is not a fix. It's a compromise.,AMD,2024-04-01 14:58:00,12
Intel,kxjpkam,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",AMD,2024-04-01 14:25:16,2
Intel,kxtj7av,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,AMD,2024-04-03 07:03:13,1
Intel,kxmam0y,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",AMD,2024-04-01 23:20:26,10
Intel,kxxefr8,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,AMD,2024-04-03 22:40:23,1
Intel,kxpad65,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,AMD,2024-04-02 14:29:30,-1
Intel,kxjq477,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",AMD,2024-04-01 14:28:37,22
Intel,kxi6i64,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",AMD,2024-04-01 05:16:16,20
Intel,kxllisv,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",AMD,2024-04-01 20:48:17,5
Intel,kxoidrh,The comment I quoted was talking about people playing games having issues.,AMD,2024-04-02 11:05:13,4
Intel,kxoc6dt,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,AMD,2024-04-02 09:57:53,3
Intel,kxoib9e,The thing I quoted was talking about people playing games though.,AMD,2024-04-02 11:04:33,2
Intel,kxjibo8,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",AMD,2024-04-01 13:38:59,5
Intel,kxj9jkm,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",AMD,2024-04-01 12:34:08,5
Intel,kxjdtt9,"Idk, I don't use Linux",AMD,2024-04-01 13:07:13,-11
Intel,kxjdrs5,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",AMD,2024-04-01 13:06:49,1
Intel,kxigqbh,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",AMD,2024-04-01 07:15:19,32
Intel,kxj2oqt,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",AMD,2024-04-01 11:35:13,1
Intel,kxj4abt,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:53,-1
Intel,kxih6b1,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),AMD,2024-04-01 07:20:59,30
Intel,kxm7xhx,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",AMD,2024-04-01 23:03:36,1
Intel,kxuiptm,Because adding a feature for a product literally gives users more control for that product.,AMD,2024-04-03 13:05:04,1
Intel,kxine7u,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,AMD,2024-04-01 08:41:11,1
Intel,kxis9nq,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",AMD,2024-04-01 09:42:40,4
Intel,kyhsjnw,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-07 17:08:48,1
Intel,kxjqk3k,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",AMD,2024-04-01 14:31:18,-2
Intel,kxzlw7y,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,AMD,2024-04-04 09:36:41,1
Intel,kxmwxwt,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",AMD,2024-04-02 01:40:54,7
Intel,kxj49ms,"Hey OP â€” Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",AMD,2024-04-01 11:49:43,-2
Intel,kxs4to2,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",AMD,2024-04-03 00:21:22,6
Intel,ky39ja5,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,AMD,2024-04-04 23:11:22,5
Intel,ky4zrtz,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",AMD,2024-04-05 07:20:00,3
Intel,kxldpfb,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",AMD,2024-04-01 20:04:38,16
Intel,kxp3oh8,*wayland users have joined the chat,AMD,2024-04-02 13:48:33,10
Intel,kxm4qt3,You're falling for slogans.,AMD,2024-04-01 22:43:30,0
Intel,kxobyv3,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",AMD,2024-04-02 09:55:25,13
Intel,kxpaw46,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,AMD,2024-04-02 14:32:39,10
Intel,kxojs3c,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),AMD,2024-04-02 11:18:39,5
Intel,kxtnu71,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",AMD,2024-04-03 08:00:44,2
Intel,kxjpcl3,Honestly after a trillion I kinda stop counting ðŸ˜‚ðŸ¤£,AMD,2024-04-01 14:23:58,3
Intel,kxjvfz1,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",AMD,2024-04-01 15:00:22,8
Intel,kxpf9fv,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",AMD,2024-04-02 14:58:15,8
Intel,kxodaii,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",AMD,2024-04-02 10:10:50,5
Intel,kxjvmo3,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",AMD,2024-04-01 15:01:28,7
Intel,kxpamp2,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,AMD,2024-04-02 14:31:05,-3
Intel,kxy4p6p,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",AMD,2024-04-04 01:24:00,1
Intel,kxpia4a,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",AMD,2024-04-02 15:15:42,4
Intel,kxjr4lw,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",AMD,2024-04-01 14:34:44,8
Intel,kxp7oc3,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",AMD,2024-04-02 14:13:24,-1
Intel,kxi7ym2,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",AMD,2024-04-01 05:31:48,3
Intel,kxm9n9f,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",AMD,2024-04-01 23:14:25,2
Intel,kxk5inl,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",AMD,2024-04-01 15:58:39,0
Intel,kxiim2c,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",AMD,2024-04-01 07:39:33,-4
Intel,kxih401,Oh then just ignore my comment ðŸ˜…,AMD,2024-04-01 07:20:10,-1
Intel,kxjfryq,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",AMD,2024-04-01 13:21:24,3
Intel,kxiojjd,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,AMD,2024-04-01 08:55:52,9
Intel,kxiiqcv,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",AMD,2024-04-01 07:41:05,-12
Intel,kxin4tk,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",AMD,2024-04-01 08:37:50,-13
Intel,kxmwd7i,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",AMD,2024-04-02 01:37:14,1
Intel,kxioc93,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,AMD,2024-04-01 08:53:17,7
Intel,kxiqori,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",AMD,2024-04-01 09:23:10,2
Intel,kxiuak1,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,AMD,2024-04-01 10:06:29,1
Intel,kxit1y6,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,AMD,2024-04-01 09:52:00,-2
Intel,kxjg5xf,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",AMD,2024-04-01 13:24:09,-2
Intel,kxjr7cc,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",AMD,2024-04-01 14:35:12,5
Intel,kxzn1iw,"Too soon to tell, but hopes are high.",AMD,2024-04-04 09:50:05,2
Intel,kxo5u7w,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2024-04-02 08:38:44,1
Intel,kxoprjw,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",AMD,2024-04-02 12:09:39,12
Intel,kxm2qa6,"Agreed, they cannot rest on their laurels.",AMD,2024-04-01 22:30:48,3
Intel,kxn01lt,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",AMD,2024-04-02 02:00:52,27
Intel,kxnsapp,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",AMD,2024-04-02 05:52:08,3
Intel,kxpuexg,console gamers know pcâ€™s are better and donâ€™t really complain about upscaling and 30fps.. youâ€™re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldnâ€™t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,AMD,2024-04-02 16:23:44,2
Intel,kxpwkoo,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,AMD,2024-04-02 16:35:41,1
Intel,kxk96s0,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",AMD,2024-04-01 16:19:33,6
Intel,kxpcxh7,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned offÂ   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",AMD,2024-04-02 14:44:41,5
Intel,kxiic2i,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",AMD,2024-04-01 07:35:56,39
Intel,kxi921e,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",AMD,2024-04-01 05:43:54,14
Intel,kxijoyb,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",AMD,2024-04-01 07:53:26,20
Intel,kxiqghx,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,AMD,2024-04-01 09:20:14,17
Intel,kxiitb5,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,AMD,2024-04-01 07:42:10,18
Intel,kxj5139,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,AMD,2024-04-01 11:56:29,6
Intel,kxio9nt,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",AMD,2024-04-01 08:52:23,11
Intel,kxn5a9z,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",AMD,2024-04-02 02:35:33,2
Intel,kxjrku0,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,AMD,2024-04-01 14:37:29,6
Intel,kxipvh2,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",AMD,2024-04-01 09:12:52,14
Intel,kxjy6gb,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,AMD,2024-04-01 15:16:31,-2
Intel,kxp15kv,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete.Â And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",AMD,2024-04-02 13:32:11,6
Intel,kxn7ur7,When did they do this switch? I remember years ago when I configured that their windows drivers werenâ€™t being so nice to the card detected in a VM.,AMD,2024-04-02 02:53:24,2
Intel,kxq0m39,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",AMD,2024-04-02 16:57:48,0
Intel,kxq98bx,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",AMD,2024-04-02 17:44:39,4
Intel,kxm4q67,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",AMD,2024-04-01 22:43:23,6
Intel,kxq0fuf,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",AMD,2024-04-02 16:56:51,2
Intel,kxpfg1v,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,AMD,2024-04-02 14:59:19,-1
Intel,kxin2k0,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",AMD,2024-04-01 08:37:02,17
Intel,kxj2kf3,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?Â    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",AMD,2024-04-01 11:34:06,3
Intel,kxnjdov,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",AMD,2024-04-02 04:23:59,-2
Intel,kxisrca,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",AMD,2024-04-01 09:48:29,6
Intel,kxs5a0e,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",AMD,2024-04-03 00:24:15,2
Intel,kxj34w0,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",AMD,2024-04-01 11:39:28,-7
Intel,kxindr9,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",AMD,2024-04-01 08:41:01,-7
Intel,kxiniuo,Oh and XE also have bug feature reporting.  Omfg!!!!,AMD,2024-04-01 08:42:51,-3
Intel,kxl4asu,Nobody is 100% right ;),AMD,2024-04-01 19:12:15,-1
Intel,kxta5m0,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),AMD,2024-04-03 05:22:28,2
Intel,kxiq2zk,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",AMD,2024-04-01 09:15:31,-6
Intel,kxjix5f,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",AMD,2024-04-01 13:43:03,-1
Intel,kxjz1ko,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",AMD,2024-04-01 15:21:32,2
Intel,kxthgxe,What about using a DP to HDMI 2.1 adapter for that situation?,AMD,2024-04-03 06:42:39,2
Intel,kxnvnrf,"2021 my guy, it's right there on the date of the article.",AMD,2024-04-02 06:30:33,8
Intel,kxqftwv,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,AMD,2024-04-02 18:20:45,-1
Intel,kxp8mfb,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,AMD,2024-04-02 14:19:07,2
Intel,kxipvcp,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",AMD,2024-04-01 09:12:49,18
Intel,kxj4mkp,And I guess infallible game developers too then. /s,AMD,2024-04-01 11:52:55,5
Intel,kxjlszk,So you decide what criticism is valid and what not? lol,AMD,2024-04-01 14:01:58,7
Intel,kxio3k4,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,AMD,2024-04-01 08:50:13,9
Intel,kxioj2i,"Yup, but do you see them making a big press release about it?",AMD,2024-04-01 08:55:43,6
Intel,kxno85r,that is not how it works but sure,AMD,2024-04-02 05:09:33,2
Intel,kxtv199,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,AMD,2024-04-03 09:31:19,2
Intel,kxjk8f2,>whine about Redditors.  The irony.,AMD,2024-04-01 13:51:48,-3
Intel,kxu2whw,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",AMD,2024-04-03 10:58:25,0
Intel,kxqg0v8,learn to comprehend.,AMD,2024-04-02 18:21:49,3
Intel,kxiqgpx,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,AMD,2024-04-01 09:20:19,8
Intel,kxj4whx,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",AMD,2024-04-01 11:55:21,4
Intel,kxnjs9x,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",AMD,2024-04-02 04:27:38,0
Intel,kxl4djq,"No, that would be you obviously /s",AMD,2024-04-01 19:12:41,-2
Intel,kxivsl5,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,AMD,2024-04-01 10:23:43,-3
Intel,kxivodj,"Yea, given the state of XE drivers every major update has come with significant PR.",AMD,2024-04-01 10:22:23,-1
Intel,kxnxxva,Why not ;),AMD,2024-04-02 06:58:11,0
Intel,kxqg47j,Go word salad elsewhere.,AMD,2024-04-02 18:22:19,-1
Intel,kxnwc84,"I have replicated the issue reliably yes, and across two different systems.",AMD,2024-04-02 06:38:43,4
Intel,kxjrbmq,If discord crashes my drivers.. once every few hours. I have to reboot,AMD,2024-04-01 14:35:55,5
Intel,kxo4jke,Discord doesn't crash my drivers  I don't have to reboot.,AMD,2024-04-02 08:22:06,0
Intel,kpp4kwl,Really love how the 6000 series radeons look.,AMD,2024-02-09 21:57:31,12
Intel,kpqv9od,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",AMD,2024-02-10 05:25:10,4
Intel,kpougfk,That's a good looking line up,AMD,2024-02-09 20:58:04,1
Intel,kps7pkq,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",AMD,2024-02-10 14:18:43,1
Intel,kpr86tx,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",AMD,2024-02-10 07:45:28,4
Intel,kpq3r57,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",AMD,2024-02-10 01:49:13,3
Intel,kptibdx,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,AMD,2024-02-10 19:15:04,-1
Intel,kptwmeu,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",AMD,2024-02-10 20:44:28,3
Intel,kpv2g8f,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,AMD,2024-02-11 01:23:45,1
Intel,kpv5euk,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",AMD,2024-02-11 01:44:32,3
Intel,kpvwyyr,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",AMD,2024-02-11 05:16:13,2
Intel,nzwgm1n,Dual gpu? Huh? Havenâ€™t seen one of those since gtx 690. It used to be useless because memory wasnâ€™t shared. I wonder how it will work this time,Intel,2026-01-16 11:14:50,10
Intel,nzxbywf,"lol. Six months late and $300 (60%) over the announced price.  If the dual GPU version was really available for $999 right now (as announced), then Intel would make significant inroads into the local AI market.  As it is, buying this for $800 over a used 3090 is a really hard sell. Compared to a B60, the 3090 is readily available for $1100, and provides the same VRAM, double the compute and memory bandwidth, better perf/watt, and CUDA support.  With the dual GPU cards even at $2k each this does have one single niche - being able to get 144 GB of VRAM in a server at under 1500W for under $10k -  which is legitimately useful for LLM inference.  It's really sad that Intel didn't put in the investment a year ago to have a lot of capacity to produce these now. For the prices to be so high they seriously must be making like 10 chips a week.",Intel,2026-01-16 14:30:14,4
Intel,nzwvdau,NAND SHORTAGE WHO?,Intel,2026-01-16 13:00:20,3
Intel,nzva35v,"> Unlike Sparkle, Maxsun has two cards in its arsenal: the regular 24GB VRAM model with a dual-fan design, and the dual-GPU 48GB model with a blower-style fan  seems nice",Intel,2026-01-16 05:12:03,5
Intel,o04avmj,One card $799? Two cards $1598...,Intel,2026-01-17 15:32:16,1
Intel,nzz5o69,It's no different. Not even an SLI/Crossfire bridge.,Intel,2026-01-16 19:25:21,5
Intel,o135x8c,"To use this card you need your motherboard to support bifurcation. Without it only sees 1 GPU and 24GB VRAM  That wasn't the case with the likes of GTX690, R9 295 etc which the system saw them via SLI/Crossfire and could work on any mobo without supporting bifurcation.",Intel,2026-01-22 18:03:51,1
Intel,nzz8xk5,Those were also kinda useless as they only really handled sync as far as I understood,Intel,2026-01-16 19:40:31,2
Intel,nzzefbl,"What do you mean? They were a proper data bus, iirc.",Intel,2026-01-16 20:05:59,3
Intel,nzzoc4y,"i'm no expert but i dove in to Wiki      it was a [1GB/s to 3.25GB/s](https://en.wikipedia.org/wiki/Scalable_Link_Interface) interface. For refference PCI-E 2.0 was 1GB/s and 3.0 was 2GB/s BUT per lane so 16 and 32GB/s.  For refference HDMI 1.4 has 10.2Gbit/s  So as i understood it most of the data bandwidth were used for tossing the image output back and forth as only a single card were the one outputting the image (assuming you weren't running tripple monitor widescreen setup on SLI GPU's....) and our image data could more or less saturate the link  someone correct my math if i'm totally bonkers, but 2560x1440p = 3.686 million pixels. each using 24 bit (8-bit color on 3 chanels) which is 88Mbit/s. if you output 60 of those that is 5.3Gbit/s. 1GB/s = 8Gbit/s. Okay okay you wouldn't always throw a full image so lets call it 2.5Gbit. You were still using quite a lot of the bandwidth on the image alone, so from my understanding most of the interface communication was more related to ""who did what"".  and in AFR you would be handing over a full frame.",Intel,2026-01-16 20:52:43,1
Intel,o0g6w5o,"It was used purely to transfer rendered images, correct.",Intel,2026-01-19 09:27:32,1
Intel,o0g7wxp,"phew! my understanding was correct! And this is why the new nvlink is different. It's not just image data transfere. But also a LOT faster. NVlink 4.0 is 900BG/s. that is 300x faster than the FASTEST sli bridge recommended for 4K monitors. and that link is 15x faster than PCI-E 5.0 at a full 16 lanes. 15 times! the old sli was only the speed of a single or two pci-e lane. not 225 lanes equivalent speed  edit. One is numbered as unidirectional and the other bidirectional, so half the numbers above. But that is still VERY fast, even if it's a bit slower than the memory bus speed.",Intel,2026-01-19 09:37:24,1
Intel,o0g86uf,"It wasn't used differently on 20- and 30-series, and it's discontinued for the 40- and 50-series.",Intel,2026-01-19 09:40:01,1
Intel,o0g9x44,what do you mean by that?,Intel,2026-01-19 09:56:26,1
Intel,o0g9zvv,I'm saying NVLink did the same job as an SLI bridge for SLI setups. There's a reason it was discontinued,Intel,2026-01-19 09:57:10,1
Intel,nz79v91,"The fact that it can even compare to AMDs halo product, which the avg consumer canâ€™t afford is a win for Intel. Intel has plenty on leg room to expand the GPU too.",Intel,2026-01-12 18:02:37,11
Intel,nz5c8tz,This suffers from bandwidth bottleneck. Strix halo is Quad channel while panther lake is dual. An igpu would benifit greatly with a quad channel,Intel,2026-01-12 11:58:24,12
Intel,nzgoxuf,"This thing is absolutely nuts  AMD BTFO unironically, I'm floored. I never, ever would have considered an Intel chip before 2025, now this is the most obvious laptop part ever. AMD is surely sorely regretting recycling the same 780M and 890M chips for another entire gen, betting that Intel would continue stagnating.  This thing is gonna be a monster in handhelds.  I really, really wanted a Strix Halo laptop, but the lack of SKUs, price and the inflexibility with RAM kind of make it unappealing to say the least, not to mention the power draw compared to Panther Lake is unwelcome. These laptops are gonna be probably the best x86 in mobile has eaten in a very long time.  On top of that, it's almost making the 5050 look like a stupid part in a laptop. Why bother when you have a vastly more power efficient iGPU that will handle every desktop workload on top of being viable for gaming?",Intel,2026-01-14 01:47:11,2
Intel,nz4rv7t,"â€œTakes on strict haloâ€ at about half the performance (:  Title aside, this looks pretty great.",Intel,2026-01-12 08:54:08,3
Intel,nzi9w6l,"Amd hasnâ€™t even reached 30% market share mobile yet (oscillating between 20% and 26% since 2020) and are about to be almost wiped from existence again save some low end designs using Ryzen â€œAI 7â€ 445 (6 core, 2+4, 4CU iGP).",Intel,2026-01-14 08:34:10,1
Intel,nznrlo6,"The performance looks fantastic for high end $1k handhelds.    But the ""80% faster than AMD's 890M"" claim is absolute bullshit.  They tested against the HX370 with LPDDR5 5600.  That said, against an 890M that *hasn't* been crippled, it should still be 40-50% faster which is great.",Intel,2026-01-15 02:38:40,1
Intel,nz7d64j,"Will intel make it affordable for consumers though, or price it like LNL (2000+ USD laptops and up)",Intel,2026-01-12 18:17:35,8
Intel,nz7r7or,"""compare"", it is half the performance. Still good for what it is, assuming it is priced right",Intel,2026-01-12 19:20:44,2
Intel,nz5loww,"Well it's not just memory bandwidth. It's got about as much bandwidth as it needs to feed the Xe3 cores.   Panther Lake's GPU tile size is only 54mm^2 while Strix Halo's GPU is 308mm^2. For Panther Lake to compete with Strix Halo it would need 2-4Â times as many Xe3 cores probably. That'd be expensive. There's a reason Strix Halo is so expensive and kind of low volume, bigger CPU more RAM and more expensive motherboard aside.",Intel,2026-01-12 13:04:48,13
Intel,nzbn738,DDR6 can't come too soon for igpus too. But in reality memory bandwidth will stay an issue for a long time. Of course cramping enough compute power in such a format is an issue too,Intel,2026-01-13 09:02:19,1
Intel,nzwms44,Framework desktop motherboard?  https://frame.work/products/framework-desktop-mainboard-amd-ryzen-ai-max-300-series?v=FRAFMK0004,Intel,2026-01-16 12:02:44,1
Intel,nz5dj5h,"Half the performance, half the power, (more than) half the price.",Intel,2026-01-12 12:08:11,16
Intel,nzd3th0,I preordered X7 358H laptop for 1300,Intel,2026-01-13 15:11:48,3
Intel,nzgpiya,Bro nothing's going to be affordable in computer hardware at this rate,Intel,2026-01-14 01:50:29,2
Intel,nzi9j83,There are plenty of LNL laptops around 1000 what you on about,Intel,2026-01-14 08:30:40,1
Intel,nzoqfoa,Lunar lake is in sub 800$ laptops now and 12xe cpus are available for sale 1300$ despite the ram and cpu shortage.   The comparison is good even before likely price hikes for strix halo,Intel,2026-01-15 06:39:34,1
Intel,nz9efnf,"Weâ€™re talking mobile chipsets here, strix halo is what happens when you throw efficiency out the window, with Power (TDP) range, typically from 55W up to 120W. The ultra H 300 has default TDP of 25W, with Maximum Turbo Power (MTP) going up to 65W-80W. Intel has a better design, if they threw 40 XeSS3 cores on it, it would prolly run circles around Strix.",Intel,2026-01-13 00:08:25,3
Intel,nzhjs1h,"Strix Halo is double the die size, this should be compared to Strix Point.  But price will tell everything.",Intel,2026-01-14 04:53:30,1
Intel,nz5xoxm,"> It's got about as much bandwidth as it needs to feed the Xe3 cores.  GPU's will take all the bandwidth you can feed them. It won't help EVERY benchmark, but it will help many.  I'd rather see 256-bit bus on something like this. maybe 192 since you can do that with LPDDR5X etc.",Intel,2026-01-12 14:13:42,5
Intel,nz7ptl5,">Panther Lake's GPU tile size is only 54mm^(2)  is this confirmed for the bigger tile?  edit: also, Halo has all the IO, en-/decoders, etc. in the ""GPU"" tile, so the comparison isn't quite valid",Intel,2026-01-12 19:14:20,2
Intel,nz5j09r,And about four orders of magnitude more availability.,Intel,2026-01-12 12:47:16,12
Intel,nz8m3ma,"> (more than) half the price  Have we seen pricing? Not doubting it, I just haven't seen anything personally but probably missed it.  Strix Halo does seem to be a pretty mythical chip due to its price.",Intel,2026-01-12 21:44:42,1
Intel,nzanyas,Keep in mind that each Xe3 core is about as wide as an AMD WGP. We're looking at 1536 vs 2560 shaders. The B390 is 60% as wide as the 8060S. 20 Xe3 cores would match the 8060S in width. 40x Xe3 is as wide as the 7900GRE.,Intel,2026-01-13 04:15:53,4
Intel,nzgqkco,"Bingo  Strix is also limited by being RDNA 3.5 and no FSR4, so it's rather dependent on raw throughput, and it can't possibly fit in a comfortable handheld that would last for more than an hour and a half under load.  I really, really appreciate what AMD has done historically in the APU space, but it is genuinely time for vendors to start considering Intel. The strides here are absolutely immense. They went from an iGPU being a thing that can do basic graphics and 2D gaming to something that competes against lower end NVIDIA parts at less power draw and can actually legitimately game. It's bonkers. In mobile it's a no brainer.  Of course, it's going to be interesting seeing AMD's next UDNA architecture and what they can pull off, but competition never hurt nobody, and it was sad seeing AMD stagnate in the APU space of all things, their bread and butter that gave them pretty much the entire console market plus the Steamdeck. The entire Windows and Linux handheld market has been nothing but AMD for years. This is even better than Lunar Lake.  We're getting to the point where Intel could legitimately compete in the home console space and make a really great product, but realistically they can't undermine AMD's relationship with vendors at this stage. I hope they keep it up, it would really be cool to see an AMD vs Intel APU console war generation.",Intel,2026-01-14 01:56:20,2
Intel,nzixqmc,It's about 50% bigger die with 1 CCD (which seems comparable CPU performance),Intel,2026-01-14 12:07:34,1
Intel,nzbnge8,"This is a big of exaggeration, as you can see with Nvidia moving to gddr7. While bandwidth has increased substantially, performance is clearly limited by lack of compute power",Intel,2026-01-13 09:04:52,1
Intel,nz610q1,">GPU's will take all the bandwidth you can feed them.Â   Didn't deny that. But 12 Xe cores is presumably considered the sweet spot, that's all I'm saying. And Strix Halo only has twice as much bandwidth to feed a GPU die 6 times the size of Panther. I'm sure it has more cache, but still. I think Intel would consider triple or quad channel memory not worth the costs. It would require new i/o, new pins, new motherboard, more RAM, and all, for what's essentially the lowest volume product.  Besides, Intel already has Nova Lake AX in the backlog, or whatever it's going to be called. Practically intel's strix halo. It'll have Xe3P cores, more powerful than Xe3, thus deemed more worthy of the halo treatment.",Intel,2026-01-12 14:31:23,1
Intel,nza15gk,"No official confirmation yet, but JayKihn leaked the tile size for the 12Xe SKU last year. Another user somewhere else said the 4Xe GPU is 33mm2.Â Â  https://x.com/jaykihn0/status/1812898063502938260   And even without the PHYs and NPU, from what I see, Halo's GPU tile is still like almost 3 times as big. So yeah, it's on another class, that's my whole point.",Intel,2026-01-13 02:11:11,2
Intel,nz68s35,Yep,Intel,2026-01-12 15:11:22,2
Intel,nzao8ks,"I have a pre-order in for an MSI 14"" at B&H for $1300. 358H, 32GB LPDDR5X-9600, 2TB, 1200p OLED. I've seen some lower-end PTL laptops rumored around the $900-$1k starting range, but those are likely the 4Xe chips. Wildcat lake with its tiny 2Xe GPU is probably going directly into the budget sector.",Intel,2026-01-13 04:17:34,2
Intel,nzdgaj0,Good catch.,Intel,2026-01-13 16:10:17,1
Intel,nzhkchb,AMD is dormant on the APU space since it had basically the monopoly for x86 because Intel was just bad.  They are taking one of the old Intel's book by releasing rebrands and reashes,Intel,2026-01-14 04:57:28,2
Intel,nz657hj,"> But 12 Xe cores is presumably considered the sweet spot  By what? much larger Xe3 GPU's exist.  We have nothing to compare against in Intel-iGPU-land that has 256bit memory.  Strix Halo die size isn't the metric you want either. It's only 2x the fps (and who knows, panther lake could be 2x its own fps with doubled memory bus, but we'll never know, because Intel won't release a strix halo competitor)",Intel,2026-01-12 14:53:19,0
Intel,nze3jni,"Halo's die is still quite a bit bigger, but from the Intel side, you need to include IO, GPU and about half of the compute die which has the MCs, encoders / decoders, etc. to match the ""GPU"" die of Halo, so it is more like 200mmÂ² to 300mmÂ² when compared",Intel,2026-01-13 18:08:18,1
Intel,nzkoj06,AMD is paying more attention to NVIDIA for sure. ESP on the data center side.,Intel,2026-01-14 17:33:11,1
Intel,nz696vi,">much larger Xe3 GPU's exist.  The biggest one for the moment is on Panther Lake X CPUs. I wouldn't know if there's something bigger tbh.  >Strix Halo die size isn't the metric you want either.\\  Sure you can't compare two different architectures. But all I need to know is it's faaar bigger.  >panther lake could be 2x its own fps with doubled memory bus  That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.",Intel,2026-01-12 15:13:24,1
Intel,nzgcub2,"Well Panther uses mixed processes, and hybrid tiles are bound to be a bit less space efficient than putting everything on a single die. And to be fair, Halo GPU uses N4P process while Panther GPU uses N3E So, still not directly comparable.   Gotta say though, Arc's PPA has improved a lot since Alchemist and Battlemage.",Intel,2026-01-14 00:38:45,2
Intel,nz753k2,"> That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.  Yeah, it doesn't matter how much memory bandwidth you have if the GPU doesn't have the raster performance to keep up with the flow of data. Case and point, AMD's R9 Fury X. Released with 4096bit bus HBM. Had a total memory bandwidth of 512GB/s. Yet the GTX 980 Ti released with a 384bit bus and 336GB/s memory bandwidth and it out performed the Fury X in pretty much everything.   That said, I have no idea how close the iGPU is to being bandwidth bottlenecked at 1080p. But I very much doubt doubling it would also double the frame rate.",Intel,2026-01-12 17:41:14,3
Intel,nziy0qm,"Yeah, I think Intel has done a great job with the improvements, I just don't want to overhype things.",Intel,2026-01-14 12:09:36,1
Intel,nyet7we,"I wonder if Valve would be considering Panther Lake for a Steam Deck 2. It sounds like a generational leap from RDNA2 (which is what they were looking for), and can be cheap enough for valve to slightly subsidize the cost.   I guess the main thing is the power envelope. I think Valve is only interested in making handhelds that deal with a 15W TDP or lower",Intel,2026-01-08 15:18:52,29
Intel,nyf9pbk,"the handhelds with these chips are going to slap. Also people gotta remember that using upscaling on smaller screens especially 7-9 inch screens is a lot more tolerable.  and these benchmarks are of triple A titles, monster hunter wilds is a dogshit unoptimized game, and people will be playing a mixture of game from older titles, indies, emulation etc.",Intel,2026-01-08 16:32:58,9
Intel,nyepcic,"Series 3 seems like the biggest Intel W in a long while.   Plays AAA games at 45-60fps with upscaling, and all other games at native at 60+. Not to mention this is 50% faster than AMD's equivalent HX370 while being massively more power efficient.   There's also already laptops listed on sites like Best Buy for reasonable prices (sub $1300). Compare this to the AI 395 from AMD that can't be found for less than $2500 while being significantly less power efficient. Granted, that APU isn't really comparable.",Intel,2026-01-08 15:00:39,28
Intel,nyhyhe2,"So remind me, the ARC B390 is not a discrete GPU?  although the ARC B580, and B570 are discrete GPUs?  and the ARC A380 is a discrete GPU?",Intel,2026-01-08 23:45:30,1
Intel,nyejwbi,"That should say ""playable at 540p""",Intel,2026-01-08 14:33:53,-5
Intel,nyf0j6v,"Linux driver support is the only problem at the moment. Lunar lake is already competitive with Z2 extreme in gaming in windows, but not even close in Linux. Hopefully this changes by the time handhelds with panther lake come around",Intel,2026-01-08 15:52:10,21
Intel,nz26s22,There's an Xbox project within the coming years apparently.,Intel,2026-01-11 23:05:33,1
Intel,nygbkt8,I think Valve is looking for an ARM chip. Intel could dip their toes in this market before Qualcomm catches up to Apple.,Intel,2026-01-08 19:16:45,-6
Intel,nyg3cne,AMD should have made an igpu from rdna4...  but nope... the engineers are too busy making AI gpus.,Intel,2026-01-08 18:41:17,5
Intel,nyf1d4m,"Yeah, amd went to extremely greedy!",Intel,2026-01-08 15:55:52,7
Intel,nyg345w,AMD should have made a RDNA4 igpu... RDNA5 igpu is gonna be a huge boost.,Intel,2026-01-08 18:40:16,6
Intel,nyfhqqg,"In a year or two,when they can get panther lake for cheap, this could be resolved. They could even get more efficiency with a refined 18a for the CPU and whatever node is available for the GPU. With the current RAM prices I wouldnâ€™t expect any major console style updates until 2027 anyways",Intel,2026-01-08 17:07:42,6
Intel,nygdjk9,Do you think there's any particular reason why they would want to go with ARM? Quite sure Intel proved here that efficiency is essentially equal between both ARM and x86 here,Intel,2026-01-08 19:25:24,6
Intel,nyhii5z,It's hilarious to think Steam would work with Qualcomm.,Intel,2026-01-08 22:26:42,4
Intel,nyhzkyv,"Very unlikely. Proton and DXVK work alright with x86 but adding ARM conversion on top is, uh, a **very** poor experience. In fact that first generation of Snapdragon laptops had among the highest return rate of laptops I have ever seen. Amazon [literally warns potential customers](https://www.tomshardware.com/laptops/snapdragon-x-powered-surface-laptop-7-gets-frequently-returned-item-warning-on-amazon) about it.  Valve definitely wants a wider adoption rate of SteamOS and ditching x86 is not going to help that. They are **not** Nintendo that can ask devs to target their architecture. It's going well so far because for most games you can just make a standard Windows version, slap Proton and it works within 10% of native performance under optimal circumstances. Anything that can increase incompatibility rate (and it VERY well can, ARM **does not** support AVX2 natively for instance meaning [a lot of games that might have issues](https://www.reddit.com/r/macgaming/comments/1dekmtz/avx2_game_list/) even starting or underperform).",Intel,2026-01-08 23:51:14,3
Intel,nyjz9hz,i'm so tired of people throwing ARM around for everything. superficial much?,Intel,2026-01-09 06:55:17,3
Intel,nyi9wg9,Yep AMD ces presentation dry as a bone until that rack announcement when ceo turned giddy.,Intel,2026-01-09 00:44:04,2
Intel,nyfnu8h,"Intel and AMD especially have this habit honestly, they hit the lead and stagnate *bad* and the catch up for whatever company stagnated takes a hot minute, though hopefully this level of pushing boundaries in the handheld PC space keeps both of them moving at a steadier pace for a while rather than one team moving way the fuck forward",Intel,2026-01-08 17:34:32,3
Intel,nyincie,why didn't they do one? Why use tech that is so old?,Intel,2026-01-09 01:55:33,3
Intel,nygxlyp,"Even more efficiency that doesn't exist yet. Snapdragon elite can do some heavy workloads for several hours at a time, but it's not powerful enough for heavy gaming. Valve says they know what they want, it's just not ready yet. I think as great as Panther Lake benchmarks are, the biggest complaint of the Steam Deck is still the 2ish hours of battery life in more demanding titles.  With the existence of other handhelds already, I don't think Valve is trying to aim for the beefier spec department here, if it means the same battery life.",Intel,2026-01-08 20:54:40,-2
Intel,nyidygm,"I'm not sure if this is sarcastic or if you missed the memo, but their new VR headset coming later this year is powered by a Snapdragon 8 gen 3.  [https://store.steampowered.com/sale/steamframe](https://store.steampowered.com/sale/steamframe)  A Qualcomm based steam deck isn't out of the question.",Intel,2026-01-09 01:05:28,2
Intel,nynct2x,"Valve's stand-alone VR headset uses Snapdragon, and x86 emulation, so will be interesting to see how well it performs",Intel,2026-01-09 19:07:13,2
Intel,nylpddj,"It depends. For a PC, yes. For a handheld, I don't think so. For the future of enterprise notebooks, probably, especially since Apple has been doing it for a while.",Intel,2026-01-09 14:38:38,1
Intel,nykkj1q,Save cost. AMD's assessment is that no one in mobile cares about gaming and if they do they should just get Strix Halo or build a PC.   cheapskate AMD as always.,Intel,2026-01-09 10:06:00,3
Intel,nyndytz,"probably didn't want to bother redesigning the APU without also having a CPU upgrade, it's expensive after all, and takes resources from other projects",Intel,2026-01-09 19:12:31,2
Intel,nyoo6vt,"""Even more efficiency that doesn't exist yet.""??????????????",Intel,2026-01-09 22:48:02,1
Intel,nyk1lrj,"But Qualcomm drivers on Linux are even worse than Intel lol. The custom AMD APU on the Steam Deck had the advantage of great Linux driver support for gaming(not just being able to support the GPU hardware and benchmarks, but run games at good performance which Intel still can't match). No other company has both high performance GPU and good Linux graphics drivers",Intel,2026-01-09 07:14:57,3
Intel,nyo6aao,isn't the APU a modular unit where they could just put the new one into the same die space?,Intel,2026-01-09 21:23:15,2
Intel,nyp4s4e,"Not my words. I'm taking Valve's. They said they know what they want, and if it was Panther Lake, we'd know already.",Intel,2026-01-10 00:15:40,1
Intel,nyo9nvh,"The Strix Halo is that basically, but the normal Strix Point APUs (e.g. HX 370) are not. The Strix Halo follow-up, Medusa Halo, is slated for 2027, and to use Zen 6 and RDNA5. While the Strix Halo could benefit from FSR4 if it got an RDNA4 update, it's still way stronger than the B390, even at similar power.",Intel,2026-01-09 21:38:47,1
Intel,nytzm0l,Medusa Halo = 2028,Intel,2026-01-10 18:55:32,1
Intel,nxy9s8q,"Hopefully they deliver. Amd needs a shake up in the APU market, mostly the GPU side of it.",Intel,2026-01-06 04:25:51,32
Intel,nxy85c1,That article claims it on par with the 4050 laptop. Jesus christ,Intel,2026-01-06 04:15:26,36
Intel,nxygoos,this is nice but the handheld market could use less ultra 9 and 7s and more ultra 3s.  the closer they can get to the nintendo 2DS XL in size while being under $400 the better.,Intel,2026-01-06 05:12:20,32
Intel,nxywtjt,I am definitely looking to get a gaming handheld PC with PTL in it. Gonna cost a fortune probably but it's my first and intend to stick with it for a long time  The only thing that would stop me is if they skimp on ram... Which might be a very real problem,Intel,2026-01-06 07:22:44,3
Intel,nxz3n59,77% faster while using 80% more power.  I rather see power matched benchmarks.,Intel,2026-01-06 08:25:58,10
Intel,nxzqm0j,"happy about more handheld focus, genuinely have put in more hours on my steam deck than my pc setup this year. i have my eyes on ARM going forward as well",Intel,2026-01-06 11:52:57,2
Intel,ny14mg9,fun to see them tout XeSS MFG on mobile gpus while the B580 still doesnt have it....,Intel,2026-01-06 16:28:10,2
Intel,nxzumfv,Really excited to see these chips on handhelds.,Intel,2026-01-06 12:22:50,1
Intel,ny3a7m6,"Iâ€™ll always want a really good handheld besides my PC. Currently own a Legion Go S and the Switch 2 so this is good for everyone. AMD stays on their toes and if intel is good and gives us a SteamOS native device, Iâ€™ll definitely try them next upgrade. The",Intel,2026-01-06 22:21:45,1
Intel,ny52cw8,"I'm looking forward to this, especially for a gaming handheld/mini pc device, having a gpu that is nearly capable of a RTX 4050 with that power profile could be game changing.  Plus all the existing Intel XESS features are icing on the cake, although support for that scaler is flakey. I'm just hopeful that more games will support XESS.",Intel,2026-01-07 04:03:23,1
Intel,ny5mck0,"Can anyone say real life performance diff, and how much increase in battery life in real laptops, Because I feel many times those ppt numbers don't nearly match real usage (especially when ppt numbers are huge).   Can I get Mac like battery(or atleast 7hrs with no performance drop) and how much does it compare to Mac m1/a18 air performance with them on a $600 laptop.(Assuming fedora/mint as OS)",Intel,2026-01-07 06:22:50,1
Intel,ny68bvw,"I don't understand, we need to see the price of this thing, because otherwise we have to compare it to the AMD 8060s which will be more powerful, but even the cheapest machine with that costs â‚¬1500/â‚¬2000. We just need to see what price point this chip will be offered at.",Intel,2026-01-07 09:39:36,1
Intel,ny87s84,"On just 2 channel / 128 bit RAM, well done Intel!",Intel,2026-01-07 16:51:19,1
Intel,nyaqr0z,An ancient Chinese proverb (roughly) states: *'Talk...* does not make rice...' ðŸ¤”,Intel,2026-01-07 23:35:58,1
Intel,nybohxz,Steam deck 2??? Take my money gaben.,Intel,2026-01-08 02:30:03,1
Intel,nzlao1v,It does look like an amazing performer. I hope the Linux drivers are up to the taskâ€¦ Windows based handhelds have been pretty bad because of Windows.,Intel,2026-01-14 19:11:22,1
Intel,nzmn683,Waiting for this since AMD 890M was disappointing,Intel,2026-01-14 22:54:25,1
Intel,nxyuq77,"Who cares, gives cheaper powerful GPUs for 2k, 4k gaming",Intel,2026-01-06 07:04:00,-19
Intel,nxznnuh,"Yeah, RDNA 3.5 lasted way too long. Admittedly, RDNA4 was a special case where they gave up on a mobile version in favour of getting UDNA ready but that's their own fault. Hopefully, this pushes them to make UDNA a mobile focused architecture as well and perhaps push more cores in igpus to take back the integrated graphics crown. Competition is very good for the consumer.",Intel,2026-01-06 11:29:47,11
Intel,ny16bso,agreed. these rehashed mobile chips with bad upscaling are well beyond their lifespan.,Intel,2026-01-06 16:36:00,4
Intel,nxyagzo,"I looked at the benchmark scores they put out and it looks pretty promising, apparently the 12XE core variant can score somewhere around 6300 on Time Spy graphics (https://www.notebookcheck.net/Early-Intel-Panther-Lake-iGPU-benchmark-impresses-with-50-faster-performance-vs-Lunar-Lake.1138923.0.html).  Intel is comparing a 4050 with low wattage (60W system TDP IIRC) so it's not as good as the full powered 4050 which scores around 8000 on Time Spy. On low powered 4050s though like the one in the XPS and other thin and lights it will compare pretty evenly. It also outscores basically any 3050 on the market since the highest powered ones get around 4500-5000 on Time Spy (which was already matched pretty decently by the old 8XE core GPUs)",Intel,2026-01-06 04:30:18,23
Intel,nxzdq9g,"It will depend on game to game basis. Some perform well on iGPUs, some tank hard due to memory bandwidth or whatever issue they have with it.",Intel,2026-01-06 10:02:48,5
Intel,nylh0bq,"A quick Google says 9 TFLOPS or the equivalent to an RTX 1080, 2070, 3060, 4050 give or take.",Intel,2026-01-09 13:55:48,1
Intel,ny086nv,They did against the 285h and it's a similar margin. Lunar lake has a power burst max wattage below panther lakes max sustain power here so they can't compare 1:1 properly,Intel,2026-01-06 13:47:07,6
Intel,ny7zxvb,>77% faster while using 80% more power.  Are you following CES at all?  The top feature of that architecture so far has been power reduction,Intel,2026-01-07 16:16:03,2
Intel,ny0hpkm,82% faster than 890M with 30% more power draw with native resolutions,Intel,2026-01-06 14:38:55,3
Intel,ny0v0jb,rdna? dude their vega lasted too long they got very complacent in their igpu department,Intel,2026-01-06 15:44:10,5
Intel,ny2t2go,"At this point, we'll be lucky if they even care about consumer cards at all.  It's AI all the way these days.",Intel,2026-01-06 21:02:33,2
Intel,ny2qsag,"And then UDNA has been nowhere to be found, probably coming next year. AMD completely blew their lead in the APU space.",Intel,2026-01-06 20:52:10,1
Intel,nxyitdu,Panther Lake with an iGPU being able to play the newest games on medium/high settings in a thin notebook is pretty crazy,Intel,2026-01-06 05:27:44,15
Intel,ny6mhyg,Mfg on or off? The article wasnâ€™t clear on that.,Intel,2026-01-07 11:42:22,1
Intel,ny5jwr8,their Zen 5 desktop iGPU still use RDNA2; a 5 yr old architecture let that sink in...,Intel,2026-01-07 06:03:30,5
Intel,ny89039,"AMD didn't have money when they were using Vega iGPUs, and they were still the best iGPUs around",Intel,2026-01-07 16:56:45,2
Intel,ny2tvmr,"Unlike Nvidia they actually can make a lot of money relative to what they do right now if they get consumer marketshare. Iirc, Nvidias gaming revenue still beats AMDs enterprise earnings.",Intel,2026-01-06 21:06:18,2
Intel,ny2rjyw,"Eh, they will still have the best igpus on the market for a while. If they price the 388 well there is hope for them. But it's never going to sell the volumes Intel will.  UDNA is a major architecture overhaul on par with the the introduction of Ryzen and RDNA. A year is a long time but AMD only really needs a single gen to recover this gap if they so wish. But UDNA will need to be made with versatility and low power application in mind.",Intel,2026-01-06 20:55:40,0
Intel,ny55lgh,"Low to medium , not high",Intel,2026-01-07 04:23:33,5
Intel,ny8apbn,native rendering,Intel,2026-01-07 17:04:31,1
Intel,nz2dezh,Those weren't good though.  They didn't get close to the 1050ish equivalent that's a decent min spec card until the steam deck.,Intel,2026-01-11 23:40:06,1
Intel,nya3a7m,"High with XESS maybe at â€˜okayâ€™ frame rates. Still, crazy.",Intel,2026-01-07 21:45:51,1
Intel,nxxv1o4,And Intel steps back into the game. The next generation of handhelds is upon us.,Intel,2026-01-06 02:58:42,23
Intel,nxxytpz,"despite me using amd, please let intel succeed",Intel,2026-01-06 03:20:08,26
Intel,nxy0xsg,"7200 DDR5 support out of the box slaps. the IMC should be bonkers, like the RAM prices(I know I know.,..)",Intel,2026-01-06 03:32:13,6
Intel,nxxlswe,This is insane for a ultrabook without a dedicated dGPU. Intel cooked! AMD brought us the same thing refreshed.,Intel,2026-01-06 02:08:08,28
Intel,nxxbn9v,"intel cooked, amd socked. I cant wait to replace my old aging i7-9750h laptop",Intel,2026-01-06 01:13:24,15
Intel,nxxy441,can't wait to get my hand on this in handheld PC.,Intel,2026-01-06 03:16:02,5
Intel,nxy6trb,Wonder why they haven't announced Wildcat Lake yet,Intel,2026-01-06 04:07:06,3
Intel,nxxbuuu,Is this meant for the Strix Point or Strix Halo price segment ?,Intel,2026-01-06 01:14:32,5
Intel,nxxslar,"I'm curious to see if framework will have Panther Lake options, but I'm not sure if these are compatible with SODIMM or not.",Intel,2026-01-06 02:45:15,2
Intel,nxzl8ju,I want this in my nuc.,Intel,2026-01-06 11:09:30,2
Intel,nxz6yui,"So it's available at Jan 27th, but is that only for laptops or is it for desktops as well?",Intel,2026-01-06 08:57:51,1
Intel,ny3n1s5,ok but can we talk about how the ultra 5 332 is worse than the ultra 5 325? i mean wtf did they do to the names?,Intel,2026-01-06 23:25:18,1
Intel,nyqhrdf,I am neither intel or amd biased...hope competition stays alive..and reduce prices..which i doubt,Intel,2026-01-10 04:58:44,1
Intel,nxys920,"Iâ€™m confused, is cooked good or bad? ELI5",Intel,2026-01-06 06:42:48,5
Intel,nxxce1j,"Strix Point, but we'll see what happens with the RAM situation.",Intel,2026-01-06 01:17:25,8
Intel,nxxf85s,"Isnâ€™t much point buying strix point if you can get this instead. Better chips and a faster iGPU, and better battery. Issue is whether you want to buy any laptop this year given the price of componentsâ€¦",Intel,2026-01-06 01:32:44,13
Intel,nxz1cdn,The Arc B390 parts are LP-DDR5x only.,Intel,2026-01-06 08:04:24,1
Intel,ny3gnxb,"No Sodimm on the existing X series parts, LPCAMM2 for DDR5 seems dead still so I only expect to see good use of it with DDR6.",Intel,2026-01-06 22:52:59,1
Intel,nxzf4k0,Its a BGA mobile part - you might see some desktops using it (particularly in SFF NUC type designs or All-in-Ones) but its not a socketed desktop part.,Intel,2026-01-06 10:15:33,3
Intel,nxzmu72,Desktop version drops next year like usual with intel. First laptop then pc.,Intel,2026-01-06 11:23:01,1
Intel,nzinznt,Both have and had their ups and downs. If the competition is alive it's better for everyone.,Intel,2026-01-14 10:48:32,1
Intel,nxytdvk,"Usually when someone cooks its good, but if they are cooked its bad.  Intel/AMD cooked/ is cooking = good Intel/AMD is cooked/ they are cooked = bad  At least that's how i differentiate it.",Intel,2026-01-06 06:52:29,25
Intel,nxytih9,"Cooking is good.  Getting cooked is bad.  One has you as the victim, the other as the victor",Intel,2026-01-06 06:53:35,16
Intel,nyvb37j,SÃ¤g mig du Ã¤r pensionÃ¤r utan att sÃ¤ga att du Ã¤r pensionÃ¤r.,Intel,2026-01-10 22:51:08,1
Intel,nxxfmz4,"There could be if I can get it far cheaper.  This kind of performance should melt good part of premiums off Strix Halo, too.",Intel,2026-01-06 01:34:55,4
Intel,nxzmr6n,Its probably LPCAMM memory.,Intel,2026-01-06 11:22:20,1
Intel,ny3iqio,"Yeah I certainly don't expect a smaller company like FW to adopt a niche standard like LPCAMM2 for DDR5, probably DDR6 given I doubt SODIMM can support the speeds that DDR6 will have.",Intel,2026-01-06 23:03:18,1
Intel,nxzyhhz,"desktop version will be novo lake, later this year or early next year.  It will a significant upgrade.",Intel,2026-01-06 12:49:16,3
Intel,nxzn7ws,So in 2027? It takes them a whole year to release the desktop versions?,Intel,2026-01-06 11:26:10,2
Intel,nxylinl,Strix halo is irrelevant because that shit isnâ€™t and wonâ€™t be available in mainstream laptops regardless of its pricing. Even in amd marketing material just now itâ€™s the same asus tablet and hp Zbook nothing else.,Intel,2026-01-06 05:48:06,2
Intel,ny3g4rp,"Nah the love of Strix Halo is it's 256 bit bus and 128GB of memory, which this doesn't address at all. It's good groundwork though, you could imagine an update that grows the current design with two Xe4 12 core chiplets connected to the I/O die that then goes to DDR6 192 bit bus being good enough to compete with an RTX5060.",Intel,2026-01-06 22:50:22,1
Intel,nxzez6p,"I get downvoted, but it literally shows on the chart that the 388H, 368H, 358H and 338H are LP-DDR5x only: [INTEL-PANTHER-LAKE-1.jpg (2629Ã—1341)](https://cdn.videocardz.com/1/2026/01/INTEL-PANTHER-LAKE-1.jpg)  You won't see Intel supporting consumer designs which use DDR5 with these parts, and if you do make a product running it you'll lose the Arc branding - same as if you do an Arrow Lake-H design with single channel memory, or a Lunar Lake with a sub-17W PL1.",Intel,2026-01-06 10:14:13,1
Intel,nxznayq,LPCAMM2 is still LP-DDR5x.,Intel,2026-01-06 11:26:51,1
Intel,ny3jgvd,"I mean they tried for Strix Halo, but AMD did not validation or design work in mind to support it sadly so framework couldn't risk it. Intel tends to be better, but still.",Intel,2026-01-06 23:07:01,1
Intel,ny2jemc,"No, I have no idea what u/kazuviking is talking about.   Panther Lake won't have desktop chips. The mobile chips might get used in some mini PCs or whatever, but PTL is a mobile architecture.   What is coming late 2026 though, near the end of the year, is Nova Lake, a completely different arch than Panther Lake. And then we will likely see Nova Lake for laptops at CES next year.",Intel,2026-01-06 20:17:53,2
Intel,nxznspb,"It was always like this, release new gen on laptop then a year later on pc.",Intel,2026-01-06 11:30:53,1
Intel,ny33fhy,It is going to be in the Asus Tuff A14 this year.,Intel,2026-01-06 21:49:56,1
Intel,nxznqea,Yeah but completely different standard.,Intel,2026-01-06 11:30:22,1
Intel,ny3kun0,"That's a bit of a different case, because they are designing a new product either way they might as well look into supporting something like LPCAMM2. They already have a mainboard for the FW13 that can support these newer CPUs, so there has to be a very compelling reason to spend money to redesign that, which I think a single Intel SKU is not unfortunately.",Intel,2026-01-06 23:14:05,1
Intel,ny3gxyu,Do we know if we are getting real Nova Lake mobile chips or just a BGA version of desktop parts with a panther lake refresh?,Intel,2026-01-06 22:54:23,1
Intel,nxzp8ts,"The DRAM standard between LPCAMM2 and LP-DDR5x is the same - that's the point. LPCAMM2 is a replaceable implementation of LP-DDR5x memory.  But LPCAMM2 is completely different from SO-DIMM. SO-DIMM is DDR5. LPCAMM2 is LP-DDR5x.  You will see Arc B390 designs with LP-DDR5x memory down on the mainboard, and you'll see (some) LPCAMM2 designs, but they'll be far less common.  But you won't see DDR5 SO-DIMM designs - if they claim to be DDR5 SO-DIMM and Arc B390 then its wrong.",Intel,2026-01-06 11:42:26,1
Intel,ny4seb7,"It certainly sounds like we will get dedicated NVL-H, and that's what the rumors claim as well. Here's an excerpt from an Intel executive at an Intel BoA conference:   >Yeah, so maybe just baseline everybody on Panther Lake, so Panther Lake is a product thatâ€™s going to launch in the second half of this year, and it is all built on Intel 18A. Really, Panther Lake is an all mobile stack. When you get to the next generation Nova Lake it is both a mobile stack and a desktop stack",Intel,2026-01-07 03:05:35,1
Intel,ny4w3p0,"Looks like some folks are saying Nova Lake replaces panther lake stack exactly while also having a really big H chip at top, missed that cause I thought it was just the big 8+16+4 design",Intel,2026-01-07 03:26:25,1
Intel,nx9rf7h,How is the AI running on the B50?,Intel,2026-01-02 15:55:47,3
Intel,nxaevc4,Where did you purchase the B50?,Intel,2026-01-02 17:45:39,1
Intel,nxc8zfu,nice case,Intel,2026-01-02 23:05:56,1
Intel,nxlmpl5,"love the size of it. love the psu, are there any psus in this formfactor that are more powerful?",Intel,2026-01-04 09:46:01,1
Intel,nxsj613,100Â°C,Intel,2026-01-05 09:52:28,1
Intel,nx9s5yu,"cute fan lol  like other user, how is the B50 performance?",Intel,2026-01-02 15:59:17,1
Intel,nyktz5u,"surprisingly fast, it has its own suite on windows, but i want to use it in linux, trying to figure out how as im not that good ta linux.",Intel,2026-01-09 11:27:16,1
Intel,nxeazfq,"weird comparison. the mac mini is the real beast and its in part thanks to not having to cater to OEMs, but the 48gb mini pro is $1800 vs this $350 drop in card so that's a strange comparison.   m5 in the ipad has only about 150gb/s of bandwidth. good for light inference but I really doubt its practical for actual scale production.",Intel,2026-01-03 06:46:02,5
Intel,nxahmts,"As an ardent and lifelong Apple hater, I must admit that they will probably come out much stronger and on the very top of the current chaotic situation if they manage to keep the current price/perf ratio of their offerings. Even with the Apple tax, they are unmatched right now.",Intel,2026-01-02 17:58:24,5
Intel,nxlml7c,"apple stuff is hard to get used to for many pc nerds and mechanical engineers and engineers in this field. When pro software like catia/nx nativly will work nativly on arm then maybe the big car/air/motorcycle/""every day crap all around us"", then product developers will adopt arm/apple. but right now x86 is the king for these guys/this sector that design all stuff u see around u.",Intel,2026-01-04 09:44:53,2
Intel,nxb0egt,"its not hard, you can literally just buy them on newegg",Intel,2026-01-02 19:25:10,5
Intel,nxbf6k9,"[https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007](https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007)  They're finally back in stock as of this reply, though likely not for long.",Intel,2026-01-02 20:37:05,3
Intel,nxvcb13,"It's the Flex ATX form factor. I think the most powerful one that is also reputable is the Enhance ENP-7660L, which is 600w.",Intel,2026-01-05 19:23:31,1
Intel,nzf8l4a,Around 65c-75c under load,Intel,2026-01-13 21:15:43,1
Intel,nxs9peb,"a lot of commercial software also just does not give a shit about improving, and I don't mean that as a defense for apple.  after effects is just ass for a 2025 product. basic filters are still using legacy code and memory management is horrible. like you're not going to clean 64gb of memory until I hit 96gb utilization, then you're going to slow to a crawl and maybe crash because of threadlock? why even bring back MT rendering? 3rd party scripts people wrote in their basements outperform this stupid thing. spoofing multiple instances and then stitching the results works better than just running the software, its baffling.  anyway yeah, there's a lot of good to x86 and not having to reinvent the wheel, but god damn if so many companies are using it as an excuse to resell garbage.",Intel,2026-01-05 08:22:17,1
Intel,nxgupsq,"It's not exactly weird. They went PPC before Intel because powerpc was more effective for workloads most people used macs for. The switch to Intel was just because Intel had node leadership and performance leadership. Instead stick to A-chips for low power mobile where intel gave up on servicing. Intel loses node leadership to TSMC for a long time, Apple moves on to everything in-house is a pretty logical progression.  Apple having bespoke solutions isn't new either. they've been doing it since their G workstation days. Their current situation is pretty much on brand for apple, but the difference is the huge mistakes intel made (particularly firing so many top engineers) that led to staff fleeing to other companies, including leadership at Apple processor design.  basically apple did their own thing as usual and did a great job don't get me wrong, not taking anything away from apple. the biggest difference however was intel's CEO and board destroying the company.",Intel,2026-01-03 17:07:56,1
Intel,nyqxyvg,"> Memory - The Core Ultra X9 will feature soldered Dual Channel LPDDR5x 9600 MT/s memory up to 96GB   96GB of RAM? So it's $20,000?",Intel,2026-01-10 07:03:20,11
Intel,nyodemu,"Great, want one!",Intel,2026-01-09 21:55:54,2
Intel,nywvp1h,any plan for Wildcat Lake variant for very cost effictive mini PC solution?,Intel,2026-01-11 03:56:05,2
Intel,nzlnhoq,Hadn't noticed this on the first read: there are separate SKUs for HDMI vs DisplayPort. Will all of the models (X9/X7/U7/U5) be available with both options?,Intel,2026-01-14 20:09:33,2
Intel,nyovlyi,It will be very interesting to compare it with the Nuc 15 pro. I am currently reviewing this model with a U5 225H processor. And I know what this processor is capable of in combination with ARC 130T.,Intel,2026-01-09 23:26:34,1
Intel,nyr50we,any word on availability?,Intel,2026-01-10 08:06:45,1
Intel,nyxxa7v,I hope it costs under 1600.,Intel,2026-01-11 08:40:19,1
Intel,nz7zsaj,5x4 nopeâ€¦ stick with the 4x4 thatâ€™s been around for a decade at this point.  Any increase in size just dilutes the meaning of NUC,Intel,2026-01-12 20:00:12,1
Intel,nzknjzr,Man I wish they'd move on to 10Gb NIC already. I would be all over these for lab and canary. Using a thunderbolt dongle is miserable.,Intel,2026-01-14 17:28:45,1
Intel,o11azs2,"Am I seeing correctly ...your specs say Bluetooth 6, but your event photo shows Bluetooth 5.4 ðŸ˜¬",Intel,2026-01-22 12:34:06,1
Intel,nyryfbb,"Ok atleast this doesn't have co pilot button,plus one to that",Intel,2026-01-10 12:30:39,1
Intel,nzad5zk,"Nothing to share about that. I see some news about that platform, but nothing has been shared with me internally to say one way or the other. For these ASUS NUC models, they are all listed from the Intel Core Ultra 5 and higher.",Intel,2026-01-13 03:15:32,3
Intel,nzlkg2h,"I suspect this would wind up being ""NUC 16 Essential"" to replace the Twin Lake-equipped NUC 14 Essential.",Intel,2026-01-14 19:55:40,2
Intel,nyudelu,"It's mentioned in there, but late Q1 - early Q2 is our current target.",Intel,2026-01-10 20:02:42,4
Intel,nzjltxw,"5x4 yes, high TDP no",Intel,2026-01-14 14:32:17,1
Intel,o13l2qw,"Ah, I see the issue. I took these snaps from one of our product videos at the show. The display was featuring all of our products, and the one that lists BT5.4 was for the ExpertCenter PN55 (you can see the copilot button on it to compare). The other information, including the spec one-pager below lists BT6.",Intel,2026-01-22 19:10:41,1
Intel,nyv67be,"If I were to buy one, are these like just hardware or do they include an OS with all the drivers installed out of the box?",Intel,2026-01-10 22:26:12,1
Intel,nyx7821,"Thanks, is that for global availability or just US? Also any word on what the lowest spec SKU will start at?",Intel,2026-01-11 05:05:43,1
Intel,o15ze6h,Talk about a poorly timed photo ðŸ§,Intel,2026-01-23 02:30:04,1
Intel,nyvi99v,"They will be available in both types of models. I don't have a full breakdown on which hardware will be included with the complete Mini PC (e.g. with memory, storage and OS), or the Barebone kit (no memory, storage, or OS), but you'll be able to purchase it in either configuration.",Intel,2026-01-10 23:28:53,5
Intel,nzacphp,"I would expect US availability to be around global availability, but that different barebone kits sometimes are configured a bit later.   To your second question, pricing information isn't available at this time, but if you're just asking the specs, I would follow what I posted above.   However, if you go to our global product page, you'll usually find a download link for our spec datasheet (sometimes this doesn't show on mobile + plus the document isn't available yet). This will be a better way to see this. I'll ask our team when the datasheet will be available.",Intel,2026-01-13 03:13:08,2
Intel,o16s6n3,"We do what we can. Thanks for noticing, however.",Intel,2026-01-23 05:25:17,1
Intel,nv0zs3r,"If only Intel had stayed in the memory business!   They'd be enjoying Micron valuations and wild profits and performance from copackaged CPU+GPU+LPDDR of their own design and manufacture...     But no, they'd rather invest billions in buying donuts as a service, or whatever their crazy investements went into.",Intel,2025-12-20 13:16:05,46
Intel,nv0mnlo,"damn an iGPU using 32GB of vRAM, I wonder if they're testing a Panther Lake laptop with 48GB RAM or even more (since X7 & X9 Panther Lake only accepts soldered memory)",Intel,2025-12-20 11:24:26,12
Intel,nv2mj8t,"If Intel is really about to release a B770, honestly the **only thing that could make it competitive is the price**. (FOR ME, competitive in 2026 means <400â‚¬) From a performance standpoint, it would need to undercut existing GPUs quite aggressively to make sense, especially given how crowded the mid-range already is.  That said, Iâ€™m pretty skeptical about how realistic that is. **With the recent RAM shortages and rising memory costs**, pricing a new card competitively while still keeping margins doesnâ€™t sound easy at all. Memory is a huge part of the BOM, and weâ€™ve already seen how shortages can push prices up across the board.  So unless Intel is willing to take a serious hit on margins (which seems unlikely), Iâ€™m not convinced the B770 will land at a price point that truly shakes up the market. Happy to be proven wrong, but for now the pricing question is the big unknown for me.",Intel,2025-12-20 18:47:27,6
Intel,nv30mtq,So there's a 20GB variant. A 28GB variant and a 32 GB variant?,Intel,2025-12-20 20:01:42,2
Intel,nv40zo0,Optane was practically **built** for the type of AI workloads that they're shoveling money at.  If Intel didn't give up literally only a matter of months before GPT released and the bubble began in earnest lol,Intel,2025-12-20 23:28:07,18
Intel,nv29blj,"If Intel stayed in memory business, it would be long dead in the 80s and killed by Japanese memory companies. CPU remains the top niche area with less competition and deeper moat. See how China has quickly come up with their GPU designs? Well it will take at least another decade for them to make 2nm CPUs",Intel,2025-12-20 17:39:17,15
Intel,nv2qfnf,"are people high or something? intel was losing money on optane and their SSD business became irrelevant the minute regular memory manufacturers slammed the market. don't get me wrong, they were some of the most durable on the market, but they were no where near printing money on the memory business.  optane may have survived if their nodes were on schedule, keeping CXL support on schedule, but not because it was profitable.",Intel,2025-12-20 19:07:24,9
Intel,nw63y3k,CXL killed octane itâ€™s that simple. No one wanted to be locked to just Intel. CXL was and is just better,Intel,2025-12-27 10:05:29,1
Intel,nvsy1nn,"I feel like the price has to be more than competitive. If they can undercut competition cards of the same performance by 100 or so (or maybe offer rebates or freebies) they could potentially steal the market in that category. With Nvidia and amd cards being tried and true for many many years, I feel like their marketing needs to grab the attention of consumers in a somewhat drastic way.",Intel,2025-12-25 00:56:50,4
Intel,nvbtvc9,If the b770 is 5060ti levels even â‚¬500 is competitive,Intel,2025-12-22 06:05:31,1
Intel,nw63tp3,Yeah sure it wouldâ€™ve been perfect but CXL killed octane and offers pretty much everything it did while not being loved to just Intel lol,Intel,2025-12-27 10:04:18,1
Intel,nv4153e,"it's not like any of this AI garbage right now is profitable for anyone except nvidia and the hardware companies anyway, it's not stopping everyone from shoveling money into it",Intel,2025-12-20 23:29:02,4
Intel,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
Intel,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
Intel,ny2u31x,"With the crazy RAM prices, I'm looking to move to a 13600K or 14600K to keep using the 64GB of DDR4 from my ancient 7700K build. Do we users generally consider Vmin Shift Instability to be fixed at this point through the series of BIOS and microcode updates?  Related: Should I expect something in the range of a 10% performance drop from any of the reputable reviews, due to the fixes? Also, are efficiency-cores pretty much working as intended at this point, or is thread scheduling still a concern on them where your high performance thread ends up on an e-core?  Thanks all!  Note: This question is not for Intel\_Support. The answer from your side would obviously be ""Yes!"". :)",Intel,2026-01-06 21:07:15,1
Intel,ny85o2z,Is Tiber cloud gone forever?  https://console.cloud.intel.com/ just gives a DNS error now.,Intel,2026-01-07 16:41:57,1
Intel,o04fx6u,"I have installed new Intel Wi-Fi 6 AX210.NGWG.NV in my ASUS laptop, bcz the old one died and couldnt connect to bluetooth since, WIFI works perfectly fine tho, so i dont know if problem is with drivers or not. Also i would just instal them from Intel, but i live in russia and i dont know any trustworthy sites, so if anybody knows, i would be really gratefull",Intel,2026-01-17 15:56:24,1
Intel,o07e6d2,"How do I know the legitimacy of an Intel wifi card, model AX210? I've been searching for it in Amazon and most are manufactured in Vietnam and China, with varying prices.",Intel,2026-01-18 00:38:33,1
Intel,o0uehap,"I keep seeing mentions of TPM in our system requirements and I'm honestly a bit lost on what it actually does for our security, so who is the best person in the org to chat with to get the full rundown?",Intel,2026-01-21 12:32:18,1
Intel,nxwkozf,"u/Chelostyles Thank you for your inquiry regarding the CPU and RAM upgrade for your Dell Vostro 3670. As much as I'd like to provide my technical insights on this upgrade path, I'm not in a position to provide specific suggestions since this involves hardware modifications to an OEM system.  For the best compatibility outcome and to ensure optimal system performance, I strongly recommend reaching out to your system manufacturer directly. They can provide definitive guidance on supported CPU upgrades (i5-8400 to i7-9700) and maximum RAM configurations for your specific model. We don't want to inadvertently bypass any warranty terms and conditions on your system by providing modification recommendations that might affect your coverage.  Your system manufacturer's technical support team will have access to the exact specifications, BIOS compatibility matrices, and supported hardware configurations for your Vostro 3670 model. They can confirm whether the motherboard supports the i7-9700, the maximum RAM capacity (64GB vs 128GB), and any potential limitations or requirements for these upgrades.  This approach ensures you get accurate, manufacturer-validated information while maintaining your system's warranty protection.",Intel,2026-01-05 22:52:24,1
Intel,nxwjdkt,"u/I_like_carsyay  XPS 15 9530 hardware does support Resizable BAR, which is why Intel's system detection shows it as available for both your Arc A370M and Iris Xe graphics. However, the system manufacturer has designed their BIOS interface to prioritize stability and user-friendliness, often managing advanced PCIe features like ReBAR automatically in the background rather than exposing manual configuration options. This approach ensures optimal system performance while reducing complexity for users. I recommend checking for the latest BIOS updates from your OEM's support site and contacting their technical support team, as they would have the most current information about how ReBAR is implemented on your specific model and whether any additional configuration steps are needed to fully utilize this feature.     I've posted an article below in case you haven't yet come across it:  **Helpful Resources:**  *  [What Is Resizable BAR and How Do I Enable It?](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html)",Intel,2026-01-05 22:45:46,1
Intel,ny3upu3,"u/QunatumLeader Hi, thanks for your interest!Â  You can find and apply for all of our jobs online atÂ [http://](http://jobs.intel.com/)[j](http://jobs.intel.com/)[obs.intel.com](http://jobs.intel.com/). We donâ€™t currently accept submissions via social.Â  Good luck!",Intel,2026-01-07 00:05:20,2
Intel,o0l3yzt,"Late to this, but I'm a 13900K owner. I have not had any issues with stability since applying the BIOS update and haven't noticed any performance loss, so I think this is fine. I did not thoroughly benchmark before and after though, partially because of how high peak temperatures were before the update. I am using a Noctua NH-D15 and a contact frame to reduce CPU temperatures.  Up until a few days ago I would have said that thread scheduling isn't an issue, but then I played the game Maneater and it's basically unplayable unless you use launch options to force the game to only P-cores. There's the Intel ""Application Optimizer (APO)"" utility but it seems abandoned and you can't add your own games if Intel hasn't added a profile. I was a big proponent of E-cores but honestly it seems like a half-baked technology that Intel never put the effort in to support properly. That said I guess I could just entirely disable them if I cared so much, but that's a non-trivial amount of performance to just give up.",Intel,2026-01-20 01:26:55,1
Intel,nya3rq0,Hi u/ConspiracyPhD **Post**Â a question onÂ [IntelÂ® Tiber Developer Cloud Community](https://community.intel.com/t5/Intel-Developer-Cloud/bd-p/developer-cloud)Â forum for further investigation.,Intel,2026-01-07 21:48:00,1
Intel,o0e1nqe,"u/Far-Common2207 In this case, we suggest buying the wireless module from authorized Distributors to mitigate the legit concerns. Other than that, the OEM module warranty is not covered by Intel. For more details, you need to work with the Distributor or place of purchase for support to further verify if the wireless card is legitimate.  Check this article: [Where to find the Serial Number for IntelÂ® Wireless Cards](https://www.intel.com/content/www/us/en/support/articles/000092302/wireless.html)",Intel,2026-01-19 00:35:58,1
Intel,o0ztjvw,"[**Plenty-Solution-3692**](https://www.reddit.com/user/Plenty-Solution-3692/)**, TPM (Trusted Platform Module)** is builtâ€‘in security hardware that helps protect important data on your PC using encryption**. Intel PTT** is Intelâ€™s TPM that lives in the system firmware instead of being a separate chip, but it works the same way. Most PCs from the last few years already have TPM 2.0, sometimes it just needs to be turned on in the system settings. . If youâ€™re not sure how to do that, your motherboard or PC manufacturer should be able to help.  You can check this article for more information: [What Is Trusted Platform Model (TPM) and Its Relation to IntelÂ® Platform Trust Technology (IntelÂ® Pâ€¦](https://www.intel.com/content/www/us/en/support/articles/000094205/processors/intel-core-processors.html)",Intel,2026-01-22 05:04:22,1
Intel,nyarzrn,Forum doesn't exist or access denied.  I guess Tiber is just gone now.,Intel,2026-01-07 23:42:22,1
Intel,o0fgizr,Do you know any authorized distributors here in the Philippines?,Intel,2026-01-19 05:37:43,1
Intel,o0zyc8d,"I see, all good thanks for your support!",Intel,2026-01-22 05:39:13,1
Intel,nz1jsfl,u/ConspiracyPhD I just checked the forum and it looks like itâ€™s up and running. Could you try accessing it again using your Intel account?  [IntelÂ® Tiber Developer Cloud - Intel Community](https://community.intel.com/t5/Intel-Tiber-Developer-Cloud/bd-p/developer-cloud)  [](javascript:void(0);),Intel,2026-01-11 21:15:16,1
Intel,o0jlcxj,"u/Far-Common2207 According to the directory, these are the distributors in the Philippines. [Distributor Partners](https://www.intel.com/content/www/us/en/partner/showcase/partner-directory/distributor.html#sort=relevancy&f:@sfdisticountry_en=[Philippine,Philippines,Phillippines])",Intel,2026-01-19 20:45:26,1
Intel,nz301xe,"Nope.  https://imgur.com/a/tYRhYoV  Access denied and a nice ""This content is no longer available.""  Guess it's a completely dead project and should be removed from Intel's website.  http://console.cloud.intel.com/ is not accessible.",Intel,2026-01-12 01:35:48,1
Intel,nz3b0gd,"u/ConspiracyPhD Please check your inbox, Iâ€™ve sent you a personal message. Iâ€™ve already coordinated your concern with the respective team, and as per their instructions, youâ€™ll need to email them directly.  [](javascript:void(0);)",Intel,2026-01-12 02:33:45,1
Intel,ntkfg69,"> With up to 192GB of VRAM across eight GPUs in a single system, Battlematrix positions itself as a relatively cost-effective alternative to other professional GPU ecosystems for AI inference workloads.",Intel,2025-12-12 01:18:37,21
Intel,ntmjjev,Hope they do some image and video generation benchmarking as well. Nice to see someone testing AI rigs out there.,Intel,2025-12-12 10:55:28,5
Intel,ntlssa2,Wish theyâ€™d give prompt processing speeds. AI coding generates very few tokens compared to input. Nvidia seem to dominate here.,Intel,2025-12-12 06:40:14,3
Intel,ntxw9ob,"How many concurrent users will this serve, 30 devs would be nice",Intel,2025-12-14 06:36:27,1
Intel,ntsaqxn,:),Intel,2025-12-13 08:27:35,2
Intel,nvplqob,"These 12Xe3 cores are pretty neat, and because it fits in a normal socket it isn't ludicrously expensive to make.  I suspect we'll see a ton of these different form factors for this chipset.",Intel,2025-12-24 13:00:31,6
Intel,nvpsi8z,Mac Pro Trashcan 2.0 is crazy,Intel,2025-12-24 13:45:33,6
Intel,nvt0j8h,"> These 12Xe3 cores are pretty neat  have there been any leaked benchmarks or gaming FPS?  on paper they look good, but... some synthetic benchmarks suck",Intel,2025-12-25 01:15:41,3
Intel,nvte7q0,No one knows. Synthetics seem to put it roughly at a 3050m.,Intel,2025-12-25 02:59:28,2
Intel,nwfd6hr,"3050 to 3050ti mobile if leaks are to be believed. Could get a bit better than that if software is still not mature, so I'm calling a max of 3060M performance.",Intel,2025-12-28 20:51:38,1
Intel,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,56
Intel,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
Intel,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
Intel,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
Intel,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,10
Intel,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,9
Intel,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,3
Intel,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,6
Intel,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,2
Intel,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
Intel,nspzeik,If itâ€™s just â€œ16% faster than 890mâ€ itâ€™s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,7
Intel,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
Intel,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
Intel,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
Intel,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
Intel,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,7
Intel,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.Â      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
Intel,nsv64t7,"I mean no offense, but Passmark is irrelevant.Â  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.Â  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,5
Intel,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
Intel,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
Intel,nsyv727,"I guess we'll see more when we get actual info about the potential devices.Â  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
Intel,nv5fgk8,"should have a 3y warranty on it. submit an RMA ticket  regarding the actual query though, the silicon is the same the 14900ks is just a slightly better bin. you wouldn't notice the difference at stock let alone normalized for energy consumption",Intel,2025-12-21 04:48:32,8
Intel,nv5nfm2,"I have i9 14900ks, what I did is that I reset bios settings to optimized defaults and then I limit pl1 and pl2 to 150w and enabled XMP, these are the only two settings i changed, the rest is default, and temperatures are in check, i still get the same performance, and itâ€™s very efficient in gaming that way, the extra heat and power consumption of 253 or 320 are not worth it, I recommend just get the ks and make these two changes and forget it.",Intel,2025-12-21 05:49:09,4
Intel,nv5h74y,The performance difference will be tiny and definitely not noticeable with a 3090. Go for the cheaper chip.,Intel,2025-12-21 05:00:55,3
Intel,nv5hr1e,"Get the KS for better silicon quality only limit power , set pl1/2 253w and set it to 350 or 325A, definitely want the better 14900ks silicon quality itâ€™s overall better and better IMC as well. Itâ€™s a better bin and typically only the best 14900k will run stable 6.2ghz and at lower voltages even if you limit your chip to 6ghz",Intel,2025-12-21 05:04:58,2
Intel,nv70zt0,"A 14900KS is nothing more then a binned 14900K. Running a 320W/400A extreme setting is not advisable with a AIO. I run my 14900KS on custom loop with 320W/307A performance setting and it does not thermal throttle at all. If you get lucky, you could get a 14900K that can run KS settings. Performance in that case is( should be) identical. Without benchmarks i can't really tell the difference between 125/253/307 and 320/320/400 except the heating of my room.",Intel,2025-12-21 13:28:34,2
Intel,nvehvk5,"As an update - I went ahead with the 14900ks and also changed my cooler to a 420mm AIO.   Ensured latest bios update then set Intel presets (performance) but also went ahead and reduced PL to 150w, set temp limits to 70c, system agent voltage to 1.12, 307A, and I was blown away by the temps!! I am getting basically identical performance (+ few fps) to my previous 13900ks, but a whole whopping 30Â°c cooler in game!!! I would average 80-85, now itâ€™s sitting super chill with same in-game settings on BF6 & ARC at 50-60c.   Thank you everyone for your inputs, I sincerely appreciate it and Iâ€™m extremely happy with the outcome!",Intel,2025-12-22 17:37:19,2
Intel,nv6um6s,I also PL1/2 at 150. My temps stay under 60c when gaming.,Intel,2025-12-21 12:41:07,1
Intel,nv7icoh,"if the cooling wasn't sufficient disable HT(useless for gaming) and undervolt it this lower CPU temperature by 20c, in games the CPU temperature should be around 65c.",Intel,2025-12-21 15:16:34,1
Intel,nvb3hok,Im using a duel air tower for my 14900k game temps are at 60 to 70,Intel,2025-12-22 02:56:21,1
Intel,nvkr6z0,"14900KS is just a better binned 14900K. All things equal, you should have lower temps/voltage/power draw for the same exact workload/clocks on a 14900ks vs a 14900k. How big of a delta between the two comes down to how well you struck the silicon lottery with the KS.",Intel,2025-12-23 17:28:35,1
Intel,nv62yz6,"I have the K version only because of the onboard gpu. In case my GPU gives issues and I'll still be able to boot. But otherwise there is almost no performance gain. I ran my i9-14900k pl on 320 watt and did a cinebench benchmark, temps were ok: average 94c, max 98c with a 360 aio.  That said, go for the cheaper version if you don't need onboard gpu.  Edit: I have my pl on 253w now. No need to go any higher.",Intel,2025-12-21 08:13:54,-3
Intel,nv6s3tx,5 years warranty on 13 and 14gens now.    I have the 13900ks. Run it at 253w. Clock locked at 5.5ghz. Temp 80c and cinebench 23 39k,Intel,2025-12-21 12:20:08,5
Intel,nvx118g,Why not keep pl2 at 253 and 1 at 150/185 ? Did you try undervolting? Most of them can take 50mv offset with 75 /85 needing a bit more stability testing. Can also cap the vr limit and iccmax. I feel like going 150 pl2 makes you miss some performance in games unless you had thermal issues and doing it to keep it from thermal throttle.,Intel,2025-12-25 20:10:57,2
Intel,nv6848y,"Thank you for the feedback. Forgive me for the dumb question; if I ran either a 14900ks or a 14900k at these settings, would they both have the same temps? Or would the KS still run hotter?",Intel,2025-12-21 09:06:11,0
Intel,nwbwttq,"Dropped the voltage further down by -0.10000 and now Iâ€™m getting 58Â°c core temp and max 65Â°c package temp under load. Really happy with this, and with some tweaks to my in-game video settings Iâ€™m able to still maintain a framerate that matches my screen refresh rate.",Intel,2025-12-28 07:43:31,1
Intel,nv7zbxm,This post is about the K and KS. Both have the same iGPU,Intel,2025-12-21 16:46:01,3
Intel,nv7imu3,even better. i take it they extended tbe warranty period for those products?,Intel,2025-12-21 15:18:10,1
Intel,nvgy84t,For 5.5 39k in CB23 is a little low,Intel,2025-12-23 01:34:28,0
Intel,nvx2yr5,"I have tried and tested all my games, i saw absolutely no difference between 253w, 150w, 125w or even 100w, the fps were exactly the same, the only difference was in temperatures, performance wise i saw no difference between any of them, i was using 100w before but then I switched to 150w because I thought it was too low, even though the performance is still the same as 100w, just higher temperatures, my cooler is pretty good kraken elite 360, it never goes above 80 even on 253w but I just like to keep temperatures between 50-70 while gaming.",Intel,2025-12-25 20:22:49,1
Intel,nyq5yj3,"Yeah your method is better, itâ€™s what I do.",Intel,2026-01-10 03:42:43,1
Intel,nv84jg4,"Oh shit, I thought only the K had an igpu! Should have gone for the ks version lmao",Intel,2025-12-21 17:12:59,1
Intel,nv7jm5d,Yes because of the degrading issue.,Intel,2025-12-21 15:23:39,2
Intel,nvhfrt3,Lol stock is 5.8ghz lol and most stock after the update get 35k,Intel,2025-12-23 03:22:04,1
Intel,nvhi35n,All core cinebench is not 5.8... I get 39k stock what are you talking about lol,Intel,2025-12-23 03:36:52,1
Intel,nxyvuyn,"That is a little on the lower end, my 12900K gets 30K. Also in single core a little below 285K, multicore just below 9900X, top of 13700K. Back to it :)",Intel,2026-01-06 07:14:02,1
Intel,nvhitar,Search on reddit on 13900-14900k.  After the code update stock most 13-14k can barely do 35k. Dont like to your ego brother.   So millions on reddit are getting those score and you are the special bin whose getting a higher score.   Mr 1 post and 7 comment history lollllllllllllllllllll,Intel,2025-12-23 03:41:34,2
Intel,nvhk1pd,LOLOLOLOLOLOL HAHAHAAHAHAHAH ARE YOU DUMB? This really shows you don't have a 13900k or 14900k,Intel,2025-12-23 03:49:33,2
Intel,nvkpd9m,"My guy what are you talking about? 5.5 ghz for 39k is a good score on 13900k. My 14900KS completely stock does 41.5k and downclocks to about 5.5-5.6 ghz with hyperthreading on. If he's got HT off, his score is even better.   You have to be rage baiting.",Intel,2025-12-23 17:19:30,2
Intel,nvkq3xk,Talk to him not me... The guy said 35k is the score ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚,Intel,2025-12-23 17:23:13,2
Intel,nsktadm,Will be a interresting CES,Intel,2025-12-06 11:12:47,22
Intel,nso11hn,I'm half-expecting this to show up as a server-only AI-focused SKU with video outputs removed.,Intel,2025-12-06 22:37:18,7
Intel,nswpbo1,Merry Christmas everyone,Intel,2025-12-08 08:52:05,2
Intel,nswyceh,"4070 performance for $400, I'm calling it. Would have been great if this had come out right after the wave of negative press that the 5070 received for only being 10-15% better than the 4070 with a mediocre 12 GB of VRAM, but I feel like Intel missed the boat again if the Steam Hardware survey is anything to go by, the 5070 has really made a comeback with recent sales.",Intel,2025-12-08 10:25:38,2
Intel,nsofcmj,They can't even ship B60's.,Intel,2025-12-07 00:01:12,4
Intel,nsp4pld,"What is taking Intel so long?      It's already been almost a year after Battlemage's initial launch. And for what? RTX 5060 performance at the same price with some extra VRAM?  I had really hoped Intel would be able to gain ground on their competitors. At this rate, we'll get the ARC C770 to compete with the RTX 6060 in another 3 years.",Intel,2025-12-07 02:38:20,1
Intel,nsm5mtm,Aren't they always,Intel,2025-12-06 16:34:23,3
Intel,nt8d8jl,give it some time...,Intel,2025-12-10 03:42:11,1
Intel,nt8d10w,Merry Bitmas,Intel,2025-12-10 03:40:49,1
Intel,nt9tjuy,"4070 performance for... used 4070 price, now with driver issues and an objectively worse upscaler!  intel greatest hits",Intel,2025-12-10 11:20:47,0
Intel,nsol1s5,Sure they can if you search for it   B60  https://www.idealo.at/preisvergleich/OffersOfProduct/207972918_-arc-pro-b60-sparkle.html   Or b50 https://geizhals.at/intel-arc-pro-b50-a3584363.html,Intel,2025-12-07 00:34:27,7
Intel,nsq0noc,"Intel's GPU division has been operating at a loss never mind Intel as a whole and ARC series cards aren't as popular as the enthusiast circles would have you believe. Coupled with how expensive R&D is for things like GPUs, it's hard for them to pump out a competitive product while remaining just profitable enough to undercut AMD and Nvidia.",Intel,2025-12-07 06:21:43,6
Intel,nswuidx,What is taking Nividia so long with the super cards?,Intel,2025-12-08 09:46:34,2
Intel,nt0mfun,"Battlemage gpu chips are made through TSMC and Intel is getting screwed on supply, this is why even if the B770 comes it will only be a small amount. Hopefully Intel can put together enough rare earth to pump out discrete Celestial Gpus but it takes time to ramp everything up. In addition Intel has their chiplet design, EMIB that could take off soon. They may be able to bring Apple back into the fold, but let us hope Discrete Arc lives on.  I have learned to not have expectations for anything that is outside my direct control, I do the best I can to just go with the flow. Whatever will be, will be.",Intel,2025-12-08 22:47:54,1
Intel,nsn1l8j,There's definitely been lame ones.,Intel,2025-12-06 19:20:53,3
Intel,ntco711,"I'm fully aware it's not a great deal, but that's my expectation when it comes to Arc.",Intel,2025-12-10 20:41:40,0
Intel,nt0k5mq,"These are European links and will be out of stock. I found a mom and pop place back in my old stomping grounds in San Francisco, and they normally only sell B60s in prebuilt systems but a special order is possible.   Intel can't rely on TSMC for Battlemage supply, so let us pray that Discrete Celestial GPUs are made (entirely) at IFS and release in 2026 / 2027.   May your Bits Byte Hard, long live the Arc.",Intel,2025-12-08 22:35:24,2
Intel,nt0ni97,"If CES 2026 comes and goes without any details for Discrete Arc GPUs then it could be awhile. The main thing being promoted is Panther Lake which should be made entirely at IFS, a step in the right direction. The TSMC monopoly is destroying the industry and it hurts companies here in the US.",Intel,2025-12-08 22:53:46,1
Intel,ntafqck,Not sure how the link being European matters. They are European shops and the B60 is in stock.,Intel,2025-12-10 13:56:29,1
Intel,nqdrca0,"We kind of know where it will land. It will be a 3050M level chip, maybe a bit better, but will have improve scaling and frame gen.   8060S is 40 RDNA 3.5 units. One Xe3 unit is about 2.1 RDNA 3.5 units. That put it at about 65% of Strix Halo, though it will have a worse memory bus and no MALL cache. Somewhere in that range.   So that 60% is almost bang on the 3050M. Maybe a bit better. It wonâ€™t be like the 4050 but 3050M isnâ€™t bad for an iGPU that fits in a normal socket",Intel,2025-11-23 16:52:41,12
Intel,nqe5eoy,"> and no MALL cache  PTL does have 8MB of memory side cache, fwiw.",Intel,2025-11-23 18:04:37,13
Intel,nr20ozq,"I donâ€™t know how this score compares to the 3050M. I only know that this score is about 55% of the B580. And the B580, at 2K and 4K, is about 1.7â€“2 times the performance of the desktop 3050",Intel,2025-11-27 14:11:52,1
Intel,nu0mh30,and also an iGPU won't be stuck with 6GB of vRAM ðŸ˜…,Intel,2025-12-14 18:17:52,1
Intel,nqga537,Really don't understand why they don't go with a larger cache.  Pretty sure they still have a bunch of cache chunks spread all around the SoC.,Intel,2025-11-24 00:43:14,5
Intel,nqg407n,"in configs without die-to-die memory performance in general should be worse if bandwidth limited. despite not being specifically dedicated to onboard memory like Lunar, people are still planning configs with local LPDDR5x, though peak bandwidth is limited by a 128bit bus.",Intel,2025-11-24 00:08:08,4
Intel,nqfqmb2,I see it personally as not really being at 20W if youâ€™re asking much from the GPU. Itâ€™ll increase the juice dynamically if it gets demanding enough. So itâ€™ll be hard to say unless you force the power limits way down manually.,Intel,2025-11-23 22:49:52,2
AMD,o18jxte,">ASUS has confirmed that it has started an â€œinternal reviewâ€ following recent reports of Ryzen 7 9800X3D CPU failures on their 800-series AMD AM5 motherboards.  >In their statement, ASUS has not unveiled why there is a spike in reports of 9800X3D CPU failures on ASUS motherboards. However, ASUS claims it is working closely with AMD to validate recent reports and ensure stability and quality. It is also working to ensure it delivers â€œtimely solutionsâ€ to these reported issues.  >For now, ASUS recommends that 800-series motherboard users update their boards to the latest BIOS revisions. ASUS has also asked affected users to contact ASUSâ€™ customer service for direct assistance. ASUS also linked to an FAQ post to help users update their motherboardâ€™s BIOS.  >Official ASUS statement on recent ASUS AMD 800-series motherboard and AMD Ryzen 9800X3D concerns  >*We are aware of recent reports concerning AMD Ryzenâ„¢ 7 9800X3D CPUs and ASUS AMD 800-series motherboards, and we have initiated an immediate internal review. Our teams are conducting preventive checks on product compatibility and performance, working closely with AMD to validate reported cases and ensure ongoing stability and quality. We are looking to provide timely solutions to ensure our products and services meet expected standards.*  >*Users are also advised to update their ASUS AMD 800-series motherboard to the latest BIOS via ASUS EZ Flash or BIOS Flashback to help ensure system stability; we provide an official technical support FAQ with detailed instructions.*  >*Customers who are affected or have concerns are encouraged to contact ASUS customer service for direct assistance. We take this matter seriously and value our customersâ€™ trust, and we remain committed to transparency and to ensuring our products can be used with confidence.*  >â€“ ASUS",hardware,2026-01-23 13:48:22,40
AMD,o19q6te,"My 9800X3D died after 1 year on ASUS 870e-e Strix. I posted it here a few weeks ago. Was on the September bios version. Went to sleep mode and never woke up. AMD shipped out a new one, but I already bought a 9950X3D to replace it.  No burn marks, temps never got above 70-75c on water cooling. My only guess is something to do with BIOS, tiny undervolt, and/or sleep mode.  To be safe, donâ€™t use sleep mode, just shut down. Do not use PBO or undervolt, performance improvement is tiny. Update to the latest 1804 Bios.",hardware,2026-01-23 17:08:54,33
AMD,o18lwna,Hope that they don't take as long as Asrock and we actually get some info soon.,hardware,2026-01-23 13:58:40,69
AMD,o1atzic,"My 9800x3D recently got fried on an ASRock. Got a replacement, bought a new motherboard... from ASUS. I want to scream.",hardware,2026-01-23 20:11:33,15
AMD,o18v6xe,Wasn't the guy who killed his 9800X3D on the latest bios??,hardware,2026-01-23 14:46:16,13
AMD,o1eggb5,Weird there doesnâ€™t seem to be problems with the non X3D chips.,hardware,2026-01-24 09:47:41,4
AMD,o192x7t,"ASUS subsequently whacked the internal review with a pipe, said it was damaged in shipping, and engaged the auto email system demanding out of warranty payment for the internal review within 72 hours or it will be returned to the customer.",hardware,2026-01-23 15:23:56,16
AMD,o19win2,Is this like the internal review theyâ€™re doing with their laptop stuttering/UEFI issues where there havenâ€™t been any meaningful updates or bug fixes since? Cool.,hardware,2026-01-23 17:38:38,5
AMD,o1aqp06,are any of these failures running stock without enabling pbo or other oc options?,hardware,2026-01-23 19:56:00,3
AMD,o19jxjp,"Let's see if tech media applies the same level of scrutiny now that AMD has failing cpus. Also, the new 9850x3d could be a new stepping to mitigate whatever this is.",hardware,2026-01-23 16:40:37,24
AMD,o1ahsvv,"It isnâ€™t just 9800x3d  I had 2 asus b850 e rog boards that once you updated bios it over volted the 9700x and killed it.  2 boards, 2 9700x, swapped to msi x870e edge to and zero problems.",hardware,2026-01-23 19:14:18,6
AMD,o1cqw4w,Impossible. I've been assured it's only an Asrock issue.,hardware,2026-01-24 02:07:19,4
AMD,o194nze,"ASRock, GigaByte and ASUS having problems with these chips. I think it's time for AMD to issue a recall.",hardware,2026-01-23 15:31:59,9
AMD,o1a3sam,So all my efforts in buying a Gigabyte Motherboard specifically to avoid this problem were for naught?,hardware,2026-01-23 18:11:23,1
AMD,o1ba6on,I am sooooo fucking glad I ditched Asus after 20+ years and went with my first MSI mobo for my 9800X3D build a few months ago,hardware,2026-01-23 21:27:58,0
AMD,o18p675,Why am I not surprised it's aSus again,hardware,2026-01-23 14:15:37,-38
AMD,o1b2292,Instability I understand but can undervolting destroy a chip?,hardware,2026-01-23 20:49:45,18
AMD,o1azy78,"Could be something with the chips too. My 7800X3D died recently, after 2 years in an MSI board. Was already acting wonky for a few months with a few failed boots and all sorts of issues waking up from sleep.",hardware,2026-01-23 20:39:46,7
AMD,o1blsxt,"> To be safe, donâ€™t use sleep mode, just shut down.  This is an extremely cucked way to operate a computer. Semi-custom server parts or engineering samples would be one thing, but on a production chip the power saving features are expected to *work*.",hardware,2026-01-23 22:22:54,15
AMD,o1as0x8,"I had 5700X3D and it died after 9 months of usage, on Gigabyte motherboard.",hardware,2026-01-23 20:02:12,6
AMD,o1b6wzh,I have seen other suggestions that sleep mode is involved.,hardware,2026-01-23 21:12:43,2
AMD,o1czfsv,"This also raises a slight possibility that the failure mode can be blamed on some windows 11 fuckup, and we all know such a thing is entirely plausible.",hardware,2026-01-24 02:56:53,2
AMD,o1adlzx,"What do you mean, donâ€™t undervolt? Arenâ€™t the best guesses for why the 9800X3Ds dying that the VSoC was too high (1.3V or more)?",hardware,2026-01-23 18:55:02,3
AMD,o1bjtcx,"Iâ€™m on the Pro Art motherboard and on a older bios, wondering if I should just leave it as it is",hardware,2026-01-23 22:13:05,1
AMD,o1d0kub,"I have a 9950X3D on the X870e-e R2, have you heard if these issues are present on the R2 version of the board at all? I can't seem to find any reason why they even made a new revision.",hardware,2026-01-24 03:03:42,1
AMD,o18queh,"ASRock has been pretty open about reaching out to AMD, it's just they haven't been receiving any word back. The fact ASUS is involved now, is finally pushing AMD to (hopefully) do something.",hardware,2026-01-23 14:24:09,82
AMD,o1cl21p,"ASRock is taking long because they don't know and can't find the cause. Gamers' Nexus and Wendell from Level 1 Techs are both looking at it as well, more emphasis on Wendell. They can't find anything solid. They can't even reproduce the issue with motherboards that have had CPUs die in them already.",hardware,2026-01-24 01:32:59,5
AMD,o18z8wb,Indeed. I am nervous about my Tuf Gaming B650 Plus Wifi potentially frying my new 9800x3d...,hardware,2026-01-23 15:06:20,6
AMD,o1bxp2i,"It's Karma, stop kicking puppies!",hardware,2026-01-23 23:24:17,5
AMD,o1ccgye,"I updated the BIOS on my MSI board due to memory instability issues, but now thereâ€™s another problem with cold boot stutters. I wonder if AORUS/Gigabyte is doing better.  At least ASUS is acknowledging the issue unlike MSI.",hardware,2026-01-24 00:43:53,3
AMD,o19dith,Friend of mine just updated his BIOS on a Gigabyte x870 which didn't post afterwards. Thought his was another victim of this issue but luckily he was able to do a BIOS recovery and then was able to flash the newest BIOS afterwards. Super strange series of events there and he is a computer guy so I'm sure he didn't grab the wrong file or something dumb.,hardware,2026-01-23 16:11:55,8
AMD,o1kdf9h,IDK about ASUS but in the case of the Asrock failures they have PBO pie-charted on their thread about it: [https://www.reddit.com/r/ASRock/comments/1oqzli4/9000series\_cpu\_failuresdeaths\_megathread\_3/](https://www.reddit.com/r/ASRock/comments/1oqzli4/9000series_cpu_failuresdeaths_megathread_3/)  Last chart on the OP.,hardware,2026-01-25 05:04:19,1
AMD,o1arrve,"It all depends on numbers.  The Intel issues were big enough that they were noticed by multiple independent third parties, and pretty quickly after release.  And the target reliability can never be 100% perfection, 0.1% of a large number sold can still be a large number of cases.  This is why I'd love people to actually publish numbers, so we can really get an idea of the *real* chances of things going wrong, and not just a collection of anecdotes that may be amplified or ignored depending on the media cycle at the time.",hardware,2026-01-23 20:01:00,12
AMD,o1fagps,"They didnt when it was 1800, they didnt when it was 5800, they didnt when it was 7800, but now that its 9800 they surely will!",hardware,2026-01-24 13:46:05,2
AMD,o19mofw,Same level of scrutiny as what?,hardware,2026-01-23 16:52:53,2
AMD,o19miwi,>I think it's time for AMD to issue a recall.  How many are faulty vs how many have been sold?,hardware,2026-01-23 16:52:12,7
AMD,o19ak57,Source on the gigabyte? My search didnt yield anything substantial. Only asus and asrock are dying in bulk. In which asrock bios 3.25 or newer is recommended. Mindfactory rma rate for 9800x3d is close to 0.5% which isnt close to a full recall and nominal.  But my take is that just SoC overvoltage madness that also killed intel chips (which was substantially worse from crash report data)  Edit: found one x870 gigabyte board dead. And in the comments 4 asus motherboards who was RMA'd and bios updated.,hardware,2026-01-23 15:58:38,10
AMD,o1ke9co,"They still seem to have a lower rate of reports, which is better than nothing.",hardware,2026-01-25 05:09:48,1
AMD,o1cdsce,"MSI isnâ€™t immune either. My X870 MSI board started having issues with cold boot stutters after a BIOS update, and a lot of people are experiencing the same problem while MSI is ignoring it.",hardware,2026-01-24 00:51:08,5
AMD,o18sglb,Itâ€™s AMD lol,hardware,2026-01-23 14:32:26,37
AMD,o1blmdv,"It's possible.  If you run a power-limit-saturating workload with an undervolt, the chip is drawing more current than it does at stock. [Current density drives electromigration](https://en.wikipedia.org/wiki/Electromigration#Electromigration_reliability_of_a_wire_(Black's_equation\))",hardware,2026-01-23 22:22:01,17
AMD,o1bj9nw,"Yeh highly highly unlikely, less voltage and lower temps really ain't gonna hurt a CPU.  So if you use PBO and a negative voltage curve offset like most people who know what they are doing will, I was be shocked if it caused ant issues outside of stability of you push it too far.  I run -40 negative curve offset on my 9800x3d and it sits at 55c in games non stop and 5.4ghz all day",hardware,2026-01-23 22:10:26,7
AMD,o1c2uq3,"Even with undervolting, you will still get voltage spikes depending on the workload. And there could be something in the CPU that was very sensitive to them.  Also, when undervolting via negative voltage curve on AMD CPUs, they will hit their max rated clock rates more frequently due to having more thermal/power headroom from the undervolting. Then using PBO raises the max allowed clock rate, which at that point changes the CPU's electrical circuit behaviors from beyond the stock setting.  Now if the CPUs are still dying at stock settings, then something went terribly wrong for AMD.",hardware,2026-01-23 23:51:59,3
AMD,o1iff29,This post here says no undervolt and chip burned [https://www.reddit.com/r/hardware/comments/1qkr8ve/asus\_issues\_internal\_review\_after\_amd\_ryzen\_7/o1f7x9z/?context=1](https://www.reddit.com/r/hardware/comments/1qkr8ve/asus_issues_internal_review_after_amd_ryzen_7/o1f7x9z/?context=1),hardware,2026-01-24 22:36:17,1
AMD,o1cb7nr,"Many motherboards have had issues with sleep and wake modes for as long as I can remember going back to the early 2010s and they still do today.  I remember dealing with sleep mode issues on my Sandy Bridge PC back in 2011 and all the way up to current gen itâ€™s still been wonky for some reason. My current MSI motherboard has issues with randomly stuttering on cold boot, and MSI doesnâ€™t even acknowledge or say anything about the issue.",hardware,2026-01-24 00:37:09,9
AMD,o1dkoma,"Wait by ""Sleep Mode"" is that just putting the PC to sleep/suspend thru the OS?",hardware,2026-01-24 05:14:04,3
AMD,o1f9poi,Sleep mode was always fucked due to how much software is allowed to issue random wakes for no good reason. I found it to be a hundred times simpler to just go into idle monitors off mode and leave it spinning.,hardware,2026-01-24 13:41:39,3
AMD,o1how1j,"Sleep is great on my 12600. It used to not wake up rarely in the past, but doesn't do that any more.  But I've also disabled fast shutdown and stopped Windows from updating. So I have a much more stable system.",hardware,2026-01-24 20:29:49,1
AMD,o1bgcqx,"I had one die too.  I got lucky though, AMD replaced it with a 5800X3D.  Hope that doesnâ€™t die now.",hardware,2026-01-23 21:56:31,6
AMD,o1aeryd,Nope. It doesn't matter if you undervolt or put a higher voltage.,hardware,2026-01-23 19:00:19,7
AMD,o1ajgnw,I had VSoC set to 1.2. Just hard set it to 1.2 then disable PBO and dont use Curve Optimizer/Shaper - it's really not worth it.,hardware,2026-01-23 19:22:08,3
AMD,o19c38b,fr it's like everyone hda to team up to get AMD's attention. let's hope for some quick updates now,hardware,2026-01-23 16:05:30,20
AMD,o1bogod,Isn't ASUS a major shareholder in AsRock and share many components?,hardware,2026-01-23 22:36:15,1
AMD,o19hd4z,"> Our teams are conducting preventive checks on product compatibility and performance, working closely with AMD to validate reported cases and ensure ongoing stability and quality.  ...means ""they haven't been receiving any word back."" to you? /r/hardware really grasps at any straw to trash AMD.",hardware,2026-01-23 16:29:08,-24
AMD,o19aho0,"It's got a decent warranty, no?",hardware,2026-01-23 15:58:19,8
AMD,o1axttf,Are the 600 series board affected? I got the ASUS X670E last year; pretty happy with it.,hardware,2026-01-23 20:29:44,3
AMD,o1axfyw,"Intel 13th/14th gen issues hit volume customers, and that's why it gained traction - it had very little to do with the average consumer PC builder's experience, other than it mimicked the kinds of experiences larger datacenter operators and OEMs experienced in their own operations.  Unless the AMD issues rise to that level of impact for any extended period of time, it will likely stay quiet.",hardware,2026-01-23 20:27:53,15
AMD,o19y1ke,Likely intel's microcode killing 13th and 14th gen. It's popular in here to drag 'techtubers' because they say mean things about the mega corps.,hardware,2026-01-23 17:45:35,7
AMD,o19ln2m,"Itâ€™s always been the PBO defaults IMHO. Also, the x3d boost crap as well.  The cache makes everything more sensitive to issues obviously. Happy with my b650 choice over the 850 and the offset/undervolt.",hardware,2026-01-23 16:48:15,2
AMD,o1af4cn,"Literally thousands 9800x3d CPUs died on Gigabyte boards, same as AsRock, Asus or MSI.",hardware,2026-01-23 19:01:53,-11
AMD,o1cy7j7,Guess we just die then,hardware,2026-01-24 02:49:38,2
AMD,o18ymiq,"I hope it isn't AMD because I would be really fucking pissed if I'm sitting on a $3,000 time bomb with an arbitrary fuse.",hardware,2026-01-23 15:03:16,9
AMD,o193pvl,and it was intel not too long ago also.... its almost like they are pre overclocking all the CPU with a boosts and increase voltages to hit peak clock speeds and win benchmarks in the consumer space.  We dont hear about this from either vendor in the server space as they are much more conservative in that regard as stability is #1 priority.  Sadly I think its going to become the new norm.,hardware,2026-01-23 15:27:38,10
AMD,o1btrcl,"The undervolting on amd is pretty insignificant, -30millivolt is less than 3%, i highly doubt that would produce significant increases in electromigration",hardware,2026-01-23 23:03:24,10
AMD,o1bthz0,"Ah, thx.",hardware,2026-01-23 23:02:01,1
AMD,o1gd48p,"Thanks, makes more sense now.",hardware,2026-01-24 16:59:01,1
AMD,o1chu6u,"Oh, I do not doubt that poor testing and underinvestment in firmware are common, but that doesn't excuse it. A $250 motherboard shouldn't have stupid firmware bugs that cost the user an extra $25/year in electricity (in cheap places...) or alternately several minutes a day recreating context.",hardware,2026-01-24 01:14:11,4
AMD,o1ifqtz,I never have problem with sleep since 2010s on desktop.,hardware,2026-01-24 22:37:54,1
AMD,o1dxsq4,"I assume that is what we are all talking about. The same part of the UI that used to expose ACPI S3 suspend when that was the only way to do it, but now exposes S0iX (except when it doesn't).",hardware,2026-01-24 06:58:40,3
AMD,o1ig2c4,The only software waking up the computer is Windows trying to do updates at 3AM.  Disable schedule updates and everything is fine.,hardware,2026-01-24 22:39:28,1
AMD,o1k2naw,Sleep mode (S4) and modern standby (S0) are obviously far more useful for laptops.,hardware,2026-01-25 03:56:48,1
AMD,o1c9yrh,lmao if someone in this thread scores a free 9850X3D if it goes belly up on them.,hardware,2026-01-24 00:30:26,2
AMD,o1f9vhy,of course it doesnt when the sock will randomly run the CPU at 1.55V whatever your settings are.,hardware,2026-01-24 13:42:36,1
AMD,o1fibjn,this is what i did. i set it to 1.22 then disabled PBO.,hardware,2026-01-24 14:31:12,1
AMD,o19ggb5,"Exactly. The issue stems beyond the motherboard manufactures, whether the issue lies within a flaw of the chip or in the AGESA coding for the BIOS.   Edit: Downvote all you want, the issue is on AMDs side when the BIOS updates clearly aren't doing anything to stop the chips from dying.",hardware,2026-01-23 16:25:02,28
AMD,o1d6fij,Isn't this stuff the reason they're releasing the 9850x3d?  With the 7800x3d we didn't need a year plus of bios updates or a 7850x3d to fix it since it was genuinely board vendors over-juicing their boards.,hardware,2026-01-24 03:39:22,1
AMD,o1fn3ph,Everyone? ASRock(Asus) teamed up with Asus to do something? Are we being fr?,hardware,2026-01-24 14:57:05,1
AMD,o1bros5,"Not since 2007/2008. ASRock was spun off from ASUS around that time period, and became their own company. Both operate under their parent (Pegatron) but are independent from one another.",hardware,2026-01-23 22:52:34,9
AMD,o19kegt,What does ASUS PR statement have to do with ASRock?,hardware,2026-01-23 16:42:44,20
AMD,o19mxrc,"If you go back and read my first comment, I was talking about ASRock being upfront about reaching out to AMD and not hearing anything back. All you did was quote an *ASUS* comment from the article, which doesn't pertain to ASRock.   So let me get this straight....ASRock, ASUS, MSI, and Gigabyte (few posts but nothing too substantial) are all experiencing issues with the 9000 series CPUs dying right? AMD was blaming motherboard manufactures over the Summer, and each one has released several BIOS revisions that...guess what? Aren't stopping the chips from dying.   Are you going to really sit here, and throw out the excuse that ""Ohhhh r/hardware is just trying to find any reason to blame AMD and throw trash at them!!!"". The fact of the matter is, the motherboard manufacturers don't know what the issue is. So clearly the issue definitely falls on AMD, because nobody knows what the hell is happening.",hardware,2026-01-23 16:54:04,14
AMD,o19oi98,"Reading comprehension is an essential skill for becoming a functional adult and joining the workforce.Â You will struggle significantly in your professional life if this is how you respond on impulse.   I highly encourage you to read books, then think about what you read, write a report for yourself, and then read more books.",hardware,2026-01-23 17:01:11,10
AMD,o1cab4d,https://youtu.be/jiEv6VTDt5c?feature=shared  Start here buddy.,hardware,2026-01-24 00:32:16,1
AMD,o19cfvm,"I'd prefer not to actually test how the Asus e-store where i bought my motherboard and Amazon where i bought my CPU, would coordinate liability in case both pieces of hardware get fried.",hardware,2026-01-23 16:07:05,13
AMD,o1f7x9z,"I am using ASUS TUF Gaming X670E-Plus WiFi, my 9800x3d died in October after being installed in February of last year. First CPU I have ever had die in 25y building personal computers...  Oddly, the CPU had some dark marks on pin contacts like it was starting to scorch, i took pictures,  I cleaned it up with alcohol to see if it'd work but that didnt fix it. Temps were ~65C under load with AIO cooler. Corsair 1000w shift PSU. Stock PSU cables.   https://imgur.com/a/vMDaEl8",hardware,2026-01-24 13:30:47,1
AMD,o1c3piy,"Also it manifested in game servers (where single-threaded performance was needed).  The real issue was that Intel issue would slowly manifest. Going from occasional software bugs/crashes (often initially reported as GPU driver crashes, which Nvidia quickly blamed Intel) and then increasingly becoming unstable. I read about one game developer who got a flood of crash reports from one user, and when they told the user that their CPU was dying, the user demanded a refund and posted a negative review accusing the game of being poorly programmed.",hardware,2026-01-23 23:56:35,8
AMD,o1adxuk,"What are the PBO offsets, if you know? Iâ€™m on MSI which donâ€™t seem to be having as many problems (and a 7800X3D), but Iâ€™m trying to be extra cautious lol.",hardware,2026-01-23 18:56:31,3
AMD,o1bgsbo,"Thousands? Got any reporting for that? Because that sounds like an incredibly large number.  But also, the kind of number someone would just throw out there.",hardware,2026-01-23 21:58:34,10
AMD,o1c7yur,Do you have a source for the thousands?  I've seen a bunch myself but wasn't aware of that many.,hardware,2026-01-24 00:19:39,5
AMD,o1faqpq,If its failing on all motherboards whats the common factor?,hardware,2026-01-24 13:47:42,3
AMD,o19l14d,Only way to run CPUâ€™s safely is the tried and tested method of static Vcore with a droopy LLC. Any other method clearly isnâ€™t safe,hardware,2026-01-23 16:45:32,-5
AMD,o1f0i7m,They have to buy ram though. 50/50,hardware,2026-01-24 12:40:58,1
AMD,o1cl779,"They're still a major shareholder, like they said.",hardware,2026-01-24 01:33:50,1
AMD,o1bxzss,I meant like yes they were spun off but still are quite related. Hell sometimes their boards have very similar defects.,hardware,2026-01-23 23:25:51,1
AMD,o1bg3mq,Haha. That's why I used overclockers for all my purchases LOL.  3-year warranty!,hardware,2026-01-23 21:55:20,3
AMD,o19d1d6,Obviously but it's not like you're stuck with a dead cpu if it's a problematic unit,hardware,2026-01-23 16:09:45,4
AMD,o1fiqph,did you undervolt or overclocked your cpu?,hardware,2026-01-24 14:33:31,1
AMD,o1ie8pc,Now I'm a little worried. I have the X670E-PRO and 9800X3D ... for about 10 months. I have a little bit of undervolt -10mv for a bit of performance boost.,hardware,2026-01-24 22:30:29,1
AMD,o1afe2d,"MSI have the same issues like any other board maker, it's just not as popular in US and A so you have to find informations about it's issues outside reddit or twitter.",hardware,2026-01-23 19:03:07,1
AMD,o1gpuw8,I really hope you're wrong.,hardware,2026-01-24 17:55:29,2
AMD,o1afm75,"Nope. It literally does not matter, your CPU can die on stock settings.",hardware,2026-01-23 19:04:10,2
AMD,o1cme6o,"That hasn't been true since 2009, when ASUS sold off its last shares of Pegatron. They are not a shareholder.",hardware,2026-01-24 01:40:56,6
AMD,o1c3w6o,"Their relation ends at the parent company, otherwise, that's pretty much it. Which defects are you referring to, outside of the current ongoing problem?  Edit: ASUS sold off its stake in Pegatron (finalized in 2012), which means they are not a major shareholder.  [Asianometry](https://www.asianometry.com/p/asus-explained-a-rare-taiwanese-global) did a mini doc/article back in 2021 going over ASUS(tek)/Pegatron separation/spin-off.   [Techpowerup](https://www.techpowerup.com/163250/asus-to-release-its-stake-in-pegatron) wrote a brief article confirming the finalization of ASUS releasing its stake of Pegatron in 2012.",hardware,2026-01-23 23:57:35,0
AMD,o1cmtmv,"No longer true, ASUS sold their stake in Pegatron in 2009. They don't have a stake in ownership of ASRock.",hardware,2026-01-24 01:43:30,0
AMD,o1fl4bx,Kept the bios default except changing RAM from 5600 to 6000. Crucial Pro 96GB (2x48) kit.   The only undervolt I did was to the GPU when i had an AMD 9070xt. Swapped this out after a month when i was able to source a 5070ti. I dont think that would affect the CPU at all.,hardware,2026-01-24 14:46:37,1
AMD,o1iwxzv,"RMA process wasnt bad with AMD, I'd only worry if you bought it used or dont have a valid warranty for some other reason. They ask for proof of purchase and a photo of the product. I gave them a PDF from a downloaded invoice from Microcenter, and a photo of the CPU on its retail box. Turnaround time was 2 weeks.",hardware,2026-01-25 00:06:42,2
AMD,o1fausm,stock settings are strongly overvolted nowadays.,hardware,2026-01-24 13:48:22,1
AMD,o1agi32,Yeah?? Iâ€™m saying static Vcore with a droopy LLC is the only safe way to run CPUâ€™s,hardware,2026-01-23 19:08:16,-3
AMD,o1cba0u,"Thatâ€™s like saying â€œthey are brother and sisterâ€¦ their relation ends at their parents, thatâ€™s pretty much itâ€.  You donâ€™t get much more related as 2 companies than having the same parent company.",hardware,2026-01-24 00:37:30,4
AMD,o1e8sfh,Pegatron is *not* Asus' patent company. Your initial comment is incorrect.,hardware,2026-01-24 08:36:59,1
AMD,o1io6f0,bios default has PBO turned on and soc on auto right? i set a static 1.2v on my soc and turned of pbo due to this. i hope that helps.,hardware,2026-01-24 23:20:53,1
AMD,o1ahaxs,Nope. Doesn't matter at all.,hardware,2026-01-23 19:11:59,4
AMD,o1j3q8s,I don't think PBO is enabled by default. You have to agree to overclocking when you go to the PBO BIOS page if I remember correctly,hardware,2026-01-25 00:41:42,1
AMD,o1ahsmw,It does.,hardware,2026-01-23 19:14:16,-2
AMD,o1aiki0,Nope.,hardware,2026-01-23 19:17:54,1
AMD,o13fpjx,Standard mid-cycle MSRP reset.,hardware,2026-01-22 18:46:52,59
AMD,o14niir,6-7% price jump for even 5% performance would still be well under the usual premium charged for Halo products. I'm ok with it.,hardware,2026-01-22 22:11:22,22
AMD,o13a7xj,"From what I can see the cheapest 9800X3D right now online (no Microcenter deals) is $469. 6-7% price increase is fine I guess for a rumoured 7%-10% perf jump (usually value decreases for more expensive parts). Then again, I'm sure that the 9800X3D price will drop when this launches, which is welcome I guess. Maybe $400 9800X3Ds? Though that will make the 9850X3D basically DOA.   Checked, and it's at $449 in the Philippines but I guess that's more on our currency dropping in value and stores not updating prices yet lmao.",hardware,2026-01-22 18:22:49,64
AMD,o137lgm,"Might as well throw out this old R7 7800X3D, there's an entire 10 extra Minecraft FPS out there for the taking now!",hardware,2026-01-22 18:11:17,50
AMD,o13jbxu,Hopefully the suicide rate for these will be lower than the 9800X3D,hardware,2026-01-22 19:02:51,14
AMD,o143ka3,So Iâ€™m not sad that I spend 260â‚¬ two weeks ago for a 7800x3d.,hardware,2026-01-22 20:35:35,3
AMD,o136ro4,I am skeptical that would actually be the price on shelves,hardware,2026-01-22 18:07:37,10
AMD,o140hdx,What is the likelihood that Nova Lake equals or surpasses the 9850?,hardware,2026-01-22 20:21:12,5
AMD,o14axnf,"It seems that memory performance has improved, but since Infinity Fabric has not kept up, it's actually just an improvement in CPU clock speed. $499 is reasonable.",hardware,2026-01-22 21:10:24,2
AMD,o156wvl,Anyone preorder on Amazon? If so what was your delivery date?,hardware,2026-01-22 23:54:06,2
AMD,o13mbig,It shows in stock at amazon now: [https://instockalert.io/us/ds/amd-ryzen-7-9850x3d](https://instockalert.io/us/ds/amd-ryzen-7-9850x3d),hardware,2026-01-22 19:16:18,5
AMD,o13ygod,Why is that guy smiling? It is not about the price.,hardware,2026-01-22 20:11:52,2
AMD,o13ronk,What about the 9800X3Ds dying?,hardware,2026-01-22 19:40:42,3
AMD,o16ua3m,Might get this and check how well it does in the CPU limited scenarios that I have used for testing 14900KS vs 9800X3D.    9800X3D saw good gains with just RAM tuning and I believe 9850X3D will do better.,hardware,2026-01-23 05:40:32,1
AMD,o16wahd,"I kinda did a stupid, got a 870e-e strix relatively cheap and now im waiting for this to hit stores. Hoping my xmp 6400 ram will work without much hastle",hardware,2026-01-23 05:55:37,1
AMD,o18we7d,Gonna ride out my 7800X3D till AM6 comes out.,hardware,2026-01-23 14:52:14,1
AMD,o1bfrvo,bro i just bought a 9800x3d................!!!,hardware,2026-01-23 21:53:48,1
AMD,o1cfzwj,Hmm. Not good. Price of 9800X3D will still be $450 then.,hardware,2026-01-24 01:03:32,1
AMD,o13cavc,Wasn't this CPU already out?  A step above the 9800X3D?,hardware,2026-01-22 18:31:54,0
AMD,o13m45k,"What is the actual difference between a 9800X3D and a 9850X3D? What is the purpose of this product?  Better to wait for the ""11800X3D"" or whatever it'll end up being called.",hardware,2026-01-22 19:15:22,-2
AMD,o13kkcu,"Really wonder if I should upgrade my 3 month old 9800X3D while these ""exist"" before Ai shitshow makes them impossible to find and probably delays the Zen6 into 2027",hardware,2026-01-22 19:08:24,-5
AMD,o18bezl,"That's cheaper than I expected.   I already have a 9800X3D though, so I'm waiting to see if Zen 6 will be worth upgrading to. The rumor mill says they will have +50% cores, so the 10800X3D will have 12 cores and 24 threads, that is going to be upgrade worthy for sure if the price is somewhat similar to current gen.",hardware,2026-01-23 13:00:15,-1
AMD,o13czg9,The people who would consider upgrading are priced out of DDR5 so this CPU is DOA.,hardware,2026-01-22 18:34:54,-9
AMD,o158a8y,"Yea, I thought there was no way they'd do 499 -  I mean for one, it's AMD, but a product like this, like a KS, normally has a stupid premium attached.  I'm surprised at anything below 549.",hardware,2026-01-23 00:01:19,2
AMD,o1g1r91,"They're hedging against ram prices, good move.",hardware,2026-01-24 16:08:12,1
AMD,o157784,>Maybe $400 9800X3Ds? Though that will make the 9850X3D basically DOA.  Products like this are never DOA if they are the best thing available. Plenty of people bought 12/13/14900KS's even though it was the same thing with slightly better binning/clock speeds.,hardware,2026-01-22 23:55:35,15
AMD,o13vlzi,There is zero chance this thing is anywhere NEAR 10% faster unless AMD is putting like twice the power through it.,hardware,2026-01-22 19:58:35,40
AMD,o149re3,think 3%,hardware,2026-01-22 21:04:47,14
AMD,o14br47,It's a \~3% gaming performance improvement even by AMD's own slides.  Productivity might be closer to 7% but that's about the theoretical max what you'd see.,hardware,2026-01-22 21:14:17,30
AMD,o1577k7,dude 400mhz boost (200 if you count pbo) is not going to be in the same universe with 7-10% perf jump LOL,hardware,2026-01-22 23:55:38,0
AMD,o13cf79,"No it won't, it was reported this replaces the 9800X3D.",hardware,2026-01-22 18:32:26,-3
AMD,o13axo7,30->40 great  290->300 meh,hardware,2026-01-22 18:25:57,32
AMD,o14z0y8,"Same, I'll probably ride my 7800X3D until Zen 7 releases ( apparently still on AM5 if the rumors are true ), then I'll probably grab a CPU in the X3D Zen 7 lineup and ride that until AM7.",hardware,2026-01-22 23:13:07,6
AMD,o18ajs2,"Yeah depending on your GPU and resolution, and the games you play, you probably get 95% of the gaming performance in real world FPS of the 9850X3D for half the price.  That's money well spent.",hardware,2026-01-23 12:54:52,6
AMD,o13cj03,Well the expensive thing will be DDR5 so the price of this is irrelevant.,hardware,2026-01-22 18:32:54,23
AMD,o13dg9i,"I donâ€™t think there will be much demand, itâ€™s basically just a replacement/refresh of the 9800X3D and most people that wanted one of those already have one.",hardware,2026-01-22 18:36:56,9
AMD,o13xzvy,Amazon drop their stock today and still in stock. That is the price. Delivery date February 1st.,hardware,2026-01-22 20:09:40,3
AMD,o14cvh6,"Nova Lake will basically be a full node advantage.  So in power efficiency, certainty.  In productivity, almost a certainty. You have to go to 9950 to be competitive against the 285HX (although that's on N3B)  In gaming performance, it depends, esp. on how much cache they're going to put on and how much latency their design has.  In terms of price/performance, depends on their yields.",hardware,2026-01-22 21:19:36,13
AMD,o157ebd,"unless Intel has an alternative for 3d cache, it's not happening lol",hardware,2026-01-22 23:56:37,7
AMD,o156ojb,Did you preorder? Mine says delivery 2/6-2/13. Wondering if others who preordered before me show 1/29,hardware,2026-01-22 23:52:54,1
AMD,o14pe21,Someone must be new to marketing,hardware,2026-01-22 22:20:49,3
AMD,o13dnyo,All the â€œtestsâ€ of it were just people overclocking 9800X3Ds to match the new speed of this,hardware,2026-01-22 18:37:52,12
AMD,o13ta4e,Are you thinking of the 9**9**50x3D?,hardware,2026-01-22 19:47:56,3
AMD,o13nzw8,AMDâ€™s product cycle for Zen has been pretty consistent at around 18 months. This is confirmation Zen 6 is going to be a little delayed compared to the usual cycle.,hardware,2026-01-22 19:23:51,10
AMD,o14fuhc,"The purpose is that improvements in binning have allowed a clockspeed to only be achieved by a % of 9800X3D's when overclocked can now be achieved by most, so it'll be relaunched with that new clockspeed.  This gives a ""new"" product from AMD to have for the press cycle to cover because we're still several quarters away from Zen 6 (it's no coincidence review embargo for 9850X3D comes the day after PTL's)  It'll also reset ASPs on this product up by $20 without ""raising"" prices",hardware,2026-01-22 21:33:46,5
AMD,o152jfw,It serves the same purpose as what Intel's i9-KS products did.,hardware,2026-01-22 23:31:17,3
AMD,o14phot,> What is the purpose of this product?  Make money,hardware,2026-01-22 22:21:19,6
AMD,o14522r,The biggest mistake someone can make is waiting for next generation.,hardware,2026-01-22 20:42:39,0
AMD,o13n75v,sounds like a waste of money to me,hardware,2026-01-22 19:20:15,15
AMD,o13tm0t,"imo, the only way that would make sense is if you could use your old cpu in another system.",hardware,2026-01-22 19:49:26,3
AMD,o13zdgp,"The only one that might be worth it is the 9950X3D2 when it comes out, with dual CCD. But even then youre pretty far into niche territory. Youd have to be hitting your CPU pretty hard to max out the 3D V-cache and need a 2nd one.",hardware,2026-01-22 20:16:04,3
AMD,o150cf4,"Why do you wonder that? Of course you shouldn't. It's not even worth the effort it would take to physically swap the CPU out, let alone the money that will be wasted.",hardware,2026-01-22 23:19:55,2
AMD,o13sqza,People who will spend $500 on a CPU are absolutely not 'priced out' because of memory. :/,hardware,2026-01-22 19:45:31,4
AMD,o13eu0d,So people who are already on AM5 but not using X3D yet don't exist?,hardware,2026-01-22 18:43:01,1
AMD,o13gv8d,"I am on 7700X and happy, but I might upgrade to this chip one day",hardware,2026-01-22 18:52:01,1
AMD,o158yjj,"12900K is a different chip (less cache, less E-cores, won't destroy itself, etc.). 13900K/14900K/KS/etc. are the same chip with different binning.",hardware,2026-01-23 00:04:54,3
AMD,o17796b,"It wont be 10%, PC's scale more or less pretty linear with performance under normal conditions, a jump from 5.2Ghz all core to 5.6Ghz is a difference of 7-8% at most and that ofc only goes if the CPU is the bottleneck.",hardware,2026-01-23 07:25:21,6
AMD,o17je19,productivity is the only reason you would get this CPU in the first place.,hardware,2026-01-23 09:15:55,-9
AMD,o177ecp,"400 with PBO still, you can PBO both chips.",hardware,2026-01-23 07:26:36,2
AMD,o14c5nh,"It's hard to imagine it won't, but that is still rumor.  2-4% gaming improvement and 5-7% productivity improvement isn't exactly a product segmentation step.",hardware,2026-01-22 21:16:10,4
AMD,o13efxe,8.3 ms vs 0.1 ms reduction in input lag,hardware,2026-01-22 18:41:17,14
AMD,o15bs2j,"Same boat. The 78x3D doesn't get the same praise as the 58x3d, but it's a considerable bump in performance and it uses so little power.   I will run this until whatever the last gen x3d on AM5 is, then probably be able to run that for 3-5 more years.",hardware,2026-01-23 00:19:47,9
AMD,o13l19t,"Just put 5 times as much DDR1 then, lmao",hardware,2026-01-22 19:10:30,11
AMD,o14ct0o,Zen 6 x3D just needs a 32 GB cache.,hardware,2026-01-22 21:19:16,5
AMD,o1fz29q,"Yeah I would upgrade (from 5800X) and buy this, but not with the current RAM situation.  I bet these companies are pissed too. Like AMD/Intel (CPU sales), Valve (new steam machine), potentially even Apple?",hardware,2026-01-24 15:55:45,2
AMD,o13kspf,"Yeah, the only real market I see for this (other than people strongly affected by FOMO who need the absolute best of the best for their system and upgrade every single time a new part comes out) are people who are just building a new system from scratch and do want the best of the best. And I don't think many people are keen on building with this current market.",hardware,2026-01-22 19:09:26,5
AMD,o16kfzw,"I believe they were talking about gaming performance specifically. Since you don't compare 1 SKU to an entire generation like Nova Lake.   But by the time Nova Lake releases, Zen 6 comes under contention as well.",hardware,2026-01-23 04:33:02,4
AMD,o18jgvt,Do you mean 285K? Because from the reports I've seen that's what the 9950x competes with. The 9900x for 265k and such.,hardware,2026-01-23 13:45:52,1
AMD,o16297m,"I think Nova Lake is supposed to have an equivalent, bLLC or something like that.",hardware,2026-01-23 02:45:45,7
AMD,o16k55p,"Nova Lake is supposed to have a large cache variant, although it is not 3D stacked, so the latency advantage of 3D stacking remains to be seen.",hardware,2026-01-23 04:31:06,4
AMD,o192zav,Last rumors has Nova-Lake with 144-288 MB of bLLC depending on the model.,hardware,2026-01-23 15:24:12,2
AMD,o1a3hbk,"I just want to know if I should wait for Nova Lake or get a 9850. For gaming, ofc",hardware,2026-01-23 18:10:01,1
AMD,o13thjw,Maybe so.  All the naming is confusing.  Why would they release a 9950 and 9800 but not a 9850?,hardware,2026-01-22 19:48:52,2
AMD,o13oy1k,"Maybe, maybe not. Zen 4 and 5 haven't stopped the seemingly endless Zen 3 releases.",hardware,2026-01-22 19:28:09,6
AMD,o18askw,"No? Where did this myth come from, anyway? The idea of constantly waiting for whatever someone thinks the ""next gen"" is, that doesn't make any sense.  I have a 9600X now. A 9800X3D isn't that great when I know that a perfectly good upgrade is coming relatively soon. Zen 6 will also likely be the last gen on AM5, so it's not likely to get better than this. Unless society simply implodes within 2026~2027, we'll be fine.",hardware,2026-01-23 12:56:25,1
AMD,o13v8kn,"Nope, I would just sell the 9800x3d for $400",hardware,2026-01-22 19:56:53,3
AMD,o155hj5,"It's not a matter of how hard you're hitting it, it's the nature of the applications hitting it. Cross CCD latency is [about as ""bad"" as that of going out to RAM](https://www.overclock.net/threads/official-zen-5-owners-club-9600x-9700x-9900x-9950x.1811777/page-53?post_id=29367748#post-29367748) (both are roughly around 75ns or so, vs ~12ns for local L3 Cache), defeating the benefit of the X3D cache entirely. [There are ways to make applications that are able to intelligently allocate their own threads within a given physical CCD](https://chipsandcheese.com/p/evaluating-uniform-memory-access), but as far as I know, the means for that feature set isn't included in AMD consumer chips (there *are* multi-X3D-CCD Epyc CPUs for niche applications). Adding those features to a consumer product would mean AMD undercutting their enterprise products by thousands of dollars per unit.  That's why I'm skeptical of a dual-X3D-CCD consumer part every time it comes up. Even if they do release it, it will have to be aggressively gimped to avoid losing them a ton of money while also being a major disappointment to the people that think they want it. The more likely move would be a 9950X3D with better binned silicon like the 9850X3D is (something akin to AMD's old ""XT"" refresh skews or Intel's ""KS"" models).",hardware,2026-01-22 23:46:35,3
AMD,o153s8u,Says you,hardware,2026-01-22 23:37:44,-3
AMD,o19qu49,You wouldn't get a 9850X3D over a 9900X3D or 9950X3D if productivity is the thing you care about.  The 9850 would be for people who would want to pay a premium to have the marginally fastest gaming CPU.,hardware,2026-01-23 17:11:53,7
AMD,o19vvct,If you care about productivity then this is not the chip for you.  285K is 69% faster MT | 15% faster ST  9950X3D is 76% faster MT | 7% faster ST  Even 265K and 9900X3D are much better options closer to that price range.,hardware,2026-01-23 17:35:39,3
AMD,o179f87,"is this, like, confirmed?",hardware,2026-01-23 07:44:30,1
AMD,o14g28s,33% increase in frames v.s 3% increase in frames is another good way to understand why the returns diminish the further you go,hardware,2026-01-22 21:34:49,11
AMD,o18j183,"I mean, I don't think thats true. The 7800X3D was unanimously the most praised AM5 chip when it came out and its efficiency was a core reason as to why. It's not as glazed relatively speaking because the 5800X3D was just such an anomaly. It offered 7600X performance to users that didn't want to upgrade to AM5, was very power efficient going against the 12900K, was fairly cheap for what you got AND was dropped mid cycle for AM5 when everyone would've thought AM4 was dead + people were complaining about DDR5 pricing when AM5 launched and this was a saviour.  Like, the 5800x3D really caught a perfect storm. And the 7800 did get surpassed a few years later (even if by a fairly small margin) so it loses that Halo effect for people.",hardware,2026-01-23 13:43:33,6
AMD,o17jxic,"you need to double the bandwidth for each gen (not exactly so, but a general approximation). So for DDR1 you would need 16 times as much running in 32 channel mode.",hardware,2026-01-23 09:21:07,2
AMD,o15b168,"You could also see people who were putting off upgrading from a 7000 series part finally decide to pull the trigger. It's obviously not worth going from the 9800X3D to the 9850X3D, but if you're moving up a generation having a little bump like this can push you over the threshold to buy.",hardware,2026-01-23 00:15:52,2
AMD,o16lw63,"Yeah, likely X3D Zen 6 will be quite a bit after Nova Lake.  So they might take the crown, but that's the one that they have the least chance at IMO.",hardware,2026-01-23 04:42:30,2
AMD,o19uzat,"Yes, sorry, brain fart. I am so sick of the terrible marketing names.",hardware,2026-01-23 17:31:33,1
AMD,o17js41,wouldnt a side cache introduce same latency issues as going to read the cache from the second CCD? in which case we know this has actual impact.,hardware,2026-01-23 09:19:41,1
AMD,o1a4aqj,nova lake is launching in like late 2026 and is a complete unknown  go for 9800x3d/9850x3d,hardware,2026-01-23 18:13:39,1
AMD,o13yr84,"Just wait, soon we'll have the 9950X3D2",hardware,2026-01-22 20:13:13,2
AMD,o14fpro,"The 9850X3D is just a better binned 9800X3D.  The step above the 9800X3D is the 9900X3D. You get more threads, but lower boost as it's contained within 120 W TDP.  You get 45% more productivity for 7% lower gaming performance or  90% more productivity for like 2% lower gaming performance.",hardware,2026-01-22 21:33:08,4
AMD,o14g2k4,"The Zen 3 parts that are getting released were all manufactured years ago. They just didn't hit requirements, so once enough defective inventory built up, they were released as a new SKU in small numbers",hardware,2026-01-22 21:34:51,3
AMD,o1401pg,Iâ€™m in the same exact scenario. I think we can get $400 for our 9800 cpu.,hardware,2026-01-22 20:19:10,1
AMD,o15ac1g,"I mean... yes? That is literally correct, I did say that.  That not withstanding, how do you justify spending ~$100 (provided you get ~$400 for your 9800X3D, and ignoring potential selling fees) for maybe 8% more FPS in the best case scenario? Are you even stressing the 9800X3D with your 9070 XT, or would you land at literally the same performance after the swap?",hardware,2026-01-23 00:12:12,3
AMD,o1bi3ij,"No, gaming gains here are irrelevant. Productivity gains of 7% can make enough difference to pay for itself. Productivity is the only scenario where this is useful.",hardware,2026-01-23 22:04:45,0
AMD,o179kpl,Nah but id be very surprised if they weren't like all other Zen 5 chips are able to do.,hardware,2026-01-23 07:45:51,2
AMD,o14j045,"Even then, going from 290 FPS to 386 FPS (33% increase) is a much less noticeable change than going from 30 to 40 because it's still a much lower decrease in input lag.",hardware,2026-01-22 21:49:05,9
AMD,o19w7d0,"Makes sense. The 78x3d was certainly very well received. It got buy recommendations from reviewers and it was the top of the line during it's run. I got mine at the low point in it's price history for ~$300, which was a steal imo. I can't imagine anyone with this cpu will feel the need to upgrade (for gaming) for a long time.   I agree that the 58x3d was the perfect cpu for a perfect situation.",hardware,2026-01-23 17:37:10,2
AMD,o180zbo,"Well, then just don't put any DDR ICs on your board, boot up from on-chip memory with a minimal bootloader, and download some more RAM onto the board from one of those webpages.",hardware,2026-01-23 11:49:17,2
AMD,o19yoy0,No problem. Laptop SKUs are especially confusing.,hardware,2026-01-23 17:48:32,1
AMD,o18jpi1,Depends on the interconnect no? From what I know infinity fabric is the only thing that prevents cross die memory access for AMD.,hardware,2026-01-23 13:47:09,2
AMD,o14rb25,"I mean this is the same thing, they just built up enough inventory of good 9800X3D bins.",hardware,2026-01-22 22:30:31,2
AMD,o1bkd1d,Productivity more so cares about multi core performance and you wouldn't pick a 8 core CPU with somewhat better single core performance over a 16 core CPU like the 9950 with more than 50% higher performance.  Gaming gains are marginal/irrelevant but they will be the reason people will buy it for and it is also how it is being marketed.,hardware,2026-01-23 22:15:45,3
AMD,o14jv4w,"True, I just think the raw percentages are easier for most people to understand.   Modern computing really is a marvel man. If you would've told 14yo me that one day I'd have a CPU / GPU that crushed basically everything at 1440p 180Hz and wasn't even technically the top end I would've shit lol",hardware,2026-01-22 21:53:13,4
AMD,o19ylof,"Definitely a steal. That's near 5700X3D cheap for like 30% better performance and more upgrades available.  I wouldn't be surprised if the 7800X3D lasts a decade. Though, the rumoured increase in core counts for the next gen does make it a tad less likely. It's still gonna last a damn long time though.",hardware,2026-01-23 17:48:07,3
AMD,o1hz6ws,"Also bought it at around its all time low, had no idea. It was in May 2024 if I remember well.  Bought it because the relatively recently released Helldivers 2 was obliterating my ole trusty i7-8700, the GPU was twiddling its thumbs at as low as 35% usage.  I was considering waiting for the 9800X3D, glad I didn't.",hardware,2026-01-24 21:18:50,2
AMD,o1bhj07,"Considering ArrowLake has increased latency issues even inside the same CCD, im not so certain Intel is going to solve this issue.",hardware,2026-01-23 22:02:04,1
AMD,o17jl8j,My old child self running the CRT at 85hz back in the day dreamed of beating 100 hz on the screen. Now i got  a 144hz monitor :),hardware,2026-01-23 09:17:50,3
AMD,o10xa2g,"Summary:  â€œWhile the Ryzen AI Max+ 395 was faster overall, the Framework Desktop was consuming more power than the Dell Pro Max GB10. On average the Framework Desktop over this entire span of CPU benchmarks saw an average wall power reading of 133 Watts with a recorded peak of 227 Watts. The Dell Pro Max GB10 meanwhile had a 103 Watt average with a recorded 164 Watt peak for all of these benchmarks. So while the Framework Desktop performance geo mean was at 1.285x, the total system power consumption was 1.29x higher on average and the peak was 1.38x.  If you are investing in the Dell Pro Max GB10 or any other GB10 platform, chances are though you are running AI workloads and other software fully engaging both the CPU and GPU. The Blackwell GPU with the GB10 offers a lot more compute potential over the Radeon 8060S graphics but similarly the Dell Pro Max GB10 pricing is around $4139 USD as of writing and the Framework Desktop around $2928 USD if going for the same RAM and storage capacity. More GB10 GPU benchmarks are on the way at Phoronix.â€",hardware,2026-01-22 10:48:38,37
AMD,o12ijgh,I have GB10 in my lab. The real issue with my unit is thermal throttling.  When it reaches 70C throttling kicks in and the performance drops a lot. Forcing additional air using a mini blower it maintains much higher interference tokens/s.,hardware,2026-01-22 16:18:58,10
AMD,o11p0dj,The GPU portion will be the juicy bit,hardware,2026-01-22 13:55:52,5
AMD,o129noe,"As someone who's mostly into applied math and stats research rather than hardware, I'm always a bit confused as to how to parse these for my own use cases. Which of these benchmarks correspond to decomposing thousands of large matrices/arrays in parallel using Python or R (which I suppose use LAPACK or something like that in the backend)?",hardware,2026-01-22 15:38:45,3
AMD,o18gj19,"Not mentioned here, but when you start going concurrent performance testing, via vLLM, the GB10 ends up being 4x to 5x faster than the Strix Halo (which itself can be quite faster with concurrency).",hardware,2026-01-23 13:29:58,1
AMD,o10z18l,"The lack of a real efficiency advantage was baffling given Nvidia/Mediatek are using an N3 node. You'd expect higher efficiency, not the same (Halo has +28% geomean performance at +29% avg power consumption).   Obviously the Nvidia chip focus is more on the GPU though.",hardware,2026-01-22 11:03:46,41
AMD,o12zufg,[That was the prior article](https://www.phoronix.com/review/dell-pro-max-gb10-llama-cpp).,hardware,2026-01-22 17:36:49,8
AMD,o12sl1e,"PETSc is functionally somewhat in that field: https://petsc.org/release/manualpages/   Video encoding is a massive matrix calculation session, but usually using heavily optimized assembly over standard libraries that you'd use.      You can check open benchmark for full results, to see if some libraries are familiar: https://openbenchmarking.org/result/2601218-NE-DELLPROMA02&sgm=1&asm=1&ppw=1&sor=1&scalar#results",hardware,2026-01-22 17:03:46,5
AMD,o1123dc,"That might simply be due to the fact that Strix Halo has a much wider CPU. It's 16 Big Cores + SMT. So at a given wattage, Strix Halo can run at lower clock speeds and thus match the efficiency of GB10.  It also does not help that GB10 is using a generation older ARM CPU cores, with less cache and a strange cluster arrangement to top it all.",hardware,2026-01-22 11:29:12,28
AMD,o111za8,The GB10 devices come with ConnectX card that is probably responsible for some 5-10W of base load.,hardware,2026-01-22 11:28:17,15
AMD,o13p03b,DGX Spark power management is uhh a tad lacking and comes with... oddities.  It has a tendency of putting itself in really deep power states too eagerly while taking a while to wake back up: https://forums.developer.nvidia.com/t/connectx-7-nic-in-dgx-spark/350417/51  Wonder how much that might have affected perf results for some subtests. It _should_ have no impact on number crunching style workloads though.,hardware,2026-01-22 19:28:24,4
AMD,o16t07w,Perhaps better optimisation of x86 code?,hardware,2026-01-23 05:31:12,1
AMD,o17e9gs,I think you are underestimating Mediateks architectural prowess here. They know how to make tiny microcontrollers. They dont have good record with CPUs. And Nvidia here is likely not doing the designs outside of the GPU.,hardware,2026-01-23 08:28:11,1
AMD,o15ajba,"I know, it deserved its own thread on here too. I think it didnâ€™t get one.",hardware,2026-01-23 00:13:14,3
AMD,o153l77,"Video encode uses simd on cpus, but doesn't do much where fast matrix multiplies would be helpful.",hardware,2026-01-22 23:36:43,5
AMD,o113oqd,"GB10 has 20 cores, although it's fair to point out half of those are small-ish A725 cores.",hardware,2026-01-22 11:41:48,17
AMD,o131f9w,You're forgetting 1 thing. Finflex.,hardware,2026-01-22 17:43:57,0
AMD,o13nbhu,"5-10W sounds low to me. Removing the DAC cables only strips off 4W right now, it's more in the 20W range eaten by the NIC.  That said the next major kernel update for it will address things hopefully by allowing the NIC to actually go in a lower power state: https://github.com/NVIDIA/NV-Kernels/commit/18a062fe0c4965eb6a2ddddfe8de8e465d05a98e",hardware,2026-01-22 19:20:47,3
AMD,o11qjex,The ConnectX card is the big difference maker in value as well. Connecting two is a pretty easy upgrade. If you were going to get only one then Strix Halo would be a better pick if you don't need something Nvidia specific. Even more so before the RAM prices really drove up pricing on the Framework.,hardware,2026-01-22 14:03:52,3
AMD,o11v05p,">Â although it's fair to point out half of those are small-ish A725 cores.  More than fair. An A725 in the GB10 has half the ST perf of their X925s, though the gap may close in nT workloads depending on how high each core is boosting there.",hardware,2026-01-22 14:26:53,8
AMD,o114ux0,Which is offset somewhat by lack of SMT and IPC of an older arm architecture,hardware,2026-01-22 11:50:39,10
AMD,o16f8e4,what about it?,hardware,2026-01-23 04:00:30,3
AMD,o1bgxu0,"Yep, the GB10's A725 are only paired with 0.5MB L2, that'd be barely larger than 1mm2, i.e. far smaller than Zen5C, similar to Intel's Mont cores  That's the same L2 MediaTek uses for the E-Cores/little-Cores (A720/C1-Pro) in their D9400/D9500  Also the GB10 has a weird asymmetric L3 cache. One X925+A725 cluster has access to 16MB L3, the other X925+A725 cluster has access to only 8MB L3. Whereas [each of Strix Halo's 8C clusters has their own 32MB L3 each](https://substackcdn.com/image/fetch/$s_!OGNa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffbc6d88c-c362-4ea6-8a86-f677313342a1_1600x1016.png)  Don't think they've released die shots yet, but the GB10's is probably around 50mm2 (estimated from the D9400/D9500). Strix Halo's CPU probably is about 2.5x larger with about 1.5-2x more transistors once accounting for N3 vs N4",hardware,2026-01-23 21:59:18,2
AMD,o00bsqr,AMD in competition with itself for eventual most specific SKU name,hardware,2026-01-16 22:45:41,326
AMD,o014ka6,"What a stupid and low effort article.   It's Geekbench 6 and some random person shared the Benchmark result of a 9950X3D2 and compares it to the result of some other persons benchmark?    They're not even running the same motherboard and no one knows what configuration is set.   The 9950X3D2 lands at 24430 in MT and 3553 in ST score.    Results for regular 9950X3D on Geekbench are between like 20k and 25k for MT and around 3400 and 3700 for ST. This benchmark doesn't show anything, might be better or worse.",hardware,2026-01-17 01:31:06,101
AMD,o00wcsd,X3D2 sounds like R2-D2's cousin,hardware,2026-01-17 00:40:21,45
AMD,o00fs9a,"Not bad, back in the day during intels dominance this used to be called a â€˜generational upliftâ€™",hardware,2026-01-16 23:06:33,115
AMD,o0112bt,Add two 4GB sticks of RAM and a 3060.  It's like skipping leg day,hardware,2026-01-17 01:08:53,6
AMD,o00f77f,"Not sure a 250w PPT is worth it, that's 9% more power for 7% more performance, on top of an inevitable price increase. Now of course there will be some applications that will benefit from the further cache increase, but the vast majority won't.",hardware,2026-01-16 23:03:25,35
AMD,o00knpv,Obama_Giving_Obama_a_medal.jpg,hardware,2026-01-16 23:33:26,10
AMD,o00ebov,I would imagine it will be incredibly game dependent.,hardware,2026-01-16 22:58:45,15
AMD,o00n82x,i want a video of someone trying to cool that in a fracta terra,hardware,2026-01-16 23:47:57,3
AMD,o01460a,When will they release the X3 double D?,hardware,2026-01-17 01:28:35,2
AMD,o02ktoa,"As expected, kinda. Would be even more interesting though to see how it will perform in games, and if AMD found a way to have both CCDs' 3Dcaches communicate quick enough with each other to make a real difference here. If so, this might be a game changer, literally.",hardware,2026-01-17 07:51:37,2
AMD,o032d6h,I think AMD need to go all-in for 3D cache now and stop the non-3D variant on mid-end and high end.,hardware,2026-01-17 10:36:17,2
AMD,o00s1fv,That's a lot more than I expected.,hardware,2026-01-17 00:15:27,4
AMD,o02ivio,We adding numbers to X3D now? What will they think of next,hardware,2026-01-17 07:33:36,1
AMD,o03ifwp,So it will beat the arrow lake refresh coming.,hardware,2026-01-17 12:52:05,1
AMD,o03t45j,I wonder what Skylines 2 population size this could support without sim speed going to hell. If only RAM prices hadn't gone crazy.,hardware,2026-01-17 13:58:49,1
AMD,o046lt8,X3D2 Electric Boogaloo Edition,hardware,2026-01-17 15:11:05,1
AMD,o04vocj,Missed opportunity for 9950X3DNOW!,hardware,2026-01-17 17:10:04,1
AMD,o05y8ww,"If this is released in sufficient supply, it may be sold in small quantities as it is tantamount to denying the 9950x3d.",hardware,2026-01-17 20:12:20,1
AMD,o0rhbwn,"Super expensive CPU, super expensive motherboard, super expensive DDR5 memory, super expensive graphics card with a 1080p monitor.",hardware,2026-01-20 23:52:40,1
AMD,o010d52,Does it fail twice as fast?,hardware,2026-01-17 01:04:27,0
AMD,o0160wc,"Welp, the people who wanted it are going to get it (only thing that comes up in the top of my head is y-cruncher, CFDs, FEAs(?), any HPC workload idk lol). That said, AMD engineers have said dual v-cache CCDs won't boost performance that much compared to a single CCD, so much so on the 9950X3D they've made efforts with chipset drivers, AGESA, windows power profiles to get games running on the v-cache CCD.",hardware,2026-01-17 01:40:27,1
AMD,o00arc6,Do I need RAM to use it?,hardware,2026-01-16 22:40:26,-3
AMD,o02x783,You don't say?  Now where did all the people who thought performance would actually be hurt disappear to?,hardware,2026-01-17 09:47:41,0
AMD,o00hvg7,At 1080p ultra low settings.  Aka. No thanks,hardware,2026-01-16 23:17:52,-33
AMD,o011g40,nnnnX3D2V1finalfinal,hardware,2026-01-17 01:11:18,69
AMD,o00ds7r,x4d wouldâ€™ve been way cooler imho,hardware,2026-01-16 22:55:56,58
AMD,o0180a4,9855x3d,hardware,2026-01-17 01:53:09,7
AMD,o022837,"While also being understandable.  Yeah, I'm throwing shade at your display industry.",hardware,2026-01-17 05:13:59,3
AMD,o01vwbp,"Thankfully they've not chosen to embed ""AI"" in the name somewhere.. for desktop SKUs, at least.",hardware,2026-01-17 04:28:45,4
AMD,o02uksb,They got that new patent for 3D stacked L2 cache. So the first chip with both types of 3D stacked cache is gonna be XXXX3Dx3D2?,hardware,2026-01-17 09:22:40,2
AMD,o19n70i,"yeah, that 5% increase every 2 years is insane",hardware,2026-01-23 16:55:14,2
AMD,o01018g,"well, something like ryzen 10800 fxxtx3d could be come out andyay.",hardware,2026-01-17 01:02:23,2
AMD,o02s5qd,9950X3D & Knuckles.,hardware,2026-01-17 08:59:40,0
AMD,o01ig6h,The single-core test is very suspicious. Why it would it be any faster unless there's a clock bump? It should be strictly identical at best.,hardware,2026-01-17 02:58:55,20
AMD,o00xytp,"More like some later model astromech, the Zen 5 to Artoo's Zen plus.",hardware,2026-01-17 00:49:47,8
AMD,o00pocr,14nm+++++,hardware,2026-01-17 00:01:52,60
AMD,o01s03g,"No need for ""back in the day"", in mobile in big 2026 AMD considers going from 5.1ghz to 5.2ghz maximum clock to be a ""generational uplift"". That's 1.96%.",hardware,2026-01-17 04:01:31,18
AMD,o00xtzj,"Plus it's before any possible OS post-launch dinkering to smooth over any real world use dual cache weirdness, though I couldn't imagine what that would be.",hardware,2026-01-17 00:48:58,4
AMD,o01fmb4,Will this bottleneck? Pls help,hardware,2026-01-17 02:41:18,3
AMD,o011p10,"9% more power for 7% more performance is actually a pretty good deal, when you consider that that 9% more power is just for the CPU. Factoring in the entire energy cost of a full system, that easily amounts to a PPW uplift.",hardware,2026-01-17 01:12:54,30
AMD,o00tfrt,9% power for 7% performance doesn't sound that bad?,hardware,2026-01-17 00:23:33,52
AMD,o00tfzf,"It's 7% in Geekbench only. There's been no other news/leaks about how it performs in other benchmarks and games so it could quite possibly see a higher increase under other loads (or conversely, see a decrease where the extra cache is not utilized).",hardware,2026-01-17 00:23:35,24
AMD,o0116tz,The main benefit is simplification of running software. You'd no longer desire to choose which CCD to pin a program to for the best experience and can just let a program use both.,hardware,2026-01-17 01:09:40,11
AMD,o01vf7v,It's to make the specs sheet look beefier. Actual power draw and efficiency should be virtually identical to the 9950X3D. If you increase the PPT manually to 250W on the 9950X3D you'd get similar results in everything apart from heavy cache-sensitive computing workloads.,hardware,2026-01-17 04:25:23,3
AMD,o021dou,> that's 9% more power for 7% more performance  Well... Yeah? That's pretty normal.,hardware,2026-01-17 05:07:48,2
AMD,o02qpnz,Still using significantly less power than Intel.,hardware,2026-01-17 08:46:04,2
AMD,o0cd213,The baseline is 23% more power for 7% more performance. Only 9% is good!,hardware,2026-01-18 19:29:17,1
AMD,o00n4xx,I think it will age very well.   If you are purely game it's a waste of money but if money is no issue and you know you are not going to change your PC for the next 5 years I would get it.,hardware,2026-01-16 23:47:28,15
AMD,o02jprf,"Looks perfectly logical to me.     * ""X"" = overclockable part (PBO)    * ""3D"" = extra slice(s) of L3 cache    * ""2"" = number of extra L3 cache slices.",hardware,2026-01-17 07:41:21,7
AMD,o051w4z,"Hey, I still remember AMD K6-2 and 3.",hardware,2026-01-17 17:39:24,2
AMD,o00d5qh,No if you install an OS that uses less than 192 MB of RAM,hardware,2026-01-16 22:52:39,14
AMD,o00iopj,That's how you benchmark a CPU for games,hardware,2026-01-16 23:22:20,27
AMD,o034vrt,Japanese dev's naming of their games are jealous.   Kingdom Hearts HD 2.8 Final Chapter Prologue  Melty Blood Actress Again Current Cod  Shin Megami Tensei: Devil Summoner: Raidou Kuzunoha vs The Soulless Army.   SD Gundam Sangokuden Brave Battle Warriors: Shin Mirisha Taisen,hardware,2026-01-17 10:59:30,19
AMD,o00k7b7,"XT3D would also make sense. XT is their ""enhanced model"" suffix for CPUs already, and 3D is just the one they add for 3D cache.",hardware,2026-01-16 23:30:51,65
AMD,o00yc74,"I honestly think they're waiting for 2-hi sram stacking before rolling that one out. ""3D"" has really permeated it's way as a status indicator for those in the know so it would be very compelling to have x4d.",hardware,2026-01-17 00:52:02,10
AMD,o05b7rr,Nice. AMD figured out a way to etch circuits onto tesseracts.,hardware,2026-01-17 18:22:34,1
AMD,o036688,Nah too simple. Ryzen 9 9950X3D2-XT-AI  I'll send you the bill AMD,hardware,2026-01-17 11:11:22,9
AMD,o0277x6,Bring back AIW.,hardware,2026-01-17 05:52:28,2
AMD,o07ulxu,Not enough X's,hardware,2026-01-18 02:04:51,1
AMD,o01uh5v,Depends on which single core was tested. Sounds like it may be comparing a non-X3D core with an X3D core there.,hardware,2026-01-17 04:18:44,5
AMD,o02bbbm,There supposedly is an increase in clock speeds similar to the 9850X3D's. The new chip with v-cache is supposed to almost match the current 9950X3D's non-v-cache chip's speed.,hardware,2026-01-17 06:26:43,2
AMD,o02xs7o,"Maybe the OS was scheduling threads on the core without cache, and with the dual cache version you don't have to worry about that any more.",hardware,2026-01-17 09:53:16,1
AMD,o039cyg,Good old binning might be able to do that.  But I agree without more than one sample and rigorous testing conditions it's hard to say there is any real improvement.,hardware,2026-01-17 11:40:07,1
AMD,o02klik,"AMD looked at this and said hold my beer, recerntly announcing RNDA3+++.",hardware,2026-01-17 07:49:32,10
AMD,o0116p6,"Pretty much all 14nm refreshes brought more than a 7% uplift, but who cares about accuracy?",hardware,2026-01-17 01:09:39,23
AMD,o07vn34,It's crazy that they aren't even a majority of marketshare yet and they're already acting like intel did at 98% lol,hardware,2026-01-18 02:10:38,4
AMD,o035uxm,Not in valorant no,hardware,2026-01-17 11:08:30,2
AMD,o0400f8,We've fallen far to use this logic to determine uplift in cpu performance?,hardware,2026-01-17 14:36:20,2
AMD,o02k841,Especially since the 9800x3d is pretty damn efficient,hardware,2026-01-17 07:46:04,3
AMD,o03a90x,intel designers in 2004 be like:,hardware,2026-01-17 11:47:45,1
AMD,o09840l,"Yes, I would be quite happy with even 15% power increase for a nice round 10% improvement.",hardware,2026-01-18 07:41:45,1
AMD,o04097g,"Normally, we aim for the power number to be smaller than the performance number. Because this is amd competing against itself, maybe we're using a different light? But we're effectively exiting the sweet spot on the efficiency curve.",hardware,2026-01-17 14:37:39,0
AMD,o03gosx,But what if my monthly electricity budget is $0.37? Thatâ€™ll break the bank!,hardware,2026-01-17 12:39:36,0
AMD,o03o3do,"afaik there has never been a correlation between geekbench and gaming performance, pretty lame how people eat up the headlines even if they're meaningless",hardware,2026-01-17 13:29:22,1
AMD,o014nk3,I think it depends a lot on how exactly the software/ hardware works. If the workload still has to fetch from the other CCD then it won't make a difference.,hardware,2026-01-17 01:31:40,7
AMD,o01ctv1,"There's still a latency penalty since it's two separate caches, so it will still be best to keep it on one in a lot of cases.",hardware,2026-01-17 02:23:49,5
AMD,o0257dm,Yeah but there will always be one CCD that will be faster,hardware,2026-01-17 05:36:33,1
AMD,o02ks60,Its actually worse now. Because if the software uses threads from both CCDs and those threads then want to read each others cache thats a lot of extra latency.,hardware,2026-01-17 07:51:14,1
AMD,o039prv,"Age very well in isolation yes. But with zen 6 coming with a 2 node jump, much better packaging, 50% more cores and cache, frankly I'd skip this one.",hardware,2026-01-17 11:43:12,4
AMD,o01sbj0,"It probably will not game any meaningfully different than a single CCD chip, at present and in future.",hardware,2026-01-17 04:03:45,6
AMD,o00wjvy,It will be only at 1080p as well. Lotsa people buying bleeding edge 16-core CPUs to play at 1080p /s,hardware,2026-01-17 00:41:30,-10
AMD,o02ldnb,"3D is for stacking the cache in 3 dimensions, as in on top (7000 series) or bellow (9000 series) the cores. Traditionally the layout is 2D.",hardware,2026-01-17 07:56:42,0
AMD,o01wsba,"Actually, I donâ€™t _think_ AMD or Intel support â€œtreating the (now giant pile of) SRAM as normal CPU ramâ€, from someone making the joke with Intel Granite Rapids AP at ~500MB L3 and AMD server CPUs up to 768MB?   Would love to know if Iâ€™m wrong about that though, thatâ€™d be super cool. Intel _did_ support booting Sapphire Rapids â€œMaxâ€ (the HBM equipped models) with nothing but the HBM on the package though!",hardware,2026-01-17 04:35:05,3
AMD,o00kn8i,"With the increased CPU load that some settings can add to a CPU, I don't entirely agree. Things like RT and crowd density can load up a CPU in somewhat significant ways.",hardware,2026-01-16 23:33:22,3
AMD,o02lq61,"Thats how they benchmark a CPU for games. How it should be benchmarked however is picking games that are CPU bound so then resolution wont matter, because a properly picked game will CPU bottleneck at 4k+.",hardware,2026-01-17 07:59:56,-1
AMD,o00jqsq,"Yes, and the point is that it's not remotely worth spending on it on this  You're such a tiny market for something like this in terms of actual real world perform people can feel, but everyone feels gaslit into thinking they need these extremely expensive CPUs",hardware,2026-01-16 23:28:16,-21
AMD,o03kvx4,> Shin Megami Tensei: Devil Summoner: Raidou Kuzunoha vs The Soulless Army.  In all fairness the Shin Megami Tensei part was added to the title by the western localizers.,hardware,2026-01-17 13:08:56,11
AMD,o07u4a8,Now go look at the titles of their light novels,hardware,2026-01-18 02:02:13,6
AMD,o00xcrn,I think they stopped using XT for their processors since it was too confusing with their GPU lineup also being called XT. I still get them mixed up anyway.,hardware,2026-01-17 00:46:09,29
AMD,o02ihed,"Or just replace the T with an F to make 3dfx, as both homage and a nod to their old FX line.",hardware,2026-01-17 07:29:57,4
AMD,o0d02cr,"Why not X3DXT?  That has two Xs, so it must be 2x as good no?",hardware,2026-01-18 21:26:37,2
AMD,o02yoi4,The cashe uses time travel?,hardware,2026-01-17 10:01:47,4
AMD,o07v2fv,It's not even rdna3+++ it's the same rdna3.5 they've been wheeling out for the past 2 years. And will continue to wheel out for the next year and a half+,hardware,2026-01-18 02:07:25,1
AMD,o0154xb,At their base clocks. But you literally couldn't exceed an overclocked 3770k with an overclocked Haswell equivalent. What was gained in IPC was most often lost in overclockability,hardware,2026-01-17 01:34:44,9
AMD,o01gl88,"Not that I'm seeing, at least not for gaming. Per TPU, 6700K to 7700K to 8700K to 9900K significantly ess than 7% gaming.Â      https://www.techpowerup.com/review/intel-core-i7-7700k-vs-6700k-game-performance/25.html   https://www.techpowerup.com/review/intel-core-i7-8700k/18.html   https://www.techpowerup.com/review/intel-core-i9-9900k/12.html",hardware,2026-01-17 02:47:19,13
AMD,o07warg,In mobile theyâ€™ve never even gone past 27%.,hardware,2026-01-18 02:14:16,1
AMD,o1cg007,"Phew, glad you squashed my concerns. Specifically bought the 9950x3dÂ² in order to get the best FPS on my new 8K OLED.",hardware,2026-01-24 01:03:33,1
AMD,o07k2dr,"Nah, total system impact is always something we should consider, because it's the most meaningful to us as users with workloads to run and bills to pay.  The fact we can buy parts as individual units doesn't mean they should only be considered that way.",hardware,2026-01-18 01:10:12,1
AMD,o0cecwb,"There are three natural points of comparison: old chip at matched power, old chip at matched performance, and buying two of them (embarrassingly parallel workloads only, obviously).  If you use the last one, you obviously need to include the power draw of everything needed to turn the chip on.",hardware,2026-01-18 19:35:37,1
AMD,o0465mb,We have had x600 vs x600X for like every ryzen generation that has had way more power for minimal performance increase.   Exiting the power sweetspot has been every enthusiasts dream buying the fastest gaming cpu on the market.   PBO being a commonly recommended feature blows past efficiency curve for minimal performance gains.  This is just a slighly better binned cpu. It doesn't feel like 14nm+++ yet.,hardware,2026-01-17 15:08:44,1
AMD,o0ceqja,There will almost certainly be some clock speed where it is.,hardware,2026-01-18 19:37:28,1
AMD,o03u6ju,"It's not only that. There is no direct comparison. Someone (anonymous) ran the Geekbench Benchmark and uploaded the results afterwards.  Someone found that result and chose a random result of a regular 9950X3D and used that as a baseline. Both benchmarks were not run on the same system or by the same person. No one knows if these results are in any form comparable or useful.   What we don't know:   \-exact Windows version   \-power limits   \-RAM configuration    \-overclocking yes or no   \-what was the background load? You can run Geekbench while doing something else.   For all Ryzen 9 9950X3D the results differ a lot. The fastest 9950X3D entries are like 30-35% higher than the lowest ones.    The best 9950X3D results are better than the presented 9950X3D.      There is only one logical explanation for this article:   Someone wanted to cash in on the hype for the 9950X3D2 and chose some existing 9950X3D result as a baseline without any explanation.    With all the results, the 9950X3D2 is up to 30% faster or up to 5% slower than the 9950X3D, for any claim between that there would have been results to go with.   But looks like someone thought 7% is positive enough to please the AMD community but also not  so unrealistic that anyone would question the methodology.    If you look behind it, it's just made up.   Not even AI slop is that bad.    But tech/hardware media can afford that, the people who consume that are dumb enough to buy that, that's why lying/making up stories became a multi million (possibly billion) dollar industry.",hardware,2026-01-17 14:04:51,1
AMD,o01paoo,"> [Paraphrasing] >  > Some programs perform worse when split across multiple CCD because the cache is not shared between the CCD.  But... all of AMD's CPU's with multiple CCD run in to that issue and for most things can be ignored, that's not unique to the X3D design.",hardware,2026-01-17 03:43:09,4
AMD,o01vmsr,Good for simulation computing and games that have similar workloads. Single-threaded perf gains nothing like you said.,hardware,2026-01-17 04:26:53,5
AMD,o02l8xv,I use a 7800x3D to play at 1440p and im almost always CPU bottlenecked. This is because the type of games i enjoy are  CPU bottlenecked. Even this CPU would mean im CPU bottlenecked.,hardware,2026-01-17 07:55:31,1
AMD,o02lkmp,"As far as i know theres no OS that can operate without operating memory, but there can be setups that utilize storage as memory. With SSDs it might even be fast enough to be usable.",hardware,2026-01-17 07:58:31,4
AMD,o02rxhk,"More than that, won't any modern motherboard just refuse to POST with no RAM?",hardware,2026-01-17 08:57:29,4
AMD,o02qo2t,"Yeah, L4 cache is invisible to x86-64. Itâ€™s managed automatically by the processor and cannot be eg used as RAM.  There might be undocumented ways you could interact/manage this (modern processors have lots of â€˜undefinedâ€™ instructions; usually used for debugging)    That is more of a design and ISA decision however. Physically it is possible.",hardware,2026-01-17 08:45:39,3
AMD,o01rx36,"You got downvoted but thatâ€™s very true, Digital Foundry does game benchmarking for CPUs perfect for that reason in my opinion. They max the RT, turn the resolution down to 1080p and turn DLSS to a low internal res, that way theyâ€™re not GPU bound but they still get all the extra CPU overhead that RT brings",hardware,2026-01-17 04:00:57,1
AMD,o00nbnm,"It's the best gaming CPU in the world, and will likely be the best general productivity CPU on consumer platforms as well at launch.   Only *you* can decide if paying a lot of money for literally the best is worth it.",hardware,2026-01-16 23:48:30,17
AMD,o02luev,It depends on the games. There are entire genres that will be CPU bottlenecked with this CPU.,hardware,2026-01-17 08:00:59,3
AMD,o00yllq,Didn't they just recently launch a Zen3 refresh of XT chips? They very much do still use that unless they've said those were the last.,hardware,2026-01-17 00:53:38,18
AMD,o04zyh8,"Unfortunately F already means ""no iGPU."" An X3DF would be an existing sku without the GPU.",hardware,2026-01-17 17:30:16,2
AMD,o0dyvic,I like it,hardware,2026-01-19 00:21:08,1
AMD,o01s5y1,"> Not that I'm seeing, at least not for gaming  The post is also not gaming but Geekbench",hardware,2026-01-17 04:02:40,22
AMD,o02qmm8,This is r/hardware not /r/gaminghardware   Non gaming performance is relevant lol.,hardware,2026-01-17 08:45:17,8
AMD,o01kitn,"The 7700K vs 6700K test was GPU-limited, so it not showing any significant differences isn't surprising. Similar for the 9900K test, as the 1080 Ti wasn't fast enough. The 9900K had an almost 10% frequency improvement over the 8700K at stock setttings (4.7 GHz vs 4.3 GHz all-core), with the added L3 cache there's simply no way it would be less than 10% faster.  The 8700K test shows that for CPU loads, the 7700K scored 84.0% while the 6700K was at 77.2%, that's a 9% improvement, the 8700K showed a 19% improvement over the 7700K. The 9900K showed another 19% uplift over the 8700K in the CPU test department.",hardware,2026-01-17 03:12:01,18
AMD,o07xc2v,"I'm actually surprised it's that high, zen based laptops are still pretty uncommon. When i bought a new one last year it felt like there were 10 intel to 1 ryzen based and i had to look pretty hard for one that fit what i wanted",hardware,2026-01-18 02:19:59,3
AMD,o1cl59x,anytime bud,hardware,2026-01-24 01:33:31,1
AMD,o07p3lh,"Both should be considered, but they should be decoupled when looking at benchmarks. You are right that TCO is important.",hardware,2026-01-18 01:35:27,1
AMD,o046s51,"Well, it's supposed to have twice the 3d cache too, so that would be where the real gains are. ISO performance in non cache sensitive tasks would have even been fine, but if they want to hyper bin parts to push the performance too for a very premium price, they'll get a couple whales. I'm guessing this will be somewhat low volume as a part.",hardware,2026-01-17 15:11:59,1
AMD,o01ryec,"I mean no, you can't ignore it, that's how it works.",hardware,2026-01-17 04:01:12,6
AMD,o05gmzu,You just set default to non x3d ccd and pin all games to x3d so that everything running in the background is on the non x3d cores and any non game program that needs more cores will just use both.,hardware,2026-01-17 18:47:27,1
AMD,o02l53y,"games that do simulation will all want to use combined cache so you dont want them using multiple CCDs. If you got some paralelized simulation software though, yeah it will be good.",hardware,2026-01-17 07:54:34,1
AMD,o037tko,This exactly. Games that tend to bottleneck CPUs tend to be relatively easy to run GPU-wise and vice versa. Games that punish both heavily are extremely rare.  Depending on the types of games people play they could experience CPU bottlenecks even at 1440p or even 4K. Bannerlod and X4 I feel are great examples.,hardware,2026-01-17 11:26:25,1
AMD,o038bfy,"It doesn't really matter either way since after all, the use cases are both extremely unrealistic. What matters is that the test stays consistent between benchmarks.  With benchmarks you're not trying to figure out how many frames a CPU can pump out in a game. What you're trying to figure out is how many it's pumping out vs. other CPUs (i.e. the relative performance). Because of that it doesn't really matter whether all the tested CPUs hover around 100 FPS or 600 FPS.",hardware,2026-01-17 11:30:52,2
AMD,o015e8a,"That was 2 years ago (zen3 itself being almost 6 years old) and only worked out because the 5000 series GPUs only went up to 5700xt, so the naming didn't overlap. The last two generations haven't had an XT (we get 7950x instead of 7900xt).",hardware,2026-01-17 01:36:23,7
AMD,o031dpj,"When comparing 6700k vs 7700k, GNs benching showed that even synthetics like Cinebench had less than 5% MC uplift at stock and less than 6% uplift in SC. Ashes of Singularity had a 2.7% uplift. They did not compare 6700k vs 7700k OCd.   8700k becomes harder to compare because they changed testing methodology and the OCd 8700k was delidded.",hardware,2026-01-17 10:27:11,3
AMD,o08kwil,They actually peaked at zen 3 in 2021. The couple of moments when Intel was still on 14nm for the H series before Tiger Lake-H and when the Intel U series was still quad core was when Cezanne looked the most competitive.Â   But they never managed to fully capitalise on it due to shitty supply and shitty OEM cooperation.,hardware,2026-01-18 04:37:38,2
AMD,o01sxnx,"Exactly! If you're someone frequently running special software that's significantly impacted by a split cache then you shouldn't be buying a CPU that does that.  Thankfully most software just wants more cores, higher clock speed and higher IPC with increases to cache capacity being a *bonus*. Cache being split, not shared across CCD's just doesn't factor in to it for I imagine most consumers.",hardware,2026-01-17 04:07:58,1
AMD,o02l044,"I think he means can ignore in a sense that the penalty, while still existing, wont impact consumer experience. If the extra latency means ill open my PDF file 5ms later, thats no problem for me. Its only in heavy load software where it really becomes an issue, gaming, enterprise compute, etc.",hardware,2026-01-17 07:53:16,0
AMD,o03dos2,"Youâ€™re right that it doesnâ€™t matter if your benchmarking is purely academic, but figuring out exactly how many frames a CPU can pump out in a game is important if youâ€™re also looking to understand which GPU you should buy.  You still get to compare relative performance as long as you keep the benchmarks consistent like you said, but if you max the CPU then you also get a real FPS number you can compare to GPU reviews to check whether youâ€™ll be CPU or GPU limited at your target res and settings",hardware,2026-01-17 12:16:08,1
AMD,o019fk2,"Well yeah, except for the Ryzen 5600 XT, which has the exact same number and suffix as the Radeon 5600 XT.",hardware,2026-01-17 02:02:13,17
AMD,o06t3t0,We also had a 5950X. Stop trying to pretend there is any sort of consistency where there is absolutely none.,hardware,2026-01-17 22:48:37,1
AMD,o060uru,"No, you said you don't need to pin processes with this and I'm explaining why that's not true. Saying ""just ignore it"" is weird.",hardware,2026-01-17 20:25:39,1
AMD,o02x1qb,"in the context of this CPU i don't think anyone is worried about opening PDF files or other basic tasks. the main use case of this is people running demanding software that may benefit from the large core count and cache, in which case yeah they're going to care and be doing their research.",hardware,2026-01-17 09:46:16,2
AMD,o02q0d1,I don't think a consumer is going to be so confused they buy a GPU instead of a CPU.   I doubt anyone buys these products based on model name alone without any other research.,hardware,2026-01-17 08:39:33,1
AMD,o01h4c5,I will literally pay more for a worse Intel CPU just so I don't have to associate myself with this company.,hardware,2026-01-17 02:50:38,-16
AMD,o031xah,"Probably, but they could still have just pick a different name. Being caution always pay out, especially considering the whole Nintendo Wii U fiasco.",hardware,2026-01-17 10:32:14,3
AMD,o140bpf,"surprised at the poor battery life of the AMD powered laptop  that being said, it looks like the amd laptop has a bigger screen than the others so maybe that's why",hardware,2026-01-22 20:20:29,56
AMD,o1493pq,"750 USD for an M4 16GB, wtf, that's already a no brainer",hardware,2026-01-22 21:01:40,103
AMD,o13zg5b,"Nice and detailed video.  The Macbook Air matches or outright beats the competition in all segments: Screen, Build, Performance, Battery Life etc...  The Microsoft Surface Laptop comes closest, particularly with it's great build and screen, but on the performance front, the M4 smashes the Snapdragon X Plus.  The Lenovo(AMD) and Acer(Intel) laptops have terrible screens and build quality, though they do have more storage/RAM, but I don't know if that's worth the difference. â€‹Despite emitting a ton of noise and heat, they still lost to the fanless M4 in most performance metrics.",hardware,2026-01-22 20:16:24,48
AMD,o14afe3,How is the M4 so wicked fast in Blender?,hardware,2026-01-22 21:07:59,11
AMD,o17u1sj,"i think the problem is not the performance, is the overall quality, macbook provide much better screen, speaker and touchpad, however you have to consider the storage of macbook is only 256gb even you are using usb dock with 1T m2 ssd, it need extra $200-250 at least.",hardware,2026-01-23 10:53:07,3
AMD,o14clr3,"Intel is the best laptop to get for being X86 and having good battery life on lunar lake. Even then, you can land on a lunar lake laptop with battlemage arc 130v or 140v and that igpu can even do gaming.  AMD and Qualcomm is automatically a NO.  Apple MacBooks are well tuned and got amazing idel battery life if you want the best arm experience with powerful chips.",hardware,2026-01-22 21:18:18,6
AMD,o16t1ki,Never thought I'll see max tech here,hardware,2026-01-23 05:31:28,1
AMD,o1476ww,and now do the same thing again with linux and see what happens  windows is currently just an unoptimized mess,hardware,2026-01-22 20:52:42,-8
AMD,o15nj48,These tests are a bit silly because $750 can also get you a much better Laptop than listed if you find one on sale.  I got a Lenovo Aura 7i 258v with 1440p OLED and 32 gb ram for $700 during BF sales.  Def a step above the Acer they got.,hardware,2026-01-23 01:23:32,-4
AMD,o16ne23,What about Huawei or Mediatek?,hardware,2026-01-23 04:52:18,0
AMD,o148t89,"Yeah, but the Macbook's screen is brighter and has a higher resolution, so the difference is negated.",hardware,2026-01-22 21:00:18,47
AMD,o14gv3e,They are on the oldest TSMC node out of the 4 so it's 100 percent expected.,hardware,2026-01-22 21:38:42,22
AMD,o1d8vif,A tale as old as time,hardware,2026-01-24 03:54:37,0
AMD,o1a2w1o,"I noticed the video is from a month ago so I checked and that M4 is $900 now. The Surface and AMD Lenovo are $800, and the Acer is the only one still at $750.",hardware,2026-01-23 18:07:23,5
AMD,o14rrkd,"Besides the software issue, some people need expandable (or big to begin with) storage and/or ram",hardware,2026-01-22 22:33:15,24
AMD,o15pyag,"Unless you want to run Windows, which a whole lot of people do (me included).",hardware,2026-01-23 01:37:23,2
AMD,o1486od,This comparison in a couple of months will ve very good to see. Small panther lake and X2P will be a huge stepup in every direction.     Everyone but AMD has huge improvements,hardware,2026-01-22 20:57:21,29
AMD,o16ywmj,"to be fair, the snapdragon x plus is outdated. It was supposed to compete with the m2/m3 and it's a 4nm chip. The snapdragon x2 elite should give about m4 levels of performance at lower efficiency.  But the x2 destroys panther lake and ryzen provided that app compatibility isn't an issue (Which microsoft appears to have mostly fixed).",hardware,2026-01-23 06:16:00,1
AMD,o14jbgz,"""The Macbook Air matches or outright beats the competition in all segments: Screen, Build, Performance, Battery Life etc...""    interesting. very much not expected, at least for me.  like, dont apple pcs are way more expensive in general? is that only true for high performance?  cus a laptop for 750usd it must be quite bad pc. (didnt rreally see the video, sry)",hardware,2026-01-22 21:50:36,-14
AMD,o1621q9,"22 seconds vs 3-4 minutes is enormous, that's what I would expect from CPU vs GPU rendering. It's far cry from the 50% difference in single core and with only 4 P-cores, the M4 shouldn't be that far ahead. If they were all running on GPU, I would expect intel's Xe2 to be more competitive. Maybe the limited iGPU vram without unified memory really was the issue here, idk.",hardware,2026-01-23 02:44:38,17
AMD,o16nn14,Because blender doesn't support mobile graphics for Intel or AMD.   Blender 4.2 will be the last release which supports GPU rendering using Metal with AMD and Intel on macOS. Starting with 4.3 Metal rendering will only be available on the Apple Silicon. CPU rendering on non-Apple-Silicon platforms will be kept fully supported.   The reason for this is ever increasing time investment which the team a whole needs to keep doing in order to ensure quality of Blender and Cycles.,hardware,2026-01-23 04:53:59,9
AMD,o15ifiv,Because Apple makes the widest cores in the industry after Qualcomm while Intel and AMD have narrower cores that run at a higher frequency  The current era of CPUs is basically defined by Apple FireStorm. ARM and Qualcomm in the end are converging to that type of design. AMD and Intel will follow,hardware,2026-01-23 00:54:59,1
AMD,o158qtl,"Because their CPUs have the highest(or close to it) single core performance.  So multiply that across a few cores.  Then add in the fact that macOS is built to take advantage of the hardware, and is much much lighter.  Since they donâ€™t need to account for all the hardware variants,",hardware,2026-01-23 00:03:45,-3
AMD,o14ykke,"The announcement of the Steam Frame has me excited about whether or not we'll get Steam emulation on an Apple Silicon chipset.  If it works in the same way that current GameHub and GameNative apps work for ARM handhelds, then this might actually make me switch over to a Mac laptop.",hardware,2026-01-22 23:10:45,3
AMD,o17ilio,"Not everythig is about gaming, the X2E for productivity is the best chip on the market on par with the M4 Max, it's literally faster than Strix Halo",hardware,2026-01-23 09:08:23,1
AMD,o14ny60,I wouldnâ€™t get an Intel pc or laptop for a few years yet. How could anyone trust them after the shitshow,hardware,2026-01-22 22:13:34,-16
AMD,o16rvtw,"That's the same CPU and ram setup as the article, I doubt the results would be much different.",hardware,2026-01-23 05:23:09,14
AMD,o17bkv4,"There is a lot more to  laptop than just the specs. Keyboard, Trackpad, SPeakers, etc.",hardware,2026-01-23 08:03:47,2
AMD,o18nca6,Black Friday was 3 months and a RAMpocalypse ago,hardware,2026-01-23 14:06:13,1
AMD,o1780cr,"Not just that, but a bigger chassis means a bigger battery, usually. So generally bigger laptops can get better battery life than smaller ones, all else being equal.",hardware,2026-01-23 07:31:59,17
AMD,o189afk,Proven by what actual stats?,hardware,2026-01-23 12:47:01,-2
AMD,o14hds3,"It's more of the fact that AMD has not invested in the low power SoC architecture like Intel with LNL/PTL, and of course Qualcomm with their mobile heritage.â€‹",hardware,2026-01-22 21:41:13,37
AMD,o15hzm4,The X Plus was also on 4nm,hardware,2026-01-23 00:52:34,5
AMD,o14jjm6,But also that AMD chip is on a better node than the Apple M2,hardware,2026-01-22 21:51:41,19
AMD,o1hreai,Its being replaced with m5 air soon but $750 it was the best value by far so it had to go up in price.,hardware,2026-01-24 20:41:50,2
AMD,o151rtj,"The Surface and Acer have soldered RAM, stuck at the same 16 GB as the MacBook. And the Mac will run Windows for ARM probably as well or better than the Surface.    You get robbed on storage, but for the average user it's incredibly hard to recommend windows laptops right now they are just broadly non competitive.",hardware,2026-01-22 23:27:15,56
AMD,o150zq6,"That'll be +$200, $400 for an extra 256GB, 768GB storage. +$200, $400 for an extra 8GB, 16GB memory. They get their margin.",hardware,2026-01-22 23:23:14,17
AMD,o16ht71,Then 3/4 of the laptops in this video are already a non starter for you,hardware,2026-01-23 04:16:15,2
AMD,o14edw3,"Panther lake looks really promising, but will it be price competitive? Or will you be paying multiples more than an older gen macbook that still kicks it's ass. It's really pathetic, I'm going to have to strongly consider switch to mac next upgrade.",hardware,2026-01-22 21:26:45,8
AMD,o17bci7,the GPU is the biggest question for X2. The GPUs of the X1 generation were potatoes.,hardware,2026-01-23 08:01:43,2
AMD,o179akm,I love your username,hardware,2026-01-23 07:43:22,0
AMD,o14qzxl,"The current crop of Macbook Airs are REALLY good value for money in general. There are very few drawbacks besides that you cannot upgrade the RAM or storage after the fact, and upgrading those while buying it adds to the price FAST.  Im a PC gal, always have been, always will be, and for many years i basically told people that buying a Mac now is not worth it, too pricey for what you get. This was back when the Airs were around $1300 with 128 gb of storage and 8 gb of ram. Now you can get a brand new 13"" with 256 gb of storage and 16 gb of ram as default for like $1000-1100",hardware,2026-01-22 22:28:56,16
AMD,o14nr2p,"Base specs of Macs are very competitively priced. With Windows you get more and most likely upgradable Ram and storage, but for Macs it's all soldered and much higher upcharged to make up for the cheaper base config (well, at least when NAND pricing is not as bad as rn)",hardware,2026-01-22 22:12:35,13
AMD,o14mnll,"you can't get a flagship mobile cpu and premium chassis at that price so you end up with some terrible windows laptops because they need to compromise, apple makes a lot of their parts so they have better margins to work with, and M chips are basically cheat codes for performance at the cost of app compatibility",hardware,2026-01-22 22:07:02,6
AMD,o18g428,Will Blender 4.3 support iGPUs on Windows/Linux?,hardware,2026-01-23 13:27:41,2
AMD,o1aft73,? Is Blender not on version 5 already?,hardware,2026-01-23 19:05:04,1
AMD,o168xa2,"> AMD and Intel will follow  If they were following, we should have seen that already. M1 is going on 6 years old now which is more than enough time for a brand new uarch to go from theoretical design to production.",hardware,2026-01-23 03:23:09,11
AMD,o17wdol,"I'll note that FEX-emu has issues with Apple silicon, since Apple is fully closed environment, development for it is slow and tedious.",hardware,2026-01-23 11:12:44,4
AMD,o17s2zo,Gen 3 igpu cores is making the gap even larger and larger vs AMD mobile space. It's on bar with a 3050 at same power.,hardware,2026-01-23 10:36:03,2
AMD,o185yh5,The vast majority of client systems in non consumer spaces runs on Intel chips.,hardware,2026-01-23 12:25:02,2
AMD,o15mtso,What shit show?,hardware,2026-01-23 01:19:32,3
AMD,o16sv6e,What shitshow in low power mobile?,hardware,2026-01-23 05:30:13,1
AMD,o196szw,And that M4 also isn't $750 anymore so what's your point.,hardware,2026-01-23 15:41:46,2
AMD,o18eudw,Size matters,hardware,2026-01-23 13:20:39,3
AMD,o14yl3y,"AMD is chasing Nvidia for the AI pie, so they probably are not invested enough to chase the low power SoC crown. Low power SoCs are low margin products that are very fungible.Â   If you only have limited TSMC capacity you would rather produce some high end chips with big margins.",hardware,2026-01-22 23:10:50,13
AMD,o15mgps,Apple has great battery life for native apps.  Anything else is a hit or miss.,hardware,2026-01-23 01:17:29,0
AMD,o16rc1n,How is the M2 relevant? Of course older chips aren't using nodes that didn't exist when they were made.,hardware,2026-01-23 05:19:14,1
AMD,o159cwj,"That's the point, base models of MacBooks, Mac minis and so on are all great deals. But if you *need* more storage or more ram (well ignoring the current ram situation...) other vendors can have better options at saner prices. I did say this is relevant for *some* people.  I say this as someone that would love that SoC, but anything below $3500 would be a downgrade in ram and storage over my $1400 Thinkbook. The software problem is actually the absence of Linux compatibility for me, if it was the absence of windows I would say great :)",hardware,2026-01-23 00:07:01,10
AMD,o1785bs,"And given that you get an actual Thunderbolt 4 port on it, you can add in a large SSD when you need it.",hardware,2026-01-23 07:33:12,6
AMD,o178b7b,Is $400 for an extra 16GB actually that bad these days?,hardware,2026-01-23 07:34:39,2
AMD,o1683i5,Consumers get good products at low prices and power users making their money with their laptop pay extra.  Not so terrible IMO.,hardware,2026-01-23 03:18:20,4
AMD,o1fyilr,"theres always this reply but in 5-10 years, which laptop is still chugging along with zero issues and high resale? apple every time.",hardware,2026-01-24 15:53:13,1
AMD,o14g2q1,"[https://www.dell.com/en-us/shop/dell-laptops/dell-xps-14-laptop/spd/xps-da14260-laptop](https://www.dell.com/en-us/shop/dell-laptops/dell-xps-14-laptop/spd/xps-da14260-laptop)  Panther Lake 8 core CPU, 4 core GPU for $1600 ?  The M5 Macbook Pro is $200 less and wipes the floor in performance.  Do Windows OEMs not know how to make laptops?",hardware,2026-01-22 21:34:53,15
AMD,o1793pp,"Honestly, for a laptop, there is no better deal than a MBA at the moment. Class leading performance, Class leading battery life, actual build quality that doesn't feel like you got it from the reject Pile from TEMU, the best screen, keyboard, speakers, and trackpad, and you don't have to deal with Microsoft using dark patterns to sell you Gamepass, no built in clickbait, and a terminal that actually works.  The downsides are that they don't come with optional RTX video cards, and you have to install a couple of apps to get logical window management.",hardware,2026-01-23 07:41:41,3
AMD,o14eutf,The performance won't be good vs Macbooks for the same price but it won't have the same issues as this gen that was tested,hardware,2026-01-22 21:29:00,6
AMD,o16qqm2,"It's probably cheaper to produce than ARL, or at least *comparable*, so presumably there will be affordable devices eventually.",hardware,2026-01-23 05:15:08,2
AMD,o17cdfo,"if qualcomm is accurate with their reporting, the GPU should be on par with the m4 or m5, I don't quite remember which one.",hardware,2026-01-23 08:10:54,1
AMD,o179s91,thanks,hardware,2026-01-23 07:47:43,1
AMD,o14nk0f,"Interesting.  So my thinking makes sense that in the low end apple is cheaper, but medium to high end its more expensive?",hardware,2026-01-22 22:11:35,0
AMD,o18lpi1,Not any time soon   https://docs.blender.org/manual/en/latest/render/cycles/gpu_rendering.html#oneapi-intel,hardware,2026-01-23 13:57:38,2
AMD,o16is8d,"Lion Cove is 8 wide, though it's IPC is still worse than M1 Firestorm.  On the SoC level, we can see Intel has seen where the wind is blowing and executed with LNL/PTL. â€‹",hardware,2026-01-23 04:22:24,-1
AMD,o17hm7w,The 13th and 14th gen issues and how they covered it up for a while. They didnâ€™t even fix the root cause for the next generation after that. Madness that Iâ€™m getting downvoted or has everyone forgot already,hardware,2026-01-23 08:59:12,-1
AMD,o17iu3g,"You canâ€™t just silo the shitshow and say it wasnâ€™t about low power chips. The fact is Intel had the problem for multiple years, and either never knew about it or hide it, probably a combination of both. Whoâ€™s to say they donâ€™t do that again and that there arenâ€™t issues with the current genâ€¦ it takes time to build trust in the company back",hardware,2026-01-23 09:10:40,-5
AMD,o15gwjw,">AMD is chasing Nvidia for the AI pie, so they probably are not invested enough to chase the low power SoC crown. Low power SoCs are low margin products that are very fungible.Â   AMD's AI GPUs weren't even margin accretive for a while. In fact, idk if they even are margin accretive today. Haven't paid close enough attention to their earnings calls to see if they ever gave an update on that.   Low power SoCs might not have as high margins as desktop DIY CPUs or anything, but their revenue is way higher too.   I also don't think AMD is such a small company that they don't have the resources to focus on the development of both types of products.",hardware,2026-01-23 00:46:45,13
AMD,o15i41a,"If you have limited capacity you use Samsung for it, it's simple AMD not putting in the work",hardware,2026-01-23 00:53:15,8
AMD,o16sdyz,"Because the M2 Macbook wouldn't have died in these tests. The ""Apple chips are so good because of the node"" argument is tired. No, they're also best in class design for laptops too",hardware,2026-01-23 05:26:45,15
AMD,o15ld5h,"On the other hand, if you need more _vram_, at this point you'll have to go to a 5070ti just to get over 8gb - even the base macbook has more than that. For the 32 or 64gb of vram Apple Silicon has? That's going to cost you a pretty penny on the discrete side.",hardware,2026-01-23 01:11:18,11
AMD,o18chvn,"Nice to see a fellow Linux on ThinkPad user here. I personally have considered moving to Mac primarily because I use my laptop primarily to remote into other machines, but if you need on hardware Linux and large integrated storage then yeah, MacOS can't offer that.",hardware,2026-01-23 13:06:56,2
AMD,o1ggu5s,For the mini itâ€™s easy to pay $20 and get a NVMe > USBC enclosure. Sure youâ€™re limited largely to 10Gbps but if you need more speed then drop $80 on a USB4 enclosure and youâ€™ll get close to native.,hardware,2026-01-24 17:15:39,1
AMD,o19s4lh,"It never was, depending on what you're using it for. That 16GB was more the equivalent of VRAM than DDR4 or DDR5.",hardware,2026-01-23 17:17:59,2
AMD,o168pu2,"Perhaps, but the last mac I owned was 256GB base SSD as well, a 2015 retina iMac 27, and it was too small back then. I just consider that the base price on these is $200 more than pictured, and really that $200 should get you 1TB. They would still make bank.",hardware,2026-01-23 03:21:57,4
AMD,o14id1h,"XPS has always been pretty pricey, thought.  On that same page, a $600 upgrades gets you 16 CPU cores, 12 Xe cores, 2x the RAM, 2X the storage, *and* higher res touch screen + OLED  The 4+0+4 CPU is gonna be in a *lot* of cheaper mid-range priced laptops",hardware,2026-01-22 21:45:59,14
AMD,o14n92r,The XPS is cream of the crop tier for Windows laptop though and that CPU is their most premium variant with the big GPU. I'm curious of the cheaper variants myself,hardware,2026-01-22 22:10:03,13
AMD,o16i0sx,XPS is eXPenSive.,hardware,2026-01-23 04:17:36,2
AMD,o19a5dq,">Â Do Windows OEMs not know how to make laptops?  Were you expecting them to know how to do that?  They were generally in the same price range as the nicer MacBook Pros back in the Intel days, once you looked at models that tried to be as nice, and werenâ€™t hot rod designs with something like an expensive processor, but everything else was trash.  Then, Apple spent the last six years cutting the price of â€œI want a MacBook with a color-calibrated high-resolution screen, a processor that feels as fast as the fastest processors on the market for tasks that arenâ€™t massively parallel, and has enough RAM and storage that I donâ€™t need to worry about itâ€ by $1200, while simultaneously getting a much more performant processor with much better battery life. Meanwhile Windows OEMs have seen fairly small laptop cost reductions over the same period, other than getting you newer CPU/GPU at same price.  Windows OEMs being completely unable to completely respond to Apple essentially making it financially unviable to not buy a MacBook if youâ€™re in the market for a nice laptop seems like a reasonable expectation.",hardware,2026-01-23 15:56:48,2
AMD,o15kokb,Thatâ€™s Dell.  MSI has a full CPU for 1300 I believe,hardware,2026-01-23 01:07:29,3
AMD,o18d9l7,"Hmm, let's see...  |SoC|GPU model|Specs|3DMark Steel Nomad Light Points| |:-|:-|:-|:-| |X2E96100, X2E94100|X2-90|2048 ALU, 1.85 GHz|5600| |X2E90100, X2E88100|X2-90|2048 ALU, 1.70 GHz|5100\*| |X2E84100, X2E80100|X2-85|1536 ALU, 1.70 GHz|3800\*| |X2E78100|X2-85|1536 ALU, 1.35 GHz|3000\*| |X2P64100|X2-45|1024 ALU, 1.7 GHz|2600\*| |X2P42100|X2-45|1024 ALU, 0.9 GHz|1400\*| |X1E88100|X1-85|1536 ALU, 1.5 GHz|2500| |M5|10 core||5200| |M4|10 core||3800| |M3|10 core||3100| |M2|10 core||2600| |Panther Lake|Arc B390|12 Xe3|6000| |Panther Lake||4 Xe3|2000\*| |Lunar Lake|Arc 140V|8 Xe2|3300|  â€‹\*estimates  We'll have to wait for real world reviews to see how it performs in actual apps and games.",hardware,2026-01-23 13:11:31,2
AMD,o14pon3,"Well macOS works very differently than windows so comparing them directly is not a blank X is always better because the OS and software that you can use on it, but generally the macbook air is killer value (as long as it runs the apps you need) for 1000$ you get build quality and flagship CPU that windows laptops only kind of get at 2000$+",hardware,2026-01-22 22:22:18,2
AMD,o14v72b,Only thing that is more expensive on Macs than PCs is upgrading to higher quantities of RAM and storage. Chip performance on Mac is head and shoulders better the whole way through the cost spectrum. Only performance advantage PCs get is with high end dedicated mobile GPUs and even then thatâ€™s at a huge cost to battery life.,hardware,2026-01-22 22:53:13,1
AMD,o17l185,That's desktop not laptop,hardware,2026-01-23 09:31:38,5
AMD,o17sr8y,"It isnâ€™t a thing for any real life audience, triply so for companies buying laptops in bulk, nobody knows or cares outside of PCMR type people",hardware,2026-01-23 10:41:58,5
AMD,o17giv1,anyone compared to nvidia is tiny lol,hardware,2026-01-23 08:49:07,2
AMD,o1amb0y,">Low power SoCs might not have as high margins as desktop DIY CPUs or anything, but their revenue is way higher too.  DIY desktop isn't a priority for AMD, either. The reason why they do so well there is that they can reuse silicon design for their server parts to make the desktop parts. In particular, X3D is great for gamers entirely by accident. Epyc X was a server project. Even Zen was originally conceived as a way to make server parts. They just started making desktop products because it was an easier market to get into.  >I also don't think AMD is such a small company that they don't have the resources to focus on the development of both types of products.  AMD has the resources now, although that's a recent phenomenon. Their problem is that they don't really know how to do laptops. That's down to a mix of technical and nontechnical factors, and the nontechnical factors are the harder ones to fix.",hardware,2026-01-23 19:35:35,1
AMD,o17dvlx,"Fair enough, they certainly are best in class silicon for laptops and have been for a while.",hardware,2026-01-23 08:24:39,2
AMD,o16qej3,"> and that CPU is their most premium variant with the big GPU  The one in the link at $1600 is the smaller CPU die and the smaller/cheaper GPU die. So no, not the most premium variant.",hardware,2026-01-23 05:12:48,3
AMD,o170sq8,">The XPS is cream of the crop tier for Windows laptop though  It's just another ""premium"" media laptop, no different from a Lenovo Yoga or HP Spectre.  There are better model families on the Windows side.",hardware,2026-01-23 06:31:09,3
AMD,o153p2g,I dont understand  there are macbook pros that cost 4.800â‚¬  what other brand has a product that is close to that price?,hardware,2026-01-22 23:37:16,0
AMD,o17nrvu,Itâ€™s all Intel. You canâ€™t just say the practices they had that resulted in that mess are separate just because chips are going into a laptop vs PC,hardware,2026-01-23 09:57:01,-4
AMD,o1cnzkb,">DIY desktop isn't a priority for AMD, either. The reason why they do so well there is that they can reuse silicon design for their server parts to make the desktop parts.  Honestly every MHz past whatever the Fmax is on their server skus is directly showing how client is being a ""priority"" for AMD, in terms of their core design, if not directly their CCDs.   And we know that every extra MHz is going to disproportionately cost them in area, and also hurt power, for their server skus.   Besides, with Zen 6, this doesn't even seem to apply anymore anyway. Apparently server is getting their own unique dies with Zen 6C becoming the main server core.   >In particular, X3D is great for gamers entirely by accident. Epyc X was a server project.  And today we have X3D only in client with X3D not being planned for Zen 5 DC at all.   >AMD has the resources now, although that's a recent phenomenon.  AMD has been doing bespoke mobile dies for a while now, no?",hardware,2026-01-24 01:50:25,1
AMD,o15b7yg,Most of them. Can't speak for your specific local market but the top spec Macbook Pro without additional upgrades is about a price match for on the cheapest RTX 5090 laptops in the US.,hardware,2026-01-23 00:16:51,1
AMD,o17rvrd,Which desktop chip got lunar lake architecture? By this logic I can't buy AMD because their Athlon chips are trash. Lunar lake + Battlemage are both ahead off AMD in mobile laptop space and the next gen panther lake announced during 2026 CES is even much better with gen 3 igpu and improved new architecture cores the gap is even wider.   What amd did during CES of 2026 for mobile chips is regurgitatiting old crap. Zero new stuff.,hardware,2026-01-23 10:34:14,5
AMD,o1cwe4m,">Besides, with Zen 6, this doesn't even seem to apply anymore anyway. Apparently server is getting their own unique dies with Zen 6C becoming the main server core.  Without going into rumors, I doubt that a CCD design that ends up desktop-only stays on the roadmap. If this ends up being true, it will be because AMD didn't know what the server guys wanted until after they completed design.  >AMD has been doing bespoke mobile dies for a while now, no?  Two problems. One, making a die doesn't mean you made a good one. It took them a few reps to make a decent mobile Zen. Two, the competition doesn't just make dies. Intel codesigns platforms with the OEM. Apple does a full vertical product.",hardware,2026-01-24 02:39:10,1
AMD,o15i5c2,i see  and are their performace comparable?  ive come to understand that the rtx 5090 compares to nothing in an appel system  is that true?,hardware,2026-01-23 00:53:27,1
AMD,o16hl3s,"True. Though M5 Max will come pretty close to an RTX 5090 mobile, but it's yet to be released.",hardware,2026-01-23 04:14:51,2
AMD,o11e6cw,"20% is a huge success for Apple, especially since Intel Macs consistently held high single-digit marketshare. With the expectation of a lower-cost MacBook launching early this year, it could grow even further. Will be interesting to see this develop.",hardware,2026-01-22 12:54:40,287
AMD,o118ps1,Thatâ€™s what happens when you sell good products.  Apple Silicon was a huge win for Apple,hardware,2026-01-22 12:18:29,325
AMD,o11ggkl,"In other words, Apple sees record high market share in laptops?",hardware,2026-01-22 13:08:33,33
AMD,o119jr0,">According to these charts below, AMD's share in the laptop sector hovers just above 20%  I'm surprised by that. I repair laptops for a living and... maybe 10% are AMD powered and that's being generous.  Guess it might vary by region. If there's an AMD option in a certain product line I rarely see it, I just don't think manufacturers push them much as far as promotions etc.",hardware,2026-01-22 12:24:18,76
AMD,o11jnsa,Even more impressive as they donâ€™t play in the low-end margin negative volume space.  Iâ€™d bet that apples share of the actually profitable price segments is massive.,hardware,2026-01-22 13:26:54,32
AMD,o11jufv,"I wonder how much of this is attributable to Apple Silicon specifically and how much is attributable to the ongoing enshittification of Windows.  I've never been a fan of Apple's attitude towards their users (egregious charges for RAM/storage upgrades, subscription fees for things that should really be part of the warranty, seeking to control what their customers do with the devices they've bought and paid for, etc) but I have to admit that MacOS at least works reliably and isn't constantly blasting you with ads, sponsored tabloid feeds, and ""helpfully"" installing sponsored third-party apps that you didn't ask for.   People want computers, not ""consumer experiences"", to say nothing of the LLM slop. For your garden variety ""Ubuntu? Never heard of her!"" user, a Mac is the only other option they know of.",hardware,2026-01-22 13:27:57,24
AMD,o126mor,Isnâ€™t 20% what apple has been on for a long time? I feel like i saw around 20% 10-15 years ago too?,hardware,2026-01-22 15:24:18,4
AMD,o12r600,"Probably will have more once they get rid of that ridiculous 256GB of storage like they did with the 8gb of Ram, might be awhile.",hardware,2026-01-22 16:57:17,4
AMD,o11n5tj,"AMD is asleep. Their mobile CPUs are literally just being renamed this year, and those are a refresh, of a refresh, of a refresh.  Intel in the other hand has went from essentially unuseable IGPU, to being a literal front runner in a few years. Their CPUs have significantly better battery, life, while still offering better performance, even without the need for hyper threading .  Most of the AMD improvements that have been seen in the past few years are in just the 9000 series GPU that's out right now, and they also first came up with the 3-D cash. Otherwise they have really not done anything to warrant all of the glazing that goes around.  AMD made significant strides with RNA's initial launch, but since then they've just been writing the wave  It's a small wonder that they are not seeing any increase to their market cap,",hardware,2026-01-22 13:46:04,22
AMD,o11qtx1,As a Developer I don't even consider windows laptops anymore...,hardware,2026-01-22 14:05:23,15
AMD,o142pgk,Windows enshitification probably helps too. Not knocking apple here apple silicon kicks ass and that helps tons too. As a developer I wouldn't even consider windows anymore. I have gone Linux but if that wasn't an option I wouldn't even think twice about getting a mac over a windows system,hardware,2026-01-22 20:31:33,4
AMD,o11x8cm,"Where I live, AMD laptops got overpriced to the point the M4 Air is not only a better value, it's outright cheaper.   AMD renamed and overpriced itself out of the laptop market.",hardware,2026-01-22 14:38:30,6
AMD,o16rim6,"Apple MacBooks are genuinely just some of the best laptops out there today, and that wasnâ€™t true during the Intel days. I wouldâ€™ve never even given Macs a second look before Apple silicon, but I ended up buying an M1 Pro for college and I have no regrets. Itâ€™s performed amazingly, and still gets serviceable battery life 4 years later",hardware,2026-01-23 05:20:32,2
AMD,o11hxig,"So in other words Intel lost around 40% in market share since Apple kicked them to the curb. That is just brutal. Considering intels  laptop market share is still declining, Qualcomm is pushing into that space and soon Nvidia the air is getting pretty thin for them.",hardware,2026-01-22 13:17:08,8
AMD,o11achu,Why is this even a comparison when  1. Amd has had difficulties making gains in laptops (not due to consumer devices) because of intel's enterprise relationships which they are investing to counter rn and is at like 25% share 2. Apple ships over 200million units of their products each year  It's almost like tpu is looking for a smaller number to compare apple's share to just to generate a clickbait headline.,hardware,2026-01-22 12:29:42,9
AMD,o11t2da,"Why do articles like this never give actual numbers.  AMDs market share is sinking but its also a shrinking market. People are buying fewer and fewer laptops. It could be just that apple is maintaining their market share in a shrinking market, or not, this article doesnt say.",hardware,2026-01-22 14:16:58,3
AMD,o126i0x,"Not surprising, Apple makes a good processor and has the mindshare. It's a winning combination really.",hardware,2026-01-22 15:23:40,3
AMD,o125ae5,How much of that increase is because of Microsoftâ€™s pivot to â€œwindows as a serviceâ€?,hardware,2026-01-22 15:17:48,3
AMD,o11y0bk,It would surpass Intel if theyâ€™d accommodate dual booting again,hardware,2026-01-22 14:42:25,2
AMD,o15gw5z,What a strange way to title an article for Apple reaching all time high laptop market share.,hardware,2026-01-23 00:46:41,2
AMD,o1342oz,More competition means better prices,hardware,2026-01-22 17:55:41,1
AMD,o15qqp0,The Intel market share is really high because of business laptops. So take this with a grain of salt,hardware,2026-01-23 01:41:56,1
AMD,o15z549,"With Windows getting crappier bday by day, falling market share of native windows apps and apple releasing cheaper macbooks, it's a great opportunity for apple to increase their market share in the laptop segment",hardware,2026-01-23 02:28:41,1
AMD,o1fpmq2,"are Apple actually selling more laptops? Otherwise, all this tells us is that they started making their own cpus",hardware,2026-01-24 15:10:10,1
AMD,o11oh2c,"Is this supposed to be impressive?   M chips have been out for a while but the avalanche of ARM never materialized. The glazing they got on release was crazy. Death of x86! In the end all Apple could do is match AMD. The AMD that doesnt even try to compete in mobile, the AMD that refuses to up production of their mobile chips when OEMs beg them to?",hardware,2026-01-22 13:53:03,-7
AMD,o12ep8g,"All of the top tier laptops don't use Ryzen, take a look at Thinkpad line for example. Intel is back to their old bullshit where they incentivise OEM'S to kneecap the competition.",hardware,2026-01-22 16:01:34,-2
AMD,o17gdwq,Translation: AMD market share plummets as low as Apple silicon.,hardware,2026-01-23 08:47:51,0
AMD,o144enn,Fuck Apple may they go bankrupt,hardware,2026-01-22 20:39:35,-2
AMD,o14v8m8,"Apple has always had a great product, but its market share growth has always been hindered by its price.  Been this way for decades and a new faster CPU isnt going to help much because we've been down this road before and every 10-15 years they abandon their current hardware in favor of something new and better and it still doesn't help them, it mostly keeps Apple users buying more Apple products.    Watch their numbers drop next year as high RAM, SSD and CPU prices push people to buying a Windows Laptop because its significantly cheaper as Apple will not sell anything unless theres a massive profit margin.",hardware,2026-01-22 22:53:27,0
AMD,o16g5gk,"I feel like the real metric here is how much more Apple is selling now vs. 6 years ago. People have never made the choice between an Apple laptop with Intel, AMD, or Apple silicon- they're either buying a windows PC (where they might shop around), or they're buying an Apple product, where they get what they get.   That is to say, if Apple released an iPad that ran on Nvidia's ARM tomorrow, Nvidia would suddenly have a huge share of the tablet market within a few years, because people would keep buying iPads. As long as the hardware isn't disastrous, that's a pretty inelastic market. Apple has branding power well beyond what is only justified by the product itself, though they certainly make a ton of very high quality products.",hardware,2026-01-23 04:06:09,0
AMD,o12fbws,I think the current state of Windows is also helping.,hardware,2026-01-22 16:04:28,162
AMD,o122jxq,"> With the expectation of a lower-cost MacBook launching early this year, it could grow even further.  If they price it right. The iPhone 16e had huge potential at <$500, but they overpriced it at $600.",hardware,2026-01-22 15:04:39,28
AMD,o12ox3t,Low cost MacBook is definitely a recession indicator.,hardware,2026-01-22 16:47:14,10
AMD,o144we5,"The whole Intel ""smart sleep"" battery drain fiasco, I've become a very big Macbook fan recently. Nothing gets close to Apple's battery life and other integrations if you have an iPhone it's really sad that performance wise for most people it's all the same especially reaching that $1000+ price point but nothing gets close to just how much more polished Macs are because they aren't using Windows",hardware,2026-01-22 20:41:53,1
AMD,o12hjjh,last number i saw (many years ago) was 11% if i recall (maybe 10 or 12)  this was back in the intel days,hardware,2026-01-22 16:14:29,1
AMD,o16000k,I donâ€™t think the low price MacBook will be a thing anymore. The amount of money they save by using less silicon will be outweighed by the cost of dram + tsmc is increasing prices for apple dramatically.,hardware,2026-01-23 02:33:27,1
AMD,o11egrq,"The discounted M2/M3 Macbook Airs can be had for as low as $700, and at that price point there is no Windows laptop that can match it. Yes, Macbooks are actually *good value* now.  [$750 Laptop SHOWDOWN: Apple vs Intel vs AMD vs Qualcomm!](https://www.reddit.com/r/hardware/comments/1qk3uvx/750_laptop_showdown_apple_vs_intel_vs_amd_vs/)",hardware,2026-01-22 12:56:27,215
AMD,o11iacg,It's also what happens when AMD shows no interest in providing OEM's enough hardware to sell. I can walk into Best Buy and I have more choices in variations of M4 based Macbooks than I do AMD based laptops in total. I just spent a decent amount of time looking at various options when I was buying my Snapdragon based Surface Pro tablet and there's even more Snapdragon options than AMD at retail nowadays.,hardware,2026-01-22 13:19:10,50
AMD,o12b4ht,It's a combination of AMD not really chasing after marketshare because of muh margins and Windows becoming increasingly more and more enshittified. The latter being a far bigger issue that Microsoft still appears to be unconcerned with.,hardware,2026-01-22 15:45:25,7
AMD,o120aiq,Key word: sell  AMD makes some great mobile processors that for some reason you can't buy,hardware,2026-01-22 14:53:40,5
AMD,o11o70x,"I do enjoy when a company breaks a decades-old monopoly/duopoly by simply designing a quality product and seeing the development process through to production.   As an engineer, I strongly suspect a lot of the ""difficult"" industries are fairly easy to crack as long as you don't have constant flip-flopping from management.",hardware,2026-01-22 13:51:35,6
AMD,o1gtggv,"As a reminder: This is not a good thing.  While the PC is relatively open (minus the x86 problem), the Apple ecosystem is completely proprietary.",hardware,2026-01-24 18:10:45,1
AMD,o17gzgg,"Too bad Apple is such an anticonsumer shithole of a company, i really wish i could use their good hardware.",hardware,2026-01-23 08:53:24,0
AMD,o11i9jx,"Market share is the share of sales volume over a period of time (usually the period of time is annoying not specified), not the share of total devices in the wild.  Especially if market share has only increased recently, repairs will be biased toward older devices, which won't reflect recent purchasing trends.",hardware,2026-01-22 13:19:02,32
AMD,o11cmvo,Apple never cared to compete for market share. What they cared about is the profit margins on each unit sold. In light of this Macbooks are far more profitable than any Intel/ AMD laptops because of cost controls,hardware,2026-01-22 12:44:54,23
AMD,o11ad36,"This could be survival bias - it could be that the AMD ryzen based laptops either havent needed as many repairs yet, or the intel ones just have much more issues, or the people who are buying AMD ryzen laptops are more tech savvy and less likely to break their laptop to the point of needing to go to a repair shop.   Whatâ€™s the average age of laptops youâ€™re repairing? Come to think of it, I donâ€™t think Iâ€™ve ever taken my laptop to a repair place.",hardware,2026-01-22 12:29:49,39
AMD,o12fdkx,I hardly ever worked on AMD CPU's too when I worked at a repair shop. Most of them were Walmart Intel HP's and an odd Intel Dell now and again. If you're hardly working on AMD machines that can be a good thing and a bad thing.,hardware,2026-01-22 16:04:40,1
AMD,o144j5v,It's because there's not enough capacity for laptop CPUs they're in low stock,hardware,2026-01-22 20:40:10,1
AMD,o14qm6p,"AMD, Intel, and Apple all are not competing for old market share. They compete for new markets.   The new markets today are mostly in Asia.   And to an extent in the US/EU but the majority of the new growth is in Asia.   You can see this globally by observing what type of new steam games are being released. And by where the developer is located.Â    A lot of those new IP are Asian themed.Â    This is also why you see Qualcomm making a push into the PC space. They want to break out of the mobile space and into a new emerging market. So think India and China along with Vietnam and Indonesia.",hardware,2026-01-22 22:26:59,1
AMD,o12yjqe,Would that mean that AMD laptops break less often than Intel ones doe.,hardware,2026-01-22 17:30:56,-1
AMD,o11kqe7,"same thing in smartphones. Apple only sells like 20% of all smartphones annually, yet takes home more than 50% of the profit in the industry.",hardware,2026-01-22 13:32:54,42
AMD,o121hla,"They still manage to compete on the low end while preserving margins which makes them uniquely problematic for windows OEMs to compete with. Apple still manufactures and sells M2 and M3 laptops, and in some markets with some retailers even manufactures the M1 Air, all of the above sold at aggressive price points that are still profitable to Apple   There has been a lot of improvement in windows laptops in recent years so I think the increased competition is doing its job",hardware,2026-01-22 14:59:26,9
AMD,o12u0ku,>Even more impressive as they donâ€™t play in the low-end margin negative volume space.  I don't understand. I'm constantly seeing Macbook Air around â‚¬700,hardware,2026-01-22 17:10:22,2
AMD,o12knv5,"While it's probably part of it, I doubt it's the whole story. Windows has basically been the ""worse GUI that you put up with"" for 40 years, so it continuing to be that shouldn't be that dramatic.  I think a bigger explanation is just cost. While the entry price for MacBooks hasn't moved much on paper, the entry-level MacBooks throughout the 10's were generally not ideal, and you generally wanted to go up the product ladder to get a better laptop if you could afford it, even for general usage, and once you were looking at the laptop you actually wanted, you were spending $2000 or more. That's an inherently limited audience, simply due to cost.  These days, the cheapest MacBook you can buy is extremely solid and has no major asterisks. The price of ""I want a MacBook with a high-resolution screen, a processor that's as fast as the most premium processors on the market for tasks that only need a couple of cores, and has enough RAM/storage that I don't have to worry about it"" dropped by well over $1000 in the last six years, in addition to the massive technological upgrades in new hardware generations (e.g. Coffee Lake to M4).  Notably, there hasn't been a similar cost proposition improvement on the Windows side over the same time, dramatically shifting the relative cost proposition between MacBooks and Windows laptops. This creates a massive problem for Windows, as the whole reason to put up with the ""worse GUI that you put up with"" was generally cost, as boxes that ran Windows were generally meaningfully cheaper than Microsoft's competitors over the years (Mac, OS/2, early UNIX). With MacBooks being competitive or even favorable on cost compared to nice Windows laptops, the incentive to put up with Windows anyway is just gone.",hardware,2026-01-22 16:28:20,15
AMD,o11s23z,it's both. Windows is not good and AMD/Intel are crap vs Apple,hardware,2026-01-22 14:11:45,7
AMD,o13mq51,> and how much is attributable to the ongoing enshittification of Windows  zero. average windows user doesn't care or know about it,hardware,2026-01-22 19:18:09,4
AMD,o13dn0m,The M1 was leagues ahead for performance vs efficiency. The Strix Point and Lunar Lake chips that recently arrived at the first time weve been able to get Linux machines running in the same ballpark as the MacBooks.,hardware,2026-01-22 18:37:45,1
AMD,o15dxr1,"> egregious charges for RAM/storage upgrades  In fairness, PC OEMs charge similar amounts for upgrades. It's only significantly cheaper if you upgrade the RAM or SSD yourself.",hardware,2026-01-23 00:31:06,1
AMD,o15h2i6,The entshitification of Windows is a relatively new thing so probably not a lot.,hardware,2026-01-23 00:47:37,1
AMD,o12e9f8,"It's had been hovering around 15 percent for a long time, there wasn't a meaningful change in market share until the M series came out 5 years ago.",hardware,2026-01-22 15:59:35,6
AMD,o12eei1,"Apple was at 7-8% worldwide prior to the Apple Silicon switch. If you saw 20% 10-15 years ago, it was likely US-only data, as Apple has always held much higher marketshare concentration in 1st world markets.",hardware,2026-01-22 16:00:13,0
AMD,o120k2a,AMDâ€™s version of 14nm+++++++,hardware,2026-01-22 14:54:57,8
AMD,o11s51t,Windows laptops? I much prefer to work with actual Linux for the stuff we write. The compability layers needed for the same apps to run on MacOS doesnt always play nice... And then MacOS thinks I am stupid and that every single app that I write is a virus and I have to MANUALLY approve it to run it.,hardware,2026-01-22 14:12:11,4
AMD,o11syva,"They ""only"" lost around 20% in market share since Apple left them in 2020. Half of what you are citing though.",hardware,2026-01-22 14:16:27,10
AMD,o11jaqb,"Yes but not really, think its best to look at several research first.",hardware,2026-01-22 13:24:52,4
AMD,o11fswo,How come amd gaining marketshare on desktops? Intel still has similar enterprise relationships with them.  Or is it because amd offering on mobile havent been any special compared to Intel?   Or amd focused on margin makers & thats where their wafers are going?,hardware,2026-01-22 13:04:38,14
AMD,o11e5jw,>	Amd has had difficulties making gains in laptops (not due to consumer devices) because of intelâ€™s enterprise relationships which they are investing to counter rn and is at like 25% share  Tough shit for them? Why would this make them not comparable?  >	Apple ships over 200million units of their products each year  This is about laptops.   >	Itâ€™s almost like tpu is looking for a smaller number to compare appleâ€™s share to just to generate a clickbait headline.  Is there any clickbait here? They pretty much gave everything away in the headline.,hardware,2026-01-22 12:54:32,18
AMD,o17irxm,That is not why AMD is having difficulties. Its simply them not shipping enough hardware to OEMs. For some reason they just refuse to manufacture enough CPUs.,hardware,2026-01-23 09:10:06,1
AMD,o15i3ix,PC market (in particular laptops) shrinking? Source?,hardware,2026-01-23 00:53:10,5
AMD,o15i50m,"None, the average user doesnâ€™t know or care about it",hardware,2026-01-23 00:53:24,3
AMD,o15krxi,"You can dualboot all you want on a macbook (hence the existence of asahi linux), there's just not a lot to dualboot to.",hardware,2026-01-23 01:08:01,3
AMD,o11rtti,Apple (and now QC too) has +25/30% higher ST than AMD or Intel lol     The Max options are obviously much faster in every way than AMD chips,hardware,2026-01-22 14:10:34,17
AMD,o11zavh,>M chips have been out for a while but the avalanche of ARM never materialized.  I mean a big part of that is that there's no good desktop OS/software ecosystem option for arm other than MacOS so you end up in a chicken and egg situation. You can get a Win11 Arm machine but you're constantly going to be running x86 code through a translation layer losing all of your sweet arm efficiency to overhead because the vast majority of Windows software has no native arm build.,hardware,2026-01-22 14:48:49,3
AMD,o12pzxg,Apple is not an ARM evangelist and does not give a shit. They march to the beat of their own drum.,hardware,2026-01-22 16:52:03,-1
AMD,o12fw5f,">All of the top tier laptops don't use Ryzen, take a look at Thinkpad line for example.Â   Could also just be that Intel's mobile lineup is genuinely as good as, if not better with PTL coming out, than AMD's now.   >Intel is back to their old bullshit where they incentivise OEM'S to kneecap the competition.  Intel is cutting down on that type of spending, while AMD is investing more into partner relations. Intel really doesn't have the type of money to be incentivizing OEMs like they used to anyway.",hardware,2026-01-22 16:07:02,11
AMD,o17jbf9,The competition kneecaps themselves by releasing nothing new for 4 years.,hardware,2026-01-23 09:15:13,2
AMD,o17sjh0,"If you check the graphs, it shows that AMD market share has been slowly recovering to it's past peak of \~20% in 2022. Since the M1 came out, AMD's share has been relatively stable, while Intel is down \~15%.",hardware,2026-01-23 10:40:05,2
AMD,o12lkh8,"It's likely a combination of factors.  GenZ is overwhelmingly iPhone users. A Mac makes a lot more sense when you're already in the ""ecosystem"" and own Airpods and an iCloud subscription. Basically anyone shopping for their first college laptop will opt for a Mac unless they're on an extremely tight budget or require certain Windows apps (such as engineering students).  Windows is doing everything they can to make their OS worse, shoehorning AI into everything and trying to do away with the idea of actually owning your computer.  Users rely way less on Windows-specific apps these days. Honestly 90% of the ""apps"" I use today are web-based and work exactly the same whether I'm on Windows or MacOS. Aside from some cases like professionals, engineering students, and gamers, Mac does everything most users need.  The latter two things are less significant factors in my opinion. The big selling point for Macs is that they are competent machines for the money (unlike the Intel-based ones) and essentially sell themselves to iPhone users.",hardware,2026-01-22 16:32:21,99
AMD,o133o4u,"Have had a MacBook Pro M1 since launch, and a Studio. These devices are rock solid even 4 generations on I'm like ""will an upgrade even make a noticeable difference... hmm...""   In 4 decades of computing (about 2/3 Windows 1/3 Mac), outside of Commodore 64, I'd say the M1 Laptop has been the biggest game changer and most solid device ever owned.   Great battery life, thermals.       No shitty boot issues, random blue screens, overheating, is it waking, is it not, perfect display. Open the lid and it's there \*every time\*. Like \*every time\*.  So 1/2 is the hardware and 1/2 is the OS/Hardware combination. It's been on long hauls multiple times, stuffed in bags, dropped (oops).  It gets used for hours every single day when not at my desk. And battery life is it 83% (!!!!!!)   It's basically the perfect laptop.   (developer, built my own SaaS startup...  had run a company of nnn staff, god the issues with wintel laptops drove me batshit crazy)",hardware,2026-01-22 17:53:55,9
AMD,o12ptsu,"Even Linux is rising in desktop users, so obviously Microsoft dropped the ball big time.",hardware,2026-01-22 16:51:18,8
AMD,o14f61s,I've been daily driving Unbuntu because with Chat GPT I can now easily use Linux. I only use windows to game because I'm not ready to figure out the gaming end of things just yet.  Recently picked up an M1 MacBook Pro. It certainly has felt like a superior experience to Windows. I'm just not thrilled about stuff like how I cant upgrade my own ram or storage without knowing how to solder. That is bull shit.,hardware,2026-01-22 21:30:30,2
AMD,o13an5z,"yup, made me jump ship to a macbook air, 16h+ of battery life is amazing",hardware,2026-01-22 18:24:41,4
AMD,o1fxx3v,"Yep. Windows is getting worse, especially the laptops which further push the AI/Copilot nonsense.   Then the pricing. I remember when a thinkpad was half the price of a macbook pro. Now they're equal with Apple lowering prices, while windows laptops have somehow increased (without improving anything lol). The latest X1 thinkpads don't even let you install Linux lmfao.",hardware,2026-01-24 15:50:26,1
AMD,o1iywq0,"Microsoft, like many companies past 2008, fail upwards. due to government contracts that have insane margins.  Windows has been nothing short of unmitigated disaster for a good decade and half and they keep making more and more money.",hardware,2026-01-25 00:16:50,1
AMD,o12kwkr,"It wasn't overpriced compared to the iPhone 16. Now that the iPhone 17 has increased base storage and a 120Hz display it's a no-brainer. If I had to guess, it's not that Apple didn't have the foresight to see that coming, instead they just wanted to make a phone for corporate buyers that had a higher margin than the iPhone SE.",hardware,2026-01-22 16:29:23,6
AMD,o16ewpq,It's not uncommon to see battery life double after installing Linux on a Windows laptop. I have an older laptop that went from a 3.5 hour battery life to 16 hours.,hardware,2026-01-23 03:58:31,1
AMD,o11ic0c,"And they are cold to the touch and not hot. Well, unless you do transcoding",hardware,2026-01-22 13:19:25,71
AMD,o128ta6,"That theyâ€™re 16gb standard new is pretty crazy considering how well Macs manage their ram. They really do just need to make 512gb their standard storage as even w iCloud thereâ€™s inconsistencies w how syncing works for photos and messages that unnecessarily eats a ton of storage with no real workaround. Itâ€™s disappointing cause they clearly know how to do it when you look at how things are handled on iPhones for both those, meanwhile Iâ€™m sitting at 100gb used between those two apps on my m3.",hardware,2026-01-22 15:34:46,19
AMD,o129i5o,"Base model Macbooks are a good value if they meet your needs. You will not find a comparable laptop that has a similar keyboard, trackpad, processor, etc. for the price. But Apple makes their profit on the upgrades from the base model. So this would be in terms of ssd and ram. Those upgrades typically come at significant premiums and is where Apple gets its profit.   On a PC, you can typically make those upgrades yourself. And save a significant amount of money doing so. You can even upgrade your PC's monitor in certain cases. This can be useful if your laptop's screen gets broken. If you had a mac, you'd probably have to go to an Apple store and pay $$$ if that happened.   But obviously if you're not an enthusiast and would never do things like that then it's a moot point. So it comes down to what your background is and what your needs are.",hardware,2026-01-22 15:38:03,7
AMD,o12fm7m,"Macbooks, max minis,ac studios, iphone 17, apple watch se, ipad base being good value relative to other products is something I wouldn't have believed if you told me 6 years ago.   Apple offering the best value across multiple segments is just mental to think of",hardware,2026-01-22 16:05:46,4
AMD,o12lshz,"I mean, the pre touchbar macbook pro retina 13s werr good value too, if you dont need a gpu.",hardware,2026-01-22 16:33:20,1
AMD,o13qdn6,"> The discounted M2/M3 Macbook Airs can be had for as low as $700, and at that price point there is no Windows laptop that can match it. Yes, Macbooks are actually good value now.  Even more so now because of they're soldered ram. Business laptops are being scrapped for ssds and ram currently so the nonrepairability of apple products actually helps in the second hand market.",hardware,2026-01-22 19:34:44,1
AMD,o12wfb1,"sadly they come with Mac os, that's a deal breaker for me",hardware,2026-01-22 17:21:15,-1
AMD,o17i5qc,You american stop assuming that the rest of the world has cheap tech like in america. In rest of the world apple costs way more.,hardware,2026-01-23 09:04:15,0
AMD,o12j1us,that video is extremely biased ragebait,hardware,2026-01-22 16:21:14,-1
AMD,o11k7u9,They are never good value when comparing them fairly with equal ram and ssd as well as screens such as oled and 120hz.  Apple's base model devices are overpriced disposable plastic toys that run great for 3 years and then are garbage after that.  If this wasnt the case apple would have double the market share,hardware,2026-01-22 13:30:03,-59
AMD,o12yh6o,"> Yes, Macbooks are actually good value now.  Depends on what you consider a good value. 8GB Ram on base model Macbooks is fucking anemic even at $700. Say goodbye to that SSD soldered on board that can't be replaced when Swap eats it up.  Want to upgrade ram? Toss out the Macbook.   I can find $300 craptops that have longer lifetimes before becoming ewaste like that.",hardware,2026-01-22 17:30:36,-4
AMD,o12gtmb,"That's another way of saying MacBooks could never match a Windows laptop. The reason for this it's because they don't have the same feature set, I cannot replace my laptop with an RTX 5060 with a MacBook and expect the same support.Â      Basically you have to limit windows to only doing Mac OS stuff for your statement to be true.",hardware,2026-01-22 16:11:15,-9
AMD,o126upa,and the promise of Snapdragon enabling Windows laptops to catch up with Macbooks hasn't panned out.  The silicon isn't the only reason why Macbooks are good.,hardware,2026-01-22 15:25:21,7
AMD,o11nkm8,"I think this hugely depends on the region. Where I live, Snapdragon laptops are nowhere to be seen... I can, at most, see one model on display, and that is only in the bigger tech shops. AMD and Intel are split almost evenly otherwise.",hardware,2026-01-22 13:48:15,15
AMD,o11wm6x,"That's very regional, in places where I lived, AMD laptops are quite common. Snapdragon tho is non existent, Surface too",hardware,2026-01-22 14:35:17,10
AMD,o16icsh,Part of that is Apple buying up so much of TSMCâ€™s fab capacity.,hardware,2026-01-23 04:19:42,1
AMD,o11z4ii,>AMD shows no interest in providing OEM's enough hardware to sell  NO MONEY,hardware,2026-01-22 14:47:57,-2
AMD,o121afs,Which ones? Apart from Strix Halo (which are more expensive than Apple Silicon) no current AMD mobile cpu is competitive at the moment.,hardware,2026-01-22 14:58:29,8
AMD,o11vmkm,">As an engineer, I strongly suspect a lot of the ""difficult"" industries are fairly easy to crack as long as you don't have constant flip-flopping from management.  Such a naivety, Apple not only has huge people or cash resources but also different requirements than AMD and Intel  Apple sells end products with full control over them unlike those two",hardware,2026-01-22 14:30:08,33
AMD,o17h8yb,"When said company is anticonsumer and worse than the ones in the duopoly, is that really good?",hardware,2026-01-23 08:55:49,0
AMD,o12403g,"I suspect that the share of 'devices in the wild' (aka userbase share) of Macbooks are steadily increasing, since it seems most people buying new Macbooks are switching from Windows.  Microsoft should be alarmed, but they are too drunk on their AI hype booze at the moment.  Google sees the opportunity, and is looking to expand their presence in PCs beyond Chromebooks.  [https://www.androidauthority.com/aluminium-os-android-for-pcs-3619092/](https://www.androidauthority.com/aluminium-os-android-for-pcs-3619092/)",hardware,2026-01-22 15:11:41,14
AMD,o12rhb4,"if we're talking profit margins, intel and amd shouldnt even bother with mobile chips since datacenter margins are vastly better than any consumer market.",hardware,2026-01-22 16:58:42,6
AMD,o12fvmd,"> Apple never cared to compete for market share.  Reports of a Iphone mobile chip powered laptop line-up that is supposed to launch in the not so distant future, may indicate a shift in that strategy.  Also the Mac Mini was already a hugely successful market share play. It's an immaculate deal that barely no competitor can beat.",hardware,2026-01-22 16:06:58,3
AMD,o12ib8k,"higher market share = more people getting to snoop to the wall around their garden  which they like  its probably not the #1 priority, but no doubt that its an important aspect of their ""tactics""  pricing matters, when it comes to this, IMO  and the apple silicon is not cheap but its damn good value compared to intel/amd laptops at the same price points, as far as ive noticed - especially the macbook air  not just cpu performance, but the overall product you buy  its gotten a bit better since the M1, for the competitors though. their screens dont suck as much :D  edit the numbers could also be more iPhone owners who wanna utilize the walled garden options, and getting a macbook is a good way to do that. most people have a laptop anyway. And while most people are more familiar with windows... a ton of people only open their browser and a couple of other apps, might as well utilize the imessage functionality etc",hardware,2026-01-22 16:17:56,5
AMD,o11d3i5,"Laptop repairs are rarely anything to do with silicon these days. It's mostly power problems, the rest is physical damage, busted ports etc.  I don't think tech savvy comes into it, most people can't solder which is the #1 reason they end up on the bench.  Average age is around 3 years, about the oldest are the occasional 9th gen/2019ish era gaming laptops in since people still have and use these.",hardware,2026-01-22 12:47:49,51
AMD,o11c7gj,"Could also be that Intel market share has been falling, so there are a lot more old Intel Laptops and the average AMD is newer",hardware,2026-01-22 12:42:08,7
AMD,o11snmv,I think the most likely answer here is that AMD laptops are more likely to be budget products that get replaced rather than repaired.,hardware,2026-01-22 14:14:51,1
AMD,o11og4a,Good point. Itâ€™s exactly like that.,hardware,2026-01-22 13:52:55,3
AMD,o13msx1,"Putting it like that definitely doesn't make me want to be an Apple customer, and I'm sure Apple doesn't want me as a customer.",hardware,2026-01-22 19:18:29,-4
AMD,o122nke,Theyâ€™re still not competing with the mass volume drivers in the Â£200-Â£300 range. Thatâ€™s where most are sold. Iâ€™d say they have broken into mid-range with their well priced older models.,hardware,2026-01-22 15:05:09,5
AMD,o12vxpe,Thatâ€™s mid-range. Laptops get a lot cheaper than that.,hardware,2026-01-22 17:19:04,1
AMD,o15e4l7,"Microsoft did force Modern Sleep on every PC OEM though (and made AMD/Intel stop offering traditional S3 sleep functionality), which fundamentally broke a basic function of laptops that people expect to work.",hardware,2026-01-23 00:32:06,4
AMD,o13nyc3,I think the biggest thing is a lot of people use their phone for stuff now so  most shit has an iphone app now they aren't losing programs/software/apps from getting a mac for big screen shit unless they are gaming.  It's also  worse for potential windows arm  since they are hopping architectures and windows phone is long gone.,hardware,2026-01-22 19:23:39,3
AMD,o17i71r,Except windows GUI was the best for a long time until they decided to do Windows 8. At the time neither macs nor linux came close to the peak that was windows 7.,hardware,2026-01-23 09:04:35,0
AMD,o15hsvd,"Idle drain is still leagues apart, even lunar lake. And no possibility of fanless laptop for anything on amd/intel x86 roadmap.",hardware,2026-01-23 00:51:34,2
AMD,o12u22j,Could be based on 1st world data. Iâ€™m in Europe so either US or Europe might be my data point,hardware,2026-01-22 17:10:33,3
AMD,o12uxqc,I mean I don't understand. MacOs is around 10% compared to Windows. So what the article is saying is that Apple is selling the same number as AMD? If that's the case I don't find that very impressive.,hardware,2026-01-22 17:14:33,2
AMD,o13asix,"Rarely do developers get linux laptops as work devices. The issue is that Linux has extremely poor corporate management software support. Regardless of your personal thoughts on such software, without it your company's liability insurance either goes through the roof or people will refuse to offer it to begin with.   Macs and Windows are what's left, and macOS is nonetheless still a Unix compliant and certified OS, keeping its BSD ancestry.",hardware,2026-01-22 18:25:20,8
AMD,o13nu59,"You can just turn that off you know. Itâ€™s in the same settings screen you go to to allow an app to run, just deselect the verification thing.",hardware,2026-01-22 19:23:08,2
AMD,o11thdm,Thats actually nice that you get the chance to work with Linux. At our company they make us choose between Windows or MACOS no linux :(  Linux is better but I still prefer MacOS to windows for professional work much more!,hardware,2026-01-22 14:19:08,2
AMD,o13vpxs,2020 wow. Thought it was much further back. That is interesting.,hardware,2026-01-22 19:59:05,2
AMD,o11gv34,> Or amd focused on margin makers & thats where their wafers are going?  I think it's this one (also yes I agree strange article),hardware,2026-01-22 13:10:55,11
AMD,o11hc41,">How come amd gaining marketshare on desktops?  Because diy doesn't exist in mobile. 100% of the mobile sales are reliant on oems and a large portion of that is enterprise  >Or is it because amd offering on mobile havent been any special compared to Intel?  You had the ex ceo of msi goin on camera to claim that they will never use ryzen chips because that would be a betrayal of intel. They've reversed course slightly since then but that gives you an idea of what an oem pushed market is like  >Or amd focused on margin makers & thats where their wafers are going?  Amd ain't lacking wafers. Up until a few quarters ago tsmc's 5/4nm utilization wasn't at 100%. Reddit loves to parrot the story of amd not providing supplies of their mobile chips, the reality is that they just don't got many design wins because those are pushed by market dev funds. Dell wasn't even shipping amd enterprise options until late last year.",hardware,2026-01-22 13:13:41,13
AMD,o11i3pq,It is clickbait... From the graph it looks like apple started from 0%. When apple laptop shipments even with Intel used to be around 8-11%. Moreover its one data point dont know how much conclusion can be drawn from here...  Why not this one? [https://www.gartner.com/en/newsroom/press-releases/2026-1-20-gartner-says-worldwide-pc-shipments-increased-9-point-3-percent-in-fourth-quarter-of-2025-and-9-point-1-percent-for-the-full-year](https://www.gartner.com/en/newsroom/press-releases/2026-1-20-gartner-says-worldwide-pc-shipments-increased-9-point-3-percent-in-fourth-quarter-of-2025-and-9-point-1-percent-for-the-full-year)  Or Mercury research is just much more accurate than others?,hardware,2026-01-22 13:18:07,-4
AMD,o11grp4,> Tough shit for them? Why would this make them not comparable?  What meaning is there in comparing laptop sales of apple to the minority marketshare player? Would you write articles to compare nintendo switch sales to gpd win's?,hardware,2026-01-22 13:10:22,-12
AMD,o15wv5v,Thatâ€™s a volunteer project and it only sorta works through M2,hardware,2026-01-23 02:16:09,1
AMD,o17j543,"Well, theoretically yes, but half of the hardware wont work on asahi and you will hardly be able to do anything useful with it.",hardware,2026-01-23 09:13:33,1
AMD,o12doiw,"QC, unlike Apple, does not maintain similar ST performance across their stack.   For some reason they are going to sell 4 GHz X2 Elite chips with no boost. That's M3 level ST against the upcoming M5 Macbook Air.  They did the same thing in the previous gen with the 3.4 GHz X Elites, and that was quite embarrassing...",hardware,2026-01-22 15:57:00,1
AMD,o11xx6k,Too bad performance isnt the sole metric when deciding on CPU architecture. My point being Apple is far too small in the computing space to affect change like this. Even if their CPUs were 2x as fast. In enterprise Apple is basically non-existant.,hardware,2026-01-22 14:41:59,-10
AMD,o13e6t3,They also decreased the price of the base Macbook and increased the minimum RAM at a great time. You could have bought a 16GB M4 Air for $800 in many places this past fall and it was an insane deal. You still can buy a Mini for $400-500 with similar specs. I tried to recommend a reasonable Windows laptop in that price range to a friend who was Windows only and there literally wasnâ€™t anything close even accepting a few compromises.,hardware,2026-01-22 18:40:10,39
AMD,o12w4x9,"They could take a huge market share if gaming if they ever decided to support  Vulkan, but they are hell bent on pushing Metal it seems",hardware,2026-01-22 17:19:58,27
AMD,o16us5q,"Macs are just way better laptops. Even the cheapest Macbook is way nicer than most Windows laptops until you get to like the $2000 point, and they are much better truly portable devices with better standby and battery life. Windows laptops are all massive exercises in compromise and competing priorities.",hardware,2026-01-23 05:44:14,8
AMD,o15j7se,>GenZ is overwhelmingly iPhone users  Only in the US,hardware,2026-01-23 00:59:17,11
AMD,o14wl1i,"Also add in how long laptops last. In the 2000s and 2010s, you could expect maybe 4 years max out of a system, depending on its uses.  Nowadays, a nice Mac will run you more than 5 years, almost guaranteed (assuming your needs don't drastically change w.r.t. memory or storage). That's to say, people will pay more for something that is certainly updated well, doesn't have crap janky features that are ditched after a few years, and have fully performant hardware (versus many PC laptops which will often compromise less-noticeable bits of hardware, like mics, touchpad, speakers, etc).  I'm personally not a fan of them, myself, but I absolutely see why people buy them, and often recommend them to people when they fit their needs.",hardware,2026-01-22 23:00:25,3
AMD,o12pvbc,"If I were a non tech interested person and wanted a laptop then I'd get a Macbook without a doubt. I'm likely to never own one because I am happy with my workflow and usage with Bazzite, Linux Mint, and W11 with debloating, but Macbooks are ridiculously nice.",hardware,2026-01-22 16:51:29,10
AMD,o149lbf,"Also the  ""games"" you would play on a laptop MacOS still supports like Stardew, Sims, Minecraft, all my friends have a Windows PC for gaming and a Macbook for literally everything else including work.",hardware,2026-01-22 21:03:59,3
AMD,o1fyg83,Yeah 10 years ago a Macbook was only for rich kids. Now it's what I would buy 100%. They've lowered the price while improving the hardware significantly. Meanwhile windows has moved in the exact opposite direction!,hardware,2026-01-24 15:52:55,1
AMD,o13rdpe,">Â Open the lid and it's there *every time*. Like *every time*  This alone made me switch. Whatever Microsoft did to sleep on Windows totally fucked it.  Iâ€™d take my laptop out of my bag and the chance was near enough 100% that it woke itself up at some point and failed to sleep, causing intense overheating and a flat battery, andÂ none of the workarounds online actually fixed the issue.  Couldnâ€™t rely on it for shit. The day I ditched it was a day that should have happened sooner.",hardware,2026-01-22 19:39:18,12
AMD,o14lvvw,Everything just feels incredibly tight on the MacBooks if I had to give it a term. The old dell 700m (the 12incher) is probably the closest thing I had in PC that felt like this and even that had a moderate amount of jank. Iâ€™ll forever update my desktop but laptops see a more limited scope of use and I see no reason to go back to windows for it.,hardware,2026-01-22 22:03:09,5
AMD,o17gvnk,"> Even Linux is rising in desktop users  Not really. Almost all of the growth is from steamdeck, which is now majority of linux users on steam survey.",hardware,2026-01-23 08:52:24,5
AMD,o1ajlxo,"If I wanted a non-Windows OS I would resort to a Mac, otherwise Windows x86-64 is being used probably because of a specific application that doesn't work elsewhere  Windows is holding on because of their application support and Linux just isn't there yet",hardware,2026-01-23 19:22:49,2
AMD,o11kr10,"The most important thing is that they're dead quiet since they have no fan. M5 Air is now at the multicore performance of M1 Max, but at like 20% of the power so it makes no noise at all.",hardware,2026-01-22 13:33:00,83
AMD,o12qqlm,"Honestly I played a few hours of Hytale on my M2 Air *on battery* (100->62%) and it was barely warm, maybe 60C on the SoC.",hardware,2026-01-22 16:55:23,0
AMD,o12k3k6,The 256GB vs 512GB is the biggest sticking point for the Macbook Air in my book. My local Microcenter has the 512GB Air selling for $1080 while the base MBP is $1350. For an extra $270 you get a huge upgrade to the display and speakers along with a better battery and more ports. If the base storage on the MBA was 512GB it would be a much better sell vs the Pro.,hardware,2026-01-22 16:25:49,8
AMD,o12b9w8,"Yeah, RAM is now a non-issue on Macbooks.   16 GB base option is suitable for 95% of Macbook Air buyers, and the fact that it's non-upgradeable doesn't matter as much, since most â€‹Windows laptops also come with soldered RAM.",hardware,2026-01-22 15:46:07,13
AMD,o12bgfj,> That theyâ€™re 16gb standard new is pretty crazy considering how well Macs manage their ram.  I hear this all the time. Can someone actually show me quantifiable evidence or numbers for once?,hardware,2026-01-22 15:46:57,11
AMD,o15pm8s,"iCloud could be so much better.  But you have to remember, Apple is a hardware company that makes software for its hardware.",hardware,2026-01-23 01:35:29,1
AMD,o12cahe,It helps now that the Jony Ive era of design terrorism is finally over. Sales go up after. Coincidence?,hardware,2026-01-22 15:50:45,7
AMD,o13czsc,What are the issues you have with macOS?,hardware,2026-01-22 18:34:56,4
AMD,o15szcw,Iâ€™ve never heard anyone hate on Unix before. You actually prefer Windows and Powershell?,hardware,2026-01-23 01:54:30,1
AMD,o11n5j8,"You seem to not understand what a ""ultrabook"" is and who it is for.   And no, in terms of longevity MacBooks are pretty good.",hardware,2026-01-22 13:46:01,25
AMD,o11ti1x,>  plastic toys  Said about the company that famously only uses metal for their devices.   Methinks youâ€™re regurgitating circa 2008 conventional wisdom sir/madam.,hardware,2026-01-22 14:19:14,25
AMD,o11sxqc,>	Appleâ€™s base model devices are overpriced disposable plastic toys that run great for 3 years and then are garbage after that.  Are you stupid or do you want to explain where youâ€™re seeing plastic in an Air or a mini?,hardware,2026-01-22 14:16:18,22
AMD,o11nwbi,"Thatâ€™s ironic, most Apple laptops will work with their hardware for a lot longer than windows laptops.",hardware,2026-01-22 13:50:00,17
AMD,o11lm3g,"This is a garbage take.  Yes Apple laptops have lots of lower specs like 60hz,  and smaller SSDs. But they will smoke a windows laptop on day one and especially day 1000. No need to format a Mac every six months.  And battery life is ridiculous. So much better",hardware,2026-01-22 13:37:43,19
AMD,o12ascd,That's simply not true. When you buy a similarly specced Windows laptop it'll have a battery life of like 2-3 hours and it's fan noise will drive you mad.   For work I have top of the line spec Lenovo and Dell laptops and compared to my base model M1 Macbook Air they're way slower and the fan is constantly making noise.,hardware,2026-01-22 15:43:53,6
AMD,o12buj9,I can tell you have never used an M series Mac. Itâ€™s leagues better than whatever youâ€™re imagining,hardware,2026-01-22 15:48:43,1
AMD,o1270zc,Wrong. Only reason they're not higher is that not everyone (like me) wants to run the Apple ecosystem.,hardware,2026-01-22 15:26:13,0
AMD,o142lss,Is there any actual evidence that SSD swapping is having a major effect on the SSD's lifespan? M1 Macs have been on the market for over 5 years now and I haven't seen any strong evidence that it's a major issue even though 8GB models have been by far the best selling Macs every year until recently.,hardware,2026-01-22 20:31:05,6
AMD,o12bwte,"It's Windows itself that is the handicap. Qualcomm isn't going to be ""saving"" anybody here because as a company they're just as user-hostile as Microsoft.",hardware,2026-01-22 15:49:01,7
AMD,o17h63u,The silicon is the only good thing about macbooks.,hardware,2026-01-23 08:55:05,1
AMD,o15fsrz,You forgot â€œmuh TSMC supplyâ€ despite the fact that everyone else that uses TSMC (AMD doesnâ€™t even use the latest nodes) has no issues supplying the consumer market relative to AMDâ€™s volume.,hardware,2026-01-23 00:40:54,1
AMD,o12ia6h,"I wouldn't say AMD mobile silicon rn is uncompetitive. Might be worse wholistically, but there are genuine reasons to go for something like the HX 370 over the 285H in models that give that choice. AVX-512, better nT perf and nT perf/watt are all advantages that AMD still retains.   And for dGPU gaming laptops, X3D laptops still hold the gaming perf crown there. And Intel's battery life lead in that segment is, frankly, overstated IMO. Does it really matter how long battery life is on a 18 inch desktop replacement laptop?",hardware,2026-01-22 16:17:48,6
AMD,o11xua0,"Word, it only took one of the most valuable companies in the world to do so.",hardware,2026-01-22 14:41:35,26
AMD,o12qs3o,"apple only serves the mobile personal computing market as well so they build their products specifically for that. amd and intel serve desktops, servers, iot, embedded on top of mobile and need the product to work for all of that.",hardware,2026-01-22 16:55:34,5
AMD,o12gqgp,"I bought a m1 MBP during COVID and I was floored with how much better the battery life, performance, and build quality was compared to my T440p Thinkpad and my gaming MSI laptop with a top of the line i7 with a GTX 1660 Ti. MacOS really shines on laptops imo, no bullshit no forced restarts and not as buggy when compared to windows. The sleep mode works way better on MacOS compared to Intel machines too, I can go an entire month without using my MacBook and it would still have 60 percent battery life. My windows machines would be dead in a matter days. Having a long lasting battery in a laptop is exactly what you want if you're carry it around while traveling.",hardware,2026-01-22 16:10:51,8
AMD,o17hoth,"is the ""monitor hinge destroys screen power cable"" still a common issue like it used to be?",hardware,2026-01-23 08:59:51,2
AMD,o15b3s8,"The other major manufacturers sell phones at similar prices or even higher. The difference is that Apple focuses on the market segment that is very profitable with just a few products. They are also more vertically integrated than most companies, designing most of the hardware and software themselves has many advantages and less overhead. Much of that technology is utilised in their other product segments too (computers, Apple Watch, Apple TV, etc.).",hardware,2026-01-23 00:16:15,6
AMD,o15gkq6,>Apple doesn't want me as a customer  You are right,hardware,2026-01-23 00:45:01,2
AMD,o15h3j0,Why would anyone want to enter the Celeron/Athlon Indian education market tier though? Probably a lot of overhead with all that volume with almost 0 profits.,hardware,2026-01-23 00:47:46,6
AMD,o12miti,"Might just show how out of touch I am, but are <$400 laptops even big sellers these days? I can't imagine having a $300 budget for a laptop and getting some disposable Windows trash when you can get a MacBook Air with 16GB of RAM for $400.   I suppose they could be high-volume sellers, but with the margins they have it's not like they're a significant segment when it comes to profits.",hardware,2026-01-22 16:36:33,-1
AMD,o13sqd9,"Laptops don't, junk does. Macbook air is basically the cheapest new laptop anyone should buy. Below that you can get a used corporate laptop for cheap.",hardware,2026-01-22 19:45:27,2
AMD,o17ooq9,"Huh? Windows 7 was generally considered good by Windows standards, but it was still considered to be substantially behind OS X 10.6, which launched at around the same time, by just about anyone who used both at the time.",hardware,2026-01-23 10:05:26,2
AMD,o16z4s9,"We're having a bit of trouble with the M4 Airs thermally throttling under sustained load at work. I'd rather have a small fan that could spin to zero, TBH.",hardware,2026-01-23 06:17:48,2
AMD,o17il7q,US has always been a stronghold for apple.,hardware,2026-01-23 09:08:19,2
AMD,o13deoq,"Apple is selling the same number as AMD, that is impressive considering apples cheapest laptop is 1k",hardware,2026-01-22 18:36:44,3
AMD,o13e3tt,"I run the IT for a dev shop, Mac/Ubuntu fleet. Our insurer are surprisingly chill about precise tech details but we're very intense about risk management and controls, so have lots of paper to throw at them.",hardware,2026-01-22 18:39:48,5
AMD,o13oq2p,Security -> Allow applications from -> App Store | App Store & Known Developers   Those are the only two options and I still have to manually approve each new app with either setting on.,hardware,2026-01-22 19:27:09,4
AMD,o14xe72,Our developers don't even get to choose.  It's mac only for them.,hardware,2026-01-22 23:04:41,5
AMD,o11j0j3,If amd has 24% of desktop cpu share. Wait how much do you think diy constitues of the marketshare?,hardware,2026-01-22 13:23:18,3
AMD,o12k9l9,">	It is clickbaitâ€¦ From the graph it looks like apple started from 0%. When apple laptop shipments even with Intel used to be around 8-11%. Moreover its one data point dont know how much conclusion can be drawn from hereâ€¦  I think it starts from 0 because Appleâ€™s ARM-based laptops started from 0, so Intel share doesnâ€™t matter here. Also, that doesnâ€™t make it clickbait as youâ€™d have to have already clicked into the article to even see this data, hence couldnâ€™t be baiting you into clicking the article.   >	Why not this one? https://www.gartner.com/en/newsroom/press-releases/2026-1-20-gartner-says-worldwide-pc-shipments-increased-9-point-3-percent-in-fourth-quarter-of-2025-and-9-point-1-percent-for-the-full-year > >	Or Mercury research is just much more accurate than others?  Not necessarily that itâ€™s more accurate, the data you linked is just different from the one in the article.",hardware,2026-01-22 16:26:34,5
AMD,o11nsbl,"Minority marketshare player, AKA the second biggest marketshare player, thatâ€™s why. Apple is third and closing in on second.",hardware,2026-01-22 13:49:24,11
AMD,o15x45h,Exactly. Apple is in no way stopping anyone from dualbooting. They explicitly allow you to disable secure boot in a way that you can boost secondary OSes.   But there's nothing to dualboot _to_.,hardware,2026-01-23 02:17:32,2
AMD,o12j0pi,yes that is true but it's because Apple only does high end chips per TDP and QC has to do mid range chips as well,hardware,2026-01-22 16:21:05,5
AMD,o11zvzp,">Too bad performance isnt the sole metric when deciding on CPU architecture  Ok, Apple chips have better battery life and ST perf/watt curves too.   >My point being Apple is far too small in the computing space to affect change like this.  They literally did gain a bunch of market share though?   >Even if their CPUs were 2x as fast. In enterprise Apple is basically non-existant.  Doesn't seem like they needed enterprise to catch up to AMD in market share.",hardware,2026-01-22 14:51:42,13
AMD,o14gmvf,"If Apple holds their current pricing for a few months, they'll be dominating their competitors in RAM/storage aspects.",hardware,2026-01-22 21:37:36,14
AMD,o130srv,"It's a bummer because even if Metal is ""better"" it truly doesn't matter as long as developers refuse to implement it into their games.  Apple isn't going to spend a ton of money convincing developers to port their games to MacOS, so it would be nice to get at least some poorly-implemented version of Vulkan on MacOS so I could *at least* play my library of 10+ year-old games.",hardware,2026-01-22 17:41:08,20
AMD,o1jf9gq,"The current Mac's are all ARM CPU's, most every game has it's binaries compiled to x86-64 along with all the supporting libraries.  You would need Wine64 + some of the emulation shims they have built to translate the resulting calls to ARM64, this comes with a pretty obnoxious performance penalty.  Apple transitioned over to a custom proprietary, patented and lawyer protected uArch in order to enforce the walled garden and keep out hardware / software they didn't like being associated with their brand.  They aren't going to suddenly reverse that position.",hardware,2026-01-25 01:45:10,1
AMD,o18y57i,"I'd be willing to bet that if you overlaid two maps, one of iPhone market share, and one of premium laptop sales, they would be nearly the same maps.   And the trend of younger people gravitating toward iPhones more than their older peers likely exists in every country, even ones without a large iPhone market share.",hardware,2026-01-23 15:00:53,2
AMD,o17gs1l,Here in europe too unfortunately.,hardware,2026-01-23 08:51:28,1
AMD,o1732po,"This is not true, Canada, Australia, Japan, the UK and most of Scandinavia are all dominated by IOS.",hardware,2026-01-23 06:49:50,-1
AMD,o1946wo,"5 years is a conservative estimate if I'm being honest. The 8GB Apple Silicon Macs are definitely starting to show their age, but my M1 Pro still runs incredibly well for everything I need it to do.  I'm really interested to see how much longer Apple decides to support the M1 Macs, because there's really nothing holding them back at the moment. The only reason I wouldn't recommend buying one is because Apple tends to stop updating the OS 6-8 years after release.   The power of Apple silicon Macs is great at selling people on their first Mac, but really bad if Apple's goal is to convince people to upgrade their Macs.",hardware,2026-01-23 15:29:49,6
AMD,o13bpt6,"As someone that's tried out gaming laptops recently. I finally understand the amount of trade-offs we generally make when we care about specs and why the pricing is different   To get a really premium hardware experience for gaming, you do need to shell out a big amount of money. You can definitely get some cheaper gaming laptops that are solid. Don't get me wrong but the hardware is nothing compared to a macbook",hardware,2026-01-22 18:29:20,11
AMD,o141i0a,"Yes! As a fellow Bazzite user I love my desktop and I would never change, but for laptops I wouldn't touch anything other than a MacBook. The hardware is fantastic, and the OS... doesn't require fiddling (like Linux) or bodging (like Windows) to get it to do what you want, it just gets out of the way. Maybe I just have different expectations for a desktop vs a laptop in terms of capability, but I couldn't imagine using Windows on a laptop in 2026.",hardware,2026-01-22 20:25:56,4
AMD,o14joce,For $400 you could try an m1 MacBook Pro with 16GB of ram on eBay.  Pretty sweet deal if you ever need the basics on the go + any sort of editing (video or music) for a good deal,hardware,2026-01-22 21:52:19,1
AMD,o16en6w,I haven't used sleep on Windows since 7 lol,hardware,2026-01-23 03:56:54,2
AMD,o1215tb,"You don't even need a brand new Air to do that! I've been running my base model M1 Pro's fans locked in at max 1200rpm, which pretty much gets rid of them down to 0 while web browsing and such. And it pretty much runs like an iPad never heating up ever unless gaming or transcoding.",hardware,2026-01-22 14:57:52,18
AMD,o12iy0n,"Even my ""inefficient"" M1 Pro is cold almost all the time. The only time the fans ever turn on is in games or big exports. For 99.9% of my use it is silent.",hardware,2026-01-22 16:20:46,3
AMD,o12jd61,M1 Max you mean?  More interesting is the GPU side of things. M5 is 10x faster than M1 in some workloads like BLENDER.,hardware,2026-01-22 16:22:36,4
AMD,o12n70l,Huh? There is no M5 macbook air.,hardware,2026-01-22 16:39:32,1
AMD,o139oz3,\> If the base storage on the MBA was 512GB it would be a much better sell vs the Pro.  and that's how the Apple pricing ladder works.,hardware,2026-01-22 18:20:31,9
AMD,o13m28y,The majority of Windows Laptops we sell still have 8GB anyway.,hardware,2026-01-22 19:15:07,0
AMD,o13wkpm,"What?  I'm sitting here with 64G and 44G used.  Chrome, Slack, Discord, VS Code, terminal - nothing fancy.",hardware,2026-01-22 20:03:02,-4
AMD,o12tx5n,people that bought 8gb ram macbooks gaslighting themselves into not seeing the 26gb swapfile on their ssd,hardware,2026-01-22 17:09:57,9
AMD,o12gf5f,"Like with everything else discussing windows laptops it will be hard to get actual stats on this because so much of this depends on the hardware configuration the OEM or end user decides on for their device. Unlike Apple, where the parts are standardized resulting in predictable minimum levels of performance.    If you have ever had to support Windows laptops in an enterprise environment, most of the laptops are going to be commodity fleet laptops with bottom of the barrel SSDs and ram. Thatâ€™s what all of my companyâ€™s clients (and my company itself) use anyway for most users. The result of using these low quality parts is that rather than handling virtual memory appropriately, the computer will instead panic and BSOD. This was actually becoming such a productivity drain on my company that management approved a mandatory ram upgrade for all employees still on 8gig laptops back in 2023. I promise you this would not have happened nor been necessary on a MacBook with 8gigs of ram, soldered merit aside they use fast enough SSDs as a baseline that you never have to worry about swap crashing your PC",hardware,2026-01-22 16:09:26,2
AMD,o12h0yk,Someone else can answer in detail but itâ€™s just better memory management from the start. I believe tasks get deallocated and then moved to storage cache routinely.  Once you use it and keep the system monitor up itâ€™ll be pretty evident.,hardware,2026-01-22 16:12:10,1
AMD,o12itgx,"Yeah I haven't bothered to look it up and it's probably online but I would be curious how much ram a Macbook uses at idle.  I had (and returned) a Qualcomm Surface with 16gb that used like 11-12gb at idle and couldn't run Siege without stuttering because, contrary to what everyone parrots online, 'used' ram in task manager is explicitly not used for caching and is not able to be freed up (cached ram shows as 'free')",hardware,2026-01-22 16:20:12,-1
AMD,o17hs45,We are talking about value. Meaning value for money aka. Cost per year of snappy usage without heavy compromises.,hardware,2026-01-23 09:00:43,1
AMD,o17hvz6,U never seen made in china plastic toys that break after a few years?,hardware,2026-01-23 09:01:42,0
AMD,o17htqb,Like what?,hardware,2026-01-23 09:01:07,1
AMD,o11subv,I have a 2008 and a 2018 HP laptop and both of them still work just fine?    I can't put 11 on the 2008 HP because no AVX2 support on core 2 but it still runs just fine.     Can you even run current macos on non apple silicon?  Like what's the oldest macbook air that will run macos 26?,hardware,2026-01-22 14:15:48,-19
AMD,o11rm21,>No need to format a Mac every six months.   WhatYearDoYouThinkWe'reIn.jpg,hardware,2026-01-22 14:09:26,13
AMD,o17h8nh,But a laptop has a screen. That is literally one selling point of a laptop.,hardware,2026-01-23 08:55:45,1
AMD,o17ijbb,You have to format it because u will run out of storage and ram ðŸ˜‚,hardware,2026-01-23 09:07:49,1
AMD,o11s094,">This is a garbage take. Yes Apple laptops have lots of lower specs like 60hz, and smaller SSDs. But they will spoke a windows laptop on day one and especially day 1000. No need to format a Mac every six months. And battery life is ridiculous. So much better  My 2018 HP laptop still works fine.   It has never needed a reformat.  It was 1/4 the price of an apple.    What planet do you live on?  It isn't this one.",hardware,2026-01-22 14:11:29,-7
AMD,o17ia6n,The current latest generations of windows machines have improved battery life. This is outdated info,hardware,2026-01-23 09:05:25,1
AMD,o17ieym,"Until you hit full storage, ram and headache and eyesteain from 60hz",hardware,2026-01-23 09:06:39,0
AMD,o156w3g,https://news.ycombinator.com/item?id=26152161  It had been widely reported on that MacOS swap settings were killing SSDs prematurely.,hardware,2026-01-22 23:53:59,1
AMD,o165ief,they now have supply and are on the latest node (but that is for AI).  But the guy is mixing timeline. AMD has money only now. Not 2 years back for booking volumes. I can see they have increased volume now.,hardware,2026-01-23 03:03:33,1
AMD,o12j06a,">better nT perf and nT perf/watt are all advantages that AMD still retains.  That's not true anymore. Intel CPUs outperform than in basically every test.   AVX512 is the only advantage than AMD currenty has in that segment, everything else is better on current intel CPUs.",hardware,2026-01-22 16:21:01,-3
AMD,o12kb8w,who in their right mind would buy Strix Point over Panther Lake?,hardware,2026-01-22 16:26:46,-3
AMD,o15g9ue,AMD needs their monolithic mobile dies and LPDDR IMCs to serve servers?,hardware,2026-01-23 00:43:24,1
AMD,o1bnkag,wtf is a â€œTarget Audienceâ€,hardware,2026-01-23 22:31:42,2
AMD,o12vu9k,"I work in this field and I can say with confidence that the cheapest laptops represent the highest sale volume.  Best case, they are margin neutral. During a sale though, pure loss.",hardware,2026-01-22 17:18:38,6
AMD,o13g1sp,> when you can get a MacBook Air with 16GB of RAM for $400.  Where are you doing that?,hardware,2026-01-22 18:48:23,1
AMD,o15haw2,"Personal devices, absolutely.Â   But those are shipped en mass to markets like education sector in shithole countries. Big volume there. Almost zero money.",hardware,2026-01-23 00:48:51,0
AMD,o1bdjis,The opposite is true based on everyone that used both.,hardware,2026-01-23 21:43:26,-1
AMD,o13wbi4,Macbook Air have been discounted heavily and you can buy one MacBook air M2 for â‚¬700,hardware,2026-01-22 20:01:50,3
AMD,o17in9f,It just shows AMD is shipping even less hardware to the OEMs if anything.,hardware,2026-01-23 09:08:52,1
AMD,o13siho,"Try `sudo spctl â€”master-disable` in terminal (thatâ€™s two dashes, not an emdash btw)  Then go to security in setting and choose allow from anywhere and enter password.  It looks like they put the option to disable it behind a terminal command. Which is a minor PITA, but I can understand the reasoning.",hardware,2026-01-22 19:44:27,6
AMD,o11kuyo,"Amd ain't at 24% desktop, they're in the low 30s compared to intel. Diy is enough to make up at least 8% and that's with how much they have been destroying intel in diy. It really shows ya that diy is just a smaller but much louder minority and most volume come from prebuilts and enterprise  You can see here that they've kinda admitted that they didn't give enough ""funding"" to their partners and they are now doubling down on it to create a partner program that rivals intel's  https://www.linkedin.com/posts/amd_exclusive-amd-makes-big-channel-funding-activity-7366965061854138369-rA4h  You don't sell chips in enterprise, you sell solutions and without strong partner relationships you ain't getting anywhere. Same applies to enterprise servers which has been dragging down amd's epyc share. They're over 60% in cloud and hyperscalers but enterprise is dragging them down to the low 40s.",hardware,2026-01-22 13:33:36,3
AMD,o12msc8,"For my curiosity, I just wonder if Mercury is unique. Because maybe triangulation of different data points will help?",hardware,2026-01-22 16:37:43,1
AMD,o11sjdl,Thank you. I donâ€™t understand whatâ€™s hard at all. â€˜Appleâ€™s laptop share is below AMD-based laptops but theyâ€™re getting really closeâ€™ shouldnâ€™t be a hard thing to explain to someone but i guess AMD-protectionism is still a thing for some in this sub.,hardware,2026-01-22 14:14:14,2
AMD,o17j99w,you could boot to WoA if Apple only cared. Hackintoshes used to be promoted feature from apple.,hardware,2026-01-23 09:14:39,1
AMD,o142tqx,That point is moot since Qualcomm's chips end up going head-om against Apple anyway.  [https://www.reddit.com/r/hardware/comments/1qk3uvx/750\_laptop\_showdown\_apple\_vs\_intel\_vs\_amd\_vs/](https://www.reddit.com/r/hardware/comments/1qk3uvx/750_laptop_showdown_apple_vs_intel_vs_amd_vs/)  M4 is 60% faster than X Plus in Speedometer / Geekbench6 SC  I am sure we are in agreement that ST performance is is crucially important for client user experience.,hardware,2026-01-22 20:32:07,2
AMD,o14j11y,"Apple would have to bear a major profit fall if they keep their pricing the same, amidst the memory shortages.  Could be worth a gain in market share if it means gaining more on subscriptions, but Iâ€™m curious what apples move will be given the shortages",hardware,2026-01-22 21:49:13,9
AMD,o149g6z,"> Apple isn't going to spend a ton of money convincing developers to port their games to MacOS  Apple actually has been doing just that. Apparently quite a few mac/iOS ports of AAA games are entirely paid by Apple, it's the only reason they even got to the platform in the first place.  It's definitely completely unsustainable though and it doesn't sound like the sales figures are enough to convince publishers to fund the ports themselves.",hardware,2026-01-22 21:03:18,8
AMD,o17gkk2,"Based on the developers i talked to, Metal is very hard to work with and they hate it. Its going to suffer same issues as CryEngine. Its technically superior, but its hard to work with and documentation is trash so noone will use it.",hardware,2026-01-23 08:49:33,6
AMD,o1hm8yi,"Most of GenZ in Europe can't afford an iPhone, Android still has over 60% marketshare here. If you think things are different, you're probably just living in a rich place.",hardware,2026-01-24 20:17:24,1
AMD,o18xn7s,Oh so like... all the countries where people can also afford $1000 laptops?,hardware,2026-01-23 14:58:25,9
AMD,o1bp86q,"I definitely leaned conservative to avoid people saying ""my Mac died at 6 years or was too old or.."" lol  It was pretty clear immediately upon launch of the M1 that these new MacBooks are made to *last*",hardware,2026-01-23 22:40:05,5
AMD,o13m59v,"Yeah, my Asus tuf14 is decently built but it's moreso in the way where I feel like someone took care with it and that it uses better than bottom shelf parts. It was $900 or so. It can game decently well despite a 4050 mobile. The screen is decent as well.   But compared to my fiancees M4 air, it's night and day. She paid $900 and it is no doubt the better built and engineered machine. The screen is gorgeous and it's so thin and light.   I had a Lenovo slim laptop, I forget the model, but it had a beautiful OLED screen and definitely had a MacBook Air inspired look and size. I got it used for $600 when it was still pretty new, and while it felt great, it didn't have half the capabilities of a MacBook Air.   To get something like a MacBook Air with better gaming capabilities, I don't even know if that exists",hardware,2026-01-22 19:15:30,5
AMD,o14a26q,"Hardware is one, but I'm at the age where carrying a 7 lb hunk of plastic and metal isn't appealing anymore, and not looking for a charger is a massive plus. I ended up needing a Macbook pro because of the active cooling system but the air for students or just everyday needs would be so nice",hardware,2026-01-22 21:06:13,1
AMD,o145ivu,"I will say that I do not like using my fiancee's MacBook or iPhone, I find them to be super difficult to use when I try to help my fiancee with something. I know that is very much not the common experience, but something about them doesn't make sense to me. I do understand that most people do understand it and it makes sense to them, though.  My laptop is what I initially used Mint on, and now Bazzite. Valve really ushered in a new era of gaming options. I have had no issues just loading a game and it just working. It's so impressive. I keep Windows on it for Lightroom Editing, but everything else is Bazzite now.",hardware,2026-01-22 20:44:53,4
AMD,o1707ob,"1) What is bodging?  2) What would you be ""bodging"" about in Windows that would stand in the way of doing what you want?",hardware,2026-01-23 06:26:24,-1
AMD,o154cny,"I thought about getting a Mac mini, they seem to go on sale for crazy deals",hardware,2026-01-22 23:40:40,3
AMD,o122le5,"My M3 Max fans spin up only when rendering, compiling, transcoding or gaming i.e. heavy multicore work.   But the Air can do all of that silently.",hardware,2026-01-22 15:04:52,11
AMD,o13gzly,"There's an M5 on a Macbook Pro, true. Air's coming soon. I meant M4 Air. But if the M5 Pro performance is any indicator, it's gonna be decent.",hardware,2026-01-22 18:52:33,1
AMD,o14koso,"Might be real controversial, but they could just increase the base storage from 256GB to 512GB for the Airs and 512GB to 1TB for the Pros. It's been nearly 5 years since they last upped the base storage on either device.",hardware,2026-01-22 21:57:12,2
AMD,o144yi3,"If you have more RAM, the OS will cache more stuff because why not? Free RAM is wasted RAM. I would bet most of that 44GB is â€œjust in caseâ€ caches and could be purged without any noticeable performance impact.",hardware,2026-01-22 20:42:10,3
AMD,o1459gw,"Macs (and windows) use their ram, so if you have 64gb  itâ€™s gonna use a good portion by caching stuff to make usage a little faster. But you could use those same apps on 16gb of ram and be fine. Unused ram is wasted ram, so modern OSes actually use the ram at hand. Also, allocated ram is often reported as used ram.",hardware,2026-01-22 20:43:38,3
AMD,o141qvf,Or they don't care because it's not affecting their real-world usage. By contrast most 8GB Windows laptops quickly start running like crap.,hardware,2026-01-22 20:27:05,12
AMD,o137dtu,"> The result of using these low quality parts is that rather than handling virtual memory appropriately, the computer will instead panic and BSOD.  I don't understand this. Are you saying machines were swapping and crashing because you had a Kingston or Adata drive/RAM instead of say Micron or OEM?  > This was actually becoming such a productivity drain on my company that management approved a mandatory ram upgrade for all employees still on 8gig laptops back in 2023. I promise you this would not have happened nor been necessary on a MacBook with 8gigs of ram  How much of that is due to the end point security and other overhead usually loaded on enterprise machines? The 8GB also has to feed the iGPU.  I also don't view frequent and efficient use of swap as a positive but a sign of not having enough system memory to begin with.  Better yet show me stats and benchmarks not feelings.",hardware,2026-01-22 18:10:20,0
AMD,o17hwha,MacBooks are arguably even better then.,hardware,2026-01-23 09:01:50,1
AMD,o1237yu,There is a famous joke that HP stands for Hinge Problem...,hardware,2026-01-22 15:07:56,10
AMD,o121ila,>	Can you even run current macos on non apple silicon?  Like whatâ€™s the oldest macbook air that will run macos 26?  I love how you conveniently disregard the fact that there was a huge architectural change buried in there. We should be having this conversation with the M1 Macs are discontinued.,hardware,2026-01-22 14:59:34,15
AMD,o12bxe3,"Look mate, I get that alternatives to apple still have a much lower price floor and offer better value for some things like ram and storage but apple got its shit together and at the higher end there is no reason to buy anything other than a MacBook if you don't require windows and can tolerate MacOS.  They're just better than similarly priced windows laptops in almost every way, especially build quality and longevity. Some alternatives may exceed Macs in individual things like screen, performance or battery life but there's nothing that does everything excellently like Macs.  And yes, my Intel 7th gen dual core HP laptop from 2017 still runs well on windows 10 and Linux for most tasks (I still use it) but my Mac is much better at almost everything and the massive gulf in build quality does matter.  Even the oldest apple silicon Mac is currently supported on the latest MacOS and will get at least next year's feature updates and 3 years of security updates on top.",hardware,2026-01-22 15:49:05,1
AMD,o12b7hw,> Can you even run current macos on non apple silicon? Like what's the oldest macbook air that will run macos 26?  Why would you want to? Those are actual garbage. Apple silicon laptops have like 10x the performance and battery life.,hardware,2026-01-22 15:45:49,0
AMD,o12iqgf,"Thatâ€™s pre-Apple silicon, which isnâ€™t the relevant comparison.",hardware,2026-01-22 16:19:50,-2
AMD,o15igq8,https://www.ithinkdiff.com/apple-fixes-m1-mac-ssd-wear-macos-big-sur-11-4/   Apparently this was a reporting error bug and didn't actually reflect how much wear and tear was being put on them.  But what I'm really asking for is data showing actual widespread hardware failure. I wouldn't care how hard the SSDs are being pushed as long as they keep working for a long time.,hardware,2026-01-23 00:55:10,3
AMD,o16m55w,"Amd is super aggressively and rightfully so in the data centre. N2 Epyc & Instinct this year and likely continue to have performance domination over Xeon 7.Â   But their mobile (apu, mobile dGPU) is nothing but stagnation and horrendous OEM support.",hardware,2026-01-23 04:44:08,1
AMD,o12m3cc,">That's not true anymore. Intel CPUs outperform than in basically every test.  Search up the HX 370 nt perf/watt curve vs the 285H. AMD has a marginal lead. Here is an [example](https://cdn.mos.cms.futurecdn.net/DLzrQ2Kr6MFwqupZdLFiCm-1200-80.png.webp).   Intel's own [CES launch event](https://www.youtube.com/watch?v=JLwW8id3efE) (6:38) last year showed them matching the 365 in perf/watt, but the 365 has fewer cores enabled than the HX 370.",hardware,2026-01-22 16:34:39,7
AMD,o12mgd1,"Panther Lake isn't out yet. The ""launch event"" was ig, but no laptops in the publics hands yet.",hardware,2026-01-22 16:36:15,5
AMD,o15uua8,"the design of the core, internal bus and interconnects, io controllers, power management systems, etc are all on silicon and are identical if not extremely similar to every product up and down the stack. every product takes from these shared pieces, add in some product specific pieces like GPU bits or memory controller bits, put it through an HDL compiler, and out comes a silicon design.",hardware,2026-01-23 02:04:52,2
AMD,o13lqha,"Ebay, Walmart, etc  Check BAPCS. They post that model occasionally.",hardware,2026-01-22 19:13:38,0
AMD,o1blagv,"That's obviously not true. While Windows 7 was universally praised for much better performance than Vista, almost all reviews at the time still preferred OS X 10.6, and even Microsoft saw themselves as behind internally.",hardware,2026-01-23 22:20:20,2
AMD,o14hv1p,And you can buy an HP with an AMD processor for a quarter of that,hardware,2026-01-22 21:43:32,1
AMD,o1616gz,spctl disable was removed in sequoia i think,hardware,2026-01-23 02:39:56,1
AMD,o11n4xm,They cant get strong partner relationships because Intel provides better solutions? so they're focusing on other areas where the barrier to entry is lower to crack?  Or Intel is doing it illegally like during the 2000s? so amd basically cant compete there?,hardware,2026-01-22 13:45:55,-2
AMD,o12nii8,"I have no idea honestly, first time Iâ€™ve heard of them. I usually donâ€™t take these market share numbers seriously anyway.",hardware,2026-01-22 16:40:56,1
AMD,o19ncy5,"You mean bootcamp, not hackintosh. Apple definitely did not promote hackintoshes.   But yes, Apple use to promote Bootcamp as a feature. But bootcamp went well beyond ""supporting dual booting"" - Apple wrote drivers for Windows for that. If that's the standard by which a company ""supports dual booting"", no windows laptop or PC today supports dual booting.   None of them are personally writing drivers for their hardware in Linux.  Apple is not doing that today, but that's far from a simple task either. The simple task is to allow the user to unlock the bootloader and boot into other OSes. Which they do allow.",hardware,2026-01-23 16:55:59,4
AMD,o1441oq,"Yes it is, personally i wouldn't go for those SKUs if you are comparing Apple to Windows PCs  If you are comparing AMD,Intel and QC then it doesn't matter as those SKUs are still faster than Panther Lake and Zen 5     (i think QC will end up refreshing the X2P and X2 SKUs with the 4.7Ghz clocks next year or by end of year  so they can have a mid cycle refresh for the mid range)",hardware,2026-01-22 20:37:54,1
AMD,o159umy,"> Apple would have to bear a major profit fall if they keep their pricing the same, amidst the memory shortages. >  >   Not necessarily. Apple has very likely signed long-term contracts that lock in the price for a while.",hardware,2026-01-23 00:09:38,16
AMD,o15okco,"Perhaps, but it depends on what RAM contracts Apple has.  It's probably that they have long term (5 years) ironclad contracts that prevent the big 3 from trying any funny business with them unless they want to get demolished by Apple.",hardware,2026-01-23 01:29:26,3
AMD,o14mpst,"They've done a pretty good job porting games over that show what MacOS is capable of (such as Cyberpunk 2077), but a really bad job at porting over games that people actually play. EA, Activision, Ubisoft, and Epic all have no interest in porting games to MacOS. Esports titles should be an easy target for MacOS ports since they can basically run on a potato, but we don't see that happening. Most ported games are single-player, which is great for showing off hardware, but not great for convincing people to make a Mac replace their gaming PC.",hardware,2026-01-22 22:07:20,4
AMD,o14jib2,Yeah they just need to create a translation layer like Proton.  Playing your steam games should be hardware agnostic  Maybe theyâ€™re just inclined to wait and see if cloud gaming can become good enough and lower-latency for a practically identical experience to gaming with sufficient compute onsite.,hardware,2026-01-22 21:51:30,2
AMD,o1jfvz6,"Lol, the dude thinks less then a quarter of the worlds population is ""most"".  Like people don't even realize that India and China is over a third of the world population and that Indonesia is almost as much as the USA.  [https://en.wikipedia.org/wiki/List\_of\_countries\_and\_dependencies\_by\_population](https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population)",hardware,2026-01-25 01:48:35,-1
AMD,o13nbj2,"I completely agree. I tried the MSI Vector ($1299) and the acer predator neo Helios 16 ($1350 on sale) and acer predator neo helios 16s  ($1399 on sale)  There was nothing wrong with any of the laptops genuinely but you realize quickly how different it is from a premium experience where the trackpad just is haptic. The cooling is very different. It's slim. It's light. The battery life is insane. The screen has a mini LED screen. All metal chasis which dispels heat. Etc etc There is nothing wrong with these other laptops but you can definitely tell where they start budgeting for stuff to account for the stronger parts   I think most people that just think specs only have a point, but at the same time you're literally paying for better hardware. There's a reason why there's a lot of more expensive windows PCS now because there absolutely is a market for premium experience and hardware. That's not just pure internals",hardware,2026-01-22 19:20:47,6
AMD,o15fsi6,"The MacBook Airâ€™s hardware is capable of playing games, the Mac gaming side doesnâ€™t have developer support and Apple is hesitant to go all in like Windows did during the 2003s. But with the advent of AI, the game developers are forced to look after the Macs for ekeing out some profit and that can solve the chicken and egg problem.",hardware,2026-01-23 00:40:51,3
AMD,o14kf1k,"I saw a Lenovo Loq with a 4050 and a Ryzen 5 7000 series on Best Buyâ€™s website for $550 used.  Wouldâ€™ve been great, but I decided to go with an Intel 14600KF + RTX 2060 + 32GB of ram for $850 ($300 more)â€” I mainly just play Fortnite, hence the big cpu small GPU, if anyone was wondering. I mainly chose it because itâ€™d be way quieter with my noctua fans, big dual tower cooler, and I already had peripherals.",hardware,2026-01-22 21:55:54,2
AMD,o15bc6z,"A MacBook air is a pretty terrible comparison though.  It's passively cooled, which means performance drops off a cliff when it's under heavy load.  It's a fantastic machine, but a gaming machine it is not. Even if Apple suddenly provided Vulcan support it'd be really terrible performance.  That doesn't mean it wouldn't be perfect for a ton of old titles though.",hardware,2026-01-23 00:17:28,1
AMD,o1c65dr,"[Bodging is a Commonwealth word](https://en.wiktionary.org/wiki/bodge#Verb) that kinda means throwing things together haphazardly, making things work with blu-tack and duct tape. It's certainly not a negative, just like fiddling isn't a negative with Linux. It doesn't stand in the way per se, it just means you need to *do stuff* to get things to work the way you want, or at least the way I want.  One example is automation. On Linux you'd probably write a shell script to do whatever. Very close to the metal (well, pretty close to the metal for an OS), very powerful, but very fiddly. Not something you'd expect a newbie to be able to do, but that doesn't mean you can't do it. On Windows you might use AutoHotkey, which is an incredible program, but very much a bodge. A kinda non-native layer sitting on top of Windows pulling the strings, with a unique way of doing things. On macOS you'd use Automator, which exposes a lot of native application hooks and lets you use AppleScript (which is user friendly but very... odd in how it's written haha).  Fiddling > Bodging > Tweaking in terms of power. Tweaking > Bodging > Fiddling in terms of user friendliness. Personally, I want to be able to customise every tiny bit of my experience (Linux) or want things to just get out of my way and let me work (macOS), not something in between.",hardware,2026-01-24 00:09:51,2
AMD,o19uxa5,Yep Mac mini is more affordable if you donâ€™t need a laptop  Check eBay for an m1,hardware,2026-01-23 17:31:18,1
AMD,o13p3n6,"Oh, got it.   BTW the only SKU out is base M5, not M5 pro. So the Air is likely to be the same performance, around 2x M1 pro. So likely the M5 air will be the same CPU performance as the M1 max (which I assume is what you meant originally?)",hardware,2026-01-22 19:28:51,1
AMD,o16farh,"How about not charging an arm and a leg for storage upgrades at all, it should not be this expensive to get more space if you need it.  I wish they'd add 2 M.2 bays inside the Pro :(",hardware,2026-01-23 04:00:54,3
AMD,o16axxi,In this economy?,hardware,2026-01-23 03:35:00,1
AMD,o145ihx,"Is the ""cache"" you speak of not tracked?  All the detailed usage doesn't add up to memory used, so I guess that could be it.  I'm still using more than 32G of app memory alone though.  Memory Used: 44.2G  * App Memory: 33G * Wired Memory: 5G * Compressed: 1.5G   Cache Used: 20G  Swap Used: 0G",hardware,2026-01-22 20:44:50,-1
AMD,o1469sf,"Sure, it's 20G of cache, but that's labeled in Activity Monitor.  See [here](https:/reddit.com/r/hardware/comments/1qjsy7q/apple_silicon_approaches_amds_laptop_market_share/o145ihx/) for reported details.",hardware,2026-01-22 20:48:24,3
AMD,o189tf7,> not affecting their real-world usage  If swap isn't affecting their usage then they don't have any usage tbh,hardware,2026-01-23 12:50:19,0
AMD,o17iqbz,For equivalent specs u get with windows laptops for 1000 usd equivalent macbooks cost 2000 usd. Value for money my arse,hardware,2026-01-23 09:09:40,-2
AMD,o12pyyv,">I love how you conveniently disregard the fact that there was a huge architectural change buried in there. We should be having this conversation with the M1 Macs are discontinued.  What?  That is literally my point.   How can OP say they last longer when we don't know that yet with current ones, and old ones can't run modern macOS?      Meanwhile X86 having only minor changes means that there are a handful of nearly two decade old machines capable of nicely running 10, though 11 sets the bar much higher due to TPM and AVX2 requirements.    But even that much higher bar is older than the oldest macs that can currently run 26.",hardware,2026-01-22 16:51:56,0
AMD,o12tcdq,"And that conversation will be much worse for Apple, all things considered. A 2015 MacBook Pro is still a very usable machine...since you can install Linux on it very easily and have full working driver support. Asahi, as admirable a project it is, is not there yet. M1's might get stuck on macOS 27 or 28, and with no good Linux support will be destined for the e-waste heap once developers stop releasing new updates or updated root CAs for those versions.   Even the T2 Macs have a bunch of special quirks you need to work around to get Linux installed and working right, since so much is handled by custom silicon.   *And* Apple Silicon also has to phone home to Apple to ""activate"". Asahi still needs a stub macOS installation.",hardware,2026-01-22 17:07:17,-2
AMD,o12pi8i,">Why would you want to?  Because OP wrote:  >Thatâ€™s ironic, most Apple laptops will work with their hardware for a lot longer than windows laptops.  So how can you say ""oh, they last longer"" but also ""no, why would you want to use them longer?""",hardware,2026-01-22 16:49:50,0
AMD,o12p8wg,"So how can we say they last longer when, in reality, none have had that ability so far?",hardware,2026-01-22 16:48:41,2
AMD,o1i20kd,"The reviews should be airing this weekend or Monday, when some sales are supposed to start. Very Soon^TM .",hardware,2026-01-24 21:32:10,1
AMD,o161g6y,I just did it on a machine on 26.2,hardware,2026-01-23 02:41:25,2
AMD,o12unk3,"AMD themselves quote mercury research numbers in financial presentations/earnings results, I forget which.   But they seem pretty legit.",hardware,2026-01-22 17:13:16,2
AMD,o1bhvbw,"If you arent writing drivers for your own hardware, then you do not support dualbooting flat out.",hardware,2026-01-23 22:03:40,-1
AMD,o19vbio,Ah true  Maybe now people will just be more likely to pay for memory upgrades than in the past.  Appleâ€™s pricing was simply prophetic ðŸ˜‰,hardware,2026-01-23 17:33:05,2
AMD,o14q6ph,"There already are translation layers like MoltenVK, but Apple doing it themselves? I doubt it, just because it would be an admission that their platform isn't inherently just so attractive to game developers.   The best we'll see is their current ""game porting"" toolkit, which has an emulation layer of sorts for Windows executables but is specifically developer-only so you can't use it to distribute your game, only to test drive it before porting manually.",hardware,2026-01-22 22:24:51,5
AMD,o17gqoh,"> Playing your steam games should be hardware agnostic  Considering how many steam games use OS level calls, they cannot be OS agnostic.",hardware,2026-01-23 08:51:07,2
AMD,o1kf53k,"Yes and an overwhelming majority of Indians and Indonesians canâ€™t afford $1000 laptops. About 15% of Indian household have a computer, and I am gonna take a wild guess that those computers arenâ€™t brand new MacBooks.  A country having a huge population doesnâ€™t necessarily mean it has a huge consumer electronics market.",hardware,2026-01-25 05:15:31,1
AMD,o143vtn,"I think what really has shifted, at least my opinion, is that Apple has not only lowered pricing and upgraded the base models in a few ways, but the M series chips are really something special. Like I said, I'm 100% a Windows/Linux/Android guy, I don't like using my fiancee's Macbook or iPhone, but I respect the hardware very much.  So they've gotten more cost-effective, better specs on the lower-end units, and are powered by great chips. It's a no-brainer from someone who isn't married to Windows.",hardware,2026-01-22 20:37:07,5
AMD,o15p8w5,"Yeah it can play games and some games have native releases, it's just not going to be super capable due to the os. I'm not sure the capabilities it has via translation layers like wine or whatever they do now. I've heard that retro emulation isn't great though",hardware,2026-01-23 01:33:20,0
AMD,o15pupw,"Sure yeah a different configuration with that chip with active cooling would be better, but I just don't even know what they can do via emulation/translation layers, I know that retro emulation isn't great on them in general though",hardware,2026-01-23 01:36:50,1
AMD,o13s8ip,"Yeah i meant the Macbook pro with an M5, Apple really like the word Pro",hardware,2026-01-22 19:43:12,2
AMD,o17jsbp,"No, not really.",hardware,2026-01-23 09:19:44,1
AMD,o12ptk4,"Well for one, Apple has a long history of providing software support for much longer than its competitors.",hardware,2026-01-22 16:51:16,-4
AMD,o17ivp5,"Companies site different researchers all the time though. Are they all legit, if they are shouldnt we mean? For example: [https://newsroom.intel.com/artificial-intelligence/idc-analysis-intel-expands-market-opportunities](https://newsroom.intel.com/artificial-intelligence/idc-analysis-intel-expands-market-opportunities)",hardware,2026-01-23 09:11:05,1
AMD,o16857b,"Apple's Game Porting Toolkit literally *is* that kind of translation layer, developed by Apple on top of the same Wine project that Valve's Proton is based on. They're just not shipping it in a user-friendly way, leaving it up to third parties like Crossover to package it all up into something somewhat usable. But the ideal thing for end users would be for that functionality to be integrated into Steam and used automatically and transparently, rather than requiring the user to go through a separate tool like Crossover.",hardware,2026-01-23 03:18:38,5
AMD,o19vjoi,Sad!!  SteamOS works pretty well tho,hardware,2026-01-23 17:34:09,1
AMD,o14548r,"Yes! The laptops are now very affordable for some of them and in line with several premium options for windows laptops .   I'm a big gamer so I won't usually buy a Mac for my own use, although I use them for work. But if I didn't there's no way I would get something like the MSI vector. That thing is a fucking brick ðŸ˜‚.  Battery life on neo helios 16s is trash",hardware,2026-01-22 20:42:56,3
AMD,o15fzc5,"Yep, you can like the hardware and hate the software.",hardware,2026-01-23 00:41:51,3
AMD,o15mym9,"Dude Iâ€™m usually the typical anti-apple guy (only cuz of their scummy practices and â€œeliteâ€ status in my home country) but I gotta admit apple has killed it with their proprietary silicon. Like I get it, itâ€™s not AS powerful as a dedicated GPU set up but my lord even M1 was a fantastic chip. Like it really is a wonder they had such running start with their M series chip I thought it would have taken at least a couple of generations to get really good.",hardware,2026-01-23 01:20:18,3
AMD,o1929nm,"Yeah, I hope it gets better",hardware,2026-01-23 15:20:54,3
AMD,o146wt6,The Macbook Pro Max Ultra Deluxe Edition!,hardware,2026-01-22 20:51:24,2
AMD,o12qnhh,">Thatâ€™s pre-Apple silicon, which isnâ€™t the relevant comparison.",hardware,2026-01-22 16:55:00,1
AMD,o17hjai,What does software mean when your screen fries your eyes from 60hz and u have headache and your 256 storage is full in a year or two and your ram also clogged swapping to your lower speed 256gb ssd of which a quarter is just the os and basic software plus the storage companies lie and u always get less storage than advertised?,hardware,2026-01-23 08:58:27,0
AMD,o16dgnj,"Yes, that's what I said? Apple doing a layer like Proton (so, *user-facing*) is very unlikely, even though they already have most of the tech.",hardware,2026-01-23 03:49:44,0
AMD,o1bi96d,SteamOS simply translates it to react how the game expects windows to. But then SteamOS does not actually work for a lot of games.,hardware,2026-01-23 22:05:30,1
AMD,o146qm1,"Do you do a lot of mobile gaming? I'm guessing that is the case if you're looking into laptops for it. I do some mobile, and my ASUS is good enough for some WoW, Hades 2, and Divinity Original Sin 2. I typically am playing a Retroid Pocket 5 on the go or my Steam Deck, but I don't usually have long gaming sessions on the go to justify a stronger laptop.",hardware,2026-01-22 20:50:35,1
AMD,o16jqyz,"They had a bunch of generations before M1, they were just in iOS devices, where they dominated Qualcomm for years before the M-series launched. M1 has four ""Firestorm""(P) cores and four ""Icestorm"" (E) cores, first seen on the A14 Bionic (but in a 2+4 config). The A-series debuted in 2010.",hardware,2026-01-23 04:28:34,4
AMD,o15o8kn,"Yeah fully agreed, it's really impressive!",hardware,2026-01-23 01:27:35,2
AMD,o12wyrm,Weâ€™re five years into Apple Silicon and the M1s are both still receiving full support and outperforming systems released well after them.   A MacBook Air remains the best value laptop you can buy today.,hardware,2026-01-22 17:23:42,-1
AMD,nzcnd0i,just run an 9800x3d at 5.6ghz and u know how this cpu performs...,hardware,2026-01-13 13:46:30,94
AMD,nzcl9w0,"Who even needs that? Cool, a slightly faster X3D CPU.    Everyone will say:   \-It might be 1-7% faster in some scenarios (testing in 720p/1080p)   \-There is no reason to upgrade from the 9800X3D   \-Something something intel, remember intel has done similar things.   \-It's too expensive right now because the msrp is higher than current market prices for the 9800X3D.   Long story short: It's a refresh, once it's abut the same price/cheaper than the 9800X3D, it's the better opinion/ That's all you need to know.    But influencers will milk it, like they always do. Expect clickbait titles for weeks/months.",hardware,2026-01-13 13:34:51,46
AMD,nzct8af,Aren't we all excited to see if it's 2% or 3% faster?,hardware,2026-01-13 14:17:52,14
AMD,nzdtm07,What has changed? Why not explain it?,hardware,2026-01-13 17:22:49,2
AMD,nzctlg8,i don't get a review embargo of a product that's essentially a refrest of an existing one.,hardware,2026-01-13 14:19:47,7
AMD,nzgjkbl,"You know what man, fuck it why not",hardware,2026-01-14 01:16:21,3
AMD,nzcpoe3,Steve's going to be standing up on this one.,hardware,2026-01-13 13:59:01,11
AMD,nzcrenm,I already have a 9850x3D  Itâ€™s my PBOd 9800x3d  Thing is a beast,hardware,2026-01-13 14:08:16,5
AMD,nze3o81,This will be the ultimate CPU for 480i low settings with an OCed RTX 5090.   Look at that whole 6% uplift over the slow 9800x3D. Complete domination.,hardware,2026-01-13 18:08:52,2
AMD,nzg1vsm,Is it going to blow up on motherboards? Is voltage sensitivity still an issue ???,hardware,2026-01-13 23:39:40,2
AMD,nzdct62,"Got a 13700k and very much looking forward to this thing, assuming the price is reasonable. Otherwise, will just get the 9800x3D.",hardware,2026-01-13 15:54:20,2
AMD,nzci13p,"Hello Jumpinghoops46! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-13 13:15:55,1
AMD,nzgtjmw,"Yaawwwwn. AMD, I know you're not listening, but if you were to, no one cares. Just release the damn thing and get your money.   9950X3D-2? Now that would be something to get some attention. 6 months ago.",hardware,2026-01-14 02:13:08,1
AMD,nzdzywr,"*""yawn....""* Wake me when ZEN 6 is announced...",hardware,2026-01-13 17:52:22,0
AMD,nzdszu6,I'm even more excited for this than I was for the 5800XT,hardware,2026-01-13 17:19:51,0
AMD,nzk4qud,"These people are just ridiculous. This has a bit more L3 and nothing else relevant, but they are treating it as some kind of major launch. I'm so sick of these companies.",hardware,2026-01-14 16:03:25,-2
AMD,nzcof7w,Exactly  And most 9800X3D are capable of doing that without problems,hardware,2026-01-13 13:52:15,24
AMD,nze98e7,Yes it's equivalent to the Intel KZ line. Just run a K processor at KZ clocks to know how the KZ performs. Problem is not all K chips can reliably hit those clocks.,hardware,2026-01-13 18:33:07,1
AMD,nzcnrwq,"In ye olden days the answer would be, it's an extra push for people on older platforms to upgrade. But with current RAM prices even that is not really a market.  On the other hand, it's not like there's any significant R&D in this thing, it's just a better bin. Releasing it might be a ""why not?"" type decision. As is the decision for influencers to cover it",hardware,2026-01-13 13:48:45,19
AMD,nzcwne5,"It's not a refresh, it's just better binning.  AMD has likely been stockpiling these better dies for a while.",hardware,2026-01-13 14:35:35,17
AMD,nzecx5o,"> -There is no reason to upgrade from the 9800X3D  Not to upgrade, but if you're building a new PC, might as well get a better one.",hardware,2026-01-13 18:49:26,7
AMD,nzgtyzy,"Right, the Intel ""KS"" SKUs. Or Extreme, in the past.   Almost never a reason to buy them other than ""Oh no, I can't hold all these bundles of $100 bills!"" kind of money and bragging rights.",hardware,2026-01-14 02:15:31,2
AMD,nzi356d,"I do data analysis for a living and am CPU bottlenecked. CPU improvements translate directly to productivity at work for me. 7% different isnt big enough for me to spend that much money, but im sure there are people for whom it will be.",hardware,2026-01-14 07:30:31,1
AMD,nzmheun,ive been sourcing parts for a new build over the last few months and was looking at getting the 9800. but might aswell wait for this slightly better one abit. if its not like 600$ and upwards i wouldnt mind waiting and paying a little extra.,hardware,2026-01-14 22:25:58,1
AMD,nzuywff,"if you already have a good x3d cpu, this release isnt really aimed towards you. as someone with a R7 7700X, im hoping the 9800x3d price goes down because of this",hardware,2026-01-16 03:59:02,1
AMD,o0723wz,Not everyone has a 9800x3d though,hardware,2026-01-17 23:35:05,1
AMD,nze9t4f,"It'll be the fastest gaming CPU (faster than base 9800X3D) on the planet, who ever wants that will need that.",hardware,2026-01-13 18:35:40,1
AMD,nzcn07g,"I see your point.   I've been holding out for a refreshed 9800, so this will hopefully fit the bill if the performance matches the price point. I may just go for a 9800 since it's price will drop (hopefully).",hardware,2026-01-13 13:44:32,0
AMD,nzcrjzb,This this this. 100% this.   All the YouTubers just got new content for a few weeks. Sooooo happy for them.,hardware,2026-01-13 14:09:04,-2
AMD,nzdptpi,More like Zen 5%,hardware,2026-01-13 17:01:34,13
AMD,nzeyk8d,At 1080p*,hardware,2026-01-13 20:28:32,1
AMD,nze27kv,You must be new to this hobby,hardware,2026-01-13 18:02:19,4
AMD,nzcwrl1,"Oh no, I'm sure AMD is shaking in their seats RN!",hardware,2026-01-13 14:36:11,10
AMD,nzd6nk4,Why?,hardware,2026-01-13 15:25:31,2
AMD,nzd6nth,You're running max 5.45ghz on that. The 9850x3d will do 5.6ghz out of the box and probably 5.8ghz with an oc. Not that that would change that much,hardware,2026-01-13 15:25:33,7
AMD,nzctxoa,9850x3d is a double pbo basically yeah,hardware,2026-01-13 14:21:34,4
AMD,nzi3vyb,"You mena ultimate CPU for anyone playing strategy/sim/MMO genres, where you are CPU bottlenecked even at 4k?",hardware,2026-01-14 07:37:25,3
AMD,nze6snz,"benchmarking nerds will love it though (as long as the memory controller is also binned), then they will ditch it immediately when the next big thing is released, like the hypothetical 9950X3D v2.",hardware,2026-01-13 18:22:31,0
AMD,nzjeypg,"9950X3D2 so that you can have completely bottlenecked performance between the CCDs? Wow, sign me up!",hardware,2026-01-14 13:55:20,1
AMD,nzd6dju,"Thats not exactly true, you need a motherboard with eclk support. Running static 5.6ghz needs a golden sample",hardware,2026-01-13 15:24:11,28
AMD,nzcrs4g,I'm not against it or anything.    If anything I'm generally supportive of releasing refreshes even though most of them were perceived poorly by at least the tech bubbles.   I just think that how they're presented by influencers hurts more than anything.,hardware,2026-01-13 14:10:16,7
AMD,nze65bf,is the memory controller also a better binning ?? Imagine selling your 9800X3D capable of doing 6400 1:1 C28 FCLK 2133-2200 with tight timings and getting a 9850X3D stuck at 6000 CL30.,hardware,2026-01-13 18:19:43,-1
AMD,nzk6cp0,"Yeah, these companies don't care at all about reviews, that's why NVIDIA had a scandal where they were trying to control their reviews. You are such a big brain genius with you dismissiveness.",hardware,2026-01-14 16:10:42,1
AMD,nzgulns,Probably cause it's going to cost 20% more and be 2% faster.,hardware,2026-01-14 02:19:03,3
AMD,nzd8h5k,Yeah Iâ€™m good. But Iâ€™m sure there will be plenty of degens with stupid money that go spend $500 to upgrade their 9800,hardware,2026-01-13 15:34:15,1
AMD,nzkpoxr,You mean the games which are least sensitive to frame rate differences?,hardware,2026-01-14 17:38:30,-1
AMD,nzdf2dk,"what.. not at all with 9800x3d it is easy peasy. u just set the multiplier and it will be locked, that is if u are have unlocked all the powerlimits and u dont have lost the silicon lottery.  have had 3 9800x3d now. all of them ran at 5.6 locked.",hardware,2026-01-13 16:04:39,-5
AMD,nzduxwc,>Running static 5.6ghz needs a golden sample  Nope,hardware,2026-01-13 17:29:06,-1
AMD,nzcss86,Fair enough,hardware,2026-01-13 14:15:30,3
AMD,nzeo6im,"> 6400 1:1   Are the 9800x3d better at this? I have a 7800x3d and didn't want the headache of testing this again. Mind you, I don't care about IF above 2000 nor lower CAS (28).",hardware,2026-01-13 19:40:20,3
AMD,nzi3rim,sounds normal for halo products.,hardware,2026-01-14 07:36:15,-1
AMD,nzi5ceo,I have a 7700x so it seems kinda cool?,hardware,2026-01-14 07:50:59,2
AMD,nzltjeo,"Hey now, I need every iota of single thread performance for my speed 5 EU4 games once the league war drops.",hardware,2026-01-14 20:37:19,3
AMD,nzowgrr,"Tell me you never had large party raids in MMOs if you dont know how framerate sensitive they are. But yes, they are less sensntive. Does not mean even turn based games dont look great running smoothly at high framerates.",hardware,2026-01-15 07:32:36,1
AMD,nzdgygt,> golden sample  > u dont have lost the silicon lottery   Youâ€™re reaffirming the comment youâ€™re replying to.,hardware,2026-01-13 16:13:20,25
AMD,nzepc43,"it's still silicon lottery, no 9800X3D is guaranteed to do better than 6000 CL30 1:1 FCLK 2000, but some (many ?) can do 6400 1:1 with tight timings, here's my setup: [https://i.imgur.com/UTA1CHL.png](https://i.imgur.com/UTA1CHL.png)  now imagine I sell my 9800X3D with its great memory controller just because I want the ""latest and shiniest"" 9850X3D and then I end up stuck at 6000 1:1 (1% low fps would be impacted, but also helped by the increased clock speed... so they will cancel each other), that's why I was wondering if they're also binning the memory controller for the 9850X3D or they're just binning the cores.",hardware,2026-01-13 19:45:36,1
AMD,nzh0aog,"How is a refresh and better binning any different?  Downvoted for being right: A refresh implies the same tape-out. It's just improved binning after manufacturing yields have improved. It's not ""new"" silicon any more than buying a brand new 9800X3D today is also ""newer"" silicon than a launch day 9800X3D",hardware,2026-01-14 02:51:04,-1
AMD,nzjeq6j,Didn't you know everyone in this subreddit is already running 9800X3Ds and this is a stupid product?   /s,hardware,2026-01-14 13:54:03,1
AMD,nzowj9j,EU4 endgame is really something.,hardware,2026-01-15 07:33:14,2
AMD,nzrv8ze,"Fair. I have a life. Never had 1,000+ extra hours to pour into a title when that time could go to literally anything else - sleep, socializing, career advancement, fitness...",hardware,2026-01-15 18:31:07,1
AMD,nzduw1y,"Pretty sure most 9800X3Ds csn hit 5.6 without any problems (unless cooling is limited, but if cooling is a limitation then that will also be for the 9850X3D)  Don't need a golden sample  Don't need to win the silicon lottery. You can have below average silicon and still reach it easily.  They are the same CPU... The 9850X3D OC is just done out of the box",hardware,2026-01-13 17:28:52,1
AMD,nzdts9g,how am I? I did not even touch the voltages. if not that is a pretty good indication that many can hit those speeds.,hardware,2026-01-13 17:23:41,-3
AMD,nzeqnrz,"Knowing corporations, just the core. Binning the memory cont. would be too much work lol",hardware,2026-01-13 19:51:38,2
AMD,nzfury3,How is your memory clock voltage (CPU VDDIO) only 0.87V? That makes no sense.,hardware,2026-01-13 23:01:44,1
AMD,nzl7w00,"How come you're running 5350 instead of 5450? Also, how could I get the core offsets automatically applied each reboot? I have a 5800x3d and always used pbo2tuner but im installing my 9800x3d in a couple days. thanks a lot",hardware,2026-01-14 18:58:53,1
AMD,nzi3frb,"A refresh implies new manufacturing, no?",hardware,2026-01-14 07:33:16,2
AMD,nzvbkmj,So why are you so hung up on the benefits people get from a better binned CPUs then? Go enjoy your life instead of making bad jokes about 480i.,hardware,2026-01-16 05:22:23,2
AMD,nzgmmlr,"*some* percentage of 9800X3D's can't hit 5.6 stable, otherwise it would have shipped at that speed",hardware,2026-01-14 01:33:59,15
AMD,o05s1sh,"Whoa. Whoa. Whoa! I want a 9850X3D for the extra Vcache! We are the same, but different. Same, but different.",hardware,2026-01-17 19:41:30,1
AMD,nzid66c,They 100% bin the IODs. Even console SoCs are binned and the refuse ends up being used for something,hardware,2026-01-14 09:06:16,2
AMD,nzlh8lt,I remember one of the reviewers saying that one of the Intel â€œKSâ€ editions were actually binned more for IMC quality than clockspeed. Iâ€™d guess that was probably one of if not the last â€œKSâ€ edition.,hardware,2026-01-14 19:41:15,1
AMD,nzfv5h4,"It isn't, it's a misread of Zen Timing, it's 1.350",hardware,2026-01-13 23:03:42,2
AMD,nzlbtpe,"Because there's not a meaningful difference between 5350 and 5450 in gaming, just increased temps and increased power draw. You can also set core offsets at the BIOS level so they're applied automatically with every boot. Also, if you liked PBO2 tuner then there's also a similar tool for the 9800X3D called SMU Debugger.",hardware,2026-01-14 19:16:38,1
AMD,nzjsm3s,RaptorLake refresh was just better bins of ~~14th~~ 13th gen. Same with Arrow Lake refresh.  None of the recent refreshes were new-tapeouts. Just new bins,hardware,2026-01-14 15:06:46,1
AMD,nzyvpl5,"I enjoy technology at an intellectual level. It's a cheap, easy intellectual high and great ""break"" if I'm waiting 15 minutes for code to compile.",hardware,2026-01-16 18:40:50,1
AMD,nzglfxh,"Got it, makes way more sense. My Zen Timings doesn't misread it, so I was concerned.",hardware,2026-01-14 01:27:03,1
AMD,o02f9ku,Does not sound like you enjoy it a lot when your first comment is complaint about some strawman benchmarking method.,hardware,2026-01-17 07:01:02,2
AMD,o0p9fk6,"""I need a top tier CPU to enjoy MMOs"" is a strawman.",hardware,2026-01-20 17:33:14,1
AMD,o0t9o75,Good thing i never said that. What i said is that for certain genres these top tier CPUs do offer tangible benefits. In MMOs it is quite important if you do group things. For example in WoW going from a 7700x to 7800x3D can double your framerate in raids.,hardware,2026-01-21 06:34:01,1
AMD,o0qoj7v,"Clickbait, isn't this just their regular release cadence?",hardware,2026-01-20 21:27:02,295
AMD,o0rfj5q,Business is slow for these tech sites and rumor/leak spreaders.,hardware,2026-01-20 23:42:49,37
AMD,o0qttsr,damn I was hoping for a: 9070XT 420 Super Turbo XXX AI69 edition,hardware,2026-01-20 21:51:18,45
AMD,o0r6obv,Who was expecting new cards in 2026?,hardware,2026-01-20 22:55:05,20
AMD,o0qt0nk,"My ridiculously expensive 2022 launch day 4080 will have been the longest lasting gpu I ever had, almost to the point of being worth it",hardware,2026-01-20 21:47:31,28
AMD,o0qtwxt,"Hadnâ€™t leaks indicated they planned to make a major bet on using plentiful and cheaper memory than GDDR7 at high bandwidths, only for them to now see the price of nearly all memory spike?",hardware,2026-01-20 21:51:42,12
AMD,o0smklk,They need Nvidia to launch first so they can -$50,hardware,2026-01-21 03:47:40,13
AMD,o0rfbn4,"That's OK, the public has been going backwards in releases anyway.  Now we wish we could buy a RX 6600 XT.",hardware,2026-01-20 23:41:40,3
AMD,o0sstzu,We went from 4 gpu generations to 2 gpu generations per console gen,hardware,2026-01-21 04:28:06,2
AMD,o1jav7f,AMD always waits for NVIDIA to drop then just releases their cards a month or 2 after. This is nothing new. Itâ€™s embarrassing af but itâ€™s their strategy.,hardware,2026-01-25 01:20:44,1
AMD,o0rf8z5,"Unless they had some hyperbinned Navi 48 SKU I'd never heard of or something planned, this is dog-bites-man.",hardware,2026-01-20 23:41:15,1
AMD,o0srse7,AMD can't launch anything before Nvidia does as they are a price taker in the GPU space. The only thing we need to follow is Nvidia's product launch schedule.,hardware,2026-01-21 04:21:05,1
AMD,o0u20up,my rtx4080 from 2022 is still relevant as hell so why bother? Just buy your 9070xt or 5070ti and wait for next generation,hardware,2026-01-21 10:56:11,1
AMD,o13qrr1,"This is clickbait. No AMD CPU or GPU was expected until 2027 to begin with.  Anyone going on about ""but the rumors"" doesn't know what a rumor is. It's hearsay, *fiction*, not basic fact to be validated and repeated. A rumor ever being correct is someone winning a lottery.",hardware,2026-01-22 19:36:32,0
AMD,o0qpb1s,"Yeah, what would they release?  A GRE model with 12GB of ram and get people more angry.",hardware,2026-01-20 21:30:33,91
AMD,o0qq0xg,"Yeah, pretty much 100% clickbait. The post they're reporting on doesn't even say ""AMD has delayed their next gen cards"". It says they've planned to release them in 2027 for a while.   > ""RDNA5 has been 2027 for a while, and they can't launch before NVIDIA anyway.   > NVIDIA margins are massive and they can counter almost anything with a price drop. What AMD needs is something that is in a difference performance bracket altogether, like the 9800X3D is.""   And anyone could have told you there release would probably be end of year 2026 or early 2027.   If they get pushed back to mid-late 2027 that'd be more news worthy, but not particularly surprising giving the current economic climate and slowly lengthening release cadence (this 10 years vs the 10 years before that).   Nvidia/AMD not releasing refreshes for this gen would be out of the ordinary, but that might not happen either. Prices have gone insane on VRAM, and there are higher margin products to sell.",hardware,2026-01-20 21:33:54,36
AMD,o0riroq,Apparently doing a normal release is bad now,hardware,2026-01-21 00:00:27,7
AMD,o0tmxq9,We had a rumor that RDNA 5 will launch 1 year after RDNA 4 that everyone here believed.  Despite RDNA3 failing to do that when they too were rumored to launch a year after RDNA2.,hardware,2026-01-21 08:34:27,3
AMD,o0qqkpf,"Intital rumors were 2026  https://www.techpowerup.com/339101/amds-upcoming-udna-rdna-5-gpu-could-feature-96-cus-and-384-bit-memory-bus  >Mass production of these GPUs is expected in Q2 2026, so availability is anticipated in the second half of 2026",hardware,2026-01-20 21:36:26,-10
AMD,o0qvoad,i need it how else will i play stardew valley,hardware,2026-01-20 21:59:47,9
AMD,o0rvay0,Followed by the 67 edition founders edition,hardware,2026-01-21 01:09:31,3
AMD,o0tdv49,"Nah,  9070XT 420 Super Turbo XXX *AI69* edition is bad. Will keep holding my 12 year old GPU until 9070XT 420 Super Turbo XXX *Waifu* edition releases.",hardware,2026-01-21 07:10:18,3
AMD,o0qv4b0,Sitting on a 1070ti that at this rate will never be replaced.,hardware,2026-01-20 21:57:12,12
AMD,o0slg9s,3090 here I might have to ride this for at least another year,hardware,2026-01-21 03:40:50,3
AMD,o0sgxzb,RX 5700 hereâ€¦ from 2019â€¦ works fine for the kids Roblox and my civ 6,hardware,2026-01-21 03:13:37,2
AMD,o0t1jl0,"I got a 3080 10 GB FE at MSRP, which still works great and I gave it to my sister. I then got a 4080 Super FE - effectively a $200 discount on a 4080. I am really happy with both the cards.",hardware,2026-01-21 05:29:12,1
AMD,o0qtb2n,Except I got a $600 launch 9070 XT with nearly the same performance for half the price lol  Makes it hard to be worth it when the value equation aged so poorly,hardware,2026-01-20 21:48:52,-24
AMD,o0ua8uu,*insert Mr bean looking at friend's exam sheet meme* ðŸ˜‚ðŸ˜‚,hardware,2026-01-21 12:02:02,1
AMD,o0u9zbg,Isn't it man-bites-dog,hardware,2026-01-21 12:00:04,0
AMD,o0wqk4r,Wouldn't it be more profitable to launch first and then undercut the price when nVidia launches new cards?,hardware,2026-01-21 19:18:11,1
AMD,o0qr9as,That already exists. 9070 GRE. I think it's like China only or something.,hardware,2026-01-20 21:39:32,45
AMD,o0vs3w8,"Yes, which is what they generally do in year 2 launches (RDNA4 launched in 2025 I.e year 1).",hardware,2026-01-21 16:45:50,2
AMD,o0qspqu,Or they could feasibly release a 8GB 9070 GRE with the full bus and 1GB modules. And just market it under the notion that they wanted to release something as cheap as possible with current market conditions.  Would still be a decent e-sports card for a lot of title.,hardware,2026-01-20 21:46:07,-4
AMD,o0rqkf8,AMD never refreshed their RDNA3 GPU's either.  And there's not a lot to refresh RDNA4 with anyways.  It's just the two GPU's and both already have fully enabled products with higher TDP than necessary.,hardware,2026-01-21 00:42:46,5
AMD,o1i5l5w,I'd be VERY shocked if the nvidia super series came out at all. The memory crisis isnt averting and they cant even find enough to create the current series nevermind a vram jacked up version.,hardware,2026-01-24 21:49:05,1
AMD,o0rd6t7,yep just another smear campaign against AMD by the usual suspects,hardware,2026-01-20 23:29:53,-2
AMD,o0qyqz4,RDNA 4 was rumored for Q4 24 and got pushed back - as far as I can tell - to the detriment of nobody. There's far more reason to be delaying this launch than that one as well. I think this news is very predictable.,hardware,2026-01-20 22:14:38,11
AMD,o0rqc3u,Sitting on a regular 1070 myself.  It's extremely inadequate for modern demanding games.,hardware,2026-01-21 00:41:30,6
AMD,o0sxnco,"Get a Arc B580 for  cheap, it will crush your 1070ti",hardware,2026-01-21 05:01:02,1
AMD,o0vpf4s,"'another' year? Newer 24GB cards (for local AI etc) cost an arm, leg **and** a liver. 3090 foreva!",hardware,2026-01-21 16:33:52,3
AMD,o0vqzeo,"Yep, same. DLSS 4.5 ultra performance at 4K helps to extend its longevity in games that are fully path-traced or at least  heavily ray-traced. Luckily, we don't have to worry about any VRAM bottlenecks.",hardware,2026-01-21 16:40:49,1
AMD,o0t7y2i,Same!,hardware,2026-01-21 06:19:37,1
AMD,o0r4zew,You are ignoring opportunity cost,hardware,2026-01-20 22:46:14,20
AMD,o0qtqlm,"The 9070xt came out 3 years after the 4080, bro.",hardware,2026-01-20 21:50:53,34
AMD,o0r8l32,I'll be getting RDNA6 11060xt for more VRAM and half the price and  better everything.  Feel worse yet?,hardware,2026-01-20 23:05:09,13
AMD,o0v4cdj,"Man-bites-dog would be UDNA launching this year or 2029, as it's a situation outside of the usual. 2 year cadence for radeon arches so far, no launches until 2027-8, it's a dog biting a man, aka the usual.",hardware,2026-01-21 14:57:00,2
AMD,o0yl7a7,"> AMD could then undercut  if the cards are selling well... it is unlikely AMD would lower the advertised MSRP, and the retailers / board-OEMs would also prefer not to lower price",hardware,2026-01-22 00:41:23,1
AMD,o0qtgwr,"Yeah, I checked now. It has less CUs then the normal 9070, I hope it is cheap at the very least",hardware,2026-01-20 21:49:38,7
AMD,o1jayj7,"No, they drop a couple months after NVIDIA, not generally, always lol",hardware,2026-01-25 01:21:15,1
AMD,o0qwi3a,"Nah, People will whine because 8GB ""isn't enough"".",hardware,2026-01-20 22:03:43,5
AMD,o0w2day,9070 is aight. 9070 XT though well past sweetspot fs.,hardware,2026-01-21 17:31:55,1
AMD,o0rpvcc,"If they have overlapping architecture teams, RDNA4 getting pushed back shouldn't make a big difference here.  Dont know why you guys are downvoting that person for simply reporting that 2026 was the initial expected window.",hardware,2026-01-21 00:39:00,4
AMD,o0yb31z,"You could get something like 3080 or 4070ti though, are they expensive in your country?",hardware,2026-01-21 23:46:59,1
AMD,o17qoqk,"Even I, a stubborn old value curmudgeon, upgraded my 1070ti this year to a 5070ti to play modern games. Hope you are able to join me, but good luck with whatever you decide to do.",hardware,2026-01-23 10:23:26,1
AMD,o0y1qhe,"And you had access to better features all this time too, arguably",hardware,2026-01-21 22:57:04,2
AMD,o0vq1bs,"Oh, I'm asking because the figure of speech is man bites dog not the other way around. Sounded weird to say it that way when it's not a known phrase.",hardware,2026-01-21 16:36:38,1
AMD,o0qzo9i,Sells for about 3600RMB ($500) these days in China.,hardware,2026-01-20 22:19:15,12
AMD,o0rdfbz,Give it another year of AI datacenters vacuuming up RAM and 8GB cards will look fantastic against 4-6GB cards.,hardware,2026-01-20 23:31:12,5
AMD,o0rr05t,"For an upper midrange GPU that would likely cost like $450, it's definitely not.",hardware,2026-01-21 00:45:11,13
AMD,o0tgra6,"As someone not well versed in this, would it be feasible to harvest old and dead GPUs for their VRAM and reuse that in modern Data Center cards?",hardware,2026-01-21 07:36:42,1
AMD,o0v2vp8,8GB cards are only bad when Nvidia makes them obviously /s,hardware,2026-01-21 14:49:49,1
AMD,o0rl6o9,Didn't you get the policy memo?  You now need 64GB of VRAM with 128GB of system RAM to play at 1080p and not be called a filthy casual.,hardware,2026-01-21 00:13:35,0
AMD,o0rq9kl,"Maybe AMD has learned that giving their teams more time to cook isn't a bad thing. There are no shortage of other reasons why delaying makes sense for this year in particular as well. The world will keep turning whether RDNA 5 comes out in Q4, or 2027.",hardware,2026-01-21 00:41:08,-1
AMD,o0ydv4h,"Oh I just bought a 5060Ti 16GB a few weeks ago to start a new build.  Not quite 3080 level in average native performance, but more VRAM and better DLSS4 performance which I think will make up the difference.  My CPU/platform is even older though(3570k), so I'm looking at a full rebuild.",hardware,2026-01-22 00:01:50,1
AMD,o0uvu32,"The problem is I paid that much money for my Vega FE 16GB 8 years ago and just do not see the point in spending just as much money for the same amount of vram and only like what, double the performance?  I'll just turn the detail down and be happy, stuff still looks good.",hardware,2026-01-21 14:13:40,1
AMD,o0sj9ze,"To me it wasn't worth the sacrifice of 4GB VRAM even if it was $100 cheaper than the 9070. I got my 9070 for 4199.  I expect stock for it to dwindle though, if the 9070 gets cuts the GRE probably would be a even bigger victim",hardware,2026-01-21 03:27:35,5
AMD,o0rw4qt,Time to get the old RX 580 8GB out of the drawer.  Ten thousand years.,hardware,2026-01-21 01:14:18,1
AMD,o0tlv1y,"No, they lack the memory density. A card has only a handful of slots for memory chips to go, most cards have the same number of slots.  The increase in memory capacity is by the memory chips going from 512mb to 1gb to 2 gb to 3 gb in capacity each.  If you harvest chips from an 8GB then you're going to only be able to contribute to cards around the same size, not a 48GB or 96GB card.  That's the basic concept.",hardware,2026-01-21 08:24:06,3
AMD,o0v3lds,How did you know that I have an AMD-GPU?,hardware,2026-01-21 14:53:19,1
AMD,o0ub2lk,Silly strawman laced with false dilemma.  8GB is not enough for modern AAA games at 1080P. Which a lot of 'filthy casuals' like to play,hardware,2026-01-21 12:08:10,3
AMD,o0wfu9c,"Well sure, but if AI wasn't such a party crasher, I think people would definitely care more about this.  Right now, people seem to be in a very ""Who the fuck cares about anything right now?"" state of mind when it comes to future hardware cuz there's so little hope.    Kinda sucks.  Used to be nice to be excited about new hardware releases.",hardware,2026-01-21 18:31:01,0
AMD,o0ym1u7,"Ahh fair enough thats cool man, hope you enjoy your new gpu!",hardware,2026-01-22 00:46:01,2
AMD,o0uzrtp,If you're happy with it I support you. But there's options   You can get 7900xtx for less than what you bought Vega FE for. 328% uplift... No redstone features,hardware,2026-01-21 14:34:09,1
AMD,o0wirlb,"People cared about the RDNA 4 delay too, until it came out and was a solid lineup. Nobody remembers or cares 6 months after launch. It's not worth complaining about, and especially not right now.  I'm still excited as shit for RDNA 5 - I'm looking forward to seeing what AMD can *really* do with regards to RT performance, and I can't wait to see how low TDP handhelds improve (<15W).",hardware,2026-01-21 18:43:48,0
AMD,o0yq8ux,"I'm actually super annoyed. lol   I should have done this rebuild six months ago when prices were better, but I was playing a bunch of indie games and didn't feel there was much hurry.",hardware,2026-01-22 01:09:21,1
AMD,o0yhjxh,"It's different to not complain AFTER something happens, versus like a year beforehand.  Knowing a wait is longer usually sucks.  But people aren't even bothered about it nowadays cuz of extraneous circumstances.",hardware,2026-01-22 00:21:42,1
AMD,nyracr1,"Solid product, nice foundation. Improve ST and intel will comfortably keep their mobile market",hardware,2026-01-10 08:56:00,114
AMD,nyr9ktg,"I think the ideal would be to get to a point where the flagship ""mainstream"" iGPUs (-H series, for Intel) compete with Nvidia's contemporary x50 GPUs, and then have big iGPU chips (Strix Halo, NVL-AX?) to compete with x60+ level.",hardware,2026-01-10 08:48:47,40
AMD,nyrhzxa,"Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.Â    Intel couldn't care less about that, they just need to be better than 890M and the game is done.",hardware,2026-01-10 10:07:58,93
AMD,nysrktd,Iâ€™d love to see its support outside of the approved games demo list. Intel has great hardware but their drivers and game support have always been the biggest question.  Whatâ€™s the point of hardware if you canâ€™t apply it to what you need.,hardware,2026-01-10 15:26:18,11
AMD,nyrvona,"If these chips end up cheap enough that they can replace the standard Intel CPU + 50/60 tier mobile Nvidia dGPU it will be very interesting.  I'm not sure they will be able to in the short term, Nvidia pricing on low end mobile dGPUs is very aggressive ($600 5050 laptops are the proof) but hopefully it isn't long before this type of powerful iGPU becomes a common thing.",hardware,2026-01-10 12:09:03,19
AMD,nyrjnuw,"This is against a 50W TBP RTX 4050 Laptop (which should be more at ease around 90-100W)  Not saying it's bad, but you can't compare Laptop performances without including TDP configuration and behavior.",hardware,2026-01-10 10:23:13,27
AMD,nyrfm1b,"""taking on strix halo"" -> result 50% of strix halo performance, ok.",hardware,2026-01-10 09:45:51,29
AMD,nyuib68,"TWELVE efficiency cores?.... that's nuts.   Anywho, these results look good. Assuming CPU and battery life are comparable or better than Strix Point/Gorgon Point, Intel might have a nice little advantage.",hardware,2026-01-10 20:27:23,9
AMD,nys66a2,Wtf is this article? Strix halo is another class product. Takes on strix halo being more than 50% slower?,hardware,2026-01-10 13:24:55,14
AMD,nyszn5m,"I'm sorry but nothing was more embarrassing than that guy from AMD the other day saying it doesn't matter because Strix Halo, a chip in so few devices that's an absolute behemoth, is still faster. Panther Lake is an absolute achievement for Intel. With the right drivers, they're going to have the perfect chip to forgo low end dGPUs.",hardware,2026-01-10 16:06:23,11
AMD,nyrf0ck,I will need at die fast ultrabook with 12hrs+ battery  Its Not a gaming product,hardware,2026-01-10 09:40:16,5
AMD,nyrhl2f,Website doesnâ€™t load with adblocker,hardware,2026-01-10 10:04:14,4
AMD,nyrng08,"I'm hoping for a thin and light 16 inch laptop with Panther lake and a B390, as it'll be perfect for photo editing, as Adobe seems to prefer Intel over AMD graphics and a discrete GPU is overkill.",hardware,2026-01-10 10:57:35,5
AMD,nyr7n57,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-10 08:30:43,1
AMD,nyvbh6q,I am more interested in next gen desktop APUs,hardware,2026-01-10 22:53:09,1
AMD,nz5usc0,"Compared to my 890M based laptop, the 890M numbers here are about 15-20% lower than what I'm seeing at the same settings.  This is likely due to power targets?  Even if the uplift is just 60% instead of 80%, that's still an impressive achievement for the B390M.  It's a shame that AMD appears to have dropped the iGPU ball in 2026.  Relying on the Strix Halo is not an option here.  It's pretty much impossible to find a good laptops that use it.  The upcoming HX470, and still without FSR4, isn't going to close the apparent large gap.  It seems that AMD forgot to stay hungry and they'll end up losing whatever ground that they'd gained in the last few years.  Mind you, Intel is known to play pressure games with laptop makers too in order to limit AMD adoption, which just makes it even crazier that AMD isn't doing what's required to keep the pressure on.",hardware,2026-01-12 13:57:45,1
AMD,nz6400s,"I think the B390 could be faster than even an RTX 5050 35W (As it could beat an RTX 4050 at 60W).       These thin and light laptops that Panther Lake is built for use way underpowered GPUs. Honestly, it makes sense why the Dell XPS 14 only has the B390 graphics. Before it used an RTX 4050, but it ran at just 30W of power.       Now that Integrated Graphics have beat the -50 Tier of GPUs I don't think we'll even see an RTX 6050 or RX 9050",hardware,2026-01-12 14:47:08,1
AMD,nzdzgjh,"They've basically maxed out the 128-bit normal socket iGPU now.  For them to beat it they need more memory bandwidth - they can put in a bit more cache, but realistically they'll need a quad channel bus (or maybe they can wait for LPDDR6 at 14.4+).  They can probably have a bit more physical room in the next generation of sockets, but without more bandwidth it isn't *that* useful.",hardware,2026-01-13 17:50:05,1
AMD,nyveqz5,"the 140v also got a 25% speed boost post launch, if something similar happens than this could be as good as a 5060 mobile... which is wild! I hope it dosen't cost as much as halo strix!",hardware,2026-01-10 23:10:01,1
AMD,nys0jf3,Needs a conroe vs fx62 moment. It doesn't look promising.,hardware,2026-01-10 12:46:32,-1
AMD,nz7yfdg,Why aren't they comparing the AMD 8060S in the current Strix Halo flagship to the Intel B390? Probably because it doesn't go intel's way... interesting.,hardware,2026-01-12 19:54:01,0
AMD,nystund,And yet maybe 5% of customers will buy this version because its absolutely irrelevant for them whether their laptop would have an Iris iGPU from 2014 or a 2500watt RTX 5090.,hardware,2026-01-10 15:37:56,-7
AMD,nyrt6fi,"Thats great. If you are nvidia making dedicated gpu, then better make something that is not shit. 4050 is a joke",hardware,2026-01-10 11:48:14,-10
AMD,nyrumb5,"But how much does it cost? It mentions it having 16 cores so I'm guessing it's going to be overpriced if you don't need CPU performance, just like Strix Halo.",hardware,2026-01-10 12:00:16,33
AMD,nyrnltb,They need a 25% IPC increase to get back to the leading edge in CPU and honestly i don't see it with their current architecture. They need a new radical design   Edit: getting downvoted for what?. Currently Apple and QC have a very solid lead. Even ARM beats Intel and AMD in general CPU workloads and Intel/AMD have been very slow to update their uarch focusing on clock speed over efficiency and IPC,hardware,2026-01-10 10:59:01,3
AMD,nyyecmo,"AMD Ryzen AI Max+ 388 just dropped cheaper than the 395 with the same GPU, it will be cheaper than the panther lake.",hardware,2026-01-11 11:18:06,0
AMD,nyrbnk5,"Depends on Intel's & amd power targets. I dont think its rly feesible for them to target cpu + gpu power usage, 100W combined at least?",hardware,2026-01-10 09:08:17,16
AMD,nyxp50b,then we wouldnt have the 50 gpus anymore. The XX30 and XX40 GPUs died because of iGPUs competing with them.,hardware,2026-01-11 07:25:59,1
AMD,nyt45q3,> Too expensive for any meaningful customer to adopt and have real mainstream products.   So basically every decent APU ever made. Too expensive to the point it bumps into dGPU territory and not powerful enough to be a direct replacement.,hardware,2026-01-10 16:27:51,28
AMD,nyrnxmm,> Strix halo is a commercial failure. Too expensive for any meaningful customer to adopt and have real mainstream products.Â  >  >   Story of AMD APUs.,hardware,2026-01-10 11:01:59,41
AMD,nys70pu,"AMD aimed Strix Halo at AI users first and foremost, thinking those folks would pay the high premiums.   But of course anybody serious about AI would have an Nvidia GPU, and so many other AI users are still just using cloud-based services anyways.",hardware,2026-01-10 13:30:14,26
AMD,nytpb98,"AMD has always had lower supply compared to Intel and yet AMD client continues to grow. Strix Point at launch had little products (Asus being the only OEM per usual) and yet they still continue to grow, at a smaller scale relative to Intel. Strix Halo is still continuing to have designs made, it wouldn't be a 'failure' if we are still getting Strix Halo products at CES...  I wrote a [comment in a previous post](https://www.reddit.com/r/hardware/comments/1q7d67m/comment/nyhh23c/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button) on the reality of the state of Intel and AMD in the mobile segment. Intel is really dependent on CCG, it is double in revenue to DCAI. They invest in what makes them money. Compared to AMD, client and DCAI is doing well, for client CPUs and GPUs are doing well, putting no pressure in mobile, in fact their strategy has remained the same in the past couple the years and even if marginally their % share is sufficient for them. Intel pushes a lot of supply for mobile, while AMD is smaller, it is all relative in the necessary investments they need to make in order to supply demand. Do I wish AMD stop stagnating in designs, yeah, RDNA3 needs to go, but these companies have motives in what they do.",hardware,2026-01-10 18:08:04,10
AMD,nysq66f,"We all saw it coming a mile away, when it came out in 2025 it was competing against discounted 4060 laptops as low as just $1000. Too little, too late, too expensive, dated on arrival with RDNA3.5, etc. But for some reason this sub and r / amd always have such a hard on at the concept of a ""big APU"" that in practice would never be economically sensible.",hardware,2026-01-10 15:18:57,15
AMD,nyrnx0m,Arc 140T is already on par with the 890m for most tasks excluding games.,hardware,2026-01-10 11:01:50,14
AMD,nyrimhl,Commercial failure indeed. Laptop with dGPUs at same price perform better. Laptops with solid CPU perf are much cheaper.,hardware,2026-01-10 10:13:42,9
AMD,nz23thg,AMD gave Strix Halo zero chance to compete by barely selling any of the lower end models. An 8 core with 32 CUs would be a great mini PC.,hardware,2026-01-11 22:50:43,1
AMD,nyt5d7h,"I dunno Battlemage was a big step forward on driver compatibility and every month Intel improves all their Arc compatibility. I'd be honestly surprised if Xe3 was worse than their current offerings. I'm sure it still has shortcomings as all Intel GPUs will because they're simply starting fresh, but even my A750 is pretty good right now at playing anything I throw at it.  The only aspect Intel kind of messed up that annoys me is their video encoder, once it gets pegged to 100%, it absolutely tanks your performance on the capture to the point where it skips frames and lags. It never used to do that and the driver also used to include capture software, now they just offload it to people having to download OBS and removed the capture aspect of the driver. Kind of dumb when both NVIDIA and AMD include it as a driver option.",hardware,2026-01-10 16:33:28,15
AMD,nz0bqe4,Driver support is better than ever and will continue to get better now that Intel has found its footing in the gaming GPU business (yes that includes igpus),hardware,2026-01-11 17:57:04,3
AMD,nysl2hn,"If TSMC does raise price on their node, Nvidia doesn't find another node for their lower-end bins and Intel can keep the price on their own node down low, we could see Nvidia simply slowly phasing out the -50 series like they used to do with the MX series.",hardware,2026-01-10 14:51:29,15
AMD,nyrlr4x,"The 4050 still has a sizeable memory bandwidth advantage, so it's still very surprising that the B390 comes so close.",hardware,2026-01-10 10:42:21,16
AMD,nyvfe2o,"oh damn, this should be a lot higher up! Most laptops have them clocked much higher so expecting 4050mobile performance is kinof a lie...",hardware,2026-01-10 23:13:25,3
AMD,nyribi5,"I get the feeling everybody is still unsure where these PTL chips slot in to and what to compare these against actually. Once we get more info on pricing, power consumption, CPU performance etc. we will get some actually useful comparisons.",hardware,2026-01-10 10:10:57,23
AMD,nys7fi0,I actually think they meant to say Strix Point in the headline there.,hardware,2026-01-10 13:32:44,12
AMD,nyrhzye,"[HP ZBook Ultra G1a 14](https://www.notebookcheck.net/HP-ZBook-Ultra-G1a-14-review-Powerful-MacBook-Pro-alternative-for-work-and-game.994758.0.html) would've been a better test  Load average: 83.3W   Cyberpunk 2077 ultra \* 110.9W = 80.7fps  Baldur's gate 3: 99.4fps   B390 wattage?   If pantherlake is designed for battery, is it better if it loses performance?",hardware,2026-01-10 10:07:59,10
AMD,nyxpaeu,So how many sub 1000 dollar laptops we have with Strix Halo?,hardware,2026-01-11 07:27:18,2
AMD,nzd2sww,It's definitely going to come down to pricing and availability,hardware,2026-01-13 15:06:48,1
AMD,nysgi6l,The new MSI Prestige 16 looks nice.  They all seem to lack Thunderbolt 5 though.,hardware,2026-01-10 14:25:49,7
AMD,nyt7vgu,"It seems the revived Dell XPS 16 will have the â€œB390â€ and no dGPU, as another option. LTTâ€™s video on it said Dell is quoting 27 hours of battery life in â€œgeneral tasksâ€ and 40 hours of video playback. Obviously remains to be seen how real those manufacturers claims are, but hereâ€™s hoping.",hardware,2026-01-10 16:45:16,4
AMD,nyrvj09,"$1100 for a little MSI 13"" laptop with one. there are also quite a few CPU SKUs that have the B390.",hardware,2026-01-10 12:07:46,41
AMD,nyt4dcl,"Those 16 cores are 4 performance cores, 8 efficiency cores and 4 â€œLow Powerâ€ efficiency cores. This is only doubling the core count of Lunar Lake, by adding the two plain efficiency core clusters. Or keeping the same core count as ArrowLake mobileâ€™s 285H (not HX!), trading 2 performance cores for 2 â€œLow Powerâ€ efficiency cores.  Iâ€™m not 100% on this but I donâ€™t think Stryx Halo used AMDâ€™s C cores, so it basically had an entire 9950x attached to the iGPU.  Prices should be more normal, as this is more part of Intelâ€™s normal lineup.",hardware,2026-01-10 16:28:50,17
AMD,nyych3q,There's an Ultra 5 chip with the B370 (10 Xe cores instead of the 12). Shouldn't be too costly,hardware,2026-01-11 11:00:58,2
AMD,nyvbkeg,It's 16 cores but it's only comparable to Strix Point 12 cores and not Strix Halo.      The highest end Intel chip here only matches the number of P cores in the M5,hardware,2026-01-10 22:53:37,2
AMD,nyrpy0o,"I really don't think that is so important for mobile devices though.  All Intel needs to do is be ""good enough"" and the OEMs will use them in flagship models.",hardware,2026-01-10 11:20:06,41
AMD,nyrzdl1,I doubt anybody is going unseat Apple from the ST throne in the near future.,hardware,2026-01-10 12:37:56,9
AMD,nyrolyj,Well unified core is supposed to be happening in the next couple of gens. Frequencies also seem to have taken a hit on 18A but I'd expect that to improve with time as usual,hardware,2026-01-10 11:08:05,11
AMD,nytxbwv,Not really. They just need to not completely bungle gaming and latency sensitive performance like with Arrow.,hardware,2026-01-10 18:45:01,2
AMD,nyylyv1,"This is about mobile devices, and since a high performing IGPU is included, the question is no longer how well the CPU performs in a system with a 5090 (what most cpu benchmarks focus on) but how well this IGPU/CPU combination performs compared to other IGPU/CPU combinations. I am positive the CPU is not the limiting factor in this IGPU performance tier, so ""leading edge CPU performance"" is not really relevant.",hardware,2026-01-11 12:23:19,1
AMD,nyrc6je,"Don't think it's completely absurd. Should get some efficiencies from less interconnect overhead and lower power memory, so not quite 1:1 with a dGPU. If we were to budget, say, 40W for the iGPU in gaming and 20W for the rest, should be perfectly in line with the higher end laptop SKUs.",hardware,2026-01-10 09:13:21,18
AMD,nyrnpsr,Intel Arrow Lake already uses 80W just on the CPU side in multicore,hardware,2026-01-10 11:00:02,0
AMD,nytxoe2,"It would be viable if AMD released their own small PCs with it to cut the MSRP of products, but they aren't interested.",hardware,2026-01-10 18:46:36,7
AMD,nyummcb,And yet AMD managed it for the PS5... it's clearly possible.  Of course we don't know the cost breakdown there as far as PS5 pricing goes.,hardware,2026-01-10 20:49:14,4
AMD,nyrpk3i,They just need to make the next iteration cost less. Most of strix halo's issues were the sky high price.,hardware,2026-01-10 11:16:37,7
AMD,nyspmud,"""Local LLM"" is such an incredibly niche thing I can't believe the tech nerd internet is so obsessed over it. Any real life business use case of AI is cloud based no question asked.",hardware,2026-01-10 15:16:05,17
AMD,nywuwh7,well said,hardware,2026-01-11 03:51:39,0
AMD,nytll12,Too much listening to MLiD who has a boner for APUs,hardware,2026-01-10 17:50:40,11
AMD,nytno6k,"Idk one can easily flip your statement. Panther lake coming in **2026** competing against continuing discounted 4050s prob less than 4060s. I don't dislike Panther Lake nor am I defending Strix Halo, but I wouldn't say your argument is a rather good one.",hardware,2026-01-10 18:00:26,-3
AMD,nyrtebm,https://m.youtube.com/watch?v=ymoiWv9BF7Q   It's already at least on par for reasonable power profiles unless you play stuck to the wall.,hardware,2026-01-10 11:50:08,5
AMD,nytp8ri,"Huge step forward, I just wish the didnâ€™t struggle with older and brand new games. Itâ€™s a great card if you are willing to do troubleshooting and know computers but I wonâ€™t recommend them to family yet.",hardware,2026-01-10 18:07:45,5
AMD,nz0sex5,I hope they start supporting dx11 stuff. Thatâ€™s a ton of games.,hardware,2026-01-11 19:10:35,1
AMD,nytykfy,Isn't TSMC planning to increase pricing on n2 by 20-30%,hardware,2026-01-10 18:50:42,6
AMD,nz0bimn,Nvidia will find another cheap node to use. Samsung will gladly oblige,hardware,2026-01-11 17:56:04,1
AMD,nyrsbne,"153 GB/s vs 192 GB/s is not that ""sizeable""  And the comparison against ""HP OmniStudio X 32-c0077ng"" is weird, even in the linked test they have GPU-Z screenshot displaying 1375Mhz memory speed instead of 2000 Mhz on most other RTX 4050 Laptop Review.  I don't understand this comparison against an All-in-One, and I'll wait for more in depth reviews to draw some conclusion.",hardware,2026-01-10 11:40:59,15
AMD,nyrm798,"PC World had power consumption tests under gaming loads. It pulled 60W through USBC with Cyberpunk, so probably 35-40W for the gpu. When they unplugged it, the benchmark numbers stayed the same. So it also pulls 60W on battery.  Unless the manufacturer actually configured the device to simultaneously pull energy from the cord and battery under full load.",hardware,2026-01-10 10:46:26,11
AMD,nyxt6pz,About as much as we have PTL laptops,hardware,2026-01-11 08:02:38,3
AMD,nysiaor,"TB5 isn't a big deal, although I don't like that they have a numpad keyboard, and usually MSI speakers are terrible.",hardware,2026-01-10 14:35:56,4
AMD,nyrxjeo,But can't you get a laptop with a 5060-5070 at that price?,hardware,2026-01-10 12:23:52,17
AMD,nyvevcg,"Damn, that's really good! it's pretty much macbook air pricing.",hardware,2026-01-10 23:10:39,1
AMD,nyvvmm4,"> Iâ€™m not 100% on this but I donâ€™t think Stryx Halo used AMDâ€™s C cores, so it basically had an entire 9950x attached to the iGPU.  I have a Strix Halo.  What you wrote is exactly what it is.  It's essentially a 9950x (so all P-cores) with a fat iGPU attached, and with a 4 channel memory controller instead of 2-channel.",hardware,2026-01-11 00:39:01,15
AMD,nyxcvyp,"The biggest difference between the P and E cores is fMax. The larger the core count becomes, the lower the all core clocks become, the smaller the gap between P and E core performance becomes.   The IPC difference between the two is like ~10%  At a certain point along the wattage curve, given a certain number of cores, there will be a point where E core performance can potentially meet or exceed what you would've gotten has you had too many P cores.    Its also more than just trading 2 P cores for 2 lpE cores. The lpE cores in ARL-H were *so* weak, they were functionally useless. In practice, it'll be more like trading 2 P cores for 4 lpE cores  edit: to be more specific, In ARL-H, below 5W per core, E cores outperform P cores. If you have 16 cores and are running all core workloads, then at 60W, each core is receiving less than 4W.",hardware,2026-01-11 05:45:32,6
AMD,nz7k3ly,No it's firmly ahead of strix its right in between. Strix point uses 8 ecores too and it gets demolished in multithread benchmarks as expected,hardware,2026-01-12 18:48:32,1
AMD,nyvb2cd,"It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.     AMD made them lower margins for laptop chips because they weren't very competitive. If they want fat margins, they need to be the best",hardware,2026-01-10 22:51:00,3
AMD,nyuqay0,Single Core is very important when Intel is doing these designs that lack P cores throughout. The cheapest X2 Elite has the same amount of P cores as the most expensive Panther Lake SKU,hardware,2026-01-10 21:07:30,1
AMD,nysbrxo,"Qualcomm is already super close with Oryon V3...  Perf/Watt for that single thread isn't close I guess, but absolute performance is breathing down Apple's neck for sure.  Also, don't compare Geekbench scores on windows vs Linux/Apple/Android... Windows just does something negatively about it and the difference is 5-7% vs non-windows.",hardware,2026-01-10 13:58:36,8
AMD,nyvsos5,>Well unified core is supposed to be happening in the next couple of gens.  I would be shocked if this has much to do with a large performance uplift. I imagine it would have to do more with rightsizing core area and power draw.,hardware,2026-01-11 00:24:01,2
AMD,nyuqf9s,Chasing above 5Ghz is stupid on laptops. It only matters for desktops,hardware,2026-01-10 21:08:07,3
AMD,nyu42wk,You are overly focusing on gaming. I mean general CPU performance,hardware,2026-01-10 19:16:57,1
AMD,nyrcv9u,Rtx 5050 is 61% faster than B390. I doubt if they change the wattage configuration and stick to 60W they'll match it. Unless the 5050 is capped to more reasonable wattages like 60-80W. Plus the 60W budget for Intel/amd will be used for other compotents and the apu budget reduces.,hardware,2026-01-10 09:19:52,8
AMD,nysfyyg,"I'm talking out of my knowledge base, but I think the switch from heat pipes to custom vapor chambers means we are less bottlenecked at power density / pulling heat from the chip and more constrained at what the radiator/fan system can push out of the system.",hardware,2026-01-10 14:22:47,2
AMD,nz3zzs3,"They announced a first party Strix Halo PC at CES, but it'll probably be really expensive.",hardware,2026-01-12 04:57:58,3
AMD,nyzybbo,"> And yet AMD managed it for the PS5... it's clearly possible.  Well for two reasons:  1. Sony bankrolls the R&D of the APU and it's underlying architectures which allows AMD to make it for basically cost and have a low BOM on it. They didn't pay as much as they normally would for the R&D, tapeout, testing etc.  2. It's a console APU, it literally has to be cost effective to make sense, otherwise it becomes like Strix Halo and SONY goes out of business. Also most consoles are sold on launch for a small loss with SONY and Microsoft recouping those lost funds off game sales, online subscriptions and store revenue. Then over time they tend to shrink console APUs on newer nodes which makes it more power efficient and less expensive to produce as a smaller chip on a newer node typically has better yields, it also allows SONY or Microsoft to put in lower quality components like less heatpipes in a new revision or Slim console, for similar thermal headroom and save on BOM cost.   I mean there's a reason why they do not offer the PS5 APU as an off the shelf product, only the cutdown bad yields go onto being some cryptocurrency mining board or some Linux APU and with the performance being cut its usually worse value than buying off the shelf dGPU parts like a 5060 or something.  I don't know why you're seriously arguing that APUs for Desktop and Laptop PCs are a viable product. For one, they've never been viable, not once. Even Strix Halo which is honestly the best APU I've ever seen has been ruined by its high cost. Don't get me wrong, I like the idea of an APU, an all in one chip that does it all. But unless you're like Intel and you're willing to do a tile based design and or basically have a true chiplet where you can link lots of smaller dGPU tiles together it doesn't really work. You're just better off buying dedicated CPU and GPU parts for better price to performance. If you don't believe me, I can buy an [RTX 5070 Laptop right now for $1900 AUD](https://www.centrecom.com.au/msi-katana-15-hx-14xwgk-156-qhd-i7-16gb-ram-512gb-rtx-5070-gaming-laptop-black) and that will easily outperform Strix Halo which has less performance and typically costs over $4000 AUD... [Even a lowly 4060 laptop fairs better.](https://youtu.be/RycbWuyQHLY)  The only thing APUs excel in is this, if you want something relatively cheap but capable. i.e it can run a game at 30 FPS with medium settings at a low resolution. i.e something like Panther Lake or Apple's M series chips. But if you want true performance, just go out and buy an RTX X060 series laptop it's far better price to perf each generation.",hardware,2026-01-11 16:53:47,6
AMD,nyvw6mt,"What is possible? PS5 uses GDDR6 instead of DRAM. And consoles are heavily subsidized by digital purchases. I bet AMD makes good money on PS5 (and Xbox X/S), Sony & Microsoft just subsidize the shit out of it with their 30% cut from selling games. Even the Steam Deck is barely profitable for Valve. High-end APU is just a waste of sillicon.",hardware,2026-01-11 00:41:51,12
AMD,nyrrj9f,Even their Ryzen 5 AI 340 laptop are too expensive and you can buy an older gen Ryzen 5 with Nvidia GPU laptop for same price or even lower with much better GPU performance.,hardware,2026-01-10 11:34:10,29
AMD,nyxoopw,Local AI (not just LLM) is universal on mobile and getting to be universal in corporate computers. You just dont see it. The background blurring in Teams meeting? 5x more battery efficient with AI. But its just going to be integrated into Teams and fire up if hardware supported without asking you.  >Any real life business use case of AI is cloud based no question asked.  All AI use cases at the place i work for is local due to confidentiality issues. We cannot and will never be able to use this on cloud. Unless the world completely flips its ideas about confidentiality i guess.,hardware,2026-01-11 07:22:00,5
AMD,nyu206m,"Panther Lake is a normal CPU, not some special ""big APU."" It doesn't make much sense to flip the argument the way you did.",hardware,2026-01-10 19:06:56,9
AMD,nysp4h4,"140T (Arrow Lake) isn't the same as 140v (Lunar Lake) though, the former is usually quite a bit weaker and inconsistent in games despite slaying all the synthetics.",hardware,2026-01-10 15:13:21,11
AMD,nysjpec,"It is probably because the AIO was one of their most, if not the most recent RTX 4050 tested (March 17th 2025) which probably enabled them to compare in newer title like F1 25 in the article, as it has already been around for like 3 years while RTX 5050 was released last year and received more attention in its place overall. From their database, the next most recent thing with RTX 4050 they reviewed was the Yoga Pro 7 in January 2025 with a 60W RTX 4050 (45 watts + 15 watts Dynamic Boost), which scored 50.8fps in Cyberpunk 2077 at the same setting and thus a bit lower than the AIO, so I would say the AIO is at an okay spot for a RTX 4050 to be compared to this Arc B390.",hardware,2026-01-10 14:43:54,6
AMD,nysoib9,"> 153 GB/s vs 192 GB/s is not that ""sizeable""  25%? What's sizeable?",hardware,2026-01-10 15:10:05,22
AMD,nyrwfu6,"It would be easier to compare mobile parts if laptop OEMs didn't lock down their BIOS and EC registers, blocking anyone from actually tinkering with the (godawful) default configs for TDP, boost behaviours and fan curves on most common laptops  You can buy the bestest Intel Core Ultra 9 285h but if some engineer at HP thinks that 45Â°C idle is too warm it will either throttle to the point that you wish you were using the Nintendo DS browser or crank the fans to Mach 3...",hardware,2026-01-10 12:15:09,6
AMD,nyxwid7,had no idea Strix Halo is this popular.,hardware,2026-01-11 08:33:14,2
AMD,nysx853,"the new Prestige 16 actually [doesnt use a numpad](https://www.notebookcheck.net/MSI-debuts-Prestige-16-AI-and-Prestige-16-Flip-AI-with-Panther-Lake-H-Core-Ultra-X9-388H-and-Arc-B390-graphics.1197009.0.html)!  and the flip version is especially intresting, they managed to tuck the stylus *under* the laptop with a slot that can also charge said stylus",hardware,2026-01-10 15:54:42,6
AMD,nys9rsz,"Laptops with 5060 at sub- $1000 weren't launch event laptops at CES. They came later as fairly cost optimized, ""compromised"" laptops that cheaped out on most of the total laptop in order to fit that CPU/GPU in its budget.   PTL-X is PTL-H with a ~60mm GPU tile. A 4050 is a binned ~160mm chip. Edit: that *also* requires its own VRAM and cooling  Intel is also on record saying 18A cost structure is flat vs Intel 7. I imagine costs between PTL-X and RPL-H + dGPU are much more competitive than you think, with the only caveat being discounts on old excess inventory and not having to redesign a new laptop (although I imagine the RAM pricing increases makes the total price different between the two shrink even more)",hardware,2026-01-10 13:46:47,38
AMD,nyrxu16,"5060 yes, but it's less power efficient",hardware,2026-01-10 12:26:08,23
AMD,nywhxd0,"5060 -5070 cannot be fitted into ultrabook or thin & light models. those item are power hungry and high temperature, need to fit it in bulky laptops which are bigger heatsink , more room space.",hardware,2026-01-11 02:38:34,4
AMD,nyt5pt1,"technically, but it will be a shitbox in basically every other aspect (and stuck with 16/512)",hardware,2026-01-10 16:35:08,13
AMD,nyrycdm,i've been looking rn but have only seen those at $1400+,hardware,2026-01-10 12:30:01,-2
AMD,nyvtubh,"> It affects their margins. The more competitive and better QC is, the less Intel can charge OEMs for their CPUs.  Last year's leadership QC laptops had to be heavily discounted shortly after reaching market. Clearly there's more to it than IPC.",hardware,2026-01-11 00:30:06,5
AMD,nyxgar9,"The X2 Elite *may* be an amazing CPU. But customers don't buy mobile CPUs. They buy full, complete laptops, and that includes all of WoA's issues. Customers have so far, by and large, mainly rejected WoA. The biggest demographic of people who research and care about strong CPU performance are people who'd also want to play games, and QC has yet to demonstrate that that's viable.",hardware,2026-01-11 06:11:34,9
AMD,nyxo27r,But this product has 4 P cores?,hardware,2026-01-11 07:16:32,3
AMD,nysorh4,Do we have 285H/HX370 scores on Linux for comparison?,hardware,2026-01-10 15:11:26,4
AMD,nyu3bo0,I was going to wait for this - but driver support comments basically said wait for it to mature.,hardware,2026-01-10 19:13:18,2
AMD,nyw054m,"it is presumably lead by the e core team that's doing a lot better so we'll see, but at the very least saving area from debloating p cores would allow a bit more cache that the cores would love.",hardware,2026-01-11 01:02:05,3
AMD,nyurnsr,Chasing 5GHz is only stupid if it costs more power than it'd save. The lower end panther lake SKUs clock their cores a lot lower compared to LNL so it's likely just a node thing,hardware,2026-01-10 21:14:19,2
AMD,nz5bvpr,Apple and Qualcomm are both doing that right now though. It's cheaper than blowing up the area of the core to increase IPC.,hardware,2026-01-12 11:55:36,1
AMD,nyvv21f,But for non -gaming tasks arrow beats zen5,hardware,2026-01-11 00:36:07,7
AMD,nyxogf6,Gaming is the only segment where your previuos comment made sense though.  Everything else Intel is still leading edge.,hardware,2026-01-11 07:19:59,1
AMD,nyrdymf,"Yeah, I'm not talking about PTL. Clearly it's too far off. But clearly there's a lot of room left for Intel (and current AMD APUs) to catch up. Also worth noting that that 5050 is given 100W, which is particularly high for that chip. Gap obviously closes when the TDP is more reasonable.",hardware,2026-01-10 09:30:19,11
AMD,nyvf643,"it may not be this generation, but at the rate iGPU performance growing; pretty soon xx50 chips is no longer relevant. \*its not like Nvidia can make fat profit anyway.   Fyi, Nvidia has abandon their low profit margin xx30 line up, or Geforce MX series in laptop.",hardware,2026-01-10 23:12:14,3
AMD,nz57ion,Nah. the price of Strix Halo is the cost of the PS5 itself. AMD has fat margins for laptops and desktops,hardware,2026-01-12 11:20:38,2
AMD,nys1wlu,Laptops with dgpu always has poor battery life. Even tinkering with the best power optimizations. These ryzens have nearly double the real world battery life from my experience.,hardware,2026-01-10 12:56:23,8
AMD,nywmg0a,"Depends on what you consider to be â€œhigh end mobile gamingâ€, the laptop 4060 is currently the 2nd most popular gpu on steam, and thatâ€™s the level strix halo targeted",hardware,2026-01-11 03:03:40,1
AMD,nyxpn7n,"What youâ€™re describing is just inference. Runs on a phone Soc. Minimal memory requirement. Like faceID on the original iPhone X over eight years ago. Strix halo provides no additional benefit over strix point or lunar lake. If there are business use cases that use outlook or Microsoft 365 or Teams, they are using cloud based copilot. Thatâ€™s the mainstream business use case at present.",hardware,2026-01-11 07:30:29,1
AMD,nyu45c4,"The statement is directly comparable. 'Big APU' Strix Halo can literally be fit into a [handheld ](https://gpdstore.net/product/gpd-win-5/)and a [surface type tablet](https://www.ultrabookreview.com/71207-amd-strix-halo-asus-rog-flow-z13/). Regardless of the effective yields due to it's size, it is coming out another year later when compared to a 40 series gen, and a tier lower than the 4060. If you want to game, like many have argued with Strix Halo when it launched, just get a discounted RTX 40 dGPU laptop... Panther Lake has a great iGPU, don't get me wrong, but the argument isn't good.  A better one would be \~10-25W Panther Lake would be competitive than Strix Point/Halo and so on, not 'Strix Halo isn't economically sensible' because it's still on the market, with CES designs still being announced.  Some people in the sub think that if they aren't the ones the product is directed to (which is pretty much gamers), then they believe 'well it must've been a failure'.",hardware,2026-01-10 19:17:17,-3
AMD,nyt80oc,"Oh I'm blind lol, my bad.",hardware,2026-01-10 16:45:57,6
AMD,nyt0j4v,Yeah I'm bewildered by this take that it's not sizeable.,hardware,2026-01-10 16:10:35,8
AMD,nz5etqc,Easily offset with a slightly bigger cache.,hardware,2026-01-12 12:17:50,2
AMD,nyw80nh,"Don't worry, the engineer at HP also made sure you can never exceed 35W continuous power draw by giving it an undersized vrm and no vrm cooling",hardware,2026-01-11 01:45:48,2
AMD,nyt058y,"Oooo neat, close to perfect for me.",hardware,2026-01-10 16:08:46,8
AMD,nyu7ff7,"> Intel is also on record saying 18A cost structure is flat vs Intel 7  The 12Xe GPU die is on N3E, not 18A. Though I still agree with the conclusion that PTL should still end up relatively affordable, and cheaper than an equivalent dGPU.",hardware,2026-01-10 19:33:11,11
AMD,nyxbhd7,"Power efficiency is a curve. There will exist points along that curve where the B390 is more efficient than the 5060.   Efficiency is more complicated than just ""perf/watt at specifically both chips maximum power draw""  edit: May have misunderstood your comment. Thought you were saying B390 was less efficient than a 5060",hardware,2026-01-11 05:35:14,1
AMD,o058umn,Asus G14 would like a word.,hardware,2026-01-17 18:11:44,1
AMD,nyt7ejr,>(and stuck with 16/512)  Those typically have open ram and ssd slots. It's the premium thin models that have them soldered on.,hardware,2026-01-10 16:43:03,10
AMD,nysokej,https://www.bestbuy.com/product/asus-tuf-gaming-a16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-32gb-ram-nvidia-geforce-rtx-5070-1tb-ssd-jaegar-gray/JJGGLH8Y2Z  [Proof that the deal at least exists at the time of this comment](https://imgur.com/a/XYQm2fn),hardware,2026-01-10 15:10:23,10
AMD,nywgma9,"QC last year had a bad product.Â  It was competitive vs AMD and Intel but Qc was selling those for 50% less than Intel or AMD chips. OEMs at first decided to price these at Intel prices then it settled at Intel -100/200â‚¬   QC laptops still sold what QC and partners expected and OEMs are increasing new models for X2 (design wins went from 60 to 100+)   X2 has a 25% advantage vs Panther Lake and it will still be cheaper because QC is an underdog. If QC captures market share and reaches 10-15%, then Intel will start to sweat and then margins will be hit. I don't think QC gets anywhere near that till like 2028/2029. The laptop market is VERY slow to move. AMD had a better product for several generations and it only netted them +10%   While QC and Mediatek/Nvidia don't hit a bigger marketshare number. Intel and AMD won't need to lower prices",hardware,2026-01-11 02:31:29,2
AMD,nyt5fdi,"[285H GB6 Windows \~2900](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+windows)   [285H GB6 Linux \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=core+285h+linux)  [HX370 GB6 Windows \~3050](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+windows)   [HX370 GB6 Linux \~3000](https://browser.geekbench.com/search?utf8=%E2%9C%93&q=ai+370+linux)  I'm just eyeballing results on geekbench browser 1st page but surprised by the HX370 results. Intel is all over the place but that may make sense since Intel is actually found on tons of laptops compared to AMD shiny hunting experience.  There was one source i had found testing X Elite on Windows and WSL2 on the same machine that showed Geekbench performing higher on WSL2 than native on windows but it may have been a yt video. Perhaps i'm mistaken.  AFAIK , the Windows Tax is real for Geekbench. this particular search on HX370 showing otherwise is a fluke imo. You can search other chips too, like 285K 3350win vs 3500linux  Unfortunately, i cannot bother looking for more controlled setups that had the same exact setup with both Linux vs Windows compared to definitely prove this, but i have seen those in the past here and there.  EDIT:   I found a source that compares GB6 Windows vs Linux relatively recent   [AM5 W11 vs Linux Performance Comparison in GB3,5,6 - Ryzen AM5 - HWBOT Community Forums](https://community.hwbot.org/topic/236884-am5-w11-vs-linux-performance-comparison-in-gb356/)  The Windows tax is still real.  [Qualcomm Snapdragon X2 Elite Extreme X2E-96-100 Processor - Benchmarks and Specs - NotebookCheck.net Tech](https://www.notebookcheck.net/Qualcomm-Snapdragon-X2-Elite-Extreme-X2E-96-100-Processor-Benchmarks-and-Specs.1127282.0.html)  Idk what actual source notebookcheck used here, but if x2 Elite Extreme reaches 4080 in GB6 on windows then +4% for linux would be 4240... Whether that's comparable to Apple M5 or not i'm not gonna say more on the subject...       I'd say Qualcomm is gonna be within spitting distance to Apple... Sure SD2X Extreme is highest end unicorn SKU vs base M5, that's valid argument, but still... within 10% of Apple i consider within spitting distance.",hardware,2026-01-10 16:33:45,3
AMD,nyt6hqk,I hope somebody tests Panther Lake GB6 on both linux and windows.,hardware,2026-01-10 16:38:46,0
AMD,nyvsukh,">Chasing 5GHz is only stupid if it costs more power than it'd save.Â   Chasing any GHz much above Vmin would cost more power than the performance it would bring, no?",hardware,2026-01-11 00:24:52,5
AMD,nyytnv8,"They are not. Like I said they are 25% behind Apple and Qualcomm in ST. Multicore the X2E can go up to 2x the performance of Arrow Lake and Panther Lake is just a refresh on 18A   Now there's more competitors.Â  They have 5 total. AMD, Nvidia, ARM, Qualcomm,Â  Apple",hardware,2026-01-11 13:19:34,1
AMD,nyrg65a,3nm will give 6050 another 20%. Whatever changes amd/intel do at power limit needs to be impressive. Otherwise I still see cpu + gpu combo yielding better perf.   Not perf/W or maybe perf/$,hardware,2026-01-10 09:51:05,12
AMD,nyrz0br,LPDDR6 coming hot with 50% bandwidth improvement...,hardware,2026-01-10 12:35:09,8
AMD,nyxwgwn,"Obviuosly. all AI *usage* is inference. Inference requires plenty of memory btw, it all depends on the model you want to run.  No, that is not the business case use.",hardware,2026-01-11 08:32:52,5
AMD,nyyeomk,"The Strix Halo was intended for local ai, the OpenAI OSS 120B fp4 model (or a 240B fp4 50% pruned like MiniMax 2.1) is run at 50 t/sec on a Strix Halo, or about 5 000 000 tokens day - 75$ if using Sonnet 4.5.  So in 20 days you get the money back (a 96GB RAM Strix Halo during 2024 has been sold for1480$ by a lot of OEMs and the 128 GB RAM - for 1600$-1700$), not to say you keep home your AI work",hardware,2026-01-11 11:21:07,3
AMD,nyyfatm,"if only AMD released a 384 bit bus Strix Halo with support to LPDDR5X 10700 MT/s, that would double the bandwidth - from actual 260 GB/s to 520 GB/s, putting it in the M4 Max category, which Apple is selling at 4000$",hardware,2026-01-11 11:26:37,1
AMD,nyua4zp,"They're completely different product classes. One is priced for mainstream and the other is decidedly not. One is purpose-built to go up against discrete GPUs, the other is not. That's why flipping that statement just doesn't work.",hardware,2026-01-10 19:46:22,9
AMD,nz565l5,"Donâ€™t even get me started! My current HP laptop straight up doesnâ€™t support any type of fan control on Linuxâ€¦  So even if I throttle my CPU manually based on temps, the vrm WILL burn a hole in my desk during prolonged use  I even found the basic EC registers for fan speed, but there is some other magic register that keeps resetting them. And trying to find the magic register might involve frying the board if you hit a voltage-related EC",hardware,2026-01-12 11:08:51,1
AMD,nyt1ch3,"ikr, im also heavily considering the Prestige 16 Flip atm (even tho I am an unhappy owner of a 8 year old MSI Thin...)",hardware,2026-01-10 16:14:27,5
AMD,nz39yac,Not using LPDDR is part of what makes it a shitbox.,hardware,2026-01-12 02:28:11,3
AMD,nz7jgiw,Screen is still dogshit,hardware,2026-01-12 18:45:44,0
AMD,nyv9rhl,"It's not just Geekbench, Linux usually has higher performance",hardware,2026-01-10 22:44:30,2
AMD,nyvzib4,"depends on the workload and the efficiency curve, but there is the race to sleep concept. Even assuming hanging around at low freq the voltage can sustain is always better power wise - which i don't think is true as you're dropping a lot of performance, you still have to power all the uncore around it  I saw someone run a couple tests on intel/amd for iirc a game server workload, and while intel peaked a lot higher from aggressive boosting, the amd cpu consumed more energy overall",hardware,2026-01-11 00:58:42,1
AMD,nz4duhh,"Neither apple nor qualcomm are real competition in a sense that Apple has its own segregated market that does not crosscompete and qualcomm practically does not exist in segments Intel is in.  ARM is hurting them in servers, but not really relevant for a laptop discussion.",hardware,2026-01-12 06:45:51,2
AMD,nyu8aqs,"> 3nm will give 6050 another 20%  But are Nvidia willing to use cutting edge nodes for their low end GPUs? If they don't move to N3 before Intel/AMD have an N2 GPU, a gap will remain. And of course LPDDR6 should be a big deal for bandwidth.   Obviously not treating this as a forgone conclusion, but doesn't seem like an unreasonable target for this part of the lineup.",hardware,2026-01-10 19:37:26,4
AMD,nystsgw,The only TRUE disable option on windows is to disable through bios for most laptops. Which becomes extremely tedious if you want to use the dGPU without constantly restarting.  I have never owned a laptop with a dGPU that didn't misbehave constantly and not fully idling.,hardware,2026-01-10 15:37:37,6
AMD,nyy6xsc,"Plenty of ""daily use cases"" have very minimal hardware requirement, the original iPhone X FaceID ran on a device with 3GB ram and it was sufficient for FaceID purpose. And I don't know nor care your particular business use case, since you made zero specific clarifications I only had to bring up one mainstream example which is Microsoft 365 and its cloud based subscription based Copilot feature.",hardware,2026-01-11 10:10:21,1
AMD,nyyfwoo,Probably that's what gonna happen with medusa halo. On N2. It will actually match Apple M6 Max pricing.,hardware,2026-01-11 11:32:03,3
AMD,nyue00o,"I am not talking about product classes though? The original statement is trying to say that an **SoC can compete with dedicated iGPU** regardless if Strix Halo is bigger. They are trying to say that it was obvious it was **going to be a flop, when competing against a 4060 that at the time was being sold at a discount**. **Panther Lake is literally coming out another year later one tier below a 4060 and a gen old**.  Yes, they are different product classes, but Panther Lake SKUs that have 10-12 Xe3 cores will most definitely be >$1000 with laptops. ""Mainstream"" pricing is subjective in this class, unlike GPUs where there are 5060s and 5080s segments. At CES, there are surprisingly dGPUs still being paired with Panther Lake, heck, Strix Halo was designed purely for it's iGPU, even the engineers stand by this (PCIe slots are being released in miniPCs because that's what the market wants).  Also, this ""big APU"" argument is based on chiplets/tiles. Strix Halo isn't monolithic, same as Panther Lake. They both have the same design strategy that makes it economically viable to tape out in the first place.  I am not trying to say that STX-H is better than PTL, PTL was like the only thing I was looking forward to at CES, but this whole thread surrounding around how STX-H is a failure doesn't make sense at all.",hardware,2026-01-10 20:05:42,-4
AMD,nyt2jkc,"I hadn't long bought a Zenbook S16, but if MSI can get a decent spec with the B390 under Â£2k then maybe.",hardware,2026-01-10 16:20:10,3
AMD,nz3mb97,Their target audience is more likely to complain about upgradability.,hardware,2026-01-12 03:35:16,2
AMD,nz9ccsy,New goalpost?,hardware,2026-01-12 23:57:14,3
AMD,nyxhb1b,"Race to sleep has value to a point. Does someone on battery want to, say, increase power consumption 4x to race to sleep 2x faster?",hardware,2026-01-11 06:19:34,3
AMD,nyxoaio,> there is the race to sleep concept.  i hope we can excise this concept as soon as possible. It leads to worst design choices.,hardware,2026-01-11 07:18:33,3
AMD,nzbv20u,"I don't think amd ryzen will match M6 Max pricing (amd is selling them at 400$), as those miniPC are manufactured by a lot of noname companies, making a true competition  There are 37 such ryzen ai max 395+ products [https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them](https://www.techradar.com/pro/there-are-15-amd-ryzen-ai-max-395-mini-pcs-in-the-world-right-now-heres-where-you-can-buy-them)  And there are also nvidia with their dgx project, Qualqom with their Snapgragon X Elite 2, a lot of RISC-V platforms like tenstorrent with 512 GB/s (but only 32 GB VRAM at 1399$), so even apple will need to double the bandwidth in their upcoming M5 pro/max in order to stay competitive with actual prices",hardware,2026-01-13 10:18:10,1
AMD,nyyn7pe,"I assume at least intel and amd do some research there for how much the cpu should boost if the oems don't, and also have to consider user impact from lower performance but I guess that's more fighting against windows getting slower.   Presumably with current nodes 5GHz is always beyond the point of being worth it but no reason that has to carry into future gens",hardware,2026-01-11 12:33:11,1
AMD,nzbx72p,Medusa halo isnâ€™t strix halo if going by what you think it is going to be. It will be much bigger and on N2.,hardware,2026-01-13 10:37:51,1
AMD,o0oxzmw,"Makes sense, nvidia buys a lot more memory and can negotiate a better deal with their economies of scale. But I'm sure AMD's deal is still miles better than what an AIB could get on their own.",hardware,2026-01-20 16:40:07,169
AMD,o0oxzug,"Nvidia is much, much, much bigger than AMD. Tech media can make you feel like they're a 50-50, but reality shows it's 95-5. Ofcourse Nvidia can afford cheaper VRAM simply because they buy them in much bigger quantity",hardware,2026-01-20 16:40:09,91
AMD,o0p1ia6,"Is that surprising? Nvidia appears to be the sole (major) customer for GDDR7, as they were for GDDR6X. It's not just economies of scale, they are the only customer. They're able to invest into it early and get a better deal in return.",hardware,2026-01-20 16:56:20,43
AMD,o0qkztu,"So have the rumours about Nvidia stopping GDDR7 bundling with their GPUs been put to bed?  It's genuinely hard to believe much of anything regarding Nvidia from the gaming oriented tech press, so much rumour and shitty sourcing.",hardware,2026-01-20 21:10:46,20
AMD,o0oxfz1,"Most likely Nvidia got to buy more than AMD and can bundle memory cheaperâ€¦ or AMD have the same level of inventory for GDDR6 but theyâ€™re squeezing more for profit  Either way, headlines from TPU and the source media outlet wins with clicks",hardware,2026-01-20 16:37:37,46
AMD,o0pgana,Buying more gives you a bigger discount? Shocking.,hardware,2026-01-20 18:04:37,12
AMD,o0qdlva,Then why its gpu isnot cheaper....,hardware,2026-01-20 20:36:54,6
AMD,o0ozoq4,Volume breaks. At 8% of the market it costs more for AMD to make the GPUs. This is nothing new. Both of these companies are public and historically Nvidia has always had higher margins.,hardware,2026-01-20 16:48:02,13
AMD,o17f0vo,"> However, the report highlights that NVIDIA's pricing for GDDR6 and GDDR7 memory is significantly lower than AMD's.   Nowhere in the linked article does it say anything close to this, just that nvidia remains cheaper despite a recent price increase.",hardware,2026-01-23 08:35:16,2
AMD,o0ox8u1,Throwback to ~2 days ago when that dude was talking about how AMD (might have been r/AMD or this sub) wasn't going to fuck over consumers as much as Nvidia if given the chance.   -  It was a simpler time...,hardware,2026-01-20 16:36:41,13
AMD,o0taego,wasnt the narrative that Nividia does not ship memory anymore? I guess perpetually outraged were wrong yet again.,hardware,2026-01-21 06:40:14,4
AMD,o0pkvrm,I thought we heard last week or so that NVIDIA isn't even sourcing memory for their AIB's anymore? Are they still or they just haven't started yet?,hardware,2026-01-20 18:25:18,4
AMD,o151xsy,Nvidia is firing on all cylinders.,hardware,2026-01-22 23:28:07,1
AMD,o0qakrk,"I read somewhere else (can't remember where) that Nvidia is currently absorbing the GDDR7 price hikes, but this would not be a long term solution. I'm guessing they are going to cut back heavily on supply to the point you can't buy the GPUs anyway, while slowly increasing the prices of the remaining skus they keep supporting.   AMD are being more reactive, I'm guessing they are betting on having a lower price ceiling for GDDR6 than Nvidia's GDDR7, so while they'll potentially take more of a hit in the immediate term, there will be long term gains when they are still delivering similar volume of product to market at only a moderate price premium. Time will tell which strategy pays off.",hardware,2026-01-20 20:22:50,0
AMD,o0pchdi,A meaningless statement without numbers to back it up.  The notion that nVidia is charging less for a given quantity of GDDR7 than AMD is for the same quantity of GDDR6 is simply not believable.  We don't even know if they're talking about the same amount of memory.  It's even more meaningless if they are comparing the BOM kit costs for an 8GB card to those of a 16GB card.,hardware,2026-01-20 17:47:15,-8
AMD,o0phr6w,Didn't Nvidia just say last month they stopped supply of memory kits to AIBs?,hardware,2026-01-20 18:11:12,-5
AMD,o0p3aaa,"most likely Nvidia is subsidizing the cost for gaming cards since it cares about gamers unlike Intel/AMD's focus on AI, otherwise it makes no sense for gddr7 to be cheaper than gddr6.",hardware,2026-01-20 17:04:38,-15
AMD,o0oyyrk,It also helps Jensen personally has fried chicken and drinks with other CEOs.,hardware,2026-01-20 16:44:41,85
AMD,o0s075w,"You're forgetting that nVidia sells itself, while for AMD GPU's you oughta fork quite a bit on Waifu art to sell them",hardware,2026-01-21 01:37:36,6
AMD,o0pcn4j,"> Tech media can make you feel like they're a 50-50, but reality shows it's 95-5  Closer to 85-15 by 2025 revenue.",hardware,2026-01-20 17:47:58,35
AMD,o0pi1hg,> Ofcourse Nvidia can afford cheaper VRAM simply because they buy them in much bigger quantity  Wouldn't have to be like that. AMD is just bad at what they are doing. Why on earth are they for example not teaming up with MS or Sony for GDDR contracts?,hardware,2026-01-20 18:12:29,14
AMD,o0pcvmq,Wonder how much of that is due laptops and pre built PCs,hardware,2026-01-20 17:49:03,4
AMD,o0tbmmb,"Which is kinda funny, since AMD choose GDDR6 specifically because it was supposed to be cheaper.",hardware,2026-01-21 06:50:46,5
AMD,o0tbspx,now we only need to put the stupid rumous about Nvidia no longer making gaming GPUs to bed. It was a single anonymous post on a chinese forums yet every tech site repeated it as gospel.,hardware,2026-01-21 06:52:13,8
AMD,o0pp70h,"Given that RDNA 4 is pretty optimized to be easy to build I am inclined to say the former, the latter doesn't really jibe with the strategy in the arch design.",hardware,2026-01-20 18:44:36,3
AMD,o0qk8nb,This is exactly why it's not cheaper. AMD has no pricing power to compete on price.,hardware,2026-01-20 21:07:15,9
AMD,o0p3a7i,What a stupid take.,hardware,2026-01-20 17:04:37,28
AMD,o0rpv3i,"100% viable. It would just make Nvidias gaming margins fall, which is fine as they have plenty of margin to spare.  Nvidia has cash to burn and they would be willing to take a margin hit as a temporary measure to keep revenues and market share high.  Wouldnâ€™t affect their bottom line, a few hundred million or even a billion isnâ€™t much at all to them these days.",hardware,2026-01-21 00:38:58,1
AMD,o0pgy86,I believe it since Jensen is a gamer himself.,hardware,2026-01-20 18:07:36,6
AMD,o0ppu5p,"Not to mention, it's quite possible that GDDR6 is both more plentiful *but* also more expensive as AMD has to fight with other manufacturers for it, versus cheap but spotty supplies of GDDR7 because only nVidia is using it in any great volume ATM, IIRC?",hardware,2026-01-20 18:47:28,-8
AMD,o0pku7v,"That was just a rumor from some guy, people just ran with it to make clickbait.",hardware,2026-01-20 18:25:06,21
AMD,o0tbwzy,They didnt. Someone made that up and everyone repeated it while doing zero due diligence. Welcome to modern journalism.,hardware,2026-01-21 06:53:14,3
AMD,o0tmazi,Redditors absolutely take RUMORS as fact and the life of me I can't figure out whY.,hardware,2026-01-21 08:28:20,3
AMD,o0qbqsz,Apparently only applies to the 5060ti **16gb** and 5070ti kits as of Jan 5th.  Everything else is still being bundled.,hardware,2026-01-20 20:28:14,4
AMD,o0p3yfj,"Yea, definitely Nvidia cares about gamers /s  Check their revenue streams",hardware,2026-01-20 17:07:47,15
AMD,o0ozk33,He paid for those hotpot meals,hardware,2026-01-20 16:47:26,41
AMD,o0p48r4,wow Jensen flew to Korea to protect gamers from high memory price.  Hope more CEOs are like Jensen and not only care about AI.,hardware,2026-01-20 17:09:07,18
AMD,o0p8t74,"And it also helps that Nvidia has a vastly superior product line-up than AMD.  RDNA 4.0 is just starting to catch-up with Nvidia, and even then you're mostly relying on mods like OptiScaler to match the now well-established DLSS.  The only reason Radeon division is still alive is because of consoles and, to a lesser extent, handhelds.  Intel is already trying to chip away at AMDâ€™s market share in the handheld space. As for consoles, an Intel + Sony or Microsoft collaboration on a next-gen console SoC could potentially send Radeon the way of the dodo.",hardware,2026-01-20 17:30:21,4
AMD,o0qtyou,I donâ€™t think they serve fried chicken at the Dennyâ€™s Jensen used to serve,hardware,2026-01-20 21:51:55,1
AMD,o0phzlz,Doesn't that include console sales for AMD?,hardware,2026-01-20 18:12:15,23
AMD,o0tb9d8,92-8 according to latest sales reports.,hardware,2026-01-21 06:47:37,1
AMD,o0qb7gh,And Steams hardware survey. AMD has made huge strides in the last two years.,hardware,2026-01-20 20:25:47,-11
AMD,o0tbk64,"AMD does not provide memory for the consoles, so they have no real inventive to team up there.",hardware,2026-01-21 06:50:10,4
AMD,o0rqu97,"I mean for one, we donâ€™t know any information about AMDs contracts for Microsoft and Sony, they might very well be doing exactly that.  But also, Microsoft and Sony are large enough that theyâ€™d likely rather source  their own memory than to give AMD extra margin.",hardware,2026-01-21 00:44:17,1
AMD,o0rh7rg,Obviously mostly because those are the majority of the consumer market,hardware,2026-01-20 23:52:03,2
AMD,o0sb6lp,I assume they're asking why nvidia isn't cheaper then,hardware,2026-01-21 02:40:10,0
AMD,o0x5ng7,yoo hold on 150$ less for the same performance is not enough?,hardware,2026-01-21 20:26:52,0
AMD,o0ppeup,"Yeah, because while nVidia may be able to land that GDDR7 for you cheaper, that doesn't ensure there will be a supply available for one thing. Could well be that there's more GDDR6, but it's having to be fought for harder.",hardware,2026-01-20 18:45:35,3
AMD,o0p6aem,Why is it stupid? AMD or at least Radeon hasn't done anything close to what Nvidia has done,hardware,2026-01-20 17:18:40,-23
AMD,o0rrigf,"I'm not sure the shareholders would be of that mind. The other way of looking at it is that Nvidia has market share to burn in the consumer GPU market. Why take a hit to margins when they are comfortably ahead in market share? Why care about such a minor revenue stream when they are supply constrained on datacenter? There is no benefit to Nvidia in ""doing right by gamers"" because gamers have already shown that they will keep buying Nvidia no matter how expensive their products are.",hardware,2026-01-21 00:48:01,-1
AMD,o0ppvv3,And it got upvoted here as fact anywaysâ€¦,hardware,2026-01-20 18:47:41,17
AMD,o0tc900,15 billion from gaming. Seems like they got the market cornered.,hardware,2026-01-21 06:56:09,5
AMD,o0p4msf,what do you mean?  Gaming is only 8% of their revenue and Nvidia is still making sure not to pass the high memory price to gamers.  If they really don't care about gaming they could have easily charge more like Intel/AMD.,hardware,2026-01-20 17:10:57,14
AMD,o0p622u,Gaming still makes Nvidia well over 10 billion a year. Is going to hit 15 billion+ this fiscal year.  They dominate the market and its their mass market product reaching million  Of course care about gaming a lot,hardware,2026-01-20 17:17:35,39
AMD,o0py5gk,"Hope you're sarcastic, Nvidia's server division produces much more revenue than the consumer market.  Also Jensen doesn't care about anyone except his server contracts. He's actually happy AI is going to make people jobless.",hardware,2026-01-20 19:24:56,1
AMD,o0panl2,That's not happening. Sony isn't gonna let Project Amethyst be for nothing and Microsoft already affirmed their commitment to AMD.  And ever since the driver override was unlocked OptiScaler has become less important.,hardware,2026-01-20 17:38:54,14
AMD,o0pbiss,"Redstone and FSR4 is already great, and it will only get better. DLSS is a fad and I'm predicting that in the future nvidia will put those features behind a subscription.",hardware,2026-01-20 17:42:52,-15
AMD,o0s6hzn,I mean youâ€™d have to include the switch on Nvidias side too.,hardware,2026-01-21 02:13:36,5
AMD,o0pj8ea,No it doesn't.,hardware,2026-01-20 18:17:52,-5
AMD,o0rh3q4,CPU yes gpu no,hardware,2026-01-20 23:51:26,10
AMD,o0qdyf1,"in hw enthusiasts forums amd platforms especially 9070xt cards and x3d cpus are by far the most popular. Sweclockers had a survey and 9800x3d was the most used cpu compared to all intel ones, and 9070xt was more popular than 5050 to 5080 combined.",hardware,2026-01-20 20:38:31,-5
AMD,o0sjp6x,Because there is no pricing power from competition to make it lower prices. AMD would have to sell at a loss to make its GPUs more enticing. And force Nvidia to lower prices. But they are not willing to do that against the company 10 times the size.  A CEO of a company has a fiduciary duty to maximize shareholder return. So Nvidia (or AMD) will charge as much as they can.,hardware,2026-01-21 03:30:09,2
AMD,o0seamu,"exactly, why nvidia isnt cheaper",hardware,2026-01-21 02:57:55,2
AMD,o0tc66p,Jensen never cared about shareholder wants. He pushed CUDA against shareholder protests all they way back in 2006 and its what ended up turning them into the most valuable company in the world.,hardware,2026-01-21 06:55:27,3
AMD,o0rsu06,"Nvidia as a company has a dual commitment. That to their executives to make money, and that of excellence to deliver a product and achieve market goals.   Taking a hit to margins means keeping your customers happy and keep them buying Nvidia.  Because any customer that chooses AMD this generation, is one thatâ€™s likely to stay AMD next generation. So itâ€™s lost future revenue as well.   Marketshare isnâ€™t this measure of whoâ€™s winning for a particular generation. It has tremendous â€œstickinessâ€ and momentum where any competitor would have to have the better product for multiple generations in order to even get minority of the market.   Thatâ€™s why youâ€™ll notice Intel taking huge margin hits to keep their market share in server and OEM. Because once those customers switch to AMD, youâ€™re going to have to offer them a much better deal to get them to switch back.   And itâ€™s why AMD Epyc still isnâ€™t majority market in server, because of the market momentum, and Intel taking margin hits to prevent it.",hardware,2026-01-21 00:55:27,1
AMD,o0tm6jq,"Shareholders only care about making money.     I don't know why people have a mystical movie like view on ""shareholders"" when they themselves are",hardware,2026-01-21 08:27:07,0
AMD,o0qmv49,"the problem is actually ppl like you, who lacks common sense and just believes everything  you read on the Internet and even changed the original rumor from ""some guy saying it"" to ""Nvidia just say last month"".",hardware,2026-01-20 21:19:22,-5
AMD,o0pb6ql,Nvidias margins on products have always been higher than AMDs. They just have such a large volume that their actual costs for production are lower since they are able to negotiate better deals.,hardware,2026-01-20 17:41:21,12
AMD,o0p5dlz,Intel is charging more for GPUs?,hardware,2026-01-20 17:14:25,5
AMD,o0pewa6,> still making sure not to pass the high memory price to gamers  Wait until their existing memory supply contracts are renegotiated and/or memory stock runs out and get back to me.,hardware,2026-01-20 17:58:15,0
AMD,o0pt8kp,"It is also the most stable part of their business and a safe fallback. The future of AI datacenter demand is extremely uncertain right now, while gaming demand can be easily and reliably predicted, and is large enough to keep them afloat should the worst happen",hardware,2026-01-20 19:02:30,19
AMD,o0p6wrm,"well if Nvidia raised the price they would have earn more from gaming, caring about gaming doesn't mean caring about gamers.  Jensen cares about gamers so he flew to Korea to make sure gamers don't pay more.",hardware,2026-01-20 17:21:32,-8
AMD,o0tasns,"AI replacing jobs and leaving people to hobbies has been like the holy grail of futurists for as long as AI as a concept existed. The bad thing is how it is being handled, not that its happening.",hardware,2026-01-21 06:43:38,3
AMD,o0pfvlm,>  Sony isn't gonna let Project Amethyst be for nothing and Microsoft already affirmed their commitment to AMD.  Mostly because nvidia doesn't do semi-custom.  And until Intel can get a package as complete and competitive as AMD's consoles won't go anywhere else.,hardware,2026-01-20 18:02:42,12
AMD,o0tawsp,"I think the implication would be more a console after that, so think PS7.",hardware,2026-01-21 06:44:39,1
AMD,o0punr6,"Calling DLSS a fad is an opinion I definitely disagree with, but, uh, I guess I can see how someone might think that, and it might be an actual opinion  But, believing DLSS to be a fad while also simultaneously believing that FSR is not a fad, is certifiably schizo",hardware,2026-01-20 19:08:50,13
AMD,o0pmqfp,I think you might have forgot to insert in a /s here,hardware,2026-01-20 18:33:35,11
AMD,o0t7lbu,"The switch 2 has very little impact considering how small and dirt chip Samsung's 10nm node from 2018.  Meanwhile, the consoles use larger and newer TSMC nodes, which are more expensive.  The PS5 Pro itself uses a TSMC N4 node. Which is more advanced than the RTX Blackwell, which uses 4N (A custom TSMC 5nm process).",hardware,2026-01-21 06:16:42,5
AMD,o0psbc5,If we are talking AMD earnings call (which are only hard numbers we have) then yes their ''gaming'' revenue does include semi-custom aka consoles.,hardware,2026-01-20 18:58:25,31
AMD,o0tbek3,"I find it quite ironic that overclockers forum prefers 9800x3D. A great CPU mind you, but one thats intentionally downclocked to keep thermals from affecting the stacked memory.",hardware,2026-01-21 06:48:50,2
AMD,o0svmtt,Because the demand meets the supply at the current prices.,hardware,2026-01-21 04:47:05,0
AMD,o0s4op9,"The server market is far more competitive than consumer graphics though, I don't foresee any action by Nvidia that could shift them out of 90%+ market share. Time will tell I guess, but I will be surprised if Nvidia keep eating the increase in GDDR7 prices past the second half of 2026.",hardware,2026-01-21 02:03:13,2
AMD,o0tmh0k,"On datacenter above all else. That's where margins have little effect on revenue. Everyone says Nvidia charges 2X more per accelerator than AMD, how would that not appear as huge margins?",hardware,2026-01-21 08:29:56,1
AMD,o0p6gv3,"Intel is even worse, not even releasing new gaming GPUs and only focusing on AI GPUs.",hardware,2026-01-20 17:19:30,-3
AMD,o0p7jx2,This has been debunked by the CEO of Gigabyte.  Nvidia still ships GPUs with memory and never stopped doing it,hardware,2026-01-20 17:24:30,15
AMD,o0p785q,That has been debunked already? It's a false rumour. The one you are reading rightnow is literally about VRAM + die bundle,hardware,2026-01-20 17:23:00,13
AMD,o0pay8s,"I donâ€™t think anyone is saying that Nvidia cares about gamers on an interpersonal level, nvidia cares about gamers as a bag of money that diversifies their revenue source.",hardware,2026-01-20 17:40:16,18
AMD,o0ppaar,"Nvidia only benefits if its them raising their prices, if its samsung raising their prices and Nvidia eating the cost or raising the prices by what samsung did they don't benefit in any way",hardware,2026-01-20 18:45:01,6
AMD,o0phu1p,"Even if Nvidia goes Semi Custom next gen is locked for AMD. If Nvidia goes Semi Custom it still won't happen. Plus, it's just easier and cheaper to make an APU exclusively from AMD because they're hands down the best when it comes to APUs. It's be a pain to get Intel and Nvidia to work well on a single chip at least until we see the fruits of their recent partnerships. With FSR4 basically being confirmed for Sony consoles there's just no incentive to go with Nvidia. Ray Tracing is gonna improve by bounds next generation either way for them and FSR4 is good enough for 4K. Nvidia wont accept less money than AMD either. AMD actually has fairly small profit margin on consoles and Nvidia doesn't have anything less than large margins in their devices.  I would also mention that unlike pre built companies Sony doesn't care about Nvidias brand. Console gamers don't care what's in their consoles so long as it runs games well and AMD does that exceptionally.",hardware,2026-01-20 18:11:33,9
AMD,o0tltzr,"Looking at the username, person may be trolling",hardware,2026-01-21 08:23:50,1
AMD,o0s1lhs,"That's' their whole schtik, all their comments are like that.",hardware,2026-01-21 01:45:39,6
AMD,o0t8f84,"I have been terrible misled by Hardware unboxed podcast then, or I remember wrong :(",hardware,2026-01-21 06:23:32,5
AMD,o0pet38,There are rumors that their gpus arent economically viable,hardware,2026-01-20 17:57:52,4
AMD,o0p7pqk,That's good to know. Thank you!!,hardware,2026-01-20 17:25:15,2
AMD,o0pg5oh,if that's the case why didn't they raise the memory price like AMD is doing.,hardware,2026-01-20 18:03:59,-8
AMD,o0pqzi6,"Intel is now fully competitive in APU space and they have the benefit of their own fabs, sure PS6 is definitely going to be AMD because its too far in its development stage to make such drastic change but I would not be surprised if it was Intel hardware in consoles gen after ps6 (if there is one that is).",hardware,2026-01-20 18:52:34,4
AMD,o0q41lb,"Like, the only ones to go back have been Nintendo and that's one Nvidia Company Culture Moment from jumping ship to probably AMD and using a x64 SOC ala the Deck or Intel if Nintendo wanted to be weird with it, so it's not surprising nVidia's been unusually well behaved there.",hardware,2026-01-20 19:52:16,0
AMD,o0tmeuu,I don't think there's any real difference between pretending to be an idiot and actually being one,hardware,2026-01-21 08:29:22,1
AMD,o0tccpd,"which is not something that really matters when answering the question of ""does Intel care about gamers"".",hardware,2026-01-21 06:57:02,2
AMD,o0pgg6u,"Why would one lead to the other? Sometimes you make more money lowering prices or keeping them the same than raising them. In fact, thatâ€™s quite often - every sale ever is that calculation.   Whether or not it is the case depends on the demand curve for your product.",hardware,2026-01-20 18:05:18,12
AMD,o0qxrho,"Because AMD priced away the bulk of their margin, and can't afford to absorb higher memory costs.",hardware,2026-01-20 22:09:49,1
AMD,o0pscqz,PS6 would mark the 3rd successive generation of AMD products in Sony consoles (and I believe the 4th for Xbox). Intel hasn't ever made something like Strix Halo. Console APUs are an entirely different beast to regular ones. Intel has to have something like Strix Halo to show it can truly compete.   Companies won't trade away years of partnership just because another company has good products now. Intel might genuinely have to take a loss to get APU contracts. AMD has also never had an issue supplying the volume Xbox and PS need. They're ridiculously efficient with those chips so Intel's foundry services aren't as big a selling point unless they undercut costs pretty substantially.  I'd be very surprised if Xbox or Sony went with Intel unless AMD completely drops the ball.,hardware,2026-01-20 18:58:35,4
AMD,o0tamcx,"I agree with you but i would like to point out not every sale is that. Many sales are getting rid of old inventory because storage space is limited and you want to bring in new, more profitable items in.",hardware,2026-01-21 06:42:07,0
AMD,o0pucxs,"Intels Panther lake is very close to Strix halo when it comes to performance while being cheaper/smaller chip and having superior upscaling and frame gen technology (obviously both of them get clapped by Apple in raw perf) but when it somes to consoles what matter is price for a performance, which Intel will be able to edge AMD as intel has their own fabs.",hardware,2026-01-20 19:07:28,3
AMD,o0px76g,"Panther Lake isn't close at all. It's about half the performance. It's much faster than their other igpus but Strix Halo is in another tier. Intel compares their igpu to a power limited laptop 4050 (and they lose still by like 10% iirc). AMDs Halo is compared to desktop 4060 in quite a few cases. That's a huge difference.  Yes, Panther Lake is a great igpu. It's not close at all in raw performance. It's got more advanced technology for sure and its RT performance is very impressive for what it is. But it's also a much newer architecture and it's lucky for them that they are in a transition period for AMD where AMD is unable to put their latest architecture in mobile chips.  Intel's fabs have lost them money. I'm not sure they're gonna bring better value for money. AMDs architecture is just better and they can use less advanced nodes if they so wish. Hell, if they're really desperate they'll go to Samsung and give them a big order. Samsung would gladly discount chips to dirt cheap levels for a major customer.",hardware,2026-01-20 19:20:28,5
AMD,o0djtyw,Fwiw should be noted that AMD owns the (small) market for scientific computing that needs that level of precision. Blackwell basically canâ€™t do FP64.,hardware,2026-01-18 23:02:40,257
AMD,o0czlcs,Note: I've provided a descriptive title because the original title completely failed to convey what the article is about.,hardware,2026-01-18 21:23:32,78
AMD,o0fyxqe,"There are two separate arguments listed in this article:  1. It only works for well-conditioned problems 2. It only does DGEMM which is a small part of scientific computing  The first one is not really warranted. Sure, if you can do with e.g. the equivalent of ""55 bit"" precision you get even more of a performance jump, but you can still emulate at *strictly higher* than 64 bit precision on the latest NV HW and get out ahead of their native FP64 performance. You won't get the exact *same* result, but it would be strange for an algorithm to rely on that rather than the numerical accuracy of it.  The second concern is far more relevant. While DGEMM emulation can be done in a self-contained way, there's not really any production-ready equivalent of that out there for all the other math that a HPC algorithm will likely be doing.",hardware,2026-01-19 08:12:28,22
AMD,o0djjzx,Canâ€™t wait for AI to find a cure for cancer using FP4,hardware,2026-01-18 23:01:16,63
AMD,o0lrpmo,"They don't argue against it, they argue for doing it in an IEEE compliant manner which is perfectly sensible.",hardware,2026-01-20 03:38:02,6
AMD,o0fqg47,Of course they would. It's in their best interests.     Notice I didn't say whether they were right or wrong,hardware,2026-01-19 06:57:25,7
AMD,o0hd4ar,"> Double precision floating point computation (aka FP64) is what keeps modern aircraft in the sky, rockets going up, vaccines effective, and, yes, nuclear weapons operational. But rather than building dedicated chips that process this essential data type in hardware, Nvidia is leaning on emulation to increase performance for HPC and scientific computing applications, **an area where AMD has had the lead in recent generations.**  I missed that AMD",hardware,2026-01-19 14:38:05,5
AMD,o0h1zwe,"I can appreciate the argument they make here but they are wrong.  However, sometimes you have to make mistakes to learn - a gold rule of technological advancement the informed understand only too well.",hardware,2026-01-19 13:37:04,-2
AMD,o0obeby,There is a niche where they are right. But if mixed precision work in 99% other areas they are wrong,hardware,2026-01-20 14:52:42,0
AMD,o0dqd3y,">the (small) market for scientific computing that needs that level of precision.  Kinda insane that this used to be one of the largest markets for HPC GPUs. And it's not like this market has disappeared either, AI is just so big that it dwarfs it by comparison.",hardware,2026-01-18 23:35:57,160
AMD,o0m1ot6,"After what I spent today debugging, I'm not sure AMD seriously wants to be in that market. Some of the bessel function code in the AOCL libraries is very fast, and not fully precise.",hardware,2026-01-20 04:37:05,3
AMD,o0j3ast,>Blackwell basically canâ€™t do FP64.  That's not true. Blackwell ultra can't but that's purposely built foor AI workloads,hardware,2026-01-19 19:21:23,2
AMD,o0g0e7n,"It's an incredibly badly written article esp. considering it's The Register..  ""*FP64 is unmatched in its dynamic range, capable of expressing more than 18.44 quintillion (2\^64) unique values*""  Er no... there are precisely 2\^64 bit combinations, and given that a 0 sign bit and 0x7FF as the exponent designates NaN for any fraction values, then 2\^52 of the entire 2\^64 values are NaN which are arguably not unique values (qNaN and sNaN and NaN != NaN not pedantically withstanding).  And unmatched ?? IEEE754 also defines FP128 and FP256 even if direct hardware support is less common",hardware,2026-01-19 08:26:00,33
AMD,o0dqbpy,Best they can do is AI pedo slop,hardware,2026-01-18 23:35:45,63
AMD,o0dr4l1,It has to be 1 of the 16 choices right??,hardware,2026-01-18 23:39:56,12
AMD,o0hhazz,"As long as we get the results we want, who cares what floating-point format is used?",hardware,2026-01-19 14:59:19,3
AMD,o0h6l3v,Sorry we're too busy building out data centers to have Gork put people in bikinis. Priorities!!,hardware,2026-01-19 14:03:02,1
AMD,o0lw2l4,"That's not the full story. Nvidia claims FP64 performance doesn't need to improve with each generation, because they claim that the Ozaki scheme can exploit the ever increasing number of low-precision tensor cores and make up for the lost performance.  But AMD argues that the Ozaki scheme is only for matrix multiplication, and that many of the HPC applications they have studied don't make heavy enough use of it to result in a net performance gain with emulated FP64.  There's a whole section in the article about the applicability of the Ozaki scheme:  > Even if Nvidia can overcome the potential pitfalls of FP64 emulation, it doesn't change the fact that the method is only useful for a subset of HPC applications that rely on dense general matrix multiply (DGEMM) operations. > > According to Malaya, for somewhere between 60 and 70 percent of HPC workloads, emulation offers little to no benefit.  > > ""In our analysis the vast majority of real HPC workloads rely on vector FMA, not DGEMM,"" he said. ""I wouldn't say it's a tiny fraction of the market, but it's actually a niche piece."" > > For vector-heavy workloads, like computational fluid dynamics, Nvidia's Rubin GPUs are forced to run on the slower FP64 vector accelerators in the chip's CUDA cores.  These facts don't change just because emulated FP64 becomes IEEE-compliant. AMD states that they are presently studying **if** making FP64 emulation complaint can make the Ozaki scheme usable for more applications. If their research concludes that the Ozaki scheme could be applied more broadly, then those applications that benefit from it can obtain improved performance, not that hardware FP64 should be replaced.",hardware,2026-01-20 04:02:58,3
AMD,o0gimql,"I mean the performance numbers speak for themselves, and the gap will widen even more with the dedicated solution they are launching (MI430X). Nvidia obviously could make something similar, but they don't want to, since as usual AI makes them much more money.",hardware,2026-01-19 11:15:56,11
AMD,o0lty5s,That AMD is still right here in the present day.,hardware,2026-01-20 03:50:28,7
AMD,o0j3s1c,It's mostly marketing bs. AMD says they do but only really refer to HPL which is a FP64 benchmark.,hardware,2026-01-19 19:23:32,5
AMD,o104qgt,"> Double precision floating point computation (aka FP64) is what keeps modern aircraft in the sky, rockets going up, vaccines effective, and, yes, nuclear weapons operational.  None of this is true.  You don't need floating point at all for nuclear weaponry or missile guidance.  You don't need *digital* anything.",hardware,2026-01-22 06:29:47,0
AMD,o0hht7x,They're not wrong. They're offering native FP64 for customers who want it.,hardware,2026-01-19 15:01:50,12
AMD,o0e19o1,"It doesn't help matters that it's a bit of a stagnant market. The bulk of orders for HPC equipment are to governments for academic research - a group that hasn't exactly been getting funding increases as of late. So even without AI weighing in on the scale, it's not a fantastic market for taking risks on building big chips.  This also means that, at least in the west, the US government effectively makes the market with its orders. Which coupled with the DOE's preference to buy from the underdog in order to ensure there is viable competition, means that there's no incentive for NVIDIA to even go after this market these days, as they would never win a supercomputer contract.",hardware,2026-01-19 00:33:53,93
AMD,o0o8kz4,I bought tons of HD 7970 and Radeon vii for my PhD. I dreamed of owning a Nvidia compute card. I spent so long trying to squeeze out performance of older cards with their higher fp64 capabilities. Good times,hardware,2026-01-20 14:38:27,3
AMD,o0dwoal,Precision and accuracy rejected in favour of guessing and vibes. Sums up the state of things everywhere at this point.,hardware,2026-01-19 00:09:20,15
AMD,o0ghv4x,"> It is a well understood and solved problem  It is if you just need FP64 ""support"", but is it if you actually need high FP64 performance?",hardware,2026-01-19 11:09:05,18
AMD,o0e56j4,">Intel's first generation dGPUs, Xe-HPG, were emulating FP64.  There is no need to specify first generation, there is not any other generation of Intel data centre GPUs that ever existed outside of a lab.",hardware,2026-01-19 00:54:26,4
AMD,o0nlahf,"lol yeah AMD software is dogshit, thereâ€™s a reason itâ€™s free",hardware,2026-01-20 12:22:01,-3
AMD,o0i73y0,"yeah, the author apparently doesn't know the difference between unsinged integer and floating point mantissa representation.",hardware,2026-01-19 16:57:35,10
AMD,o0g2937,That's basically the same number.,hardware,2026-01-19 08:43:27,-7
AMD,o0dwthw,Nice username,hardware,2026-01-19 00:10:09,3
AMD,o0edejt,300+ billion 1 of 16 choices one after the other.,hardware,2026-01-19 01:40:37,13
AMD,o0fym1f,"Depends, read somewhere that some frameworks differentiate between +0 and -0 so you are left with 15 choices, ezpz",hardware,2026-01-19 08:09:29,1
AMD,o0hidxa,As long as you don't want your result to be 3,hardware,2026-01-19 15:04:43,1
AMD,o0mn19w,And to simulate protein folds among other things presented every GTC,hardware,2026-01-20 07:19:48,2
AMD,o0n3i9t,"They are perfectly correct. The Ozaki algorithm absolutely is only for matrix multiplication, that's the entire extent of it. That fact doesn't change either, and it still doesn't mean they are arguing against it in any way at all.",hardware,2026-01-20 09:52:51,6
AMD,o0n0ecp,"so a FP64 benchmark would be good to test double precision floating point computation (aka FP64) for HPC and scientific computing applications, right?  what am I missing?",hardware,2026-01-20 09:23:19,2
AMD,o0j54zy,So does nvidia. NVIDIA offers both native FP64 (with far better software support) and emulated FP64 with far higher performance.   Blackwell offers \~37TFLOPS in native FP64 and up to >200TFLOPS in some scenarios with emulated FP64.,hardware,2026-01-19 19:29:47,-4
AMD,o0e75h4,"nvidia did get a supercomputer contract recently. https://nvidianews.nvidia.com/news/nvidia-oracle-us-department-of-energy-ai-supercomputer-scientific-discovery. the government enforces a strict vendor rotation system, and buying the underdog just happens to be a side effect of that.  x86-64 amd and intel supercomputers recently came online, so it makes sense that the next one will either be aarch64 or ppc64le. if a vendor offers a compelling rv64gc/rva23 option, then they will probably put that on the roadmap as well.",hardware,2026-01-19 01:05:11,39
AMD,o0eh374,"> means that there's no incentive for NVIDIA to even go after this market these days, as they would never win a supercomputer contract  What? Nvidia has and continues to win such contracts. At most, the government seems to balance between vendors, not strictly favor the underdog. And practically, AMD's the only alternative to Nvidia right now.",hardware,2026-01-19 02:00:55,6
AMD,o0pngcu,"There's also the fact that a lot of researchers are finding ways to get neural networks to approximate EXACT calculations with way less compute.   Imagine 20 years of calculations being obsoleted with one, better model that does it with 10,000x lower cost.",hardware,2026-01-20 18:36:48,1
AMD,o0gmarr,"The simulation being talked about is bit level accurate and is not AI based. Its not even from Nvidia, it was only developed on their GPUs by Japan  Google Ozaki scheme",hardware,2026-01-19 11:47:09,0
AMD,o0i5sg9,"it depends on the specific kernel's behavior.  If it is a nice streaming use case, then the emulation just acts as a deep software pipeline and you can get some decent performance. Obviously having full HW FP64 will always be faster.  FWIW even double precision (FP64) is not enough for a lot of numeric codes, which is why scalar processors traditionally have used 80 or 96 bit FPUs.  E.g. a lot of these supercomputers use very beefy x86 (or ARM/PPC) CPUs on each node for extended precision support, contrary to common belief some scientific codes don't run most of their compute on the GPU.",hardware,2026-01-19 16:51:37,2
AMD,o0ezuiu,"There was a need to specify generation, because Xe2-HPG has support in HW for FP64 and it is out in the market.",hardware,2026-01-19 03:43:50,15
AMD,o0ox2z1,Intel's versions are also free?,hardware,2026-01-20 16:35:55,7
AMD,o0qcguy,Nvidia's libraries are also free.,hardware,2026-01-20 20:31:36,5
AMD,o10qnc2,The author probably ChatGPTâ€™d for some technical sounding things to say,hardware,2026-01-22 09:48:39,2
AMD,o0g3i52,"My point is that it can't ""*express* ***more than*** *2\^64 unique values*""... The Register always used to pride itself on having journalists who understood the technical details of what they were writing about enough to express it in simpler but accurate terms, and this article is pretty much doing the opposite",hardware,2026-01-19 08:55:13,14
AMD,o0ji1h6,Same,hardware,2026-01-19 20:29:50,4
AMD,o0nqt2m,Alphafold is years old bud,hardware,2026-01-20 12:59:04,1
AMD,o0r71o0,"> The Ozaki algorithm absolutely is only for matrix multiplication, that's the entire extent of it.  I'm sorry, but was I claiming otherwise? I'm genuinely perplexed as to what your objection is.  > ...and it still doesn't mean they are arguing against it in any way at all.  From the article (emphasis mine):  > Emulated FP64, which is not exclusive to Nvidia, has the potential to dramatically improve the throughput and efficiency of modern GPUs. But not everyone is convinced. > > ""It's quite good in some of the benchmarks, it's not obvious it's good in real, physical scientific simulations,"" Nicholas Malaya, an AMD fellow, told us. **He argued that, while FP64 emulation certainly warrants further research and experimentation, it's not quite ready for prime time.**  Further more:  > Despite Malaya's concerns, he noted that AMD is also investigating the use of FP64 emulation on chips like the MI355X, through software flags, to see where it may be appropriate. > > IEEE compliance, he told us, would go a long way towards validating the approach by ensuring that the results you get from emulation are the same as what you'd get from dedicated silicon. > > ""If I can go to a partner and say run these two binaries: this one gives you the same answer as the other and is faster, and yeah under the hood we're doing some scheme â€” think that's a compelling argument that is ready for prime time,"" Malaya said. > > It may turn out that, for some applications, emulation is more reliable than others, he noted. ""We should, as a community, build a basket of apps to look at. I think that's the way to progress here.""  It is clear that AMD's position here is that the wider applicability of Ozaki is still unknown.",hardware,2026-01-20 22:57:00,1
AMD,o0nam6q,"Well, you got a point, I phrased that badly.   No, HPL is a politically motivated benchmark with zero applicability in the real world that causes real harm to the HPC world and is horrendously expensive to run.    If you're interested in this, here is a slightly longer explanation (It's a slightly modified copy paste from a different explanation from me, so some things might seem a bit off to you)  It was created to ""prove"" performance claims by manufactures. Back in the day even CPUs had a claimed theoretical max performance, usually in GFLOPS. But what does it even mean and where does this number come from? It tells you how many floating point operations (typically 64 bit) a processor can theoretically do. It's calculated by core count \* clockspeed \* fp operations per cycle. So, it tells you how many operations the CPU can do if every single clock cycle was used to calculate as many operations as theoretically possible per cycle. For modern CPUs, lets use a 7950X as an example: It has 16 cores, runs at a base frequency of 4.5GHz, has two 256 vector ALUs and supports FMA (fused multiply add, performs two operations in one cycle), so it's two operations per cycle on 8 (2x 256bit = 8x 64bit) FP per core on 16 cores 4.5G times per second. So 4.5 x 16 x 8 x 2 = 1152GFLOPS. The same math applies to GPUs where the performance claim is usually still marketed.   HPL is a benchmark that was created to get as close as possible to this number. It's designed to minimize overhead, memory, networking etc, and use 100% speedup from wider vector units etc. And not only that, every single manufacturer has their own optimized HPL version. On intel CPUs you run an intel optimized version on AMD an AMD optimized version etc. To get back to the example, [here](https://wp-cdn.pugetsystems.com/2022/08/hpl-zen4.png) is a test from puget systems with a result of 1011GFLOPS which means a \~87% efficiency.    And that's basically the thing, the more time you spend optimizing, the closer you get to the optimum which is about 85-95% of theoretical maximum. And not only that, if you spend enough time optimizing and distributing load, you can even throw in a GPU and add the performance to the total performance. It's essentially just about how much time you're willing to invest into it.   The problem is, none of that translates into any real world performance. This [chart](https://www.pugetsystems.com/pic_disp.php?id=68180) shows how random many results seem to be (intel dominates there because of their early AVX 512 adoption). In the real world, especially in HPC the software is far more complex and more I/O, network and memory dependent than pure compute bound.    There isn't a single common HPC workload that scales with performance as HPL does, none.   The benchmark exists for the top500 list and to ""validate"" the performance claims. It's for politics, to be able to say ""our supercomputer is faster than yours"".",hardware,2026-01-20 10:57:21,3
AMD,o0jbynw,> Blackwell offers ~37TFLOPS in native FP64 and up to >200TFLOPS in some scenarios with emulated FP64.  Only when doing matrix x matrix multiplication. Matrix x vector or anything else would fall back to native.    It all depends on the needs of the customer ðŸ¤·â€â™‚ï¸,hardware,2026-01-19 20:01:17,12
AMD,o0ee5ha,New ppc deployments? Which?,hardware,2026-01-19 01:44:50,9
AMD,o0h0ggw,"Except:  > The Lux AI Cluster will be deployed at ORNL in 2026, and Discovery will be delivered in 2028.  [ORNL, AMD and HPE to deliver DOEâ€™s newest AI supercomputers: Discovery and Lux](https://www.ornl.gov/news/ornl-amd-and-hpe-deliver-does-newest-ai-supercomputers-discovery-and-lux)  > When fully deployed, the Lux and Discovery systems will represent a combined $1billion investment of private and public funding which will enable the DOE to build a secure, federated and standards-based infrastructure for sovereign U.S. AI and science.  [AMD Powers U.S. Sovereign AI Factory Supercomputers, Accelerating an Open American AI Stack](https://www.amd.com/en/newsroom/press-releases/2025-10-27-amd-powers-u-s-sovereign-ai-factory-supercomputer.html)",hardware,2026-01-19 13:27:53,6
AMD,o0gz6ho,What part is vibes exactly? It's just a fact that hardware fp64 has been sacrificed by Nvidia in the pursuit of AI,hardware,2026-01-19 13:20:05,-4
AMD,o0gymem,"I think they mean that hardware fp64 was sacrificed in favor of less accurate formats, primarily driven by AI. Not that the scheme itself is based on AI",hardware,2026-01-19 13:16:36,5
AMD,o0j2vo0,Does SSE/AVX support 80-bit or 96-bit floating-point? I thought only the x87 FPU was 80-bit.,hardware,2026-01-19 19:19:27,2
AMD,o0fi05b,"Well not the data centre gpu where it would really matter (Ponte Vecchio was forced out intelâ€™s anus because of the DOE contract, while Rialto Bridge and Falcon Shores are chopped, and probably Jaguar Shores yet be yet another epic fail)",hardware,2026-01-19 05:48:59,-2
AMD,o0g47o6,"They're not saying so, they're saying you can express more than 18.44 quintillion unique values and that that number is (loosely) given by 2^64.",hardware,2026-01-19 09:01:56,2
AMD,o0po04s,Alphafold is just one of many different generative AI platforms for protein folding research.,hardware,2026-01-20 18:39:15,0
AMD,o0rixib,"He argues that using that approach is not in fact better than using dedicated hardware for fp64. He is correct. Nvidia is sloppily rushing this through in a **non-compliant** manner. That means it is premature, not ready for prime time, similar to their new power cable.  > ""We should, as a community, build a basket of apps to look at. I think that's the way to progress here.""  Progress. Not rejection, not stagnation, not moving backwards. Certainly **not arguing against emulated fp64 at all**, merely not to rush it.",hardware,2026-01-21 00:01:20,1
AMD,o0nape6,"(2/2 I hit the comment length limit)  And that's where the problem starts. I did work at a super computer place at a time where they were looking for a new system. Because most scientific super computers are paid by the state/city/whatever and it's a political thing, HPL is a real problem because politicians want to see results and they only care about HPL and where it lands in the top 500 list.   Thing is that running HPL is horrendously expensive. If you want good results you can easily spend like 2-4 weeks on optimizing and benchmarking, which doesn't sound too bad until you realize that if a supercomputer is used for 5 years, that's 1-2% of the whole lifetime and therefore cost.   Next problem is that GPUs are still not that great in HPC and a lot still runs on CPUs for many different reasons. The place I worked at had zero uses for GPUs as their main software had no GPU support. They had some, mostly for analyzing results (and visualization) that was purely built on CUDA. But GPUs offer like 10x the performance in HPL compared to a CPU socket and are far more cost effective.   But money came from politics and they didn't want a rather small uplift (plans were to spend a lot on networking and storage) and therefore they had to buy some GPU racks and spend less on storage and CPU nodes. And because AMD GPUs were faster at that time in HPL, there was pressure to buy AMD GPUs (which they didn't but they bought a lot of nvidia GPUs).   I can't speak for all supercomputers but at least to me it's not surpring how much more successful AMD is with their datacenter GPUs in the Top500 list (which AMD likes to say for marketing reasons).   Just makes it a bit funny when AMDs only example for the need for native FP64 performance is HPL. I have an idea why.",hardware,2026-01-20 10:58:08,3
AMD,o0jdhxe,It's still nonsense. AMD says there is no point in offering FP64 emulation and that is wrong. There are use cases where it can be useful and in nvidias case it comes at essentially zero cost.,hardware,2026-01-19 20:08:26,-5
AMD,o0eywu1,"thats the thing there havent been any new ppc64le supercomputer deployments we know of. historically there has always been at least 1 ppc supercompting cluster operational at any given time, and a new one is usually brought up when they decommission the old one.",hardware,2026-01-19 03:38:06,16
AMD,o0ez8mg,"IBM is coming up with their new POWER11, but I don't think there are any major supercomputers planned around it.",hardware,2026-01-19 03:40:06,10
AMD,o0huqpt,Ozaki scheme gives exactly the same precison and results as native fp64.,hardware,2026-01-19 16:02:11,-2
AMD,o0jbnvl,"SSE/AVX are handled by FP64 SIMD ALUs.  However, modern X86/ARM/POWER uarchs have superscalar FPUs, so they can dispatch multiple extended precision (80+ bit) FP ops per cycle as well for non data-parallel code paths.  Some uarchs also support fused 64bit FPUs, so they can issue 2 FP64 per cycle, or a single 128bit quadruple precision float.",hardware,2026-01-19 19:59:54,4
AMD,o0g4mmd,"Like I said, incredibly badly written",hardware,2026-01-19 09:05:53,9
AMD,o0rr0g9,"I think you're confused as to what my position on emulated FP64 is. I'm supportive of hardware support for FP64. I'm quite troubled by the direction that NVIDIA is going towards (stagnant or retrograde FP64 performance with each successive generation; and its reliance on the Ozaki scheme as its approach for improving FP64 performance).  >> ""We should, as a community, build a basket of apps to look at. I think that's the way to progress here."" > > Progress. Not rejection, not stagnation, not moving backwards.  The progress that is being referred to here is a call to for more research into the applicability of emulated FP64, not progress in its deployment, given that it is still a known unknown as to whether emulated FP64 is sufficiently applicable as to justify displacement of hardware FP64 or not. This is justified by the preceding context, which you have conveniently refused to acknowledge (emphasis mine):  > It **may** turn out that, for **some applications**, emulation is more reliable than others, he noted. ""We should, as a community, build a basket of apps to look at. I think that's the way to progress here.""  > Certainly not arguing against emulated fp64 at all, merely not to rush it.  Are we reading the same article?  > Even if Nvidia can overcome the potential pitfalls of FP64 emulation, it doesn't change the fact that the method is only useful for a subset of HPC applications that rely on dense general matrix multiply (DGEMM) operations. > > According to Malaya, for somewhere between 60 and 70 percent of HPC workloads, emulation offers little to no benefit.  > > ""In our analysis the vast majority of real HPC workloads rely on vector FMA, not DGEMM,"" he said.  As I've said before, the issue of IEEE compliance cannot change the fact that many HPC applications do not use DGEMM.",hardware,2026-01-21 00:45:14,3
AMD,o0o5ud6,thanks for the information,hardware,2026-01-20 14:24:09,1
AMD,o0kvrn4,"You missed the earlier (a couple of months ago) rumor that Nvidia would drop native FP64 support starting with R100 or whatever it will be called.  Something like the old H100 of course is just fine, but AMD isn't referring to currently available GPUs here.",hardware,2026-01-20 00:41:56,7
AMD,o0jhfve,"Like I said the Ozaki scheme is only for DGEMM. If they need FP64 for other things then maybe native is the way to go. AMD is offering MI430X with enhanced FP64, and the low-precision focused MI450X.  And customers are buying both.  And the emulation isn't an Nvidia only thing. It can be implemented on AMD GPUs if required.    Customers have lots of options.",hardware,2026-01-19 20:26:59,10
AMD,o0lu7zu,"Nowhere do they say there is no point to it, in fact they say they are exploring it.",hardware,2026-01-20 03:52:00,7
AMD,o0hyjt2,"That's not the point. The point is that fp64, which is required for HPC, is now slower because it's no longer natively hardware accelerated.",hardware,2026-01-19 16:19:11,12
AMD,o0jhpta,Did you even read the article?,hardware,2026-01-19 20:28:18,-2
AMD,o0mm5nj,"And when you have working solutions then what's the point? Are they still ""guessing and vibes""",hardware,2026-01-20 07:12:07,0
AMD,o1f15rj,Could DLSS 5 end up being limited to RTX 50 and 60 series if the next big update is built around NVFP4?,hardware,2026-01-24 12:45:44,71
AMD,o1f44wd,"I want to add, that for the widespread adaption of Neural Shading/Rendering the Direct3DÂ Cooperative VectorÂ needs to be finalized and released, and it got [pushed out](https://devblogs.microsoft.com/directx/shader-model-6-9-and-the-future-of-cooperative-vector/) of Shader Model 6.9 that is releasing in Q1 2026 and will arrive in a later version. When its out, it will allow developers to use the MATMUL units of GPUs in the rendering pipline, it will be interesting to see what kind of thing this can be used for, or are we just gonna see texture compression and ML material shaders.",hardware,2026-01-24 13:06:32,29
AMD,o1gaozd,"You can't quite compare RR to regular DLSS though, since you also need to subtract the compute time of the denoisers it replaces, right?",hardware,2026-01-24 16:48:23,13
AMD,o1g44w9,"What Iâ€™d really love to see improved with RR is the ghosting and trailing artifacts. That is the biggest flaw currently, imo.",hardware,2026-01-24 16:19:06,6
AMD,o1glfge,"As an aside, does anyone know where the FSR4 having a transformer component came from? I've heard it a few times now. From what I can tell it's entirely U-Net (CNN) based with zero attention mechanisms. Someone took apart the model from the leaks and it's a [well optimized U-Net](https://woti.substack.com/p/understanding-fsr-4). It's more like a neural network enhanced version of FSR2's architecture with a few clever tricks to reduce compute. I'm kind of impressed AMD's ideas with FSR2 were so forward facing, but then again they did state their heuristics were all tuned using machine learning anyways. I don't think AMD actually has any transformer stuff ready, their joint denoising-upscaling framework is also pure U-Net.  Edit: not to say you can't have vision transformers in or connected to layers in a U-Net (there's many architectures that do this) but none of the blocks explained in the FSR4 architecture have any transformer component. If there is a transformer in the FSR4 model it's not doing any of the heavy lifting at least",hardware,2026-01-24 17:36:04,7
AMD,o1f20ch,"thanks for the analysis, I had no idea of the details behind the names",hardware,2026-01-24 12:51:48,8
AMD,o1gk716,"yeah, not everything needs its own vid. would be cool as a side note in their weekly roundup tho",hardware,2026-01-24 17:30:35,3
AMD,o1f4qwx,What is preset L and why is it so much more expensive than M if theyâ€™re both FP8?,hardware,2026-01-24 13:10:35,5
AMD,o1fukos,"Interesting info and analysis. When I watch these presentations it's easy to assume the features are ready to use, or at least be implemented in games within a couple years, but it varies a great deal.",hardware,2026-01-24 15:34:38,2
AMD,o1f314s,Almost certainly yes.   Models run 4x faster at FP4 than good old BF16. Nvidia will leverage the performance increase.,hardware,2026-01-24 12:58:56,73
AMD,o1fbj7c,It could and I'd be fine with that. My 3090 doesn't have th hardware. It's a 5 year old card. Long as it still falls back to dlss 4 then fine. Can't let old hardware hold back new tech long as there's a fall back,hardware,2026-01-24 13:52:21,32
AMD,o1fet7v,"I'm pretty much sure Nvidia would find reason to limit DLSS5 to RTX60 only. They'd say something like ""our flow motion engine 2.0 is required to achieve it"".",hardware,2026-01-24 14:11:24,-11
AMD,o1fbi0g,Well since it's delayed to 6.10 now. I wonder if some Nvidia sponsered game using NVAPI will use it till cooperative vector is out.,hardware,2026-01-24 13:52:09,10
AMD,o1feyrv,"Yeah it needs API standardization. Contingent on widespread adoption but seems like nextgen HW will be universally capable here so that's good news.   But like I said as it stands rn doubt even NVIDIA sponsored titles will use NRC and other neural lighting techniques.  Pretty much any lighting effect you can think of (DI, GI, refractions, reflections, caustics, various volumetric effects, and iridescence. Also math such as Control Variates, Basis functions, BDRFs etc. Common theme seems to be feeding a ray or path tracing input to a neural model and training that to approximate the quality of offline quality rendering.  Also a lot of non-rendering related tasks I can't conceive of rn.  Further out maybe some [diffusion based GenAI techniques](https://www.tomsguide.com/computing/gpus/were-working-on-things-that-are-utterly-shocking-nvidias-ceo-on-pc-gamings-future-solving-the-ram-pricing-crisis-and-the-lore-behind-his-leather-jackets), although ATM that seems a bit fantastical.   *""And in Team Greenâ€™s labs, Huang says the company is working on things that are just â€œutterly shocking and incredible.â€ To put a more specific point on it, he talked about extreme photo realism: â€œbasically a photograph interacting with you at 500 frames per second.â€*",hardware,2026-01-24 14:12:16,6
AMD,o1fgg7b,"Other than initial ray tracing push, Microsoft has always been the one lagging behind in these graphics updates",hardware,2026-01-24 14:20:42,6
AMD,o1hwlhm,"True, but I suppose you can make that argument with SS too. You're not going to have no denoiser but you're also not going to bilinear upscale from quarter res.",hardware,2026-01-24 21:06:35,2
AMD,o1gl7xh,I thought they fixed that with RR transformer. Seems like they still have a lot of work to do,hardware,2026-01-24 17:35:09,1
AMD,o1gr8yq,"Thanks for the interesting blogpost. Will read it later.  FSR4 INT8 =/= FSR4 FP8. AMD confirmed FP8 version a hybrid model, not pure CNN. Based on research papers with CNN/ViT hybrid designs and that DLSS4 was purely transformer based most people concluded FSR4 uses a hybrid ViT/CNN model.   Here's the quote from 19:29 in the RDNA4 unveil vid on YT: [https://www.youtube.com/watch?v=GZfFPI8LJrc](https://www.youtube.com/watch?v=GZfFPI8LJrc)  *""Our new technology leverages a proprietary hybrid model resulting from extensive research across different types and combinations from neural networks and unique training techniques.""*   The AMD RR research paper being CNN is not good news. Now I'm not even sure they'll have a serious competitor to DLSS RR when the consoles and the next gen Radeon cards launch.",hardware,2026-01-24 18:01:25,5
AMD,o1fc5r9,It's the preset tuned for DLSS Ultra Performance mode. 720P -> 4K or 480P -> 1440p.  Model has to infer good output from lesser inputs. No way around this other than brute force.   Same thing with Ray reconstruction. Complicated tasks just require far more compute.  **Edit:** Preset M = P and Preset L = UP are just what NVIDIA recommend and the NVIDIA App default overrides. As long as sharpening is disabled you can even use preset L with DLAA if you wanted. Recommendations are there because new models are a lot heavier than prev models.,hardware,2026-01-24 13:56:02,19
AMD,o1ix4en,FP8 is just the precision. It has nothing to do with the size of the models themselves. L is a beefier model and M,hardware,2026-01-25 00:07:37,2
AMD,o1fg7i1,Preset M exists to make a less intensive scaled back version of preset L is my guess,hardware,2026-01-24 14:19:21,3
AMD,o1fclgo,"It must be a bigger model, with more neurons and hence more computations.",hardware,2026-01-24 13:58:35,2
AMD,o1gqrja,"There's some heavy lack of research here that's making it that way.  Research has taken the path of least resistance here, there's some tradeoff with FP4, FP8, FP16 etc... but there's been little incentive to do *interesting* research in this area because it's way easier to just change the format than to figure out what isn't being captured in a neural network as a result.  Could more complicated activation functions that wouldn't work on low precision hardware but could do more obviate the need for lower precision neural network acceleration hardware?  Maybe, but no one is really looking into that. architecturally, it's dead simple to add fp4 to hardware compared to paying for the research for alternatives, tensor cores in general are dead simple architectural improvements compared to almost anything else on the GPU stack.   Nvidia added sparse tensor operations in ampere, found that their models didn't need half their values and they could be re-trained with out half the nodes and perform about as well. That should have deeper implications (maybe networks shouldn't be homogeneously connected, an architecture which heavily benefits for GPUs, but with out would make it much harder to compute on GPUs) but Nvidia just kind of ignored it and moved on.",hardware,2026-01-24 17:59:20,9
AMD,o1fqpoz,"imo makes sense, nvidia loves locking features to newer hardware. kinda sucks for people with oder cards tho",hardware,2026-01-24 15:15:39,-37
AMD,o1fzapf,Or just the fact that the RTX 50 series supports NVFP4.,hardware,2026-01-24 15:56:51,21
AMD,o1guzib,"That quote is very vague - the architecture shown there is also a hybrid model, just not a hybrid CNN-Transformer. I can't find AMD actually confirming this anywhere (it's free PR to say it's transformer based, after all) and have seen some people running with that quote by linking it with largely unrelated papers claiming that CNNs struggle with long term dependencies, no good with FP8, etc, when these algorithms have vastly better information to work from and aren't single-image approaches.   Not only does FSR4 predict kernels which are perfectly reasonable with FP8 but part of how FSR4's architecture works is by using clever reprojection tricks to reproject state information into the network to give it longer history. This sounds like a very well informed design trying to maximise a small CNN. I'm not sure there's any transformer component to speak of even in the FP8 model but you are right to say that the leaked code wasn't the final model. Maybe there's some better information somewhere.",hardware,2026-01-24 18:17:13,5
AMD,o1fthwu,So for 4k dlss performance model M is better?,hardware,2026-01-24 15:29:25,5
AMD,o1ffhul,"The footprint is actually slightly smaller than Preset M. Ray reconstruction is somewhere in the middle. Difference are negligible though we're talking 2-5MB.  At least based on the DLSS programming guides it looks like the models have the same size, they're just tuned for very different outcomes.",hardware,2026-01-24 14:15:18,5
AMD,o1hjc1r,"This is a dumb take.   The math is not that complicated.   Itâ€™s just like fighting a war, and saying â€œmeet at the location 17Â° 26â€² 19.32â€³ South, 152Â° 55â€² 16.68â€³ Eastâ€  Or saying 17Â° 26â€² 19.322639264932726492â€³ S, 152Â° 55â€² 16.6872262828625260283362â€³ E  Two battleships do not need to be subatomically precise, lol.   A single neuron changing activation by 0.001 is not going to affect a neural network much at the end of the day. Itâ€™s just trading off precision for speed, thatâ€™s it.",hardware,2026-01-24 20:03:45,6
AMD,o1fxyww,"> nvidia loves locking features to newer hardware  If anything, they've been backporting a lot of features, even DLSS 4.5 which this post is about.  If older hardware simply doesn't support it, what do you expect?",hardware,2026-01-24 15:50:40,49
AMD,o1g4wb1,"I mean it is literally four times faster.  Nvidia famously doesn't do that unless there's a hardware limit, they let you run DLSS 4.5 Transformer M on RTX 2050 laptop GPU, the performance is poor ofc but it works.",hardware,2026-01-24 16:22:31,28
AMD,o1ghmg9,"Well yeah, it does support fp4, int4 and it doesn't look like next generation will go for lower precision. Doesn't mean Nvidia wouldn't find a reason to make dlss5 available only to rtx60 or rather rtx50 would get dlss5 without any new features, possibly without some improvements too",hardware,2026-01-24 17:19:09,-1
AMD,o1h8lto,"Thank you for this info. I've edited post to avoid spreading misinformation and speculation.  I don't think just Redditors and Tech netizens are to blame. You can look at how NVIDIA marketed CNN vs ViT from the original DLSS4 transformer Geforce blog posts. Could be summed up as :   \- CNN = tile by tile basis/myopic and really poor tracking of dependencies over time and across frame   \- Transformer = analysis entire frame and excellent tracking of dependencies over time and across frame.  I think another reason why most people thought it was transformer was that IIRC FSR4 is even more demanding to run than Preset K and J despite using FP8. People could not conceive of a CNN model running this poorly when NVIDIA CNN models run fast, but then again didn't Sony's PSSR also have serious issues with performance overhead?   Maybe this is a FSR4 and Ray regeneration are both early beta releases. But regardless of what you think it's clear NVIDIA has the lead in terms of talent and design. AMD and everyone else is fighting an uphill battle against DLSS fs.  That's interesting. Yes the FRS4 INT8 leaked model seems pretty solid overall.  No I doubt it. AMD isn't disclosing anything. Even less forthcoming than NVIDIA. A bit surprising given their historical commitments to open source.",hardware,2026-01-24 19:15:44,2
AMD,o1g5an6,No L is still better than M regardless of the input resolution. Its actually a really large difference from my testing. Some games with boiling noise on M feel almost unplayable vs K. while L minimizes it and has all the other 4.5 benefits.  The reason it's recommended for ultra performance is because its more heavy to run than M. But on my 5090 the difference is usually just a couple frames tbh,hardware,2026-01-24 16:24:20,13
AMD,o1g01ji,"Yes, M is recommended only for use with Performance mode right now - if using balanced/quality, it's recommended to still use K.  And L is only for the niche Ultra Performance use case.",hardware,2026-01-24 16:00:18,2
AMD,o1huu4w,"Its clear you're just copying and pasting an argument you heard about going beyond double precision float, this argument is not even remotely relevant here.Â  You simply don't understand just how low precision fp8 or even fp4 is.Â  You can only represent 16 different possible values period with fp4, and it's even more restrictive because the mantissa only occupies one single bit in that representation, so to a human it appears even worse than 16 values   I suggest attempting to do any non trivial math using fp4 (or heck, even trivial math!) and compare that to even fp16.Â  The reason fp4 works at all for neural networks is because the shape of the function matters way more than the actual value in an activation function, and it's gone to the point of simply using bits instead of fp4/8 etc... however at the same time the trend has been simplifying activation functions as simple as possible which complements this approach, and virtually all research in activation functions has been towards this simplification process.Â  If you could instead do more work per activation function you might be able to go the other direction, more complicated activation functions, but smaller overall neural networks by node size.Â  This however is not possible which such abysmally small floating point types.",hardware,2026-01-24 20:58:15,10
AMD,o1g9vws,"well, tbf there's zero chance a 4090 can't run MFG, so there are definitely exceptions.",hardware,2026-01-24 16:44:48,-9
AMD,o1igkyh,Don't think they are that cartoonishly evil. We have not seen Nvidia go with that short of an adoption rate with RTX gens before.,hardware,2026-01-24 22:42:05,3
AMD,o1hlgfx,"Not trying to be pedantic, sorry for all the edits ðŸ˜… I was very curious about FSR4 model too, all these upscalers exist with a gulf in knowledge about their architecture! Ended up falling down a rabbit hole on relearning how FSR2 worked. The black box of it all makes it very frustrating and ends up in semantics war.  NVIDIA are right with their marketing, it seems CNN models are often augmented with help from motion vectors to avoid needing to learn motion reprojection (FSR4 does it, and so did [DLSS 2](https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-2-0-a-big-leap-in-ai-rendering/)). I'm guessing the transformer model is able to learn this itself and that's why it's so much sharper and handles animated textures better. It's not that the sentiment is wrong, just it's not indicative one way or another - DLSS3 got very far with a CNN and FSR4 is (imo) better yet. What AMD has achieved combining a lightweight model with previous FSR insights is very impressive and isn't any less impressive if it's not transformer based.  I would be very curious to know why DLSS is faster - despite its additional complexity, it definitely is faster, but AMD claim otherwise. Their listed performance figures are [optimistic](https://gpuopen.com/manuals/fsr_sdk/techniques/super-resolution-ml/#performance), if those numbers could be achieved that would've been very impressive. Less than DLSS4, more than DLSS3.5 which is somewhere near where the model visually performs. Wonder if there's some overhead and the actual FSR4 passes do genuinely run that quickly. Regardless I would hope that AMD can upgrade to a more modern model, they should have the compute for it! Maybe the software stack isn't there yet.  I really hope they are at least experimenting with transformers. It does seem like despite no public information, many of these upscaling models end up following practices set by NVIDIA. I would love to see a competent RR alternative but we all remember how the CNN variant looked ðŸ«   Edit: forgot that FSR4 already uses FP8, wonder how much compute is left if they used a bigger model..",hardware,2026-01-24 20:13:39,2
AMD,o1i48ii,"â€¦ Iâ€™m a ML engineer and I literally have a [comment yesterday about AdamW optimizer hyperparameters](https://www.reddit.com/r/LocalLLaMA/s/QoBWhc0Z2l) being ideal at 1E-8. And of course the AdamW (or modern Muon) would be the most bit sensitive part where youâ€™re doing learning at fp32, before QAT.  Tell me more about basic bit count knowledge?  There are bitnet models that literally just use 3 values: -1, 0, 1. Those are 1.58 bits per parameter. You realize everyone knows the direction of the neuron matters a lot more than the amount? Why do you think OpenAI released gpt-oss as fp4?",hardware,2026-01-24 21:42:42,14
AMD,o1i4eoo,"Do you suspect FP8 + moving from logarithmic to linear space is the culprit behind some of the image quality regressions we've seem with DLSS 4.5?   With RR ViT they also said they used FP8 to train the model. I assume they repeated that with DLSS 4.5, so the new model is 100% FP8, no FP16 it seems.  NVFP4 could still work, it's not regular FP4 or MXFP4, yes there's a bit of overhead but it gets very close in accuracy to FP8, so NVIDIA might still opt to use it for DLSS5 training.  The idea is interesting and might be the kind of paradigm shift that can provide us with another DLSS 3.7 -> DLSS 4 increase in image quality. Would be a shame if ViT's and chasing ever smaller formats is the end goal for DLSS and other image upscalers.",hardware,2026-01-24 21:43:31,2
AMD,o1gs3aj,That's not how it works. A feature needs to be able to run on *any* 4000 series GPU to be enabled. There has to be feature parity between all the SKUs of the same architecture.,hardware,2026-01-24 18:04:59,12
AMD,o1gc6g4,I mean it just doesn't have the AI cores to do it realistically. And literally only the 4090 everything below it definitely can't do it so it's probably not worth spending millions to make a feature only a few people who own the 4090 can use.,hardware,2026-01-24 16:54:50,5
AMD,o1i1s0g,"No worries. If anyone is pedantic then it's me xD  That's very interesting and I would be interested to see how it would handles a purely driver based implementation (Like NIS, except with preset L).  AMD's claims are wrong. You can look at the percentage wise performance scaling of upscalers in Hardware Unboxed's 9070XT review. DLSS4 is superior in IQ (but like you said it beats DLSS 3.7) and ms overhead.  IIRC FSR4 generally is a lot softer than DLSS4 being much closer to DLSS 3.7. That might be an indication of it not being ViT but this is just a guess. But DLSS3.7 -> DLSS 4.5 is almost like a glasses on vs off situation in many games in terms of motion clarity.   Yes that seems odd but doubt we'll understand why that is.  Hmm. IDK but the MB number for 4K are roughly the same as (within 10MB) as preset J and K despite using FP8. If I extrapolate preset J and K numbers based on 40/50 -> 20/30 series MB scaling DLSS preset J and K could sit at \~230MB.   Now this means very little if it's not the same architecture and even if it is as we've seen with Preset L, M, and RR those have the same MB footprint but wildly different ms overhead. So this is probably a waste of time.  Also as a side note you can see here just how efficient the old DLSS CNN model is. [https://www.studocu.com/en-gb/document/middlesex-university-london/games-fundamentals-2/dlss-programming-guide-release/120964782](https://www.studocu.com/en-gb/document/middlesex-university-london/games-fundamentals-2/dlss-programming-guide-release/120964782)   \- Sorry for link but can't find old download on Github + internet archive won't load it.   Footprint at 4K is \~200MB and 2080 TI and 3070 ms cost from CNN to Preset J/K is more than doubled, and roughly doubled for the rest of the cards. Now I do know that FSR4 is generally in between DLSS 3.7 and DLSS4, but getting a model inferior to DLSS4 that runs with a higher cost despite using FP8 is def not a situation where AMD wants to lay back and be complacent. They still have a lot of work ahead of themselves and it'll only get worse nextgen when NVIDIA moves the goal post yet again.",hardware,2026-01-24 21:31:02,2
AMD,o1j96u9,"> Do you suspect FP8 + moving from logarithmic to linear space is the culprit behind some of the image quality regressions we've seem with DLSS 4.5?  I do not believe so, but I may be wrong.  In my opinion this is likely just due to training.    Basically every model upgrade Nvidia tries to address issues where people have had problems in specific games.  For example, they might notice many people are complaining about ghosting from small particle effects and the like.  So in addition to making models larger/more powerful, they throw in more datasets meant to correct that specific deficit when they train their new models.  But there's no guarantee that each model is going to be able to produce the same results with out regression.  So any new training could change the model causing regressions, and training for specific scenarios could shift weights in ways that may mess up another area.  There's not really any specific way they can fix this beyond training more, this is just an inherent problem with up-scaling using AI.     In theory they have some sort of model testing system to find these kinds of regressions, but it's impossible to account for every single scenario, networks aren't procedural algorithms, so they can fail in unexpected/unpredictable ways.   FP4, and smaller representations will probably lead to larger networks with better quality in the same amount of space if Nvidia chooses to use them, my point though was research in ML in general has been myopic and focused on the lowest hanging fruit to the neglect of other plausible avenues of making more compact neural networks.",hardware,2026-01-25 01:11:18,3
AMD,o1gj4od,The only thing distinguishing 40 and 50 series is NVFP4 and HW Flip metering.  40 series could easily run MFG it would just have slightly worse frametime consistency.,hardware,2026-01-24 17:25:52,10
AMD,o1grs5t,"> I mean it just doesn't have the AI cores to do it realistically> but Nvidia just kind of ignored it and moved on.  The excuse for a lack of 4x MFG had nothing to do with tensor cores, but image motion vector/optical flow hardware which dates back to ampere.   This is hard to validate since Nvidia hasn't really exposed what exactly the hardware that supposedly does the optical flow analysis actually is capable of/does or how they've upgraded it in other generations of cards so we have to take their word for it.   >  And literally only the 4090 everything below it definitely can't do it so it's probably not worth spending millions to make a feature only a few people who own the 4090 can use.  That's not how any of this works.",hardware,2026-01-24 18:03:42,6
AMD,o05drwi,Same die. I kind of assumed xt would always be prioritized and the non xt are just poorly binned ones,hardware,2026-01-17 18:34:15,221
AMD,o05dtj2,Non xt is the same die right? It's fused off parts due chip production defects?   Yield of the full chip must be getting good enough that it might be harder to make the non xt chips.,hardware,2026-01-17 18:34:27,88
AMD,o05hbrs,"My understanding was that the base RX9070 was just binned 9070XTs, they've always made up significantly less of the production numbers. I honestly wasn't sure if they were actively choosing to make some of them as the 9070.",hardware,2026-01-17 18:50:35,43
AMD,o05g8y5,Duh? Better margins better card. But it sends like they will still be making some so meh,hardware,2026-01-17 18:45:39,27
AMD,o06h90f,"The 9070 XT was already much easier to find than the regular 9070, even before the memory price shenanigans began. (In Canada, at least)",hardware,2026-01-17 21:49:33,10
AMD,o06bltx,"I'd be curious if the 9070 GRE 12GB won't see a launch. They'll still have some defective dies , and if it's not going to the regular 9070 16GB, I'd imagine a 12GB variant would be a way to get rid of them.",hardware,2026-01-17 21:21:07,7
AMD,o068rla,pump them out like its the next 2080ti,hardware,2026-01-17 21:06:26,5
AMD,o05otm1,"With the 5070Ti shitcanned, one would think the 9070XT is now the undisputed bang for the buck leader.   Maybe that's why we've seen the DLSS 4.5 hypebot flood here... To try and head off that sentiment before it starts.  EDIT: GPU tribalism. Wild shit.",hardware,2026-01-17 19:26:00,24
AMD,o05v22k,Company prioritizes higher margin product? Now thatâ€™s a shocker.,hardware,2026-01-17 19:56:13,3
AMD,o05ce4e,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-17 18:27:56,1
AMD,o065wnj,Thatâ€™s largely fine.  Kinda hard to narrow a lineup that is already narrowed enough.,hardware,2026-01-17 20:51:37,1
AMD,o06sklk,"For a second, I thought they were prioritizing 9070s over 9070XTs and I was thinking â€œWhy?â€  Re read it and the title made sense again",hardware,2026-01-17 22:45:50,1
AMD,o0fh41n,"9070 should have been a bit more cheaper. I don't think people are buying 9070 over xt, at least in my country.",hardware,2026-01-19 05:42:12,1
AMD,o066t0s,"From what I heard the XT has a higher chance for coil whine, which is one of the reasons I went for the non-XT version.",hardware,2026-01-17 20:56:16,-7
AMD,o05nvp2,"Apparently, the XT die yield are so good that there's barely any non XT.",hardware,2026-01-17 19:21:28,133
AMD,o05f9nt,Likely has more to do with prioritizing vram supply.,hardware,2026-01-17 18:41:07,36
AMD,o0b4s6p,"It's quite common to cut down perfectly good dies in order to produce more of the lower priced SKU, because usually that's where they sell more units.   This effectively amounts to a price increase of the non-xt relative to the xt",hardware,2026-01-18 16:01:40,3
AMD,o067a29,"Base 9070 shows up on the Steam survey, the XT doesn't for what it's worth.",hardware,2026-01-17 20:58:44,16
AMD,o05n3cc,Is it possible they can increase yields by slowing production?   Otherwise I think itâ€™s just them artificially cutting down chips based on demand.,hardware,2026-01-17 19:17:42,4
AMD,o0benq7,"There was a short while here now in EU at the end of 2025 where the 9070 had good availability. And the price difference vs the XT was holding at a similar level as the performance delta or even better. Which made the 9070 a decent deal, since you can gain a lot of performance from OC/power modding.  But now they are back to ""9070 XT minus 50-75 euro"" at best",hardware,2026-01-18 16:48:31,1
AMD,o061rgr,If you think Nvidia thinks DLSS 4.5 will somehow make people buy GPUs they wouldn't want to purchase otherwise you're high. It's legit just an upgrade to the already great transformer upscaling and more frame gen options.  If AMD wanted people to buy GPUs they would focus on prebuilts and laptops.,hardware,2026-01-17 20:30:21,28
AMD,o06sg40,"Bruh what? Thatâ€™s the craziest thing Iâ€™ve read so far.  DLSS 4.5 hype is most likely not a botting conspiracy because NVIDIA is trying to ward negative sentiment, and more likely due to there being a larger market of people who can test the thing out for free.  Again, need I mention that NVDIA has most likely sold more 50 series cards than AMD has sold 90 series cards? Not to mention that DLSS 4.5 still runs optimally on 40 series cards, and runs at all on 20 and 30 series cards.   This wide coverage of people getting a free upgrade will naturally generate hype.  As for the 9070 XT being the best bang for buck once the 5070 Ti is shitcanned, this will be true, though sadly not as good of a bang of a buck before the whole VRAM apocalypse went down.",hardware,2026-01-17 22:45:12,24
AMD,o069lqk,"I bought a Gigabyte Windforce OC 5070ti for $750 about three weeks ago. The cheapest I can find it in stock right now is $1300.  With Nvidia saying they're going to de-prioritize the 5060ti 16GB, if AMD can keep the 9060xt 16GB and 9070xt in stock at somewhat reasonable prices then they have the best 1080p and 1440p option.",hardware,2026-01-17 21:10:46,14
AMD,o0qcyr0,> GPU tribalism. Wild shit.  Says the fella deluding himself into believing conspiracy theories that don't make any sense.,hardware,2026-01-20 20:33:54,2
AMD,o0db6ql,Pretty sure that's just the higher board power pushing the VRM harder. Should be similar if you just power limit the XT to non-XT levels.,hardware,2026-01-18 22:21:13,3
AMD,o06sf9s,Shades of the old Phenom II dual-core CPUs that had four fully functional cores that could be unlocked on certain motherboards.,hardware,2026-01-17 22:45:05,46
AMD,o07rz9p,Also makes sense to push for higher margin since they have the same VRAM capacity,hardware,2026-01-18 01:50:48,11
AMD,o06sfkm,My assumption is that it's a low stock product so they aren't going to sell them for now while they build up supply.,hardware,2026-01-17 22:45:07,3
AMD,o05gla4,"Thats a given, but both can be true that AMD may also no longer have enough stock to justify the sku.",hardware,2026-01-17 18:47:13,54
AMD,o07dat0,"That's dumb, both cards have the same exact memory, so making a non-XT shouldn't have any effect on the XT from a VRAM standpoint. Spork is likely correct about high yields leading to low supply of defective dies. AMD really optimized the frick out of that N48 die.",hardware,2026-01-18 00:33:46,4
AMD,o06cf3r,The steam survey is hardly scientific and can wildly swing between surveys. You have to look at the long term trends to get a good picture which GPUâ€™s are popular with consumers.,hardware,2026-01-17 21:25:19,23
AMD,o05o4gj,"No, as far as I'm aware it's not possible. The likely answer here is that they do not have that many defective chips anyway so they're prioritising the SKU that is sold for more (same chip).",hardware,2026-01-17 19:22:39,28
AMD,o075ehc,"My understanding is that TSMC wafers tend to be batched - switching to new masks takes time and cost more than making everything in one go and keeping dies in a warehouse which are then binned, packaged and put on cards to sell.  There's a decent chance that the silicon has already been produced, and has for some time. They can't do anything to change their quality.",hardware,2026-01-17 23:52:12,6
AMD,o064rs5,"The screaming sentiment in /r/radeon for the last couple weeks has been the polar opposite of this.   Sooo many shrill posts about ""Nvidia supports all of their GPUs with DLSS4.5 and FSR4 is for the newest GPUs only! I'm selling my Radeon and buying a 5070Ti!"". Weirdly including people that claim to have RX9000 GPUs.  For example: https://www.reddit.com/r/radeon/s/VcHeMoArKj",hardware,2026-01-17 20:45:50,-4
AMD,o08044v,Same with some 4 core ones that could be unlocked to 6 (960T comes to mind),hardware,2026-01-18 02:35:15,6
AMD,o08h6tq,more margin on the xt,hardware,2026-01-18 04:13:23,14
AMD,o0ahqeb,"Yeah, that thing is 'design for manufacture' city, and frankly, given how hard it's optimized for that and how it gets pretty comparable performance... there's a damn good argument it's a flat out better arch then Blackwell given that the latter just doesn't get enough improvement from that new VRAM format to justify the bottlenecks it risks.",hardware,2026-01-18 14:01:43,1
AMD,o0wwhbk,"An XT gives more profit than a non-XT, so prioritizing the 9070 XT means less profit is lost.",hardware,2026-01-21 19:45:08,1
AMD,o0764h4,I'm sure the RX 9070 at 0.14% while the 5070 is at over 2% says enough...  *Also yeah the XT isn't even on there*,hardware,2026-01-17 23:56:01,11
AMD,o0nr3sm,"Ah yes direct data from steam with massive install base of primary GPU consumers is not scientific, but r/radeon wild ramblings are facts. Do you believe that earth is flat too?",hardware,2026-01-20 13:00:55,-1
AMD,o0gozoi,And TSMC N4P has excellent yields.,hardware,2026-01-19 12:08:51,3
AMD,o06rsrb,Something like that. They can just use 9070 firmware on a 9070xt chip instead of physically disabling stuff on the chip. It certainly would not be the first time lol.   Imagine it being the opposite of the situation when nvidia couldn't keep 3090s on the shelf. They were probably selling the worst possible qualifying chip as a 3090 to maximize profits. So if you got a 3080 it was probably the worst of the worst lol.   This case would be the opposite if demand for the 9070xt was so weak (compared to 9070 sales) that they started selling the good chips as 9070s to move them. You're more likely to get a really good 9070 chip that might even work as a 9070xt with the right firmware lol.,hardware,2026-01-17 22:41:54,1
AMD,o0c0kdi,Keep in mind that reddit is just a bubble. The average buyer doesn't hang around on a radeon subreddit so this really doesnt mean too much.,hardware,2026-01-18 18:31:06,4
AMD,o0h444e,"It was a ton of Nvidia bots, since after a few days they suddenly stopped posting basically everywhere lmao",hardware,2026-01-19 13:49:15,0
AMD,o0cjfax,And 8 core 1600X out of the box.,hardware,2026-01-18 20:00:07,2
AMD,o079qzt,"There were at least reports in past months with from people with screenshots with 9070XT's of it showing up on the hardware survey as ""AMD Radeon Graphics"" the same it shows for APU/iGPU graphics.  I don't doubt in my mind both GPUs got outsold by the 5070 and 5070ti but I also have seen that's hard to get actual numbers from Steam so we don't know. But I imagine some percent of that 2+% of ""AMD Radeon Graphics"" are 9070XTs, I'm pretty sure they sold more than 0 9070XTs.  Reminder when the RX480/RX580 were priced against like 2GB 60 series Nvidia GPUs while having 4 times the VRAM and nearly double the performance for the same money they still got outsold like 5 to 1 by the Nvidia GPUs. And this is a pre-RT and upscaling era so none of the bells and whistles/software suite type features would be a reason either. So even in situations where an AMD card is just flat out better in every way, it still usually gets outsold hand over fist, so in situations where it's debatable or worse it's an even wider margin.",hardware,2026-01-18 00:15:04,15
AMD,o074giq,">So if you got a 3080 it was probably the worst of the worst lol.  Have the 3080 12GB - below the 3080 Ti, and then the 3090, all using the same die - despite having the EVGA FTW3 version (more power!), it was a dog.     So, can confirm!     (I bought because it was all that was available to buy...)",hardware,2026-01-17 23:47:09,1
AMD,o0ht35s,Thing I donâ€™t like being posted - bots  Thing I like being posted - factual  Get your fucking head checked,hardware,2026-01-19 15:54:48,3
AMD,o08mh0s,"All screenshots i have seen that supposedly showed this were actually showing the wrong part of the survey, and not the right one where the GPU is supposed to be shown",hardware,2026-01-18 04:48:12,9
AMD,o08jgec,"Got a B850-I and 9800X3D... I do wonder if those affected had done some tweaks, like PBO or BIOS settings.  I do have to say, during my build, I did notice some quality issues with my board. Once i installed my M.2, that stupid toolless metal bar thing bent so hard it made contact with components on the daughterboard. Had to disassemble everything to get to that piece and bend it straight. During the process i noticed that one of the heatsink screws for the VRM had snapped off. So yeah, i wouldn't rule out issues with the boards here.",hardware,2026-01-18 04:28:00,60
AMD,o09eet2,it seems like 9800x3d issue not motherboard at this point,hardware,2026-01-18 08:39:17,66
AMD,o08rshq,"Ignoring the fact that this has been reported the most on AsRock boards but is not exclusively an AsRock issue, the most common feature of these deaths is the CPU trying to turn itself on from a low-power or off state.  Issues like that are difficult to reproduce for anyone other than AMD and the motherboard manufacturers.  It won't be as quick for AMD in relative terms to root-cause this issue like Intel did with theirs.  So in the meantime they're happy to RMA dead CPUs quietly.",hardware,2026-01-18 05:25:42,37
AMD,o087f3t,AMD likely released a very large batch of defective products but decided to stay silent because they know what happened when Intel recognized their instability issues. Asrock just killed the CPUs quicker.,hardware,2026-01-18 03:15:44,273
AMD,o08bsr7,This was not the post i needed to see after ordering a new pc haha,hardware,2026-01-18 03:40:53,49
AMD,o08wipe,Does this seem to effect the 9950X3D or just the 9800X3D?,hardware,2026-01-18 06:01:57,13
AMD,o08vfef,"> At least five Reddit posts from the past two weeks describe AMD Ryzen 9800X3D systems that stop posting on ASUS AM5 boards  ok  And no videocardz, you being a jackass and disabling my ability to highlight text is not going to stop me from quoting the article.",hardware,2026-01-18 05:53:18,32
AMD,o08bwbv,"I seem to recall the 9000 series launch was postponed over a quality issue. I know it's rumored it was over a typo on the box, but I wonder if there's a connection here.",hardware,2026-01-18 03:41:28,31
AMD,o0betur,I was one of the few who had this happen with my 9950x3D. amd RMAâ€™d the chip and I received a new one with no issues.     [https://www.reddit.com/r/ASUS/comments/1ohkr3z/x870e\_proart\_vga\_white\_led\_wont\_boot/](https://www.reddit.com/r/ASUS/comments/1ohkr3z/x870e_proart_vga_white_led_wont_boot/),hardware,2026-01-18 16:49:19,7
AMD,o08fort,"Given how specific these mysterious deaths are to just the X3D parts and that everyone (except maybe MSI) is affected again, if there's not some underlying design flaw... or perhaps some specific voltage is flying too close to the sun like with Intel's Raptor lake chips.",hardware,2026-01-18 04:03:56,28
AMD,o08icbe,"Not ideal.  Starting to really side eye the 9800X3Ds....  Had thought of getting one, but think I'll  just stick with my 7800X3D.",hardware,2026-01-18 04:20:45,26
AMD,o09ivso,"When everybody understand that this is a CPU issue not motherboard, yet people still blaming motherboards with eyes wide shut.",hardware,2026-01-18 09:20:45,18
AMD,o09mo6t,So is this exclusively an issue with the 9800X3D? I'm not seeing any reports of it affecting other 9000 X3D parts.,hardware,2026-01-18 09:56:09,5
AMD,o09safp,Iâ€™m starting to sweat over here with my Asus X870-P + 9800X3Dâ€¦,hardware,2026-01-18 10:47:55,5
AMD,o09vup5,Where's renowned investigation journalist Tech Jesus when you need him ?,hardware,2026-01-18 11:20:03,11
AMD,o09onjt,"I have never had problems with gigabyte mobos, thatâ€™s why i chose it instead of ASUS or MSI, but seeing another 9800x3d being fried questions the processor. My current setup is gigabyte x870x + 9800x3d",hardware,2026-01-18 10:14:27,3
AMD,o08m7io,"This is not news. Dead 9800X3D has been happening with asus, msi and gigabyte boards too besides asrock. But there are definitely overwhelming more reports with asrock boards. My guess is that X3D CPUs have a general reliability issue and then asrock fucked up extra hard with their bios.",hardware,2026-01-18 04:46:24,14
AMD,o096e95,But when Intel CPUs die everyone and their moms make news about it,hardware,2026-01-18 07:26:21,17
AMD,o0acg5w,So the problem is from all Asus 800 series board or 9800x3d?  Because now I am worry that I just buy a 9800x3d + Asus ROG strix B850-FðŸ˜Ÿ,hardware,2026-01-18 13:29:28,2
AMD,o08j059,What are the chances AMD has been binning the 9800s because of 9850s?,hardware,2026-01-18 04:25:03,4
AMD,o08xc9c,Fuck I just bought an ASUS mobo after having this death trap of an Asrock for the last year LMAO,hardware,2026-01-18 06:08:34,4
AMD,o0chddf,"I wonder about the ratio between ""five dead cpus"" and ""sold cpus"".",hardware,2026-01-18 19:50:13,4
AMD,o08jfgq,"Eeep I have a 9800X3D and Asus B850-I motherboard. Been running great since April, hope this is a rare problem!",hardware,2026-01-18 04:27:50,4
AMD,o0bv5vy,"So 2 of the 5 listed examples canâ€™t even really be counted. 1 the user had a bent pin that they tried to correct, so we canâ€™t be sure the issue was with the part or if it was damaged. 1 the user canâ€™t even get into bios and has had multiple faults so again you canâ€™t say if itâ€™s the mobo or cpu or something else going on.",hardware,2026-01-18 18:06:31,2
AMD,o08xny6,"so like... maybe the x3d stuff isnt the best after all?  Unless you are ABSOLUTELY gaming at 1080p in esports (or play loads of mmos) just get a normal cpu. These things seem way too risky at this point   Honestly with the pricing being insanely good, Intel Core ultra is supremely compelling now. Free games, cheaper prices, better bundles. SAME performance as AMD at 1440p/4k. WAY better in pro work  or if you simply must have amd for whatever strange reason, get a 7700x or 9700x. Decent pro work, great gaming perf.   - Also consider stuff like the 12700k, 13600k, 14600k. These also support DDR4, which costs far less right now. These cpus can GAME and they do pro work well",hardware,2026-01-18 06:11:12,1
AMD,o0dx9mw,"Two time my Asus Tuf B850-Plus completely lost Bluetooth and Wi-Fi, and the only way to get it back was to shut off, unplug, hold the power button for 10 seconds, then plug it back in. Bluetooth and Wi-Fi magically reappeared in Device Manager after doing those steps.",hardware,2026-01-19 00:12:34,1
AMD,o0i20mi,"So my feeling is all early 9800X3Ds will die. Sooner in ASRock motherboards, soonish in ASUS motherboards, eventually in Gigabyte & MSI motherboards.",hardware,2026-01-19 16:34:43,1
AMD,o0oi0me,this is still happening...?,hardware,2026-01-20 15:25:35,1
AMD,o0fb8qu,People are literally killing their chips for about 8 frames ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚,hardware,2026-01-19 04:58:59,1
AMD,o0832ig,I've had an asus x870 board for more than a year...  no issues,hardware,2026-01-18 02:51:17,-12
AMD,o087wd4,Thank God I ordered an Asus ROG STRIX B650E-F Gaming WiFi with my 9700X,hardware,2026-01-18 03:18:31,-2
AMD,o082ccc,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",hardware,2026-01-18 02:47:22,-1
AMD,o095kqe,"Iâ€™m one of the people with a dead chip mentioned in the post. I was running 3x Scalar with -10/-5 CO. 1.2 Vsoc, temps never got above 70. Went to sleep and never woke up. AMD sending a new one, but I already replaced with 9950X3D, will sell the RMA 9800x3d  Definitely not touching PBO on new chip, the performance improvement is tiny IMP and not worth the risk.",hardware,2026-01-18 07:19:04,47
AMD,o0cfpvc,"It's exactly this. In fact, there's been issues with both AMD and Intel silicon as far back as Zen 2 (3700x) and Raptor Lake for Intel. I personally had two dead 3700x chips, that started to blue screen at idle after just a year in service each. There are numerous Reddit posts talking about this issue from from a few years ago. The silicon is very delicate, and we are reaching the limits of how much power we can push into these chips without damaging them.",hardware,2026-01-18 19:42:13,15
AMD,o0m11nn,"I think so, too. It's now happening on 9800x3ds specifically with 2 different board makers.",hardware,2026-01-20 04:32:56,1
AMD,o0ahqav,This is interesting to me. I bought an Asus  x870e Crosshair Hero and 9800X3D in January 2025. Ran fine for a month and a half until one night I restarted my PC and  it just didn't boot back up. Q-Code showed 00 and days of troubleshooting did nothing.   RMA'd the CPU and the new one has had no hiccups since on the same motherboard with the only difference being there were multiple BIOS updates released in between.,hardware,2026-01-18 14:01:42,13
AMD,o088qiz,"Yeah, Asrock, Gigabyte, Asus all having issues points to the actual CPUs having issues.Â    I feel like something is wrong with my 9800x3d PC too, but I haven't found a way reliably reproduce my issues...",hardware,2026-01-18 03:23:19,147
AMD,o0ak645,Didn't Pugit systems notice a similar trend? I remember them releasing stats during the whole Intel scandal that seemed to suggest an issue with Ryzen chips too.,hardware,2026-01-18 14:15:42,12
AMD,o09uht6,"Yea that's my base case. That or Asrock boards just being more popular at the CPU's launch and being especially prevalent in enthusiast communities leading to them looking like the cause.  I'm just going to pray my close to launch 9800X3D is not affected, it's in a MSI board atm but I don't think that means anything at this point.",hardware,2026-01-18 11:07:52,7
AMD,o0finqb,"I agree.  It's extremely weird that we keep seeing motherboard vendors getting blamed for killing AMDs x3D chips all the time, it's almost as if the x3D chips are the problem themselves and AMD is just forcing their partners to shift the blame around so there isn't a massive decrease in sales.  Anyways, who gets the blame in Q2?",hardware,2026-01-19 05:54:05,8
AMD,o0arxt0,"This is supported by the fact that (a) AMD has been replacing the chips affected by the Asrock failure mode without question and (b) the fact that the failure rate, last I checked, was substantially higher on some batches.",hardware,2026-01-18 14:57:59,12
AMD,o08krug,"I don't think it's anything like the Intel situation, and I'm not personally convinced that ASRock ever were actually killing them faster. I think the ASRock community just picked up on it first and started documenting it first.  Gamers Nexus and Wendell from Level 1 Techs have both been trying to get some insight and they can't replicate or make an ASRock board kill a CPU, and they've even been using boards that have had CPUs die in them before.  It does seem to be some sort of manufacturing issue, but Intel's one was pretty much 100% of specific models of CPUs were affected by this issue and would eventually fail.  AMD's issue doesn't seem to be this. I think it just looks a lot more common than it is because of how many CPUs AMD is selling now.",hardware,2026-01-18 04:36:46,44
AMD,o089pt9,Damn I hope I lucked out,hardware,2026-01-18 03:28:46,18
AMD,o08grrm,"We've been hearing about dying X3Ds on ASRock since the start and the occasional GB board, which if it's the same cause would preclude it being any single batch problem and instead more of a general design or systemic issue. Too many reports are on brand new builds, and it's guaranteed there isn't old 9800X3D stock laying around from November 2024 given the extremely high demand it's been in.  Makes me happier to own a 7700X from 2022, but I was planning to buy a Zen 6 X3D part for a drop in upgrade, and the longer this madness drags on the less confident I am in doing so.",hardware,2026-01-18 04:10:41,9
AMD,o09d894,"Despite having INTC stock I had 5700X3D and it died after 9 months of usage, on Gigabyte motherboard.  And there were other reports suggesting issues around that time  https://gmplib.org/gmp-zen5  >We have fried two Ryzen 9950X CPUs in a few months by running GMP tests and the GMP-based factoring program GMP-ECM. This is not expected, of course. In this page, we provide as much information as possible to help analysing the problem.",hardware,2026-01-18 08:28:15,8
AMD,o08y4le,"Yep. And people LOVE to shit on Intel despite them making insane strides in mobile (making amd irrelevant), coming out absolutely swinging in desktop GPU, and making desktop cpus that offer wild good value (as of now) with outstanding pro performance.   But... intel bad... or something i dunno (probably just paid amd and nvidia bots)",hardware,2026-01-18 06:15:02,17
AMD,o08s4pv,"No, thatâ€™s impossible, astock are evil bad guys!!!",hardware,2026-01-18 05:28:16,6
AMD,o0apycd,I literally just started my new PC for the first time yesterday lol. Asus 800 series board and 9800X3D,hardware,2026-01-18 14:47:30,6
AMD,o0fk4nv,Im halfway through putting my new pc together.  9800x3d + 5080,hardware,2026-01-19 06:05:42,2
AMD,o08xvr5,I was worred about the same thing and found some articles  about the  9950x3d and the asrock board issue but it was all the same guy being reposted by different websites.,hardware,2026-01-18 06:12:59,14
AMD,o0belu4,Iâ€™m here to say Yes it does. check my post history and you will see the headache I endured with a 9950x3d paired with the ASUS ProArt x870e.   [https://www.reddit.com/r/ASUS/comments/1ohkr3z/x870e\_proart\_vga\_white\_led\_wont\_boot/](https://www.reddit.com/r/ASUS/comments/1ohkr3z/x870e_proart_vga_white_led_wont_boot/),hardware,2026-01-18 16:48:16,9
AMD,o08da6s,"I have a 9950X3D and I did file a complaint to AMD since compared to the 7950X3D, the serial number on the box just rubs off. The QR code didnâ€™t work either. And this was on launch day.",hardware,2026-01-18 03:49:33,19
AMD,o09tn3g,"This guy here is exactly how these reports and rumours start. No fucking idea about the situation or issue, guesses and copy+paste rumourmill.",hardware,2026-01-18 11:00:09,0
AMD,o0cbjfz,"Here I was thinking I was safe with a 9950X3D on an X870-I.  Well, shit.",hardware,2026-01-18 19:21:58,3
AMD,o0h8r6c,Is it on earlier units?i got mine 2 months ago no problems untill now but im a little bit concerned now,hardware,2026-01-19 14:14:58,2
AMD,o0avsj7,It may be a weirdness induced by the upsidedown-cake MCM design that's the signature of Zen 5 3d.,hardware,2026-01-18 15:17:56,7
AMD,o0a7c7z,"Or maybe the whole thing is and has always been a nothingburger.  Without knowing the RMA rate it's hard to say a CPU or a MB actually has a problem. 1-2% of chips causing problems is normal, unless a single motherboard or a single bios update, or specific software are causing the problem.  Even when people were blaming ASRock MBs, the whole thing was suspicious, because once a popular news outlet writes an article ""ASRock motherboards are *frying* our CPUs!!!"" everyone with that mobo will find out about it and report it. Everyone else will assume they just bought a faulty CPU and replace it without making it a story.  But without the numbers it's impossible to say what's a real problem and what's completely normal defect rate. Now I guess it's ASUS turn, everyone in this thread is getting scared by anecdotal evidence of 5 (five) unlucky buyers.  And by the way the Raptor Lake issue was real, but it also got out of hand, e.g. people were scared for parts (like the 13600K/14600K) that had completely normal replacement rates.",hardware,2026-01-18 12:54:28,7
AMD,o091msb,"Absolutely not worth ditching the 7800X3D for a 9000. Unless of, of course, you're an absolute baller competitive FPS player with a 500hz monitor.   I'm gonna ride my 7800X3D until the Zen6 X3D is nearing end of life, then buy one.",hardware,2026-01-18 06:44:40,35
AMD,o0994ag,Probably not.,hardware,2026-01-18 07:50:41,19
AMD,o0fjgi3,"Not unless they are hounded for these failures like Intel was.  AMD was denying the reference design 7900XTX design flaw hotspots, it wasn't until like every reviewer talked about the issue that they started doing exchanges and free returns.",hardware,2026-01-19 06:00:22,6
AMD,o0beyrl,This happened to my 9950x3D.  [https://www.reddit.com/r/ASUS/comments/1ohkr3z/x870e\_proart\_vga\_white\_led\_wont\_boot/](https://www.reddit.com/r/ASUS/comments/1ohkr3z/x870e_proart_vga_white_led_wont_boot/),hardware,2026-01-18 16:49:59,4
AMD,o0dinqv,Heâ€™s busy releasing negative videos on news first reported on Reddit,hardware,2026-01-18 22:56:44,5
AMD,o0fjwds,"X3D is the internet's golden child, Steve won't report on this because it will go against the masses opinions.",hardware,2026-01-19 06:03:52,4
AMD,o0a209y,Every single manufacturer has made shit boards on many occasions in the past.,hardware,2026-01-18 12:12:53,6
AMD,o09rnov,gigabyte has the worst boards lol,hardware,2026-01-18 10:42:15,-3
AMD,o09clq0,What? X3Ds dying were major news. So far only certain boards has killed them though so that makes this different.,hardware,2026-01-18 08:22:28,10
AMD,o0asj6o,"in short the problem is for all mobos, msi and gigabyte also had reports of failures, its just that Asus has the biggest market share",hardware,2026-01-18 15:01:05,5
AMD,o091pg6,All chips are binned.,hardware,2026-01-18 06:45:17,17
AMD,o095w21,100%,hardware,2026-01-18 07:21:51,4
AMD,o0f3ksu,Im 99% positive itâ€™s cpu design flaw and not mobo issue,hardware,2026-01-19 04:07:21,3
AMD,o0aqlhv,"Same ... mine is pretty much brand new D:  First time in ages that I built a completely 100% new PC, no old parts in it what so ever and now I get news like that lol",hardware,2026-01-18 14:50:57,1
AMD,o17po13,"Hey, I also have the ROG STRIX B850-I. For what itâ€™s worth, I havenâ€™t been able to find a single dead 9800X3D report with this exact board.  Do you mind telling me which BIOS version you are using? I built my SFF PC in early July 2025 and am still on version 1063. I am on the fence if I should update to a newer/the latest version or not since Iâ€™ve read elsewhere that newer AGESA versions could also be part of the issue (I sadly donâ€™t know what â€œnewerâ€ means in that context, it didnâ€™t specifically say which versions).  I havenâ€™t gotten around to use this PC much so far, perhaps 50-75 hours total turned on of which maybe 15-20 hours were more demanding gaming, the rest was pretty much only idling/desktop usage. Thatâ€™s why I wanted to ask someone with the same board who probably ran their setup a lot more than me for their input.  I mostly kept the BIOS settings at stock, besides enabling EXPO and adjusting some fan curves.  Thank you!",hardware,2026-01-23 10:14:20,1
AMD,o09jwry,"I mean, Ultras are great and all, post updates, core count is high, bla bla. Awesome.  But this isn't the point here. At all.",hardware,2026-01-18 09:30:22,7
AMD,o09l1ry,"As somebody who plays VRChat and Risk of Rain 2 a lot, you'll have you pry 3D cache off my cold dead hands",hardware,2026-01-18 09:41:05,2
AMD,o099llv,Zero regret with my 9700x. But note that I only play Enter The Gungeon. Every other game ever made is just lame in comparison.,hardware,2026-01-18 07:55:03,-3
AMD,o084maf,I would hope it's not a 100% kill rate lol.,hardware,2026-01-18 02:59:53,49
AMD,o08fydi,Yet,hardware,2026-01-18 04:05:37,18
AMD,o0871k0,*knock on wood*,hardware,2026-01-18 03:13:37,16
AMD,o08nxg5,"This is stupid. If you already bought it, use it. Otherwise you are burning through any potential warranty period by just letting it sit and collect dust.",hardware,2026-01-18 04:58:11,21
AMD,o08rjbz,They have the lowest failure rate of the 4 big manufacturers. ASUS just has an elevated report rate because they have the most sales out of all the vendors by far.,hardware,2026-01-18 05:23:51,4
AMD,o09vho4,"Just dont touch scalar, that can really mess up your cpu.",hardware,2026-01-18 11:16:49,28
AMD,o09dcr6,"I tried PBO on my 9800x3d, but I couldnâ€™t get it stable at all. It just wasnâ€™t worth the risk, especially now with how expensive everything is getting.",hardware,2026-01-18 08:29:24,12
AMD,o0b6sn1,> Went to sleep and never woke up.    You just need a prince to kiss it!,hardware,2026-01-18 16:11:11,7
AMD,o0d2n05,Tweaking it's fun but it feels like they are maxing out the silicon from the factory.  Enjoy your new chip,hardware,2026-01-18 21:41:09,4
AMD,o0dhhga,I thought you were talking about never waking up and not the chip,hardware,2026-01-18 22:51:04,4
AMD,o0idxu8,"I've used PBO to set the thermal limit to 90C, at least that option is buried under the PBO menu. Also limited VSOC to 1.2v. No idea if any of this is a risk, although its meant to reduce spikes, power draw and noise.",hardware,2026-01-19 17:28:34,1
AMD,o169446,Seems like sleep state is killing a lot of these CPUs. Or waking it up from sleep state.,hardware,2026-01-23 03:24:16,1
AMD,o0ds078,I feel like my 5800x3D has degradation on the memory controller as well. Launch unit so among the oldest out there but still,hardware,2026-01-18 23:44:36,3
AMD,o0ax6n5,"Yup - restarting, waking from sleep, or even turning on after a couple of days being off - all these actions seem to be the moment the affected users reported failure.",hardware,2026-01-18 15:24:55,9
AMD,o08cexq,"Unlike the Raptor Lake issue, this X3D issue manifests as a CPU at 100% working and then 100% dead. If you're getting inconsistent crashes or wonky behavior, it's most likely RAM.",hardware,2026-01-18 03:44:27,74
AMD,o09pdae,As someone who just bought a 9800x3d and a gigabyte mainboard I now feel a little bit concerned.,hardware,2026-01-18 10:20:57,5
AMD,o098in4,What issue you have?,hardware,2026-01-18 07:45:18,5
AMD,o0f27xr,"Yea, they said AMD failure rate was higher even on top of Intel drama - iirc",hardware,2026-01-19 03:58:38,8
AMD,o0eklxj,"I've been on a x870e Nova for about a year at this point. If I was one of what looked like a set of fishy batches, she'd have died by now I'd think.",hardware,2026-01-19 02:20:18,1
AMD,o09jl35,I'm still leaning towards it being because ASRock had the most compelling lineup this time around. A lot more enthusiasts having their mobos means more posts with traction with their boards when there's a failure,hardware,2026-01-18 09:27:22,19
AMD,o0bk7at,"It's also seeming to be associated more with some batch numbers then others last I checked, which to be fair was a while. My guess is that this is a 'quietly replace dodgy chips as reported, dissect failed MCMs as they arrive, quietly revise Zen 6 with whatever they find if it's a design rather then batch issue'.",hardware,2026-01-18 17:14:54,5
AMD,o0gwew0,Me too. I built my most recent rig in late February / early March of 2025. So the CPU I got was fairly close to their release date. I have my 9800X3D in a X870 Steel Legend WiFi at the moment and it has been flawless so far. I did buy a X870E Taichi as well on sale to upgrade but have not swapped them out just yet due to all this. Not sure if waiting is really going to make things better or not. I should be getting my 5090 in a few weeks. I will make my decision to swap the boards out at that time.,hardware,2026-01-19 13:02:18,1
AMD,o08sxwe,"The affected userbase seems to be small. I personally own a 9800x3d in an Asrock board and had no issues so far. Latest bios, pbo ON set to tjmax=85C. Many other systems running fine.had the system for more than a year now",hardware,2026-01-18 05:34:19,8
AMD,o09uuym,"388H is 8% faster 1T and 0.1% slower MT than 395 Max at equal TDP, that's ""making AMD irrelevant""?",hardware,2026-01-18 11:11:10,6
AMD,o0a379b,Imagine what you would say if *intel* itself 'paid' (a polite euphemism!) certain companies to shun its competition... Oh wait...,hardware,2026-01-18 12:22:39,-1
AMD,o0966f6,"> ""Enhanced system stability""  That's pretty much the default changelog entry for when the vendor can't be bothered to put in a real explanation for what's updated in the new version. You should regard that phrase as having *zero* information content.",hardware,2026-01-18 07:24:24,14
AMD,o090qpq,"Iâ€™m curious about where you managed to get reliable data with failure rates by motherboard manufacturer considering nobody has access to that, probably not even AMD.",hardware,2026-01-18 06:37:01,15
AMD,o096w86,"I think whatever happened to DLVR for Raptor Lake can be fairly considered a defect in that silicon, and disabling it late in the development cycle likely had something to do with Intel's other voltage regulation mistakes.",hardware,2026-01-18 07:30:51,8
AMD,o0a2p9k,"I mean, some 13th gen cpus were actually literally defective and it was known.",hardware,2026-01-18 12:18:37,2
AMD,o0aqyww,"Mine is getting delivered tomorrow so i really feel you haha, upside is we likely both have a warrenty should something happen.",hardware,2026-01-18 14:52:54,2
AMD,o0g3x5u,Out of curiosity did you have PBO active?,hardware,2026-01-19 08:59:12,1
AMD,o098yow,What was AMDâ€™s response?,hardware,2026-01-18 07:49:17,5
AMD,o0dculp,"Well, yes, it would be much better if AMD could just tell us what's going on.",hardware,2026-01-18 22:28:59,2
AMD,o0ip8ws,I purchased mine in July. It was solid until October/November.,hardware,2026-01-19 18:19:11,2
AMD,o0aeck0,"Valid points. So many instances were just bent pins or a bad install, some of the most vocal people on reddit in the end had bent pins. Even in the above article there were more bent pins discovered by one user which rather invalidates that result.  It didn't help motherboard vendors were using some crazy high voltages for the first year of AM5, and I don't just mean the VSOC either. But that shouldn't be a factor with current chips and 99% of new builds because anyone using a 2022/23 dated UEFI would be having enough memory headaches to have updated by now I'd hope.  It's just annoying how this problem won't go away, if it was just \~5 reports I wouldn't give it any thought. But then something utterly random like that cafe owner using ASUS boards having a 10% failure rate on 9800X3D systems makes the news. It's always just enough to make me wonder. I use an ASRock board with a 7700X and I still swear it's the best computer I've built since 2006 with the Core 2 Duo E6300. And I've built many Intel rigs since then and now. These questions are not enough to put me off my planned Zen 6 X3D upgrade but I will not be happy if this keeps going on with whatever the next-gen X3D part will be called.",hardware,2026-01-18 13:41:23,3
AMD,o0avm7o,"Rumor has it that the last AM5 will be Zen 7, but yeah, not a bad plan.",hardware,2026-01-18 15:17:02,3
AMD,o09buv7,They should,hardware,2026-01-18 08:15:33,16
AMD,o0bo2s8,"Fun. When you said you bought a new CPU, do you mean another 9950X3D?  I just bought an Asus Pro WS B850M-ACE SE and am waiting for my 9950X3D...",hardware,2026-01-18 17:33:28,1
AMD,o0fk62s,Neither of those are cache heavy games that  would struggle with any decent modern CPU,hardware,2026-01-19 06:06:01,-2
AMD,o08j8yt,I do have an older bios version so maybe its fine,hardware,2026-01-18 04:26:38,-6
AMD,o0be74t,Yep. I just tinked with the per core CO and left everything else well alone.,hardware,2026-01-18 16:46:20,1
AMD,o09b90n,"On the Asrock Reddit loads of people have said it started with long boot times, inconsistent booting etc so your statement isnâ€™t necessarily true.",hardware,2026-01-18 08:10:03,34
AMD,o09q9a2,"Not neccessarily - my two dead Raptor Lake CPU's worked fine unless you put on a load on them. Then i started getting internal compiler errors, memory corruption and random freezes. CPU replacement fixed this, then second CPU died, now same memory lives happily in an AMD CPU.",hardware,2026-01-18 10:29:19,1
AMD,o090rh3,I tested my ram with memtest86 though.,hardware,2026-01-18 06:37:11,0
AMD,o09qs22,"As someone with the same combination, I'm not worried.   I bought a Gigabyte board because they had the least reports of 9800x3D killings. Even Techyescity's video showed that Gigabyte had the most conservative voltages, which may or may not be the reason they die.  Seeing them being conservative with Bios updates makes me confident in them as well, MSI and Asus have released Bios updates (Agesa 1.2.0.3E) too soon, with lots of issues, meanwhile Gigabyte is more slow and stable this generation it seems. However some of their bios settings were in Spanish for a while which seemed a bit incompetent lol",hardware,2026-01-18 10:34:14,3
AMD,o0dphzt,"Asrock also had the least problems on AM4. Few of the boards had the usb dropout issue, they lasted well and had big enough bios chips that they were about to be the only partner that was going to keep all of the AM4 CPUs working on x570 and b550 until AMD backtracked  And I followed sales in my country closely for the first few months of AM5 and Asrock always had a minimum of 4 boards in the top 10",hardware,2026-01-18 23:31:24,6
AMD,o0awmj1,"It's rare that one marque was that overwhelmingly good a spin, from the Steel Legends being the best value option up to what had a good claim to be the best bang for buck x870e, the Nova. So yeah...",hardware,2026-01-18 15:22:06,8
AMD,o08yekg,Are you saying it works on your machine?,hardware,2026-01-18 06:17:16,19
AMD,o0a7oyh,"Oh man, so it's gonna be essentially the same performance about a quarter of the priceâ€¦ Goddamnit it signed me up  This is what you don't seem to understand, the 395 needs like 150 W to get its peak performance, and it's being sold in systems that cost $2000-$3000. And doesn't come with upgradeable memory either.  The Panther Lake stuff is offering the same performance, it's gonna have access to XESS in gaming, which is massively better than FSR three, and it's also gonna cost a fraction of the price  ðŸ˜…. Not gonna lie, I love how your poster ended up, coming out as a straight up advertisement for panther Lake",hardware,2026-01-18 12:57:01,2
AMD,o0hppf0,"a poorly written microcode does nos mean defective chips. 13 and 14 gen i7 and i9 problems came from poorly written microcode that requested wrong voltages from motherboard did it not? You implied that problems were due to defective batch, which they didn't.",hardware,2026-01-19 15:39:35,0
AMD,o0997yr,They only confirmed the chip was authentic. They couldnâ€™t read the QR code either,hardware,2026-01-18 07:51:37,13
AMD,o0gqeed,"So, tell us based on 5 claimed CPU's ? Out of how many. There is no fucking way any company could survive, if they started commenting on every possible reddit thread of dead component or product.  Intel did coverup for YEARS on massive number of dying CPU's. Thats a problem you need to adress. Or if you have certain mobo maker burning up (and i mean literally) CPU's, kinda like asrock.  This ""reporting"" was so poorly done, no actual linking things, other than ""asus board"" and ""x3d"". Didnt even bother to find out if they had any link in bios revisions, microcode etc. I found couple threads of dead CPU's (was that even confirmed on all?), ill just write a story pointing fingers at Asus, not AMD im guessing.",hardware,2026-01-19 12:19:46,0
AMD,o09q9h3,"Yeah, but I guess the independent media outcry isn't loud enough, and AMD appears to be on the road to sitting it out in the hope everybody just forgets about it.",hardware,2026-01-18 10:29:22,18
AMD,o0c4aun,"Yes. I bought a new 9950x3D, RMA'd the faulty one, received a replacement, and returned the new one I bought. What a rollercoaster.",hardware,2026-01-18 18:48:00,2
AMD,o0fo2mu,"Untrue. VRchat has absurd benefits from 3D cache. [https://github.com/Greendayle/VRChat-benchmark-cache](https://github.com/Greendayle/VRChat-benchmark-cache)  So does Risk of Rain 2. I'll be doing my own benchmark numbers soon, but I can only compare 5800X vs 9800X3D since that's all I have.",hardware,2026-01-19 06:37:42,2
AMD,o08q166,Isn't that usually worse?,hardware,2026-01-18 05:12:59,12
AMD,o0fl54m,Just the sheer size of the pool of people that have OC memory profiles causing random issues is bound to overlap with dead CPU issues.   On the Raptor Lake degradation problem it was particularly problematic because memory instability and core degradation symptoms had a large overlap. People would swap in a new CPU with new microcode and still crash in UE5 games every 1-2 weeks from unstable XMP profiles but never think it could be the memory.,hardware,2026-01-19 06:13:43,7
AMD,o09zsre,"He specifically pointed out that sudden death wasn't typical for the Raptor Lake failures, only the X3D failures...",hardware,2026-01-18 11:54:32,19
AMD,o099oz9,"Use karhu, hci memtest, or tm5. I run karhu for at least a full day for new builds.",hardware,2026-01-18 07:55:55,6
AMD,o098822,memtest86 is pretty useless,hardware,2026-01-18 07:42:45,5
AMD,o0ek9t6,"My x470 taichi was a champ, there was a reason I found it so unsurprising that the 800 series lineup from them was also top notch.",hardware,2026-01-19 02:18:23,2
AMD,o093b67,"Yeah.Â   To me it feels a bit like the 12VHPWR connector issue on the 4090. Reddit was full of users posting pictures of their burnt connectors, yet NVIDIAâ€™s investigation revealed that only a small portion of users were affected.   Likewise, 9800x3D burning is a real thing, but so far no one has been able to pinpoint a cause. Even re-using the same mobo that killed a previous CPU is not enough to replicate the issue.   Taking into consideration the popularity of this CPU and the fact that it sells well, if it was a widespread issue we would have heard many reports of failures. For reference, on Amazonâ€™s product page it says â€œ1K+ bought in the last monthâ€ in UK. And that s only 1 site in 1 region.",hardware,2026-01-18 06:59:12,3
AMD,o090lsb,"Works on mine too. I'm running a 9800x3d on an AsRock b850 board and have had no issues so far that I'm aware of. Running with latest bios, pbo2 on, etc, etc. Had my system going for quite awhile now.  Only real quirk is that I'm running a wifi board that can only use a bios from the non wifi version. Oh well...",hardware,2026-01-18 06:35:51,0
AMD,o0a9pgr,">in gaming  lol, lmao even.  do you understand that dGPU-less laptops are only bought for gaming by people with no money, therefore they are completely irrelevant as a market force?  Professionals buy iGPU-only laptops for work.  Gamers buy laptops with dGPU for gamers.",hardware,2026-01-18 13:11:12,1
AMD,o094on5,You bring these â€˜statisticsâ€™ as fact when itâ€™s you scrolling forums and making assumptions? Bold move.,hardware,2026-01-18 07:11:18,15
AMD,o0htqj6,No I didn't. You just don't know what I'm referring to.  I am talking about a specific batch of only 13th gen chips that experienced an oxidation issue that was separate to the voltage and microcode issues.  Hence the word 'some' and no mention of 14th gen. I would suggest reading words and then thinking about them before replying next time.,hardware,2026-01-19 15:57:42,1
AMD,o09qs0f,\> They couldnâ€™t read the QR code either  That sounds rather incompetent coming from the manufacturer. How is that possible?,hardware,2026-01-18 10:34:13,14
AMD,o08xjm9,"DependsÂ    I an update introduces a problem, no.   If an update fixes the problem, yes.",hardware,2026-01-18 06:10:12,2
AMD,o091zk0,"No.  If you're stable, there's almost always zero reason to update firmware.  It's usually a risk.",hardware,2026-01-18 06:47:43,1
AMD,o0gg6um,"Even amd gpu drivers timeouts will be caused by unstable RAM. My previous system, I upgraded from 2 to 4 dimm and got a lot of issues with my gpu. when I dialled back the RAM OC, they were gone. If i tried to run with only 2 dimms, it was also fine.",hardware,2026-01-19 10:54:09,1
AMD,o09et46,Side note but Finnish supremacy.,hardware,2026-01-18 08:42:57,4
AMD,o09y7qq,"Memtest 86 is good since you have to boot it to run so windows can't interfere with it. Also, karhu is paid software, how dare you even recommend that stuff in the year of our lord 2026?",hardware,2026-01-18 11:41:04,-7
AMD,o09ybvp,why?,hardware,2026-01-18 11:42:03,5
AMD,o09q7lt,">To me it feels a bit like the 12VHPWR connector issue on the 4090. Reddit was full of users posting pictures of their burnt connectors, yet NVIDIAâ€™s investigation revealed that only a small portion of users were affected.  Yet over tens of thousands of people still got burned by it, the number of 12VHPWR burnt cards steadily going through repair shops is mind boggling because they often will have the warranty denied. It was even mentioned in the recent Brother Zhang repair shop video. Having to treat the cable as if it was made of paper because it can't be bent, or angled, or tugged on, or pulled because it messes up the pin alignment is a joke. People that ran 4090's for years without problem do a board swap, or a rebuild, or just carry the thing outside to dust it and the cable isn't positioned straight when they lock the side panel closed and it's all over for their GPU. It's an inexcusable design when a slight bend in the cable can disrupt pin connections in a way to cause the entire connector to fail. People could abuse the hell out of PCIe 6pin and 8pin cables, even bend them at 180 degrees to hide them and it was safe to do so.   At least when AMD's CPUs die mysterious deaths AMD is graciously and expediently warrantying them. But you'd have to buy more than a half-dozen 9800X3Ds to equal the cost of a 5090 today. There was no reason to design the cable with practically no safety margin let alone for a product that now costs more than some people's cars.",hardware,2026-01-18 10:28:53,14
AMD,o0a9yvu,"Lol do you realize there ARE NO 395 laptops with a dGPU. And the 395 laptops cost a fucking fortune?  You just contradicted your own statement   The panther lake costs WAY less (aka same gaming perf for these ""gamers who have no money"") AND they are paired with dGPUs lol  You completely destroyed yourself in the comments  I'm not sure I've seen someone completely dismantle their own arguments so effortlessly ðŸ˜…   - panther lake matches the 395 in pro work and gaming. It costs less and ALSO can be paired with a dGPU. It's literally better in every single way ðŸ’€",hardware,2026-01-18 13:12:59,11
AMD,o0dhvcs,"It was the QR code on the box, which is the one AMD uses to direct you to the Authenticator website. Like it is somewhat important to know the 9950X3D is not counterfeit, since there were reports of fake chips in circulation around launch week.",hardware,2026-01-18 22:52:53,3
AMD,o0azppw,Its AMD what more do you expect from them.,hardware,2026-01-18 15:37:21,5
AMD,o0ch8hp,"Thats where Im at   It works, doesn't crash, and updates can literally kill my hardware   So... I do windows updates and driver updates but bios updates I do very rarely",hardware,2026-01-18 19:49:33,1
AMD,o0hhbpr,Bad RAM can cause NVENC frame drops as well. That was a fun wild goose chase.,hardware,2026-01-19 14:59:25,0
AMD,o0ax85i,"Memtest doesnt stress anything hard enough. I've run several passes on bad ram that finished without errors, then error within seconds on karhu.",hardware,2026-01-18 15:25:08,9
AMD,o0a6z54,"It's generally worse at picking up borderline unstable configs, though you can't beat it being bootable directly and having access to all ram in the unlikely scenario there's a specific region that's unstable",hardware,2026-01-18 12:51:47,10
AMD,o09xtcj,"We just don't know the total number of failures for either Nvidia GPUs or AMD CPUs. They are higher than normal, but these products have also sold truckloads of them. Even 2% would mean lots of impacted users.  I've not heard of any AIB denying RMA because of the burnt connectors, there might be cases but I am not aware, at least in Europe.  Anecdotally, I have both a 12VHPWR connector (before the revised spec) and a X870/9800X3D system. I took my system apart many times and had no issues whatsoever.  My point is, chances of the person reading this comment being affected by these issues is small, and if you don't want to deal with it, there are plenty of alternatives.",hardware,2026-01-18 11:37:34,1
AMD,o0azkki,I have worked for AMD. Its a terrible company that unlike Intel is not not pushing forward in tech so much. The 395 is such a Halo product which was never meant to be purchased by anyone. Zen5% and eventually Zen6% along with the whatever AMD is launching as the new RDNA while not supporting FSR on them in a year and dropping support for them quickly. Panther Lake is a much superior setup with Intel oneAPI which is straight up better than ROCm. There is so much wrong with AMD and bow that their focus has been with AI that leaves Intel to innovate and give us much better products that AMD has ever. AMD competing with Nvidia has yielded nothing LOL.,hardware,2026-01-18 15:36:38,8
AMD,o0m23m1,"Is Karhu the most stringent? I don't mind paying actual people for good software. They know how many times the software is launched each day which is kinda sketchy, but it is what it is. What about OCCT? I've personally experienced MemTest86's inability to find errors despite the memory clearly erroring out in regular usage.",hardware,2026-01-20 04:39:43,1
AMD,o0ajhkj,Is memtest86 even able to detect infinity fabric errors? I have had overclocks that passed all ram and CPU stress tests and as soon as I added a VRAM stress test I get rounding errors within 10 minutes.  Ryzen is so finicky that you have to bench everything at once (and it was not temp related because I have everything covered by custom loop).,hardware,2026-01-18 14:11:49,5
AMD,o0aruti,"The feedback exists for the GPUs, outlets like GN reported information from AIBs as well as etailers and repair shops each, and all three indicated a significant increase in returns, warranties, and GPUs with burnt connectors in for repair.   The 1.10 factor safety margin on the cable has been calculated to the nth degree by almost every youtube hardware channel.    No alternative exists to the 5090. Even the 4090 now costs more than a 5090, and the 9070 XT doesn't even match a 4080.   I'd love to buy something not from NVIDIA, and absolutely something not using 12VHPWR, but I'm not going to invest in a card that's underpowered for what I need, has too little VRAM, and especially one that has a shorter warranty period. On average I buy a flagship card and then keep it for 6-8 years to get the value back out of it, my last was a 1080 Ti that I would've kept on using except a forced monitor upgrade to 4K was too much for it to handle. So don't get me wrong, I own a 4090 and I treat the thing like it's a glass explosive. I also use an ASRock board for my 7700X and wouldn't even consider swapping it for any other brand. I'll agree the AMD issues seem to be very minimal, but there simply is no excuse for the 12VHPWR thing. As per denied warranties I've read about the occasional one, but I have no idea how prevalent it is. In the Brother Zhang repair shop video Zhang mentions people are sending his shop burnt cards because they were outside warranty or because they were imports (such as from Taiwan) and so they couldn't make use of the warranty that the card had.",hardware,2026-01-18 14:57:33,1
AMD,o0oj320,"TLDR: If $10 is so much that it puts you off then use the free version of HCI and OCCT. Otherwise yes go pick up Karhu ram test, I definitely think it is more than worth the money.  HCI and Karhu are pretty comparable when it comes to results, though Karhu runs significantly faster and feels like a more finished product. Karhu 2.0 is also coming out pretty soon which looks even better.  I have not tried OCCT for memory testing, though I do use it for GPU and CPU stress testing. I do not like that you basically have to subscribe to their Patreon for $6 per month if you want to run tests longer than an hour, whereas Karhu it is a one time payment of like $10.   HCI's Memtest pro is just a wrapper for the free version of Memtest that runs several instances of it at a time, so really you can do that all manually yourself.   If anyone also has Karhu I would suggest trying out this neat gui expansion for it: https://github.com/jjgraphix/KGuiX",hardware,2026-01-20 15:30:43,2
AMD,o0yykba,"I'm totally cool with paying $10 one time for good software, especially since it's just one guy, from my understanding. I won't use OCCT on principal for going with the subscription model. Sounds like it's best to use a couple different tests, though? Say, Karhu and TM5?",hardware,2026-01-22 01:56:44,1
AMD,nxxyy8i,Does this pair well with Nvidiaâ€™s newly release of the 3060?,hardware,2026-01-06 03:20:50,37
AMD,nxxmhi8,"So exactly the same specs as the 9800X3D but clocked slightly higher? Seems incredibly pointless, but I suppose it's a convenient way for them to make extra money off the chips that win the silicon lottery.",hardware,2026-01-06 02:11:50,83
AMD,nxxkel3,Would be killer value if this replaces 9800X3D at the same msrp. But it's AMD so they will price it at $549 and sell it alongside 9800X3D.,hardware,2026-01-06 02:00:35,52
AMD,nxyyl7u,"So, no 9950X3D2 announcement? I wonder if it was just a rumor after all.",hardware,2026-01-06 07:39:03,11
AMD,nxxnb08,"it just exists to keep the prices high and higher when it replaces 9800x3d which is already too expensive frankly but what you gonna do, buy intel?",hardware,2026-01-06 02:16:19,22
AMD,nxzdpug,"I'd rather it have a better memory controller so you could do 6400 6600 1:1, not 200 extra mhz but we get what we get",hardware,2026-01-06 10:02:41,4
AMD,nxxp3jo,Release ðŸ‘ more ðŸ‘ AM4 ðŸ‘ X3D ðŸ‘ chips ðŸ‘,hardware,2026-01-06 02:26:02,22
AMD,ny0hjfh,"Intel Nova Lake-S with bLLC can't come fast enough, AMD just slapping X3D to everything and not doing anything interesting at all. At least Intel's APO+ will be special sauce along with their bLLC technology.",hardware,2026-01-06 14:38:00,3
AMD,nxywuqr,Hopefully the 9950x3d2 too,hardware,2026-01-06 07:23:00,2
AMD,ny08w0s,cant wait to push this baby to 5.9 ghz or higher <3,hardware,2026-01-06 13:51:01,1
AMD,ny2l8d6,I just got the 9950x3d and had no clue about this until todayðŸ˜‚ðŸ˜‚ not mad but I mightâ€™ve waited,hardware,2026-01-06 20:26:26,1
AMD,ny2w378,"""4% more performance, 20% more cost."" All jokes aside, halo products do be like that. I'm assuming it'll come in at an MSRP of $549.99, but I hope it's more like $529.99 so at least it makes sense for the price to performance gain if you just want the absolute best CPU for gaming.",hardware,2026-01-06 21:16:16,1
AMD,ny3gx28,"I was not expecting amd to release a new cpu, am so closing to finishing my pc too just need cpu I might wait a little longer to save up and buy this cpu instead of the r7 9800x3d, I read itâ€™s going to have a tdp of 120w I hope my psu is enough for it considering I have an rx 9070 xt.",hardware,2026-01-06 22:54:15,1
AMD,ny6geho,Think I'm just gonna wait for the 9875X3D at the 2027 CES,hardware,2026-01-07 10:52:17,1
AMD,ny7779f,"Is there no new CPU generation coming ?  The 9800x3d is over a year old now , usually this is when the new chip comes out",hardware,2026-01-07 13:54:34,1
AMD,nxyagl1,"No, need more CPU for that if you want to play CS2 at 640x480p with below minimum graphics settings.",hardware,2026-01-06 04:30:14,13
AMD,ny00uek,if you plat MMOs then yes.,hardware,2026-01-06 13:04:22,9
AMD,ny397o2,"It doesn't have AI in the name, so clearly no. Zero future proofing, dead on arrival, no place in the market with zero useful performance metrics.  Once again, AMD never misses an opportunity to miss a opportunity.Â   /s",hardware,2026-01-06 22:17:00,4
AMD,nxxo2dm,If it translates to 7% better peak performance it could be worth it. The 9800X3D is already pretty awesomely fast.,hardware,2026-01-06 02:20:25,52
AMD,nxyire2,"Yes, pretty common for a CPU sku. This should at least be more manageable than the Intel version (KZ CPUs).",hardware,2026-01-06 05:27:19,5
AMD,ny1hdpn,Everything always pointed to it just being the equivalent to a -KS version of a 9800X3D.,hardware,2026-01-06 17:26:07,3
AMD,nxyeem4,it has a new step which allows memory OC to 9800,hardware,2026-01-06 04:56:30,1
AMD,nxz6guz,"Can't tell without the pricing, but if it'll be a price bump as expected, why exactly would one get this over a 9950x3d that already has a higher clocking cache ccd while not being an overpriced 8 core cpu?",hardware,2026-01-06 08:53:08,8
AMD,ny07opx,I'm betting this will be $499 and the 9800X3D will go to $449. AMD confirmed they will continue selling the 9800X3D.,hardware,2026-01-06 13:44:21,1
AMD,ny1fihl,"CES rumors has the MSRP at $499, slight bump from the 9800X3D at $479 I believe. That chip is still at $469 right now.",hardware,2026-01-06 17:17:38,1
AMD,nxxqv30,"hey, 12/13/14 with DDR4 is gona be the way to go for a ton of people rofl  i would laugh if they started to make 5850X3D soon",hardware,2026-01-06 02:35:48,22
AMD,nxypfeh,"It's not replacing the 9800x3d, both will exist side by side.",hardware,2026-01-06 06:19:09,3
AMD,nxyu9ss,delusional,hardware,2026-01-06 07:00:04,-1
AMD,ny11elk,"With that much cache, does the additional memory bandwidth even improve performance? I'm genuinely asking.",hardware,2026-01-06 16:13:26,3
AMD,nxy9rjp,Ryzen 5500X3D with PCIe3.0 speeds /s,hardware,2026-01-06 04:25:43,6
AMD,ny00xl8,never going to happen.,hardware,2026-01-06 13:04:56,2
AMD,nxy1dz6,AMD is saying a 2-3% uplift over the 9800X3D.  https://www.digitalfoundry.net/news/2026/01/amd-unveils-ryzen-7-9850x3d-fast-incremental,hardware,2026-01-06 03:34:49,45
AMD,nxxrngk,"7% higher boost at stock, but the 9800X3D can already hit 5.6-5.7 via ECLK and I doubt the 9850X3D will reliably do 6.0-6.1.  Serious overclockers will probably still want it for the better binning. Hard to say it will be a better value for anyone else yet.",hardware,2026-01-06 02:40:07,38
AMD,nxymq6e,Is this confirmed? Or is it still just that one screenshot and MLID's source?,hardware,2026-01-06 05:57:30,9
AMD,nxz68tb,a new strap means absolutely nothing except to 5 OC people when the iod is the same,hardware,2026-01-06 08:51:00,5
AMD,nxyiu8v,Woah,hardware,2026-01-06 05:27:54,0
AMD,nxz92km,"9950x3d are more usefull for multi-threading while 9800x3d bench show better perf at single-thread gaming  Which was my hope for 9850x3d, but 2-3% for probably 150-200$ more isn't worth it, better to just OC the 9800",hardware,2026-01-06 09:18:24,6
AMD,nxz7od0,Same reason people get 9800X3D over 9950X3D to be honest. Gaming vs AI,hardware,2026-01-06 09:04:41,1
AMD,nxy8w8y,"If they made a 5850X3D, I would be first in line to buy one.  I doubt they would since it seems chip packaging is a big bottleneck at the moment and it has to be more profitable to make 9000 series CPUs.",hardware,2026-01-06 04:20:08,12
AMD,ny1r5tm,[It seems to at 4K.](https://www.youtube.com/watch?v=EBF6B-f5me0),hardware,2026-01-06 18:09:59,2
AMD,nxyadpc,PCIE3.0 speeds and cut down lanes.,hardware,2026-01-06 04:29:43,3
AMD,ny1ehnc,You must be fun at parties,hardware,2026-01-06 17:12:59,-2
AMD,nxyt3ba,atleast they are honest,hardware,2026-01-06 06:49:59,23
AMD,nxyq5cx,Then I'd say that would warrant maybe a 5-10% price hike at most!,hardware,2026-01-06 06:25:04,9
AMD,ny07azf,AMD said last night that gaming uplift will be 7%. Which just about the same uplift as the 9800 was over the 7800.,hardware,2026-01-06 13:42:13,11
AMD,nxznkht,"Which you'd need an ECLK board for in the first place.  Meanwhile with 9850X3D 5.8 will just be enabling PBO, seems pretty sweet to me.",hardware,2026-01-06 11:29:01,6
AMD,nxzp2sm,Was seen with a valid benchmark.,hardware,2026-01-06 11:41:08,-2
AMD,ny3hp8u,"My understanding is that thereâ€™s virtually no difference for gaming. Digital Foundry had the 9950x3d performing within 1% of the 9800x3d in most games.  If youâ€™re only gaming obviously the 9800x3d has much better price:performance, but you arenâ€™t losing gaming performance by getting a 9950x3d, just gaining productivity performance.",hardware,2026-01-06 22:58:09,3
AMD,ny15xzk,"AI? The 9950X3D is more useful for classic ""HPC"" tasks, at least for the subset that sees the benefits of the additional cache",hardware,2026-01-06 16:34:12,8
AMD,ny39k7o,"Or for all the countless stuff that actually uses more cores, rather than AI.  Plenty of current popular ""AI"" stuff are heavily GPU bound and notoriously don't care at all what cpu you use, 4 core or 40 core processor. 9950X3D is probably one of the worst choices you should select when balancing out building a machine dedicated to ""AI.""",hardware,2026-01-06 22:18:40,2
AMD,nxy9vmv,"yeah, I mean, it would be a value play and it would make no sense unless 78 and 98 demand craters hard or something  it was a joke that I think will never happen, or rather, I pray never happens because it means that DDR5 got ratfucked",hardware,2026-01-06 04:26:28,2
AMD,ny0otr1,I know you're trying to make a joke but it's a dumb one.  The 5500 has 20 PCIe 3.0 lanes available.  It's up to the motherboard to determine how those lanes are allocated.,hardware,2026-01-06 15:14:47,3
AMD,ny6kvod,Just by the fact that i do not use emojis i am automatically more fun.,hardware,2026-01-07 11:29:43,1
AMD,ny028ex,"Yes, I hope +200 at least is still nearly guaranteed. That 2-3% figure in the DF article worries me a bit.",hardware,2026-01-06 13:13:01,3
AMD,ny1p6b2,"""AI"" is the new know-it-all buzzword.",hardware,2026-01-06 18:01:09,5
AMD,ny3m9fu,"Ddr5 is ratfucked. By the time prices come back down to even half of what they are, drr6 will be moving in. And you know ddr6 is going to start at the price ddr5 is at.    It's GG for home computing unless the americans wake up and do something about their technocracy problem. So it's GG for home computing.",hardware,2026-01-06 23:21:17,0
AMD,ny03vxc,For games I'd imagine it could hit that quiet well since they aren't really heavy workloads but I'm really interested to see aswell how good these chips manage.,hardware,2026-01-06 13:22:47,2
AMD,ny3a68x,"Haha so true.  I'll buy a fancy 5000 dollar computer case made with exotic materials and has too much RGB fans!  Why?  Because AI needs it!  It also needs this 3000 dollar HDMI cable, I can practically hear the AI singing in high fidelity from the digital bits!",hardware,2026-01-06 22:21:34,2
AMD,o0t6q4y,"Ignore bottleneck calculators, they are nonsense. 32GB is perfect for gaming.",buildapc,2026-01-21 06:09:36,10
AMD,o0t6qcg,"just get a dual channel 16 gb, more than enough for sims4 and chrome",buildapc,2026-01-21 06:09:39,3
AMD,o0t6uwr,"32 GB. For DDR5, always 32 GB if you can afford it (unless its a workstation)",buildapc,2026-01-21 06:10:41,1
AMD,o0t6vew,"Don't use bottleneck calculators, they're snake oil and bullshit  https://www.techspot.com/articles-info/2852/bench/2024-06-19-image-p.webp  https://www.techspot.com/review/2852-how-much-ram/",buildapc,2026-01-21 06:10:48,1
AMD,o0t6y7a,"get at least 16Gb. it's the minimum these days given how much RAM is needed to run Windows. if you can, 32Gb is better. not strictly required if you are not running anything heavy, which seems your case, but better for future proofing. you don't need 64Gb.",buildapc,2026-01-21 06:11:26,1
AMD,o0t6zuu,For Windows 11 you typically want 16gb of system memory as a minimum with 32gb being considered the sweet spot for general gaming.  This is regardless of any CPU or GPU you may have.,buildapc,2026-01-21 06:11:49,1
AMD,o0t6zx6,"The only way to know is to do whatever it is you're going to do on the PC and look at the ram usage.  If you get 32gb and your games/work + any open software and stuff never get past 15gb, then the extra 16gb stick is literally doing nothing and was a waste of money.  (keep in mind browsers allocate more ram than they use, so don't count those as they allocate it, but give it up when something else requests it, and they put hold on tabs that are not in focus releasing that from ram if idle long enough, etc..).  Just assume ""browser"" is going to use like 5gb MAX unless you have open browsers on like 3 displays all playing videos and HTML5/js games at the same time.   I doubt minecraft or sims 4 needs more than 16, and probably leaves plenty of headroom to do other things at the same time within reason.   Buy a 16gb kit, install it, play your games and look at ram usage, if you're maxing it out or identify a bottleneck, return it (so buy somewhere u can return), and get 32gb kit.",buildapc,2026-01-21 06:11:50,1
AMD,o0v1hga,32gb in my opinion. It's more than enough.,buildapc,2026-01-21 14:42:54,1
AMD,o0t7bcl,I put as much as I can but you do you boo.,buildapc,2026-01-21 06:14:25,0
AMD,o0t784e,"okay, thanks so much!!",buildapc,2026-01-21 06:13:40,1
AMD,o0t79kk,does that mean getting two 16 gb ones?,buildapc,2026-01-21 06:14:00,1
AMD,o0t7bfr,"okay, thank you so much!",buildapc,2026-01-21 06:14:26,1
AMD,o0urmlq,16 gb is also good for now I know it's not enough but 32gb is needed only for 4k or 2k,buildapc,2026-01-21 13:51:15,1
AMD,o0t7auy,okay thanks so much!,buildapc,2026-01-21 06:14:18,1
AMD,o0t7d24,okay thank gosh haha!,buildapc,2026-01-21 06:14:48,1
AMD,o0t7gbm,"alright, thank you a ton!",buildapc,2026-01-21 06:15:34,1
AMD,o0t7fh0,okay thanks a ton!,buildapc,2026-01-21 06:15:22,1
AMD,o0t7qo5,"i understand that and i lowkey wish i could, but iâ€™m still having to save up lolol and i dunno if i quite like my kidney haha!",buildapc,2026-01-21 06:17:55,1
AMD,o0yeeyd,"16GB (2x8GB) is plenty to run the games you mention Iâ€™m running both of these games with a 10 year old Acer laptop with an Intel i5 processor 8GB ddr4 (maybe ddr3?) RAM and an ancient AMD Radeon processor with 500MB VRAM and they work perfectly, so Iâ€™d suggest save some money now as RAM is very pricey currently and upgrade to 32 when you need to, which from your post is likely to be a good few years yet.",buildapc,2026-01-22 00:04:49,1
AMD,o0tbqct,"no no ! 16gb dual channel means two sticks of 8gb, if you get single channel i.e one stick of 16gb it will be slower so avoid the single channel setup (one 16gb stick) i could have advised 32gb but you are not a workstation user plus there are some games recently who are using 11-12gbs of ram but you are not gonna play those as you said so 16gb js fine",buildapc,2026-01-21 06:51:39,3
AMD,o0t8796,"They should be included together. 9 times out of 10, if you buy 32GB at a time it should be 2 16s",buildapc,2026-01-21 06:21:43,2
AMD,o0uq2p8,DDR5 doesn't have an 8gb stick i think.,buildapc,2026-01-21 13:42:51,1
AMD,o0td7ss,alright thank you! iâ€™ll keep that in mind!,buildapc,2026-01-21 07:04:37,2
AMD,o0t8gnd,alright! thank you so much!,buildapc,2026-01-21 06:23:51,1
AMD,o1it3bg,I'd buy this in a heartbeat if it wasn't sold out at my local MC.,buildapc,2026-01-24 23:46:42,25
AMD,o1ixnbe,"Great deal. Honestly, I don't understand how they are making money from this.",buildapc,2026-01-25 00:10:21,7
AMD,o1j08w8,Great so it's sold out everywhere,buildapc,2026-01-25 00:23:45,5
AMD,o1jurnm,"I always thought dual-channel is a must, am i out of touch and why?",buildapc,2026-01-25 03:10:38,2
AMD,o1jbzbj,"lol whatâ€™s the point of this, itâ€™s been sold out basically since the start. Who still has these?",buildapc,2026-01-25 01:27:01,1
AMD,o1kc4bl,Sold out really quickly at my local MC,buildapc,2026-01-25 04:55:44,1
AMD,o1j09sk,"Inventory bought before the hike Im guessing, or this MC isnâ€™t moving anything at all and theyâ€™re just trying to get people in the door",buildapc,2026-01-25 00:23:53,8
AMD,o1jwfsf,"It's not as bad on DDR5. Still not ideal, but it's not the end of the world.",buildapc,2026-01-25 03:20:21,2
AMD,o1k7xpd,"It's definitely a good thing to have, but RAMageddon doesn't give us the courtesy of being picky about our RAM.",buildapc,2026-01-25 04:29:49,1
AMD,o1j1531,"It's a Microcenter, so I expect it's the former. Get it while it lasts!!",buildapc,2026-01-25 00:28:20,1
AMD,o1h3wrq,"If you pull the cooler off cold, there is some chance the old CPU will stick to it and get pulled out along with it. Can help if you run a game for a few minutes to loosen the thermal paste a bit before taking it apart.",buildapc,2026-01-24 18:55:13,6
AMD,o1h8d97,"If this is your first upgrade since you built it, make sure to take pictures of how and where everything is plugged up for reference. Also, the gpu latch on the PCIE slot can be a bitch to unlatch. A wooden chopstick can help in getting it to release",buildapc,2026-01-24 19:14:41,2
AMD,o1h8zz6,5700X3D 2070S to 5070. It is insane jump at 1440P plays everything I want and need at 144FPS max settings. You are in a good hands with such setup,buildapc,2026-01-24 19:17:29,2
AMD,o1hijur,where are you planning on getting the 5700x3d,buildapc,2026-01-24 20:00:08,2
AMD,o1hlogq,"I upgraded to 5700x3d from the 3700x, and my next upgrade will be gpu. Also thinking 5070, but I just don't need it yet. I play 3440x1440p, 144hz, and in the games I play everything runs above 120fps.",buildapc,2026-01-24 20:14:42,2
AMD,o1h5pj5,"I did a similar upgrade, 5600g+3060ti to 5700X3D+5070. It's a huge jump in performance at 1440p!  Just make sure your BIOS is up to date, and when you first boot to windows, go to the nvidia app, select reinstall drivers, and choose ""clean install,"" since it will delete the old drivers and install just the new ones  Enjoy the upgrade!",buildapc,2026-01-24 19:03:02,1
AMD,o1h9qwq,Huge jump in performance. Theres only a 10-20% difference between the 5800x3d and a 7800x3d. Enjoy the new DLSS on the 5070 also.,buildapc,2026-01-24 19:20:50,1
AMD,o1hd9ot,"Just to be safe, it doesn't hurt to have a fresh boot drive of windows ready. I recently switched out my Ryzen 5 3600 and 2070 super for a 5800xt and 5070ti... I didn't update my bios properly and for some reason when I put my old CPU back in to download the bios USB, I didn't have windows",buildapc,2026-01-24 19:36:21,1
AMD,o1h51kq,Good idea!   I didn't think of that,buildapc,2026-01-24 19:00:07,2
AMD,o1hntxd,Oh man. You just saved me!  I just realized it got me flipped somewhere to a 78003xd. Not sure how because I was definitely looking for the 5700.  Oof. Now to find a reasonable replacement,buildapc,2026-01-24 20:24:49,1
AMD,o1hdoiw,Oof!  Thanks for the heads up,buildapc,2026-01-24 19:38:12,2
AMD,o1hdx9g,I would have a bios update saved to a USB and a windows install on a separate just to be safe.,buildapc,2026-01-24 19:39:17,2
AMD,o1j1jrj,"Yes its normal. CPUs are boosting super fast, even if you only opening one more browser tab. Maybe just increase your fan curve just to start at 65 degrees. You can dm me and i will show your my fan curve from fan control (7500F and its the same behavior)",buildapc,2026-01-25 00:30:27,2
AMD,o1j0ykn,"Your fan curve probably needs to be tweaked-  Like it hits a certain temp, that temp apikes your fans, which cool the cpu enough to make the fans drop- cycle repeats.  Increase your stepdown and make the curve more gradual",buildapc,2026-01-25 00:27:24,1
AMD,o1k4gfv,"Totally normal, totally fine, been the case for like 15+ years, now. The every 30 seconds bit is odd, but it's probably just some background task repeating, and not a bit deal.  Don't use single cores for controlling fans, and you'll be alright. You should have multiple values to choose from for all cores, the package temp, etc.. The cores can change loads, speeds, and temps way faster than your cooling can adapt.",buildapc,2026-01-25 04:07:41,1
AMD,o1j1rvx,"Thanks. I should clarify that for the testing I'm doing now, the fan control is not being used. Just the bios controlled fan curves are in use. Is it typical for just one core to go up then down due to the fans?",buildapc,2026-01-25 00:31:36,1
AMD,o1fxmpx,"It's fine, but you need fans as the case doesn't come with any.",buildapc,2026-01-24 15:49:04,2
AMD,o1gmm0v,"I would also recommend a better cooler, something like a 240mm AIO or Thermalright 120mm dual tower should be more than enough.",buildapc,2026-01-24 17:41:20,1
AMD,o1j16ll,"I would get a better cooler, maybe Artic liquid Freezer III 240, the motherboard and the power supply could get upgraded to a gigabyte and 850W if you are getting more fans in",buildapc,2026-01-25 00:28:33,1
AMD,o1hytid,"good catch, thank you!",buildapc,2026-01-24 21:17:05,1
AMD,o1hz3z6,good call and thank you! ended up going with this https://www.amazon.com/dp/B09LGY38L4?tag=pcpapi-20&linkCode=ogi&th=1,buildapc,2026-01-24 21:18:26,1
AMD,o1ew1fi,Hold until you have for RX 9060XT 16GB or RTX 5060TI 16GB. Both are amazing and minimum for future. Don't buy most expensive versions please because its not worth at all. Cheapest versions are great also and will do a job.,buildapc,2026-01-24 12:06:25,10
AMD,o1ew50m,"If you save up a little more money, you could buy a 9060 XT 16GB.   It performs very well at 1080p and well at 1440p.   It has FSR + FG, so if you're not getting the FPS you're hoping for, you can enable those options.",buildapc,2026-01-24 12:07:14,2
AMD,o1ewr5n,"Make sure you educate yourself on nvidia pulsar. It only works with nvidia gpus and is the clearest screen tech yet (clearer than oled at the same frames even). So keep that in mind!  This is me being sad I have AMD at the moment, even though my card was a good deal :)",buildapc,2026-01-24 12:12:12,1
AMD,o1ey3a2,"I Upgraded from my old rx 5600 xt to rx 9060 xt. I have a 1080p monitor.  I am able to run cyberpunk 2077 and space Marine 2 on ultra graphics at 90-100 fps.  Itâ€™s been great. For games that donâ€™t have Frame Gen for amd cards, you have Fluid Motion Frame 2.1 which is in driver frame gen that works pretty well and is a definite upgrade from the previous Fluid Motion Frame 2.0.  I got it cheap before the price hike. But if the price difference in rx 9060 xt and rtx 5060 ti is massive, then go for rx 9060 xt without hesitation.  Just get the cheapest one from decent brand. I got sapphire pulse.",buildapc,2026-01-24 12:22:46,1
AMD,o1eyebp,"Where are you located? Iâ€™m assuming based on your post history Germany, but thatâ€™s important to know. I was going to say you could get a [9060XT 16GB for $370](https://www.microcenter.com/product/696272/asrock-amd-radeon-rx-9060-xt-challenger-overclocked-dual-fan-16gb-gddr6-pcie-50-graphics-card), or a [5060ti for $430](https://www.microcenter.com/product/694730/zotac-nvidia-geforce-rtx-5060-ti-twin-edge-dual-fan-16gb-gddr7-pcie-50-graphics-card), but thatâ€™s not really relevant if youâ€™re on a different continent from me (or not near a MC).",buildapc,2026-01-24 12:25:08,1
AMD,o1eypo9,Went from the 1070 ti to a 5070 ti and it was life changing. In your scenario I'd buy a console honestly if you're only willing to spend $400. Probably better bang for your buck,buildapc,2026-01-24 12:27:33,1
AMD,o1eyq8r,"5070 is op, 5060 ti - if you can't find 5070.",buildapc,2026-01-24 12:27:40,1
AMD,o1f0fio,"5060 TI and heavily lean into dlss, I guess",buildapc,2026-01-24 12:40:25,1
AMD,o1febmd,9060xt 16gb. Do NOT buy 8gb. Also nvidia and Linux dont mix,buildapc,2026-01-24 14:08:36,1
AMD,o1fgm7q,$400 buys you a reaper 9060 xt.,buildapc,2026-01-24 14:21:39,1
AMD,o1fqmzw,"Snag a B580, in my oponion",buildapc,2026-01-24 15:15:16,1
AMD,o1frsnl,Wait for steam machine or get a B580,buildapc,2026-01-24 15:21:03,1
AMD,o1ewjm8,for 1440p and for Linux you should consider Intel B580,buildapc,2026-01-24 12:10:30,1
AMD,o1exm5q,"two main options:  1. 9060xt (16 gb version, not the 8)  ( [https://www.amazon.com/PowerColor-Reaper-Radeon-9060-GDDR6/dp/B0F9QM1M6R/ref=sr\_1\_1?crid=11X56MQPJO04O&dib=eyJ2IjoiMSJ9.bfLDzFIN2czvWyLlVfu2CvzPxsdmpq8LSGpHsVUXzP-f62atk0e3wzt2O-Ye\_GnDEYfa2T0Qf2fqQWKnNylIXQcHzBrgn6A5xviJlEIOr-CBnMQSksmOHdNmYoPWaaV0GLe4TAEIPt2ACHpP04XFMG6ZYCCuDUAipWzM-r6bxO0vjGNiaBsOXH4nK5tjpuEwdq05Bwkpa7yi6YkbtJAFmCWQtEN6fRCNOTc-qSWSOtP8vPK95aeoCHi6Nxnq1KEStuLJemxzgL49LXoRKMew6xNV-QSsNBrYd51UGbEaYrE.Zqt9feLkRA2UcnXvypUu65VzhFesXSVAiIJh-65Zdfw&dib\_tag=se&keywords=9060%2Bxt&qid=1769256999&s=electronics&sprefix=9060%2Bxt%2Celectronics%2C160&sr=1-1&th=1](https://www.amazon.com/PowerColor-Reaper-Radeon-9060-GDDR6/dp/B0F9QM1M6R/ref=sr_1_1?crid=11X56MQPJO04O&dib=eyJ2IjoiMSJ9.bfLDzFIN2czvWyLlVfu2CvzPxsdmpq8LSGpHsVUXzP-f62atk0e3wzt2O-Ye_GnDEYfa2T0Qf2fqQWKnNylIXQcHzBrgn6A5xviJlEIOr-CBnMQSksmOHdNmYoPWaaV0GLe4TAEIPt2ACHpP04XFMG6ZYCCuDUAipWzM-r6bxO0vjGNiaBsOXH4nK5tjpuEwdq05Bwkpa7yi6YkbtJAFmCWQtEN6fRCNOTc-qSWSOtP8vPK95aeoCHi6Nxnq1KEStuLJemxzgL49LXoRKMew6xNV-QSsNBrYd51UGbEaYrE.Zqt9feLkRA2UcnXvypUu65VzhFesXSVAiIJh-65Zdfw&dib_tag=se&keywords=9060%2Bxt&qid=1769256999&s=electronics&sprefix=9060%2Bxt%2Celectronics%2C160&sr=1-1&th=1) )  2. B580  ( [amazon.com/ASRock-Intel-B580-Challenger-Graphics/dp/B0DNV4NWF7/ref=sr\_1\_2?crid=2EUWOQO98VN6N&dib=eyJ2IjoiMSJ9.JjtaVfZt4jd8M-L3kjQLp6cZjDv-k\_6PbXUIO4VOZzdFgBIwrexBZYBgmFv5T\_zL9JtfRqMz\_cHIGqynVuYJJwsMTHx7lOhzUHyz6\_t5kZ8jZsWG2fKyrt-d\_yPIV-exkhe5KTaIJe7pytlUh71R\_jmuYMT3FD-323QxG1RqAdtl\_e6TwVgEQL15aTHYi1NHuUD-w4ZKd3KGQzBbwa3WhveUWQ3AeEJL6dJfSmsndDd9eAobN00Tc9dLY2\_ftmtz1T58J\_FqhC2ftqiaTjdCv16S2ZWjAaNupc867RO22ag.Tp3Xy0S3zdCiQ9K0XA\_rovEEMTvqjJFKoNybYm8Tudc&dib\_tag=se&keywords=b580&qid=1769257061&s=electronics&sprefix=b580%2Celectronics%2C144&sr=1-2](http://amazon.com/ASRock-Intel-B580-Challenger-Graphics/dp/B0DNV4NWF7/ref=sr_1_2?crid=2EUWOQO98VN6N&dib=eyJ2IjoiMSJ9.JjtaVfZt4jd8M-L3kjQLp6cZjDv-k_6PbXUIO4VOZzdFgBIwrexBZYBgmFv5T_zL9JtfRqMz_cHIGqynVuYJJwsMTHx7lOhzUHyz6_t5kZ8jZsWG2fKyrt-d_yPIV-exkhe5KTaIJe7pytlUh71R_jmuYMT3FD-323QxG1RqAdtl_e6TwVgEQL15aTHYi1NHuUD-w4ZKd3KGQzBbwa3WhveUWQ3AeEJL6dJfSmsndDd9eAobN00Tc9dLY2_ftmtz1T58J_FqhC2ftqiaTjdCv16S2ZWjAaNupc867RO22ag.Tp3Xy0S3zdCiQ9K0XA_rovEEMTvqjJFKoNybYm8Tudc&dib_tag=se&keywords=b580&qid=1769257061&s=electronics&sprefix=b580%2Celectronics%2C144&sr=1-2) )  Both will be significant upgrades, though the 9060 is better. If you live near a microcenter, they are selling 9070s for $500, which is an amazing deal, if you can swing it.",buildapc,2026-01-24 12:19:02,1
AMD,o1f081o,Exactly what I would say,buildapc,2026-01-24 12:38:53,5
AMD,o1ewhal,Also linux support on AMD is generally better which OP has expressed interest in,buildapc,2026-01-24 12:09:58,6
AMD,o1eyfvk,You need to specify the 16GB version as the 8GB version is kneecapped in most games.,buildapc,2026-01-24 12:25:28,3
AMD,o1eypjn,Make sure you get the 16GB versions if you get either of these.  The 8GB versions will struggle.,buildapc,2026-01-24 12:27:31,1
AMD,o1ez0dr,The B580 is significantly worse than rx 9060 xt 8 gb and rtx 5060 it would be a very bad purchase,buildapc,2026-01-24 12:29:50,-1
AMD,o1eyvns,"The B580 is stupid, its worse than both 8gb 5060 and 8gb 9060xt at 1080p, dont recommend garbage",buildapc,2026-01-24 12:28:50,-2
AMD,o1ewoxk,I didn't know that. Good fact!,buildapc,2026-01-24 12:11:42,1
AMD,o1eylqu,"Oh yes, that's true!  But, the most probably the OP is gonna see there're two versions of it, and probably buying the 16gb.  Gonna edit the comment!",buildapc,2026-01-24 12:26:42,1
AMD,o1f0bs2,Yup. Exactly. Do not get 8GB variant.,buildapc,2026-01-24 12:39:38,1
AMD,o1ezmqt,"Not worth it to buy a 8gb card in 2026, hoping it will carry you years into the future. I don't know what partisan cope you're smoking to make that more attractive than a 12gb card. You must have never run out of vram before - it is the worst case scenario, and a 12gb card for a low price is far superior to an 8gb card that costs more, for that reason. The value proposition just doesn't check out.  Beyond that, the B580 perform well, even in 1440p. I live with someone that uses one and the hate they get seems very much overblown by partisan pc brand wars.",buildapc,2026-01-24 12:34:31,1
AMD,o1f05wd,"Vram is not everything rtx 5060 and rx 9060xt smoke that card in every single game.  Stop with the vram circlejerk, the b580 doesnt get to use that vram before its raster performance runs out.  https://youtu.be/xlbNsP5ySmA?si=2n49RoIgYztU45k3 check the benchmark  Ofc a 16gb card would be better, but the 580 is bad compared to those 2  Im pretty sure the 5060, 9060xt 8gb and b580 cost the same",buildapc,2026-01-24 12:38:27,-2
AMD,o0ypoaz,"I love my 9070 non xt. It runs very cool, has solid OC potential. Low power draw means you can use a less potent psu. Performance is decent at 4k and solid at 1440",buildapc,2026-01-22 01:06:06,13
AMD,o0ypj3b,"The performance gap between the 9070 and 9070 XT is honestly not as huge as people make it out to be, maybe 10-15% in most games. At 1440p high settings you're still gonna get solid framerates with the regular 9070     If saving that extra cash means you can actually finish your build sooner rather than waiting months, I'd just go with the non-XT and call it a day. You can always upgrade down the line when prices aren't completely bonkers",buildapc,2026-01-22 01:05:17,29
AMD,o0ypoax,The one you have money for,buildapc,2026-01-22 01:06:06,12
AMD,o0ysg7p,"I like that 9070 will run at 220w while xt will run at 300w, while only being 10%ish slower. 32% lower power consumption, 10% lower performance. I'll take that any day.   But to be fair, I live in Phoenix AZ, with a SFFPC, so hot rigs are not fun.",buildapc,2026-01-22 01:21:53,7
AMD,o0yqbve,Nothing wrong with the non xt. I had a 6700 non xt and was really happy with how quiet and cool it ran. Most purple were saying to get the 9070xt mainly cause the non xt didn't go down in price enough. Might as well pay $50 for something better. If you see a fair bit of a decrease where you are then do that.,buildapc,2026-01-22 01:09:49,4
AMD,o0yr82s,"I picked the 9070 specifically because I wanted a GPU that would be below 200W during gaming, with a bit of tweaking to power, my 9070 runs at a max of 195W, at 1440p I can run any game without issue, and I run RT in Cyberpunk 2077 and still get 80-90FPS on average with FSR balanced mode.",buildapc,2026-01-22 01:14:54,4
AMD,o0z8z44,non-XT serves me really well. I went with it because the XT is around 25% more expensive and I love power efficient GPU.   > but is the actual difference that massive?  [around 10% fps difference on average](https://youtu.be/YWUqsqcM4Hs?si=8ZRhNp8x-zxmzHrQ&t=818),buildapc,2026-01-22 02:55:17,4
AMD,o0z6dvy,"There are a lot of factors were missing too. Factors like the games your playing, which monitor you have, what your other hardware you have, and if/when you're planning on upgrading, will all have a huge impact on accurate recommendations in a vacuum.  But considering it's your first PC, and you need to save for it to that degree, in this economy, my advice is neither. Save your money and get a used RTX 3070 on eBay. About 1/3 the price for about 2/3 the performance, and don't worry, it'll still crush 1440p in most games if you're not too greedy with the settings.",buildapc,2026-01-22 02:40:53,3
AMD,o0ziqi3,"I have a non-xt.  It's pretty great.  If the cost difference matters to you, you won't be leaving much behind in terms of performance.  I generally get 100FPS+ on my 3440x1440 display with most any game on higher settings.  I might have to turn this or that down on occasion, but I never feel like I'm missing anything with my setup.  I'm in a SFFPC case, so I was willing to trade peak performance for the lower heat generation, and I feel like I've found a good balance there.",buildapc,2026-01-22 03:53:13,3
AMD,o0zwmdm,"I have a sapphire nitro+ 9070, and I couldn't be happier with the performance at 1440p.  For an extra 100w of power usage you are getting about 10-15% more performance with a 9070xt over a 9070 but when you are already hitting 100+ fps in games it's not a huge deal.  Avatar: Frontiers of pandora runs at over 100 on high  Indiana jones and the great circle runs at over 100 on high  Fortnite on max settings including raytracing is over 130fps.   Honestly, the only problem with going for the 9070 over the 9070XT is most of the focus goes to the XT version so you'll see way more people talking about that in videos and such.",buildapc,2026-01-22 05:26:21,3
AMD,o0ypsfm,with the ability to easily flash the bios now I think that makes the 9070 even more appealing,buildapc,2026-01-22 01:06:44,2
AMD,o0ytzxi,"GPUs are probably only going to get more expensive with AI and reduced production, so make of that as you will. Assuming nothing changes things won't improve until around 2028, so personality id get the xt if you can afford it, makes your rig a little more futureproof and now is probably the lowest price it'll be for a while.",buildapc,2026-01-22 01:30:42,2
AMD,o0ywfzw,"at 1440p high settings the non-xt will be plenty fine, you're looking at like 15-20% performance difference which sounds bigger than it feels when you're actually playing. if you're not chasing 144+ fps or maxing everything out, save the money and don't think about it again.",buildapc,2026-01-22 01:44:41,2
AMD,o0zz3z1,"My brother in Gaben buy the one you can afford. If you keep waiting they will go out of stock, then down the line you will regret it. If you have the money, and want to buy it now, just do it.",buildapc,2026-01-22 05:45:02,2
AMD,o10rn4e,"Iâ€™m currently waiting to buy a 9070XT for my first ever buildâ€¦. But Iâ€™m one of those guys that when I put my mind to wanting something, thatâ€™s what I want. And I hate that sometimes",buildapc,2026-01-22 09:57:59,2
AMD,o10vhlt,You pay low bills? 9070 XT  You don't? 9070,buildapc,2026-01-22 10:32:55,2
AMD,o12006n,"Get the XT if you can.  Honestly you're going to eventually play something that's barely running at 55 FPS, and at that moment you would really wish you bought the XT and had that little extra power.",buildapc,2026-01-22 14:52:15,2
AMD,o13ymjq,you are tight on dough so you need future-proofing. 9070xt.,buildapc,2026-01-22 20:12:36,2
AMD,o0zkt1f,I was looking at RX 9070s but ultimately couldn't justify the expense. However they will be excellent 1440p or 4K gaming GPUs.,buildapc,2026-01-22 04:06:16,1
AMD,o0zvjkt,"I got myself a 9060xt 16gb because 9070(xt) is just about double the price where I'm from, and with that money I could upgrade to a much better gpu a few years down the road.",buildapc,2026-01-22 05:18:31,1
AMD,o10tbsr,"I got a 9070 non XT and I'm honestly really happy with it. The performance difference isn't that big, and it is a lot more efficient. I play 1440p, most things on very high/max, it handles everything great.",buildapc,2026-01-22 10:13:32,1
AMD,o10te2c,"Can't remember where I first heard this, but the best (insert part here), is the one that you can afford. And the regular 9070 is still a modern, mid-high end GPU. I have the 9070xt, and it's been running all of my games at 1440p, maxed settings, and barely breaking a sweat, so I can't imagine the regular 9070 would be all that much different. You'll be fine.",buildapc,2026-01-22 10:14:07,1
AMD,o13h4sn,I'm really happy with my 9070 - as mentioned in other comments it is one of the best performance per watt cards on the market. 220W instead of 304W with the XT for only 10-15% performance loss is worth it for me.,buildapc,2026-01-22 18:53:12,1
AMD,o1afhyj,9070 non-xt is very high on the performance/dollar scale at the moment. It's slightly slower but you get a lot more for your money.,buildapc,2026-01-23 19:03:38,1
AMD,o0ysnz4,"Ray Tracing is becoming non optional with new AAA games. 9070 XT is about 10% better then the non XT, so you decide if that's worth the extra money to you. Personally I would go for RTX 5070 instead of the non XT 9070, DLSS 4.5 is really good and it's even cheaper.",buildapc,2026-01-22 01:23:07,1
AMD,o0ywxyu,Consider the 9060xt 16gb. What monitor do you have. That really matters! 1080p-1440p-4k is a huge difference. As is the frequency. Doesnâ€™t matter if your GPU can push 600fps if your monitor can only output 120fps. Iâ€™d rather get a 9060xt 16gb and a monitor upgrade than a 9070 or 9070xt.,buildapc,2026-01-22 01:47:33,0
AMD,o0yw6gb,"It depends on your objective. Obviously get your ram asap. Check out the used market for that. That will save you tons. Iâ€™m always about getting a bit more performance than you think you need so your system will have longevity. But getting going can be a greater good, so donâ€™t think you need to wait. The 9070 will serve you well. I did a few builds. The 9060xt system I built for their momâ€™s house does great for what they play. I have a 3060ti that still works great for a lot of what they play. My other two, 7900xtx and 9070xt are overkill, but the 3060ti was struggling, and certainly canâ€™t do 4k on the 32 in screens I have. Itâ€™s on a 1440p screen.",buildapc,2026-01-22 01:43:11,0
AMD,o10o1y1,"gtx 970, smirkcat",buildapc,2026-01-22 09:23:52,0
AMD,o0yrwon,"Yeah it's a beast of a card, and i don't know why but i haven't seen people having issues with their 9070. About the XT i read it all the time. Might be simply because the XT is much more popular, who knows.  I was actually just messing around with undervolting and OCing the 9070 and managed to run it upto 3200 seemed pretty stable so far.",buildapc,2026-01-22 01:18:45,4
AMD,o0zcsgw,On average it's about 13% or 11 FPS difference at 1440p.  9070 is way more power efficient too.,buildapc,2026-01-22 03:17:14,10
AMD,o0zpimu,"Got a regular 9070 on 1440 myself, eats everything I throw at it. Even pathtracing in 2077 is playable with FSR4.",buildapc,2026-01-22 04:36:57,4
AMD,o11e7kf,"I think the reason people make it out to be such a big difference is because in a lot of places the price difference is only about â‚¬50 and the more expensive models of the 9070 are selling for more than the cheapest 9070 XT models.  In the Netherlands at the reputable stores, the 9070 is selling for â‚¬650 to â‚¬780 (with most models being â‚¬700 or more), while the 9070 XT is selling for â‚¬700 to â‚¬900.",buildapc,2026-01-22 12:54:52,2
AMD,o104epe,prices will never decrease on gpus only increase with the value of gold. Sacrifice something else to make the difference happen for an XT.,buildapc,2026-01-22 06:27:07,-1
AMD,o0zsyqq,"I mean you could just set a -25% power limit in adrenaline and it will bring it down to around 220 with more performance. If the price is close enough, the xt is the way to go. But the 9070 is a beast too if that's what you want.",buildapc,2026-01-22 05:00:17,1
AMD,o0zwv0r,"This matches my experience at 3440x1440 in our SFFPC (Fractal Terra). The 9070 does a great job. We have a 5070 Ti in the other PC and unless you have an FPS counter on, the differences aren't perceptible in most games.",buildapc,2026-01-22 05:28:06,3
AMD,o1ilwcb,That 15-20% performance difference only makes since at 4k at 1440p is whatever cheapest gpu,buildapc,2026-01-24 23:09:07,1
AMD,o0z49n3,"This! Or a 3070 or 4060 ti, nearly the same performance, but $100+ cheaper yet on eBay. If you're at the point where you need to save for a couple of months to get a GPU, you probably shouldn't be getting one, and if you do, you should get the cheapest thing you can get away with that gives you the performance you want.",buildapc,2026-01-22 02:28:58,1
AMD,o0zsac8,"For a couple months the XT was available below MSRP while it's competition, the 5070 ti was $200 more expensive. So everyone and their grandmother bought one (me included)",buildapc,2026-01-22 04:55:39,3
AMD,o10pilj,"Indeed, 9070 uses 220W, while 9070XT uses 305W. That's space heater level energy usage for at most 15% more frames.   I would get the 9070, save on psu, fans and electricity now, and save on your air conditioning in the summer.",buildapc,2026-01-22 09:38:00,8
AMD,o11ob58,"same here, ended up getting one for $480 on black friday, thatll be it for the next few years for me.",buildapc,2026-01-22 13:52:11,1
AMD,o11hs5h,"You can get offline and line interactive UPSs with pure sinewave inverters, theyre just a little more expensive.  For calculating what size you'd need, you will need to figure out the power consumption of your pc as well as any extras (like monitors) you plan on having plugged in. I tend to recommend just getting a 1500va unit though, more run time and they dont cost that much more than a 1000 or 850va.",buildapc,2026-01-22 13:16:15,1
AMD,o11imrq,"You probably won't need more than like 800 watts including the monitor, depending on how old and big it is.",buildapc,2026-01-22 13:21:08,1
AMD,o18et63,"I did some research, but unfortunately, there is no manufacturer or supplier of the model you mentioned in my country. Even if there were, they would be low-quality models. What do you think I should do in this situation? Should I buy a line-interactive one? I'm consulting you based on the information I found online because I'm quite unfamiliar with this type of equipment. Naturally, when someone online says â€œsimulated sine waves are harmful,â€ I get worried. Please excuse my lack of knowledge.",buildapc,2026-01-23 13:20:28,1
AMD,o18e71w,"Would you recommend the Schneider 1000VA? My goal is to shut down the system properly; I have no other purpose. Please excuse my ignorance, as I am very unfamiliar with these types of components.",buildapc,2026-01-23 13:16:58,1
AMD,o18frji,"What country are you in? Do you have any specific models in mind, if so send the model numbers and I'll take a look and Steer you towards one or another.",buildapc,2026-01-23 13:25:47,1
AMD,o18h44g,"Honestly i'm not the right person to ask which specific one you'd want, but if you do some research and look at some (real and unsponsored) reviews you're probably fine.",buildapc,2026-01-23 13:33:08,1
AMD,o18lz0v,"I live in Turkey, and as citizens, we are not treated like â€˜human beingsâ€™. That's why the electrical infrastructure is so poor.   I have the Schneider 1000VA line interactive model in mind.   To be honest, I don't trust my power supply because it's the first time I've used a power supply from this brand. I honestly don't know how this PSU will work with a UPS. My only goal is to be able to shut down my computer properly during power outages. I don't think I'll be using the battery for minutes on end, which is why I chose the 1000VA model.",buildapc,2026-01-23 13:59:01,1
AMD,o18r9ud,"Schneider here in the states is APC. They've got a pretty good reputation, I would trust it personally.  Its pretty hard to find stuff specifically sold in turkey, outside of the data center specific stuff from vertiv and tescom. And while they do sell desktop units they'd likely be way out of your price range.",buildapc,2026-01-23 14:26:20,1
AMD,nyvrtkt,"Great combo, if you go on Amazon you can get the upgraded version Thermalright Phantom Spirit for around $35, or for a tad more the Phantom Spirit Evo! My wife has the regular one and I have the Evo version, both rock.   As far as the 5800xt goes, great choice! I've beaten myself up countless times for not upgrading my wife's 5700g when the 5800xt was selling for $125 last year. Regardless upgrade and sell your old 3600 with the wraith prism that's included with the 5800xt. Easy $75 to put back in your pocket.   You have a good plan ðŸ‘  Also my wife's case has the 160mm clearance and it works perfect. Her ram sticks were a tad tall so I the first fan up a tad and doesn't affect anything! Or you could move front fan to the back and only lose 1-2 degrees of efficiency ðŸ¤Ÿ",buildapc,2026-01-11 00:19:26,12
AMD,nyvtjkv,"""My CPU is running very hot and some games that I have played for years are starting to freeze my computer and I have to forcefully turn it off and back on.""   This usually means a CPU repaste rather than a replacement...try that before you drop a Benjamin plus.",buildapc,2026-01-11 00:28:34,13
AMD,nyvxdvx,>My CPU is running very hot and some games that I have played for years are starting to freeze my computer and I have to forcefully turn it off and back on.  Have you tried reapplying the thermal paste on the CPU cooler? Thatâ€™s the most likely issue.,buildapc,2026-01-11 00:47:54,3
AMD,nyvyv16,Thatâ€™s about as good as you can do since the x3d CPUâ€™s are so expensive now. Can you run your ram at a higher speed? Youâ€™ll see a good improvement if you can,buildapc,2026-01-11 00:55:17,3
AMD,nyw0g80,You donâ€™t need an upgrade. You need to remaster it and maybe get a better cooler. Iâ€™ve had a 3600 for 5 years and it champs.,buildapc,2026-01-11 01:03:44,3
AMD,nyvrvm2,Very good plan! That's gonna be your best bet for sure,buildapc,2026-01-11 00:19:43,2
AMD,nyw280s,Do consider getting a 3200 Mhz RAM kit if possible later down the road. It makes a noticeable difference.,buildapc,2026-01-11 01:13:28,1
AMD,nyxevq4,kinda sucks that I saw a 5800xt going for like 150 a few months ago but I figured it would be even cheaper in like 6 months or a year when I was planning on upgrading.,buildapc,2026-01-11 06:00:33,1
AMD,nyvroua,"Both great choices. Unless you trip over an msrp 5800x3d, that's the best you can get without going ddr5",buildapc,2026-01-11 00:18:45,1
AMD,nywsatr,i like the aesthetic of the one on amazon with the digital display and its only like 10 bucks more than the non digital display one.,buildapc,2026-01-11 03:36:42,1
AMD,nyzvuoq,Only downside is that it will block my second ram slot. I will have to remove my third ram stick and downgrade back to 16gb ram.,buildapc,2026-01-11 16:42:09,1
AMD,nyvy4nv,"I figured this might also be the case, but I do not mind upgrading as I can afford the new components.",buildapc,2026-01-11 00:51:32,5
AMD,nywnp1e,I would check the mounting of your CPU cooler.  Uneven pressure can cause overheating as well.,buildapc,2026-01-11 03:10:44,1
AMD,nyvy5up,"I figured this might also be the case, but I do not mind upgrading as I can afford the new components.",buildapc,2026-01-11 00:51:42,1
AMD,nyvz0nh,"I am not sure, I have never tried increasing ram speed.",buildapc,2026-01-11 00:56:06,1
AMD,nyw0sjk,"It does work good! Like I said, it has almost been 6 years and it is still going strong.",buildapc,2026-01-11 01:05:36,1
AMD,nywtneg,"I almost bought that one, but reviews were inconsistent about the reliability of the display so I played it safe and got the PS Evo off eBay for $40 total, been satisfied so far!",buildapc,2026-01-11 03:44:31,1
AMD,nz05atv,I guess I'm a little confused. The fan on the cooler will block the ram stick? If that's the case you can move the fan to other side of the cooler if that doesn't bother your aesthetic,buildapc,2026-01-11 17:26:57,2
AMD,nywinir,"I would say 5800xt. It is cheaper than 5800x for some reason and a Lil faster. Buy a beefy air cooler tho, nx600 or pa120se",buildapc,2026-01-11 02:42:30,2
AMD,nyvznkp,"You may want to try anyway, in case you have other faulty components that are causing the freezing/crashing (eg could be bad mobo, ram, PSU)  If one of your other components is bad, that may change your purchasing decision. _Eg if itâ€™s a bad mobo you may want to switch to Intel and get a faster 14th gen CPU for the same price._  Also, right now you canâ€™t really sell the CPU in good faith as â€œworking conditionâ€ because it could well be faulty. That changes how much money you get back from selling it.",buildapc,2026-01-11 00:59:28,2
AMD,nywx9ru,"the display model is just the ps evo with a magnetic attachment that has a single connector for the mobo. you could probably buy the magnetic attachment/ display thing separate if you wanted. i just look it because i get curious about temps. i dont have to like run a separate program to see where im peaking at.   it is a software start at windows (there is a linux program as well) but the software is barebones, hence the ability for fans to make a linux program.",buildapc,2026-01-11 04:05:28,1
AMD,nz05k46,I am just looking at pictures of videos of it. I will have a tech expert install it for me and they can decide the best way to orient it.,buildapc,2026-01-11 17:28:09,2
AMD,nyx9opl,Is the wraith prism that it comes with not sufficient enough?,buildapc,2026-01-11 05:22:26,1
AMD,nz0ko9g,"I'm sure they'll get it set up for you! Enjoy whatever upgrades you go with, I think you have a good plan",buildapc,2026-01-11 18:36:59,2
AMD,nyyfozn,"No, they run quite hot, and a twin tower, dual fan is likely what op will need, possibly an aio if planning on an oc",buildapc,2026-01-11 11:30:10,1
AMD,o1ep8cc,"Itâ€™s a great beginner PC, youâ€™ll be able to play any esports title at high fps and youâ€™ll still be able to play modern AAA games at reasonable settings at 1080p 60fps",buildapc,2026-01-24 11:08:00,3
AMD,o1eoy92,That's a decent 1080p combo. Make sure to get a < 25' inch 1080p 165+ Hz monitor. You'll have a great time.,buildapc,2026-01-24 11:05:26,2
AMD,o1er0xq,> I did look at intel but I hear the Ryzen are lower power consumption with much better ram speed which is important for gaming  THERE IS NO OBSERVABLE DIFFERENCE because you're using a relatively weak gpu. in a gpu bounded situation [cpu choice doesn't have much impact](https://i.ibb.co/j92JMb6V/untitled.jpg) so just pick whatever best suits your need and budget,buildapc,2026-01-24 11:23:54,2
AMD,o1epjxc,"With how RAM prices are right now, youâ€™ll get better performance per dollar by only getting 16GB RAM and putting the extra budget into your GPU. Just looking at UK prices, a 16GB stick of DDR4 is now around Â£50, and that could bump you from an RTX 3060 to an RTX 3070 which will give you 50% more performance. There are still only a couple of games out there that struggle with 16GB RAM, so I would argue the utility of 32GB in a budget build is minimal.",buildapc,2026-01-24 11:10:51,1
AMD,o1eseg6,I'd go for 16 GB RAM and use that money for RTX 4060 or 4060 TI 16 GB,buildapc,2026-01-24 11:36:02,1
AMD,o1f5qgc,This is currently my set up on my second PC.  It's fine for 1080p 60fps. It will do some titles even at 1440p but will struggle with some new titles even at 1080p on max settings.  I personally would rather get 16GB RAM and put the extra money in a faster GPU. The 3060 is starting to show it's age.,buildapc,2026-01-24 13:17:02,1
AMD,o1gsb26,What's your budget?  What games do you plan on playing?  Are you buying your parts used?,buildapc,2026-01-24 18:05:53,1
AMD,o1eqc22,Didn't think about the screen. Was hoping to keep my 60hz 1080p Sony for the task. There goes another Â£200+.   Jheez this pc gaming it's beginning to hurt,buildapc,2026-01-24 11:17:46,0
AMD,o1ft3zv,Thing is with the price of that CPU anything similar in price will be alot weaker. And then something that costs alot more will either be only a bit more powerful or alot more money. For reference I've seen a Ryzen 5600 with a rtx 2060 for 360Â£. I'm thinking sell the 2060 for the 3060. Adding Â£50.   The intel i5 9th with the same GPU is Â£320-Â£360  and it's been well used compared to this recently built Ryzen. I'm taking the 5600  Anything better is Â£500-700,buildapc,2026-01-24 15:27:33,1
AMD,o1eq4m5,Or save the Â£50 and keep the same gpu and 16gb ram sounds like a better plan. I'm already at the peak of my budget. I was even thinking of getting a gtx 1080 to save the extra Â£80 over the rtx 3060,buildapc,2026-01-24 11:15:56,1
AMD,o1etcxw,You can absolutely use your old monitor. Having a high refresh rate panel is nice but it's not mandatory especially if you are tight on money. And even if you had 200Â£ to spare I would spend it on a better graphics card since the 3060 isn't amazing.,buildapc,2026-01-24 11:44:20,3
AMD,o1er15l,"Lol it's a bit expensive to get into! But it's worth it.    Honestly  if you are going used I would aim for this if you want a budget PC ( will someone budget)  CPU: 5600/5700 X or X3D ( maybe someone sells an X3D cheap if you get lucky)  GPU: ideally a 4070 ( 3XXX sadly is really showing it's age)  SSD: min 1TB , preferably 2TB   RAM: 16GB min , 32GB is nice but not necessary",buildapc,2026-01-24 11:23:57,2
AMD,o1f4xia,>Didn't think about the screen. Was hoping to keep my 60hz 1080p Sony for the task.   Who said you can't keep it? If you are happy with your TV/Monitor there is zero reason to get a better one.  Your set up is ideal for 1080p 60 anyways. You will not get 165 fps on many newer titles.,buildapc,2026-01-24 13:11:49,1
AMD,o1g9ic5,"You can always just keep your current monitor and upgrade later.   In that case then, your build is way overkill for a 1080p 60hz monitor.",buildapc,2026-01-24 16:43:07,1
AMD,o1es0bm,I would definitely at least get an RTX level card as DLSS is genuinely a game-changer over older generations. If you have to go down to an RTX 2060 Super thatâ€™s still a much better buy than a GTX 1080.,buildapc,2026-01-24 11:32:33,2
AMD,o1j4rh7,Probably a PSU upgrade and then an RTX 3060 and then a Ryzen 5 3600  Edit: Also forgot to mention 2x8gb of DDR4 RAM up to... i think it's 3600mhz / MT's RAM for that Ryzen sweetspot  But honestly any speed is ok as long as it's 2x8gb,buildapc,2026-01-25 00:47:05,2
AMD,o1j34pg,"Another 8GB of RAM is mandatory.  Update the bios on your motherboard and get at least a Ryzen 3600 (or 5500 if cheaper), or ideally a 5600 (non g) or 5700X.   GPU wise, if you are going used to save money either an Nvidia RTX 3060, or Radeon RX 6600 or newer.  This is all bearing in mind your as cheap as possible criteria.",buildapc,2026-01-25 00:38:36,1
AMD,o1jy0xr,"Define, ""too much,"" in currency.",buildapc,2026-01-25 03:29:30,1
AMD,o12kzjd,The bottleneck is in your mind. A figment of your imagination. Just have fun my dude.,buildapc,2026-01-22 16:29:45,14
AMD,o12k98a,This PC is good. No upgrades. Have fun.,buildapc,2026-01-22 16:26:31,10
AMD,o12mb88,"no bottleneck, just save that money for later bud",buildapc,2026-01-22 16:35:37,3
AMD,o12kuym,You have no bottleneck to worry about and that build should still work great.,buildapc,2026-01-22 16:29:12,2
AMD,o12lmgf,"The PC is fine. Normally I'd say that the CPU is a bit too weak for the GPU, but with the current situation I wouldn't change anything.",buildapc,2026-01-22 16:32:35,2
AMD,o12kxeo,"> What do y'all reckon my bottleneck is?  Depends on the situation. It's not a static thing you can point to unless you're running *grotesquely* mismatched hardware, like an RTX 3070 Ti alongside a Pentium 4.",buildapc,2026-01-22 16:29:29,1
AMD,o12l3ri,"Your parts are all relatively modern, maybe the 5600 might be a little slow but it's more than enough for the 7800xt and shouldn't be a bottleneck in anything but the most CPU demanding games like dragons dogma 2 or something.  Just have fun.",buildapc,2026-01-22 16:30:16,1
AMD,o12lfpq,I donâ€™t see a bottleneck. Enjoy it and that badass 7800xt until it canâ€™t play the games you like playing.,buildapc,2026-01-22 16:31:46,1
AMD,o12lnv5,"I mean, your pc is good. This is a horrible time to be upgrading, especially with only $1000. ddr5 ram alone is like half of your budget, and the upgrade will be miniscule.  There's no worthwhile upgrade for your am4. Your gpu is great still.   I'd say consider a monitor, peripherals, or chair upgrade honestly. Lowkey lighting upgrade is underrated, having dedicated warm ambient and task lighting is much nicer than bright overhead lighting",buildapc,2026-01-22 16:32:45,1
AMD,o12lvaz,"If youâ€™re really itching for an upgrade that would be a 9070xt and a 5700x3D.  It might be a little over 1000$, but it would be a killer upgrade. Additionally I should add your current PC is already really good. But if you have the itch, the cash and want to upgrade whatâ€™s the route I would go.",buildapc,2026-01-22 16:33:41,1
AMD,o12m4mt,U don't have one but since I saw u put 1000 aside for the upgrade why not just upgrade to am5? For 800 u can get a 9700x a gigabyte b850 mobo and 32gb of ddr5  https://pcpartpicker.com/list/HghWPJ  If u can swing it get the 9800x3d,buildapc,2026-01-22 16:34:49,1
AMD,o12n6l8,"Put that 1000$ back in your savings account because now isnâ€™t the time to buy a new platform. RAM is absurd, new GPUs that would be a reasonable improvement over a 7800 XT are rising in price fast, and AM4 X3D chips have been out of production for over a year and are insanely expensive.  You could do a 9070 XT and a Ryzen 5700x if you have a micro center near you and that money has a bomb set to detonate in a day I guess but, frankly Iâ€™d just ride out what you have for a couple years more if you donâ€™t have any actual problems.",buildapc,2026-01-22 16:39:28,1
AMD,o12qb3q,"Bro, invest that band  Your PC is perfectly balanced",buildapc,2026-01-22 16:53:27,1
AMD,o12rfo4,I have a similar spec pc. I plan on getting a 9070xt than moving to am5. Ill prob go with a 7800x3d.,buildapc,2026-01-22 16:58:30,1
AMD,o12rwk8,"As others have said, youâ€™re better off holding the $1k for an upgrade later down the line.  But if you absolutely HAVE to go to AM5 RIGHT NOW, just get any of the X3D chips with 32 gigs of RAM. The 7800/9800X3D will give you plenty of headroom for a long time, while the 7600X3D (if youâ€™re near a microcenter) would be cheaper + probably better suited if youâ€™re planning to upgrade again on AM5  Again thoughâ€¦ you really donâ€™t need to upgrade",buildapc,2026-01-22 17:00:38,1
AMD,o13r24b,"I saw your update but bottlenecks happen no matter what and change depending on what games you play. The goal is to minimize this bottleneck.   An ultrawide will help shift the load more to the GPU though, which is more ideal tbh",buildapc,2026-01-22 19:37:51,1
AMD,o13r2o2,>Casually comments he's using Linux without nobody asking nor being relevant at the bottlenecking   The slander is real,buildapc,2026-01-22 19:37:55,1
AMD,o15mg2j,"U mean from a normal 1440p to and ultrawide 1440p..30% increase in pixels...try it first see if u are ok with the decrease in performance. 7800XT is a good enough GPU but likely struggle with RT.  Without a GPU upgrade, the other upgrade may yield minimal improvements, especially on an 1440p ultrawide..something which u may not be happy with. Getting a 9070xt may be an alternative option.",buildapc,2026-01-23 01:17:23,1
AMD,o12l1tu,"Dawg, I have $1000 set aside for this lmao, so Im definitely going to be upgrading, Im just not sure what to",buildapc,2026-01-22 16:30:02,0
AMD,o12m0f8,Then wait set aside 2k and blow it all when the next gen releases.,buildapc,2026-01-22 16:34:18,8
AMD,o12li1r,Retirement,buildapc,2026-01-22 16:32:03,5
AMD,o12n59j,"Listen to this man. You've got a solid 1440p rig that will play any of today's games. You'll be paying 1k just to see your fps counter go up at this stage. Invest that money. Ideally into a tfsa or retirement.  But if not, I'd buy ETFs now and in 2 more years use that money to overhaul when it's worth it and your money grew",buildapc,2026-01-22 16:39:18,4
AMD,o12p2s0,Save another 1k blow it on a 6080 next year,buildapc,2026-01-22 16:47:56,3
AMD,o12n0ts,"You don't have a bottleneck right now, you can upgrade, but you will not see a performance increase other than 1% lows most likely. If you are upgrading now with the intention of being able to upgrade the GPU later, then the combos at Microcenter are by far the best deals if you are anywhre near one.   If you want 32gb ram i their bundles, you need to go 9700x or better, which was is $599. It was $499 a month ago and $449 like 6 months ago.   GPU upgrade would be a better investment if it's for gaming. You will bottleneck some but 9070xt is proabbly still going to give you around 40-50% performance increase whereas a CPU will not give you anything noticeable.",buildapc,2026-01-22 16:38:45,2
AMD,o130l8y,You asked for what your bottleneck is and our answer is that it's a well balanced system.,buildapc,2026-01-22 17:40:10,2
AMD,o12l66n,The CPU,buildapc,2026-01-22 16:30:34,1
AMD,o12o39z,Go to microcenter and get a 9800x3d bundle. Itâ€™s $750. You could get away with a 7800x3d if you wanted to. If all youâ€™re doing is gaming and then regular stuff on the side- youâ€™d be fine.,buildapc,2026-01-22 16:43:31,1
AMD,o0mkp5f,"u shouldn't mix mix ram with any cpu but it probably won't be a big deal u could just manually tune it yourself even if they are different dies to get something decent but that requires some tinkering. Could also get lucky and find out they are the same die which would be nice.  the 5700x would be somewhere between a 12400f and 12600k/kf or around the 10700k,11700k, 11900k, 10900k, 10850k.",buildapc,2026-01-20 06:59:36,4
AMD,o0mnu7m,"10700k/11700k iinm. I have the 10900 before and it's pretty much stable with xmp set @ 3600MHz.  But if you want the best intel cpu for gaming then it's the 14600k/14700k. Those are pretty good in gaming and workload situations. The e-cores don't do much in gaming but for everything else, it's pretty good.  Remember to always update your bios to the latest microcode if you intend to buy either of those cpus.",buildapc,2026-01-20 07:26:55,1
AMD,o0p1vzu,"Given that the 12600k beats the 5800x and the 5700x is a downclocked 5800x, I would say this is the closest Intel equivalent to the 5700x.",buildapc,2026-01-20 16:58:06,1
AMD,o0pw3cm,"Depending on what programs compare, anything from a 10600 to 12700K. AMD and Intel CPUs are very different, and only got even more different, as of 12th gen. If you can swing it, buying new, consider the 5800XT.  > it's a problem and to not mix ram sticks with a AMD CPU.  No, in that it can be a problem with Intel CPUs, too. But, a pair that works well on one may not on another. Things usually work OK, with DDR4, but mixing RAM is a bit of a lottery, if you can't verify the ICs being used.",buildapc,2026-01-20 19:15:21,1
AMD,o0mkknh,Donâ€™t go intel and about the ram: you will have the chance it not working or it will run at the speed (in this case latency) of the slower ram. Xmp will be tricky because itâ€™s already fonky.,buildapc,2026-01-20 06:58:34,0
AMD,o0ml5tf,"How does it work if I want to tinker?   I put the hardware in the case, turn on the PC. Is there a chance of it not working there already or will it always go to the basic **without** xmp 2133 and I can tinker later?",buildapc,2026-01-20 07:03:29,1
AMD,o0ml3rn,"How does that work?   I put the hardware in the case, turn on the PC. Is there a chance of it not working there already or will it always go to the basic no xmp 2133 and I can tinker later?",buildapc,2026-01-20 07:03:00,1
AMD,o0mm3n4,yeah JEDECÂ should run fine. Different JEDECÂ profiles do exist but the system should default to the slower one if they are different.,buildapc,2026-01-20 07:11:37,1
AMD,o0mrzzc,There IS a chance not working at all but probably it works with 2133,buildapc,2026-01-20 08:04:18,1
AMD,o0mmuia,"Thansk you. Do you think the risk is big enough that I better switch to Intel or is the different neglectible if I lower xmp speeds or even turn it off?   If I run the ram on 2133 speed instead of the 3000 it works on now, do I lose a lot of performance? I'm pairing with a 90690xt so my hardware isn't top notch to begin with, I don't know if that matters.",buildapc,2026-01-20 07:18:08,1
AMD,o0n41yk,You know xmp is from intel and basically indicates that it works with intel cpuâ€¦even you shut down xmp,buildapc,2026-01-20 09:57:56,1
AMD,o0nx9si,Xmp works with amd too,buildapc,2026-01-20 13:37:43,1
AMD,o1dpnjt,5090,buildapc,2026-01-24 05:51:06,94
AMD,o1dpp5a,"Reject the term Bottleneck from your vocabulary. You won't have to worry much about bottleneck issues, at least issues that will impact your gameplay....  Best GPU with a 5600x, and you're with a 3060 now...Well if you're running an 8GB model or a 12GB. If you're on a 12GB model, I'd honestly stick with it. 70FPS, in what kind of game and 1080 or 1440? If you're running 1440, and running an 8GB card on a game that's using all 8GB, then you've identified your issue. If that's not the case, then it would be best to see whats holding you back. Is the CPU maxing out usage in that game? or is it maybe because you're running out of RAM.  Lots of info we need here before making a real recommendation.   A 4070 would be a decent bump in performance, but you might have better options depending on availability and budget.",buildapc,2026-01-24 05:51:27,29
AMD,o1dpwfd,"4070 or 5070 would be very reasonable options. You could go up to 5070Ti levels, but I wouldn't go past that.",buildapc,2026-01-24 05:53:03,11
AMD,o1elpk9,This gets asked every day and the answer is always the best gpu you can afford.,buildapc,2026-01-24 10:36:05,10
AMD,o1drwaa,9070xt or 5070ti,buildapc,2026-01-24 06:08:54,7
AMD,o1dp9u5,What resolution do you play at? If it's anything over 1080p then basically any card you can afford will be fine.,buildapc,2026-01-24 05:48:09,3
AMD,o1e3tur,"we need to know your resolution, the games you play, etc  single player games can range from vampire survivors to cyberpunk, you need to be more specific  quick note, you will ALWAYS have some sort of bottleneck going on, if you play a variety of games, it can change between cpu and gpu, no combo is perfect",buildapc,2026-01-24 07:51:57,3
AMD,o1ejjk8,9060xt 16gb?,buildapc,2026-01-24 10:16:16,3
AMD,o1enjn6,"The best you can. The 5600x is still very capable and a solid gaming CPU, at that point it just comes down to how much you want to spend.   Sure if you choose a 5090 it might bottleneck it a little, but realistically you'd be okay choosing any GPU that you can afford with that CPU",buildapc,2026-01-24 10:52:33,2
AMD,o1f7k48,5090,buildapc,2026-01-24 13:28:31,2
AMD,o1fybwz,I have a 5070 paired with the 5600x. I play at 1440p and have no issues. Gpu is between 97% and 99% utilization in almost all games,buildapc,2026-01-24 15:52:22,2
AMD,o1gcdcj,I would go for the 9070 non xt,buildapc,2026-01-24 16:55:41,2
AMD,o1gs5yo,I went from 3060ti to a 5070 and its night and day in the games I play. Paired that with a 5600x as well,buildapc,2026-01-24 18:05:17,1
AMD,o1gv2w0,"Just got a 16GB 9070xt to replace my 8GB 3060Ti for 1440p gaming, and itâ€™s a huge upgrade paired with my aging Ryzen 5900X.",buildapc,2026-01-24 18:17:37,1
AMD,o1hr2wh,"5070 ti if you can find one at a reasonable price is the go to. I wanted to rebuild my whole pc in the summer of 2026 to am5 and ddr5 seeing that the gpu market sort of cooled of. That was thrown out the window with the whole ram news, so i got lucky and bought a 5070 ti near msrp before they announced it would not be in production anymore. And postponed my pc rebuilt indefinetly. I play at 1440p and am very happy with my 5600x, 16gb ddr4 and 5070ti",buildapc,2026-01-24 20:40:18,1
AMD,o1ijq7l,"You'll be fine up to 5070 or 9070. Bottleneck kicks in above that tier increasing stutters. Others saying 5090 is best, well NS but you wont even utilize it and would have to limit fps.",buildapc,2026-01-24 22:57:55,1
AMD,o1dpkeo,9070xt-it might slightly bottleneck by like 5% but should be an awesome pairing.  Kinda depends on the games you play.  If you play CPU intensive games maybe a bit more.,buildapc,2026-01-24 05:50:26,1
AMD,o1dqe0f,1. 3060 is the 12gb  2. 1440p  CPU usage usually stays within around 40 percent and I just upgraded to 32 gb of ddr4 ram so I am not positive those are the culprits. I have no budget so hopefully some of that info would help,buildapc,2026-01-24 05:56:51,1
AMD,o1dpfen,1440p,buildapc,2026-01-24 05:49:21,4
AMD,o1fct7h,What game,buildapc,2026-01-24 13:59:51,1
AMD,o1dpm7y,"Yup. Whatever you can afford will be fine. Sure there might be some ""bottlenecking"" but unless you're playing insanely cou intensive games (which there are few of) you'll be fine even if you wanted a 4070 or whatever.",buildapc,2026-01-24 05:50:49,2
AMD,o1dquau,Yes thank you for the advice. I was unsure if I had to drop some serious money on a new setup or not ðŸ˜…,buildapc,2026-01-24 06:00:26,2
AMD,o1drhxc,No problem. Given the market if you don't already have ddr5 ram you can use I wouldn't recommend it. Unless you've got the serious coin ofc. But AM4 still plenty good and if you wanted to upgrade CPU you can still get the 5800XT 5700X and even 5900/5950X new for a reasonable enough price. So there's that.,buildapc,2026-01-24 06:05:40,1
AMD,o0f2s7p,"It's good for light gaming, speaking from experience since a friend still has it. But it's not gonna run new titles very well, if at all. Almost certainly not the new RE game.  I'd like to ask what price you're seeing this at, too. There may be way better options. If you can find a 580 8 GB for nearly the same price, I'd recommend that a lot more. But maybe we can find something even better for you.",buildapc,2026-01-19 04:02:14,17
AMD,o0fe8yo,"It's a solid little GPU. However, make sure you aren't spending too much on it (I.e RX 580 prices). If you're spending more than Â£100 on it, i'd just get a GTX 1660 Super/Ti over it, but if it's sub 50-60, it's fine",buildapc,2026-01-19 05:20:31,3
AMD,o0fq0xa,"Yes. The people who are saying no don't know what they're talking about. [Here's a video from last year showing a bunch of games and their performance on the RX 470.](https://www.youtube.com/watch?v=GU0BhOQnSPU) You won't be able to play super intensive games, but stuff like Cyberpunk, Red Dead Redemption 2, and Resident Evil should be playable at 1080P low.",buildapc,2026-01-19 06:53:54,3
AMD,o0f4b9t,Short Answer: No  Long Answer: Nooooo,buildapc,2026-01-19 04:12:08,7
AMD,o0f2npq,"Get an RX 580 8 GB, it still holds up decently for 1080p gaming, while RX 470 is really subpar, and suffers from some driver issues.",buildapc,2026-01-19 04:01:27,8
AMD,o0f2874,it's a MSI btw,buildapc,2026-01-19 03:58:41,2
AMD,o0fpewa,"If you're going for a older graphics card, look for 1660s/2060s/1080ti  Those cards will serve your needs much better and honestly rx470/570/480/580 were ripped dumped in huge quantities after the mining boom so I'd never go with that even if going for a gpu that old",buildapc,2026-01-19 06:48:51,2
AMD,o0h77jk,"Am currently testing anÂ **E3-1270 v2/16GB DDR3/RX 470 8GB/Garuda Dr460nized Gaming/256GB SSD boot drive/500 GB SSD storage drive** build. The major stutters that were happening while textures load from aÂ **1.5TB 5400 HDD**Â storage drive are gone. There are some micro stutters still but those are tolerable given the age of the system.  The 2022 Tiny Tina's Wonderlands (TTW) via STEAM running at 1080p LOW/MED/FSR 2.0 mix getsÂ ***88+ FPS***Â on the in-game benchmark...onÂ ***Linux***Â of all things!Â It's all in the settings mix. Been keeping me entertained even though it clearly is slower than my Ryzen 5 5600/B580 LE build that runs TTW at 1080p ULTRA with 95+ FPS. Even managed to undervolt the RX 470 by 243 mV to jump from 82+ FPS to the current 88+ FPS.  Another test just completed is that with a Linux distro and an AMD GPU that has no hardware ray tracing support (i.e., the RX 470 8GB), it is possible to have ***software emulation ray tracing***. Ran Little Nightmares Enhanced (STEAM) on 1080p ULTRA with HIGH Ray Tracing (the lowest available RT option) and got 10-20 FPS...unplayable...*BUT* with FSR Balanced turned on, then got 40-50 FPS that is totally playable. Did my test after seeing this video:  [https://www.youtube.com/watch?v=wDDdbb7wuCA](https://www.youtube.com/watch?v=wDDdbb7wuCA)Â (go to 01:00...***Indiana Jones on an RX 580***!!!)  So, if a RX 470 8GB can do all this with an E3-1270 v2...a 14 year-old CPU...with your 5500, the 470 certainly will be able to too.  The only caveat is don't overpay for that RX 470 8GB. Check recent Sold listing prices on eBay to gauge. Because of the success I got, I bought a new RX 580 8GB for $80.81 USD total for my E5-2697 v2 build that will be converted from Win10 to a Linux gaming distro.",buildapc,2026-01-19 14:06:31,2
AMD,o0fc4zb,"No it is not justified. I came from rx 570 4GB and i had a really bad time with many titles to achieve stable 60fps/medium to high settings, my bare minimum with newest of games. Terrible experience, bad lows, lack of VRAM, terrible optimizations since no more support from AMD thru drivers...a mess.  Don't buy it. I bought an 9060xt 16GB triple fan and i am very very happy with performance, temperature and efficiency let alone upscaling and frame generation is dope for heavy titles. Let alone 60fps is too easy to hit, i get 120-150-200 fps on 1080p max settings in most games, taking advantage of my 144hz monitor on single player games .. so dope.",buildapc,2026-01-19 05:05:19,1
AMD,o0fgcsl,You got a budget in mind? Any leftover parts?,buildapc,2026-01-19 05:36:24,1
AMD,o0g56cd,"Its a 10year old GPU, what do you think, it can run games from that era aka PS4 but anything newer it will struggle or not even work.  The 4gb vram will hold you back as much as raw performance.",buildapc,2026-01-19 09:11:04,1
AMD,o0f21qs,"Probably not adequate for most modern titles at 1080p medium settings, or even on low settings for that matter.",buildapc,2026-01-19 03:57:33,1
AMD,o0f7qju,Terraria       and Min  e  cr  aft.     Will WORK.  PROCEED.,buildapc,2026-01-19 04:35:00,0
AMD,o0f2yr4,There is no such thing as an RTX Radeon 470 try getting something like an rx 5700 XT or something,buildapc,2026-01-19 04:03:23,-2
AMD,o0f53jv,"Also why the fuck would you even get an RX 470? That's like the most insane shit I've heard of, there is no reason to get one, literally NO REASON",buildapc,2026-01-19 04:17:18,-5
AMD,o0fby0c,"Very, very light, 720p probably best for frames and clarity (no scope x software please on that",buildapc,2026-01-19 05:03:57,2
AMD,o0mm0vk,"i saw it at roughly 40 USD, and with what you said now i'm not gonna get it. thanks for that info ðŸ¤ðŸ™",buildapc,2026-01-20 07:10:57,1
AMD,o0mm87f,"this was super helpful, thanks ðŸ™",buildapc,2026-01-20 07:12:43,1
AMD,o0mm4ir,thanks ðŸ˜‚ðŸ˜‚,buildapc,2026-01-20 07:11:50,1
AMD,o0fbeig,"Yeah, at bare minimum, get this OP.",buildapc,2026-01-19 05:00:06,1
AMD,o0mm55i,thanks I'll look ðŸ™,buildapc,2026-01-20 07:11:59,1
AMD,o0mlvfl,"wow this was reall thorough and excellent, i really appreciate this and it was mad helpful mateðŸ¤",buildapc,2026-01-20 07:09:37,1
AMD,o0mmcqu,i'm trying to keep it under $100 and this is my first build,buildapc,2026-01-20 07:13:49,1
AMD,o0f26c7,how manu fps do you think i would be getting?,buildapc,2026-01-19 03:58:20,2
AMD,o0mmhey,that's why i'm asking ðŸ˜­ðŸ™ i've never built a pc,buildapc,2026-01-20 07:14:57,1
AMD,o0fqy35,Nah the rx 470 can play any game from 2019 and before at 1080p,buildapc,2026-01-19 07:01:39,2
AMD,o0fr1ch,You can play any game released in 2018-2019 and before at 60 fps 1080p,buildapc,2026-01-19 07:02:25,3
AMD,o0f3h3k,"You know its really hard to say.  Depends too much on the specific titles and settings you're choosing.  Just as an example, I'll take a wild guess that it could play Witcher 3 at 1080p at medium settings at around 40-50 fps.  Something like Cyberpunk would probably be barely functional, even on low.  Maybe 10-20 fps.",buildapc,2026-01-19 04:06:41,0
AMD,o0hh305,"Modern games, like gta 6 (which will never come out), call of duty black ops 7, possibly tarkov(?) on 720p so it isnâ€™t a slideshow",buildapc,2026-01-19 14:58:12,-1
AMD,o0fprjw,Cyberpunk will be more than 30FPS. Anything after the 400 series is surprisingly capable still.,buildapc,2026-01-19 06:51:45,1
AMD,o0hlakx,"I said 2019 and bellow, ofc that thing wont run gta 6, who even cares about that it will be released in like 2028-2029 for PC",buildapc,2026-01-19 15:18:55,2
AMD,o0hp1in,Modern aaa games are not runnable. 2019 is ancient,buildapc,2026-01-19 15:36:33,-1
AMD,o0hqknh,"Ofc he cant run modern AAA never said he could, 2019 is not ancient he can play a lot of very good games and all of the competitive ones",buildapc,2026-01-19 15:43:30,3
AMD,o0hut4p,"That is 7 years ago, brother. In game years, that is beyond retirement age",buildapc,2026-01-19 16:02:30,-2
AMD,o0hv7w5,"CSGO, Fortnite, league of legends, dota, Battlefield 1 still popular to this day and way older  He can even run games like Arc Raiders on medium 1080p 60fps  And Singleplayer games never age he can play so many good ones",buildapc,2026-01-19 16:04:20,3
AMD,o0hvri9,"In that case, Iâ€™m still playing online on my ps1, and itâ€™s from the 90s, about time for it to retire.",buildapc,2026-01-19 16:06:45,0
AMD,o18k87j,">ASUS has confirmed that it has started an â€œinternal reviewâ€ following recent reports of Ryzen 7 9800X3D CPU failures on their 800-series AMD AM5 motherboards.  >In their statement, ASUS has not unveiled why there is a spike in reports of 9800X3D CPU failures on ASUS motherboards. However, ASUS claims it is working closely with AMD to validate recent reports and ensure stability and quality. It is also working to ensure it delivers â€œtimely solutionsâ€ to these reported issues.  >For now, ASUS recommends that 800-series motherboard users update their boards to the latest BIOS revisions. ASUS has also asked affected users to contact ASUSâ€™ customer service for direct assistance. ASUS also linked to an FAQ post to help users update their motherboardâ€™s BIOS.  >Official ASUS statement on recent ASUS AMD 800-series motherboard and AMD Ryzen 9800X3D concerns  >*We are aware of recent reports concerning AMD Ryzenâ„¢ 7 9800X3D CPUs and ASUS AMD 800-series motherboards, and we have initiated an immediate internal review. Our teams are conducting preventive checks on product compatibility and performance, working closely with AMD to validate reported cases and ensure ongoing stability and quality. We are looking to provide timely solutions to ensure our products and services meet expected standards.*  >*Users are also advised to update their ASUS AMD 800-series motherboard to the latest BIOS via ASUS EZ Flash or BIOS Flashback to help ensure system stability; we provide an official technical support FAQ with detailed instructions.*  >*Customers who are affected or have concerns are encouraged to contact ASUS customer service for direct assistance. We take this matter seriously and value our customersâ€™ trust, and we remain committed to transparency and to ensuring our products can be used with confidence.*  >â€“ ASUS",pcmasterrace,2026-01-23 13:49:53,42
AMD,o190vgg,"If itâ€™s anything like ASRockâ€™s internal review itâ€™ll be something like:  1) Thereâ€™s debris in the socket! Weâ€™ve cleaned it out and a new CPU in the same motherboard works. Not an ASRock problem! The motherboard still works so weâ€™re good, guys. Right?! Right?! 2) IF YOU HAVE NO PROBLEMS DO NOT UPDATE BIOS (X3) 3) Silence 4) Silence 5) Asked by â€œprominentâ€ YouTube figure a public event about it, they quietly tell him â€œoh yeah thereâ€™s an issue maybe with PBOâ€ 6) No public announcements 7) â€œFixedâ€ BIOS still causes failures 8) New â€œfixedâ€ BIOS also causes failures 9) â€œWeâ€™re working closely with AMDâ€ 10) Crickets",pcmasterrace,2026-01-23 15:14:15,128
AMD,o1ab36d,So I should just save money and buy a 7800x3D for now,pcmasterrace,2026-01-23 18:43:51,14
AMD,o18lovg,![gif](giphy|6ILjOfJ1oL7NAc9SQ7),pcmasterrace,2026-01-23 13:57:33,35
AMD,o19kr3y,*chuckles* I'm in danger,pcmasterrace,2026-01-23 16:44:17,10
AMD,o1abn8z,Man I hope gigabyte never gets this shit,pcmasterrace,2026-01-23 18:46:19,12
AMD,o1ag9wo,"Here comes GN, smelling some likes/subs",pcmasterrace,2026-01-23 19:07:13,5
AMD,o1a4std,I have the asus rog strix x870e e gaming wifi mb And 9800x3d worked fine then after 2 weeks pc wouldn't post the cpu ended up dying and had to get a replacement but didn't noticed any damage to the board or cpu itself i also have the lastest bios as well,pcmasterrace,2026-01-23 18:15:54,3
AMD,o199ody,"Wow I recently had this happen and could not find anything about it, fortunately microcenter replaced it and I got a new mobo (MSi) to be safe. Working fine sinceâ€¦",pcmasterrace,2026-01-23 15:54:43,5
AMD,o19d0ux,PBO voltage problems again maybe?,pcmasterrace,2026-01-23 16:09:41,2
AMD,o1abllr,"Oh great,  and of course I have an 870E-E. I was feeling so good about my build too.   At least the mobo and CPU are under a protection plan.",pcmasterrace,2026-01-23 18:46:07,2
AMD,o1aytw8,"9800X3D, ProArt, on a B850-I. Everything has worked great so far but Iâ€™m so tired of this kind of thing. I really think Iâ€™m just going full Gigabyte.",pcmasterrace,2026-01-23 20:34:28,2
AMD,o1bagt7,"Wait, so my 650 is safe?",pcmasterrace,2026-01-23 21:29:16,2
AMD,o19dr00,Currently running X870E Apex w ryzen 7 9800x3d so update bios or no update biosðŸ« ,pcmasterrace,2026-01-23 16:12:57,4
AMD,o1ag7a5,This is why I stopped buying Asus shit.  They make quality components but in the rare instance you do have issues their customer service is between abyssmal and non-existent.,pcmasterrace,2026-01-23 19:06:52,1
AMD,o1b9oti,"I hope AMD doesnâ€™t pull the â€œwe are shifting more wafers towards our AI clients due to issues with X3D productionâ€, or some nonsense.",pcmasterrace,2026-01-23 21:25:40,1
AMD,o1bgwes,So glad I bought my MSI X870E Carbon!,pcmasterrace,2026-01-23 21:59:06,1
AMD,o1brhbx,Is this in response to the recent posts about 3 failures over the space of a whole year? Or are there new failures they are responding to?,pcmasterrace,2026-01-23 22:51:31,1
AMD,o1bz5xi,Well fuck. I just spent $2k on a build including a 9800X3D and an Asus X870E board.,pcmasterrace,2026-01-23 23:32:06,1
AMD,o1cuvfu,"After all of the constant failure with ALL of the 9000 series CPUs happening now, the intel 14k series is looking pretty good now.  At least their bios update fixed the issue.",pcmasterrace,2026-01-24 02:30:20,1
AMD,o1e2dxg,Soo a x870 mobo safe with a 7800x3d?,pcmasterrace,2026-01-24 07:39:06,1
AMD,o1ftfs7,Uh oh. Iâ€™m in danger,pcmasterrace,2026-01-24 15:29:08,1
AMD,o1hnh3i,Back in May/June of last year I upgraded to the Asus ROG STRIX B850-F w/ the 9800x3D and genuinely have never had an issue to this day. Should I be worried?,pcmasterrace,2026-01-24 20:23:08,1
AMD,o1ke79s,"Iâ€™ve had an Asus TUF X870 Plus Wifi paired with a 9800x3D for almost a year now. No issues thus far, but this def makes me nervous.   Iâ€™m still running a bios update from last year, unclear if I should upgrade or notâ€¦",pcmasterrace,2026-01-25 05:09:25,1
AMD,o1ai3tx,Man iâ€™m sleeping so good with my 7600x (no x3d grenade) and 9070xt (no fire connectors) build lol,pcmasterrace,2026-01-23 19:15:42,1
AMD,o19xnvn,"After this recent new build i did, it's hard to have good feelings about ASUS anymore. So many issues with their products.",pcmasterrace,2026-01-23 17:43:51,0
AMD,o1atds7,grateful for my gigabyte aorus,pcmasterrace,2026-01-23 20:08:40,0
AMD,o1a7cm0,Iâ€™ve had two Asus motherboards melt my cat 5 cables in the last few months. Their NICs are a short circuiting mess. Done with them.,pcmasterrace,2026-01-23 18:27:10,-3
AMD,o18zaen,"""It's not us but flash your bios""",pcmasterrace,2026-01-23 15:06:32,65
AMD,o19k9ta,"AMD blamed and ghosted asrock, similar will happen to asus as well.",pcmasterrace,2026-01-23 16:42:09,16
AMD,o19wk4w,So my warranty won't be voided if I flash bios right Asus?,pcmasterrace,2026-01-23 17:38:49,0
AMD,o19k1fv,Atleast asrock gave updates while AMD blamed the manufacturers then went radio silent.,pcmasterrace,2026-01-23 16:41:06,52
AMD,o1ari9v,I did this after seeing all the 9800x3d failure reports popping up on reddit,pcmasterrace,2026-01-23 19:59:45,2
AMD,o1cg80q,"This is what I ended up doing. I have an ASRock B650M board, and while the overwhelming majority of failures appear to be on B850 and X670 boards (at least from what I could gather), why take a chance?",pcmasterrace,2026-01-24 01:04:51,2
AMD,o1dd2yl,I got a 7950X3D for $330 USD and have been super happy.,pcmasterrace,2026-01-24 04:22:10,1
AMD,o1aved5,*nervously glances at a 9800X3D in an Asus 800-series motherboard*,pcmasterrace,2026-01-23 20:18:13,6
AMD,o1djl2v,Think I'm good,pcmasterrace,2026-01-24 05:06:25,1
AMD,o1c9un3,Yappy Jesus,pcmasterrace,2026-01-24 00:29:50,2
AMD,o1bs6i5,I have the same board and cpu. Did you have PBO enabled in AI tweaker or the advanced settings menu?,pcmasterrace,2026-01-23 22:55:08,1
AMD,o1bsoor,Asus has PBO in their AI tweaker bios page and then what some say is the real AMD PBO in a different section so maybe Asus PBO setting is supplying too much power?,pcmasterrace,2026-01-23 22:57:44,1
AMD,o19mbe7,"I wonder if I should do the same, but so far it has been working fine so I am afraid to update and cause problems that I currently donâ€™t have. ðŸ¥²",pcmasterrace,2026-01-23 16:51:16,3
AMD,o1cl4zj,"ya ur safe, the 7800x3d dont seem to be having issues tho, u got a good lil cpu, I bet good ram and ram tuning will help it. My pc is basicly a gernade",pcmasterrace,2026-01-24 01:33:28,1
AMD,o19zyrw,Considering other manufacturers have run into the same issues I feel like this one is on AMD.,pcmasterrace,2026-01-23 17:54:16,11
AMD,o1aes1o,What? I've never heard anyone being rejected a warranty claim because they flashed their bios. ASUS motherboards even have buttons on the rear I/O for flashing a bios.,pcmasterrace,2026-01-23 19:00:20,9
AMD,o1aross,Asrock is a sub brand of ASUS iirc.,pcmasterrace,2026-01-23 20:00:36,-5
AMD,o1ck4ig,"I have a asus b650 pro wifi, theirs been soom failers but not as much as the 800 boards, I do have a newer bios..... But I do exspect my 9800x3d to fry as least i have like 2 years and 10 more months of warranty.....",pcmasterrace,2026-01-24 01:27:30,1
AMD,o1b2dfe,Ya now im stressed about my tuf gaming x870e that I literally just built. I should have bought an MSI board :(,pcmasterrace,2026-01-23 20:51:13,4
AMD,o1bzu4f,All my stuff is just set to auto which i think it just defaulted to,pcmasterrace,2026-01-23 23:35:41,1
AMD,o1ckjhm,"I have b650 pro wifi, i am using PBO im pretty sure its AMD on gear 1 with newer bios, theirs some out their setting maxed out gear 10 when thats not needed at all and early bioses where giving too much soc power.....",pcmasterrace,2026-01-24 01:29:56,1
AMD,o1cpsyn,No patience for that. All I have done is -60mv and -20% undervolt on my 9070xt. I donâ€™t push my CPU enough in gaming for any work on that sode to be worth it.,pcmasterrace,2026-01-24 02:00:59,1
AMD,o1b42b3,No one will blame AMD,pcmasterrace,2026-01-23 20:59:10,2
AMD,o1d5728,"During the issues with Intel 14th Gen, ASUS released hotfixes BIOS updates as beta releases. However, this also required agreeing to terms stating the warranty was void if you accepted the â€œbetaâ€ update. Failing to install the hotfix would lead to eventual CPU failure.  It got some media attention (namely, Gamers Nexus), and ASUS claimed this was not an attempt to prevent warranty claims. They claim it was released as BETA to deliver the update as quick as possible; likely avoiding a number of required and lengthy QA steps.",pcmasterrace,2026-01-24 03:31:52,2
AMD,o1auba8,was\*,pcmasterrace,2026-01-23 20:13:05,19
AMD,o1awyox,"Asrock was initially a budget brand created by Asus because they didn't want their main brand on low quality components, which may hurt the Asus brand.  But Asus split the company into 3 parts around 15 years. Asrock was split from Asus as as part of Pegatron, their OEM manufacturing operations. Asus is a still a large shareholder of Pegatron but they have no direct control over Asrock for a long time now.",pcmasterrace,2026-01-23 20:25:36,5
AMD,o1bjxrt,Eh its been a year and not even gn can find why. At this point these people that are reporting failures aren't being honest. I've had my 870x and 9800x3d and it's fine. We really don't know what the affected numbers are. It could be under 1% which is the standard for tech.,pcmasterrace,2026-01-23 22:13:41,5
AMD,o1cq9ja,"If gaming on couch with controller no need, I play marvel rivals with mouse so any and all performance I can get. i use.",pcmasterrace,2026-01-24 02:03:41,1
AMD,o1avlul,TIL. When did that happen?,pcmasterrace,2026-01-23 20:19:12,5
AMD,o1iyvwe,"Dismissing the 1% and then pulling a ""I've had my 870x and 9800x3d and it's fine"" is like looking at the mirror and saying they're wrong",pcmasterrace,2026-01-25 00:16:43,1
AMD,o1cqlro,"Exactly what I do. Right now, playing Expedition 33 and CPU is at about 30-35% utilization.",pcmasterrace,2026-01-24 02:05:39,1
AMD,o1izypa,"Not really, we weight the vocal minority to much. I'm saying the problem might not be as big as we make it out to be. Computers still have a lot of user errors. We just assume people did everything correctly. If GN can't replicate it a year later. Who replicated the 12v pin on a 4090 catching fire. Maybe these people arent being honest on how they built their pc. I had a 11700k memory controller fry do I blame the manufacturer or the person that built it at abs.   That is why warranties need to always be avaliable and fast exchange for the 1% or lowe that something dies.",pcmasterrace,2026-01-25 00:22:18,1
AMD,o1cs2n4,HAH! i played that on my living room rig too. Im a long time pc gamer have have always put my older gamer to use on the big screen :),pcmasterrace,2026-01-24 02:14:06,1
AMD,o1kge2h,"For all we know these chips could be extra sensitive to ESD damage or something like that. God knows, reddit has a disdain for ESD precautions ""nah bro, you don't need to worry about that anymore, I didn't bother with it and my PC runs just fine"". No one understands latent failure, no one puts two and two together when their component shits the bed several months after they handled it while shuffling their sock-clad feet around their carpeted floor.  The huge tech youtubers could do a better job of modelling ESD hygiene.",pcmasterrace,2026-01-25 05:23:58,1
AMD,nzou89w,Hell yea dude,pcmasterrace,2026-01-15 07:12:25,1
AMD,nzoywq3,In this market? Yeah,pcmasterrace,2026-01-15 07:54:57,1
AMD,o148bie,if a pc doesnt sound like this then i dont want it,pcmasterrace,2026-01-22 20:58:00,8
AMD,o14amzl,That's how you know it's *working!*,pcmasterrace,2026-01-22 21:09:00,4
AMD,o153j60,AMDAir Romeo six niner seven zero taxiing to runway three over.,pcmasterrace,2026-01-22 23:36:25,4
AMD,o14y8no,*Blast* from the past!,pcmasterrace,2026-01-22 23:09:02,3
AMD,o15y5gv,I had a Sapphire HD 6950 that was unlocked to a HD 6970. I miss getting free performance with a flip of a bios switch. Good times.  (Also it was no where near this loud lol),pcmasterrace,2026-01-23 02:23:13,2
AMD,o16dvwm,"Oh man, I used to have a 7970- that card got every friend into PC gaming, passed it around as a rite of passage almost.",pcmasterrace,2026-01-23 03:52:16,2
AMD,o14bq8f,I have a HD 5870 eyefinity 6 somewhere. Those things are really good at making noise.,pcmasterrace,2026-01-22 21:14:10,1
AMD,o15yadf,I had just cranked the fan up to max. Its way quieter normally,pcmasterrace,2026-01-23 02:23:58,1
AMD,o14btzp,Spins faster then a honda civic,pcmasterrace,2026-01-22 21:14:39,2
AMD,o0v66g5,"Lol you only needed FSR 4, but they don't give it to you.",pcmasterrace,2026-01-21 15:05:56,2144
AMD,o0vmril,"VSCode? Python?... these are development utilities you *could* also use for developing ""AI"". I guess its a way of telling the user ""Look, i know nvidia has cuda and all... but you can now also train shit with our gpus aswell!""  This is a nice convenience box.",pcmasterrace,2026-01-21 16:21:52,337
AMD,o0v7md8,"At lwast it's opt in, not opt out...",pcmasterrace,2026-01-21 15:12:51,796
AMD,o0vhs4f,"This post is kinda the result of AI being a buzzword that has lost all meaning.  Don't know all of that stuff, but the ones I do recognize are developer tools, to write your own neural network (or code in general). Notably not even necessarily generative AI, I used them for example to write a network that would remove noise from CT-scans as part of my thesis.  As for why they come with your GPU, that stuff often needs to run on a GPU and the longest time only NVIDIA had actual support with their CUDA-api. So this might be AMD informing people about them catching up; in a bit of a clumsy way to be fair.  But others have said, it does not require additional user interactions to not have it, so I would not be bothered by it personally.",pcmasterrace,2026-01-21 15:59:26,534
AMD,o0vdfly,This for using AI models on your local pc. It has not much to do with the big AI provider which are causing the higher hardware prices. With 20GB vram + 32GB ram you can use a 20B model or smaller models with higher context sizes with usable output.,pcmasterrace,2026-01-21 15:40:00,437
AMD,o0vhukl,Kudos to AMD for doing this properly instead of shoving AI down your throat:Â  - Self-hosted AI - Optional,pcmasterrace,2026-01-21 15:59:45,568
AMD,o0v9z3r,"All of those are tools to work with AI models. None of those are ""let me supervise all your data and force AI assistents into everything"". And none of those are mandatory or pre-selected.",pcmasterrace,2026-01-21 15:23:56,718
AMD,o0v9qp0,I mean yeah its useless for most people but just dont install it.,pcmasterrace,2026-01-21 15:22:49,114
AMD,o0vljxo,"not defending AMD here but come on bro, this shit is optional",pcmasterrace,2026-01-21 16:16:25,131
AMD,o0vmnaj,"I don't hate most AI, I hate AI companies. There's a very strong difference and if you can't recognize that then you're no better than the tech bro chuds.",pcmasterrace,2026-01-21 16:21:19,13
AMD,o0vihdz,Bro it's optional so what's the issue here?,pcmasterrace,2026-01-21 16:02:37,23
AMD,o0vorho,"None of this is some stupid AI bloatware. These are developer tool for AI creation and development obviously meant for people that work with AI about which you know dogshit. You have 0 idea about what are you even complaining about.  Its literally OPTIONAL and UNCHECKED by default but it has AI in it so... ""REEEEEE...AI...MUST... BE...BAD..."". I hope one day you can forgive AMD for daring to think you might want to use your PC in any meaningful way besides of games.  And for people cry about ram, rom and GPU prices... computer are tool meant for achieving meaningful, productive things. Games never were and never will be their main point. They will always be side product.   Current AI market is bloated by bullshit useless AI apps a tool. I know that it's frustrating and I feel it to. It's a bubble that needs to pop. However it doesn't mean all AI is bad. AI is powerful new kind of technology that will advance whatever we can do.",pcmasterrace,2026-01-21 16:30:54,88
AMD,o0zi8hf,"So amd offers dev tools, local AI, and adds ROCm 7.2 to the game driver.(opt in, not opt out) Reddit loses its mind..?  Amd does plenty to get annoyed at but this ain't it chief.",pcmasterrace,2026-01-22 03:50:04,11
AMD,o0vey20,"What glazing? It is optional, not even preselected, you have to select it if you want it, you are complaining about nothing. I hate bloatware but this is not even preselected, you have to opt-in for them",pcmasterrace,2026-01-21 15:46:50,50
AMD,o0v9fte,What the actual fuck is with these posts..,pcmasterrace,2026-01-21 15:21:25,214
AMD,o0vg6mk,"""PC gaming will be a thing of the past"", redditors are so dramatic lol",pcmasterrace,2026-01-21 15:52:20,45
AMD,o0v8c10,"Are you even aware that half those things are used by developers even when unrelated to AI ?  **Edit**: I'm sorry but OP is functionally illiterate:  >""ETA: the glazing is so funny. Apparently you can put anything in anything as long as it's optional. Including a 34 GB AI bundle that I didn't ask for.""  If its optional, its not included and they aren't giving you anything you didn't ask for. And yes, they could even put VLC there as long as its optional, its literally a installer that will download the things you actually want like Nanite would.   As many people explained all over the thread, these are tools that people want. You may not want them, and that's why its optional. This panic at everything that mentions AI is at the level of the techbros that think AI is literally Christ reincarnated.",pcmasterrace,2026-01-21 15:16:15,252
AMD,o0v9724,"I strongly disagree it's good that they are opt in AND they are usefull for some people, why they shouldn't give them as an OPTION to the user?      This sub has trully fallen.",pcmasterrace,2026-01-21 15:20:17,76
AMD,o0vexvc,"It looks like opt in (even in express install mode), so for me it's OK, I guess",pcmasterrace,2026-01-21 15:46:48,8
AMD,o0vixcj,shoot i'd be happy to download pytorch and pycharm and start messing around,pcmasterrace,2026-01-21 16:04:38,6
AMD,o0vetxo,![gif](giphy|ukGm72ZLZvYfS)  So they make it easier for some people to download everything all at once??? OMG what has this world come to?????? This is socialism and WE NEED TO PUT AN END TO IT!!!!  /s,pcmasterrace,2026-01-21 15:46:20,15
AMD,o0v84se,Perfection. You are given all the libraries you need if you need to do AI development  I would still prefer manual setups as I am not sure what version they give.  You just don't accept to install them if you don't need them.,pcmasterrace,2026-01-21 15:15:17,55
AMD,o0v6d4w,"Wow, if only it was... OPTIONAL.",pcmasterrace,2026-01-21 15:06:51,137
AMD,o0v8xcl,Its bad because *checks notes... Options,pcmasterrace,2026-01-21 15:19:02,58
AMD,o0vbuw8,Would be nice if they let us pick what to install.   I don't need Ollama but I can use PyTorch for example,pcmasterrace,2026-01-21 15:32:40,4
AMD,o0vcxf2,Meanwhile AMD stock through the roof,pcmasterrace,2026-01-21 15:37:40,6
AMD,o0vb6q0,"Ah yes AMD has fallen because they give me the possibility to instantly install something like VSCode and Python.  GG for having the IQ of an Freezer. Besides that all of this is for local running LM on your own card.  I run them also locally and if I wanted a fast setup I would use it that way if they are up to date.   To give options nowadays is a bad thing, what a waste of a post.",pcmasterrace,2026-01-21 15:29:33,56
AMD,o0v8lqq,Its opt in if you click next you don't dl anything. This only does anything if you click the checkmark.  Your complaining about them giving an option.,pcmasterrace,2026-01-21 15:17:31,117
AMD,o0vac7e,Does this mean you can create your own local LLM and not have it stored to their cloud. That's would be a win,pcmasterrace,2026-01-21 15:25:37,20
AMD,o0xczmz,"you shold be mad at trump, zuckie, bezos and ABC for high prices, they bought the hardware for the next 3 years. they decided you are a less important customer than them. Nvidia and AMD are just the providers, it's shortsighted to be mad at them",pcmasterrace,2026-01-21 20:59:59,4
AMD,o0xdipg,"Are such features actually created because people at work have nothing better to do, or is there really a reason behind it?",pcmasterrace,2026-01-21 21:02:25,4
AMD,o0xztmy,"This is actually S-tier self hosted software that can be difficult to install. AMD stepping up tbh, all optional too",pcmasterrace,2026-01-21 22:47:21,4
AMD,o0ych7c,Wait... ComfyUI works natively on Radeon on windows now?,pcmasterrace,2026-01-21 23:54:26,4
AMD,o0z0f5b,"Not glazing, but that it's actually cool.  For the longest time, trying to do anything related to AI with AMD GPUs was a pain in the ass.  If anything, using any of that locally, is a way to make every other large corporation's waste money by not using their crap AI that 100% harvest your data, and phones back home.  You should give ComfyUI a try though, it's quite cool.",pcmasterrace,2026-01-22 02:07:15,4
AMD,o0z66dp,"It's open source LOCAL AI, literally opposite of AI data centers that are hoarding memory, more people use it locally = less demand for centralised AI tools = less profit for big AI = faster the bubble bursts. Of course, bubble bursting won't get rid of AI, it's still gonna be around, like the internet after dotcom bubble, but it will stop the price hikes.",pcmasterrace,2026-01-22 02:39:42,5
AMD,o0v6b6c,"You mean offering the option to automatically install useful(for some) libraries and programs is somehow the mark of AMD 'falling'? If anything, is that it's bundled without options to choose parts of it",pcmasterrace,2026-01-21 15:06:35,57
AMD,o0v93pv,"i actually like that, is an optional ready package to install and get you running models locally. Getting all those open source libraries correctly installed for your gpu (happens for both nvidia and amd) is a pain in the ass.",pcmasterrace,2026-01-21 15:19:52,21
AMD,o0vao7x,"Itâ€™s optional and some of these are useful tools to have on your computer, like VSCode or Python.",pcmasterrace,2026-01-21 15:27:10,19
AMD,o0ve2p7,Holy shit the majority of you just completely missing the pointâ€¦really shows the current environment this sub is in. Absolute brain-rotted morons.,pcmasterrace,2026-01-21 15:42:55,9
AMD,o0verhy,"Oh no. The horror of having the option to install or not install some stuff which you have to choose to be installed on your PC. No agency.   I'm really sorry your eyes were traumatized by the, trigger warning, AI word.   Consider suing for 2 billion dollars for emotional damage.  Stay strong and stay, trigger warning, AI free.",pcmasterrace,2026-01-21 15:46:02,9
AMD,o0vd7zn,">ETA: the glazing is so funny. Apparently you can put anything in anything as long as it's optional. Including a 34 GB AI bundle that I didn't ask for.  As you know you had not download 32gb of driver's package, what's the actual sense of your statement?  If you don't need, just don't select...",pcmasterrace,2026-01-21 15:39:02,11
AMD,o0vlq5u,"""Including a 34 GB AI bundle that I didn't ask for.""  Ok then don't use it? When did AMD need to start asking you for permission to add optional things?",pcmasterrace,2026-01-21 16:17:11,3
AMD,o0wpvzq,"I think you're  looking at this the wrong  way. These are just developer tools that allows you to explore machine learning locally  using the power of your GPU. Weather you  want them or not it's completely up to you. I recognize and even use a few of them   VsCode - a very popular code editor used by a lot of developers   Python - just the programming language   ComfyUI - it's a  popular  open source  workflow tool  tailored mainly at running  image gen models locally   Ollama   - for local LLMs   PyTorch -  framework for machine  learning applications. It's used quite popular  I think AMD is doing a decent job here, just showing you what you can do with your shiny GPU.  I love  they are not shoving anything  down our throats for the  sakes of AI . I guess good guy AMD ?",pcmasterrace,2026-01-21 19:15:11,3
AMD,o0yl6te,Hm did people also complain about optional software you most of the time don't need 20 years ago too? That's the 2nd post today about the same thing and I wonder what's so outrageous. The AI? The Bloatware? That you have to be aware of what you install? I understand the concern but not the method of dealing with that. If it wouldn't be optional software this would be another thing.,pcmasterrace,2026-01-22 00:41:19,3
AMD,o10fkkt,"Bro... That AI bundle and the AI causing the price meltdowns are two different things. I guess I'm just spraying pee on a burning forest fire, but yeah, relax.",pcmasterrace,2026-01-22 08:04:15,3
AMD,o10kkx4,"What is your issue? You do know that graphics cards are used also for.. work and productivity? Just because you are unable to use it for anything else than playing games, does not mean it is the same for others.",pcmasterrace,2026-01-22 08:51:01,3
AMD,o10ksnq,"What are you on about? If you install those things, you can train and run AI on your computer, offline. Like image generation, LLMs, and so on. Donâ€™t do it if you donâ€™t want it. But it is kinda cool that it is an option!",pcmasterrace,2026-01-22 08:53:01,3
AMD,o10xl6f,"considering ts they're showcasing here is free - this is not a forced ad or anything, so that's good. Then - what's the issue? They probably added this because of how many people do AI stuff with GPUs - y'know, there's enough demand so they cater to it. It isn't being forced - it's not even selected by default. It doesn't even make the installer UI much worse or anything. So, again - what's the issue with that?  If your issue with this is purely by principle, then please remember the fact that whoever this ""AI Bundle"" is meant for - is absolutely anyone but those who are actually responsible for hardware downfall.",pcmasterrace,2026-01-22 10:51:18,3
AMD,o10zgck,"I think you are kind of stupid for this post.  First, it is optional.   Second, it is for hosting and working with local AI. You can download this and work completely offline with open source models. So even if you download this, nothing happens, except for you now have some programs installed and you can open python code.",pcmasterrace,2026-01-22 11:07:22,3
AMD,o15nb2i,"Dude. The programs offered are a nice suite of tools for you to start using your new in-hand silicon to write on-device AI solutions for your own use cases. They literally just want to give you a nice headstart into AI development - something which could be daunting to aspiring developers: Which coding environments do I need? Which IDEs? Do I need to pay for jetbrains rider? Skip all of that and get into it with this >>optional<< bundle.  This has nothing to do with the rising NAND prices caused by the AI bubble. It's almost entirely the opposite-> YOU have chip and ram available, so if you are interested, now you can get in on it.",pcmasterrace,2026-01-23 01:22:16,3
AMD,o0va4yx,"What's wrong with this exactly? Its unticked by default... This is just letting you install all that software in one go, which is a great idea IMO...",pcmasterrace,2026-01-21 15:24:41,13
AMD,o0v6nwf,You literally stated itâ€™s optional. I dislike AI too but this ainâ€™t it,pcmasterrace,2026-01-21 15:08:16,31
AMD,o0vberc,"I understand the hate towards AI, but this fake outrage is hilarious.",pcmasterrace,2026-01-21 15:30:35,22
AMD,o0vazv4,Imagine some people use GPU not for gaming ðŸ˜‰,pcmasterrace,2026-01-21 15:28:40,11
AMD,o0vdfje,"oh no, the optional AI download is optional. whatever will you do",pcmasterrace,2026-01-21 15:40:00,13
AMD,o0v9i62,"Pretty sure if it wasn't called ""AI Bundle"" no one would bat an eye.  Most of those are useful utilities that have nothing to do with AI.",pcmasterrace,2026-01-21 15:21:43,15
AMD,o0v6kz3,"Given what's in the package, I wouldn't even be mad at them.",pcmasterrace,2026-01-21 15:07:52,20
AMD,o0vbu8x,"It's opt in so I don't mind, it does make me worry they'll start making it opt out or adding stuff in we don't want without giving options",pcmasterrace,2026-01-21 15:32:35,6
AMD,o0v9h3x,What? Those are useful developer tools bundled in one place. I have atleast half of that on my computer by default.,pcmasterrace,2026-01-21 15:21:35,13
AMD,o0v6v9f,its optional bird brain,pcmasterrace,2026-01-21 15:09:15,23
AMD,o0vg3fq,"It's good that these options are coming out; GPUs are used in many areas other than playing video games, so stop complaining about everything.",pcmasterrace,2026-01-21 15:51:56,5
AMD,o0wbwlg,Who cares? You're not forced to download it.,pcmasterrace,2026-01-21 18:13:55,4
AMD,o0wh7wl,If it's completely optional and the box is unchecked by default what's the problem?  You don't have to install it.,pcmasterrace,2026-01-21 18:37:05,4
AMD,o0wqny7,"Yes you quite literally can put anything as long as it's optional. If you have to check a box to add it, then there's literally no problem. You are objectively complaining about nothing.",pcmasterrace,2026-01-21 19:18:40,5
AMD,o0xaa2o,"So instead of clicking on the ""?"" Icon to comprehend what that is, you make a ragebait karma ehoring thread because current trend is basically ""ai bad, bad slop""?   Got it.",pcmasterrace,2026-01-21 20:47:48,4
AMD,o0xc25o,"I'm with OP here, I don't know why this is in the consumer driver package even as an option, developer tools should be a separate package.",pcmasterrace,2026-01-21 20:55:48,3
AMD,o0xs5jq,"These are dev tools man. SDKs are big, just try installing Visual Studio (not VS Code) with all its extras.",pcmasterrace,2026-01-21 22:09:42,4
AMD,o0v9wrw,"there is people out there that uses the PC for real work, not play games. It's good to see AMD cares for them too.",pcmasterrace,2026-01-21 15:23:36,9
AMD,o0v8ee5,"well its optional, so eh",pcmasterrace,2026-01-21 15:16:33,8
AMD,o0vbr6w,"So, can it install ComfyUI without way too much dances with bubn, like standard method needs?  If so, old cracked Amuse may not be the only adequate option for local gen in future.  Update: seems to be a joke, in real installer the only odditional thing is still a chipset driver and privacy view. That's just sad.",pcmasterrace,2026-01-21 15:32:11,2
AMD,o0vcdnr,"At least it didnâ€™t force you to install them first.  Also some of those arenâ€™t even ai, theyâ€™re just useful tools to have. Although youâ€™d probably be better getting them from the source",pcmasterrace,2026-01-21 15:35:07,2
AMD,o0vfipw,Try ComfyUI. It great tool for local image generating. If it's install is optimised for AMD it could run smooth.,pcmasterrace,2026-01-21 15:49:22,2
AMD,o0vfj7n,At least most of these are coding tools. Not everybody needs to install still.,pcmasterrace,2026-01-21 15:49:25,2
AMD,o0vfyiz,"So.. if it is optional, why are you crying?  And not, it is not because Ai into everything, it is because AI is being controlled by corporations. There you have Comfy UI, the posibility of using Ai without depending of corporations.",pcmasterrace,2026-01-21 15:51:20,2
AMD,o0vi9tn,"That's the good AI that runs directly on your computer though. And they are actually legit tools that one would want, not necessarily ads that recommend you corpo stuff.",pcmasterrace,2026-01-21 16:01:39,2
AMD,o0vksg6,Why would they even suggest development bundle for everyone?,pcmasterrace,2026-01-21 16:12:59,2
AMD,o0vlxu1,"What's the issue? It's optional and local. I would never want it, but it seems fine unless you just hate all things AI.",pcmasterrace,2026-01-21 16:18:10,2
AMD,o0vlz3v,What are you even complaining about? Most of it is just standard software and env tools to work with your own LLM and ML applications. And it's optional.,pcmasterrace,2026-01-21 16:18:19,2
AMD,o0vmazj,"Their hardware is built to support these tools, they are simply giving developers an easy \*option\* to download the software tools that they bought supporting hardware for.  Like when I buy a home security network, they give me an option to download remote viewing and cloud storage software that is compatible with the hardware I just bought.  but yeah... it has ""AI"" in he title so you just took the rage-bait and triggered yourself. Good work.",pcmasterrace,2026-01-21 16:19:47,2
AMD,o0vv9x1,Further makes me happy that I've migrated my desktop machine off of Windows.,pcmasterrace,2026-01-21 17:00:03,2
AMD,o0vw9qt,"If everyone thought ""good guy"" AMD wasn't going to chase the AI crap like both of their competitors then you weren't paying attention.",pcmasterrace,2026-01-21 17:04:32,2
AMD,o0w3we6,I'm wondering if this is easily avoided in linux,pcmasterrace,2026-01-21 17:38:46,2
AMD,o0wia5a,"You know you are not forced to download it, right? Besides, those are tools for developers, not only AI. Grow up and stop complaining about fabricated issues.",pcmasterrace,2026-01-21 18:41:40,2
AMD,o0wpcie,OP out here complaining about AMD packaging optional developer tools that make it easier for people who want to run ROCm accelerated compute.   I hope this is viral advertising.,pcmasterrace,2026-01-21 19:12:45,2
AMD,o0wxsnq,If it was written Dev Bundle instead this post wouldn't even exist,pcmasterrace,2026-01-21 19:51:03,2
AMD,o0wz6g1,"Honestly, really good offerings, Pytorch, Ollama, VSC, LM Studio, nothing here screams snakeoil, nor adware as you may think, and honestly it is â€œoptionalâ€ just donâ€™t tick it and move on?",pcmasterrace,2026-01-21 19:57:18,2
AMD,o0x7wft,"No clue what are comfyui or amuse butâ€¦  LM Studio is a GUI front end for running local models like the one listed there Ollama. No clue what the parameter count they are using but I am assuming itâ€™s something like 13B.  VS Code, PyTorch, and Python are all for developing AI. Not just generative AI, but stuff like I have seen an old MC YouTuber Sethbling make an AI just to play Super Mario World. Not the greatest explanation, I know, but itâ€™s mostly development stuff.",pcmasterrace,2026-01-21 20:37:04,2
AMD,o0xsgct,"Llama.cpp is missing there :D  Faster development than Ollama, although not as easy to use. Good for a weekend self-knowledge project!",pcmasterrace,2026-01-21 22:11:08,2
AMD,o0ybcm6,"You realise it doesn't use the 34GB (or even download it) if you don't tick the box, right?",pcmasterrace,2026-01-21 23:48:26,2
AMD,o0ybnv4,"Afaik they don't actually include the AI bundle as in it will download that stuff only if you want it. I think the main reason for AMD doing this is simply to make their GPU more appealing to the AI crowd since right now when you want to do AI, people (including me) always say ""go with Nvidia"" which is honestly still true. I would be very happy if this push will make more AI devs not only make their AI solution work on AMD GPU but actually optimize it to run good on AMD.  I know if you don't care about AI then this will be annoying to you, but you can't deny that the demand for AI is there and AMD is trying hard to catch up to Nvidia.   I wish they would also pay attention to other productivity stuff tho, like using their GPU for rendering in Blender is still a lot slower than the spec suggested, thus there is a potential for more optimization.  Also I really want AMD to make the Vulkan version of FSR4 and just let the RDNA3 GPU run FSR4 upscaling even if it is just running the FP8 version in FP16 thus slow. I would rather see a slow FP8 version running on RDNA3 vs nothing.",pcmasterrace,2026-01-21 23:50:05,2
AMD,o0yivaa,"oh damn, they actually support local models now? I though CUDA was a hard requirement, that's awesome!",pcmasterrace,2026-01-22 00:28:47,2
AMD,o0yndir,"Vscode and python aren't even AI related at their core.   I'd tick that box myself tho, local LLMs are cool cuz they don't ruin the consumer market.   Fuck openAI and the others tho",pcmasterrace,2026-01-22 00:53:16,2
AMD,o0zeb21,"If you want to run ai locally this is hella helpful, but most people donâ€™t. I would have hidden this elsewhere in Radeon.",pcmasterrace,2026-01-22 03:26:12,2
AMD,o0zizkd,Eh. Fortunately that's all local AI development stuff. I'm sure a lot of work stations appreciate the simplicity of have a dev environment fully set up,pcmasterrace,2026-01-22 03:54:48,2
AMD,o103ion,"PyTorch is actually pretty useful in my research project, but ig not in normal day to day use",pcmasterrace,2026-01-22 06:19:48,2
AMD,o104vqa,"""AMD has trully fallen""  After they added an optional AI package into the installer. Don't you think that's a bit dramatic?",pcmasterrace,2026-01-22 06:31:00,2
AMD,o10adgp,"It is bloat bundled like this but they're directly appeasing to all the people allegedly buying 4090s and 5080s/90s for ""AI Stuff"". So many comments last year on this subreddit about people shouldn't buy AMD if they want to use any AI tools. I think this is why they're pushing it so hard.",pcmasterrace,2026-01-22 07:17:41,2
AMD,o10gd0l,"Honestly, I don't fully hate this. Yes, AI is annoying and god I am sick at it being shoved down our throats and driving up ram prices, but these are all mostly used for self-hosting LLM's. There is a lot that can actually be learned from doing that, and its much better then just installing like, grok ai on our system. Granted, they shouldn't really be doing this at all, especially since amd gpus are god awful at self hosting llms compared to nvidia, but whatever.",pcmasterrace,2026-01-22 08:11:27,2
AMD,o10ipwa,">the glazing is so funny. Apparently you can put anything in anything as long as it's optional. Including a 34 GB AI bundle that I didn't ask for  Uh. Yeaaaah. I absolutely fuckin' hate AI but I also accept other people don't hate it and if they wanna install it they can. I don't want it, I don't install it. AMD made the right call making it optional unlicke those bitch motherfuckers at Microsoft trying to force copilot on me every windows update.",pcmasterrace,2026-01-22 08:33:35,2
AMD,o111iq3,"Weird to complain about opt-in installs. There's a ton of ""optional"" software in many Linux dists as well, so stay away from those if that's an issue.",pcmasterrace,2026-01-22 11:24:33,2
AMD,o129ml5,"We will obviously hate this because it's under the AI banner thing, but honestly the worst part is that you have to install the entire thing or nothing.   This would actually be useful if you could select which ones you want. As a software engineer student I would like it if I could only select vs code and python.  A similar thing but for gaming would also be cool. Like if while installing the gpu drivers it gave you the option to install software like steam, discord, firefox/brave, obs, gog galaxy, and any other gamer software from a list letting you pick which ones you want.",pcmasterrace,2026-01-22 15:38:37,2
AMD,o12e23k,this is like actually quite useful,pcmasterrace,2026-01-22 15:58:41,2
AMD,o153t9b,"â€œthat I didnâ€™t ask forâ€ what a dumb way of thinking. Do you also shun every other creation thatâ€™s made without your say so? Who gives a shit if you didnâ€™t ask for it, the world doesnâ€™t revolve around you bro ðŸ˜­",pcmasterrace,2026-01-22 23:37:53,2
AMD,o16vx70,"Bruh that's self hosted Ai, which is safe, useful especially when bundled with vscode and other apps they have in that bundle, self hosted Ai has no impact on ram prices, the issue is big Ai companies like gpt, not dudes self hosting at home",pcmasterrace,2026-01-23 05:52:48,2
AMD,o0vbl5e,"""AI bad give me upvotes"" ahh post",pcmasterrace,2026-01-21 15:31:24,6
AMD,o0wsvt6,"Shit take, It is a dev Tool for specific users and OPT-IN for a reason",pcmasterrace,2026-01-21 19:28:39,4
AMD,o0xqpj4,"This is standard software you'd want for getting into AI. For less tech savvy people it can be really useful. I don't understand the hate. I'm not the fan of AI either, but this is not a proper reason to push back",pcmasterrace,2026-01-21 22:02:51,4
AMD,o0va5kk,AMD we just wanted the INT-8 files and bug fixes. You already made FSR 4 possible on the PS5 pro using RDNA 2 silicon,pcmasterrace,2026-01-21 15:24:45,2
AMD,o0vgja5,You do know GPU is not for just gaming right?,pcmasterrace,2026-01-21 15:53:54,3
AMD,o0vhsj5,"That stuff is for AI development haha, what makes them think regular consumers care about installing that?",pcmasterrace,2026-01-21 15:59:30,4
AMD,o0w8116,"All meanwhile the only reason FSR4 doesnâ€™t work natively is because they donâ€™t want to install .dll and set a flag in a driver that this gpu can use it, what a joke.",pcmasterrace,2026-01-21 17:56:50,2
AMD,o0w8smp,Going back to the days when McCaffee was packaged with every single fucking installer...,pcmasterrace,2026-01-21 18:00:09,4
AMD,o0x5b8v,Disgusting,pcmasterrace,2026-01-21 20:25:20,2
AMD,o0x98l1,"It's fine. They are just offering an easy way to install these tools. They are offline, so it's not like they are trying to scrape your data.  The fact that you put quotes around AI Bundle indicates you have no idea what the tools are for, and are karma farming based on the partly negative public response to the AI boom.  How has AMD ""fallen"" by this? This is them catching up to Nvidia.  When you fill up at the gas station, you must be irate to see that the store also offers snacks and wiper fluid.",pcmasterrace,2026-01-21 20:43:09,4
AMD,o0xbo8p,![gif](giphy|givYLFG9wAv4gathsF),pcmasterrace,2026-01-21 20:54:04,4
AMD,o0z3wbp,"Tell me you know nothing about AI without telling me you know nothing about AI.  For those interested, this is just a bunch of open source tools and libraries that ML researchers and Data Scientist can use to train models on their own. Presumably using GPU as compute.  Chill with the knee jerk reactions people.",pcmasterrace,2026-01-22 02:26:51,3
AMD,o0zsvsv,"Alright, time to switch to... Uh...  Nvidia wouldnt give you the option to install, theyd just install it...  Intel might offer thr genAI as a last ditch effort to gain customers...  Can't rely on Chinese branded GPUs to be any better...  Qualcomm doesnt make desktop GPUs...  Alright, I'm requesting a couple billion to invest in manufacturing a nanometer chip fab to make GPUs which dont shove gimmicks with drivers. And a few million for R&D, plus a few more million per month for operating costs...",pcmasterrace,2026-01-22 04:59:44,2
AMD,o0v7kmv,"I don't understand what the fuss is about.    It's optional, and it's (supposedly) directing you to download various AI stuff optimised for AMD GPU's, which has historically been a huge complaint that AMD's AI integration isn't as good as nVidia's.  There was a gap, and they're trying to fill it, AND IT'S OPTIONAL!  Of course, it'd great if there was FSR4 included, and people should be angry at them for not including that, but giving you an option is hardly a sign of the collapse of AMD.  I just don't ""get it"".",pcmasterrace,2026-01-21 15:12:37,9
AMD,o0vfbud,Imagine being mad at an OPTIONNAL package AMD doesn't force you to download. What a life ðŸ™„,pcmasterrace,2026-01-21 15:48:31,3
AMD,o0w4rto,"This is crazy, but it's even crazier to see this many AMD fans defending this and giving kudos to AMD. I don't think I've ever stanned for any company *that* hard.",pcmasterrace,2026-01-21 17:42:37,4
AMD,o0v8m34,I would be upset if AI decided to select the AI bundle installation feature.,pcmasterrace,2026-01-21 15:17:34,2
AMD,o0vbam3,Driver only minimal install,pcmasterrace,2026-01-21 15:30:03,2
AMD,o0vfvi5,Nice quality of life improvement.,pcmasterrace,2026-01-21 15:50:58,2
AMD,o0vhft3,People complain that Nvidia is so much better for AI work then they improve their cards for said AI workloads and people still complain.,pcmasterrace,2026-01-21 15:57:55,2
AMD,o0vmwbw,"I went to McDonald's the other day and they asked if wanted fries with my burger, I said no thanks because I wasn't interested in that product and moved on with my day.",pcmasterrace,2026-01-21 16:22:27,2
AMD,o0w0gcs,AMD cards are 40% slower than Nvidia when it comes to AI.,pcmasterrace,2026-01-21 17:23:24,2
AMD,o0wdghq,"If this were Nvidia, you would be on your knees sucking them off... Literally.  Guess you forgot Nvidia is auto enabling replacement of Audio Drivers, autochecks geforce experience install...  But hey, gobble harder.",pcmasterrace,2026-01-21 18:20:38,2
AMD,o0wuqb9,"What a non-issue. An optional suite of programs that you are, by no means, obligated to use.",pcmasterrace,2026-01-21 19:37:04,2
AMD,o0x2lgk,"That's so fucking weird cuz like, it's not even consumer stuff. This is all mostly dev shit and devs are not gonna download and AMD AI bundle lol they'll just download whatever they need.",pcmasterrace,2026-01-21 20:12:58,2
AMD,o0xwkrk,"This is for self-hosting, which has been around for a while already. The cause of the RAM and SSD price spike is because of companies and data centers buying up everything, not the average Joe trying to run an AI locally.",pcmasterrace,2026-01-21 22:31:07,2
AMD,o0vfaqs,"I mean, you have the choice not to install it if you don't find it useful, someone else might benefit from it.    So what exactly is the problem here?",pcmasterrace,2026-01-21 15:48:23,2
AMD,o0vgi9q,Bro its just an addition for developers. AI isn't a bad thing its just a fucking math function. People who want limitless scaling in a limited environment is the culprit you are looking for. This is probably something like CUDA which optimizes drivers for AI,pcmasterrace,2026-01-21 15:53:47,2
AMD,o0vh6hw,Oh this is what they meant by AI bundle? This is actually kinda neat. I thought they were gonna try to put a chatbot in the driver's lol,pcmasterrace,2026-01-21 15:56:45,1
AMD,o0vwlrk,"As someone who's used self hosted AI on my AMD system for years, this is actually pretty nice to see. Nothing, and I mean nothing is designed for AMD first. It's been so hard to get these programs running, and now I can get comfyui going with one click? And I don't even have to visit GitHub? Sign me up.  You don't have to like it, that's why it's opt in. And although it IS weird AMD is offering extra programs to install in their driver program, I think, given that it's all free, open source, offline, and how difficult it used to be to install, that it's justified.",pcmasterrace,2026-01-21 17:06:03,3
AMD,o0vyg1d,"I mean this is useless, if i want to run llm i know where ollama is why force it down on me",pcmasterrace,2026-01-21 17:14:24,2
AMD,o0vysdm,My 7800xt wont get FSR4 BECAUSE OF THIS btw,pcmasterrace,2026-01-21 17:15:57,2
AMD,o0vzgk8,">ETA: the glazing is so funny. Apparently you can put anything in anything as long as it's optional. Including a 34 GB AI bundle that I didn't ask for.  Yeah, you didn't ask for it, its optional, so what's the problem?  This is a set of AI development tools that are conveniently bundled and expected to work with this hardware. This is *not* a set of invasive, resource-consuming AI products that spy on your stuff or eat up 2/3rds of your RAM to maybe answer a question once a year.  Some of y'all need to learn nuance, and that ""AI"" has been heavily overused in the past 5 years to encompass a whole hell of a lot of disparate things.",pcmasterrace,2026-01-21 17:18:58,2
AMD,o0wuwew,>that I didn't ask for  Here's a thought...don't download it?,pcmasterrace,2026-01-21 19:37:52,2
AMD,o0xafhz,">You can't buy RAM or an SSD for years to come and gaming PCs will be a thing of the past becuse AI is being shoved into everything, but it's ok because there is a tick box next to this (for now).  You have no idea what you're talking about. These are all (to my knowledge) open source professional/developer tools to run your own local AIs. None of these are the ""AI being shoved into everything"" people complain about, and they certainly don't make your RAM more expensive.",pcmasterrace,2026-01-21 20:48:29,2
AMD,o0xs7po,"Did you read what the programs are? Itâ€™s VSCode and python, they are used to code AI as well as a bunch of other things. Holy fuck reading comprehension at an all time low",pcmasterrace,2026-01-21 22:10:00,2
AMD,o0vhla9,don't want to be the bad guy but I kinda like the new bundle.,pcmasterrace,2026-01-21 15:58:35,2
AMD,o0xez2g,So many users here keep recommending AMD cards as if it is less evil and now look at this driver crap,pcmasterrace,2026-01-21 21:09:04,1
AMD,o0vaf6r,"Ironically AI stuff doesn't run that well on AMD so it's a bit odd choice to advertise it, but it offers decent stuff here and it's not some AI slop like Comet Browser, AI Video Editor, Topaz or whatever.",pcmasterrace,2026-01-21 15:26:00,2
AMD,o0vb2a2,"well, majority of people with high end PCs are probably gonna be using those.",pcmasterrace,2026-01-21 15:28:59,3
AMD,o0vf8no,If nvidia tried something like this reddit would be up in arms.  But it's amd so who cares just untick the box uwu,pcmasterrace,2026-01-21 15:48:08,2
AMD,o0va6xw,I don't even install the adrenalin app since I always pick the naked(?) driver only,pcmasterrace,2026-01-21 15:24:56,1
AMD,o0vj0bv,"I'm happy that AMD is finally prioritising their ROCm software stack,. Currently locked into Nvidia cause I rely on CUDA and their cards are SO expensive.",pcmasterrace,2026-01-21 16:05:00,1
AMD,o0vj60a,These are all for working on and running local models I see nothing wrong with this. Its not like this is copilot or anything forced down your throat and collecting data and such.,pcmasterrace,2026-01-21 16:05:44,1
AMD,o0vkh87,"Et tu, Brute?",pcmasterrace,2026-01-21 16:11:35,1
AMD,o0vkwkt,People who claim LLMs are going to bring us AGI are charlatans aiming to steal all the wealth and resources in order to be better at spying on you.  Why else would they all be building doomsday bunkers?,pcmasterrace,2026-01-21 16:13:30,1
AMD,o0vl3nc,">You can't buy RAM or an SSD for years to come and gaming PCs will be a thing of the past becuse AI is being shoved into everything, but it's ok because there is a tick box next to this (for now).       Sadly, some people always want to take a side and defend it. It's difficult for some to deal with the fact that none of these corporations are your friends       And I can almost see why. There's nowhere left to go for PCs now. Except maybe ironically Intel. But Intel's graphics are far from amazing, and it's only a matter of time before they push this shit on you as well",pcmasterrace,2026-01-21 16:14:23,1
AMD,o0vlndq,Aww hell nah,pcmasterrace,2026-01-21 16:16:51,1
AMD,o0vm4qi,Never thought Allen Iverson is still so popular.,pcmasterrace,2026-01-21 16:19:01,1
AMD,o0vn6l2,"You can hate AI datacenters, rising PC costs, evil mega corporations (and politicians like Trump) using AI, all of that, and that's fine. But if you do, you also need to understand that *local* AI is a completely different thing. Local AI prioritises consumers first. It enables privacy and freedom with incredibly powerful tech that will otherwise be used against you by said evil mega corporations.  Local AI is good. Corporate AI is bad.",pcmasterrace,2026-01-21 16:23:46,1
AMD,o0vn9we,Its optional though? I mean even though you didn't ask for it there are alot of people who have asked for amd to better support AI since most AI is built around nvidia drivers. Idk why youre getting mad at them for adding something completely optional. Thats silly.,pcmasterrace,2026-01-21 16:24:10,1
AMD,o0vnlvn,"At least its optional... for now...  The funny thing is that Python is listed as AI stuff but is more about automation, the same with PyTorch, lol",pcmasterrace,2026-01-21 16:25:41,1
AMD,o0vq48g,Does pytorch work with AMD?,pcmasterrace,2026-01-21 16:37:00,1
AMD,o0vryzv,They could have just named it the Programming Bundle.,pcmasterrace,2026-01-21 16:45:13,1
AMD,o0vthuq,![gif](giphy|29nDtEH1ViY8FcPeaV),pcmasterrace,2026-01-21 16:52:05,1
AMD,o0vxsc8,https://i.redd.it/moj4csy9jqeg1.gif,pcmasterrace,2026-01-21 17:11:26,1
AMD,o0wdm7p,"I mean, it's optional. PyTorch, Python, and Ollama are solid starting points for local AI.",pcmasterrace,2026-01-21 18:21:19,1
AMD,o0wg9hp,"I like how Python is in there, not even TensorFlow or anything, just straight-up python",pcmasterrace,2026-01-21 18:32:54,1
AMD,o0wifzt,Are there any benefits to using Pinokio as a launcher/installer?,pcmasterrace,2026-01-21 18:42:23,1
AMD,o0wkvvx,"I'm curious, was it enabled by default and you unchecked the box?",pcmasterrace,2026-01-21 18:53:04,1
AMD,o0wlrgr,"As terrible as all of this, thanks for the heads up on new drivers",pcmasterrace,2026-01-21 18:56:54,1
AMD,o0wv9g9,I thought amd GPU were not well supported to local LLM.,pcmasterrace,2026-01-21 19:39:33,1
AMD,o0wzekn,I'm so happy I'm on Linux save from this.,pcmasterrace,2026-01-21 19:58:19,1
AMD,o0x5u9k,"I saw your edit and look I get it; the AI industry is destroying personal computing and a few rich billionaires have decided that AI is going to be the next technological goal of society even though most people hate it and do not agree. However, I do see this as a W in some ways. You mentioned the rising cost of PC parts and how it's related to AI. This is from huge tech companies purchasing all the NAND flash inventory. Companies like OpenAI farm huge amounts of data and use legal loopholes to use people's data for training. It is quite honestly a hellscape, but what they're doing here is showing you that your AMD GPU is actually a legitimate option for running this stuff locally and not necessarily having to rely on a service like ChatGPT. They could easily just pour all their inventory into SenseMI for data centers and leave the gaming industry behind. Not only are they choosing to not do that, but are showing you that your AMD GPU is an affordable option to be able to run some of this stuff locally without having to give money to OpenAI.",pcmasterrace,2026-01-21 20:27:44,1
AMD,o0x6g7m,"thats what ""optional"" means like gamers have the worst vocabulary in the world not know what words mean",pcmasterrace,2026-01-21 20:30:29,1
AMD,o0x75bt,"Given that size, I'm curious how many models (and which ones) they're also bundling.  Because those 7 apps do not take up 34.2GB themselves.  I would wager there's probably 4-8 different models included with that.",pcmasterrace,2026-01-21 20:33:40,1
AMD,o0y63yl,Is it safe to update to these new drivers. I had to go all the way back to the October ones due to constant timeouts in the recent ones,pcmasterrace,2026-01-21 23:20:01,1
AMD,o0y8d2l,"I have the same card, what's your fan speed OP? The fan even with 40% speed is loud af",pcmasterrace,2026-01-21 23:32:08,1
AMD,o0y90he,"Curiously, I switched from AMD to Nvidia precisely because of the difficulty in installing this junk... and now it's standard?!",pcmasterrace,2026-01-21 23:35:40,1
AMD,o0yldix,How does Nvidia do it with their CUDA lib?,pcmasterrace,2026-01-22 00:42:20,1
AMD,o0yswvz,"so what can you even do with those ""tools""",pcmasterrace,2026-01-22 01:24:32,1
AMD,o0z7x0l,"""Let's give people things they absolutely hate""",pcmasterrace,2026-01-22 02:49:28,1
AMD,o10haml,Why is it so big?,pcmasterrace,2026-01-22 08:20:07,1
AMD,o10hd3f,Hol up this is actually insanee,pcmasterrace,2026-01-22 08:20:45,1
AMD,o10jbhe,Those aren't 10 million fireflies,pcmasterrace,2026-01-22 08:39:15,1
AMD,o10td84,"Manufacturers shipping useful opt in tools with their drivers has been common for ages. Either editing software or virusscanner, raid drivers or all kinds of other stuff a manufacturer thing some of their consumers might be interested in. It doesn't matter use it or not who cares.   They'll never make it not opt in, since less than 5% of their user base will probably use these tools. And even if you do accidentally install them, they'll have no effect on your pc other than taking up storage space, unless you actually start using them.   How useful it is is debatable. I would assume people interested in ai development would have no issue finding and installation these programs. But it's there for whoever wants it.",pcmasterrace,2026-01-22 10:13:54,1
AMD,o1128o4,Why would you wanna install these things along with a gpu driver tho lol,pcmasterrace,2026-01-22 11:30:23,1
AMD,o119hpc,Were you forced to download any of those models?,pcmasterrace,2026-01-22 12:23:54,1
AMD,o119hvv,I had gigabyte control center download norton antivirus because when installing drivers i missed a little checkbox that said to install suggested software.,pcmasterrace,2026-01-22 12:23:56,1
AMD,o11l6w5,"OP thats a pretty dumb post. These are open source, local models you CAN run. Calm down. Nothing is being shoved anywhere, these are tools for people clearly more advanced than you to build, develop and deploy their own local models.",pcmasterrace,2026-01-22 13:35:25,1
AMD,o11ysyb,In my opinion AMD is kind of doing something against the RAM and SSD pricing situation because it makes companies like OpenAI (which is the main culprit) no money when you run your own tools locally and don't subscribe to their cloud services. But in reality this is probably just to show that you don't need CUDA.,pcmasterrace,2026-01-22 14:46:22,1
AMD,o1242of,Mesa drivers ftw,pcmasterrace,2026-01-22 15:12:01,1
AMD,o12cf62,The first L in LLM means Large.,pcmasterrace,2026-01-22 15:51:20,1
AMD,o12oifa,"the hell is wrong with amd i dont get it. cant even play hogwarts legacy on a 7700xt on LOW settings (recommended are ultra, but it crashes every 15mins then) without crashing every 40 mins or so...   forget that, even chrome starts glitching while im playing a game. half the page scrolls while the other half doesnt even realize its got to move. sometimes i just feel like agreeing witrh the nvidia fanboys atp",pcmasterrace,2026-01-22 16:45:23,1
AMD,o135z5z,So you get offered a bunch of free stuff and bitch about it?,pcmasterrace,2026-01-22 18:04:05,1
AMD,o143ubn,"I don't get the glazing. I wouldn't rely on a GPU driver installer to setup my dev environment correctly. Besides, aren't all these pretend developers on Linux because it's so much easier?  AMD still hasn't fixed the issue with their Adrenalin software constantly scanning mouse inputs, causing jitter with high poling rate mice.",pcmasterrace,2026-01-22 20:36:55,1
AMD,o145d8d,"So that is why on a ""Consumer"" Presentation, that AMD ony presented AI and even had someone from OpenAI on stage....",pcmasterrace,2026-01-22 20:44:09,1
AMD,o14etcd,"The AI plays the games for you, see. Why would anyone want to play games themselves?",pcmasterrace,2026-01-22 21:28:47,1
AMD,o14itew,Wonder if my 6900xt would have that crap if I was on windows.,pcmasterrace,2026-01-22 21:48:12,1
AMD,o15tdjy,I love how everyone thinks AMD is not shoving ABS just like Nvidia is they are both guilty on all spectrum you canâ€™t trust any giant company now or the government,pcmasterrace,2026-01-23 01:56:41,1
AMD,o17fmkr,This would actually be a good thing to lower ram prices if everyone used these tools. Would remove dependence from the data centers if everyone self hosted,pcmasterrace,2026-01-23 08:40:50,1
AMD,o17gcrx,Nothing worth playing requires fake frames.,pcmasterrace,2026-01-23 08:47:33,1
AMD,o17i6sh,You are such a dummy.,pcmasterrace,2026-01-23 09:04:32,1
AMD,o17ko5h,">the glazing is so funny  You don't know any of those tools, I guess.",pcmasterrace,2026-01-23 09:28:09,1
AMD,o1jncyu,"Considering those are free and allow you to run any open source model of any kind locally, I'd say it's pretty based that it's integrated as easy optionals to pick out of with including prerequisites.  Only if FSR 4 was included too...  Edit: was ignorant not to realise AMD had been behind on support for running AI local, so this is a big flex to Nvidia",pcmasterrace,2026-01-25 02:29:20,1
AMD,o0vdw3r,"It's optional, it's like complaining about dlcs ðŸ˜­ðŸ¥€",pcmasterrace,2026-01-21 15:42:05,1
AMD,o0ve878,"\>I didnt ask for this   \>It's optional     AI bros can fuck off, but you can quite literally choose not to install it.  I don't see the issue here",pcmasterrace,2026-01-21 15:43:37,0
AMD,o0vehq6,Its optional what are you complaining about?,pcmasterrace,2026-01-21 15:44:48,0
AMD,o0vi1hl,"This is literally optional, as you said. Whats the big deal? And honestly, those are good things to install if you're into it and need it, not EVERYTHING in the optional bundle is purely AI ya know. It's used for other things, especially modders.   This post is beyond dumb.",pcmasterrace,2026-01-21 16:00:36,1
AMD,o0vk555,"You are the one glazed apparently.  For people want to run some local LLM on their GPU, having the option right there is quite nice. I fail to see how AMD has fallen because it gives option that were clearly requested for them to even have put ressources into it. Not everyone uses their computer like you think they should.  The irony of you being pissed at an unchecked box by default and leaving the send system data checked is lost on you clearly.",pcmasterrace,2026-01-21 16:10:06,1
AMD,o0vkmlf,Scrolling down to find out how this is actually not bad at all and actually maybe even a good thing!,pcmasterrace,2026-01-21 16:12:14,1
AMD,o0voemx,"I donâ€™t see the issue here. This is an opt-in choice that provides commonly used dev tools in the AI space. Iâ€™ve personally used VSCode, Python, and PyTorch for coursework.",pcmasterrace,2026-01-21 16:29:19,2
AMD,o0vqngo,"What's your issue? It's optional. I'm sure someone will find it useful, and those that don't care can ignore it.",pcmasterrace,2026-01-21 16:39:22,2
AMD,o0vsotx,Dumb post is dumb,pcmasterrace,2026-01-21 16:48:28,1
AMD,o0vv6fe,"Yeah, I probably won't be able to sleep tonight because there's an optional option for something I don't want to install in an installer. What shall I do?  ![gif](giphy|XdDsb6psTES0eQo8Vg)",pcmasterrace,2026-01-21 16:59:37,1
AMD,o0vyhec,â€œTruly fallenâ€ lmfao sometimes all you gotta do is laugh at these ppl who really have no clue on what theyâ€™re crying about.,pcmasterrace,2026-01-21 17:14:34,1
AMD,o0vz54z,"Why? It's an optional feature. And it's to run AI models LOCALY on your OWN machine, not to use cloud data centers. I see nothing wrong with this.",pcmasterrace,2026-01-21 17:17:32,2
AMD,o0w00f3,"""you can put anything in as long as it's optional""   but they didn't put it in. It's optional.  And you didn't ask for it because you didn't check the box. And it didn't put it in.",pcmasterrace,2026-01-21 17:21:25,1
AMD,o0w7ltg,"Personally I love AMD for doing this because you get all of the tools working as opposed to before setting up PyTorch and ComfyUI sometimes you had some weird kirks in the whole setting up process (especially on Windows as opposed to Linux). Now you can just tick whatever AI development tool you want and have it ready straight away.  AMD has truly fallen for bringing development tools in their drivers as an optional install, please don't speak on the matter if you have no idea.",pcmasterrace,2026-01-21 17:54:59,2
AMD,o0w8ic7,garbage post,pcmasterrace,2026-01-21 17:58:56,1
AMD,o0xietn,what CEO keeps making these poor decisions needs a bonk,pcmasterrace,2026-01-21 21:24:46,2
AMD,o0y1qpv,"Its fun checking back into windows to see how bad things are getting. ""But, but its optional!"" Yeah, so was copilot, local accounts, not having ads in the OS you paid money for, subscription office services etc etc.  How hot does the stove have to get before you take your hand off, or are people just frogs, simmering in slop and subscriptions forever?",pcmasterrace,2026-01-21 22:57:06,1
AMD,o0vaesk,mmmh i think i need the ai bundle i wanna try some ai stuffs,pcmasterrace,2026-01-21 15:25:57,1
AMD,o0vbgfb,"Shouldâ€™ve done more research amd has been wanting to go into ai for a while now, but couldnâ€™t back then they tried but failed but since their cards are better they can try again. I have amd gpu and what their doing isnâ€™t has bad has Nvidia.",pcmasterrace,2026-01-21 15:30:48,1
AMD,o0vcmqk,"You realize this is an AI bundle for developers, right? They're not trying to push you some AI slop... Although it baffles me still why they would offer it at all, considering it has nothing to do with a GPU driver,  but it's really not a big deal.  Stop calling it ""glazing"" just because you don't understand what most of those apps do.",pcmasterrace,2026-01-21 15:36:19,1
AMD,o0vdw48,I am not concerned about the optional bundle. I am concerned though about the required size.,pcmasterrace,2026-01-21 15:42:05,1
AMD,o0vffr5,>Including a 34 GB AI bundle that I didn't ask for.  Was it included by default?,pcmasterrace,2026-01-21 15:48:59,1
AMD,o0vgmto,"Big please, read something about this before posting. This isn't your normal AI slop, but rather an add-on for AI models.  You won't use it, but many other people will. Also, it is literally just a checkbox that you could've easily ignored. Talking about how AMD has truly fallen is outrageous and wrong in every single way.",pcmasterrace,2026-01-21 15:54:21,1
AMD,o0vj8q4,"in my opinion this is the proper way to do it, those tools allow you to run LOCAL ai, which isn't data hungry, and uses open source projects.  not only that but it's Opt In rather than opt out.  to me this is basically the same as when Nvidia was putting GeForce experience in the installer, which as a reminder isn't mandatory, but was checked by default anyway",pcmasterrace,2026-01-21 16:06:04,1
AMD,o0vo3ej,Thatâ€™s actually a decent selection if you want to set up your own local LLMs.   34gb must be because it comes with some models included.,pcmasterrace,2026-01-21 16:27:54,1
AMD,o0vr2gg,Ppl who use AI crap can run it local its optional and ofc the better way. Just dont install it if u dont need it?,pcmasterrace,2026-01-21 16:41:12,1
AMD,o0vryfj,"1. It's optional 2. RAM/SSD prices are not driven up by local LLMs, but by data centers. Local LLMs are needed and it is the future of data analysis, but as long as the data centers are squeezing every little bit of ram chips, we will never have great local LLMs.",pcmasterrace,2026-01-21 16:45:08,1
AMD,o0vzei9,"What the heck? This is so good. If I had this a year ago, I'd have saved myself from so much mental torture.",pcmasterrace,2026-01-21 17:18:43,1
AMD,o0vzoj1,"Local Open source AI is fucking dope and this is AMD making it easier to deploy. No internet connection, no cloud subscription, total privacy.",pcmasterrace,2026-01-21 17:19:58,1
AMD,o0w0gs4,"Pretty good selection of local tools, nice!",pcmasterrace,2026-01-21 17:23:27,1
AMD,o0w1iqz,This is a great idea because I had to install everything from the cli in the early days and things broke all the damn time,pcmasterrace,2026-01-21 17:28:11,1
AMD,o0w2n23,I hope OP doesn't get upset when he discovers his PC comes with productivity tools like a word processor as well. Computers are for gaming only >:(,pcmasterrace,2026-01-21 17:33:08,1
AMD,o0w41hv,"Those are for AI/ML engineers, not gamers",pcmasterrace,2026-01-21 17:39:24,1
AMD,o0w47e9,"Why are you mad? This is for local LLM development, those software is what I manually installed for my workflow",pcmasterrace,2026-01-21 17:40:08,1
AMD,o0w61f1,"... so you sre upset that they are providing OPTIONAL tools in their software packages that help people who need them for their work.Â  They arent forcing it, but giving you Python and VSCode should you choose it.Â  Â This is a stupid post at best.",pcmasterrace,2026-01-21 17:48:11,1
AMD,o0w63xy,"So you donâ€™t understand it, but youâ€™ve learned to have hatred triggered when you see two letters. Fascinating. Eh. Maybe not fascinating. Weâ€™ve seen humans do this literally forever. They get you to hate the witch, then they call anything they donâ€™t like a witch and you jump as high as they tell you to.",pcmasterrace,2026-01-21 17:48:29,1
AMD,o0w6vnk,"I mean, if you are coding you actually want most of those anyway. That software is actually good.",pcmasterrace,2026-01-21 17:51:50,1
AMD,o0wnodo,"My dude, these are tools for running your own AI. If you currently use any AI that's not local and switched to these you're kicking Nvidia in the balls. Of course AMD wants to make it easy for you.",pcmasterrace,2026-01-21 19:05:18,1
AMD,o0xulch,"I can now easily run all kinds of ai models on my personal PC? Wow that sucks, so hard................ Intel has fallen..........   ragebait post",pcmasterrace,2026-01-21 22:21:24,1
AMD,o0xdr40,">""AMD"" has truly fallen  A bit extreme, no? If you want it you can opt into it. Some people might use this. Do you bitch about all the features you dont use? I dont use half of the options in Adrenalin",pcmasterrace,2026-01-21 21:03:29,0
AMD,o0vd45q,"Optiscaler and that one FSR4 file that leaked a few months ago, paired with a dx12 game.  boom we now all have FSR4. Took my Spiderman 2 from 75 to 140fps.",pcmasterrace,2026-01-21 15:38:33,485
AMD,o0veqvb,"Crazy that modders/hackers get FSR 4 to work on steam deck and older AMD cards with better results than FSR3, but AMD wonâ€™t let us use it natively due to greed.",pcmasterrace,2026-01-21 15:45:57,135
AMD,o0v9upz,"It works very well with int8 via optiscaler, and is a better AA than native TAA. Worth it, even if zero performance is gained, for the much clearer image quality on newer games.",pcmasterrace,2026-01-21 15:23:20,21
AMD,o0v8515,"I really don't get why people keep asking for FSR 4 on older cards when we already know the performance penalty makes it impractical. It's a similar issue to DLSS 4.5 on 20/30 series NVIDIA cards.  EDIT: Tested it, and I guess it wasn't as bad as I remembered, even on RDNA 2",pcmasterrace,2026-01-21 15:15:19,46
AMD,o0wh2wh,FSR 4.5 when? Can't wait to see news about an improved upscaler in 2 years time.,pcmasterrace,2026-01-21 18:36:28,2
AMD,o0vvru0,X gon' give it to ya.,pcmasterrace,2026-01-21 17:02:17,1
AMD,o0yltcg,A lot of ppl stick to intel for bad marketing decisions too you know,pcmasterrace,2026-01-22 00:44:44,1
AMD,o128206,Modded driver is the answer my friend [https://rdn-id.com/](https://rdn-id.com/),pcmasterrace,2026-01-22 15:31:09,1
AMD,o0wl0id,it is a bit strange how they advertise it but YES it is really nice that finally for workloads with compute in mind Radeon is starting to become an option,pcmasterrace,2026-01-21 18:53:38,52
AMD,o10rslh,Python is a runtime dependency for quite a few of the programs listed.,pcmasterrace,2026-01-22 09:59:25,2
AMD,o0zte5q,"Turns out, the genAI was really thr PyTorch we made along the way",pcmasterrace,2026-01-22 05:03:16,1
AMD,o0v9ffe,"Or worse - ""up yours, it's installing anyway"".",pcmasterrace,2026-01-21 15:21:22,338
AMD,o0vncon,And most of them are interesting open source software packages. It doesn't feel great but if it's opt-in it is possible they are really just trying to be helpful providing an easy way to install them all.  Edit: it does feel like the usual AMD marketing completely failing to read the room to bundle this in the GAMING drivers though..,pcmasterrace,2026-01-21 16:24:30,62
AMD,o0vfd88,"what's opt in today, will be opt out tomorrow, and opt to go fuck yourself the day after tomorrow",pcmasterrace,2026-01-21 15:48:41,11
AMD,o0zgw4f,For now at least,pcmasterrace,2026-01-22 03:41:47,1
AMD,o102rp4,It's how it starts. Eventually it'll embed itself without your input or consent.,pcmasterrace,2026-01-22 06:13:46,1
AMD,o0w7khf,2 are local AI art and video generation tool. 2 are local LLM tools. PyTorch and python are just dependencies of the 4 I just mentioned and I guess AMD makes it easier to install this way to make sure you install the AMD optimized version with ROCm support or whatever AMD uses on windows now.,pcmasterrace,2026-01-21 17:54:50,129
AMD,o0x7orp,"I agree this looks a lot like AMd trying to make the point that you can run all the major AI tools, although if it's anything like doing it on a Mac, some things are pretty hit and miss",pcmasterrace,2026-01-21 20:36:07,1
AMD,o0vvxqs,"Exactly, and it's pretty sweet open source stuff. Most people messing around with AI models on their home computer are using gaming graphics cards to do so, much better performance on a GPU vs CPU",pcmasterrace,2026-01-21 17:03:02,150
AMD,o0vu9qa,tbf it just makes the process of setting your own models 5 steps more easily which isnt bad at all.,pcmasterrace,2026-01-21 16:55:35,26
AMD,o0vy2gd,"That was my first thought too, but then... why would I want anything else than my GPU driver when downloading my GPU driver ? I hope it's not ticked by default.",pcmasterrace,2026-01-21 17:12:42,15
AMD,o0wu73w,"Yeah I straight up returned my 7900 XTX in favor of 4080S a couple years ago because it didn't work as well with Stable Diffusion and RVC as even my older 3070, despite far more VRAM. So there are definitely people who did indeed ask for this.",pcmasterrace,2026-01-21 19:34:37,3
AMD,o1a91at,Yeah but it has a bad buzzwords so it has to be bad. Totally forcing us to take up ass and ram space but it existing and it'll kill the PC duh.,pcmasterrace,2026-01-23 18:34:42,1
AMD,o0vwv5c,"AMD gpus sucks for AI workloads tho. Basicly only 7900xtx have above 16gb vram and that card have 0 tensor cores. Will it run local AI? Yes, but its like using a threadripper for gaming. Highly ineffective and slow.",pcmasterrace,2026-01-21 17:07:13,1
AMD,o0vqrul,"it's not even that alone, it's mostly a software development kit for anything that uses machine learning, amd had not great support for that in the past so its good they finally are catching up to nvidia, it's not just for hosting generative ai",pcmasterrace,2026-01-21 16:39:54,75
AMD,o0vlygh,"Self hosted is the way to go, let me play and have fun but don't shove my data into their servers",pcmasterrace,2026-01-21 16:18:14,82
AMD,o0vl9jy,"Yeah, they are getting a disproportionate amount of hate for this. I get hating big AI companies like OpenAI and xAI for polluting communities, consuming copious amounts of electricity, and collecting way more personal data than they have any right to, but none of this applies to running open source models locally on your computer.  Environmentally, it isn't any more harmful than playing a demanding game, and you also have the benefits of privacy, Nothing leaves your computer.",pcmasterrace,2026-01-21 16:15:07,77
AMD,o0ve465,"For people that use local AI, it also addresses one of the biggest complaints with Radeon cards. Before, you'd have to jump through so many hoops to get something like ComfyUI working (at least on Windows). You'd need to install dependencies, HIP SDKs, certain versions of Python, ZLUDA (an emulated version of CUDA) and do a bunch of command line shit just to get it working.  Because of this, people would overwhelmingly recommend buying Nvidia for AI work because it was plug and play. You downloaded it and it worked.  The point of this AI bundle is to finally bring that plug and play experience to Radeon owners. I don't know if I'll actually need any of this (I already have the portable install of ComfyUI and LM Studio running through Vulkan), but it does make it way easier for people who do want to run AI locally on their card to do so without any fuss.",pcmasterrace,2026-01-21 15:43:06,168
AMD,o0vlsu5,"OP is a moron who sees the world in black and white while knowing less than nothing about the topic, and it shows.",pcmasterrace,2026-01-21 16:17:32,114
AMD,o0vbo6r,"Yep, seems like a decent offer to help someone jumpstart their cards 12vhpwr demise. Â But yeah, those don't seem malicious at all, genuine good will there.",pcmasterrace,2026-01-21 15:31:47,123
AMD,o0ve24q,Add it to the pile of â€œIâ€™m angry because of AI even though I am not educated on the topic or tools.â€ Ignorance continues to run rampant,pcmasterrace,2026-01-21 15:42:51,86
AMD,o0vpg9q,And I bet you think IR sensors can't be used for imaging.,pcmasterrace,2026-01-21 16:34:00,3
AMD,o0viodg,I'm looking at you Copilot.   My MS bill said $99/yr and I was like you want an extra $30 for some shit I never asked for? Downgrade immediately. I can write a sentence.,pcmasterrace,2026-01-21 16:03:29,5
AMD,o0ydrui,For now,pcmasterrace,2026-01-22 00:01:21,1
AMD,o0w74st,"They put the tin foil hat on saying it's ""optional (for now)""  Yes, computer nerds, especially ones willing to go with AMD, are known to commonly invite installing bloatware...   they're just getting mad about things they don't understand",pcmasterrace,2026-01-21 17:52:56,9
AMD,o0xb180,"Don't bother, it's the same circlejerk that happens with issues related to gamedev, it's always the fault of the engineers not the managers or leaders who make the bad decisions.",pcmasterrace,2026-01-21 20:51:12,1
AMD,o0vqwvi,This guy must hate steam because it has call of duty as an optional download,pcmasterrace,2026-01-21 16:40:31,79
AMD,o0x9b3u,â€œamd has fallenâ€œ pretending that AMD wouldnâ€™t be behaving exactly the same way as nvidia if their places were swapped,pcmasterrace,2026-01-21 20:43:28,7
AMD,o10sghg,"idk what op is whining about, it's an optional thing. Besides, most of us have been trying to run all those programs on AMD gpus for years,",pcmasterrace,2026-01-22 10:05:36,2
AMD,o0ws8tl,What is there to defend? AMD is making it easier for you to install software in case you want to develop and run local AI models.,pcmasterrace,2026-01-21 19:25:49,1
AMD,o0ws648,"You see, AI bad, so everything that has AI written on it has to be bad.",pcmasterrace,2026-01-21 19:25:28,11
AMD,o133k7m,"No no, you forget, OP dislikes the thing being offered, so nobody should get to have it.",pcmasterrace,2026-01-22 17:53:26,1
AMD,o0vm5v1,Karma farming.  Even though they are incredibly dumb they are still getting upvotes.,pcmasterrace,2026-01-21 16:19:09,69
AMD,o0vdruo,He asked in the cesspit of hate,pcmasterrace,2026-01-21 15:41:33,16
AMD,o0vgfqb,"AI firms are throwing hundreds of billions of dollars around -- think it's maybe possible they're spending a bit on ""directing the online discussion"" regarding their tools?  Heh.",pcmasterrace,2026-01-21 15:53:28,-8
AMD,o0vwi6e,Ollama is a shitty llm inference engine which is just a wrapper for an actually good inference engine (llama.cpp),pcmasterrace,2026-01-21 17:05:35,9
AMD,o0w13q9,"Amuse is for image generation, ComfyUI too. LM Studio is a better Ollama in my opinion, Pytorch and Python are necessary for Comfyui i believe. I'm not sure if amuse uses ComfyUI, that's possible.",pcmasterrace,2026-01-21 17:26:18,1
AMD,o0v964y,"Some of those, like Python, are just nice to have even when you aren't a developer.",pcmasterrace,2026-01-21 15:20:10,46
AMD,o0vaqnh,Iâ€™m a developer. I donâ€™t really want to download my tooling from my graphics driver ui,pcmasterrace,2026-01-21 15:27:29,142
AMD,o0vhyk3,Half ? I can only see 2 out of the 7 that can be used by developers that don't use AI.,pcmasterrace,2026-01-21 16:00:15,2
AMD,o0vprqe,I frequently have to take a deep breath and remind myself that many of the users here are teenagers. It is fascinating how incendiary attitudes around a broad subject drive so much nonsensical finger pointing.,pcmasterrace,2026-01-21 16:35:26,13
AMD,o0zqshd,"They do, it's an opt-in, not opt-out.",pcmasterrace,2026-01-22 04:45:30,1
AMD,o10s0rc,"I haven't tried it, but I'd assume the check box next to AI Bundle would download all of them, whereas clicking on the individual products would allow for them to be installed from there?  I obviously could be wrong.",pcmasterrace,2026-01-22 10:01:33,1
AMD,o0vcj4g,"Donâ€™t insult freezers, there are smart ones nowadays.",pcmasterrace,2026-01-21 15:35:51,21
AMD,o0vdc6d,"Yup, personally would've switched VSCode for Spyder or pycharm, but that's personal preference",pcmasterrace,2026-01-21 15:39:33,7
AMD,o0vbpbh,"What does it have to do with a graphics driver though? Seems incredibly odd. Who is updating their graphics driver and is like ""oh yeah I forgot to download 7 pieces of software for development work""  I don't think its malicious but it is weird.",pcmasterrace,2026-01-21 15:31:56,0
AMD,o0vdguw,"Yeah, that's what they do.",pcmasterrace,2026-01-21 15:40:10,7
AMD,o0vczfv,"Yes.  Model size is limited, and token speed quite low, but yes, thatâ€™s what it is for.",pcmasterrace,2026-01-21 15:37:56,10
AMD,o0vck8r,"VSCodium would be better, same program minus the telemetry",pcmasterrace,2026-01-21 15:35:59,3
AMD,o0yy192,Virtue signal bs. And karma farming!,pcmasterrace,2026-01-22 01:53:45,2
AMD,o0vafzf,Ppl are gonna complain no matter what a company dos,pcmasterrace,2026-01-21 15:26:06,8
AMD,o0vq9gb,"What about when your not even using your computer? There is also a program called ""Folding at Home"" where the program uses your computer to do complex protein folding to solve medical conditions.",pcmasterrace,2026-01-21 16:37:39,1
AMD,o0veh8r,Complain that he can't complain that it is mandatory ðŸ™ƒ,pcmasterrace,2026-01-21 15:44:44,7
AMD,o0vbh9o,Look again.   Itâ€™s AI development SDK.,pcmasterrace,2026-01-21 15:30:54,4
AMD,o0ve3sp,"Besides python (which is probably just here for pytorch), all of them are either solely for AI or are inherently linked to it. Even VSCode pushes copilot really hard.",pcmasterrace,2026-01-21 15:43:03,2
AMD,o0vwkme,"You can also use the newest Amuse, just do this  User>AppData>Local>Amuse>Appsettings.json and then open via Visual Studio Editor.  Then look for the line:   IsModelEvaluationModeEnabled  It says false, change that to true. Voila. Amuse Filter is off",pcmasterrace,2026-01-21 17:05:54,1
AMD,o0yczvd,"Optiscaler let's you use fsr4 on 7000 series :) You just have to find and download the .dll for it, one that they accidentally leaked :D",pcmasterrace,2026-01-21 23:57:11,1
AMD,o0vxfc9,LMstudio and comfyui are for inference and can 100% run consumer gpu tier models,pcmasterrace,2026-01-21 17:09:46,4
AMD,o0vr94d,Driver only is dogshit and causes many issues and gives up features.  That said these are opt in you have to click custom then manually select them to get the tools they are not forced on.,pcmasterrace,2026-01-21 16:42:01,2
AMD,o0vryus,They do have amd chat as optional software as well.,pcmasterrace,2026-01-21 16:45:12,2
AMD,o12ghb4,"You must've overlooked the word ""optional"".  It isn't even checked by default.",pcmasterrace,2026-01-22 16:09:42,1
AMD,o0vlh8d,"I think it's to show they are somewhat confident in being able to run them with acceptable performance. They weren't able to run these tools at all, they are basically letting users know that this is feasible now.",pcmasterrace,2026-01-21 16:16:04,4
AMD,o12g690,"You don't even have to untick it, it's unticked by default.",pcmasterrace,2026-01-22 16:08:18,1
AMD,o0vxu0o,still local models are trained on copyrighted data. both are bad,pcmasterrace,2026-01-21 17:11:39,1
AMD,o0ydkfo,Its literally opt in.  Unlike Nvidia where you have to opt out on install stuff or they fuck your audio drivers.,pcmasterrace,2026-01-22 00:00:16,2
AMD,o0wxwmk,They are reasonably supported now. Less efficient but very usable for hobbyist.,pcmasterrace,2026-01-21 19:51:33,1
AMD,o110kiw,"Local private AI usage, Dev Tools for coding, Tools to develop your own AI model....  All of them open source as well lmao",pcmasterrace,2026-01-22 11:16:44,1
AMD,o1109c5,"Cuz it includes models in the download already.  32-34GB is the full package of the Dev tools.  And its opt in aka optional, so you only download it when you fuckin want it.... Dont see his issue xD",pcmasterrace,2026-01-22 11:14:11,1
AMD,o114wg2,"wait till you see how big diffusion models, checkpoints, vaes, text encoders, etc can get",pcmasterrace,2026-01-22 11:50:58,1
AMD,o138ow8,"May be worth investigating that a bit further. I was able to play the game with 0 issues on a 6700xt. It has been long enough now that I canâ€™t remember the settings I used, but I do know I played it through entirely without issue.",pcmasterrace,2026-01-22 18:16:09,1
AMD,o12fbxv,These are development tools. Are you afraid one day you'll be forced to learn coding in Python or what?,pcmasterrace,2026-01-22 16:04:28,1
AMD,o0v8tw2,More like you go to the pharmacy for cough medicine and the pharmacist asks you if you want those extra things and you say â€œnoâ€ and go about your day leaving with exactly what you came for.  Hell you donâ€™t even have to say â€œnoâ€ because the boxes are already unchecked.   They didnâ€™t even offer theyâ€™re just in the shelf behind them and you saw them and decided you were upset at the pharmacists for having things you donâ€™t want.,pcmasterrace,2026-01-21 15:18:35,12
AMD,o0vsp24,You have to click custom then manually select it to download it.  Op acts like its forced on but its opt in.,pcmasterrace,2026-01-21 16:48:30,3
AMD,o0vtpvr,Are you sure those frames are real frames ? - Morpheus,pcmasterrace,2026-01-21 16:53:08,187
AMD,o0wbzao,â€œOur FSR4â€ ðŸ«³ðŸ»,pcmasterrace,2026-01-21 18:14:14,16
AMD,o0wly5a,Itâ€™s a shame it gets flagged by anti-cheat.,pcmasterrace,2026-01-21 18:57:42,10
AMD,o0w2srb,HUH WUT,pcmasterrace,2026-01-21 17:33:52,4
AMD,o0x52tj,"Why do people like you keep bringing up external sources like it's such a huge win? It works for you, for games you like, okay, but it's still unplayable for most people in anything using anti-cheat for example. Why are we trying to shut down any requests for official support? Why do people need to take 20 extra steps to enable it in select games?",pcmasterrace,2026-01-21 20:24:18,3
AMD,o0xg169,That INT8 version is not as good as the proper FP8 one though.,pcmasterrace,2026-01-21 21:13:57,1
AMD,o154kiu,Can it do on a 5700 xt,pcmasterrace,2026-01-22 23:41:47,1
AMD,o0vzabb,Does it work on RDNA 2? I have a 6900 XT,pcmasterrace,2026-01-21 17:18:12,25
AMD,o0yi3e2,"I thought that it was actually missing the neural compute to run true FSR 4, or is that not the case?",pcmasterrace,2026-01-22 00:24:36,1
AMD,o0w1zsj,Motion clarity is still rough though. There's bad ghosting I've seen.,pcmasterrace,2026-01-21 17:30:15,5
AMD,o0v9q1a,"It works very well with int8 via optiscaler, and is a better AA than native TAA. Worth it, even if zero performance is gained, for the much clearer image quality on newer games.",pcmasterrace,2026-01-21 15:22:44,98
AMD,o0v8ktb,"people are simple, they want the choice to make that decision themselves.",pcmasterrace,2026-01-21 15:17:24,120
AMD,o0vau3u,"Not everything boils down to ""bigger number = better"".  I'd take a performance hit for better visuals in some cases. That's the great thing about PC gaming being the most customizable experience.",pcmasterrace,2026-01-21 15:27:55,32
AMD,o0vaogt,Fsr 4 performance mode beats fsr 3 quality in many title with foliage at 4k.,pcmasterrace,2026-01-21 15:27:12,23
AMD,o0v9yrx,"I've been using FSR4 on a 7900XT via Optiscaler, and it's not so bad. Idk how well DLSS4.5 runs so I can't speak to that, but the performance dip really isn't that bad and the quality jump is insane. ""Performance"" FSR4, even on INT8, looks better than ""Quality"" FSR3 or earlier.  Because of that, I've been able to get the same or better results on all but the most demanding games. There are exceptions; Borderlands 4 saw something like a 10-15 FPS drop. But that seemed to be a BL4 problem as other demanding games (even ones on UE5, such as Oblivion Remastered) ran fine.",pcmasterrace,2026-01-21 15:23:54,9
AMD,o0v9cqc,"Not true. 7900 XT and XTX would have no trouble using FSR 4, at least at 1440p",pcmasterrace,2026-01-21 15:21:01,13
AMD,o0v9mpl,all new apus come with rdna 3 and 3.5 ever the 395 max. 4060 performance.,pcmasterrace,2026-01-21 15:22:17,4
AMD,o0v8tpe,"No, I use mods that enable FSR 4 and it's perfectally usable in every game I got it working. The performance hit didn't bother me at all.",pcmasterrace,2026-01-21 15:18:33,22
AMD,o0vbo3b,"Have you even looked into it, performance is wsy better than dlss4.5 on 20 and 30 series, its similar or better than dlss4.0 on 20 and 30",pcmasterrace,2026-01-21 15:31:47,2
AMD,o0vemof,so it would gimp the 7900xtx?,pcmasterrace,2026-01-21 15:45:25,2
AMD,o0vi7mx,"Because this is simply not true  It's very much worth it, atleast in my experience, maybe on 6000 it's too much, but on 7000, or atleast a 7900xt it would definitely be a nice to have",pcmasterrace,2026-01-21 16:01:22,2
AMD,o0w23jt,"Thatâ€™s not really true. Itâ€™s gotten better and better with more community optimisations and presumably AMD could do a lot more.  Also much like DLSS 4.5 more options is never a bad thing. Iâ€™ve seen people with a 2080 use ultra performance mode combined with preset L to get surprisingly good image quality in path trace titles. Meanwhile, Iâ€™ve seen people with something like a 3090 TI just use preset M and brute force the image quality improvements as basically a native anti-aliasing alternative.   This is why Iâ€™m so against this AMD style of thinking where they know best or Nvidia for that matter with frame generation which now could very well run on the 20 and 30 series after they removed the need for flow rate acceleration in their new model but they havenâ€™t done that. Iâ€™m sure there are some CPU bound games that could deliver a good experience even on the 20 and 30 series cards but because Nvidia is greedy or Nvidia thinks they know best they havenâ€™t done that.",pcmasterrace,2026-01-21 17:30:44,2
AMD,o0w9h9q,"The penalty is when comparing to FSR3 performance, not native render. I get 42-45 fps in cyberpunk 2k/all max/rt max/pt off and just enabling quality fsr3 gives me +30ish fps. I wouldnâ€™t care if it gave me +20fps but actually good image quality instead",pcmasterrace,2026-01-21 18:03:10,2
AMD,o0vbvrg,"DLSS 4.5 works quite well on my 3060ti. I've been using it in Cyberpunk 2077 since they released it. Yes, it drops the performance down just a little bit, but it's not much.",pcmasterrace,2026-01-21 15:32:47,4
AMD,o0veksx,Impractical? Nonsense. Both INT8 and FP8 are very performant.,pcmasterrace,2026-01-21 15:45:11,1
AMD,o0vwamq,I have so much respect for you for the edit. Most wouldn't even check and just double down,pcmasterrace,2026-01-21 17:04:38,1
AMD,o11eqvk,Respect the edit,pcmasterrace,2026-01-22 12:58:11,1
AMD,o0vb4o8,"DLSS4 runs just fine on older and weak RTX cards also 4.5 can be acceptable in some titles, but apparently AMD can't keep up with DLSS 4.0 quality. In terms of FrameGen even Intel has way better one.",pcmasterrace,2026-01-21 15:29:18,1
AMD,o10spbg,"It's not just nice, it's amazing, people have been struggling to run all those models on AMD for years. If I had this I'd still have my 6900xt, instead of overpriced 3090",pcmasterrace,2026-01-22 10:07:50,3
AMD,o0vb3uy,"Ah yes, the Norton antivirus special",pcmasterrace,2026-01-21 15:29:11,167
AMD,o0vnnnb,The Microsoft way.,pcmasterrace,2026-01-21 16:25:55,13
AMD,o0wkzxn,"I actually like it. I use PyTorch for work.  Being able to use the game driver and have current version ROCm support is amazing to me. Before that, my option was a like, 3 months old experimental driver if I wanted ROCm 7.1. Which lacked many optimizations (such as, making framegen work without crashes in wilds).",pcmasterrace,2026-01-21 18:53:34,6
AMD,o0visiy,"bro its out of context but, 96 gb ddr5 god damn",pcmasterrace,2026-01-21 16:04:00,0
AMD,o0wjfn5,Honestly trying to setup cuda and tensorflow is such I nightmare Iâ€™d click that box immediately,pcmasterrace,2026-01-21 18:46:45,66
AMD,o0wknm1,"Calling pytorch just a dependency of modern AI applications is a bit wild. PyTorch is an open-source framework for anything AI-related. It's used in academia for so much.  And yes, the bundle there bundles the newest ROCm version (7.2) which is a large stepup from ROCm <7.",pcmasterrace,2026-01-21 18:52:04,17
AMD,o0xzwvn,"Dude literally called the single most important machine learning framework ""just dependencies""  (Yes I am using hyperbole)",pcmasterrace,2026-01-21 22:47:49,1
AMD,o0wpogf,"Yeah, this is what gets me about people thinking that AMD has some ulterior motive, or that this is the work of evil big AI, or that this will eventually be forced upon users.   Out of all the AI apps listed here (not the developer tools VSCode, Python and Pytorch), all of them are freeware. 3 out of 4 of them are open source. AMD does not develop any of these apps (Amuse was sponsored, but no one's going to use that with ComfyUI available). And with the exception of Amuse, all of these originally worked on Nvidia first.  AMD does not directly benefit from people having this software on their computer in any way. All of these are for running AI locally on your own hardware. You do not pay for any cloud compute or transfer any data to AMD. They make zero money by bundling these apps for you.  So why are they doing this? It's stupid, but the point of this is to let people know that these things now work hassle free on AMD cards. One of the many things they lagged behind Nvidia in other than upscaling, ray tracing and all that was local AI.   Getting something like ComfyUI working on Nvidia was as simple as downloading it and installing it. It just worked out of the box. By comparison, getting it working on AMD was a complete pain in the ass. [They even acknowledged as such](https://youtu.be/UZHffzyyatY) in their promotional video for this.  All this was aimed to do was to let people know this is no longer the case. Was it a hamfisted, clumsy way of doing it? Totally. But this is very different from something like Microslop trying to shove in Copilot into everything.  It's opt-in, for fucks sake. My drivers automatically updated because I forgot to turn off auto update and none of these things were installed on my computer. Hell, I wouldn't have known this ""AI Bundle"" thing even existed until I saw it on reddit.",pcmasterrace,2026-01-21 19:14:15,39
AMD,o0ww25g,"Oh yeah. Llms are easy going, especially with Lmstudio, but Stable Diffusion is my endboss. Never got it working right.",pcmasterrace,2026-01-21 19:43:13,3
AMD,o0vz8kx,"My gpu, a 7900 XT has 20 GB vram and I can do, what I wrote pretty well. The 7900 XTX has 24 GB vram and the 9070 XT has ""only"" 16 GB vram. Still good enough for most of the smaller models.   I have to check it again, but from memory I got around 7 to 12 tokens per second from coding models in Lmstudio.  Amd ist not as easy to setup as Nvidia, true, but then this is just an argument for the AI bundle, right?",pcmasterrace,2026-01-21 17:17:59,14
AMD,o0wv2mx,">Basicly only 7900xtx have above 16gb vram  I mean, realistically, what consumer GeForce cards have more than 16 GB of VRAM? It's only ever been the Titan/90 class. Sure, Nvidia does have a couple generations of it available, but these were (and are) prohibitively expensive, so it's not exactly a point against AMD. When it was still available, the 7900 XT was the cheapest card that could fit gemma3-27b-qat completely into its memory with a usable context window.  >Highly ineffective and slow.  It's really not that bad. I have a 7800 XT and with Z-Image-Turbo I can generate a 1536x2048 image in under a minute. Not as fast as an Nvidia card, sure, but still pretty damn fast for an image of that resolution. Flux.2 Klein-4b edits images in under 10 seconds for me. Would an Nvidia card make it even faster? Sure, but not faster enough to say that AMD sucks for these applications.",pcmasterrace,2026-01-21 19:38:41,5
AMD,o0yky0k,My 6800xt is extremely fast for my local hosted ai as long as I keep the model below 16Gb,pcmasterrace,2026-01-22 00:40:00,1
AMD,o0vww4l,Isn't self-hosted severely limited and kind of stuck as you will reasonably not even have half the amount of memory available compared to commercial models? Assuming that you have at best 32GB of VRAM compared to the Nvidia AI optimized cards with 80GB of VRAM. Or is there not that much different between the smaller and larger models?,pcmasterrace,2026-01-21 17:07:20,1
AMD,o0vkego,"Nah, ComfyUI already simplified the process themselves. You just select ROCm on their installer and they handle everything for you.",pcmasterrace,2026-01-21 16:11:15,11
AMD,o0w649e,I literally just bought a 5070ti because of all the BS I'd have to go through to get Comfy and a few others working on an AMD GPU. This would have been nice to have last week.,pcmasterrace,2026-01-21 17:48:31,2
AMD,o0wtlfy,"If i already have installed rocm with Zluda, do i need to install anything else to gain the performance they claim this has?",pcmasterrace,2026-01-21 19:31:52,1
AMD,o0vq0v5,So basically your average Redditor?,pcmasterrace,2026-01-21 16:36:35,51
AMD,o0w76k1,So many people don't even do a simple google search before commenting.,pcmasterrace,2026-01-21 17:53:09,5
AMD,o0vd2wh,You jest but having ROCm PyTorch finally available for Windows is big news for some,pcmasterrace,2026-01-21 15:38:23,86
AMD,o0vf2le,Actually yeah this update is solid for people new to local LLMs (I run ollama via Vulkan on my windows + 9060 XT 16GB rig).  I hope they tweaked the ollama install. Not all amd  cards (like mine) can use local ollama unless the environment variables tell it to use Vulkan instead of ROCm. Vanilla ollama performance like shit otherwise as it won't touch the GPU without the env vars  Edit - apparently the installer will skip ollama install if it's already installed. The ollama build that the installer installs is preconfigured to use rocm including the libraries needed. I'll have to configure some env vars manually to take advantage of it,pcmasterrace,2026-01-21 15:47:23,10
AMD,o0vhqsh,"God yeah  Im in computer science and absolutely hate the trend of LLMs and how its feasting on resources.  But good lord do I despise the people who will toss anything Machine Learning, or even remotely """"""""AI"""""""" into that same pot and burn it immediately. Harrassing people doing genuine research or using it actually efficiently pisses me off greatly just because they cant see the lines inbetween.",pcmasterrace,2026-01-21 15:59:16,46
AMD,o0vkjqp,Actually crazy to behole when you're familiar with the topic,pcmasterrace,2026-01-21 16:11:54,5
AMD,o0vup17,"i mean, hating cod is a must",pcmasterrace,2026-01-21 16:57:27,15
AMD,o0waat1,It's really strange to me how almost every reply to this post is disagreeing that it's bad with lots of upvotes and yet the post itself has 1.5k upvotes.  I guess all the people who just hate AI blindly are upvoting it without really bothering to engage in the discussion.,pcmasterrace,2026-01-21 18:06:50,24
AMD,o0vpr3z,an ai tool to run local offline ai using your gpu as the accelerator while using free open source model...,pcmasterrace,2026-01-21 16:35:21,21
AMD,o0wak2m,"This is not that. These are local llm tools. They are tools and open source softwares, not openai, claude etc... You can say these things are bad. It's worth a discussion but it's like comparing nestlÃ¨ to perhaps a mid size wheat farm.",pcmasterrace,2026-01-21 18:07:59,8
AMD,o0vmbir,Cept this is local AI and it's optional.  No one is making money off them (cept maybe you if you use commercial local models).,pcmasterrace,2026-01-21 16:19:51,26
AMD,o0w1t8x,It just makes the entry to local LLMs easier. Nothing wrong with that.,pcmasterrace,2026-01-21 17:29:27,1
AMD,o0vgp12,"Im not a developer. I just used python for the first time the other day to make a tray icon app that tells me the last episode i opened of the show I'm watching on VLC. I always forget to make a note of the last episode i watched, so this tiny script i learned how to make is very very helpful. I'll be using python some more, just have to think of ideas.  Its really simple and just the exact thing i needed. Basically it reads VLC for file names and updates a text file with the last opened episode, and then i have a little tray icon that reads the text file and when i hover over it with my mouse, it gives me a tooltip popup ""Last Opened: The Office S05E03"". Yes, I'm watching The Office",pcmasterrace,2026-01-21 15:54:37,17
AMD,o0vciv1,Idk if it means installing the AMD supported version easily without hunting and building a AMD specific version I'm down,pcmasterrace,2026-01-21 15:35:49,80
AMD,o0vnlx4,And that's why its optional. Wild concept.,pcmasterrace,2026-01-21 16:25:41,17
AMD,o0vr23z,good thing its optional,pcmasterrace,2026-01-21 16:41:09,3
AMD,o0zq3ty,I keep reminding myself that 53% of human population is below average when it comes to cognitive capabilities and it's okay.,pcmasterrace,2026-01-22 04:40:54,3
AMD,o119wn7,"Yup, I agree, it's sometimes tiring.",pcmasterrace,2026-01-22 12:26:44,1
AMD,o0vpqq3,Those aren't that smart either. Mostly is just s way for Samsung to feed you ads.,pcmasterrace,2026-01-21 16:35:18,2
AMD,o0vgatq,Jetbrains FTW man.   My teammate's VSC keeps freezing coz of thousands of plugins. And my IntelliJ works without any plugin,pcmasterrace,2026-01-21 15:52:51,4
AMD,o0viaio,"Radeon is notorious for being a tricky to get certain developement tooling (Pytorch with Rocm as far as I understand) working on - so it does somewhat make sense that AMD is the oen to push out a ""fix"" (fix being them offering to install for you so you dont have to struggle) its lousy cause the process should just be less complicated but its the quickest fix to adress peoples frustrations I suppose",pcmasterrace,2026-01-21 16:01:44,26
AMD,o0vjmoc,"doesn't seem weird at all to me, those tools are meant to run local AI models, which need your GPU and some small configuration steps",pcmasterrace,2026-01-21 16:07:48,18
AMD,o0vgm70,Could it be because GPUs are used in areas other than video games?,pcmasterrace,2026-01-21 15:54:16,32
AMD,o0vij7k,A lot of developers working in AI use their graphics card to run models. This looks like a convenient way to get that set up without too much fiddling.,pcmasterrace,2026-01-21 16:02:51,8
AMD,o0vegg7,It's just QoL tbh,pcmasterrace,2026-01-21 15:44:39,21
AMD,o0vd5o9,"Not really sure what the problem is, itâ€™s an optional download to take advantage of the GPU to do other tasks.",pcmasterrace,2026-01-21 15:38:44,21
AMD,o0vjb4h,Believe it or not those tools run locally on your GPU.,pcmasterrace,2026-01-21 16:06:22,5
AMD,o0vklb7,The AI stuff is used by the GPU (but you can offload some processes to cpu)  Plus AMD has had a lot of issues with local gen stuff working without putting effort in.  It at least makes sense since AI is the trendy thing.,pcmasterrace,2026-01-21 16:12:05,2
AMD,o0xebtv,"So like ""hey radeon, build me a calc app in python"" and then go to sleep for 8h?",pcmasterrace,2026-01-21 21:06:07,1
AMD,o0vd9md,Python is one of the most popular programming languages in recent years. And VSCode is a common development environment.,pcmasterrace,2026-01-21 15:39:14,8
AMD,o0visbe,idea is what instead of copilot you are using something locally runned in LM Studio.,pcmasterrace,2026-01-21 16:03:59,1
AMD,o0vypat,"Tried it on 3 different version and it never worked. So, returned to 2.2.2.",pcmasterrace,2026-01-21 17:15:34,1
AMD,o0yhiqq,"I know Optiscaler can, but for like 99% of people it is either the built in solution or none. To make the FP8 version run in FP16 should be relatively easy for AMD and they should at least do that. If some people want more upscaling performance then they can use Optiscaler with the INT8. Of course it will be for the best if AMD actually uses the INT8 version and also keeps the model updated in parallel with the FP8. That means maintaining 2 different main models (which in each model there are optimisations for several upscaling targets) which I doubt they will do.",pcmasterrace,2026-01-22 00:21:31,2
AMD,o0wx2nm,Ah yes I forgot. The mere mention of AI anywhere near people like this makes them quiver in their boots.,pcmasterrace,2026-01-21 19:47:49,3
AMD,o13my15,This is just an opening,pcmasterrace,2026-01-22 19:19:07,1
AMD,o0zvwlu,oh... I got nvidia instead of AMD mainly because of AI,pcmasterrace,2026-01-22 05:21:07,1
AMD,o18oryf,"Huh, I'll see. Thanks for letting me know",pcmasterrace,2026-01-23 14:13:37,1
AMD,o0wc88a,"Take the blue gpu, go back to the old world, take the red gpu and wake up.",pcmasterrace,2026-01-21 18:15:19,101
AMD,o1h395k,"What is real? How do you define 'real'? If you're talking about what you can feel, what you can smell, what you can taste and see, then 'real' is simply electrical signals interpreted by your brain.",pcmasterrace,2026-01-24 18:52:25,2
AMD,o0w43li,https://preview.redd.it/483o0h5boqeg1.png?width=1320&format=png&auto=webp&s=9683d9f7c24191622db43ae687759b90178a5b89,pcmasterrace,2026-01-21 17:39:40,35
AMD,o0w97or,"Yeah itâ€™s the FSR4 INT8, I use it all the time on my 7900 XTX.",pcmasterrace,2026-01-21 18:01:59,9
AMD,o0y18w4,I've never heard of it and want to know more.  -edit-  I found out.  [Optiscaler](https://github.com/optiscaler/OptiScaler)  and  [The leaked DLL](https://gofile.io/d/fiyGuj),pcmasterrace,2026-01-21 22:54:34,1
AMD,o0zqi0g,"This guy gets it.   Btw, I doubt it even works on Spider Man 2. People are so full of shit nowadays.",pcmasterrace,2026-01-22 04:43:32,1
AMD,o17lsvk,Honestly I'm not sure. I know 6000 series can use it so maybe it's worth a try.,pcmasterrace,2026-01-23 09:38:51,1
AMD,o0w7aun,"Yup, and it does have a visual quality upgrade over fsr3 but there seems to be issues with multiplayer games and anti cheat but haven't looked into it",pcmasterrace,2026-01-21 17:53:40,30
AMD,o0vxcqk,does this work on Z2E devices by any chance like the Legion Go 2?,pcmasterrace,2026-01-21 17:09:26,1
AMD,o0v9g33,Except when given that option they try it and when it does t work they blame the company and demand they make it work,pcmasterrace,2026-01-21 15:21:27,36
AMD,o0vp2c9,Nah.  It just won't run as well as it will on RDNA4.  AMD is being dumb by trying to keep it on RDNA4 only.  Sure isn't getting them any sales and has pissed off a whole lot of people.   Its a pretty egregious unforced error by AMD execs at this point.,pcmasterrace,2026-01-21 16:32:16,1
AMD,o0wihxm,I can play performance mode at 1440p in cyberpunk 2077 with all settings cranked without any noticeable artifacts on DLSS 4.5. Finally Path Tracing makes some sense on lower tier cards that don't cost the same as a used car especially with the ultra plus performance mod set on PTNEXTV3 medium. I use Reshade with AMD RCAS for sharpening and it works wonders too.,pcmasterrace,2026-01-21 18:42:38,3
AMD,o0vjgxt,Norton virus*,pcmasterrace,2026-01-21 16:07:06,64
AMD,o0x3y9k,fortunatly it wants internet straight away. interactive firewall wont even allow that. or any malware to acces their network,pcmasterrace,2026-01-21 20:19:11,1
AMD,o0w2byx,Microslop*,pcmasterrace,2026-01-21 17:31:45,5
AMD,o0zjadq,If only. Micro$oft doesn't tell you about the slop it's installing.,pcmasterrace,2026-01-22 03:56:41,1
AMD,o0vromo,best financial decision of my life was getting 96 gigs pre ram crisis lol,pcmasterrace,2026-01-21 16:43:57,2
AMD,o0wl3ho,"not even that, 6400cl32 is pretty good.",pcmasterrace,2026-01-21 18:53:59,1
AMD,o0wl9lj,Realistically you want a virtual environment for each individual project as they will all have conflicting dependencies. I have no idea how this tool from AMD does it and the announcement page is not helpful. If all your AI project share the same packages you are going to have a rough time.,pcmasterrace,2026-01-21 18:54:43,16
AMD,o0wprrf,"*Laughs in Linux*  (For the uninitiated, this is a super easy task for us)",pcmasterrace,2026-01-21 19:14:39,2
AMD,o0wypwf,"AMD doesn't benefit from it right now, but might benefit in 20 years when some of today's CS freshmen become project managers.",pcmasterrace,2026-01-21 19:55:14,8
AMD,o0w0mdb,"Interesting, i was under the impression that if you want to run semi-usefull local AI you should atleast be in the 50-70b models for it to be 'usefull' but for tinkering its probably fun to play with. I want to buy a H100 when they become more affordable to see if I can make my own local 'jarvis'.    On the contex of the post, I don't like that everyone is trying to force AI into everything, desprate to find a niche to try and make it profitable to recoup trillions of dollars invested into AI, and that is basicly what this is from AMD.",pcmasterrace,2026-01-21 17:24:09,2
AMD,o0w0xvz,"I tried a couple models recently with 12GB Vram and for light tasks the models are certainly usable. Of course not as good as the latest big models, but fully local. This seems like a good way to have a quick play around with this stuff, especially the user friendly tools.",pcmasterrace,2026-01-21 17:25:34,10
AMD,o0vlv2a,"Yeah, I'll admit that the portable install was already extremely simple (literally plug and play), but if you didn't want the portable install for some reason [it was still quite an involved process.](https://www.youtube.com/watch?v=CTp8FnT883E)",pcmasterrace,2026-01-21 16:17:49,11
AMD,o0wfyko,"Not on the 9000 series GPUs as they only just got ROCm support this update, before this you had to jump through hoops to get it working or use Linux.",pcmasterrace,2026-01-21 18:31:32,1
AMD,o0waq3x,"I'm not sure what other software you wanted to use, but for ComfyUI I've actually been using the Portable Install for ROCm for about a month now on my 7800 XT. Even before this AI Bundle, the Portable Install automatically set everything up for me. I downloaded it, ran the .bat, and was immediately able to start generating.  That's rough. I mean, if it's only been a week and you'd really appreciate the extra $150 bucks, you could probably still return it and buy a 9070 XT instead?  That said, you might want to keep your 5070 Ti, because there's still a few things on the local AI side that are still designed completely for Nvidia. Nunchaku quants don't work on AMD and the devs have stated they have no intention of getting it working for it, and one of the best S2T models out there right now, Nvidia's Canary, is unsurprisingly only able to run on Nvidia cards because it uses Nvidia's NeMo as one of its dependencies.",pcmasterrace,2026-01-21 18:08:45,1
AMD,o0x368x,"You don't need to install anything, the whole point of this is that it does it all for you.  What you might need to do is uninstall things. I switched over from ZLUDA to ROCm a little while back, and while it worked it was acting weird because I had an old version of ROCm (6.2.4 or something) installed along with a bunch of HIP SDKs from my ZLUDA installation. Uninstall those before you update and remove variables from PATH or things might be weird.   I can't remember everything I had to uninstall, but ask an AI like Gemini and it'll walk you through what needs to be done. That's what I did.",pcmasterrace,2026-01-21 20:15:35,1
AMD,o0veudl,It's me. I'm some. Goddamn ROCm seems to always be available for every card and environment except the ones I have.,pcmasterrace,2026-01-21 15:46:23,31
AMD,o0vikty,"Don't you see   If it's made before 2020 it's "" machine learning "" but if it's the same shit made after 2020 it's """""" AI """"""",pcmasterrace,2026-01-21 16:03:02,15
AMD,o0vjrcc,"In fairness they canâ€™t see the lines inbetween because the marketing craze for the last, what? 3-4 years now has been AI = LLM/GenAI stuff and not ML. Like if you talk about AI to some rando off the street they probably think of ChatGPT",pcmasterrace,2026-01-21 16:08:23,10
AMD,o0xnpzh,"I know itâ€™s not the most popular but slather it in beer batter and fry it up and I canâ€™t get enough, donâ€™t understand the hate",pcmasterrace,2026-01-21 21:48:55,1
AMD,o0y1vf2,"Reddit *loves* loudly hating things, it's seriously like a hobby for y'all.",pcmasterrace,2026-01-21 22:57:45,1
AMD,o0wgg80,"Yeah, tha'd make sense.  Could also be reddit manipulating votes to drive engagement or bots (with the same end goal).  Controversial posts often generate more engagement and I wouldn't put it past reddit doing it given facebook already does.",pcmasterrace,2026-01-21 18:33:43,8
AMD,o0zk0uh,More akin to someone growing weat in their backyard really.,pcmasterrace,2026-01-22 04:01:17,1
AMD,o0w7d3m,"What IS wrong with it is that it completely depends on an open source piece of software that is BARELY credited. And if that wasn't bad enough, there was the whole deal with calling deepseek distils ""deepseek r1"", making people think they were actually running the original model. It's overall incredibly dumbed down, not even letting you choose the number of layers to offload or other useful llama.cpp arguments like override tensor.",pcmasterrace,2026-01-21 17:53:57,2
AMD,o0vrhbd,"That's a decent solution but there are video players like PotPlayer that automatically do this - it can show you a list of last played files, and it can resume playing at the same exact time you paused on last time.",pcmasterrace,2026-01-21 16:43:02,2
AMD,o0vvz0v,"What does optional even mean? It means I need to install it, right?",pcmasterrace,2026-01-21 17:03:12,0
AMD,o0vqk38,Still smarter than a person complaining about optional bundle that is not checked by default.,pcmasterrace,2026-01-21 16:38:57,5
AMD,o0zps74,"Not that slow.   But if you tell it to analyze and document 10K lines of code application and build unit tests, it will run out of context window.",pcmasterrace,2026-01-22 04:38:44,1
AMD,o0vetga,To be fair if you decide to install one it looks like youâ€™re installing all of them and all except VS Code and Python are AI tools,pcmasterrace,2026-01-21 15:46:16,3
AMD,o0vdzg3,"No shit.   SDK stands for System Development Kit, they often come with IDE and runtimes.  Python is needed by PyTorch and LM Studio, and better VSCode than some custom IDE.",pcmasterrace,2026-01-21 15:42:31,3
AMD,o0wbv2m,"Interesting, its the only thing I changed and it works lmao",pcmasterrace,2026-01-21 18:13:44,1
AMD,o0x7ck9,"You know, I know these frames don't exist. I know that when I watch these pixels, the Matrix is telling my brain that it is buttery and smooth. After nine years, you know what I realize?  *[Loads up Crysis]*  Ignorance is bliss.",pcmasterrace,2026-01-21 20:34:35,53
AMD,o0w4vir,ðŸ˜‚,pcmasterrace,2026-01-21 17:43:06,8
AMD,o0wdecd,Can you link me or show me a thread on how to do this for my 7900xtx? Didn't know it was a thing,pcmasterrace,2026-01-21 18:20:22,4
AMD,o0wizwq,"Propably only because it's not natively activated and you need some third party shenanigans. If AMD allowed you to use it, it'd be fine.",pcmasterrace,2026-01-21 18:44:49,18
AMD,o0xckzn,you canâ€™t use OptiScaler with anything that uses anti cheat bc you have to mod the game files to get OptiScaler working. itâ€™s very simple modding but obvs that wonâ€™t fly regardless,pcmasterrace,2026-01-21 20:58:10,5
AMD,o104ny6,Any chance it will work on rx6400?,pcmasterrace,2026-01-22 06:29:12,1
AMD,o0vewzs,"Almost because itâ€™s a hack and not supported natively by AMD. still works better on many games on some cards. A lower FSR4 setting looks better than a higher FSR3 setting, and performs about similarly. Thatâ€™s a big jump in visuals for not much performance hit. Iâ€™m going to run cyberpunk that way on my 7900XT since FSR4 is the next best thing compared to DLSS, and way better than FSR3. Even on Steam Deck there are some games that look and play better with FSR4 hacked in compared to FSR3.",pcmasterrace,2026-01-21 15:46:42,20
AMD,o0vfqzf,"Consumer-blaming isn't the solution to late stage capitalism's problems.  Consumers can demand anything they want. The market can then decide if there is enough demand to supply a solution. If one company cannot make it work, someone else can, or at least try. But since we only have 2 alternatives in the market, these companies can do whatever the fuck they want.",pcmasterrace,2026-01-21 15:50:24,17
AMD,o0w2g15,"I havenâ€™t really seen that much anger towards Nvidia over DLSS 4.5 not running that well on the 20 and 30 series. There have been some criticisms of it because Nvidia has recommending it even on those cards for performance and ultra performance when it can in quite a few cases delivered worse performance than quality mode using the old preset, but otherwise people just seem happy to have the option.",pcmasterrace,2026-01-21 17:32:16,1
AMD,o0vv9c6,So it's not that they can't they just won't got it,pcmasterrace,2026-01-21 16:59:58,2
AMD,o0w8471,Adobe Reader / McAfee injection,pcmasterrace,2026-01-21 17:57:13,17
AMD,o0x63j5,"Comfy ui does it. What is on screen is really a bundle, everything you need to start with your local ai. I suppose this four models are just installing in comfy to work ""out of box""",pcmasterrace,2026-01-21 20:28:53,10
AMD,o0y8wj6,"I wish I knew about venv when I started doing uni projects on my pc, I have around 100gb of useless libraries or temp files of 4 years old finished projects.",pcmasterrace,2026-01-21 23:35:05,2
AMD,o0ykyi2,"still struggling to run davinci resolve on my machine, amd on fedora. haven't had the time to go around fix it lol",pcmasterrace,2026-01-22 00:40:04,2
AMD,o0w5c6j,"It's local so they aren't making money from it, you run whatever you want to. Also you can run quantized 70b model with just 32gb ram in won't be fast but you can try it out.",pcmasterrace,2026-01-21 17:45:09,4
AMD,o0ycwge,"Yeah, since most of the fun models and software ""just works"" on the Ti, I will be keeping it. I've been ""team red"" for so long it will be interesting to see what's going on with ""team green"".",pcmasterrace,2026-01-21 23:56:41,1
AMD,o0vfjz7,Just curious: is installing and using PyTorch through WSL2 better than natively for Windows?,pcmasterrace,2026-01-21 15:49:31,9
AMD,o0vy1mv,"Oh without a doubt, im not saying people are intentionally ignorant, like 85% of this is straight on the companies pushing this bs. Tho the last 15% get to be upset-ness about people being ignorant.",pcmasterrace,2026-01-21 17:12:36,5
AMD,o10l2tk,"To be honest the reason i didnt say that, is it still requires a lot of energy, and data to train open source models. Its not like someone can just do it. But if the model is done then yes.",pcmasterrace,2026-01-22 08:55:37,1
AMD,o11xm2d,"The fact that Ollama is fully dependent on llama.cpp is really not well known, I have to agree on you with that. But things like offloading layers etc. is just something a new user that wants to play around with local models is not going to use or understand. It is dumbed down, but that is the exact purpose. Power users can still use alternatives like llama.cpp, vLLM, etc.",pcmasterrace,2026-01-22 14:40:25,1
AMD,o0w7d09,"i just like VLC. good to know tho, i'll probably stick with my solution since i already did it, but i might recommend that player to someone in the future. cheers",pcmasterrace,2026-01-21 17:53:56,1
AMD,o0vrjwr,And Python is there because those AI tools depend on it.,pcmasterrace,2026-01-21 16:43:21,3
AMD,o0xv9dg,The realization comes when you know the truth. There are no frames.  ![gif](giphy|3o6Zt0hNCfak3QCqsw),pcmasterrace,2026-01-21 22:24:38,23
AMD,o0xn1qh,"Hey, is this done on the driver level or can i have fsr3 on one game and fs4 on another",pcmasterrace,2026-01-21 21:45:49,1
AMD,o1017wa,"Weird. It just doesn't run, or what?",pcmasterrace,2026-01-22 06:01:27,1
AMD,o0vox0p,"I asked the same question about ROCm in r/ROCm a little while ago, and the consensus was that it wasn't worth it. There'd be a performance loss due to the additional overhead.  Don't know a damn thing about Pytorch because I don't develop so I'm not sure if that translates over, but if AMD is promising better native Pytorch support I'd imagine that sticking with Windows is the play for now.",pcmasterrace,2026-01-21 16:31:35,2
AMD,o0vz2m8,"Yeah that's kinda where I'm at. I have ethical issues with Gen-AI as well, but the core of the issue is corporations trying to shove it into literally everything even when it's not wanted nor needed, all in an effort to justify the absolutely ungodly amounts of money they've burned trying to get it to where it's at now. And that's not to say it doesn't have good uses, just that it doesn't need to be in *everything*",pcmasterrace,2026-01-21 17:17:14,4
AMD,o0wwten,They are intentionally ignorant. They have access to all of the same info we do. But instead of understanding it they just say slop because they donâ€™t have the faculties to understand anything ai related.,pcmasterrace,2026-01-21 19:46:39,1
AMD,o1206v3,"Llama.cpp can be used the exact same way. If -ngl isn't specified it uses fit-params to automatically find a decent value for your vram (because -fit is on by default). Nowadays it even has a nice little frontend bundled with llama-server that you can just use after loading a model with it, there is really no excuse to use ollama whether you're a newbie or an enthusiast.",pcmasterrace,2026-01-22 14:53:10,1
AMD,o0zdug1,Classic GT 710 user...,pcmasterrace,2026-01-22 03:23:29,11
AMD,o0z9cqb,![gif](giphy|MC6eSuC3yypCU),pcmasterrace,2026-01-22 02:57:25,5
AMD,o0zjcso,"Then you will realize it is not the frames that render, it is only yourself.",pcmasterrace,2026-01-22 03:57:05,3
AMD,o0xnl18,"if youâ€™re on RDNA 2/3 trying to use FSR 4 via INT8, itâ€™s game by game. youâ€™re modding the game files the same way you would to add a texture pack or sumn like that. check out the official OptiScaler GitHub page  if you have an RX 9000 card then you can either use driver override in the AMD Adrenaline app for supported games or use OptiScaler per game.",pcmasterrace,2026-01-21 21:48:16,2
AMD,o10nyq9,"it runs, but whenever I drag something onto the timeline and hit play, it just hangs.   Tried installing ROCm (from fedora repo not copr) but no luck, still hangs.  rocminfo returns that I have a valid installation.  Fedora 43 KDE, 6700XT",pcmasterrace,2026-01-22 09:23:00,1
AMD,o184ztp,That game ran so smoothly in a GTX6200,pcmasterrace,2026-01-23 12:18:26,1
AMD,nzd2hb5,Itâ€™s fine.,pcmasterrace,2026-01-13 15:05:12,4
AMD,nzd2l52,Solid option under $200. Not many good deals on the 7800x3d unless near a Microcenter.,pcmasterrace,2026-01-13 15:05:44,2
AMD,nzd3thz,"I game at 4K/144Hz with an RX7900XTX and the same CPU and mobo as you. It runs with High settings or better in all games.  Up until Christmas I was 4K gaming with the same card and a 12400 on a Z690 board, which also ran very well.  The new rig was a PITA to set up: AM5 is very picky with series 9000 CPUs, especially with fast DDR5 and reused NVME drives.",pcmasterrace,2026-01-13 15:11:48,1
AMD,nzd53lo,"It's fine, I got the 7600x combo at microcenter and matched it with a 9060xt 16gb for my dad, and my nephew was using it for cyberpunk for hours at 1440p, played and looked great",pcmasterrace,2026-01-13 15:18:00,1
AMD,nzd5dyd,Its a great entry level cpu.,pcmasterrace,2026-01-13 15:19:24,1
AMD,nzd91ee,"I'm rocking the same CPU, but with a 5070 Ti. I totally agree that this CPU isn't talked about enough. I'm more than happy with it.",pcmasterrace,2026-01-13 15:36:55,1
AMD,nzdahkx,Coming from a i5-7600k to a 9600x I think it's a damn good CPU!,pcmasterrace,2026-01-13 15:43:39,1
AMD,nzdc5gk,"Yes, quite good. It's not good anymore only for new builds as ddr5 ram is very expensive so AM4 is way cheaper",pcmasterrace,2026-01-13 15:51:19,1
AMD,nzdo4kw,>because nobody talks about it i  What?  Great combo btw.,pcmasterrace,2026-01-13 16:45:52,1
AMD,nze31yr,"Especially in 4K, as the general workload will be heavily GPU dependent. Wife's got a 9600X and 5070 Ti at 4K no issues.",pcmasterrace,2026-01-13 18:06:07,1
AMD,nzdtww5,For 1080p itâ€™s one of the best CPU price to performance wise. And pairs well with mid range GPUs like 5060 Ti 16gb / 5070 but for 4k definitely want something stronger.,pcmasterrace,2026-01-13 17:24:18,1
AMD,nzduhtx,Yeah iâ€™m using a 5060 TI 16gb with the 9600x and It gives me 120+ frames on Arc Raiders in 1080p and Balanced DLSS,pcmasterrace,2026-01-13 17:27:01,1
AMD,o0wh0hz,"I look at it like this. If AMD was doing a huge number of RMAs and they thought it was due to the packaging exposing the CPU to impact, etc. they would likely change the packaging. I have a feeling the retail boxing is a non-issue.",pcmasterrace,2026-01-21 18:36:11,2
AMD,o0wwyzb,This is the Answer! ðŸ˜Ž  https://preview.redd.it/b9avfnd3breg1.jpeg?width=283&format=pjpg&auto=webp&s=bb6eb9b8bd15ed2c58c544cb6324109322a9fb71,pcmasterrace,2026-01-21 19:47:21,1
AMD,o0wewbv,"Not ideal...  But realistically you could probably throw it at a rock and, assuming no physical defect to inhibit putting it in the socket(or pad damage), probably be okay.",pcmasterrace,2026-01-21 18:26:54,0
AMD,o0wgnsb,I just find it amusingly contradicting when the store wanted me to package the damaged CPU in 2inches of padding when sending it. And they just yeet the thing in a box with nothing and its good to go in their opinion.,pcmasterrace,2026-01-21 18:34:38,1
AMD,o1feyi5,What's the model of the 9070xt? Never seen this one. Is it a prebuilt exclusive or something? Looks pretty neat (as does the whole build â€“ congrats!),pcmasterrace,2026-01-24 14:12:13,6
AMD,o1fb0li,"Wow, weâ€™re seriously fucked if prebuilds are better value than buying parts individually.   I know DawidDoesTechStuff did a video on comparing prebuild pricing to building it yourself, the only way he could get it close to even was by getting a bundle at microcenter, which thatâ€™s obviously not applicable here.",pcmasterrace,2026-01-24 13:49:17,4
AMD,o1feh2n,Ngl this is looking pretty decent for a pre-build,pcmasterrace,2026-01-24 14:09:28,6
AMD,o1gjbrw,nice looking prebuild with great specs  I almost pulled the trigger on a costco prebuild here in the states but was able to build a custom one that was slightly more expensive with everything I wanted before prices rose more than a couple hundred on those parts only a month ago  if you have a second disk install and game on linux with that all amd build ðŸ˜Ž,pcmasterrace,2026-01-24 17:26:45,3
AMD,o1fk8n8,"Prebuilts are pretty good now as long as you watch prices and build quality. I actually just helped my step-dad buy a new prebuilt last month. After pricing all the same components it was like $50 more for a prebuilt that was already assembled with a full warrenty vs doing it ourselves.  At this point for me, new builds from scratch for friends will probably just be pre-built recommended ones. Ill look up new parts when they need to upgrade individual components down the line.",pcmasterrace,2026-01-24 14:41:52,2
AMD,o1fi0d1,I think you should jump on this asap,pcmasterrace,2026-01-24 14:29:28,1
AMD,o1ficik,"Yea itâ€™s OMEN branded, so Iâ€™m guessing exclusive for their prebuilds. Clean looking card tough!",pcmasterrace,2026-01-24 14:31:21,7
AMD,o1fiic6,I donâ€™t know how the prices are the States. In Europe they are fuckedâ€¦,pcmasterrace,2026-01-24 14:32:14,6
AMD,o1fiio8,I did the exact same thing as OP but with a ROG (same specs). I was keeping an eye on prices for a few prebuilt systems and they went OOS fast or the prices went up. I think the prices will slowly catch up once they go through their inventory.   What I wouldâ€™ve got different if I had built it myself: a modular PSU and a less cringey case (Iâ€™m keeping the RGB off). Everything else is decent parts.,pcmasterrace,2026-01-24 14:32:17,3
AMD,o1gnsw9,Thx for the tip! Lucky you for building one before the prices went Crazy!   Maybe iâ€™ll have to install Steamos.,pcmasterrace,2026-01-24 17:46:34,2
AMD,o1fimct,Already bought it :) dindâ€™t hesitate one bit!,pcmasterrace,2026-01-24 14:32:51,3
AMD,o1hojkg,Thatâ€™s actually a pretty nice looking build and a good price,pcmasterrace,2026-01-24 20:28:10,3
AMD,o1fjnc6,"Oh yeah, theyâ€™re absolutely fucked here too. Maybe slightly better without VAT, but still",pcmasterrace,2026-01-24 14:38:36,4
AMD,o1glrcg,"Good for you, what do you plan to play?",pcmasterrace,2026-01-24 17:37:32,1
AMD,o1gn8js,Well! My previous pc had a 1080ti and a ryzen 2700x. Arc raiders and tarkov were running like shit. So for sure those!,pcmasterrace,2026-01-24 17:44:05,1
AMD,o1gxdqq,"Nice nice, 1440p monitor?",pcmasterrace,2026-01-24 18:27:28,1
AMD,o1ihv38,Yes dual g27 from Samsung!,pcmasterrace,2026-01-24 22:48:31,1
AMD,o19noli,Edit: I have a ryzen 7 3700x not a 5700x,pcmasterrace,2026-01-23 16:57:27,1
AMD,o19ocmn,Very nice upgrade. It does look like it's just one ram stick.,pcmasterrace,2026-01-23 17:00:29,1
AMD,o19qi7z,I was about to say I think raw load the 5700X is faster real world itâ€™s blow for blow. But if you have 3700X yeah this is an upgrade. Iâ€™ve seen better deals lately but if itâ€™s all you can access yeah itâ€™s okay.,pcmasterrace,2026-01-23 17:10:22,1
AMD,o19pnms,https://preview.redd.it/vwof6lg7s4fg1.png?width=1068&format=png&auto=webp&s=8afe13822c3f32c566a0ad83da5a46e6c32acc5f,pcmasterrace,2026-01-23 17:06:27,3
AMD,o19r5or,Yep,pcmasterrace,2026-01-23 17:13:24,1
AMD,o19vaii,"Yeah Ik about this, but I want it to be in general not for specific apps",pcmasterrace,2026-01-23 17:32:58,1
AMD,o1a0409,your laptop doesn't have a mux switch so it's impossible,pcmasterrace,2026-01-23 17:54:56,3
AMD,o178cb9,are the bioses for the 9850x3d even out,pcmasterrace,2026-01-23 07:34:55,608
AMD,o17trc2,"https://preview.redd.it/e3n9f37tw2fg1.jpeg?width=4000&format=pjpg&auto=webp&s=4dbacccdf420dc58b4eb0f8c90350ed7cf29f598  mine is not boosting over 5700Mhz, whaatever I try. MSI Carbon Wifi with AGESA PI pre-1.3.0.0. CO -30, Fmax +200. I think it's the BIOS since don't reach temp-limits.",pcmasterrace,2026-01-23 10:50:41,90
AMD,o178x69,i will also get mine in the mail today... weird,pcmasterrace,2026-01-23 07:40:03,81
AMD,o179cwx,wait is this real? it says 2024 but its hasnt been released yet right?,pcmasterrace,2026-01-23 07:43:57,108
AMD,o180dmj,"It's obviously better than a 7800x3d, but I'm wondering do you see any noticable difference? Like not numbers but exeperience-wise?",pcmasterrace,2026-01-23 11:44:42,22
AMD,o17iakx,Benchmarks Please!,pcmasterrace,2026-01-23 09:05:31,34
AMD,o18vagi,Same TSMC 4nm node as 9800x3d with 7.7% faster boost clock so doesn't seem like a significant upgrade.,pcmasterrace,2026-01-23 14:46:46,8
AMD,o1b838f,"Honestly it is disappointing. It is not any faster than my stock 9800X3D at 5.2GHz in MH wilds benchmark with an RTX 5090 and it runs way hotter. Frequencies not even hitting 5.6GHz, hovering around 5.3-5.5GHz stock with 6000MT/s CL26 2000FCLK 3000UCLK. Adding a -10CO makes it go to 5.45-5.58GHz but I get BSODs on desktop.   Running AGESA PI 1.2.0.3g on MSI X870 Pro Wifi.   Screenshots attached below. 1080P Ultra no DLSS/AA/HDR:  https://imgur.com/a/GfHuQua   Edit: It wont do 8000MT/s either which my 9800X3D can without issues.",pcmasterrace,2026-01-23 21:18:13,7
AMD,o17pv6x,"Oha, Gratuliere",pcmasterrace,2026-01-23 10:16:07,4
AMD,o17y5kc,Interdiscount? I was tempted to do the same. Unlisted now.,pcmasterrace,2026-01-23 11:27:14,3
AMD,o18rlfu,How's the IMC? Do you think its easier to run 6400mhz,pcmasterrace,2026-01-23 14:27:59,3
AMD,o18jvfb,wo hesch de den fÃ¼r 450 kauft?,pcmasterrace,2026-01-23 13:48:01,2
AMD,o193039,"And I got your fingerprint, by chance :D",pcmasterrace,2026-01-23 15:24:18,2
AMD,o17du64,Really interested to see if these have the same issue as the 9800x3d,pcmasterrace,2026-01-23 08:24:16,8
AMD,o17qc9w,Is the boostclock 5.75 Ghz on all cores?,pcmasterrace,2026-01-23 10:20:21,2
AMD,o192813,Itâ€™s literally a 9800x3Dâ€¦.nothing special about this cpu at all,pcmasterrace,2026-01-23 15:20:41,2
AMD,o17l15o,I just bought asus tuf b850 and waiting for this cpu to build pc. Should work out of box or i need Flash bios update before pc start ?,pcmasterrace,2026-01-23 09:31:37,1
AMD,o17pj3v,TimeSpy Score?,pcmasterrace,2026-01-23 10:13:06,1
AMD,o18k5qq,"When will the first user hit 6 GHz (with normal cooling)? Should be possible to set PBO to +200, undervolt all cores by 20-30, set power limit to 180-200 W, increase scalar and increase mhz of external clock generator with the right board.",pcmasterrace,2026-01-23 13:49:32,1
AMD,o18yhu1,Say would you mind sharing info on that seller if there is stock available that is!,pcmasterrace,2026-01-23 15:02:38,1
AMD,o19hre5,How is the imc,pcmasterrace,2026-01-23 16:30:56,1
AMD,o19indy,Wo genau?,pcmasterrace,2026-01-23 16:34:54,1
AMD,o1a5jl0,it will be hot remember that :D,pcmasterrace,2026-01-23 18:19:12,1
AMD,o1cquqb,Are these supposed to be a little brother to 9950x3d or better ?,pcmasterrace,2026-01-24 02:07:06,1
AMD,o1f384l,me la regalas :),pcmasterrace,2026-01-24 13:00:18,1
AMD,o1ga1z4,Retailer name?:),pcmasterrace,2026-01-24 16:45:32,1
AMD,o1h1vvr,"Would love to see if the IMC is better. Can FCLK 2233 or higher  boot and be stable? (Beware, you may need to clear CMOS if it doesn't boot)",pcmasterrace,2026-01-24 18:46:38,1
AMD,o17s3a9,hesh es BI interdiscount bestellt? mine chunt hÃ¼t VO deht,pcmasterrace,2026-01-23 10:36:07,1
AMD,o17qpxd,"Are you gonna try +200 PBO on it? Would love to know if its supported, even though id think it is",pcmasterrace,2026-01-23 10:23:43,1
AMD,o17m5cu,"ja lÃ¤ck du mir, probier de shit",pcmasterrace,2026-01-23 09:42:05,-1
AMD,o17x2bn,"mine is coming next week monday or tuesday.   I live in Central EU, I never thought I would get it before release, thats so random. hahaha",pcmasterrace,2026-01-23 11:18:24,0
AMD,o18csei,Looks cool. Hope it gives you those extra fps you need.,pcmasterrace,2026-01-23 13:08:42,0
AMD,o18md9p,how big is the difference with 7800X3D ?,pcmasterrace,2026-01-23 14:01:05,0
AMD,o17tbiu,So this is the 14900k.. i mean the 9850x3d.  Let see how long it last,pcmasterrace,2026-01-23 10:46:53,-7
AMD,o17bdb3,They usually add support for new cpus beforehand,pcmasterrace,2026-01-23 08:01:54,338
AMD,o1796sc,"it managed to boot with the latest msi bios (from Dec)  edit: MSI X670E GAMING PLUS WIFI, Bios: 7E16v1C7, AGESA PI 1.2.0.3g, 2025-10-22  not sure if the bios pbo settings are working",pcmasterrace,2026-01-23 07:42:26,228
AMD,o17dhkl,"The reason we knew the CPU existed at all before it was announced, is because people pull apart BIOS updates every time there's a change, and that's how they found support for this CPU.",pcmasterrace,2026-01-23 08:21:05,146
AMD,o17ddxs,"It's just a higher clocked 9800X3D, no need for the new bios",pcmasterrace,2026-01-23 08:20:10,74
AMD,o18u5sl,They should be! How else would reviewers and YouTubers be able to make their content.,pcmasterrace,2026-01-23 14:41:07,1
AMD,o1874xl,https://preview.redd.it/9k27ej4ff3fg1.jpeg?width=2052&format=pjpg&auto=webp&s=eed20a475be23972fbbccec9ae0cae85d1b2f9aa  CPUz Screenshot,pcmasterrace,2026-01-23 12:32:58,26
AMD,o19rhnq,How you like that msi carbon?,pcmasterrace,2026-01-23 17:14:57,2
AMD,o17at6c,i think the trademark is from 2024,pcmasterrace,2026-01-23 07:56:55,139
AMD,o17bfls,The manufacture week/year is under the gray box OP put.,pcmasterrace,2026-01-23 08:02:29,40
AMD,o17wpay,Yes should be real. Here in Austria/Germany you could buy it yesterday from some online retailers for 515â‚¬. I already have a 9800x3D so i did not buy it lol,pcmasterrace,2026-01-23 11:15:27,9
AMD,o19gqzj,Itâ€™s in retail stores right now,pcmasterrace,2026-01-23 16:26:23,1
AMD,o186ikt,"nope, not at all",pcmasterrace,2026-01-23 12:28:49,29
AMD,o182qny,Only if you play below 1440p resolution,pcmasterrace,2026-01-23 12:02:18,7
AMD,o1ckgbv,Turn off the frame counter and your average gamer can't tell the difference between a 5800x3d and a 9800x3d.,pcmasterrace,2026-01-24 01:29:26,2
AMD,o1druci,"The 9800x3d is already like 10% better than the 7800x3d, and the 9850x3d is like 6-7% better than the 9800x3d. With pc parts this high end, you wont be able to tell the difference without the numbers.",pcmasterrace,2026-01-24 06:08:27,0
AMD,o17nahq,"Spoiler alert, it's a tiny bit faster then a 9800X3D",pcmasterrace,2026-01-23 09:52:33,115
AMD,o17pauq,"stock bios settings (only with expo enabled)  Cinebench 2026, Single Thread: 568",pcmasterrace,2026-01-23 10:11:03,31
AMD,o17yiwa,yep,pcmasterrace,2026-01-23 11:30:12,2
AMD,o18uzls,ich gib der mine fÃ¼r 540 :D  ish grad vorher ih de post cho,pcmasterrace,2026-01-23 14:45:15,3
AMD,o18m9a8,si hÃ¤nd de fehler korrigiert und es isch nÃ¼me online.,pcmasterrace,2026-01-23 14:00:30,2
AMD,o17rk8n,"if it dies, it dies",pcmasterrace,2026-01-23 10:31:21,34
AMD,o17en20,What is the issue? Genuinely curious.,pcmasterrace,2026-01-23 08:31:41,13
AMD,o1b437s,whats the fps gains by doing this from a 5.6ghz stock clock? Its worth all the try & error?,pcmasterrace,2026-01-23 20:59:17,1
AMD,o1ere9u,I did that last april on my 9950x3d vcache CCD. Not stable at 6ghz but can get sensor polls there. My stable clocks are up to ~5850mhz idle and ~5700 in games (1x scalar).,pcmasterrace,2026-01-24 11:27:09,1
AMD,o1degdo,It's a 9800x3d that's a tiny bit better. Still the same architecture just a tiny bit more mhz boost,pcmasterrace,2026-01-24 04:31:22,2
AMD,o1gk5on,"the poor soul that made the mistake, I don't want to add more oil to the fire.",pcmasterrace,2026-01-24 17:30:25,1
AMD,o17rst0,when i find time,pcmasterrace,2026-01-23 10:33:31,3
AMD,o17p9o6,"ich bin dran, aber ich habe keine zeit",pcmasterrace,2026-01-23 10:10:46,1
AMD,o17y66z,the website where I bought it doesnt list it anymore.  So it must have been a mistake :D,pcmasterrace,2026-01-23 11:27:22,1
AMD,o1de47m,Anywhere from a little bit to none depending on the rest of your set up,pcmasterrace,2026-01-24 04:29:05,-1
AMD,o18t6w4,correct way before like 2 months or so!,pcmasterrace,2026-01-23 14:36:11,74
AMD,o19w73n,"Be careful, and wait for official updates. I still remember X3D chips frying",pcmasterrace,2026-01-23 17:37:08,19
AMD,o19sk55,What board?,pcmasterrace,2026-01-23 17:20:05,3
AMD,o1c4343,I mean yah it was like a year ago man,pcmasterrace,2026-01-23 23:58:39,1
AMD,o187dwf,"https://preview.redd.it/y2e3adspf3fg1.jpeg?width=4000&format=pjpg&auto=webp&s=a220db0069e5cd30d7ba01161474085226575eea  CS2 - Benchmark Map, avg. 913 fps, P1 334 fps",pcmasterrace,2026-01-23 12:34:39,21
AMD,o18kehm,Thx for reassuring me on my 7900x,pcmasterrace,2026-01-23 13:50:48,9
AMD,o18be8n,Or paradox games. Maybe you can play large map Stellaris till 2400,pcmasterrace,2026-01-23 13:00:07,4
AMD,o190hzz,"I don't think resolution is going to play such a major role in cpu usage. They actually lower the resolution to 1080p to compare cpu's to remove the gpu bottleneck. Very game dependent ofc.  I believe this cpu is made for the no-compromise setup, stellar at gaming Ã¡nd productivity, that last is where it will leave the 7800x3d in the dust",pcmasterrace,2026-01-23 15:12:26,0
AMD,o1es9vi,The average gamer isnâ€™t getting a 9800x3d,pcmasterrace,2026-01-24 11:34:55,1
AMD,o1erj77,> and the 9850x3d is like 6-7% better than the 9800x3d.   AMD claims that it's 2.8% better than the 9800x3d.,pcmasterrace,2026-01-24 11:28:20,1
AMD,o17rdhg,But how do you know!?!,pcmasterrace,2026-01-23 10:29:39,12
AMD,o17qpza,"Yo. If you enable a 200 boost clock override in the bios and play a game that is receptive to single thread boost like Hogwarts Legacy, does it boost up to 5.8GHz",pcmasterrace,2026-01-23 10:23:44,9
AMD,o1ewbij,"I get 562 with a 9800X3D PBO+100   https://i.imgur.com/kAUmokG.png  that's 300 MHz slower than stock 9850X3D, interesting.",pcmasterrace,2026-01-24 12:08:42,1
AMD,o18vdh0,danke aber scho guet ganz zfride mit mit 9800,pcmasterrace,2026-01-23 14:47:11,2
AMD,o17f51s,they die early.,pcmasterrace,2026-01-23 08:36:19,8
AMD,o17sf7n,"Dann schick ihn mir, ich teste ihn",pcmasterrace,2026-01-23 10:39:03,1
AMD,o17qfma,eh,pcmasterrace,2026-01-23 10:21:11,-2
AMD,o182btr,"its 100% a mistake. but I was still willing to try it out as long as they kept it showing. within the same hour it got taken down. yet, I still got the email conformation, that the product will be shipped on monday (still 4 days before official release). great times I suppose :)",pcmasterrace,2026-01-23 11:59:15,1
AMD,o1a37kw,Frying from what exactly? Just upgraded to an AM4 5700x3d so just curious.   I did upgrade my BIOS well in advance though.,pcmasterrace,2026-01-23 18:08:48,7
AMD,o17mxkw,"Wrong. Same 96MB L3 + 8MB L2.  Check with official sources or news sites, not AI overviews",pcmasterrace,2026-01-23 09:49:13,53
AMD,o17vwl6,Are you confusing this with the 9950X3D that is coming sometime later this year?,pcmasterrace,2026-01-23 11:08:46,4
AMD,o18dj8j,Iâ€™am getting similiar frames with 7800x3d,pcmasterrace,2026-01-23 13:13:06,15
AMD,o1gdhqr,https://preview.redd.it/qt038cg3wbfg1.jpeg?width=1024&format=pjpg&auto=webp&s=9b1b51f57aec08cdf343238b31b23ec76ca78161  With CO -31 and fmax +200 I'm getting now a 5780Mhz boost out of the 5850Mhz that should be possible ðŸ˜ƒ so there are still 70Mhz missing ðŸ¤£,pcmasterrace,2026-01-24 17:00:41,1
AMD,o1i2133,what setting do you use?,pcmasterrace,2026-01-24 21:32:14,1
AMD,o1cur9a,Interesting to see how it fares against a 14900k with tuned ram :S [https://imgur.com/a/uakJ4bg](https://imgur.com/a/uakJ4bg),pcmasterrace,2026-01-24 02:29:40,-1
AMD,o1ce8kl,I can either my 9800x3D ðŸ˜ƒ,pcmasterrace,2026-01-24 00:53:37,2
AMD,o18eiy4,"Same architecture, same cache, just faster clock",pcmasterrace,2026-01-23 13:18:52,25
AMD,o186h8p,Bigger number = faster,pcmasterrace,2026-01-23 12:28:34,27
AMD,o17sclj,"I also had this happen to me, and with a +200MHz override with didn't move past 5.6GHz, I assume because the bios doesn't technically support it yet (X870E Apex), external clock gen does work though, and I got mine to around 5.9GHz with it.",pcmasterrace,2026-01-23 10:38:25,9
AMD,o17j7au,"Isn't that because of motherboards, or is it confirmed a CPU issue?",pcmasterrace,2026-01-23 09:14:09,28
AMD,o1a4qp6,It was the AM5. Release of 7000 series. I dont remember 5000 series frying.  https://preview.redd.it/wxeko72k45fg1.jpeg?width=576&format=pjpg&auto=webp&s=379f6fc9eff44e7d10d6daced45b0b8f520de0df  Dark times,pcmasterrace,2026-01-23 18:15:38,25
AMD,o1b7acp,"Certain motherboards from certain manufacturers only. They have not had a great time with the 800 series BIOSes overall imo.  I had an x870 tomahawk that had beta labels for all bios versions for months after release. They eventually just removed the beta entirely, but it feels like little has changed. So many of these boards have memory compatibility problems that go away with a motherboard swap, for instance.",pcmasterrace,2026-01-23 21:14:27,6
AMD,o1b2gyh,Certain motherboards like Gigabyte,pcmasterrace,2026-01-23 20:51:41,2
AMD,o1d0gl4,https://videocardz.com/newz/asus-800-series-boards-are-killing-ryzen-7-9800x3d-chips-five-dead-cpu-reports-in-two-weeks,pcmasterrace,2026-01-24 03:02:59,1
AMD,o17wri1,"the 9950X3D is already out.   But the 9950X3D2 is coming out later...   wonder why they didnt name it the 9975X3D tho, feels a bit more sensible.",pcmasterrace,2026-01-23 11:15:56,17
AMD,o18fosx,I was getting the same score with my 9800x3d. I think we'll need to wait for a BIOS-update.,pcmasterrace,2026-01-23 13:25:21,23
AMD,o19ye1p,tbf those CS2 benchmarks are pretty useless.,pcmasterrace,2026-01-23 17:47:09,3
AMD,o17nzrm,It was certain motherboard software making it run to hot and toasting it. There was a bios update to fix it.,pcmasterrace,2026-01-23 09:59:02,15
AMD,o183ynf,We don't know,pcmasterrace,2026-01-23 12:11:08,4
AMD,o17jk4q,"they still haven't admitted to which is really the culprit but regardless, them cpus getting toast quick",pcmasterrace,2026-01-23 09:17:31,-18
AMD,o1duxzd,Every manufacturer really,pcmasterrace,2026-01-24 06:34:17,2
AMD,o18efbe,Cause the main selling point is the dual 3dvcache in both CCD? 9975X3D suggests its just a 9900x3d with better clock boost,pcmasterrace,2026-01-23 13:18:17,8
AMD,o18o9sl,"Anything above x950 is reserved for Threadripper (Threadripper PRO is effectively +5, 9945WX, 9955WX, etc up to 9995WX)  Also would conflict with the 32 core PRO 9975WX",pcmasterrace,2026-01-23 14:11:03,2
AMD,o1alzt7,"Yeah, sorry typo.",pcmasterrace,2026-01-23 19:34:06,1
AMD,o18fto0,Fair. share an update when itâ€™s post bios update,pcmasterrace,2026-01-23 13:26:07,6
AMD,o19ytbj,"I see you have identical setup to mine, what is your benchmark result? Mine avg 930, p1 320",pcmasterrace,2026-01-23 17:49:05,1
AMD,o17jt7l,"So far, haven't seen any cases with Gigabyte motherboards. Obviously, that's not a guarantee, but it seems like ASRock and Asus have been the main culprits.",pcmasterrace,2026-01-23 09:19:58,14
AMD,o1dvahl,"I've had mine since day of release with no issues, when it started happening 99% of them had Gigabyte and AsRock mobo's. ðŸ¤·â€â™‚ï¸",pcmasterrace,2026-01-24 06:37:11,0
AMD,o18i16x,"thats true, but 75 > 50, so just thought so.   Your statement does make sense tho.   Now that i think of it, an OCed 9900X3D whould be a 9925X3D?",pcmasterrace,2026-01-23 13:38:07,5
AMD,o18orm2,"oh, yeah I forgot about that...   also congrats of being part of the ThinkPad represent.   Im joinig soon as well!",pcmasterrace,2026-01-23 14:13:35,3
AMD,o1dpwjx,"Old cs2 use to perform better back in September ish. Now I get 1208avg and 470p1, varies a bit run to run",pcmasterrace,2026-01-24 05:53:05,1
AMD,o17o1gq,"And especially they're software, too much power, too much heat.",pcmasterrace,2026-01-23 09:59:29,5
AMD,o1h5iwp,"Cases have happened across all mobo manufacturers, the chances of a CPU dying is still extremely low compared to the units out there but they have happened across all manufacturers and with a huge variety of settings/undervolts.",pcmasterrace,2026-01-24 19:02:13,1
AMD,o17jxa2,yes so far have only seen Asrock and Asus as well.,pcmasterrace,2026-01-23 09:21:03,-1
AMD,o17oqdq,"Its honestly an AMD problem.  Asrock/Asus just has more aggressive default overclocks in the bios when you turn it on compared to other companies, however all of their overclocks are within the spec sheet of AMD.  Its AMD's bad batch vs an aggressive overclock.",pcmasterrace,2026-01-23 10:05:51,-3
AMD,o1dzjdw,"Same here, have had it since release. Msi and Gigabyte have had the least amount of failures though, not sure where you are getting 99% other than Asrock and asus.  While not everything, i havent seen more than a handful (literally 4) gigabyte failures. Where are you getting your information?  Tried to post the megathread with that stats, automod didnt like it, and while not everything, gigabyte still has the least",pcmasterrace,2026-01-24 07:13:47,3
AMD,o19lm68,what about 9952X3D? or is it too convoluted/feels too close to 9950X3D?,pcmasterrace,2026-01-23 16:48:09,1
AMD,o18p33e,"Yeah it's kinda odd lol, but again HEDT is kinda dead. Wish they could follow something closer to the Epycs in naming convention  ThinkPads are great machines. They'll last you a long time.",pcmasterrace,2026-01-23 14:15:11,1
AMD,o18ayl1,"Nothing to do with OC. It is all about voltages and Asrock and ASUS (but especially ASUS) pump around unnecessary high voltages and some more sensitive chips might not enjoy it and die. But as i keep saying, the incidence of dying chips is pretty low. I built over 100 systems with 7800X3D and 9800XD combined and i am yet to receive a return due to dead CPU.",pcmasterrace,2026-01-23 12:57:26,1
AMD,o19quvz,... What about 9955X3D?,pcmasterrace,2026-01-23 17:11:59,1
AMD,o18qmnl,higher voltage gives more headroom for a stable oc.  Its overclock.,pcmasterrace,2026-01-23 14:23:03,1
AMD,o1d5ify,"I just hope AMD releases a 9900X3D, but with 3D V-cache in both CCDs",pcmasterrace,2026-01-24 03:33:47,1
AMD,o19fxpu,"But you need to apply the OC manually, the board does not automatically OC the CPU. The increased voltage should not be applied from the factory.",pcmasterrace,2026-01-23 16:22:44,1
AMD,o0rawu6,What next gen path tracing does that game use? ðŸ˜…   And why is it in Lego game? ðŸ˜…,pcmasterrace,2026-01-20 23:17:33,342
AMD,o0rex5g,">Next year you prolly gonna need a 4080 to play a remaster or snake or some like that  But look at it this way: soon, your GPU will no longer appear in the system requirements!",pcmasterrace,2026-01-20 23:39:26,215
AMD,o0rbige,"At first I too wondered why a Lego Batman game of all things would have such insane system requirements. But apparently, it's UE5, so there's the answer to that.",pcmasterrace,2026-01-20 23:20:47,160
AMD,o0reta1,\*laughs in EVGA 1080\*,pcmasterrace,2026-01-20 23:38:50,61
AMD,o0rh9dg,Ive got a 6800xt. Cutting it close...,pcmasterrace,2026-01-20 23:52:17,20
AMD,o0rf8qi,![gif](giphy|GJVpbMjfT2Ftm),pcmasterrace,2026-01-20 23:41:13,12
AMD,o0rdtij,Nah i've got the super i'm still good,pcmasterrace,2026-01-20 23:33:22,16
AMD,o0tbbnq,We are truly cooked once the 4090 starts appearing in the system requirements,pcmasterrace,2026-01-21 06:48:09,7
AMD,o0rfwiu,Me who's gpu isn't as powerful as the one in the minimum requirements,pcmasterrace,2026-01-20 23:44:51,11
AMD,o0rg4q6,I was surprised to see my 7700XT in the recommended requirements for Indiana Jones last year.,pcmasterrace,2026-01-20 23:46:06,6
AMD,o0rhtnu,You ought to have seen the uproar about the massive requirements of Oblivion in 2006. Folk screaming from the rooftops how lazy developers stopped optimizing games in the 1990s with PCs being so powerful since.  Nothing changes.,pcmasterrace,2026-01-20 23:55:21,12
AMD,o0rtp4r,I blame Ray tracing. My 1080 TI holds up incredibly well at 1080 P without Ray tracing.,pcmasterrace,2026-01-21 01:00:21,3
AMD,o0tf3tm,well...   https://preview.redd.it/k532p4szlneg1.png?width=331&format=png&auto=webp&s=0e1474a83e9ae4316d7c6e3b59c8915cb04f98b1,pcmasterrace,2026-01-21 07:21:28,4
AMD,o0sy5tv,UE5 SLOP,pcmasterrace,2026-01-21 05:04:39,3
AMD,o0sr8uw,Me whose just glad my CPU is still technically above minimum requirements:  ![gif](giphy|s9Y0czwWdTtB7U6d5I),pcmasterrace,2026-01-21 04:17:29,2
AMD,o0syecu,The 3080 isnâ€™t even that old especially with the memory shortage.,pcmasterrace,2026-01-21 05:06:19,2
AMD,o0t6the,"me, a 1060 owner, reading this post   ![gif](giphy|11rxTjOcUlCScw)",pcmasterrace,2026-01-21 06:10:22,2
AMD,o0rjx6y,"So because the game has requirements like this it means it isn't optimised, you know you can't use the same stuff forever",pcmasterrace,2026-01-21 00:06:43,4
AMD,o0s8mjt,I never pay attention to system requirements.  And at this point those might have been written up by AI lol.,pcmasterrace,2026-01-21 02:25:33,3
AMD,o0spxd8,"Just repasted my cpu and rebuilt my whole PC for maintenance and cleaning the other day  Ryzen 2600x Sapphire nitro+ 5700xt SE  Running borderlands 3 at 1440 ~100 fps  I seriously don't understand what's up with these crazy ""requirements"" nowadays",pcmasterrace,2026-01-21 04:08:52,2
AMD,o0revc2,Hopefully my 5600X can hold out for awhile longer. Wish I could find a 5700X3D though.,pcmasterrace,2026-01-20 23:39:09,1
AMD,o0rg7r1,Bro my old 1070 appeared so far back lmao,pcmasterrace,2026-01-20 23:46:34,1
AMD,o0rhz0b,"It happens to us all, just at different times.  These are some times tho.",pcmasterrace,2026-01-20 23:56:08,1
AMD,o0rjqeg,My arc a770 16gb is minimum?!? I dont have $1500 for a new card dammit.,pcmasterrace,2026-01-21 00:05:42,1
AMD,o0rk3p6,looks like the only way to play this game is gonna be on consoles..,pcmasterrace,2026-01-21 00:07:43,1
AMD,o0rluht,You know system requirement lists aren't exhaustive and are often formed from the hardware the developer had on hand to test and are guidelines.,pcmasterrace,2026-01-21 00:17:11,1
AMD,o0rm8e8,"all this for a lego game, i hope it looks better than GTA 6 then",pcmasterrace,2026-01-21 00:19:17,1
AMD,o0rn5y1,What the actual fuck,pcmasterrace,2026-01-21 00:24:18,1
AMD,o0rnznk,"That's the recommended tho. Not the minimum. People did this with JWE3 as well, freaking out over the recommended specs and not looking at min at all.",pcmasterrace,2026-01-21 00:28:48,1
AMD,o0ropa4,Iâ€™m finally upgrading from a 2060. Thing was a workhorse. I was able to keep 60-70 fps in most games at 1440 with some custom graphics lowered on almost all modern games.,pcmasterrace,2026-01-21 00:32:39,1
AMD,o0rpnjx,They have to render each brick for it. I all seriousness they made Lego batman on the PSP what are they doing to increase the spec requirements that much,pcmasterrace,2026-01-21 00:37:49,1
AMD,o0rqrsj,I saw 9600 under CPU and thought *what* game needs THAT as a minimum?? then realized it's a *K*,pcmasterrace,2026-01-21 00:43:55,1
AMD,o0rrlet,GeForce now is an optionâ€¦,pcmasterrace,2026-01-21 00:48:28,1
AMD,o0rsn0h,"You'll be fine, just lower a couple settings and use framegen to round the edges.",pcmasterrace,2026-01-21 00:54:22,1
AMD,o0rv95j,Every game company is unintentionally or intentionally helping hardware companies with their unoptimized game... You want to play stronghold remastered on your 2070ti? Too bad buddy think again when you have a 4070ti! Basically just a monopoly over our wallets,pcmasterrace,2026-01-21 01:09:14,1
AMD,o0rvovr,"Kept seeing 1660ti, knew it was my time.",pcmasterrace,2026-01-21 01:11:46,1
AMD,o0ryi0t,my 2070 is still holding strong. In it for the long run,pcmasterrace,2026-01-21 01:27:47,1
AMD,o0s0slr,At least your GPU is the recommended system requirements instead of the minimum requirements so it could run somewhat ok for you. *cries in GTX 1080*,pcmasterrace,2026-01-21 01:41:01,1
AMD,o0s1nve,"I'm one gen behind those, but I mostly just play Xenotilt and Demons Tilt, and other pinball games (and few shmups).  Edit- maybe not a full gen. Rocking a 5500 Radeon Xt 8gb oc. Pretty happy with it for what I play though.",pcmasterrace,2026-01-21 01:46:01,1
AMD,o0s4f1s,This was just a placeholder btw confirmed by the devs via discord,pcmasterrace,2026-01-21 02:01:41,1
AMD,o0s7d1e,7700x and 32 GB of DDR5 ðŸ’”,pcmasterrace,2026-01-21 02:18:27,1
AMD,o0sjhn0,I wonder how long will my 4080 super appear on the list,pcmasterrace,2026-01-21 03:28:52,1
AMD,o0slvlb,are you gonna play gta 6 RT or what,pcmasterrace,2026-01-21 03:43:24,1
AMD,o0sn6yn,I read somewhere that a developer or rep said specs are more of a placeholder for steam but most games come in hot at launch so who knows,pcmasterrace,2026-01-21 03:51:28,1
AMD,o0sogbx,Vote with your wallet,pcmasterrace,2026-01-21 03:59:22,1
AMD,o0stale,I played for years with systems that weren't even in the minimum requirements section.,pcmasterrace,2026-01-21 04:31:14,1
AMD,o0sxm4b,"Why are you looking at recommended specs and use the word ""need"" to upgrade? Games are shit but 3080 is still insanely good today",pcmasterrace,2026-01-21 05:00:48,1
AMD,o0sxoql,"I know I'm always cooked, because my GPU is a laptop 1050.",pcmasterrace,2026-01-21 05:01:18,1
AMD,o0sz2qp,Companies are going to have to scale those requirements down or they're going to be losing sales.,pcmasterrace,2026-01-21 05:11:12,1
AMD,o0t2pb1,"People who play Lego Whatever, you exist! I've found you! My life is complete!",pcmasterrace,2026-01-21 05:37:52,1
AMD,o0t2pwe,"I just bought an RX 9060XT for future proofing after Doom the dark ages required more than my RTX 2060, at this pace my new GPU will appear in system requirements by next year.",pcmasterrace,2026-01-21 05:37:59,1
AMD,o0t47ba,People say ram is crazy expensive but used ddr4 ram is still cheap af. You can get 32gb under 100â‚¬.   People always just want the newest and hottest shit.,pcmasterrace,2026-01-21 05:49:22,1
AMD,o0t55oc,Why optimize when you can just vibe-code?,pcmasterrace,2026-01-21 05:56:49,1
AMD,o0t5s77,"I am hoping that one possible positive (?) from the RAM shortage is that developers will release games like this, less people buy due to requirements, forcing developers to better optimise their games.  Itâ€™s unlikely but fingers are crossedâ€¦â€¦",pcmasterrace,2026-01-21 06:01:53,1
AMD,o0t6wiw,*stares at my 3090*,pcmasterrace,2026-01-21 06:11:03,1
AMD,o0t7wsb,PSX games worked with 2MB of RAM. PS2 has 32MB of RAM.  Game devs have gotten too lazy.,pcmasterrace,2026-01-21 06:19:20,1
AMD,o0t8res,You can usually run games at quite a bit under minimum requirements. It's not something i'd worry about.,pcmasterrace,2026-01-21 06:26:20,1
AMD,o0tddmv,"I will do you one better.  In the future, games will require that your GPU supports frame generation.",pcmasterrace,2026-01-21 07:06:00,1
AMD,o0tekfq,This requirement is most likely with raytracing turned on. Turning it on results in these high requirements for even the most basic looking games.,pcmasterrace,2026-01-21 07:16:34,1
AMD,o0tew73,canâ€˜t even blame people with not enough spare money switching to console at this point.,pcmasterrace,2026-01-21 07:19:31,1
AMD,o0tf8nd,Not really. My old GPU wasn't even stated in minimum system requirements. Still I could play new games for like another 7 years.,pcmasterrace,2026-01-21 07:22:42,1
AMD,o0tk8gn,"a 2070 is the requirement.. 3080 is recommended. it seems pretty reasonable to me, given the most recent releases.",pcmasterrace,2026-01-21 08:08:49,1
AMD,o0toznx,Y'all remember when lego games were actually optimized?,pcmasterrace,2026-01-21 08:54:03,1
AMD,o0tqfs0,Games need to chill out.,pcmasterrace,2026-01-21 09:08:02,1
AMD,o0ttlk8,"I replaced my 5700XT recently, glad I did. Let's hope the replacement lasts.",pcmasterrace,2026-01-21 09:38:53,1
AMD,o0tynh0,Imagine not even making minimum requirement list,pcmasterrace,2026-01-21 10:26:04,1
AMD,o0ugx1w,"Im not saying this is good, just stating the potential reasons. The game uses UE5, which is already heavy out the gate, and those minimum specs line up with the PS5 and XSX.   So, given that the XSS is a thing lower than minimum, it will be viable for 30 fps at 1080p. I'd imagine.   Either the consoles are 1080p@60, and they're just using that as PC min spec, or they just chucked the equivalents as min spec even if they aren't the minimum to run it.",pcmasterrace,2026-01-21 12:48:22,1
AMD,o0uo6pv,Nothing is as bad as the AI sys requirements for indiana jones or whatever the fuck game that was. ðŸ¤£,pcmasterrace,2026-01-21 13:32:18,1
AMD,o0upc4s,"The game dev was definitely like ""this runs on my system buttery smooth, it will work everywhere else fine""  Meanwhile the Dev's pc is a workstation with four 4090 stack with liquid cooling",pcmasterrace,2026-01-21 13:38:47,1
AMD,o0uzx0z,I remember when my GPU fell below the system requirements..... (GTX 770). Felt bad....,pcmasterrace,2026-01-21 14:34:54,1
AMD,o0v0jt9,My PC doesn't even meet recommended specs... FOR A LEGO GAME,pcmasterrace,2026-01-21 14:38:08,1
AMD,o0vbtwh,"Even since 2020, am looking for a title that is worth playing, let alone upgrading....",pcmasterrace,2026-01-21 15:32:32,1
AMD,o0ves7l,Are you *complaining that your GPU is the recommended GPU* for this game?,pcmasterrace,2026-01-21 15:46:07,1
AMD,o0vlh2q,Wow why does it need a 2070? Would a 1080 run it?,pcmasterrace,2026-01-21 16:16:03,1
AMD,o0vpp1j,Crypto Miner with some Lego gameplayâ„¢,pcmasterrace,2026-01-21 16:35:05,1
AMD,o0w03m6,"I know the A770 isnt exactly a beast, but seeing a 16G VRAM card in the minimum specs is fucking wild.",pcmasterrace,2026-01-21 17:21:51,1
AMD,o0wqvbs,"I never see my specs recommended now im worried that I bought bad parts =( why wont any game recommend my system specs?  /s  Fr though, the recommended specs never make sense. They never give a target or configuration for the minimum or recommended, so it ends up being meaningless. I always thought recommended specs should be controlled like nutritional facts on food are.  For example:  Cyberpunk 2077  Minimum (720p, 30fps, lowest settings, Fsr 3.1 performance) GTX 1030 Xxx Xxxxx Xxxx  Recommended (4k, 240fps, maximum settings, DLSS 4.5 performance, FG 2x, pathtracing enabled) RTX 5080 Xxx Xxx Xxxx",pcmasterrace,2026-01-21 19:19:36,1
AMD,o0x2804,"I mean itâ€™s game dependent.  Iâ€™ve seen my 4090 as recommendedâ€¦ But it was path traced Indiana Jones, so fair enough.  If I saw my 4090 being necessary for a Borderlands game THEN I would be worriedâ€¦ ohâ€¦ waitâ€¦.",pcmasterrace,2026-01-21 20:11:15,1
AMD,o0x8dkj,Bro am still on the i5 12400f,pcmasterrace,2026-01-21 20:39:15,1
AMD,o0ylbvz,"Yeah, my 3080Ti is starting to creep into recommended hardware.",pcmasterrace,2026-01-22 00:42:05,1
AMD,o0yom39,Lego builders journey or something like that with RT on looks really freaking good for a kids game.,pcmasterrace,2026-01-22 01:00:09,1
AMD,o1027hp,better than mine lmao,pcmasterrace,2026-01-22 06:09:16,1
AMD,o10z1d4,9600K + 2070S + 16GB DDR4 super was my old PCâ€¦,pcmasterrace,2026-01-22 11:03:48,1
AMD,o12j7gc,I'll be worried when my GPU starts showing up in the requirements :D,pcmasterrace,2026-01-22 16:21:54,1
AMD,o16x2qc,Glad I finally upgraded my CPU after seeing 9600K in that ðŸ˜¬,pcmasterrace,2026-01-23 06:01:43,1
AMD,o1c9h31,2070 as a minimum is ridiculous,pcmasterrace,2026-01-24 00:27:49,1
AMD,o1jlogs,Ya...that happened about 2 weeks ago with my 590px &6700xt....sigh,pcmasterrace,2026-01-25 02:20:04,1
AMD,o0tpb26,shit take,pcmasterrace,2026-01-21 08:57:04,1
AMD,o0rds1l,"It's a nearly 8-year-old GPU. The last Lego Batman game (2014) also had 8-year-old GPUs as its minimum specs (The NVIDIA 7600GS and ATI Radeon 1950, both released in 2006).   The recommended specs are slightly more generous than the last Lego Batman game: recommended was 4 to 5-year-old GPU's (GTX 480 and Radeon 5850), while the 3080 is nearly 6 years old.   You being uncomfortable with the passage of time has nothing to do with optimization. It's been more than a decade since the last game. Graphics improve. Not just the actual effects, but also the amount of stuff that can be rendered at once in a map.",pcmasterrace,2026-01-20 23:33:09,-16
AMD,o0sil8m,another unoptimized slop,pcmasterrace,2026-01-21 03:23:26,0
AMD,o0sckv9,"I have the explanation.  You know how everyone was begging for a return of AA games, right? Everyone was fed up of only AAAs with inflated budgets and indies? People wanted mid-budget ""AA"" games more often?   That's where Unreal Engine 5 comes in. The Monkey's Paw curls a finger.   It allows teams to make games much more quickly, and gives lower budget games the opportunity to have a much greater scope both in terms of graphics and content (e.g. Clair Obscur Expedition 33). Also just because it's Lego doesn't mean it's not going to look gorgeous. If this has Lumen, this is going to look better than Arkham Knight in terms of lighting.   But UE5 is notoriously tougher to run. That's why they don't want to underbelly how intensive the game is. Odds are, it should run fine on a lower end card. But they don't want to be complained at if it doesn't.",pcmasterrace,2026-01-21 02:48:05,-1
AMD,o0rq838,NOOOOOOOO,pcmasterrace,2026-01-21 00:40:55,0
AMD,o0s7o0t,Minimum 3080 wow is it crysis,pcmasterrace,2026-01-21 02:20:10,0
AMD,o0rf9qa,The 3080 is 6 years old and the equivalent of a 5060 Ti which is a current gen mid range GPU. For such a card to appear in the recommended requirements is very reasonable if you ask me,pcmasterrace,2026-01-20 23:41:22,-18
AMD,o0rmh3i,With requirements like that I better feel like I'm in Legoland watching a moving diorama,pcmasterrace,2026-01-21 00:20:35,79
AMD,o0rj1b7,Cant remember if it was in the Skywalker Saga Lego game but that shit looked amazing for it being Lego.,pcmasterrace,2026-01-21 00:01:55,24
AMD,o0t4iud,cant be rt or pt judging by how they put 3080 and 6800xt which are no where near comparable in that.  the game is just that fucking horridly made it seems...,pcmasterrace,2026-01-21 05:51:49,5
AMD,o0s0abf,But it will be *under* the system requirementsâ€¦,pcmasterrace,2026-01-21 01:38:07,11
AMD,o0rj3wk,His won't but then mine will :(,pcmasterrace,2026-01-21 00:02:18,22
AMD,o0xpo4u,And then it'll hit minimum sys req and that's the end of it.,pcmasterrace,2026-01-21 21:58:01,1
AMD,o0rjhil,UE5 does not like my 9070 I can tell you that much,pcmasterrace,2026-01-21 00:04:22,18
AMD,o0rhme6,I guess the question is if the recommended spec is for 4k ultra at 120fps. Then that makes more sense.,pcmasterrace,2026-01-20 23:54:15,21
AMD,o0sbe5b,Yeah Unoptimized Engine 5 is the worst.,pcmasterrace,2026-01-21 02:41:22,8
AMD,o0tkcia,UE5 will bring a 5080 to its knees if it does on of those close ups of a face. Â What the fuck is up with that engine,pcmasterrace,2026-01-21 08:09:53,2
AMD,o0t1rh5,Why does a Lego game need ue5?,pcmasterrace,2026-01-21 05:30:51,0
AMD,o0rukrc,"As long as RT isn't required you're golden...even so, the GOAT lives on.",pcmasterrace,2026-01-21 01:05:24,11
AMD,o0rn6p6,I still have a 1060. Basically playing PowerPoint slides at this point,pcmasterrace,2026-01-21 00:24:25,7
AMD,o0s90t8,Apparently Iâ€™m out. Still rocking the regular RX5700 (and Ryzen 3600),pcmasterrace,2026-01-21 02:27:47,6
AMD,o0rr3ds,"6700XT here, my heart sank when I saw the 700XT part",pcmasterrace,2026-01-21 00:45:41,3
AMD,o0t7mtt,Yeah I got the same card. It really isn't cutting it for the latest single player games I've noticed. Such a shame. It would've gotten more longevity if they back ported FSR4,pcmasterrace,2026-01-21 06:17:02,1
AMD,o0ruof0,I9 7920x still rocks though.,pcmasterrace,2026-01-21 01:05:58,2
AMD,o0z58oh,*Cries in 1060*,pcmasterrace,2026-01-22 02:34:25,1
AMD,o0rprxb,It's time brother. I went from a 2060 to a used 3080 to a 5070 in the past 5 or so months. Massive difference.,pcmasterrace,2026-01-21 00:38:29,1
AMD,o0rvor4,"Indiana Jones was two years ago  We're all gettin old. But honestly Indiana Jones looks fuckin incredible and it uses RT for everything so it makes sense that the reqs would be kinda high. New DOOM game is also like that, and it runs crazy smooth.",pcmasterrace,2026-01-21 01:11:45,1
AMD,o0rvycg,Games stop being optimized when they can't run on the PC I built 5 years ago.,pcmasterrace,2026-01-21 01:13:16,12
AMD,o0rn8s1,"Or when a graphics card started to appear in required specs.   Or when EverQuest released an expansion with a graphics upgrade, that recommended 256 RAM. Which found a lovely Windows bug, where versions older than XP could get lost in the RAM and freeze. Forcing the user base to need to upgrade windows",pcmasterrace,2026-01-21 00:24:43,8
AMD,o0rojr0,"To be fair Oblivion had a bunch of seriously stupid things going on, like massive texture files for random little prop rocks and the like. That sort of thing is very sloppy and it's not the kind of optimisation that costs anything to do. Bethesda's reputation isn't unearned.",pcmasterrace,2026-01-21 00:31:49,4
AMD,o0rq2kd,For a bit of context on this. I had a 2 year old ATI X800 XT that couldn't run Oblivion properly. Todays equivalent would be a 4090 being only able to run a game on potato settings.,pcmasterrace,2026-01-21 00:40:05,4
AMD,o0s3u6g,"Yeah, pretty sure most games from 2006 would struggle with a GPU from 2000. The 3080 is over 5 years old now. I just replaced my 3080 a few weeks ago with a 9070 XT.",pcmasterrace,2026-01-21 01:58:24,3
AMD,o0s7ems,"I remember that, and bought a 7900gt for $330 to max it out, show me a top tear graphics card for the current equivalent price of $500 that can play the current oblivion remaster at max graphics.",pcmasterrace,2026-01-21 02:18:42,1
AMD,o0sbcxq,This is a lego game tho that doesn't look any better than previous lego games but requires 4 times the hardware power to do so,pcmasterrace,2026-01-21 02:41:10,-1
AMD,o0rq28o,"I found one for 220 a week ago, but went for am5 instead. Should've bought it just to flip ðŸ’€",pcmasterrace,2026-01-21 00:40:02,1
AMD,o0rkqsx,2070 is listed as minimum. 3080 is listed as recommended.,pcmasterrace,2026-01-21 00:11:11,3
AMD,o0t4hby,Bud. I got my gf 64 gb 16x4 for 100 cad.  32 gb is 200 cad rn it's sad  EDIT that 64 gb was last summer,pcmasterrace,2026-01-21 05:51:29,1
AMD,o0rgxfz,8 year old GPUs now are much closer to modern GPUs than 8 year old GPUs were 8 years ago.,pcmasterrace,2026-01-20 23:50:30,13
AMD,o0rf68q,"Yeah no, this is a bad optimisation, a Lego game should run on a toaster. This is UE5 doing what it does best",pcmasterrace,2026-01-20 23:40:50,14
AMD,o0rf7db,"The problem is that how many factors of 2x power increase GPUs launched in that time, and also look at the CPU minimum spec which is really high compared to the old one. The older one could run on really old CPUs. The new one needs something released relatively recently. Not that this is an unfair ask but it's still high as a minimum.",pcmasterrace,2026-01-20 23:41:01,2
AMD,o0rvbtb,"This a terrible comparison in so many ways. Much akin to people comparing gpu/cpu prices in the 00s to today.   Look, I don't expect the gtx 1080 to be running this game at ultra, but I kinda thought stuff like it would at least reach minimum requirements.Â    RTX 3080 recommended is crazy. Even the 5060 ti and rx 9060 XT are slightly slower.",pcmasterrace,2026-01-21 01:09:39,2
AMD,o0szzlf,A nearly 8 year old gpu that is more powerful than the 5060ti 16gb which isnâ€™t even a year oldâ€¦..,pcmasterrace,2026-01-21 05:17:48,1
AMD,o0rmhkx,"It has everything to do with optimization though, since lego games don't do bleeding edge technology graphics, besides it's 6yo high-end GPU. It's still better than 5060ti which is new gen",pcmasterrace,2026-01-21 00:20:39,0
AMD,o0rhzaj,"Itâ€™s closer to the 5070 lol, especially the 12gb version which is only 10% or so slower than the 5070, much faster than a 5060ti.",pcmasterrace,2026-01-20 23:56:10,6
AMD,o0rjv7d,Same with Lego Horizon on the PS5.,pcmasterrace,2026-01-21 00:06:25,11
AMD,o0ve6tg,"Dw, mine will too ðŸ˜†",pcmasterrace,2026-01-21 15:43:26,2
AMD,o0rlybg,Ue5 is so bad on amd cards.,pcmasterrace,2026-01-21 00:17:46,9
AMD,o0t9v1z,Doing fine for me,pcmasterrace,2026-01-21 06:35:38,1
AMD,o0rmqho,"it still doesnt make sense that way in my opinion, its a frigging lego game",pcmasterrace,2026-01-21 00:21:59,14
AMD,o0tg5xi,"> We truly need to call bad craftmanship out more.  People really want 'Indy' or 'AA' games while demanding a degree of technical excellence that even most AAA productions can't afford...  E33 made an excellent compromise to deliver on its aesthetic.  You can't do this on the same budget with a custom or older engine, or at the very least take a huge risk that it just won't work out.  It takes much bigger studios to maintainwell optimised engines with modern feature sets that are suitable for general game development, like id with its id tech engine or Ubisoft's Anvil engine.",pcmasterrace,2026-01-21 07:31:12,5
AMD,o0teih2,"Comparing a 2015 game to a 2025 game is kind of weird man idk.     Clair obscure runs decently on modern hardware, I've seen people with budget rtx4050 laptops running it.",pcmasterrace,2026-01-21 07:16:05,4
AMD,o0vawer,"In which sense does Clair Obscur look like it should scale down nicely? It features all the standard photorealistic graphics and characters you'd expect from a high-end 2025 release, with a large amount of natural environments. I would expect it to scale down as well as, say, MGS Delta.",pcmasterrace,2026-01-21 15:28:13,2
AMD,o0w9k9c,RT isn't required in 99.9%+ of available games. I'll survive without the remainder. Those devs are choosing to make their games exclusive to a small portion of the market.,pcmasterrace,2026-01-21 18:03:32,1
AMD,o0ru2fg,I'm here with you brother... 1070 but it's almost the same at this time,pcmasterrace,2026-01-21 01:02:27,4
AMD,o0slr7z,My 1050 is rocks notepad pretty hard.,pcmasterrace,2026-01-21 03:42:40,3
AMD,o0wfxvt,"I'm on a 6700xt and 3600x, I fear I'm not too far behind",pcmasterrace,2026-01-21 18:31:27,1
AMD,o0tswee,It would have gotten more longevity if the way developers use UE5 wasn't an abomination.,pcmasterrace,2026-01-21 09:32:04,1
AMD,o0tcclb,"saying Indiana Jones release was ""two years ago"" when it's 1 year and a month is diabolical",pcmasterrace,2026-01-21 06:57:01,6
AMD,o0rzmie,Damn. Was it really two years?,pcmasterrace,2026-01-21 01:34:17,2
AMD,o0rwl19,"My kid's 2070 is *outraged*, *outraged, I tell you* but the kid just drops to medium or uses DLSS and he's happy.  Probably throw a 9060XT at him later this year.",pcmasterrace,2026-01-21 01:16:54,3
AMD,o0tgvci,"I remember buying Command and Conquer Generals only to find that I couldn't run it on my still fairly new PC. It often only took around 2-3 years for hardware to start becoming seriously obsolete.  Generals released in 2003 and required a DirectX 8.1-capable GPU with at least 32 MB VRAM. This ment it needed an Radeon 8500 or GeForce 4 from late 2001/early 2002. Radeon 7000 and GeForce 3 GPUs from 2001 were already outdated.  Those GPUs released at $299 MSRP. Adjusted for inflation, that's almost $550 today. So depending on how you want to compare prices, that's roughly on par with requiring an RTX 4060 or RTX 4070 Super for a game released in 2025.  The fact that people now believe that games requiring 7 year old hardware is a sign of declining optimisation is kind of hilarious.",pcmasterrace,2026-01-21 07:37:45,4
AMD,o0rneua,"Oh heck, don't remind me about Evercrack. I was on Windows 2000 at the time with 512 MB, so didn't see that particular issue!",pcmasterrace,2026-01-21 00:25:39,2
AMD,o0rqaj0,I've got the original big box PC version of Everquest pre expansion and the list of recommended GPUs on the back is absolutely wild. Shit you've never heard of. Diamonds and Orchids and Matroxx this that and whatever.,pcmasterrace,2026-01-21 00:41:17,1
AMD,o0rw6ls,"dudes who be yappin about ""games aren't optimized anymore!"" don't know shit about ""unoptimized"" games. GPUs from 3+ years ago are still completely viable today but back then a GPU that's a year old would barely be able to play anything new",pcmasterrace,2026-01-21 01:14:36,5
AMD,o0s4ni4,"I was running a Radeon 9700 when Oblivion dropped, bought it in late 2002. It was the king of the hill, I had it overclocked slightly over 9700 Pro spec.  Oblivion *killed it*. Even on ""Medium"". Even at 1024x768. FPS was below 20 at all times and dropped into single figures when refraction shaders were used.  That'd be like an AAA game today running badly on an RTX 4080 or 4090 from 2022.",pcmasterrace,2026-01-21 02:03:01,3
AMD,o0szufv,Gpu improvements have heavily stagnated. The 3080 is still stronger than the 5060ti 16gb and even trails the 4070 from last gen.,pcmasterrace,2026-01-21 05:16:46,1
AMD,o0t77ea,you couldnt play games maxed out on $500 graphics cards even in 2005 without framerate dips.  as for now. a $500 5060ti 16gb running dlss 4.5 at balanced 1440p looks good and hits 60+fps.,pcmasterrace,2026-01-21 06:13:30,2
AMD,o0ssam5,"It's a LEGO game now set in a larger open world and the visuals absolutely look more detailed than the last Star Wars LEGO game, which was set in smaller linear environments. Open world games just take more to render then closed environments do.   Like, I dunno what else to tell you. The 3080 is almost six years old. It's aging and it's perfectly natural that older GPUs move down into the ""recommended"" spectrum.",pcmasterrace,2026-01-21 04:24:29,-2
AMD,o0rkutf,Thank u i was panicking. Still not great,pcmasterrace,2026-01-21 00:11:48,1
AMD,o0rtkwr,"tbh for real, a lego game shouldn't be demanding this much lol. ue5's wid for this",pcmasterrace,2026-01-21 00:59:42,0
AMD,o0rj5qs,What would you say is closest to a 3080ti?,pcmasterrace,2026-01-21 00:02:36,1
AMD,o0rkz30,The 4070 super is about the same,pcmasterrace,2026-01-21 00:12:26,0
AMD,o0rlsmt,"This isn't something subjective, it's verifiable. According to TPU, the 5060 Ti is only 11% slower while the 5070 is 15% faster.   The requirements talk about the 10gb variant so that's the one I'm considering.   Iceberg Tech also made a video comparing the 3080 12gb and the 5060 Ti and they seemed to be close, so the 10gb variant would be closer.   I'm sure there are more benchmarks you can lookup to verify the performance of the cards and reevaluate your understanding based on the results",pcmasterrace,2026-01-21 00:16:55,-2
AMD,o0rvflj,thought you were talkin about the Forza Horizon 4 expansion for a sec,pcmasterrace,2026-01-21 01:10:16,9
AMD,o0rogr6,Playing Horizon with my son right now and it looks amazing.,pcmasterrace,2026-01-21 00:31:22,4
AMD,o0sqi9r,"Me, personally, have a much better time with ue5 games than I did on the 4070.  Less stuttery",pcmasterrace,2026-01-21 04:12:40,5
AMD,o0txbkv,UE5 is bad on all cards,pcmasterrace,2026-01-21 10:13:50,2
AMD,o0t1i2k,Arena Breakout Infinite is UE5. It runs in 3440x1440 at over 200fps on my 7900XTX. It's buttery smooth so to say.   UE5 runs fine on AMD cards.,pcmasterrace,2026-01-21 05:28:53,3
AMD,o0wpicy,"Seriously. When folks do these comparisons of older games vs. newer games, it's always games that at the time they released were technical achievements from some of the biggest studios pushing the boundaries of what was possible at the time.   That often had not only outsized budgets, but development timelines relative to what a smaller team can afford.  It's like uh, yeah, the game looks good because they put an immense amount of time and labor into bespoke assets and toolkits to achieve a specific aesthetic within the boundaries of that period's hardware. (and oftentimes not even within those boundaries, but pushing beyond them like Arkham Knight)",pcmasterrace,2026-01-21 19:13:30,3
AMD,o0wzqwb,Truth. I had no issues when I had a GTX 1080 last year.,pcmasterrace,2026-01-21 19:59:52,2
AMD,o0wu8za,The fact you can play the majority of modern games on 10 year old hardware seems to say the opposite of the point you're making.,pcmasterrace,2026-01-21 19:34:52,1
AMD,o0t2fe2,Yeah but you need RTX and more CUDA cores to get all the best AI copilot features,pcmasterrace,2026-01-21 05:35:50,1
AMD,o0uin2h,Big part of it as well. Everything is UE5 slop nowadays.,pcmasterrace,2026-01-21 12:59:15,1
AMD,o0sbdzb,I know the feel. I have a xeon w 2145. It's basically the i7 7820x. Stick rocks and better than my previous i7 3770.,pcmasterrace,2026-01-21 02:41:20,1
AMD,o0rzrfz,"Unless it's for medical reasons, 3 years unemployment is wild. Like brotherman, mcdonalds and Walmart are always hiring. A job is a job. Even a demeaning one. I work at a grocery store.",pcmasterrace,2026-01-21 01:35:05,3
AMD,o0s6do3,"Just barely. It released in December 2024, so yes but no.",pcmasterrace,2026-01-21 02:12:56,6
AMD,o0u8cdy,"I played it on a 6800xt 1440p no upscaling and it ran great, so you missed out.",pcmasterrace,2026-01-21 11:47:59,3
AMD,o0uvtei,"RT capable cards have existed for eight years, if your PC rig doesn't have one it might be time for an upgrade ngl",pcmasterrace,2026-01-21 14:13:34,2
AMD,o0t6xep,the 9060xt is pretty similar speed as it lacks dlss and fsr3 looks bad (unless the game supports fsr4). 9070 or 5070 are upgrades though,pcmasterrace,2026-01-21 06:11:16,0
AMD,o0u93qb,"And before Steam and indie games really took off we had a period of publishers dismissing all PC gamers as pirates. That era was full of just *atrocious* ports from consoles, if the publishers even bothered at all.Â    It was pretty much just Valve and Blizzard holding the PC flag for a while, and Bioware was at least good about adapting their UIs for PC (tactical mode in DAO, for example).",pcmasterrace,2026-01-21 11:53:38,2
AMD,o0roykh,"I had ME at the time, and after adding ram to my system, I was soft locking about every 30-45 min.",pcmasterrace,2026-01-21 00:34:03,1
AMD,o0tgjsl,"Not to mention that a lot of the stutters of the Oblivion remaster are not because of UE5, but because the original Oblivion code still stutters on hardware from over 15 years later.  Asset streaming in open worlds is a hard problem. We live in a blessed time to treat the occasional shader compilation stutter in UE5 games a as a major issue.",pcmasterrace,2026-01-21 07:34:47,3
AMD,o0s71he,I had a vanilla GeForce 6800 which I was able to unmask some extra pixel pipelines for extra performance. Not sure if I ever played Oblivion on that though because I played it a couple years after release. I might've played it with my next build with the GeForce 9800 gt.,pcmasterrace,2026-01-21 02:16:40,2
AMD,o0szko3,It doesnâ€™t matter how old the 3080 is. Itâ€™s not a weak gpu by any means. Itâ€™s like 10% stronger than the 5060ti 16gb and only slightly trails the 4070 for comparison.,pcmasterrace,2026-01-21 05:14:47,2
AMD,o0rtra0,Eh I blame developers just as much,pcmasterrace,2026-01-21 01:00:41,0
AMD,o0rmkqr,"12gb 3080 and 3080ti are essentially the same performance, so the closest equivalent would be a base 4070 and 4070 super or 5070",pcmasterrace,2026-01-21 00:21:07,2
AMD,o0rn9q5,"Yeah there are a lot more advantages to the 3080(like its near 2x bus), and there are 2 variants of the 5060ti... don't know ""iceberg tech""... so wont speak to that.  When looking at either. We see that when in RT, the 3080 just smacks the crap out of the 5060ti 8Gb, and makes the 5060ti 16GB do a very good impression of the french. Only winning when it can leverage the 16GB Vram buffer, which is predominantly 4k, which is not a great res for that GPU anyway.",pcmasterrace,2026-01-21 00:24:52,2
AMD,o0rn503,"Tech power up isnâ€™t accurate down to the perctanges like that, itâ€™s more of a rough idea on where the perf of a card stacks up.   In real world benchmarks and game testing at 1440p/4K the 3080 is much closer to the 5070.   You even said that the 3080ti is closest to the 4070 super, which is correct. The 12gb 3080 is essentially the same thing as a 3080ti in perf, and a 4070 super is around 5070 perf too, so you basically agree with me already.",pcmasterrace,2026-01-21 00:24:09,1
AMD,o0s7zya,haha. Lego Horizon Adventures to fully clarify!,pcmasterrace,2026-01-21 02:22:03,3
AMD,o0sqozz,"Probably able to just brute force it a bit more, I found I had a better time when I went with the x3d chip from my 12th gen i5. The stutters hit less frequently recovered much faster than the gpu upgrade",pcmasterrace,2026-01-21 04:13:50,1
AMD,o0u3u8p,bad developers are bad on all cards* ftfy,pcmasterrace,2026-01-21 11:11:44,1
AMD,o0w0qoc,I mean 7900 XTX is still tied for like 8th best amongst non-commercial cards.  Its kind of a beast.,pcmasterrace,2026-01-21 17:24:41,1
AMD,o0t27ld,"There are definitely examples of ue5 that run fine on amd cards, but there are a lot that donâ€™t.",pcmasterrace,2026-01-21 05:34:13,1
AMD,o0yo7id,Worse than a game at max settings? Well yeah. A game from 2014 at lowest also looked worse than a maxed out 2004 game. Maxed out 1440p half life 2 vs lowest settings 720p shadow of Mordor (which was considered a good looking game on releaseâ€¦not so much later). Half life 2 clears.  Itâ€™s been like that for a very long time. Basically ever since graphics settings.,pcmasterrace,2026-01-22 00:57:53,1
AMD,o0wylh1,I think we're saying the same thing? Only a small number of recent games require RT.,pcmasterrace,2026-01-21 19:54:41,1
AMD,o0ucpgf,"And the physical hardware was so low quality.  GPU didn't even need a 12VHWPR connector to frequently fail within 2-3 years, because capacitors, coolers, PSUs, and the general build quality were so bad. [This thing was a $299 card in 2002](https://www.techpowerup.com/gpu-specs/geforce4-ti-4400.c180), and it's not like high-end cards were built much better. Even on a purely physical level, it's awesome how much better built a [modern equivalent like the 5060 is.](https://www.techpowerup.com/gpu-specs/geforce-rtx-5060.c4219)  I lost 2 GPUs to those dreaded flickering polygons (afaik indicating that some memory cell or data channel had broken) just in the 3-4 or so years I was playing World of Warcraft.  Not to mention the buzzing audio because most cables and speakers broke easily. I had to weigh down the cable of my speakers to make up for a partial cable break constantly, and had buzzy sound every summer when temperatures went up.",pcmasterrace,2026-01-21 12:19:56,2
AMD,o0vedp8,Whoever said it's *weak*? It's now a mid-range GPU. It is *literally the recommended GPU* for this game.,pcmasterrace,2026-01-21 15:44:18,1
AMD,o0rr75m,Thank you for the insight ðŸ¤,pcmasterrace,2026-01-21 00:46:17,1
AMD,o0rt02w,Is HUB a big enough channel for ya? Their review puts the the 5060 Ti 11.5% behind the 3080 12gb at 1440p so within 10% of a 3080 10gb. I would not call that a smacking.   I'm not saying that the 5060 Ti is a good card or that the 3080 10gb is bad. All the comparisons I'm saying indicate that they're reasonably close to each other and that's about it,pcmasterrace,2026-01-21 00:56:25,0
AMD,o0rryte,"The post specifically mentions the 10gb variant of the 3080 though. HUB's 5060 Ti puts it 10% slower than a 4070/3080 12gb at 1440p as well, and the 10gb variant is a few percent slower. Am I missing something?  https://preview.redd.it/pvqh1o7aoleg1.png?width=1763&format=png&auto=webp&s=ed5bda7f6f8891119c5126bc371b1930bca8bf04",pcmasterrace,2026-01-21 00:50:34,1
AMD,o0ss6c5,"Makes sense, I have a fairly mid range CPU, maybe I'll upgrade it, but to be honest, I don't want to till the next generation of CPUs hit",pcmasterrace,2026-01-21 04:23:41,1
AMD,o0vd4yi,"Oh yeh it usually runs bad, but this is Id's engine which runs amazing on amd cards generally. They did black magic to get it running smoothly. Obviously I couldn't use the path tracing mode but who cares.",pcmasterrace,2026-01-21 15:38:38,2
AMD,o0vdgye,"That's funny, because of course both Indiana Jones and Doom TDA at max settings run *much* better on your GPU than Control does. Because they have 'forced RT' so the games are actually built to *properly utilize* it, unlike Control which has the nice non-forced RT everyone loves - because it's bolted onto the existing engine and runs like crap. Just like *every* game that has RT bolted on runs like crap when it's enabled.",pcmasterrace,2026-01-21 15:40:11,2
AMD,o0uhod9,"I had one of the earlier fan cooled GPUs, and it had the bearing in the fan fail. It rattled for a bit, then the card died.   I remember all the issues with audio drivers, and it being generally recommended to have a separate sound card to get better stability for games",pcmasterrace,2026-01-21 12:53:12,2
AMD,o0vf8n6,What I meant was the guy I was replying to says itâ€™s perfectly natural for a 6 year old gpu to be seen in the recommended. Kinda insinuating that the 3080 is an older less powerful card that should be relegated to this status. Despite it being stronger than the less than a year old 5060ti,pcmasterrace,2026-01-21 15:48:07,-1
AMD,o0sflta,"Again, what 5060ti...  You used a source. I used the exact same source. When you actually dive into the specific results, the ""12%"" average. Turns out its a lot more than 12% but occasionally the 3080 didn't run a vram parasite. You just have to open up the techpowerup results to see the details.",pcmasterrace,2026-01-21 03:05:37,1
AMD,o0rvpqp,Where my bro the 3070 ti? He falls right above the 7700 XT in 1080p.,pcmasterrace,2026-01-21 01:11:54,1
AMD,o0ssf1o,9800s only recently came out and honestly even the 5800x3d is still a premium chip. You wouldnâ€™t be putting yourself in a bad position if you grabbed any x3d chip,pcmasterrace,2026-01-21 04:25:19,2
AMD,o0viifn,"Sure, but also it *is* an older less powerful card. It's simply relegated from a flagship status to midrange - while 5060Ti although more recent was of course midrange to begin with.",pcmasterrace,2026-01-21 16:02:45,1
AMD,o0t2uzq,You can't just grab a CPU and call it a day. Moving from LGA 1700 to AM4 would be monumentally stupid. He can already upgrade to CPUs equal to the 5800X3D on the existing platform.,pcmasterrace,2026-01-21 05:39:04,2
AMD,o0t3r93,"Of course he can, I was just on the same train of thought of I upgraded to an x3d chip.",pcmasterrace,2026-01-21 05:45:55,2
AMD,o0ta593,You upgraded to AM5 which makes sense. The 5800X3D isn't an upgrade over the best Intel CPUs.,pcmasterrace,2026-01-21 06:38:03,2
AMD,o1ctn1r,"If it works, don't mess with settings in the bios.  Get an anti-sag bracket to support the GPU.  Don't worry about the cables and add one back exhaust fan, bottom fans are not really necessary.",pcmasterrace,2026-01-24 02:23:09,2
AMD,o1cvtrb,"Will do, thanks for the heads up!",pcmasterrace,2026-01-24 02:35:53,1
AMD,o15d5rg,"1) You can't buy 5700X3D new anymore, and the used ones are priced horribly imo. Unless you could get a good deal, I personally like the 5700X.    2) 9060 XT has a 8 GB and a 16 GB version. Get the 16, and XFX makes a white version. Triple fans too.",pcmasterrace,2026-01-23 00:27:01,1
AMD,o15e80k,"Those parts seem fine to me. You should put those parts into [pcpartpicker.com](http://pcpartpicker.com) to check the compatibility. A white GPU is fine, but you should expect it to be more expensive than a black one. They usually are. Lastly, it's important to remember that your PC will always have a bottleneck regardless of what parts you put inside it. That's just how computers work. What matters is what's creating the bottleneck. Ideally you want the GPU to be creating the bottleneck. As long as you're not pairing a high-end GPU with a low-end CPU then you should be fine in that regard.",pcmasterrace,2026-01-23 00:32:35,1
AMD,o15h086,Would this work and donâ€™t mind the BiOs I can upgrade anytime. [pc part picker list](https://pcpartpicker.com/list/BkxRNz),pcmasterrace,2026-01-23 00:47:17,1
AMD,o15kre7,"Yeah, that should work. Something worth noting though, you have picked a 750w power supply which is more than what you need unless you care about future proofing the build. PCPartPicker will tell you the estimated wattage of your build. You can pick your PSU based on that.",pcmasterrace,2026-01-23 01:07:56,1
AMD,o160gbz,I have an extra 200 watts which should be good for this build if I donâ€™t overclock and if needed I can ge a new psi,pcmasterrace,2026-01-23 02:35:58,2
AMD,o0wdm44,Anyone tried gow 2018 to see if ahadows are fixed with 9000 series,AMD,2026-01-21 18:21:18,16
AMD,o0wgloi,"Darktide still not listed as known issue , amd is even ghosting Fatshark ( dev of darktide / vermintide ) for 1 year....Â [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)",AMD,2026-01-21 18:34:23,57
AMD,o0zebnj,No clue whats up but my first crash after installing the drivers completely uninstalled Adrenlin software. First time seeing that happen.,AMD,2026-01-22 03:26:18,7
AMD,o17hk3v,Roadcraft - Crash   Factorio - Crash   Borderlands 4 - Crash  This driver is a gammon. How on earth has this made it through QA. I've rolled back to 25.12.1,AMD,2026-01-23 08:58:39,3
AMD,o0wcqxd,Thank god its optional amirite,AMD,2026-01-21 18:17:34,11
AMD,o0w9oos,Another AI Crap,AMD,2026-01-21 18:04:05,6
AMD,o0wkwuh,So...what exactly should this thing be used for? Ask if the driver is working? Can't understand why it's there and what is the use for it.,AMD,2026-01-21 18:53:11,6
AMD,o13srd6,Did adrenaline get really leggy for anyone else with this update?,AMD,2026-01-22 19:45:34,1
AMD,o15reun,Roadcraft crash in 9070XT after 26.1.1,AMD,2026-01-23 01:45:43,1
AMD,o0zbj8r,Anyone tried Starrail yet to see if the error pop up still comes up? Like for mine the game still runs fine but stutters for a few seconds on launch and crashes adrenaline.,AMD,2026-01-22 03:09:54,1
AMD,o0wm7tl,destroyed my 6800,AMD,2026-01-21 18:58:52,-6
AMD,o0wlcs5,Does the ai bundle also for 7900xtx?,AMD,2026-01-21 18:55:07,-3
AMD,o0x2t3c,Nueva versiÃ³n y aÃºn no arreglan el problema de crasheos en battlefield 6,AMD,2026-01-21 20:13:56,-2
AMD,o1bea4y,im on 25.9.2   it works fine there,AMD,2026-01-23 21:46:50,1
AMD,o0wijbi,No,AMD,2026-01-21 18:42:48,-4
AMD,o0x5oq8,No,AMD,2026-01-21 20:27:01,-5
AMD,o0wj0wj,"Cyberpunk crashes are also not fixed but we get AI, yay!",AMD,2026-01-21 18:44:56,34
AMD,o0xvkne,"It's unbelievable that the game is still broken and AMD just doesn't care one bit. I love the game but can't play it as it's a stuttering / low FPS mess. The worst part is my GPU only seems to use 50% or less.  I'm not sure what's wrong, but the AMD driver just doesn't seem to treat it like a game and allocate high power mode to the GPU.",AMD,2026-01-21 22:26:11,4
AMD,o0xg37g,It's optional and when you use it it's local running on your own GPU.,AMD,2026-01-21 21:14:12,10
AMD,o0wr6ir,Ai image/video gen with comfyUi etc. and local LLMs,AMD,2026-01-21 19:21:01,8
AMD,o15ilu3,"I get random crashes, no lag though.",AMD,2026-01-23 00:55:56,1
AMD,o0wcska,For the time being it's optional.,AMD,2026-01-21 18:17:45,30
AMD,o0wmar1,Its optional and not clicked by default. Its litterally only a benefit. If you accidentally click custom install then accidentally click each ai tool on you dud something wrong.,AMD,2026-01-21 18:59:13,12
AMD,o0xcfit,How that?,AMD,2026-01-21 20:57:28,1
AMD,o0wmdz1,From what I've heard it's for 7000 and 9000 series. So yes.,AMD,2026-01-21 18:59:36,5
AMD,o0wnkjq,Cyberpunk just crashes at the logo for me. I hear like a second of sound and then back to the desktop.  Edit: Reverted back to 25.12.1 and it is working again.,AMD,2026-01-21 19:04:50,9
AMD,o0yj308,yay  :),AMD,2026-01-22 00:29:58,1
AMD,o0znjf2,"On Steam or GoG? I have the GoG version and I rarely crash, even with mods. I mostly have problems with the launcher sometimes, but not in the actual game itself.  That said, it's odd and bad that Cyberpunk crashes aren't fixed given how big of a game it is. AMD really ought to do better.",AMD,2026-01-22 04:23:55,1
AMD,o18znhd,Atleast the the pink path tracing is fixed  How long roughly does it take to crash for you? I drove around the map for a good 10 mins to see if the path tracing was fixed and it didn't crash,AMD,2026-01-23 15:08:18,1
AMD,o0yoszk,How do you actually generate images and stuff?,AMD,2026-01-22 01:01:14,0
AMD,o0x3dkt,I am having a blast with it so far.,AMD,2026-01-21 20:16:32,-5
AMD,o0wmgq9,You only see these if you click custom. And they are not including the files only options to dl them.,AMD,2026-01-21 18:59:57,2
AMD,o0wnegr,"While I agree I think TPU got it right when they speculated as to why AMD did this the way they did. Drivers are just more far reaching as compared to a web page. AMD clearly wants far more people working with ROCm and knowing that they're capable in AI development and this was the most wide spread way for them to do so.   Thankfully, it's optional so it shouldn't have much of an impact in day to day use.",AMD,2026-01-21 19:04:05,1
AMD,o0wmpem,"when I go to install the new 26.1.1 I dont see the option, even though its for 7700 or newer, 7900xtx is a part of that so was wondering if anyone else knows whats happening?",AMD,2026-01-21 19:01:00,1
AMD,o0xxm2v,"If you have mods, delete your r6/cache folder + verify files via Steam. if you tried this already, then RIP.",AMD,2026-01-21 22:36:17,7
AMD,o15bba1,"1. Have you tried using the command ""  --launcher-skip "" in launch options?    2. Also might sound very basic, but have you also tried to run the game as administrator?    3. Do you have all the Visuall C++ files installed? (Like the Visual Redistributable 2015-2022?Even the newer ones)   4. Turning off all overlays.   5. Rolling back the drivers to... well, older ones, I always keep an install of older drivers just in case (Like 23.12.1, 24.4.1, 25.6.3, which I found very stable for me personally)   If any of these doesn't work, there's a guy with 17 mins of fixes on youtube, lel",AMD,2026-01-23 00:17:20,3
AMD,o13e3j6,ComfyUi for more advanced stuff. Amuse for sinple beginner friendly stuff,AMD,2026-01-22 18:39:46,0
AMD,o0ypwws,Press custom install.,AMD,2026-01-22 01:07:27,2
AMD,o0y2bqf,I don't have any mods and I already tried verifying & restarting.  : /,AMD,2026-01-21 23:00:04,3
AMD,o17mxvx,"1. Just gets to back to the desktop quicker as I don't have to press play on the launcher 2. Same CTD 3. I'm pretty sure I have as the game ran fine just a few weeks ago. Installed the newest one 2017-2026 ones. Didn't help. 4. Disabled Steams overlay and closed RivaTuner, which I use to limit fps in GTA or The Witcher. Didn't help. 5. Never done this before but I guess it's worth a try now.  Btw tried it with disabled HDR and also didn't help.  **Ok reverted back to 25.12.1 and the game works again**. Wow.",AMD,2026-01-23 09:49:18,2
AMD,o12crof,"Wasn't working, had to install driver, than there was an AI tab at the top of the software that let me get the bundle through there.",AMD,2026-01-22 15:52:54,1
AMD,o0y2pg5,"Yeah, ugh damn. Very sorry to hear you're having issues. AMDpls :/",AMD,2026-01-21 23:02:03,3
AMD,o17vzk1,"Lovely to hear this, also this is why I keep a pair of older drivers, they just randomly work XD Have a blast, sir!",AMD,2026-01-23 11:09:29,2
AMD,o0y4hop,I'm going to try a reinstall tomorrow and see if that helps. I didn't have had anything like this before until now :<,AMD,2026-01-21 23:11:25,5
AMD,o13hml1,Reinstall did not help. ;\_;,AMD,2026-01-22 18:55:20,2
AMD,o13rae3,"There is an issue where you have to delete the newest auto save and then turn off auto save. Worked for a bunch of friends of mine.Â  Give it a try.Â  If it works, not an AMD issue.",AMD,2026-01-22 19:38:54,2
AMD,o13xday,I deleted all AutoSave folders and it still crashes to desktop after the Cyberpunk 2077 splash screen animation. I see like less than a second of the CD Project Red logo and then it's gone.,AMD,2026-01-22 20:06:45,1
AMD,o1976ga,"But Asus has all its BIOS in beta, and that voids the warranty. ðŸ‘€",AMD,2026-01-23 15:43:27,24
AMD,o19h3ez,Please do not burn my CPU while also causing CPU shortages in the near future thank you....,AMD,2026-01-23 16:27:56,9
AMD,o1fhhe4,i lowered my SOC to 1.22v and turned off precision boost overdrive for the meantime. better be safe than sorry,AMD,2026-01-24 14:26:32,2
AMD,o19kgcj,"I've long suspected that the ASRock 800-series motherboard issues had more to do with poor 9800x3D QC practices by AMD than the boards. The same issues being encountered with the same series of boards made by a *different* company is definitely lending some credibility to this theory. If true, this doesn't fully absolve ASRock or ASUS but it should certainly shift the spotlight towards AMD.",AMD,2026-01-23 16:42:57,3
AMD,o1b5bvp,"ASUS- ""voiding warranties since 1886""...",AMD,2026-01-23 21:05:10,3
AMD,o1a1v3e,"Didn't we already go through this with the 7800X3D & ASUS motherboards?  As Bush once said ""Fool me once, shame on you. Fool me...you can't get fooled again""",AMD,2026-01-23 18:02:46,0
AMD,o16rebn,"Hey OP â€” /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q1 2026, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1q1efc5/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-01-23 05:19:41,-1
AMD,o1ewcex,"Absolutely NOT a fan of ASUS, given their behavior over recent years, but that statement IS completely false. They stopped doing that shit back in May of 2023. Of course, they did replace it with god awful customer service, and trying to actively scam people with their RMA service, but still, the point stands that all BIOS are now covered by warranty.",AMD,2026-01-24 12:08:53,7
AMD,o1b5ck3,"ASUS- ""voiding warranties since 1886""...",AMD,2026-01-23 21:05:16,8
AMD,o1bsfan,This is false. [Both official and beta BIOS updates are covered under warranty](https://www.asus.com/us/news/ihctikmgahafyrib/).,AMD,2026-01-23 22:56:23,6
AMD,o1icbkb,it was above that before?     Expo on my gigabyte X870 board has the SoC voltage at 1.195V out the box,AMD,2026-01-24 22:21:04,2
AMD,o19ozdx,"They are both owned by pegatron and share practices. For all intents and purposes, Asus is Asrock in everything but name.",AMD,2026-01-23 17:03:22,-4
AMD,o1ewf91,"No, that was ASRock. Still is ASRock, too, as they seemingly haven't done anything to fix the issue.",AMD,2026-01-24 12:09:31,1
AMD,o1a9b31,That wasn't an ASUS issue,AMD,2026-01-23 18:35:54,0
AMD,o1bvce8,"Only for AM5, not for the rest",AMD,2026-01-23 23:11:47,9
AMD,o1insve,yeah my stock board + expo had a current reading of 1.238 soc on auto,AMD,2026-01-24 23:18:57,3
AMD,o1ad2d6,"Incorrect. ASRock started as an ASUS brand that was later placed under the Pegatron umbrella when ASUS spun it off into an independent company. ASUS is still Pegatron's controlling shareholder, but they've significantly divested from them over the years. They currently own 16% of Pegatron.   Pegatron started with legacy ASUS manufacturing assets but established independent processes not long after their split. They have their own in-house blend of plastics, metal tooling, thermal system design features, and RF validation labs. However, the two companies still have similar supply chain clustering.",AMD,2026-01-23 18:52:36,6
AMD,o1ic0g7,"No, it was Asus with the 7800X3D     [https://www.youtube.com/watch?v=cbGfc-JBxlY](https://www.youtube.com/watch?v=cbGfc-JBxlY)     [https://www.youtube.com/watch?v=kiTngvvD5dI](https://www.youtube.com/watch?v=kiTngvvD5dI)     Asrock has had no issues with 7000 series chips, it's the 9000 series chips, especially the X3D ones that they have been killing multiple per day for close to a year.",AMD,2026-01-24 22:19:36,1
AMD,o1icip8,"it was, Asus boards were literally burning up 7800X3D chips",AMD,2026-01-24 22:22:03,0
AMD,o1ew5g9,That's not been a thing for nearly 3 years. May of 2023 they changed their warranty coverage so that Beta BIOS was covered in the same way as regular BIOS.,AMD,2026-01-24 12:07:20,1
AMD,o1ahf88,"So when pegatron makes boards for both asrock and asus, sharing practices between them both.. you are stunned both have the same failures and suddenly its about semantics of who owns how much of what when they are making boards at the same factories?",AMD,2026-01-23 19:12:32,6
AMD,o1icnxh,"Nope, the issue came from another company that needed to fix their shit which also needed to wait on AMD's approval to get released",AMD,2026-01-24 22:22:45,0
AMD,o1ewkqd,"You can't make objectively false statements and then expect to be taken seriously, mate.",AMD,2026-01-24 12:10:46,0
AMD,o1idu1m,"which ""other company"" was responsible for Asus overvolting VSoC when Expo was enabled with 7000X3D chips?  you can't just sit and lie, it's publicly known that Asus was the only one killing 7000X3D chips and the issue was wholly their fault to the point where AMD had to put a hard cap on VSoC after that to protect customers from board vender stupidity.",AMD,2026-01-24 22:28:29,0
AMD,o1ig92t,Don't remember the name. However this is a fact,AMD,2026-01-24 22:40:25,0
AMD,o1ih7a4,"it's not a fact, it was Asus, and if Asus is relying on an external company to configure their bios and set unsafe voltages then guess what, it's still Asus's fault, no other board vendor had this issue.",AMD,2026-01-24 22:45:13,0
AMD,o1ih9u6,It also affected MSI and not just ASUS. It is a FACT,AMD,2026-01-24 22:45:34,0
AMD,o1iiztg,"got any proof? every single 7800X3D burning report or result i've found has involved an Asus board      you're making empty claims blaming it on some ""unknown company"" with no source and saying MSI was also impacted, but again with no source.",AMD,2026-01-24 22:54:11,0
AMD,o1ijzda,"https://www.theverge.com/2023/4/27/23700688/amd-ryzen-7000-x3d-cpus-burnt-out-am5-motherboard-fix  Here it quite clearly says 'AMD has resolved the issue', not ASUS has resolved the issue  Do your research correctly next time",AMD,2026-01-24 22:59:12,0
AMD,o0v3anv,I \*really\* hope this fixes the driver timeouts I've been getting constantly using hardware acceleration on Chromium based browsers.,AMD,2026-01-21 14:51:50,185
AMD,o0vjurm,"Did anybody elses AMD Adrenalin Edition software just... go away? I checked today to update, and its totally gone. Not in my drives, nowhere, and i never uninstalled it myself.",AMD,2026-01-21 16:08:49,29
AMD,o0v563f,"Avatar was unplayable with prior drivers because of heavy flicker while using FSR4, hope it's fixed now.  Edit: it's not fixed still a mess. Workaround is either a driver rollback or disabling fsr upscaling on the driver just for this game and using optiscaler with fsr 4.0.2.",AMD,2026-01-21 15:01:00,72
AMD,o0w3ct1,"When will AMD or Microsoft (whoever is at fault) fix the completely random driver timeouts caused by MPO? Whenever anything interacts with MPOs (game going fullscreen, alt tabbing out of a fullscreen game, dragging a window over a fullscreen game) there is a 15% chance that I will get a 2 second black screen -> driver timeout and a 5% chance the pc just freezes/reboots on its own  This used to happen back in Windows 10 2 years ago too, turned off MPO and it fixed it for 2 years, but then after I updated to Windows 11 a few months ago it started happening again. Even switched from a 6700xt to a 9070xt and it still happens.   Turning off MPO again fixes it but many programs nowadays rely on MPOs (see my [post about discord](https://www.reddit.com/r/discordapp/comments/1nwg7ny/bug_megathread_vol_11_october_november/nr2nqht) where if you stream a game without MPOs it will force the game into `Composed: Flip`), so I can't really use that trick anymore.  I send a report with the amd bug reporter popup thing every time, but I'm not sure if these are useful or if they are even checked by anyone",AMD,2026-01-21 17:36:21,27
AMD,o0vbjo4,"Wait, ComfyUI can run on Radeon cards on Windows? Since when? That's huge!",AMD,2026-01-21 15:31:12,10
AMD,o0v5dk4,Did they fix amd noise supression?,AMD,2026-01-21 15:02:02,31
AMD,o0vscsg,"Annoying that AMD stopped listing the internal version numbers (Windows driver versions) in the release notes. It was very useful in finding out what wacky 32.somethingsomething driver translated to in Adrenalin version numbers...  For the record, Adrenalin 26.1.1 translates to  32.0.23017.1001 for RX 7xxx/9xxx series  32.0.21041.1000 for RX 5xxx/6xxx series",AMD,2026-01-21 16:46:56,10
AMD,o0v3ny2,did they fix noise suppression yet?,AMD,2026-01-21 14:53:40,18
AMD,o0ybxbe,"Yo AMD, can we maybe start getting some fixes for Borderlands 4? The game really doesn't look that good on my RX 9070 compared to my old 3070. Lots of lights flickering, bugs with vegetation sway, shadows not showing correctly sometimes, and honestly FSR could need a visual boost.  https://i.redd.it/4islq8yciseg1.gif",AMD,2026-01-21 23:51:29,7
AMD,o0v9rix,Why is there a b version which is 900mb and a c version which is 1.6gb?,AMD,2026-01-21 15:22:55,13
AMD,o0vnop4,"AV1 still crashes Chrome. It used to happen in Firefox as well, but it has become less common.",AMD,2026-01-21 16:26:03,6
AMD,o0v575r,Hope this solves Battlefield 6 crashes! I've been on 25.12.1 since I bought my 9060 XT and it crashes every day or so while playing!,AMD,2026-01-21 15:01:09,18
AMD,o0vc20u,"For GPU's that are no longer being maintained according to numerous reddit experts the 5000 and 6000 series sure got a lot of fixes.  I'm glad ROCm support was official added to the normal drivers now. Before this if you wanted to mainly game but dabble in some local AI stuf you'd, officially, needed to switch drivers. I got it working with the normal drivers, and after changing a setting it was pretty usable but would still crash every so often. hopefully that's fixed now.",AMD,2026-01-21 15:33:36,39
AMD,o0y3k02,This update fixed vsync not working on DX12 games with a 9070xt for me on the previous driver if anyone else had that issue.,AMD,2026-01-21 23:06:33,4
AMD,o0z1flx,"Driver Timeouts like no tomorrow, can only play roughly 10 minutes of Zenless Zone Zero",AMD,2026-01-22 02:13:01,5
AMD,o0v54p3,Will Monster Hunter Wilds still crash? Almost definitely lol,AMD,2026-01-21 15:00:48,19
AMD,o0vncn2,can't all those AI gigabytes port FSR4 INT8 to RDNA2/3?  /s,AMD,2026-01-21 16:24:30,7
AMD,o0w4i7c,"No FSR4 yet for the 7900xtx, while Steam deck has it (modded in)",AMD,2026-01-21 17:41:27,6
AMD,o0viuz5,Just installed these good so far.,AMD,2026-01-21 16:04:19,3
AMD,o0vwsuc,![gif](giphy|NWpTZunsRRaJor5KD1)  Dare I hope the this one is more stable than the last couple for my 7800XT? I'm still sitting on 25.9.2,AMD,2026-01-21 17:06:55,3
AMD,o0yqkcy,All I want is official FSR4 support for 6/7000 series gpus,AMD,2026-01-22 01:11:09,5
AMD,o0vsm1t,"Umm. Iâ€™ll wait for you guys to test and approve /reject. Iâ€™m good back here on 25.9.2.   Also, fuck amd and ai bundle and ai naming convention.",AMD,2026-01-21 16:48:07,9
AMD,o0w7p2h,"Stop with the stupid AI bundle that's useless and takes 34GB user's disk space. Use that effort to add better river functions, like FSR 4 for RDNA2/3. My patience is running thin.",AMD,2026-01-21 17:55:22,7
AMD,o0vbggg,"on the last 25.12 driver I experienced slowdowns on AC Shadows when fsr framegen is enabled on my 9060xt, my fps goes from 60+ to under 40, if I use xess framegen instead it works fine.   It doesn't matter if I use the in-game fsr 3.1 or the driver redstone, also doesn't matter which upscaling is in use",AMD,2026-01-21 15:30:48,2
AMD,o0ven3w,Cassette beasts fix woo,AMD,2026-01-21 15:45:28,2
AMD,o0vjvdl,"ironically, I didn't notice anything extra installing because I always install my drivers without my internet connected and was confused what everyone was complaining about lol.     For those that have used the rocm on windows, is it any faster than just using zluda? I tried it a while ago and it really wasn't any faster and created more issues.",AMD,2026-01-21 16:08:53,2
AMD,o0xc436,Any hope the Adrenalin Panel not opening issue is solved? Iâ€™m without internet connection ATM and canâ€™t try the new releaseâ€¦,AMD,2026-01-21 20:56:03,2
AMD,o13n1b9,Still not fix for the default driver overclocks that make any amd card unstable unless manually downclocked. Why is this still an issue in 2026?,AMD,2026-01-22 19:19:32,2
AMD,o15i3cs,What is the changelog for polaris? its empty on their site,AMD,2026-01-23 00:53:09,2
AMD,o15i751,"My AMD Adrenaline Edition seems to randomly crash or restart when I click menu options, like a 1/100 chance each time. Has anybody experienced this?  More of a nuisance than a problem, but it's a huge one and I'd prefer if it ended.",AMD,2026-01-23 00:53:43,2
AMD,o0v6hi3,Happy to see that all the naysayers have been proven wrong. Actual fixes for games. You're welcome boys,AMD,2026-01-21 15:07:25,7
AMD,o0vm2cm,"This AI bundle, I have most of those things installed and working already, is there a point in getting this bundle to have them ""managed"" by AMD or something?   Is there an upside using the bundle instead of doing this separately like i've been doing basically?",AMD,2026-01-21 16:18:43,4
AMD,o0w12xq,Aiaiaiaiaiaiai,AMD,2026-01-21 17:26:12,3
AMD,o0wpcfq,No fsr4 for vulkan No fsr4 for rx 7000 series,AMD,2026-01-21 19:12:45,3
AMD,o0v7tx9,The damn CP Pathtracing bug has been there for like 3-6 Month... ffs,AMD,2026-01-21 15:13:51,5
AMD,o0v5p1y,"At the same time Mesa 26.0 is in the works (probably in more then a month) and there will be no actual proper support from AMD...   Looking at the Win driver - total shitification, waiting for the users to install it and report the new problems and crashes",AMD,2026-01-21 15:03:37,5
AMD,o0vg7sw,"They can release ""AI bundle"" for RDNA3, but can't release FSR4 for it... Insane.  They should go the same route as Nvidia (they release newest versions for even 2000 series, even with heavy performance loss) and give us a choice if we want it or not.  The unofficial one already worked alright for me, there's literally no reason the official one wouldn't work just as fine or better.",AMD,2026-01-21 15:52:29,4
AMD,o0v6u6d,"As someone who's been using nvidia for a long time, it's this a new Radeon graphics driver release?",AMD,2026-01-21 15:09:07,2
AMD,o0v98aw,Is the overlay fixed?,AMD,2026-01-21 15:20:26,2
AMD,o0vf45p,"I know that the ""AI Bundle"" is optional but what makes AMD think that consumers have a demand for anything with AI?   I think the technology industry is very out of touch with consumers at this point and that was harshly evident with CES.",AMD,2026-01-21 15:47:34,3
AMD,o0v77iy,I hope theyâ€™re working on the lighting issues in GoW 2018ðŸ™,AMD,2026-01-21 15:10:52,1
AMD,o0v7ymv,1.6gb ?!  EDIT: Installer also crashes for me when it loads up.,AMD,2026-01-21 15:14:28,1
AMD,o0v95sa,I donâ€™t have permission to download from their website? Thatâ€™s new,AMD,2026-01-21 15:20:07,1
AMD,o0w0cmr,the pytorch part fails to install : /,AMD,2026-01-21 17:22:56,1
AMD,o0wetn9,"Will this help on getting ollama running with an RDNA4 card?  I heard a lot of ppl have issues and llama doesn't detect 9070xt on windows and are forced to use Linux,  or use a variable to run via VULKAN.",AMD,2026-01-21 18:26:34,1
AMD,o0wgopz,"6750XT, still have the problem with the refresh rate dropping to below 100 on the desktop after exiting a game instead of going back to 180, although after some troubleshooting the culprit appears to be Wallpaper Engine, without it running everything works fine.",AMD,2026-01-21 18:34:45,1
AMD,o0wgzj3,omg they fixed black screen issue when alt tabing with freesync on.,AMD,2026-01-21 18:36:04,1
AMD,o0wi1e3,This suposed to have rocm 7.2 but comfy start with 7.1. Now we have to wait for comfy to update rocm?,AMD,2026-01-21 18:40:37,1
AMD,o0xoigv,"Ok, so does this break 9070XT usage still and I need to stay with 29.5?",AMD,2026-01-21 21:52:38,1
AMD,o0xq417,"Has anyone tested if this fixed the web apps (discord, firefox, etc.) crashing that 25.12.1 had?",AMD,2026-01-21 22:00:03,1
AMD,o0xzjze,is it possible to somehow only install ComfyUI from the AI bundle? I don't want all those other no-name stuff.,AMD,2026-01-21 22:46:00,1
AMD,o0ybhdx,Someone should post a TLDR if this is worth updating to,AMD,2026-01-21 23:49:08,1
AMD,o0ynlvp,Still broken noise suppression for the very same reasons as the previous couple of drivers.,AMD,2026-01-22 00:54:33,1
AMD,o103vgh,Should I install the new driver over 25.12.1 or use DDU first? Have the 9070XT.,AMD,2026-01-22 06:22:42,1
AMD,o105mxf,The update broke my display controls,AMD,2026-01-22 06:37:19,1
AMD,o105vd0,is the amd noise suppression still broken on this version?,AMD,2026-01-22 06:39:16,1
AMD,o10e6rc,This driver set my default audio device to my monitor instead of my USB DAC... Easy fix but a tad annoying.,AMD,2026-01-22 07:51:47,1
AMD,o10h9wq,"I've just installed it on two of my systems. this is becoming a monthly game of its own. xD  Seems normal so far.(Adrenalin interface, browser graphic functions. Sys1:7900xtx, Sys2:6700xt)",AMD,2026-01-22 08:19:56,1
AMD,o10tqy8,"I found culprit of driver timeouts on windows. Its windows update. Nothing new, right?   Imagine setup that works solid for few **MONTHS**, then suddenly driver start having timeouts, few per day. During this time i installed some software on system, which also installed VC++ libraries. I heard (and confirm it works) opening command prompt as admin, typing `winget update -all` fixes this (reboot required), and it is, then after few days driver timeouts started again, on same driver version. So what changed? Windows update have changed. Same day that timeouts started, WU installed update. Keep in mind i am on WINDOWS 10, which got opinion of being stable. I guess quality of W11 updates have spreaded into Windows 10 ESU updates.  What next? Well, increasing voltage of my UV have helped, but now i am left at just -15mv, when -40 was initially stable. Crashes (driver timeouts) started on non demanding titles, like Gates of Hell Ostfront, in totally random moments.   I'll see how the new driver will perform, but with Fedora on dual boot, i'm starting to thing about complete block of windows update at this point. What this system is becoming? I tried Linux rolling release distros (or bleeding edge) and they were in better shape than Windows currently. This is seriously out of control.",AMD,2026-01-22 10:17:23,1
AMD,o11efth,still havent fixed Second Life issues since 25.10.2 really taking their sweet time [https://github.com/secondlife/viewer/issues/5048](https://github.com/secondlife/viewer/issues/5048),AMD,2026-01-22 12:56:17,1
AMD,o11w3o1,Nothing burger,AMD,2026-01-22 14:32:37,1
AMD,o124gfi,Driver Timeouts in AC Valhalla still not fixed.,AMD,2026-01-22 15:13:50,1
AMD,o125tic,"For anyone who, like me unable to get a minimal install, do this.   Thanks in advance.  Send a new mail toÂ [**consumer.techsupport@amd.com**](mailto:consumer.techsupport@amd.com)  Just copy and paste the following.  **Hello AMD Tech Support.**  **Can you please fix the issue where selecting Minimal Install still leads to a full installation ?**   **When I choose Minimal Install, itâ€™s because I donâ€™t need all the bloatware from the full install.**  **I would like the Minimal Install option to actually just works as it should ...**  **Can you please address this annoying problem ?**",AMD,2026-01-22 15:20:22,1
AMD,o126pd1,might as well stop wasting time on that AI shit amd,AMD,2026-01-22 15:24:39,1
AMD,o12g77y,"Is anyone having trouble starting Cyberpunk on a 7900xtx? I just updated and the game won't start; it just crashes and exits, giving an error.",AMD,2026-01-22 16:08:26,1
AMD,o12svjt,Still no fix for PoE2 long loading time if using Vulcan?,AMD,2026-01-22 17:05:07,1
AMD,o12zymf,"Nvidea has DLSS 4.5 available for VR, when we are going to have the FSR4 for VR??",AMD,2026-01-22 17:37:20,1
AMD,o13gnn7,Any link for the combined driver?,AMD,2026-01-22 18:51:04,1
AMD,o1480rv,"Has anyone updated their 6900XT to the non AI version of these drivers? Wonder if I should even bother, in the past couple updates I've noticed 0 in performance.",AMD,2026-01-22 20:56:35,1
AMD,o149xo7,Still getting directx crashes in BF6,AMD,2026-01-22 21:05:37,1
AMD,o14l7aw,"While playing Diablo IV, i keep getting driver timedout randomly, it comes back after 30s to 1min or so, but its very annoying, is there a fix for this? Ive reinstalled windows 11, updated everything, than installed this driver version for the entire reason of Ai bundle because it seemed fun, but now my gameplay sessions are suffering random freezes from this error.",AMD,2026-01-22 21:59:45,1
AMD,o150f2p,"Is this version stable? I removed Adrenalin from my PC, and have had zero crashes.  It kept disconnecting my monitors and putting them to black screens within windows randomly, and the only fix was to reinstall Adrenalin or restart my computer",AMD,2026-01-22 23:20:17,1
AMD,o157kq3,"u/amd_Vik Hi, I'm having a strange issue in Project Cars 2 where photo mode just destroy the performance. In races the game runs at flawless 144 FPS, while in photo mode the FPS drops to 40 FPS or less. GPU usage also drops. Can you help me?",AMD,2026-01-22 23:57:32,1
AMD,o16kuys,this broke monitor communication in twinkle tray for me :(,AMD,2026-01-23 04:35:46,1
AMD,o17zqqf,does it fix this?   [https://www.reddit.com/r/Warzone/comments/1qi4pzc/graphics\_bug%D0%B1%D0%B0%D0%B3\_%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%BA%D0%B8/](https://www.reddit.com/r/Warzone/comments/1qi4pzc/graphics_bug%D0%B1%D0%B0%D0%B3_%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%BA%D0%B8/),AMD,2026-01-23 11:39:50,1
AMD,o1b3w8s,can anyone confirm this wont crash my GPU. I am currently on 25. 9.1,AMD,2026-01-23 20:58:23,1
AMD,o1fnoep,26.1.1 causes audio silence on 780m after resuming from sleep.  Going back to 25.12.1.,AMD,2026-01-24 15:00:06,1
AMD,o1g056s,"For those wondering, I use a 6800xt and this driver seems fine and stable. So far I tested it on battlefield 6 and path of exile 2. However, I don't have multiple displays, only one monitor.",AMD,2026-01-24 16:00:46,1
AMD,o0v4l0o,"AMD update ðŸ‘ðŸ‘ðŸ‘, I'll install it. Thank you AMD!",AMD,2026-01-21 14:58:10,0
AMD,o0v4zna,Glad to be on linux and not deal with this software bullshit,AMD,2026-01-21 15:00:07,-6
AMD,o0w3sks,Hmmm BitDefender is blocking the driver for Malicious behavior.,AMD,2026-01-21 17:38:18,1
AMD,o0xy6ir,How the fuck have they not fixed the texture corruption issue for BF6??? This is unacceptable,AMD,2026-01-21 22:39:07,1
AMD,o0z20s0,Booooooooring. We want Redstone on RX7000 series.,AMD,2026-01-22 02:16:20,1
AMD,o0wd1fa,How do u guys keep getting driver timeouts?,AMD,2026-01-21 18:18:49,1
AMD,o0w10nf,No int8 fix for rdna 2 if you are asking,AMD,2026-01-21 17:25:54,0
AMD,o0vx3f4,Can we please STOP PUSHING AI tools in our drivers please...  K' Thanx. byeee,AMD,2026-01-21 17:08:16,-1
AMD,o0wix2d,"The option to load files with overclocking and undervolting settings in Performance/Tuning section is now gone for me, nice...",AMD,2026-01-21 18:44:29,-1
AMD,o0yrq4c,you guys are saying that there's no issues with path of exile 2?,AMD,2026-01-22 01:17:44,0
AMD,o0wxzqf,"They really cant make stable good drivers huh?  Back to 25.10.2 Ig  I didnt even open browser on those new drivers and shit went poof with error.  After opening yt video it was even worse. Grey screen and crash xD  Every time there is new release of those ""drivers"" there are some kind of bugs. Some of them mind you not fixed for months.     But hey at least we have an option to install AI slop :3",AMD,2026-01-21 19:51:57,-2
AMD,o0wklzv,Anyone able to get a real minimall install without bloatware shit like this screen ?      [**https://ibb.co/9H52DYrL**](https://ibb.co/9H52DYrL),AMD,2026-01-21 18:51:53,-3
AMD,o0v809x,"No worries the tech overlords will make sure we all live on a subscription model so we wont need to deal with GPU updates anymore, cant wait for the futureâ€¦ðŸ˜’",AMD,2026-01-21 15:14:42,-12
AMD,o0vgflg,Interesting. I'd have this issue with Discord streaming sometimes which uses Electron and its Chromium. I rolled back a couple driver versions and it was fine.,AMD,2026-01-21 15:53:27,14
AMD,o0v72pb,"open the flags tab and change Choose ANGLE graphics backend flag to D3D11, fixed the issue for me",AMD,2026-01-21 15:10:14,28
AMD,o0vc61d,Those driver timeouts have happened intermittently for as long as I can remember. I dealt with them back in 2018 on a Vega 64 and ended up disabling HW accel for years. They would fix it and it would pop back up randomly. Happened in Firefox too.,AMD,2026-01-21 15:34:08,45
AMD,o0vm8i6,"Omg, so it's not just me. I had to roll back like 3 driver versions to get it to stop. Glad I'm not the only one and I hope this is fixed soon. I'm scared to upgrade to the latest driver so I'm going to wait to see people confirming it's fixed lol.",AMD,2026-01-21 16:19:29,12
AMD,o0yrpbp,"I got tired of this, moved to Linux and I havenâ€™t experienced a single timeout or crash since.  Iâ€™m glad I donâ€™t play any of the stuff that requires anticheat because Adrenalin is pure garbage at this point.  Itâ€™s unreal how bad the driver on windows is.",AMD,2026-01-22 01:17:36,7
AMD,o0vdrjq,There you go  https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/  It's not a driver issue,AMD,2026-01-21 15:41:31,21
AMD,o0vg803,firefox too,AMD,2026-01-21 15:52:30,8
AMD,o0zyu6l,"This has been happening to me since May on my 7900XTX. I've done driver rollbacks, Windows 11 reinstalls, RMA, everything set to default.. nothing worked.  I've narrowed it down to anything that uses hardware video encoding/decoding. It's definitely video-related.    In Firefox, I've changed **media.hardware-video-decoding.enabled** to False and I can watch videos normally without crashes.  Discord and anything that auto-plays videos will sometimes TDR and crash my whole system.",AMD,2026-01-22 05:43:00,2
AMD,o0vjpnt,"TLDR: me too, don't know why, at least now I see I am not the only one  Edit: I also disabled MPO as one of my last changes, and that might have helped the most.  System: Intel i5 12400, 32GB ddr4 ram, 1000W psu, Asus TUF RX 9070. Nothing overclocked. Default Adrenaline settings. Main display: 1920x1080@240hz Second display: 2560x1440@60hz  I've had this issue since I first installed my rx 9070. It's been driving me crazy, being new to AMD after a long time made me think it was just me. I never had this happen before on my system, so when the only change was replacing my gtx 1060, it didn't make sense. Went through it all, replaced the PSU, clean windows installs, new and old driver reinstalls, stress testing my system(showed no issues), changing bios settings etc. Even reseated RAM and GPU, replugged cables, flipped bios on GPU. Sent multiple bug reports through adrenaline software with all data I could find.  I use Firefox and it wouldn't happen so often, but it was still frustrating when it did, so I swapped to Edge. Thought it was just a Firefox problem because it seemed like it wasn't going to timeout, until it did. Tried Chrome, happened on my first attempt. Back to Firefox now and after a recent update, v147, it looks to be stable. It could have been windows updates being helpful, though there was no description of this problem in the change logs. This occurred from driver 25.9.1 to 25.12.1. Haven't tried Chrome and Edge again since I don't use them normally, but I did it at the time to try and find a solution.  I opened a bug report on Firefox: [https://bugzilla.mozilla.org/show_bug.cgi?id=1996433](https://bugzilla.mozilla.org/show_bug.cgi?id=1996433)  After all of this, I think it is not a browser specific issue, though at first I thought it was. But I do think that for whatever reason, hardware accelerated programmes displaying media as it is drawn in or interacted with, on a second display, have a chance to cause a driver timeout. What made this clear was when I simply scrolled in Discord one time and it timed out, as it is also hardware accelerated. At this point I still try to reproduce it because it's hard to believe it's fixed, but lately it seems to be better, for now...",AMD,2026-01-21 16:08:10,5
AMD,o0v84ao,I thought it was because I'm using Brave and only happening to me...,AMD,2026-01-21 15:15:13,4
AMD,o0v3ujt,"Bro I doubt this is a general issue, if you're talking about Microsoft Edge, the preinstalled browser on every single Windows installation, that would be a pretty famous and known issue :|",AMD,2026-01-21 14:54:35,4
AMD,o0wzoop,I've never had that problem with any AMD GPU. Huh.,AMD,2026-01-21 19:59:35,1
AMD,o0xyzj9,Dude I thought I was going crazy,AMD,2026-01-21 22:43:08,1
AMD,o0z30iu,"You know what? I've been having the same problem. Not with Chromium apps per-se, as I use Firefox, but I've had a hanging problem happening for the last few months which happens at pure random. Only when playing / scrubbing through video in Firefox or VLC. The hang usually happens very quickly after starting playback. I've also crashed it with Vegas Pro, which also uses the Media Engine.  But I can run video for hours, or game + play video for hours, and have absolutely no timeout. But if I lock and unlock my PC, I might get a hang 1/50 times. Or if I spend a half hour or so scrubbing through video, which is heavily stop and go. Eventually I'll crash it.  There is also a REALLY annoying VRAM memory leak I've been trying to track down. I caught Discord once using over 8GB of VRAM. It's another one of those random and intermittent things which shows up, but doesn't correspond with any GPU usage. Only uptime.  I can't crash the card with only games, or with any stress test though. Even tried OCCT with variable and fixed loads. No crash. No errors. Even tried both of the BIOSs, which were both stable for a good two years, with my card.  EDIT: Hopefully not just speaking too soon, but I remembered that my card has a problem with one of the DisplayPort outputs. If I connect just the right monitor to it, the port drives the monitor just fine, BUT... the GPU will become very unstable for whatever reason. Usually coincides with the HDMI display dropping out randomly. The other two DisplayPort ports work just fine! Might just be a board problem with the 7900XTX I have as I've heard of other reports of this happening on cards with Type C ports, with the Type C port in particular being broken.",AMD,2026-01-22 02:21:52,1
AMD,o1hlowj,disabling MPO stopped my drivers from timing out when alt tabbing from a game.,AMD,2026-01-24 20:14:45,1
AMD,o0v3spa,driver timeout aka TDR is a windows problem.,AMD,2026-01-21 14:54:19,1
AMD,o0v4k99,thats on you,AMD,2026-01-21 14:58:04,-11
AMD,o0vwlti,Yes it actually happens quite often for me and requires a manual launch or a complete restart,AMD,2026-01-21 17:06:03,25
AMD,o0xb3kk,Windows sometimes rips out the driver with their updates and replaces it with an internal driver. There was a big one recently so you could have got caught in that. It's happened to me at least 3 times since I upgraded this past year. You'll have to DDU and install a new adrenaline driver.,AMD,2026-01-21 20:51:30,7
AMD,o0ybtjw,Once you've used DDU to uninstall the basic Windows AMD Driver find the DDU setting option to stop Windows automatically installing its basic VGA drivers again.,AMD,2026-01-21 23:50:55,1
AMD,o0vzdrs,My god I thought that was just me. I'll need to test as soon as I get home from work.,AMD,2026-01-21 17:18:37,22
AMD,o0whzab,"Interested to know as well, had to install Optiscaler to fix it",AMD,2026-01-21 18:40:21,3
AMD,o0yn678,It isn't,AMD,2026-01-22 00:52:09,1
AMD,o13ehlx,Did you update the game? There was a patch today that is supposed to fix this?    >[PC] Fixed graphical corruption and flashing when using AMD Freesync and/or AMD Frame Generation.,AMD,2026-01-22 18:41:29,1
AMD,o0whbsj,"I've seen this too (it's unbearable in the clouded forest) but I don't believe AMD can fix it as it is most likely an FSR implementation issue. Ubisoft will have to fix this. The issue is not seen in other games and is highly scene dependent even in AFOP which points to instability in the game's g buffer, not an upscaler defect. If there's anything wrong with the motion vectors, depth, reactive masks, transparency masks, or disocclusion data that the game feeds into fsr, then the upscaler output will be affected.",AMD,2026-01-21 18:37:32,1
AMD,o0xldk3,"Given that Avatar runs reasonably enough on consoles already and that the weakest FSR4 capable GPU is a 9060XT(which is about as powerful as a PS5 Pro), can you not just turn off FSR4?  Might take an image quality hit, but that's very different from being 'unplayable'.",AMD,2026-01-21 21:38:13,0
AMD,o0xq1go,"Running Windows 11 and never had any issues with my 6950XT. I bought my 9070XT on day one and had to disable MPO for my PC to function even slightly normal. Itâ€™s still more laggy than my 6950XT *ever* was, but itâ€™s useable now.  Prior to disabling MPO, the system became utterly unusable whenever you tried to alt-tab. Multitasking was  an absolute joke.  Wish this issue had ever been addressed, but as far as I know the pool of affected users is small enough for AMD to just ignore itâ€¦",AMD,2026-01-21 21:59:43,7
AMD,o0x764g,"I had this happening on my 9070xt for a few weeks now, before that everything was going smooth. I had a slight undeclock and fast memory timings with increased mhz, and it worked fine for like 2 months. Then this started happening, and resetting it to default fixed it for me. Didn't test if it was because of memory or Gpu tweaking, I'm running full stock now.",AMD,2026-01-21 20:33:45,1
AMD,o13qzvx,"It's halfway fixed in Win11 Insider Build 26220.7653. I could drag windows over a game even with a video playing and it was fine. Frame gen was still working too. 9070XT. 25.12.1 driver was used at testing time. 1x 2160p 144Hz + 1x 2160p 60Hz monitors.  Windows volume control pop up or Steam notification pop-up? Instant frame rate drops and temporary HDR disablement. So, not everything works yet.  Oh and there's still random display flickering in games too. Annoying.",AMD,2026-01-22 19:37:34,1
AMD,o0w4ehw,I've never had this issue on my system.  And still on Win 10.,AMD,2026-01-21 17:41:00,1
AMD,o0vxhmj,"There was an AMD portable build, but it ran at like 1/10th the speed of the ROCm version. The ROCm version was available on Windows with the ROCm 7.1.1 release, but with the AI bundle I'd imagine you can just install ComfyUI straight without the need for any Powershell commands.",AMD,2026-01-21 17:10:03,11
AMD,o0vi6a5,"Lol... Living under a rock, buddy? It's been some time.",AMD,2026-01-21 16:01:12,11
AMD,o0vgzfe,"I remember running ComfyUI on W11 before v1.0, but my memory could be at fault, that was a long time ago.",AMD,2026-01-21 15:55:53,6
AMD,o0wom8z,Nope. It's embarassing that they have this feature broken since 25.9.2 drivers ( so like 4 months). Seems their QA department is on a permanent vacation.,AMD,2026-01-21 19:09:30,34
AMD,o0w7lbq,"Not fixed on my 6900XT. Not tested on my 9070XT but I doubt it'll be different. Granted, I only performed a restart, not a clean install. But still, it's a really really bad joke at that point",AMD,2026-01-21 17:54:56,7
AMD,o0wrwmx,No.,AMD,2026-01-21 19:24:17,5
AMD,o149017,can confirm it's still broken. sad,AMD,2026-01-22 21:01:11,1
AMD,o0v6zm1,Also curious,AMD,2026-01-21 15:09:50,9
AMD,o0w7jar,"Not fixed on my 6900XT. Not tested on my 9070XT but I doubt it'll be different. Granted, I only performed a restart, not a clean install. But still, it's a really really bad joke at that point",AMD,2026-01-21 17:54:41,5
AMD,o0wrz6j,No.,AMD,2026-01-21 19:24:37,4
AMD,o0vjk5n,One of them works just for the latest cards and one also includes effectively second version of the drivers for older RDNA 1/2 cards.,AMD,2026-01-21 16:07:29,17
AMD,o0vk6fx,"a, b, and c are different branches. C is combined branches that just includes everything.",AMD,2026-01-21 16:10:16,13
AMD,o0w98m9,"b version is just for RDNA3/4, c version includes all RDNA GPUs...so if you have a RDNA1 or 2 GPU you must use c version. Also if you have a RDNA3/4 dGPU and a ryzen CPU with integrated graphics you must use the c version.",AMD,2026-01-21 18:02:06,10
AMD,o0w7anb,"There's probably a d version also, given Polaris/Vega also got updated, lol",AMD,2026-01-21 17:53:38,4
AMD,o0vomjd,What AV1 video are you watching I would like to test this.,AMD,2026-01-21 16:30:17,4
AMD,o0w7ype,Same! I get a driver timeout sooner or later while playing BF6. I've tried most solutions and nothing has fixed it. It's either the game itself that needs to be optimised for the 9060XT or it's AMD drivers causing it. Gave up on figuring which one it is.,AMD,2026-01-21 17:56:33,7
AMD,o0wjb7s,"I had crashing issues for a long while with bf6 after playing 1 or 2 games. Turned out it was an ssd issue (Silicon Power UD90 SSD) and getting a new one fixed it completely for me. Also I found that specific ssd had issues with bf6, monster hunter, and other games which definitely me",AMD,2026-01-21 18:46:12,2
AMD,o0vq216,"Weird, I have been running 25.12.1 since the day they dropped and never had a single crash.",AMD,2026-01-21 16:36:43,1
AMD,o0wfnir,Are BF6 crashes even a known driver issue for the 9000 series?,AMD,2026-01-21 18:30:12,1
AMD,o0xpig1,Check for your memory stability. Unstable XMP can cause that,AMD,2026-01-21 21:57:18,1
AMD,o0wb7x9,I had to disable xmp for it to stop crashing so it might be something else on your system.,AMD,2026-01-21 18:10:56,0
AMD,o0vd3hq,"Yeah, Reddit looooooveees to suck off Nvidia.  Remember? First the Optical Flow sensor didnt exist on RTX30 for FG, then it existed and was all of a sudden too slow for it, then on the RTX40 it was all of a sudden incompatible to the MFG on RTX50.   Always finding new excuses lol",AMD,2026-01-21 15:38:28,28
AMD,o0veuhn,"Reddit expert here, where is my pytorch for 6000 series?",AMD,2026-01-21 15:46:24,13
AMD,o0vcy15,"Lol backlashed forced AMD to do anything to begin with.  Not to mention we still don't have FSR4 INT8, and I modded my drivers to get the unofficial verison working and yes it is very viable on RDNA2, superior to XeSS and FSR3.1",AMD,2026-01-21 15:37:45,15
AMD,o0veygo,Yeah lotta fixes... 3 if you count only RDNA1/2 exclusive. Plentiful harvest.  There is still a separation of drivers coming and they will be delegated to a maintenance branch. Call me when they add FSR4 to those older arches/,AMD,2026-01-21 15:46:53,-5
AMD,o0vbw6o,It crashes with Nvidia too. Wilds is just a dumpster fire.,AMD,2026-01-21 15:32:50,54
AMD,o0vu2jg,"For MH Wilds turn off the ML Frame gen in the drivers, that's what was causing crashing for me.",AMD,2026-01-21 16:54:42,4
AMD,o0vmgfk,Installing the reframework via nexus mods fixed the issue for me,AMD,2026-01-21 16:20:28,1
AMD,o0vd49z,For me wilds crashes if i enable framegen on driver level if not game runs fine on my 9060xt will give a try to this driver tonight and see if that still happens.,AMD,2026-01-21 15:38:33,1
AMD,o0vdydl,"Look into the mod that disables the dlc check, enjoy 75% performance boost /s  Edit: People clearly didnâ€™t take this as a joke but it is. Cant win em all on Reddit.",AMD,2026-01-21 15:42:23,-7
AMD,o0vbaot,"i reverted back to 25.9.2 a month ago for that same reason, waiting for user feedback or iâ€™ll skip this new release too.",AMD,2026-01-21 15:30:04,0
AMD,o12sree,Still good?,AMD,2026-01-22 17:04:35,2
AMD,o0v9okw,It's local llm,AMD,2026-01-21 15:22:32,8
AMD,o0v9r4i,It's for running AI models on your own hardware. like stable diffusion.  You can also subselect the parts you want. But if models are included then each one can be multiple gig's.,AMD,2026-01-21 15:22:52,6
AMD,o0wf4el,Do you even know what that bundle is for?,AMD,2026-01-21 18:27:53,6
AMD,o0w8icx,what would this AI package be? ðŸ¤”,AMD,2026-01-21 17:58:56,-2
AMD,o1enydc,u/AMD_Vik could you clarify? Thank you,AMD,2026-01-24 10:56:17,1
AMD,o0ym39t,"Cant wait to play Cyberpunk again, without crashes when changing settings. They will fix it in the next update, they sure do...",AMD,2026-01-22 00:46:14,1
AMD,o0wdi5i,"No, it's mostly for users that are not yet using it to make it easier to try.",AMD,2026-01-21 18:20:50,2
AMD,o0wpy1p,was this suppose to be in this driver there was no announcement from AMD?,AMD,2026-01-21 19:15:26,3
AMD,o0zpsw9,"FSR4 for older gens (RX 5000, RX 6000 & RX 7000) are not supported by AMD officially.",AMD,2026-01-22 04:38:51,-1
AMD,o0vq343,![gif](giphy|TcYsW9HlxtDNJ6mBoe),AMD,2026-01-21 16:36:51,19
AMD,o0wrj21,One file fix for the crash due to Nvidia SHARC being incompatible with AMD GPUs - https://www.nexusmods.com/cyberpunk2077/mods/24132 - I know you don't give a crap since you went 5080 but for anyone else that sees this and has a 9070 and wants to use PT without crashing or pink tint.,AMD,2026-01-21 19:22:35,5
AMD,o0v8sev,"Blame nVidia's SHARC, that's the sole reason.",AMD,2026-01-21 15:18:23,10
AMD,o0w5igz,https://www.nexusmods.com/cyberpunk2077/mods/25673   This mod worked for me,AMD,2026-01-21 17:45:55,2
AMD,o0w7in9,Since April or May,AMD,2026-01-21 17:54:36,1
AMD,o0vj6sa,Sorry I don't see the correlation between AI bundle and FSR 4 for RDNA 3....  What does adding AI software to the installer have anything to do with upscaling?,AMD,2026-01-21 16:05:49,13
AMD,o0v8iq2,"Yep, the first number always tells you what year the driver is from, the second number tells you what month it released in, and the last number tells you the ""number"" of the driver   so    26.1.1   is released in 2026 in January and it's the first AMD Driver in January.   If they would release another one this month it would be 26.1.2   one of the Few things I think AMD does WAY better then Nvidia does, cuz I switched to Nvidia recently and I have 0 freaking clue how their driver numbers are supposed to be read lol",AMD,2026-01-21 15:17:07,16
AMD,o0vkghj,It was apparently fixed in 25.12.1 but I can't confirm it since I don't use it. >Intermittent application freeze when using the in-game Radeonâ„¢ Overlay.,AMD,2026-01-21 16:11:30,3
AMD,o0vgu02,I bought the rx 9070 for dual use for gaming and training dl models  It was an important selling point to me the support for pytorch in windows,AMD,2026-01-21 15:55:13,16
AMD,o0vqx4y,This is for simplifying running local models optionally and actually cool instead of pushing some weird integrated thatâ€™s running back to a datacenter. If itâ€™s as promised in that linked blog post at least.,AMD,2026-01-21 16:40:33,8
AMD,o0vh77c,Of course there is demand from the consumer side. Why do you think it's not the case?,AMD,2026-01-21 15:56:51,8
AMD,o0vil35,There is a large and rapidly growing community of people and small businesses who run local LLMs for all kinds of things. Check out /r/localllama   AMD gpus offer great bang for buck for AI use. Iâ€™ve got a 7900XTX and a W7900 I use for AI everyday.,AMD,2026-01-21 16:03:04,7
AMD,o0vhw5k,"Consumers have **a ton** of demand for a lot of things with AI.  The demand for FSR4 has been all but rabid, and the amount of goodwill AMD is torching for no good reason by failing to release the INT8 model in-driver for the Radeon 7900/AI 395 GPUs is ramping up dramatically.  ComfyUI downloads will likely explode among RDNA3/RDNA4 AMD GPU owners as soon as it's not a PITA to set it up.  They just don't have any demand for 6 gigabytes of shitty chatbot or whatever the fuck Microsoft expects to get done with 40-50 TOPS in Windows while thoroughly dismantling any expectations of personal PC privacy and ownership.",AMD,2026-01-21 15:59:58,8
AMD,o0vhsle,"Probably a little percentage of consumer. But my guess is to prepare their stacks (AI included) for pro-consumer and consumer with UDNA. Honestly, this is good news.",AMD,2026-01-21 15:59:30,2
AMD,o0w316r,"Exactly, no one wants this shit. Only corp to enslave us and scammers .",AMD,2026-01-21 17:34:55,-1
AMD,o0xowdr,Report to them but expect nothing Iâ€™m trying to play Silent Hill 2 and Oblivion Remastered for two months now. Their drivers break one or the other.   I hope that the UE hardware lumen fix they mention here is kind of generic because a lot of unreal engine games have crashing issues with hardware lumen.,AMD,2026-01-21 21:54:28,1
AMD,o0vdd2z,"It is pretty much 2 installer bundled together. One for rx 5000/6000, and one for 7000/9000",AMD,2026-01-21 15:39:41,3
AMD,o0vkfbw,Can't use deeplinks to AMD driver files. Open the page of the release notes and find the link on that page. Or just use the AMD driver page and the small download that then detects which one you need and downloads less. The full 1.6GB is not really neeed for most users - only if you need to have the full offline installer for like deploying to a larger set of machines.,AMD,2026-01-21 16:11:21,1
AMD,o0zbv27,"i have 6750xt and wallpaper engine, this never happened to me.",AMD,2026-01-22 03:11:49,1
AMD,o104et4,"Yes, in the custom install you can deselect stuff.",AMD,2026-01-22 06:27:08,2
AMD,o0yzkl2,"LoL, I think they just dont care enough about it",AMD,2026-01-22 02:02:26,1
AMD,o112fup,DDU.,AMD,2026-01-22 11:31:59,2
AMD,o12qdm9,Install over. DDU only if you have issues or if the old driver is like 3 years old.,AMD,2026-01-22 16:53:46,1
AMD,o1465v3,"Same with me, i got an RX 7600, it was just fine yesterday but now it wont even open the game",AMD,2026-01-22 20:47:53,1
AMD,o15fb3g,Is this a new behaviour with 26.1.1? I'm not too familiar with PC2 but generally photo modes will enable the most computationally expensive effects for its purpose. Is this a regression or something?,AMD,2026-01-23 00:38:19,2
AMD,o16icvy,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-01-23 04:19:43,1
AMD,o0v5qax,"Upgrading the kernel rn, not sure what's new on our side",AMD,2026-01-21 15:03:47,2
AMD,o0wq8he,probably undervolted systems.  I've never had one on my RDNA 3 gpu.,AMD,2026-01-21 19:16:44,0
AMD,o0w1u7g,Why its optional not mandatory just don't install it and keep moving.,AMD,2026-01-21 17:29:34,10
AMD,o0wkonf,No its not  https://i.redd.it/g32an4491reg1.gif  You probably need to use the AMD cleanup util or DDU,AMD,2026-01-21 18:52:12,3
AMD,o1ek8jo,"Nothing you're describing has happened to me or loads of other people, you're the only one describing them. Most people in fact seem very happy with this version.Â    So it's a issue with your system or installation. At a minium you should use DDU and try again.",AMD,2026-01-24 10:22:36,1
AMD,o0wyded,"I almost forgot about non-existent support for fsr in games.  Somehow nvidia managed to drop 3.1 for older games like hogwarts legacy and whatnot.  Fsr?  One can dream  Thank ""God"" for optiscaler.",AMD,2026-01-21 19:53:40,-1
AMD,o0xhh24,"You can disable startup services. I do that with the full install for full customization and no bloatware nor adrenalin running (you can still open adrenalin but it won't load in ram at startup) :  AMDRyzenMasterSDTask  AMDNoiseSuppression  StartCN  StartDVR  AMD Crash Defender Service  AMD External Events Utility (leave this on it's needed to apply games settings, gsync etc)",AMD,2026-01-21 21:20:31,0
AMD,o0vjeze,Flags tab? What? Where can I find this?     edit: found it  edge://flags/,AMD,2026-01-21 16:06:51,10
AMD,o0wcckx,No need to disable hardware acceleration ?,AMD,2026-01-21 18:15:50,1
AMD,o10ex7s,Thank you ! I try that,AMD,2026-01-22 07:58:24,1
AMD,o11h9rz,I tried that but it immediately froze on me after trying to go to LinkedIn lol - I've switched to D3D11 WARP to see if that does anything.,AMD,2026-01-22 13:13:19,1
AMD,o0y7h9l,"I get these in Black Ops 1, been happening with my old Vega 64 and my current 9070 XT",AMD,2026-01-21 23:27:22,3
AMD,o0z69sb,"Firefox may actually be even more stable now, or less stable no issues over here yet, this was recent update on Firefox 147 so AMD was actually behind Intel and NVIDIA, no idea if it was disabled all this time because not supported, or because it was broken before and now fixed, videoplayback has been buggy in the past, and no one believed it was a driver issue until it got fixed and everyone proved otherwise.  They also testing HDR now finally on Firefox nightly  * Improved video playback performance on systems with AMD GPUs by enabling zero-copy playback for hardware-decoded video where supported, bringing them to parity with Intel and NVIDIA GPUs.",AMD,2026-01-22 02:40:15,2
AMD,o0vkq9t,"This is something else, itâ€™s either caused by the new driver branch from AMD or itâ€™s an issue from Microsofts side. This has been going on for 1 or 2 months now not sure. It seems to get more frequent as well.   Hopefully this driver will fix it.",AMD,2026-01-21 16:12:42,1
AMD,o0zyz02,"Maybe, just maybe. I've always use Edge and never installed Chrome. My GPU history is like Vega64 -> 6800XT and now 9070XT. Never had a driver timeout once. I even only know it is a thing a year ago when I saw people talking about it on reddit.",AMD,2026-01-22 05:44:00,0
AMD,o101air,"There's still the occasional issue... For instance, Hytale seems to crash on 9070 series cards on Linux without warning.  Though, the difference is, on Windows, you're SOL until the driver or the game fixes it.  In Linux, you can set an environment variable for the game (and only for the game) to load a different driver (zink) that loses a bit of performance but solves the issue, so... S'long as you're willing to crowdsource fixes, Linux just lets you fix it.  And then, yeah, come in and look at the problems people are having on windows and... Well, people act like it's hard or something but I'm legitimately putting less effort into it than the people still waiting on basic fixes on the windows side, so...",AMD,2026-01-22 06:02:02,3
AMD,o0vgrjk,"It is, once I rolled back to the 25.4.1 drivers I stopped getting the timeouts. Anything newer than that was very unstable.",AMD,2026-01-21 15:54:55,23
AMD,o0wgxqh,"No - like others if I rollback to an older update from last year, this does not happen. It could be a combination of things, but I think it's definitely a bug in the driver.",AMD,2026-01-21 18:35:52,8
AMD,o0vg0wk,So this applies to issues before this update how?,AMD,2026-01-21 15:51:38,4
AMD,o0z5kpo,"Ah, I've seen the issue outlined here except with the entire DWM with some of my crashes/hangs. Thanks for this!",AMD,2026-01-22 02:36:19,1
AMD,o0zqsja,"Its weird I had it once, and never again. Exact same driver. Frankly at this point I wouldn't be surprised its Windows and the driver not agreeing with each other once in a while.",AMD,2026-01-22 04:45:30,1
AMD,o10w3y0,I'm on windows 10 lol,AMD,2026-01-22 10:38:26,1
AMD,o0vl8l3,"Hmm... I've never seen it in Firefox though I'm using Windows 10, what card at you on?",AMD,2026-01-21 16:15:00,6
AMD,o0v4ldz,Haha your logic is good but you are horribly overestimating Microsoft :D,AMD,2026-01-21 14:58:13,16
AMD,o0v5vtv,It's on any Chromium based browser if you have hardware acceleration enabled. If you are playing any media it has a chance to freeze and cause a timeout.,AMD,2026-01-21 15:04:32,10
AMD,o0v5zb3,"Possibly, however downgrading back to October 2025 driver version resolves the issue, so I am not convinced it's just Windows",AMD,2026-01-21 15:05:00,4
AMD,o0vd43f,"Might be, but I only started getting them immediately after I switched from Nvidia to AMD last autumn.  So there is either a very specific Windows update timing that happened to overlap with my GPU change or there is an issue with AMD drivers.  Now that I think back, I don't recall getting driver freezes in January, so something might have been fixed by Microsoft.",AMD,2026-01-21 15:38:32,1
AMD,o0x8eek,"I've had TDR issues survive a full OS reinstallation, I don't think it's just a Windows problem.",AMD,2026-01-21 20:39:21,0
AMD,o0v63rq,yet some how rolling back amd updates too 25.9 somehow fixes it.... hmmm   ![gif](giphy|3o85xnoIXebk3xYx4Q),AMD,2026-01-21 15:05:35,4
AMD,o10sxqo,"that's a different issue, sometimes Adrenalin app just doesn't launch and refuse to launch until restarting or killing the process and launching again",AMD,2026-01-22 10:09:58,5
AMD,o0w2csc,"Yeah you had to switch to XESS or TAA, which wasn't ideal.",AMD,2026-01-21 17:31:52,3
AMD,o0wrqz5,"I only got it fixed with a driver rollback, still at work but gonna try the new driver and see how it goes.",AMD,2026-01-21 19:23:35,3
AMD,o13lk6w,"I played it like 8 hours ago and the game didn't update.   Also I wasn't using nor framegen nor free sync, only FSR4.  I'm gonna remove optiscaler tonight and test it again.",AMD,2026-01-22 19:12:51,1
AMD,o0wry8j,Try driver 25.9.2 the problem is not there with that driver.,AMD,2026-01-21 19:24:29,4
AMD,o0xxdej,"I I have a 4k display there's no way I can run the game at reasonable settings without upscaling. 9070xt is what I have. I can just rollback to 25.9.2 and be done with it, gonna see if this driver does it if not I'll roll back just for this game.",AMD,2026-01-21 22:35:06,6
AMD,o0yazbx,"Since updating to Windows 11 25H2 I've had so many issues with MPO on my 6700xt, the multi-monitor or even just multi-window experience is so bad. Part of me is glad others are having similar problems so I can hope that either company can adress it at some point ðŸ˜­",AMD,2026-01-21 23:46:26,2
AMD,o177zuw,"Switched to win 11 a few months and just recently disabled MPO, had the alt tab lags / freezes, but now everything runs great",AMD,2026-01-23 07:31:52,1
AMD,o0x03lk,Same.,AMD,2026-01-21 20:01:28,1
AMD,o10t995,"> I can run on Radeon cards on Windows? Si  bit out of loop here, so I can install the new AI Bundle and just install ComfyUI after that and it will work? Did not look into what that AI bundle even is yet tbh",AMD,2026-01-22 10:12:53,2
AMD,o0xib41,"Holy shit, this makes me genuinely mad",AMD,2026-01-21 21:24:18,8
AMD,o0yzmhe,"LoL, I think they just dont care enough about it",AMD,2026-01-22 02:02:45,5
AMD,o1a017u,it's been months,AMD,2026-01-23 17:54:35,1
AMD,o1491dk,sadly not,AMD,2026-01-22 21:01:22,2
AMD,o0wxdy2,thx!,AMD,2026-01-21 19:49:15,1
AMD,o0xn04m,How to download offline C version? i have been using b version despite having igpu as well.,AMD,2026-01-21 21:45:37,1
AMD,o0vpw54,Itâ€™s random. Iâ€™ll be watching YouTube and the entire tab suddenly becomes unresponsive. It also happens in Firefox with Instagram. Iâ€™m on a 9070 XT with a 9800X3D on an X870E motherboard.,AMD,2026-01-21 16:36:00,4
AMD,o0wfwaq,"For sure, I was told it was gonna be there by people on reddit but lo and behold, it's not. And it will never be. FSR4 too.",AMD,2026-01-21 18:31:15,4
AMD,o0vhp65,Im still waiting for my framegen on my rtx3000.   :( Thanks to amd i can use it on my old nvidia card.,AMD,2026-01-21 15:59:04,9
AMD,o12n7uq,"one 24 hour compile-job away-it does work, if you'd believe it.  amd doesn't want it ship it tho so",AMD,2026-01-22 16:39:38,2
AMD,o0vm9v1,">Lol backlashed forced AMD to do anything to begin with.  lol, AMD just doing what they've always done with 2 generation old GPU's. Nothings changed.",AMD,2026-01-21 16:19:39,8
AMD,o0vgi9t,You can mod a driver to get the FSR4 toggle in it even on older cards?,AMD,2026-01-21 15:53:47,2
AMD,o0vgc6k,Until u pay attention to things like ghosting smearing noise artifacting etc My homie thought his 7900 was dying until i told him to stop using fsr4,AMD,2026-01-21 15:53:02,-3
AMD,o0vjp6o,a bunch of people whining on reddit and techtubers throwing oil on the fire didn't change anything as they never said they were going to stop support.,AMD,2026-01-21 16:08:07,3
AMD,o0vkycc,They are literally just doing exactly what they've been doing for EVERY GPU they've ever made.,AMD,2026-01-21 16:13:43,2
AMD,o0vlv0o,Bug fixes are part of maintenance.  And why would you only count the exclusive ones? Their claim was those older cards wouldn't get any fixes.,AMD,2026-01-21 16:17:49,2
AMD,o0vxys1,can i ask you something? i only play newer games once in a while... do you think with a card like the 9070xt you even need fsr support for older titles? can't these cards just brute force rasterization? or are there still titles that are a few years older that benefit from fsr4,AMD,2026-01-21 17:12:15,1
AMD,o0wg20o,"Same here with a 9070 XT, unchecking those ML FSR / FG boxes fixes all crashing.",AMD,2026-01-21 18:31:59,3
AMD,o0vmkzb,Isn't that only for lobbies?,AMD,2026-01-21 16:21:01,8
AMD,o0vmq1e,"It turned out that only applies when the support desk felyne is loaded and you don't already have a DLC notification, the rest of the game doesn't benefit.",AMD,2026-01-21 16:21:39,6
AMD,o0vufdg,It only works when near the desk in the base and only when thereâ€™s a notification.,AMD,2026-01-21 16:56:17,1
AMD,o135dwp,Yes played BF6 and ARC Raiders last night two hours each zero issues.  And infact I think performance went up in ARC,AMD,2026-01-22 18:01:27,2
AMD,o0wiedh,"6GB of shit is all I know.  Honestly though, I do not. And im not bold enough to find out. Iâ€™m still on 25.9.2 cuz their stuff has been poor. I can only imagine what this does.",AMD,2026-01-21 18:42:11,2
AMD,o0wes99,Lets users more easily run Local LLMs without need of cloud or subscription on your GPU.   [https://www.techpowerup.com/review/amd-ai-bundle/](https://www.techpowerup.com/review/amd-ai-bundle/),AMD,2026-01-21 18:26:24,8
AMD,o0wab6w,A chatbot that helps you to workround driver issues. Ironic. They could have spent the effort to fix the actual fucking driver than tell users what to do with it.,AMD,2026-01-21 18:06:53,-4
AMD,o0vrthh,what's a cyberpunk?,AMD,2026-01-21 16:44:32,6
AMD,o0w699a,thanks but I replaced my 7900XTX with an 5080 last week ðŸ˜…,AMD,2026-01-21 17:49:08,1
AMD,o0v8ruv,thanks!,AMD,2026-01-21 15:18:19,4
AMD,o0vhlpx,"oh yeah, running Python ML libraries in Windowas are PITA, don't get me started with dlib + CUDA combo",AMD,2026-01-21 15:58:38,3
AMD,o0yr85o,"Question, if you wanted to run local LLMs why not get a 5070 Ti?",AMD,2026-01-22 01:14:55,1
AMD,o0w4kop,Hope you're right. Hopefully the new unified pipeline should make porting optimizations and SW side work easier. AMD can't be a serious alternative to NVIDIA with their horrible ROCm perf and lackluster FSR support.,AMD,2026-01-21 17:41:45,2
AMD,o1ejlmj,How do corp enslave you with locally run LLMs?,AMD,2026-01-24 10:16:48,1
AMD,o10my0e,"Probably something is wrong with the OS, as the Adrenalin doesn't show the average FPS in the games stats no matter which driver version I install or whether I use DDU or not. At least everything else works fine, so that's good enough for me.",AMD,2026-01-22 09:13:15,1
AMD,o10r7rs,ah yes I got it. thanks!,AMD,2026-01-22 09:54:00,1
AMD,o0z0u9h,They may as well deprecate it then since its not working anyway,AMD,2026-01-22 02:09:37,1
AMD,o161atc,"I'm not sure when the behaviour started, but I tested with 25.12.1 and 26.1.1 and they have the same issue. I've also tested with all graphics settings in the lowest possible configuration (texture to low, disabling AA, motion blur, post processing) and was able to replicate the same issue.",AMD,2026-01-23 02:40:35,1
AMD,o0v6q6q,I upgrade mine every week or two or if i feel like it and nothing ever breaks or really has a noticeable change to anything. I prefer the stability.  It would be nice to have clear changelogs though that are easy to access before downloading updates through package managers.,AMD,2026-01-21 15:08:35,1
AMD,o0x8uv1,"I've tried overvolting to fix driver timeouts and it doesn't help, 5700xt",AMD,2026-01-21 20:41:26,1
AMD,o0ymych,It's still PUSHING it. And it needs to stop. Sure have ads and links about it while it installs but it literally stops and it's front and center.   The world is enshittifcated enough.   Just....   Stop...,AMD,2026-01-22 00:50:56,0
AMD,o0wnjfl,It reappeared after reinstalling the drivers. None of the buttons were present.,AMD,2026-01-21 19:04:42,1
AMD,o134kbt,Ohno amd fanboys attacked me cause im right xD  Cpu is the only thing worth buying from them. Amen or smth,AMD,2026-01-22 17:57:50,0
AMD,o0xinb6,"AMDRyzenMasterSDTask  AMDNoiseSuppression  StartCN  StartDVR  AMD Crash Defender Service  AMD External Events Utility (leave this on it's needed to apply games settings, gsync etc)    100% useless.       This is a minimal install >>> [**https://ibb.co/9H52DYrL**](https://ibb.co/9H52DYrL)     I re did the test with 26 1 1 again and selected driver only, it still install the full bloated shit (same as a full install) , gG AMD.",AMD,2026-01-21 21:25:48,-2
AMD,o0vpqwr,"As I use chrome, I type chrome://flags in the address bar, it could be different bases on your browser",AMD,2026-01-21 16:35:20,4
AMD,o0wdx5w,No,AMD,2026-01-21 18:22:38,2
AMD,o1064cj,"There's probably some variance based on your GPU's stability, it's base and boost clocks, monitor setup, state of your windows install, and your RAM/VRAM. XMP/EXPO will exacerbate pretty much any of them. It was definitely a problem for a while",AMD,2026-01-22 06:41:21,1
AMD,o12tpgt,"Imagine using Edge and sharing every click, cookie, passwords, history and key typed with directly Microsoft servers. Wow. I thought people were not this submissive.",AMD,2026-01-22 17:08:58,1
AMD,o1193ea,tbh we'll get mesa 26.1 pretty soon. Maybe it will fix it.,AMD,2026-01-22 12:21:08,2
AMD,o0vm17g,"win11 25h2, 7900xtx. sometimes its one single frame corruption (once per 30 minutes or so), then, eventually, card freezes for solid 2 minutes, then screen goes black, then i see TDR & bug report tool appearing. turning off acceleration fixes both issues.",AMD,2026-01-21 16:18:35,8
AMD,o0v4we2,That's also true,AMD,2026-01-21 14:59:42,1
AMD,o0vavik,"Aggressive undervolts can cause this, including on cpu. Also, bad soc voltage can also do this.",AMD,2026-01-21 15:28:06,-1
AMD,o0v6z1m,"I was getting some intermittent driver timeouts on system boot which never seemed to actually affect my system performance or stability, but I haven't any pop up since the latest Windows 11 update last week.",AMD,2026-01-21 15:09:46,0
AMD,o11gglt,I had that issue i think the second time windows update ripped out my driver software. Most recently I didnt realize that windows had done it again because it updated my GPU driver for the 9070XT to the internal driver but my CPU a 7700X still held onto to a 25.10.1 driver with the software that I also didnt install myself.,AMD,2026-01-22 13:08:33,1
AMD,o0ws6yj,Does FSR4 upgrade of Avatar frontiers of Pandora fsr3 work in that driver version?,AMD,2026-01-21 19:25:34,2
AMD,o0yean6,"Even if you dont run upscaling, you can still lower resolution.  I know it's not normal for PC players to understand, but you dont \*have\* to run things at native resolution.    Sure, it'll take an image quality hit, but again, you still do have the option to play the game in any meantime.  It's not totally unplayable.",AMD,2026-01-22 00:04:11,0
AMD,o1101dc,From what I've seen it's literally plug and play as in just download comfyui by selecting it from the bundle and it installs it with ROCM 7.1.1 setup for you. Now you just need to download the models and workflows and put them in the right folder (tpu article shows where this is).,AMD,2026-01-22 11:12:21,2
AMD,o12dps3,"You have two options:  1. Install the AI bundle including ComfyUI. It's an older version, and you can't add custom nodes, but it works. 2. Install the AI bundle (to get PyTorch) and separately install the ComfyUI desktop app. There's an option at install where you basically say ""I have AMD/ROCm"" and it works at native speeds on your card, plus it's the latest version.",AMD,2026-01-22 15:57:09,1
AMD,o16hbh8,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",AMD,2026-01-23 04:13:13,1
AMD,o0xpo63,"Go to the release notes [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-26-1-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-26-1-1.html)  Scroll down where it says ""The AMD Software: Adrenalin Edition 26.1.1 installation package can be downloaded from the following link:"" and check there, it should be a link to 26.1.1 drivers. That link is the c version. Can't post direct link as it doesn't work, you have go there in your browser.",AMD,2026-01-21 21:58:01,2
AMD,o0vr3oc,I watch twitch on a regular on edge with my machine hasn't seen this.  Firefox is my primary browser and I do watch alot of youtube on this one nothing yet.  I'm still on Win 10 also.,AMD,2026-01-21 16:41:21,6
AMD,o0vjb9v,"Amd being promising rocm on windows for years, and now they give support only for 7000 and 9000...",AMD,2026-01-21 16:06:23,9
AMD,o0wk6bl,DLSS Frame Generation was never in the cards for RTX 30.  AMD's FSR3 FG is not the same technology. Frame pacing issues alone disqualify it compared to DLSS FG on RTX 40 and newer.,AMD,2026-01-21 18:50:00,5
AMD,o0wr0em,"tf you mean, you can use fg on ur card xd",AMD,2026-01-21 19:20:14,-1
AMD,o12q0po,I saw people being compile pytorch for rdna2 themselves since preview drivers. Radeon just proves what they hate own customers,AMD,2026-01-22 16:52:09,1
AMD,o0x2zrb,"No. You can DLL swap FSR 4 INT8 into the game directly, but for RDNA 2 you have to mod the driver too to get it to work right. With RDNA 3 you don't",AMD,2026-01-21 20:14:46,9
AMD,o0voq6i,"That was never the claim. Neither from AMD, nor from Reddit.  AMD always stated that they will continue updating the driver but RDNA1/2 are moved to maintenance mode, and that game support will continue. Those things didn't really change, the only thing that Reddit achieved was getting AMD to acknowledge them better.  RDNA1/2 are still in maintenance.  I counted only the exclusive ones because the generic ones imply that the change happened outside of arch specific parts of the driver.",AMD,2026-01-21 16:30:44,0
AMD,o0vzfqz,"Plenty of titles benefit from proper upscaling, especially with RT involved. Cyberpunk 2077 was released in 2020 and is still one of the best looking and heavy games out there. Thanks in no small part to the continued support.  You're looking at the problem from the wrong side: older GPUs are limited to garbage upscaling tech like FSR 3 and older (or XeSS which isn't great either), AMD released the RDNA1/2 compatible version of FSR4 which looks better than 3 and XeSS, but they're yet to acknowledge it or provide it in the driver as a DLL override.  Those older GPUs would benefit greatly from proper upscaling.",AMD,2026-01-21 17:18:52,2
AMD,o0x6p2q,I just upgraded to 26.1.1 do i need to disable both AMD FSR Upscaling and AMD FSR Framegen? Isn't the first one required to get FSR4 in FSR3+ games?,AMD,2026-01-21 20:31:35,1
AMD,o0vn034,It also wasnâ€™t 75% lol  It heavily depends on your specific setup esp. your CPU.,AMD,2026-01-21 16:22:56,7
AMD,o0wqtgr,It's preconfigured Local LLM tools. If you want to run a LLM model on your PC using your GPU. No cloud or subscription needed. There are people that wanted this even in this sub.  It's not related to the AI chatbot in the driver.,AMD,2026-01-21 19:19:22,9
AMD,o0wejxa,Wrong. It for running local LLMs.,AMD,2026-01-21 18:25:23,9
AMD,o0wp33b,Just making stuff up.,AMD,2026-01-21 19:11:34,5
AMD,o0yrbgy,Bruh,AMD,2026-01-22 01:15:26,1
AMD,o1ej9fi,Proof yet again that nvidia deliberately sabotaging AMD gets rewarded every time.,AMD,2026-01-24 10:13:44,1
AMD,o0wl8y4,wise choise,AMD,2026-01-21 18:54:39,1
AMD,o180tev,Price and tdp since I have a 520w psu  And I'm not running llms but training some image model so I needed vram but it's not like it's some 24/7/365 business running,AMD,2026-01-23 11:48:03,2
AMD,o16iazd,Can you roll back and tell me the last known good driver?,AMD,2026-01-23 04:19:23,1
AMD,o0xjejk,Is it timeouts while playing if so how many hrs in,AMD,2026-01-21 21:29:11,1
AMD,o0yxays,Nothing is pushed you have to click on the download link next to the item for to action anything......     You sound like you need to take a chill pill,AMD,2026-01-22 01:49:37,3
AMD,o0wee2g,"Thank you. I have a stupid question. If I'm playing games and have a Chromium tab running in the background, supposedly YouTube, will it affect my gaming performance/ cause lag freezes without this settings along with hardware acceleration? I have 7900XTX",AMD,2026-01-21 18:24:40,1
AMD,o108t12,"In my previous and current setup, I always enable XMP/EXPO and slightly overclock on CPU. For AMD GPU I mostly run stock, not even UV. Perhaps it is just I happen to run none of the program or game that causes driver timeout.",AMD,2026-01-22 07:04:05,1
AMD,o15coi8,"If you are already using windows, I donâ€™t see the difference",AMD,2026-01-23 00:24:31,2
AMD,o0y3dsc,"exact same issue, Windows 11, 7900xtx, firefox. I'll minimize a window, click on taskbar to reopen and it will stall for 15 seconds, black screen, then timeout. happens multiple times a day and then nothing for a few days",AMD,2026-01-21 23:05:37,2
AMD,o1014wf,Go in about:config and change **media.hardware-video-decoding.enabled** to False and see if you crash anymore. You can keep hardware acceleration on with that flag changed.,AMD,2026-01-22 06:00:49,2
AMD,o0vbva6,"I'm just using stock voltages and like I said in one of my other posts, if I revert back to the October 2025 driver the issue disappears - it could be \*something\* in Windows causing it but I think it's the driver.",AMD,2026-01-21 15:32:43,2
AMD,o0wsfgi,Yes it does.,AMD,2026-01-21 19:26:37,3
AMD,o0ygqdm,"I know I can drop the resolution but I don't want to, 4k FSR quality renders at 2k but looks miles better than native 2k on a 4k display.   I can play the game with older drivers as well like I'm doing now, the drivers before the one are the one's that broke the game. If the new ones don't work I'll stick to the old ones until I'm finished with the game.",AMD,2026-01-22 00:17:16,2
AMD,o0xvpsf,Thanks!,AMD,2026-01-21 22:26:53,2
AMD,o0yj455,I'm on win11 25h2.,AMD,2026-01-22 00:30:09,1
AMD,o0x5ek3,"You mean amd frame gen, right? Lol",AMD,2026-01-21 20:25:46,3
AMD,o12q6yl,yeah-i'm one of them. works great.,AMD,2026-01-22 16:52:56,1
AMD,o0yioeo,"it was absolutely the claim from reddit experts all over the place, and i got downvoted every time i said AMD would still be providing game fixes.",AMD,2026-01-22 00:27:46,1
AMD,o0w4fj7,"hmm from what i'm hearing is the performance hit is too great in doing what you're saying...but knowing amd, this could just be a money grab.  i still remember when they said older am4 boards can't use 5000 chips because of bios limitations lol",AMD,2026-01-21 17:41:07,0
AMD,o0x7jwe,"Just go in the drivers and click on the game icon for MH Wilds, and (if they are checked) uncheck the boxes where it says ML FSR / Redstone and ML Frame Gen.  I havenâ€™t seen this issue in any other game so Iâ€™m assuming itâ€™s safe to leave these options on globally.  FYI these are the settings that tries to convert FSR Upscaling and FG to the new ML Redstone model, if a game supports it.",AMD,2026-01-21 20:35:30,1
AMD,o0wcgyq,The hub doing stupid stuff still hammers even bleeding edge CPUs and sees notably lower perf. Though idk if that's all the DLC check crap.,AMD,2026-01-21 18:16:22,1
AMD,o0wxfvh,Thanks for that info. Helpful to know that. Appreciate it.   Main takeaway though is that itâ€™s check ON for default installations - or was. So make sure you do custom install and toggle it off if not interested.,AMD,2026-01-21 19:49:29,1
AMD,o10cu4b,"Okay, I was wrong.Just saying, not everyone wants LLM. But a card like 7900 could definitely benefit from having FSR 4 ported to it.AMD got the priority wrong.",AMD,2026-01-22 07:39:48,1
AMD,o0yl7ex,How do you know a user is actually using Nvidia.  Just show them this post lol,AMD,2026-01-22 00:41:24,4
AMD,o1ezdn2,"na me switching had nothing to do with the cyberpunk bug, you can't use PT properly anyways on a 7900XTX unless you wanna enjoy the muddy mess that is FSR3 performance mode",AMD,2026-01-24 12:32:35,1
AMD,o0xomqc,"varies greatly, sometimes it's pretty fast, 30mins, sometimes its way longer, like 10h+  i also don't restart often  happens way less with recording disabled",AMD,2026-01-21 21:53:12,1
AMD,o0wf874,"I have a 7900xt and I don't see any performance degradation, maybe 1-2% depending on the game you will try, as the browser uses more CPU than GPU",AMD,2026-01-21 18:28:21,2
AMD,o0vf5mo,Why are you downvoting me? Im just sharing something that happened to me. Lol,AMD,2026-01-21 15:47:45,1
AMD,o0yll1m,"Fair enough yea.  Not trying to defend the state of drivers, just saying that maybe FSR breaking shouldn't totally make a game 'unplayable' when you're still rocking something as powerful as a 9070XT.",AMD,2026-01-22 00:43:29,1
AMD,o0w54li,"BIOS limitations were kinda real but not unsolvable.  As for the performance hit, yeah it's larger than FSR3, but you can still claw back a ton of performance while keeping the image quality okay",AMD,2026-01-21 17:44:12,2
AMD,o0xi0w3,"Thanks, just tried the game with only AMD FSR Upscaling enabled in Adrenalin and it seems to work fine, the game complains when it starts since the recommended driver is still the older one i was using (25.9.2) but so far so good, no weird stuttering or crashes, managed to do a couple of hunts.",AMD,2026-01-21 21:23:01,1
AMD,o0wys5q,Here is a review for each of them if you are interested   [https://www.techpowerup.com/review/amd-ai-bundle/](https://www.techpowerup.com/review/amd-ai-bundle/),AMD,2026-01-21 19:55:31,4
AMD,o0y6u07,I barely restart as well but don't have this problem I'm on driver 12.1. did u switch from Nvidia or anything?,AMD,2026-01-21 23:23:53,1
AMD,o0wfjdh,That's awesome. Thank you so much.,AMD,2026-01-21 18:29:41,1
AMD,o0wfkk4,That's awesome. Thank you so much.,AMD,2026-01-21 18:29:50,1
AMD,o0w9faf,"Just so you know, on Reddit if you post additional information or causes people will see it as you claiming it's the only cause.  People are very very binary and literal here sometimes. It can be exhausting if you fight it.",AMD,2026-01-21 18:02:56,4
AMD,o0ymesm,"I only play one game at the time so a driver rollback is fine for me, but I hope this one is ok because I wanna try the AI slop myself ðŸ¤£",AMD,2026-01-22 00:47:58,1
AMD,o0zr74c,"The fact that it's an AMD-sponsored title and it's broken is truly wild though.  I understand OP's plight. Rather than going down to 1440p and dealing with scaling issues when sitting at your desk, just roll back the drivers. I weep for someone who plays a lot of game and has a 1440p monitor though (like me) and relies on FSR. Both 1080p and 720p look AWFUL on a 1440p monitor",AMD,2026-01-22 04:48:16,1
AMD,o0xr00m,"With 26.1.1 I could load up and play MH Wilds with the ML Frame gen,   I also had the popup at the start for not being the optimal driver but it's working fine for me.",AMD,2026-01-21 22:04:13,1
AMD,o1hy7lf,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-01-24 21:14:12,1
AMD,o1hzwuw,We will see if there is any reason to have this chip. If a game accidentally goes on the other ccd there has to be a latency/performance hit.,AMD,2026-01-24 21:22:14,50
AMD,o1iiwax,"Be hold it cometh.     Finally the first non-epyc cpu with more than one V-Cache. One for the productivity guys. AMD isn't releasing this for gamers, its for the workers. Hence not at CES.",AMD,2026-01-24 22:53:42,6
AMD,o1iv0zc,This is taking way too long.,AMD,2026-01-24 23:56:40,5
AMD,o1jl1w6,"They should name it 9960X3D or something, the 2 after 3D sounds weird",AMD,2026-01-25 02:16:42,2
AMD,o1j724s,"just when I thought I could wait for zen 6, we get zen 5 2",AMD,2026-01-25 00:59:27,1
AMD,o1i7jx2,"Why would they give it this name instead of 9960x3d?  Edit - I don't care about downvotes, but what a pointlessly toxic thing to do. People are just broken inside, I think.",AMD,2026-01-24 21:58:20,-11
AMD,o1ioqr6,Does not even matter when nobody can afford RAM.,AMD,2026-01-24 23:23:47,-2
AMD,o1i4edi,"We'll have to wait for new schedulers and X3D drivers that will somehow *keep* all threads from one process on one CCD, but still fairly distribute them across the whole CPU to get a performance increase.",AMD,2026-01-24 21:43:28,22
AMD,o1ifvoy,Yes this chip does nothing to address inter CCD latency.,AMD,2026-01-24 22:38:34,18
AMD,o1jcb63,"IDK, the latency penalty on the on-X3D CPUs with multiple dies hasn't been massive. The 9950X is slightly ahead of the 9700X in Techspot's testing, for instance.",AMD,2026-01-25 01:28:51,7
AMD,o1isdi4,Itâ€™ll likely depend on the title.   Thereâ€™s a possible reality where a game loads 4 cores per ccd and thereâ€™s just tons of cache,AMD,2026-01-24 23:43:00,3
AMD,o1ii2db,"The whole purpose of this is fixing whatever was wrong with dead 9950X3D CPU's. They know, they just don't wanna admit they screwed up.",AMD,2026-01-24 22:49:31,-4
AMD,o1j1940,"Unlike the 95% of corporate ai slop presentations at the Consumer Electronics Show, right? :)  Not even sure gaming got more than 2-3 slides total across all presentations there.",AMD,2026-01-25 00:28:55,6
AMD,o1jysz9,X4D.,AMD,2026-01-25 03:34:03,2
AMD,o1i9v07,That name would probably have some people thinking it was an X3D version of the 9960X.,AMD,2026-01-24 22:09:15,11
AMD,o1it1ci,"Personally I thought the ""2"" was to represent it having 3D v-cache on both CCDs, so it makes sense from that aspect.",AMD,2026-01-24 23:46:25,5
AMD,o1jciih,> Edit - I don't care about downvotes  I don't believe you.,AMD,2026-01-25 01:29:59,6
AMD,o1it9gb,"Or 9950XT3D , 9950XTX3D",AMD,2026-01-24 23:47:35,-4
AMD,o1iu9kh,thats not how that works. each thread still needs to communicate and has to go through the infinity fabric to communicate if they are on seperate ccds.,AMD,2026-01-24 23:52:44,2
AMD,o1j9atx,"Yeh, stupid AI.   But the 9950x3D2 is not about gamers. AMD has specifically said it isn't going to be of interest to gamers AT ALL.   It is specifically for those doing CFD, electronic design, etc and it can make things 50-100% faster in those specific workloads.",AMD,2026-01-25 01:11:56,4
AMD,o1k2ei6,XD,AMD,2026-01-25 03:55:21,1
AMD,o1ial3x,So then give it another name.   There is already a 9950x and a 9950x3d,AMD,2026-01-24 22:12:44,-11
AMD,o1iwuk1,"So I know all of that... That's really basic stuff and the overwhelming majority of people on this specific subreddit are aware.      \-----  And there's nuance. It REALLY depends on the task.   You end up in a situation where there's a trade off between needing to go through the IF more for core to core communication vs needing to go all the way to system memory for memory pulls.   If you have a task where memory bandwidth and latency sensitivities are VERY high and the need for communication between threads is relatively low then the task would work better spread across two CCDs with tons of cache. One hypothetical would be background tasks (windows, etc.) on their own CCD and whatever minor spill over there is from a game also living there.  The ""burden"" that comes with CCD to CCD communication isn't THAT high.      One example that somewhat conveys this - R5 5500 vs R5 3600. The 3600, despite having 2 CCXes (read: requires that hop you mentioned when going past 3 cores worth of workload), the 3600 is generally faster since it has 2x the L3 cache - this is also in spite of the many improvements Zen 3 brings over Zen 2.",AMD,2026-01-25 00:06:12,4
AMD,o1k80uc,"Threads donâ€™t necessarily need to communicate though. How chatty threads are depends on the program, but universally for best performance you want as little communication as possible.  I can absolutely see the extra cache compensating, but it will depend on the program",AMD,2026-01-25 04:30:22,1
AMD,o1k1ypv,Can you share some benchmarks? Genoa-X never got 50-100% gains over Genoa with same core count that Iâ€™ve seen anywhere,AMD,2026-01-25 03:52:48,1
AMD,o1ibwau,"That's why it's called 9950X3D2...  TBH, AMD painted themselves into a product naming corner starting with Zen 2.  Remember, some not-so-tech-savvy customers thought that the 3200G and 3400G were Zen 2 parts.",AMD,2026-01-24 22:19:02,18
AMD,o11cvj7,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-01-22 12:46:25,1
AMD,o11i741,Can't wait to pair it with 4 gigs of ram,AMD,2026-01-22 13:18:39,170
AMD,o11kg3z,Have AM5 CPU sales dropped off with the RAM shortage? If I'm so surprised prices haven't fallen a little. I am thinking of upgrading to a better AM5 processor but I ain't paying that.,AMD,2026-01-22 13:31:18,30
AMD,o11nal0,Can't afford it cuz..well.. Ram prices.,AMD,2026-01-22 13:46:45,14
AMD,o14jcuj,How does this compare to the Ryzen 9 9950X3D?  I'm still unclear as to why the Ryzen 7s are better than Ryzen 9s for gaming?,AMD,2026-01-22 21:50:47,4
AMD,o12gg5b,Did Amazon drop earlier than they should have? I already ordered one no issues.,AMD,2026-01-22 16:09:33,2
AMD,o13vusb,Wonder of they fixed the 9800x3d self killing feature?,AMD,2026-01-22 19:59:42,2
AMD,o16bbt4,why are 90% of the posts on this sub by OP,AMD,2026-01-23 03:37:16,2
AMD,o11vrzx,I've been needing a reason to RMA my Asrock PG Lightning X670E.,AMD,2026-01-22 14:30:55,1
AMD,o13hct6,"If I'm in the market for a new CPU to pair with a 5080, should I bite the bullet on this or stick to an older one like 9800X3D or maybe 7800X3D?",AMD,2026-01-22 18:54:10,1
AMD,o1534jw,Can anyone help me with my pc I upgraded to a ryzen 7 5800x cpu with a radeon 6600 gpu is a setting wrong because it loads very slow on startup about 5-8mins,AMD,2026-01-22 23:34:19,1
AMD,o16vsde,Is this the fixed 9800X3D?,AMD,2026-01-23 05:51:47,1
AMD,o11dc6z,I dont know why they product this cpu. 9800x3d is enough,AMD,2026-01-22 12:49:22,-5
AMD,o12sctq,"Sweet, already ordered on amazon!  my new to me build is now going to be complete, swapping the 9900x for the 9850x3d, and will just sell off the 9900x to recoup some of the cost.",AMD,2026-01-22 17:02:43,1
AMD,o11mqvf,Massive nothingburger.,AMD,2026-01-22 13:43:50,-1
AMD,o11p4ha,"Why does anyone even bother with this site.  Hey! Hey! Look over here! Clickbait Nonsense!.... (Bunch of useless words)... Oh, BTW, this doesn't actually mean anything until we have official pricing.",AMD,2026-01-22 13:56:27,0
AMD,o13oeds,Amazon listed in stock now: [https://instockalert.io/us/ds/amd-ryzen-7-9850x3d](https://instockalert.io/us/ds/amd-ryzen-7-9850x3d),AMD,2026-01-22 19:25:40,0
AMD,o12mnnj,"Really wonder if I should upgrade my 3 month old 9800X3D while these ""exist"" before Ai shitshow makes them impossible to find and probably delays the Zen6 into 2027  Edit: Low, found the poor people that downvote",AMD,2026-01-22 16:37:09,-6
AMD,o1453jl,Should I buy this for the 60 series? Or should I hold on out for zen 6,AMD,2026-01-22 20:42:51,-2
AMD,o11iyg3,Richie McRich over here with 4 whole gigabytes,AMD,2026-01-22 13:22:59,64
AMD,o11zhv6,It has 96 MB of cache.  Just run Windows 95 from CPU cache to save on RAM costs [*taps forehead*],AMD,2026-01-22 14:49:46,33
AMD,o11k2x7,Don't advertise that. It's liking walking around a mugging hotspot wearing a Rolex.,AMD,2026-01-22 13:29:16,13
AMD,o12pvhw,I plan to install Windows 95 so that I can finally leverage my 256 MB kit and be a boss,AMD,2026-01-22 16:51:30,3
AMD,o11lg6r,I snatched 9950x3d for 20% a few weeks ago on amazon.de (sold & fulfilled by Amazon).,AMD,2026-01-22 13:36:50,17
AMD,o11pab8,sadly they are unrivaled at that price point (or even higher price point) for gaming. So they will sell regardless of the price. You can find punctual deals though.,AMD,2026-01-22 13:57:17,7
AMD,o124v0e,Dunno about across the stack but I'd expect the most price consistency with x3D chips. They're the one line-up that can be paired with the worst RAM you can find and still perform good. As long as the RAM isn't unstable you're good to go even with terrible timings.,AMD,2026-01-22 15:15:46,3
AMD,o1324xt,"Microcenter is still cleaning house, especially with the 7600x3d.",AMD,2026-01-22 17:47:07,2
AMD,o14j7tp,"Perhaps other models (9700, 9600, etc.) will drop in price; but the 9000X3D chips are the best gaming CPUs in the world so are going to command a premium because of that.   Same reason why you see the 5700X3D/5800X3D doubling or tripling in price recently given the RAM shortages while other AM4 chips like the 5600 and 5800 remain cheap.",AMD,2026-01-22 21:50:07,2
AMD,o1264ti,motherboards lost like 50%,AMD,2026-01-22 15:21:52,1
AMD,o15pguv,"got my kid a b650 + 32 GB RAM + 9800X3D for $600 at Microcenter recently, was decent",AMD,2026-01-23 01:34:37,1
AMD,o14c65z,I didn't realize at the time I got a bargain about 9 months ago  Â£800  For a 7950X Corsair 1000w PSU motherboard   And ...... 192GB of DDR5 5600Mhz RAM   Looking back I struck gold,AMD,2026-01-22 21:16:15,4
AMD,o14ey5f,Get a job then,AMD,2026-01-22 21:29:27,-8
AMD,o1ej34y,"There are constraints on having an extra 8 cores on the CPU such as cooling, voltages, additional strain on the infinity fabric, etc.   And the nature of AMDs chiplet design for Ryzen is that all chiplets are generally made equal, and games rarely use more than 8 cores.   The only way the 16 core would be faster is if AMD artificially slowed down the 9800. And they kind of do, by using better bins on the 9950, but not enough to matter.   Unlike GPU where performance scales pretty linearly with core count, on CPU most gaming performance waits on your fastest single CPU core to complete its task.",AMD,2026-01-24 10:12:10,1
AMD,o12myl1,where?,AMD,2026-01-22 16:38:29,1
AMD,o12uuif,"I ordered one as well, though estimated delivery isn't till feb 5th",AMD,2026-01-22 17:14:09,1
AMD,o16qgvk,Nope,AMD,2026-01-23 05:13:15,0
AMD,o14k92l,Unless the 7800X3D has a nice discount I would go for the 9800X3D. It's quite a bit faster which you will appreciate in CPU bound titles.   9850X3D is just a binned 9800X3D so will be barely faster - maybe a few percent (just the clock speed difference).,AMD,2026-01-22 21:55:06,3
AMD,o153guj,I've disabled things that I don't need to startup,AMD,2026-01-22 23:36:05,1
AMD,o11dxdo,"This is 9800x3d, a better binned version sold with a premium.",AMD,2026-01-22 12:53:08,30
AMD,o11iyk9,"This isn't for people who already own a 9800x3D. Its a better binned 9800x3D and if people want it, they get it.",AMD,2026-01-22 13:23:00,10
AMD,o11efc4,"If yields improved enough that they can create a faster SKU, why not? Obviously nobody is going to replace a 9800X3D for this one. But someone doing a new build might go ""I'll pay a few bucks extra for the higher binned one"".",AMD,2026-01-22 12:56:12,12
AMD,o11ftki,Is there a point? Not really.  Do companies like money? Shockingly yes.,AMD,2026-01-22 13:04:44,9
AMD,o11h6r1,"we live in a time where people love to have the best, and this is it.... too many have the 9800x3d",AMD,2026-01-22 13:12:49,4
AMD,o11m0y7,Why wouldnâ€™t they if they have the dies and can sell them? You already know there are people who will upgrade from their 9800X3D just because.,AMD,2026-01-22 13:39:58,3
AMD,o1241to,9800X3D KS,AMD,2026-01-22 15:11:55,3
AMD,o11fdmx,"It's prolly another 3300x case going on here, where AMD had extra silicon that was decently high quality yield, but not large enough for 12 or 16 cores, so we get this.   Personally if a person waited I'd grab this as this will be something that sells real high as the years go by.",AMD,2026-01-22 13:02:02,1
AMD,o11fnj7,What?,AMD,2026-01-22 13:03:43,1
AMD,o11fevk,\> I dont know why they product this cpu. 9800x3d is enough  That doesn't make any sense. Enough for what and whom?,AMD,2026-01-22 13:02:15,-4
AMD,o138ie7,You should! Give corporate overlords $500 for 3% speed boost.,AMD,2026-01-22 18:15:21,4
AMD,o12qq5m,"There is no reason to upgrade to this unless you're a hardcore overclocker and the speculation that it'll have a binned IMC too turns out to be true.  That's the only reason even the enthusiasts, myself included, would consider it.  Otherwise just wait for Zen 6, whenever that releases.",AMD,2026-01-22 16:55:20,2
AMD,o14kkyv,datacentres/AI don't give a shit about X3D chips. They are buying RAM/VRAM and GPUs. AI compute is not done on a CPU and they'll be purchasing Xeon/Epyc CPUs anyways to get the number of PCIe lanes that they need for maximum GPUs,AMD,2026-01-22 21:56:41,2
AMD,o11zsb1,4 whole BILLIONS of bytes!,AMD,2026-01-22 14:51:12,12
AMD,o11jb6h,"Yeah, windows 7 runs like a charm",AMD,2026-01-22 13:24:56,16
AMD,o11o4af,If they could do that for one of the lower x3d models that would be great. I'm not spending 100e more than a 7800x3d just to get a slightly faster 9000 series version.Â    German Amazon usually has the best prices of all the Amazons for me but still more expensive than other German stores.,AMD,2026-01-22 13:51:12,5
AMD,o133w01,Maybe one day they'll expand into Europe ðŸ˜¢,AMD,2026-01-22 17:54:53,3
AMD,o14jnci,Depending on the RAM configuration and how much you actually need you might be able to sell 2 stick (96GB) and make a good chunk of change.  32GB is all your really need for gaming these days but 64GB is the safe number. Many other workloads can obviously go beyond that though and I won't presume to know your workloads.,AMD,2026-01-22 21:52:11,1
AMD,o12suug,https://a.co/d/bzzHacw shipped and sold by Amazon,AMD,2026-01-22 17:05:02,2
AMD,o132cn5,Because is supposed to release the 29th (embargo ends 28th) so Amazon just decided to go first.,AMD,2026-01-22 17:48:05,1
AMD,o15psud,"When you upgraded, did you format your PC?  Itâ€™s highly recommended to format your PC after a GPU upgrade, specially if you went from Intel/Nvidia to AMD and vice versa.",AMD,2026-01-23 01:36:32,1
AMD,o120m2k,"Maybe someone can clarify, but isn't 9800x3d a single CCD while 9850x3d is a dual CCD?  If that is true, is there still weird gaming issues with game swapping CCDs?  not trying to argue, just trying to learn.  My work buddy says dual CCDs suck at gaming unless you do weird kernel and driver hacks.  He's an intel fan boy though so a bit skeptical.  I have 9800x3d for gaming and it's great.",AMD,2026-01-22 14:55:13,-1
AMD,o11pxh6,I think a good amount of people will upgrade from a 9800X3D cause they are not exactly broke already and want the best of the best. Anyone that does not have an unreasonable amount of cash to spend on pc parts will just keep buying Ryzen 5s or the 7800X3D.,AMD,2026-01-22 14:00:41,-11
AMD,o12qc4b,> Obviously nobody is going to replace a 9800X3D for this one  Are you sure about that?  --John Cena,AMD,2026-01-22 16:53:35,2
AMD,o11p3n7,"I am the situation of building a pc with 5090. And waiting for the 9850x3d. So you recommend it over 9800x3d in this case?   Also why do you think it will sell real high later on? I mean there will be zen6 too, for AM5. So there will be a  better cpu for AM5 later on. Or am I missing something?  Also how was the situation with the 3300x?",AMD,2026-01-22 13:56:20,1
AMD,o13ccgn,I'll sell 9800X3D for $400 so I honestly only give them $100 in my view,AMD,2026-01-22 18:32:06,-1
AMD,o14noqm,For now....,AMD,2026-01-22 22:12:15,1
AMD,o11rv17,"I thought the 9000 series x3d was a massive upgrade over the 7000 series which was only a slight upgrade over 5000 or did I get that wrong  Edit: even if I'm wrong, with the way hardware is going, in a year the current 9950x3d price being half the price of the 7800x3d would but surprise me",AMD,2026-01-22 14:10:44,6
AMD,o1h8r8e,I assume you are playing cpu dependent games?,AMD,2026-01-24 19:16:24,1
AMD,o1358pd,"yup, just pointing out you can buy them now, but will have to wait till feb to get it, they are not shipping anything out yet.",AMD,2026-01-22 18:00:48,1
AMD,o12349r,"No, 9950x3d2 should have 3D-V cache on both CCDs; 9850x3d has the same internal architecture as the base 9800x3d.  The other part is also false; technically, it used to be true, but schedulers on both Windows and Linux have been improved in the meantime, so there are no such problems anymore.",AMD,2026-01-22 15:07:25,9
AMD,o125dms,"> I think a good amount of people will upgrade from a 9800X3D cause they are not exactly broke already and want the best of the best.  Those people aren't the sharpest crayon in the box. I ended up turning on full eco-mode because the gains at higher clocks in most the applications I do were next to non-existent. Slightly higher clocks over that is underwhelming as hell for the money.   As is par for modern consumer parts everything is pushed out of its optimal efficiency curve for that last couple percent that doesn't matter at all, at the cost of way higher heat and way higher powerdraw.",AMD,2026-01-22 15:18:14,3
AMD,o11wa3v,"""best of the best"" brother in christ the only competition to the 7800X3D is the 9800X3D, the latter so overkill for gaming its insane",AMD,2026-01-22 14:33:33,5
AMD,o11vxah,"Usual case with 'final cpu' for a gen, they tend to stay high value for years to come. Just look on ebay for a 4790k devils canyon cpu, prolly still going for near $200.  This one will also be binned better and prolly handle memory OCs better as well, to which the 9800x3d was known to have a bit of a weaker memory controller.  3300x was just a really cheap 4c8t cpu that AMD put out for a short while to get rid of some silicon they had lying around. It was popular cause for like $80 you had a cpu that could do some pretty decent gaming.  Knowing AMD as well, when they're trying to get rid of old silicon and make these products, they don't stay around long at all. It's a one and done, so that rarity will make them worth all the more down the line.  Say you get this now and upgrade 5 years later, you could prolly sell it still for $300+.",AMD,2026-01-22 14:31:41,3
AMD,o13a78r,"Oh I bought mine already. I am hoping they already release early lol. However, I havenâ€™t seen a bios or chipset update to support these yet.",AMD,2026-01-22 18:22:44,1
AMD,o13ly95,"ok, thanks for the reply.  So it's basically the exact same layout, just faster clock speeds?  Both are dual CCDs?  I have 9800x3d and never had a problem with it, so 9850x3d would work the same, just a bit faster?",AMD,2026-01-22 19:14:38,1
AMD,o127cgn,I couldn't agree more,AMD,2026-01-22 15:27:45,1
AMD,o12ffda,"""Overkill for gaming"" you say as my PC chugs into late-game strategy game runs :(",AMD,2026-01-22 16:04:54,3
AMD,o120if4,"Thanks for the detailed answer. Yeah I'm just meaning this wont be the final ""gamer"" cpu for this gen cause of zen6 will arrive too for am5 later on this year, or am I missing something? There should be another better x3d for am5 beside 9850x3d, Im just saying.  I think I will go for 9850x3d now, anyway. Of course I wont wait for another year (have already everything else).",AMD,2026-01-22 14:54:44,1
AMD,o11w8cy,"You sure about that? I mean obviously I'm aware it's AM4 vs AM5, it's what's kept me on my 5800x3d, but my (very tuned) system benchmarks very close to median 7800x3d, 13000 time spy CPU score compared to 14000-15000. However, 9000 series are on another level in performance",AMD,2026-01-22 14:33:17,6
AMD,o13d335,"oh dang, didn't think of that. They will come i'm sure after the announcement, I'll just have to update my bios before popping it in.",AMD,2026-01-22 18:35:20,1
AMD,o16d7il,any board that already supports 9800x3d will support these just fine,AMD,2026-01-23 03:48:16,1
AMD,o1acw8z,Theyâ€™re saying shopping until feb but when it comes out (Jan 29th) theyâ€™re gonna ship the same day.,AMD,2026-01-23 18:51:51,1
AMD,o121b1m,"Yea, there's zen 6 which is supposed to be the last for AM5, least that's the rumors. But still this will be a gens king CPU so hence why it'll hold most of it's value.",AMD,2026-01-22 14:58:34,1
AMD,o12b7tu,My 9800x3d was a pretty large 25-30% jump in the games I play over my 7800x3d.,AMD,2026-01-22 15:45:51,6
AMD,o1aeav4,"Knowing Amazon, they ship once they have the item in hand. Considering they open pre orders a week before everyone else too",AMD,2026-01-23 18:58:09,1
AMD,o121o2v,thank you,AMD,2026-01-22 15:00:19,1
AMD,o12f6nm,"Yeah that seems about right when comparing gaming benchmarks like time spy CPU scores  I think the fact that the 5000 to 7000 series being a am4 to AM5 jump highlights how big of an improvement the 9000 series is over the 7000 series  Even with the platform jump of am4 to AM5 and everything that comes with it (ddr5 ram, better mobos), the average score and highest score of the 5800x3d and the 7800x3d are closer than the average and highest of the 7800x3d and the 9800x3d, but the 9000 and 7000 are obviously both on AM5 so the 9800x3d isn't pulling that advantage with the jump from ddr4 to ddr5 as wind at its back",AMD,2026-01-22 16:03:47,3
AMD,o12ju1q,Which games are those? Because that might actually convince me to go for the 9800x3d.Â  I thought it was pretty close but I should have checked in more depth to verify. I thought it was only a few % quicker in certain games/Resolution? I should have informed myself better.,AMD,2026-01-22 16:24:40,1
AMD,o12p7o8,"I've been playing Escape From Tarkov mostly since I got it, but you'll see similar results in games that are very CPU bound and/or cache sensitive. Most games that are GPU bound will get a nice bump to 1%lows, even if they don't get a max fps increase.   Using Cinebench r23 as an example, my old 5800x3d scored ~15,000, the 7800x3d scored ~17,000, then my 9800x3d scored ~23,500.",AMD,2026-01-22 16:48:32,1
AMD,o141qnj,"I saw more than 20% gains on average, but BG3 and Satisfactory are in the 30%'s.",AMD,2026-01-22 20:27:03,1
AMD,o15igzh,Ok I will give a round of applause to amd. Instead of pricing this to like 550 or even 600 which they definitely could get away with. They actually priced this to an acceptable level. Yeah the 9850x3d is just a better binned 9800x3d but for an extra 20 bucks that's not even an outrage price lol. They could have pulled an intel with the ks level price gouge but they didn't. Good job amd.,AMD,2026-01-23 00:55:12,167
AMD,o161xmi,I hope they fix the burning CPU mystery.,AMD,2026-01-23 02:44:01,56
AMD,o15nlyp,I hope it does need to be paired with RAM,AMD,2026-01-23 01:23:59,35
AMD,o180gza,"Time to build new PC, oh wait the memory nm......     GG to AM5 owners already :D   Best decision was to jump AM5 some time ago when memory prices were good.  As far we know AM6 is way off and next gen CPU's are also on AM5.",AMD,2026-01-23 11:45:25,11
AMD,o15kzdl,"Nice, glad that they're not charging a huge premium",AMD,2026-01-23 01:09:11,20
AMD,o17bdni,Great! Now i just need some ddr5 ram...,AMD,2026-01-23 08:02:00,5
AMD,o185ww9,This is like the first bit of good news from a PC parts manufacturer in a long while. Respect,AMD,2026-01-23 12:24:45,5
AMD,o16n1kz,"This is a good MSRP, not sure if it holds but I was expecting them to make it like $520 at least. Good job AMD. Not sure the retailers will follow suit though.",AMD,2026-01-23 04:50:00,2
AMD,o19hrpz,My question is whether it will still be stable at CO -30. The low temps and power usage make 9800x3d extra comfy for me.,AMD,2026-01-23 16:30:58,2
AMD,o15w8e1,Price makes sense for the product stack.,AMD,2026-01-23 02:12:40,4
AMD,o15zk7m,Going to be great when scalpers buy them out to flip for twice the price,AMD,2026-01-23 02:30:59,5
AMD,o187m3p,"I want one, but I'm definitely reluctant sitting on the X670E Steel Legend. Sure, older board, but I'd be forced to upgrade my BIOS for compatibility, and I just don't want to risk frying anything in this market.",AMD,2026-01-23 12:36:10,1
AMD,o1acge5,I just bought it ðŸ˜,AMD,2026-01-23 18:49:52,1
AMD,o1e4cky,RAM prices will force AMD to price aggressively and/or limit their consumer allocation for CPU's. RAM prices are killing the whole consumer hardware ecosystem.I expect Zen 6 sales will disproportionately be for upgrades vs earlier gen launches.,AMD,2026-01-24 07:56:38,1
AMD,o1htqyy,I wonder if it's worth selling my 7800X3D for this. I'm playing at 4k so probably not.,AMD,2026-01-24 20:53:02,1
AMD,o17ga65,It goes for 750 - 999 euro in Spain.,AMD,2026-01-23 08:46:54,1
AMD,o16gzvg,"This could be the most power efficient gaming CPU ever, I would just set FMAX to 5350 or so with a reduced voltage and watch it sip less power than an undervolted 7800X3D while running faster than a fully stock 9800X3D (matching a PBO+100 setting). Still can't find any mention of a ""binned"" memory controller anywhere though.",AMD,2026-01-23 04:11:15,-4
AMD,o16zaik,R.I.P 9800x3d,AMD,2026-01-23 06:19:02,-5
AMD,o16kbhs,"Agreed. If my 9800x3d dies, Iâ€™ll RMA, sell it & buy the 9850x3d. But if it never dies then Iâ€™m good too, 7% isnâ€™t worth the hassle.",AMD,2026-01-23 04:32:14,34
AMD,o18ghcr,"Thing is, the 9800x3d is no longer at MSRP so this isn't *really* just +Â£20 but more like +Â£120 which is a much harder sell",AMD,2026-01-23 13:29:42,7
AMD,o17p41p,$499 is fine. do I expect most people will be able to buy it for the msrp.. nope ðŸ¤·â€â™‚ï¸,AMD,2026-01-23 10:09:20,3
AMD,o15yjb4,They could have just sold it as a 9800x3d and won public support by selling a good product without segmenting the best of it and inflating prices. Expect more from your massive corporations please.,AMD,2026-01-23 02:25:21,-48
AMD,o1705wa,"It's the same chips, just binned harder. It won't fix anything",AMD,2026-01-23 06:26:01,33
AMD,o16f9me,asrock,AMD,2026-01-23 04:00:43,-1
AMD,o16jym0,> I hope asrock fix the burning CPU mystery.  ftfy,AMD,2026-01-23 04:29:56,-5
AMD,o15p81l,Or a GPU..,AMD,2026-01-23 01:33:12,16
AMD,o18pz9k,Yeah at this point Iâ€™m just gonna wait until the AI bubble pops and prices come crashing down. Until then Iâ€™m rocking my 5900x which honestly has been great for like 98% of the games Iâ€™ve thrown at it.,AMD,2026-01-23 14:19:46,5
AMD,o1h5opb,"I built one anyway, my wallet is weeping. Fkinh ram was the worst hit, followed by the gpu and then the ssd (at least lucked out on getting the newest G5 kingston for almost the original price while budget trash was more expensiveâ€¦)",AMD,2026-01-24 19:02:56,1
AMD,o19mnoe,"I got the lambo battery, now I just need the car",AMD,2026-01-23 16:52:47,2
AMD,o1621a4,nah no one  will buy it if there are no ram,AMD,2026-01-23 02:44:34,18
AMD,o1eagp5,where?,AMD,2026-01-24 08:52:10,1
AMD,o1jvz6d,"I wonder the same thing, but mainly for VR use. I love my 7800x3D, but more power is more power haha. If it's 10% or better, it might be worth it.",AMD,2026-01-25 03:17:40,1
AMD,o17yx4v,"Lol, I've got mine 9800X3D for ~440 Euro and that's with VAT.",AMD,2026-01-23 11:33:21,1
AMD,o173hwm,This chip is probably not even 5% faster,AMD,2026-01-23 06:53:13,4
AMD,o1dqy53,I still is tho at microcenter and Amazon itâ€™s like 470,AMD,2026-01-24 06:01:16,1
AMD,o1dt0hx,Both Amazon and Newegg in the US have the 9800x3d at $469 in the US. So it's a $30 difference here,AMD,2026-01-24 06:18:01,1
AMD,o16j5p7,"With expectations like that, youâ€™ll always be disappointed.",AMD,2026-01-23 04:24:47,26
AMD,o16jmsi,"A SKU has a specific configured default performance target with specific guarantees. If you have a different target configuration then you have different guarantees and therefore a different SKU. The point of binning is if some chips can't make the better guarantees then instead of throwing it away you offer it as a cheaper product.  Yes, they could have potentially given it the same price with a ""subject to availability"" disclaimer, but it was always going to be a different part number for legal reasons if nothing else.  Unless you meant the original 9800X3D should have been a 9700X3D?",AMD,2026-01-23 04:27:50,11
AMD,o1j2dyv,"What's the point of winning ""public support?""",AMD,2026-01-25 00:34:45,1
AMD,o173mao,"It even runs faster now, and likely hotter",AMD,2026-01-23 06:54:15,22
AMD,o19tw18,There are rumors of a new stepping which would mean itâ€™s better silicon. If that is true it will attain higher frequencies at similar voltage and temperature.,AMD,2026-01-23 17:26:32,5
AMD,o16lz8r,Its not just asrock. Asus has been having cases pop up recently,AMD,2026-01-23 04:43:04,17
AMD,o15qo33,Or SSD,AMD,2026-01-23 01:41:31,18
AMD,o18roqd,Yeah. I went to AM5 and X3D for stable fps and better low % fps. It does boost quite a lot depending game if its heavy CPU. 9800 wasnt that big vs 7800. But 5800X3D to 7800X3D was quite noticeable,AMD,2026-01-23 14:28:27,2
AMD,o1ef90b,Amazon ðŸ¥¹,AMD,2026-01-24 09:36:27,1
AMD,o19ihfr,"9850/9800 = 100.51%  ######So 0.51% faster, in fact.",AMD,2026-01-23 16:34:09,1
AMD,o16ylsd,Sorry for being an optimist and a conscientious consumer.,AMD,2026-01-23 06:13:38,-14
AMD,o1bghi3,The 9800x3d should be sold as a 9800x3d. Not as a 9850x3d. Inarguably.,AMD,2026-01-23 21:57:09,1
AMD,o19uo3d,It's the same B2 stepping,AMD,2026-01-23 17:30:07,1
AMD,o16p3jh,Yes but asus sells higher volumes than other mobo companies. They sell significantly more volumes than asrock. Trying to say a few examples of 9800x3d failing on asus boards are equal to the massive amount of asrock ones is silly. Like asrock makes up a bigger portion of failures than any other companies. Trying to compare the two to each other is stupid.,AMD,2026-01-23 05:03:50,12
AMD,o15zeg7,or with food and shelter,AMD,2026-01-23 02:30:06,9
AMD,o1c6hu9,![gif](giphy|BmmfETghGOPrW),AMD,2026-01-24 00:11:44,1
AMD,o1cgqjb,So basically just don't have a separate SKU with different f/v curve & clocks?,AMD,2026-01-24 01:07:49,1
AMD,o1cki4u,Proof?,AMD,2026-01-24 01:29:43,1
AMD,o16tqdy,"Do you realise that asrock is actually very intertwined with asus? To the point where they actuslly share some workers, namely the ones who work on the bioses",AMD,2026-01-23 05:36:31,5
AMD,o164j2c,![gif](giphy|GSE1BzJG4JVbq),AMD,2026-01-23 02:58:00,4
AMD,o1cl1n8,"Yes. If its the same product but just higher quality silicon, just sell it as it is and win back public support by making a pro-consumer move.",AMD,2026-01-24 01:32:55,1
AMD,o177mj9,That actually points to it being a more asus/asrock problem if those two are seeing the most problems then.,AMD,2026-01-23 07:28:37,7
AMD,o183moz,You can rent more in the cloud,AMD,2026-01-23 12:08:45,3
AMD,o1a8bgb,"Yes in the ram apocalypse, X3D becomes even more important because you can get away with buying the most basic ram that you can find without losing any meaningful performance.",AMD,2026-01-23 18:31:30,661
AMD,o1a9m28,"""average retail price $400""  That's crazy how badly the ram market is fucked at the moment.",AMD,2026-01-23 18:37:14,151
AMD,o1ar5au,"Yeah, it's very simple - larger L3 cache means fewer calls to the system RAM. Fewer calls to the system RAM means less dependency on its speed.   It's not just 9850X3D thing, it's basically a given for any X3D CPU.",AMD,2026-01-23 19:58:05,56
AMD,o1aap0x,store on mbord ram \~78 nano second access/retrieval     store on x3d cache on 9850x3d cpu \~7 nano seconds access/retrieval   with todays ram prices amd really got a extra win there,AMD,2026-01-23 18:42:06,31
AMD,o1ac5sj,"Same on my end. The only negative impact is on 0.1% lows on games that have sections where I'm actually CPU bound. In WQHD, that happens next to never.",AMD,2026-01-23 18:48:35,8
AMD,o1bzsez,"That's quite the marketing BS. But I'm sure you can cherry pick and find titles where it really is less than 1% or make yourself GPU bound.  But in reality even with X3D RAM speed can have a large impact on performance, even if less than with regular CPUs.",AMD,2026-01-23 23:35:26,9
AMD,o1a7eoh,Sounds about right.  Never seen much from RAM outside of Geekbench.,AMD,2026-01-23 18:27:25,41
AMD,o1ac8l9,Isn't lower latency more significant than speed for 1% lows?,AMD,2026-01-23 18:48:55,5
AMD,o1acm92,"I found a ""deal"" on some Corsair 128GB DDR5 6400 off of Facebook Marketplace for $800 but the timings are horrendous (CL42-52-52-104) and I was worried I was handicapping my 9950X3D.  Worries were very unfounded.",AMD,2026-01-23 18:50:36,4
AMD,o1aku14,"Cheaper Chinese DDR5 when? I know they are a couple of process nodes behind, but with current prices they could still turn a major profit.",AMD,2026-01-23 19:28:36,10
AMD,o1aflzg,"Well I've tested EXPO 6000 CL30 vs tuned 6400 CL30 both 1:1 in Space Marine 2, R. Evil 4 with ray-tracing, B Gate 3, Helldivers 2, TLoU2 and many others and there's a significant improvement in 1% low fps, my 9800X3D is set at PBO +100 scalar x1 and per-core CO with harmonized core voltages, tested at 1080p high/ultra settings. Maybe the 9850X3D is different but now I kinda want to test 4800 vs 6000 on my CPU.",AMD,2026-01-23 19:04:08,10
AMD,o1b2ckj,Until the 10800x3d comes out with 12 cores and rh e we more efficient memory controller and we start to see 20-30% of difference between 6000 and 8000,AMD,2026-01-23 20:51:06,3
AMD,o1btkdw,"Waiting for the 10850X3D, hopefully my 6000 CL30 won't bottleneck those 12 fat Zen6 cores too much. Save me X3D!",AMD,2026-01-23 23:02:23,2
AMD,o1h97il,"Fun fact - massive L3 cache is also why DDR3 is still a viable option with minimal performance impact vs DDR4 for X99 Xeon CPUs as well. That is quickly becoming a new meta in the budget space, especially with new X99 boards supporting TPM 2.0 chips.",AMD,2026-01-24 19:18:25,2
AMD,o1b6d94,"Lies.  I saw a ~2% difference in CS when cpu limited on my 7800x3d between 5600cl26 32/32/64/96 and 6000 cl28 36/36/72/108, so as the 9850x3d is supposed to reach much higher clocks thatâ€™s just not believeable.",AMD,2026-01-23 21:10:08,4
AMD,o1b81x6,make sense to me. ppl are no reason to use high tier ram with x3d .,AMD,2026-01-23 21:18:03,1
AMD,o1bixnq,"it makes sense because x3d offloads a lot of data fetching/querying to L3 cache from memory, which heavily reduced the stress on memory side.",AMD,2026-01-23 22:08:48,1
AMD,o1bol1r,"I run my 6000 kit at 4800 because anything above it is not stable.   7950x3D, latest bios.   Maybe I'm just bad with computers or I lost hard on the silicon lottery. Idk",AMD,2026-01-23 22:36:52,1
AMD,o1bp08q,I gave up on ram oc anyway. Most of the time xmp/expo made the system unstable in the long term.,AMD,2026-01-23 22:38:59,1
AMD,o1bwm4w,"I'd believe this. Benchmarking my only DDR5 CPU, the i5 13400f (yes not AMD sadly, but anyway) I managed to overclock Samsung 4800mhz DDR5 stable (+custom RAM heatsinks) up to 5400mhz with tight timings. Was benchmarking faster/better than my Corsair Vengeance XMP 6000 RAM!!   Even made the Top 10 in CPUz  for multiple placings in the world for months, but have since lost that #10 title.",AMD,2026-01-23 23:18:32,1
AMD,o1cfbtd,"so, is anyone gonna read the missing fineprint of what gpu is used? that difference is only believeable if they're running in a **gpu bottleneck**",AMD,2026-01-24 00:59:43,1
AMD,o1dg27z,"If the expanded L2 Infinity Cache rumor is true, this will also lower RAM speed requirements. L2 is an actual write-back working cache. Large victim caches are nice, but the sizes should be balanced between L2 and L3 to stop L1+L2 from relying too heavily on slower L3 (albeit still an order of magnitude faster than RAM). Sign me up for 4MB L2 + 16MB L3 per core, keeping a nice 1:4MB ratio between them. So, a 12-core would have 48MB L2 + 192MB L3 total. That's wild!  It'd be nice if we could use a physical CXL RAM storage cache that is something like NVRAM or MRAM because if RAM is slow, NVMe drives are still barely improving random reads/writes where we'd see the most performance improvement. Those high peak, deep queue throughput numbers kind of don't mean much. I thought DirectStorage would be better too. Instead it stutters like the rest of Windows features. Gah!",AMD,2026-01-24 04:42:09,1
AMD,o1dj5o7,"Between DDR5 4800 and 5200 on x3d at 1080p, how much difference am I suppose to see? Because on 7950x3d and RTX 5080, I didn't see any difference in my gaming benchmarks.",AMD,2026-01-24 05:03:23,1
AMD,o1dmg2w,"Yeah this is just false. Absolutely disingenuous.  What sort of FPS are we talking here? Average FPS or .1% lows? How is 6000mhz configured, is it in sync with the memory controller or is it in 2:1 mode? What resolution and in game settings are the games being run at?  Don't fall for this marketing BS, yes its true that x3D is good if you don't have fast ram, in fact it's great. But if you are running RAM that slow, it will impact your average FPS and especially your 1% and .1% lows which I would argue are more important for a consistent feeling gaming experience that your averages.  Man I hate marketing misinformation.",AMD,2026-01-24 05:26:46,1
AMD,o1duri6,Idk about this one I get better performance to hit consistently above 480hz when I overclock my ram vs when its not overclocked it dips significantly.,AMD,2026-01-24 06:32:46,1
AMD,o1e133a,ITT people buying AMD marketing without any question when we have actual ram scaling benchmarks on 9800x3d,AMD,2026-01-24 07:27:25,1
AMD,o1eabkd,good thing i'm running 4800 CL50 lol,AMD,2026-01-24 08:50:54,1
AMD,o1eg4it,Probably RAM manufacturer are a cartel and saw the RAM shortage miles ahead,AMD,2026-01-24 09:44:34,1
AMD,o1eoj2p,Nothing about timings.,AMD,2026-01-24 11:01:35,1
AMD,o1eve3u,"Given these RAM prices, AMD should entice its customers by reducing its prices rather than continuing to charge top dollar for a CPU that is essentially mid-range (there are at least six other, faster consumer CPUs out there for various workloads).  Not gonna happen though.",AMD,2026-01-24 12:01:09,1
AMD,o1f2ts4,"Can confirm. Using DDR5-5600 ECC at 5400 (better timings yield better performance at this freq). In benchmarks, I am within noise relative to DDR5-6000 results.",AMD,2026-01-24 12:57:31,1
AMD,o1f550d,"I have 32gb of ddr5 6000.  For the longest time I didn't know about expo and it was running at 4800 or something (CPU is a 7700x).  I enabled expo, and task manager is reporting it's now at 6000  I've not noticed any difference whatsoever either in gaming or development.  So I'm not sure it's just a x3d thing.",AMD,2026-01-24 13:13:11,1
AMD,o1f7b67,"Right now its not worth it to spend on expensive ram but next gen they might improve the memory controller, and if they do and the ai bubble hasn't burst yet we'll be in a funny position.",AMD,2026-01-24 13:26:59,1
AMD,o1f9hhu,anyway to know the difference with a 7950x3d on rendering?,AMD,2026-01-24 13:40:16,1
AMD,o1g2sr4,"I would very much not be surprised if that was the case. On my 7800X3D I get slightly better performance ( especially 1% lows ) using my CL30/6000 RAM kit at 4800 using Buildzoid's ""easy timings"" than by using the EXPO 6000 profile without the easy timings.  MT/S doesn't matter much in most games, what matters are the timings and latency.",AMD,2026-01-24 16:12:59,1
AMD,o1gyhmm,guess they never played Escape from Tarkov,AMD,2026-01-24 18:32:11,1
AMD,o1h6n34,"Within 1% difference? There are a ton of (gaming) benchmarks stating otherwise. Also what is the CL latency when you make these comparisons AMD? The MT/s is only one aspect of the performance of RAM.  It has gotten to a point where you cannot take anything that AMD, Nvidia or Intel says anymore and just wait for people to do actual testing.",AMD,2026-01-24 19:07:07,1
AMD,o1avemq,Seriously tired of the reddit hivemind telling people to buy DDR5-6000 with their X3D in this day and age. You might as well throw money out of the window.,AMD,2026-01-23 20:18:15,1
AMD,o1afnix,Latency is more important than Bandwidth for Zen,AMD,2026-01-23 19:04:20,1
AMD,o1b1442,its def more than that,AMD,2026-01-23 20:45:17,1
AMD,o1d7qsj,"Cherry-picked single-player games at 4K? Sure, I'll buy the \~1% number.  CPU heavy multiplayer games (WoW, Star Citizen, Escape from Tarkov etc.) at 4800C40 vs. 6000C28 tuned? Around 20% higher average framerate and \~20-40% higher 1% lows. Massively superior stutter-free experience, even with X3D chips.",AMD,2026-01-24 03:47:29,1
AMD,o1a9lsy,unless it's soc. Paying for RAM speed is not worth of it.,AMD,2026-01-23 18:37:12,0
AMD,o1a6qx6,Then weâ€™ll just need to go even further beyond,AMD,2026-01-23 18:24:28,0
AMD,o1ampap,And 640KB of memory ought to be affordable for anyone nowadays,AMD,2026-01-23 19:37:28,0
AMD,o1e6f4s,"Zero chance my 5800X3D will be replaced until my AM4 system hits 10+ years old, especially with current prices.  It feels a lot more capable now than my X58 system (with X5650) felt when it was the same age in 2016, so it should be easy.",AMD,2026-01-24 08:15:05,42
AMD,o1adnxo,this is AMD marketing take all claims with a grain of salt. Remember there video cards can do 8k resolution wink wink nudge nudge please don't double check the numbers.,AMD,2026-01-23 18:55:17,141
AMD,o1c3soh,Time to use sodimm to dimm adapters.,AMD,2026-01-23 23:57:04,1
AMD,o1akyz4,It will only get worse.,AMD,2026-01-23 19:29:15,40
AMD,o1bsmio,"And on purpose, Ram production could ramp up way faster if they wanted to. But even the ram business fears a bubble. Yes they have years of contracts currently... but it's questionable how man of those will survive and how many of those will continue to come in. It's just too risky for them.",AMD,2026-01-23 22:57:25,9
AMD,o1dcfsk,"It's marketing bs though, yee X3D buffers better than lower cache CPUs, but the headline from this post is absolutely incorrect or taking cherry picked games because there absolutely is a bigger difference than just 1-2%, *especially* if you tube ram yourself.",AMD,2026-01-24 04:17:51,10
AMD,o1evycj,Yeah but these ram calls are still needed. Avg fps might be not affected but large cache wonâ€™t help against 1% lows,AMD,2026-01-24 12:05:43,1
AMD,o1a7yjr,"On your CPU(9900X), ram speed can make a big difference to performance",AMD,2026-01-23 18:29:54,34
AMD,o1adg29,Yes. After reaching 6000 you want to get CL (and the other timings and sub-timings) as low as possible while keeping your system stable. Disclaimer: you cannot RAM-timing your GPU-bound 1% lows away.,AMD,2026-01-23 18:54:18,6
AMD,o1akfx2,Itâ€™s gonna be fine.,AMD,2026-01-23 19:26:45,3
AMD,o1adpn8,Higher timings still can be bad.,AMD,2026-01-23 18:55:29,-1
AMD,o1aowcq,"Most likely eaten up for their own internal market, like local hyperscalers or big datacenters, plus Chinese consumer demand.",AMD,2026-01-23 19:47:44,15
AMD,o1cd1mz,"Imo even for China, it's gonna take time. They don't setup a single factory, they build a whole logistic chain to produce thing.",AMD,2026-01-24 00:47:03,3
AMD,o1dpvqr,after they have used it for their own uses probably,AMD,2026-01-24 05:52:55,2
AMD,o1beiem,"You put so many details and numbers but described the most important part in just one ""significant"" word. Because from what I've seen in tests and my personal usage of various dram kits with 7800X3D, the avg difference is between 3 and 10%, which is not significant compared to their prices even before shortage. Some people now can save hundreds and lose their 5% FPS in 1% lows(in CPU bottleneck tests), and in reality it will still be GPU bottleneck in most games except for online FPS or cybersport.",AMD,2026-01-23 21:47:53,9
AMD,o1bghc0,"If the application doesn't overflow cache and hit memory, or isn't notably punished for doing so, there is effectively no difference between speeds for the average fps. AMD is correct about this, games don't care much under that specific circumstance.  Whats particularly important is latency though. Most applications \*are\* punished for overflowing cache, which the 3d chips have a bit of an advantage about, and the latency hit is what kills the 1% or 0.1% lows.  AMD doesn't seem to say anything about the specs of each kit beyond the speed. Its possible both kits have relatively similar latency. A 4800 CL24 has nearly identical latency as a 6000 CL30.  Theres a techpowerup article somewhere about Zen5 memory scaling using a 9950X and even that shows less than 10% going from 6000 CL28 to 4800 CL40 so you could close that gap pretty easily with better timings.  (Alternatively just benchmark at a higher resolution, reducing the memory impact. Which they've again seemingly not specified but I'll assume 1080p.)",AMD,2026-01-23 21:57:08,3
AMD,o1binl5,> tested at 1080p high/ultra settings.   Thing is outside of esports most people are not buying $500 CPUs to play games at low res.,AMD,2026-01-23 22:07:27,3
AMD,o1j6vb6,crazy seeing ddr3 builds come back but hey if they are actually viable why not :D at least that's still affordable lol,AMD,2026-01-25 00:58:24,1
AMD,o1dm8vl,Huge sample size you've got there.,AMD,2026-01-24 05:25:18,2
AMD,o1c2kf4,"What's the CL rating for those sticks? You can probably easily run them at 6000 by reducing the timings a bit. If the sticks say 6000 you should easily be able to reach that speed with the default EXPO profile, iirc all Zen 4 chips are rated for 6000mhz at MCLK=UCLK. If you're not getting that there's gotta be something wrong",AMD,2026-01-23 23:50:24,1
AMD,o1dv4k3,"Techspot found a 4.5% average/8% 1% lows difference between 6000MHz CL40 and 4800MHz CL40 on a 7950X3D. For CL30 it was 7% and 11%.   When 32GB of DDR5-6000-CL30 was only Â£35 more than 4800 RAM, it did make sense. Less than 3% of total build cost for 7% extra perf was a no brainer.Â    Doesn't make nearly as much sense now that there's a Â£100 price gap.",AMD,2026-01-24 06:35:49,2
AMD,o1bo2eb,In real life scenarios they canâ€™t even tell the difference between a 4800 CL40 and 6000 CL30 but they will keep telling you 6000 CL30 is the only way to go and rest is garbage. Hivemind is really pathetic,AMD,2026-01-23 22:34:15,1
AMD,o1ed4jq,"Not wrong. I tested it myself for my 9800x3d. Good test is Shadow of tomb raider in-game benchmark (full game not demo as diff engine versions and they benchmark vastly diff). This game is extremely sensitive to ram oc when you put all lowest to make yourself 0% gpu bound.    I ran stock jdec speeds so 4800 vs tightly tuned 6000cl28 and the average fps was 20%+. This was at 1080p.   Of course as you increase resolution, and as the average fps is lower overall plus if a game isnâ€™t cpu fully cpu bound, the fps gain difference decreases.   For 1440p or 4k single player games sure ram mostly irrelevant. Play any competitive games at high fps? It matters, a lot.",AMD,2026-01-24 09:16:47,2
AMD,o1abv1n,"Paying for ram speed on Intel is absolutely worth it, just not worth on x3d chips as the big cache is doing the heavy lifting.  On intel there is a huge fps difference between 4800 and 6400.",AMD,2026-01-23 18:47:15,-1
AMD,o1bcnmt,Its not funny anymore,AMD,2026-01-23 21:39:22,3
AMD,o1ac2ol,No shit,AMD,2026-01-23 18:48:12,4
AMD,o1ad6ir,Itâ€™s about the X3D not some APUs,AMD,2026-01-23 18:53:07,4
AMD,o1iy6om,"Yessir, AM6 gang unite",AMD,2026-01-25 00:13:09,1
AMD,o1jhq46,"It's completely understandable!  BUT, I upgraded my 5700X3D to a 9800X3D, and the difference is unbelievable",AMD,2026-01-25 01:58:36,1
AMD,o1g5hle,my 6600k from 2016 is still in use. Works fine.,AMD,2026-01-24 16:25:12,1
AMD,o1aete5,We already know from tests that the X3D CPUs of every generation suffer significantly less performance degradation when using JEDEC RAM,AMD,2026-01-23 19:00:29,199
AMD,o1aj8uf,"We have years of benchmarks with 5000X3D, 7000X3D, and 9000X3D, corroborating the claim that RAM performance, doesn't greatly affect overall performance in gaming, with an X3D CPU. Why are you insinuating we don't, and AMD's claim isn't something we can immediately verify?",AMD,2026-01-23 19:21:05,95
AMD,o1c31pr,Most cards can do 8k resolution these days. That doesn't mean you'd really want to play a game at that resolution.,AMD,2026-01-23 23:53:00,4
AMD,o1bb7vc,"To be fair, they can do 8k, just not fast",AMD,2026-01-23 21:32:45,6
AMD,o1akh5x,"We've seen what happens with 3dcache even on DDR4.  For X3D, the timing is what matters and makes most of the difference. Where we do see data transfer/bandwidth matter on system RAM for X3D is when you run out of VRAM by gigabytes.",AMD,2026-01-23 19:26:55,8
AMD,o1ai7ep,I love how the PS5 boxes had 8K all over them while running the equivalent of an RX 6700,AMD,2026-01-23 19:16:10,9
AMD,o1d7jrw,they can do 8k! even 3080 Ti can do 8k gaming on some particularly well-optimized games and dlss performance.  i play all my gaming at 32:9 2160p which is half the pixels of the 8k 16:9 standard. smooth as butter.,AMD,2026-01-24 03:46:16,1
AMD,o1ecjqa,"I donâ€™t see the problem. I CAN do 8K, just not to most peoples expectations. It outputs 8K if asked, itâ€™s just low settings and low framerate hehe",AMD,2026-01-24 09:11:25,1
AMD,o1er8ql,"I forgot to turn on EXPO after UEFI update. Ran some tests on JEDEC 5200 MT/s CL40. Then turned on EXPO and reran the same thing with 6000 MT/s CL30. On 9800X3D I saw a difference of 0.3%.  The workload needs to be memory bound to make a larger difference and with gaming it rarely is. The only way I managed to have it memory bound is with VRAM thrashing, but that's an unplayable experience to have my FPS tank to 28-30 while the textures are loading. ReBAR and EXPO helped, but this is something that's best to be avoided.",AMD,2026-01-24 11:25:48,1
AMD,o1kbf1z,lol the 5090 gets like 12fps 8k even on good optimized software,AMD,2026-01-25 04:51:21,1
AMD,o1bdt9r,"I was thinking of switching to AM5 but then the RAMpocalypse happened, I just got myself a 5700X3D which I luckily found a brand new, tray type at around 180 bucks. Also glad I was able to upgrade to 32gb DDR4 earlier last year. Just need to upgrade to 9070XT before GPU prices goes up.",AMD,2026-01-23 21:44:40,25
AMD,o1ar67g,"Glad i built my system last summer, the 32GB 6000 c30 corsair kit i bought has quadrupled in price since then",AMD,2026-01-23 19:58:12,9
AMD,o1bf7le,Will it? Arenâ€™t investors back tracking on AI? Open AI isnâ€™t doing so good either. Not to mention people protesting against data centers.,AMD,2026-01-23 21:51:11,4
AMD,o1f4x2x,is it too risky or is it on purpose? choose one,AMD,2026-01-24 13:11:44,1
AMD,o1dz1j1,I am interested in those 30+ games in AMD's first party benchmarks as well as the memory timings.,AMD,2026-01-24 07:09:28,3
AMD,o1a944b,"The thing to note here is that this is an X3D chip. The extra cache does a pretty good job at smoothing over RAM-related performance issues.  If this were a non-X3D chip, RAM speed would matter a lot more.",AMD,2026-01-23 18:35:03,29
AMD,o1a97lw,"It's much less important on X3D chips because the L3 cache stores stuff that would otherwise need loaded in and out of ram at much less of latency penalty.  It's why X3D cpus are so good for gaming, bc L3 cache is much faster than even the fastest system ram.  A regular 9700x would probably lose 10 percent of its performance using shitty basic jedec spec ddr5.",AMD,2026-01-23 18:35:28,10
AMD,o1ciozh,"It's paired with a 6700 XT. Strictly in gaming performance, their GPU is going to be the bottleneck by a very wide margin.",AMD,2026-01-24 01:19:09,1
AMD,o1eiqtk,"Nah, not really.    At absolute most in games at 1080p where youâ€™re specifically cpu bound 5-7% at MOST.   In the 99% of cases where your gpu bound itâ€™ll make ZERO difference the faster ram.",AMD,2026-01-24 10:09:00,1
AMD,o1ag7fh,â€œBigâ€ is very subjective here. Most players are unlikely to notice between 5200-6000,AMD,2026-01-23 19:06:53,1
AMD,o1an5c3,Yeah it's been running great.,AMD,2026-01-23 19:39:33,3
AMD,o1als1e,"I lowered the clocks to 6000mhz for care free setup/stability (I didn't want to worry about instability running 1:1 at 6400mhz) and set it to the settings I saw in HWinfo for 6000, I think I'm running 39-48-48-96 now or something like that.  Probably losing 1-2% in games vs running my 32 gig cl32 kit but having the extra memory is worth it for me.",AMD,2026-01-23 19:33:05,6
AMD,o1c1pen,Yep most likely this I'm pretty sure China has put a ban on importing foreign tech components wanting to focus more on local production and becoming self sufficient. Anything they produce now will likely be forced to stay in house,AMD,2026-01-23 23:45:49,1
AMD,o1bmfu1,"RAM timing is significant, even for X3D in esports. And by a considerable amount, in the 1% and 0.1% lows. Optimizing RAM is beneficial with or without X3D, unless you're blind. But yes, it's always better to have X3D for gaming.",AMD,2026-01-23 22:26:04,2
AMD,o1eaeyc,"The keyword is tuned, the kit doesn't matter that much when comparing tuned v xmp(ofc if comparing tuned v tuned then it does),  even some 16Gb micron that is probably in a 4800 kit that can't go above 5600, fully tuned still  beats a 6000 hynix at xmp/expo, mainly cause stock TREFI is so bad and that's where vast majority of ram tuning benefit comes from.",AMD,2026-01-24 08:51:45,1
AMD,o1bz36n,"6000 CL28 to 4800 CL40 is actually a pretty wild difference in performance, just goes to show how little memory speed matters",AMD,2026-01-23 23:31:41,1
AMD,o1bivzv,"What about 1440p DLSS Q or Balanced, Q isn't even 1080p",AMD,2026-01-23 22:08:35,0
AMD,o1cv24f,"Yeah that's the issue, it's still not stable with the EXPO profile. It's a fairly standard cl36 kit.",AMD,2026-01-24 02:31:24,1
AMD,o1fv2qw,"I saw that article, and it was for 1080p (nobody buying this stuff is still playing at that resolution). Maximum difference is between 2% and 5% for 4k/1440p in a couple of games.",AMD,2026-01-24 15:37:02,1
AMD,o1ejdsf,"Yeah, some people are braindead.   In 99% of games youâ€™re gpu limited anyways and nanosecond faster ram will have ZERO performance benefit.",AMD,2026-01-24 10:14:50,2
AMD,o1ammha,"â€¦paying for Intel for gaming isnâ€™t worth it lol   Other stuff sure, but not for gaming.",AMD,2026-01-23 19:37:05,5
AMD,o1gtnuv,"A8-6600K, or i7? I assume i7",AMD,2026-01-24 18:11:37,1
AMD,o1cz3yv,JEDEC?,AMD,2026-01-24 02:54:55,9
AMD,o1alybz,"But what workflows? For most games, probably not.",AMD,2026-01-23 19:33:55,-59
AMD,o1at6oy,https://youtu.be/XW2rubC5oCY?si=VPVG5hccDfUPBVbx?t=7m02s  Receipts for anyone who doubts.  TL;DW  4800 CL40 vs 6000 CL30 w/ 7950x3d  6% for max fps across 7 games at 1080p with a 4090 (204fps vs 218fps)  10% for 1% lows 149fps vs 166fps  Its reminds me of the meme about Lil Caesars; â€œ4800 CL40 is good with an x3d when you donâ€™t have a  <insert insult here> screamin in your ear how nasty it isâ€,AMD,2026-01-23 20:07:44,44
AMD,o1axh7k,seriously my wife's 3000mhz ram is within 1% of my 3800mhz ram with the 5800x3d we both have. normal ryzen 5000 chips would have a way bigger differnece,AMD,2026-01-23 20:28:03,13
AMD,o1bi48v,"> Just need to upgrade to 9070XT before GPU prices goes up.  Idk if AMD is lagging on price increases, but given what I've observed with Nvidia you need to move on that like... yesterday. Only reason I got some 16GB VRAM cards at MSRP for builds I was upgrading is local stores are slow to react to price shifts.",AMD,2026-01-23 22:04:51,19
AMD,o1dy1z0,Im glad i had 4hrs to waste at microcentre on 9070xt release day lol,AMD,2026-01-24 07:00:51,1
AMD,o1arhqi,I paid 200 for my 64GB 6000 C30 DDR5 kit. Now its almost a 1000.  Edit: What's up with the massive down-votes???,AMD,2026-01-23 19:59:41,11
AMD,o1d7gea,"As someone who lives about 25 miles from the data center capital of the world, I can assure you pushback against it is miniscule compared to th demand to expand, and as long as the counties of northern Virginia remain atop the highest median pay counties in the US that won't be changing. There is also still tremendous demand for data center and high performance conpute for non gen AI usage.   I do think one day this bubble will pop and investors are certainly less bullish than they were a year ago, but don't expect these data centers to panic surplus their hardware.",AMD,2026-01-24 03:45:40,3
AMD,o1aczck,"The person I was replying to has a 9900X which is what I meant when I said ""your CPU"". Will edit the comment to make it more clear.",AMD,2026-01-23 18:52:14,18
AMD,o1ad0m8,"The person I was replying to has a 9900X which is what I meant when I said ""your CPU"". Will edit the comment to make it more clear.",AMD,2026-01-23 18:52:23,5
AMD,o1aa8e7,"> A regular 9700x would probably lose 10 percent of its performance using shitty basic jedec spec ddr5  So... measurable, but not noticable?",AMD,2026-01-23 18:40:01,-3
AMD,o1ampfu,Yea at least some better timings,AMD,2026-01-23 19:37:28,1
AMD,o1an6xp,Probably not even losing that.   Honestly not at all a bad deal there really even before the AI boom - itâ€™s hard to get a good four stick kit that just runs!,AMD,2026-01-23 19:39:45,1
AMD,o1dlz8f,"""Significant"" by itself is one of the most vague words in the English language. All it means is that the effect is not so small that it can be completely ignored.",AMD,2026-01-24 05:23:21,2
AMD,o1kcsej,"Give me some numbers, tests and recordings of that. Because what I've seen doesnt look significant to overspend hundreds on. What is ""considerable amount"" in your metrics? 5%? 25%? 50%?",AMD,2026-01-25 05:00:05,1
AMD,o1bn2hi,"I still don't think the difference is so tangible based on my own hands on experience that it's worth the extra voltage, expense, and tweaking. The biggest jump itself was just getting x3D. Honestly been pretty underwhelmed with the ""gains"" higher clocks so I turned on eco-mode with my 9800x3D much quieter and cooler without tangible losses even in titles where I am heavily upscaling.",AMD,2026-01-23 22:29:12,1
AMD,o1edgm6,"Sounds like faulty ram sticks. If you canâ€™t get anything stable at 6000mhz even with very loose timings and high voltage, that suggests the ram sticks are bad.   Itâ€™s not as uncommon as people think. Not even a year ago for my partners setup, I upgraded the ram had same issue. Returned, got a replacement of same kits and then was easily able to do good ram oc.",AMD,2026-01-24 09:19:53,1
AMD,o1g3war,"> nobody buying this stuff is still playing at that resolution  Yes, but also not really? Most people are using DLSS/FSR now, playing at base resolutions of 1080p or less.   If you care about high framerate gaming at all, CPU bound performance is critically important. You can always lower graphics settings or turn on DLSS to reduce GPU load. The same can't really be said of CPU load.",AMD,2026-01-24 16:18:00,1
AMD,o1ep934,"Yes but the 0.0000001% lows, have you thought about the 0.0000001% lows?!?!1!11",AMD,2026-01-24 11:08:11,1
AMD,o1dl7x1,"Yes, JEDEC.",AMD,2026-01-24 05:17:52,18
AMD,o1dpoma,"Basically, the normal RAM, there is OC vs JEDEC RAM",AMD,2026-01-24 05:51:20,6
AMD,o1ayd3e,"for games, yes, that's the where the advantage of X3D comes from, this has been know since the 5800X3D launched and was tested with cheap DDR4 kits",AMD,2026-01-23 20:32:17,44
AMD,o1b8brv,"Literally for games, been true since the first x3D chip. Most people have experienced it first hand.   People have been paying out the ass on fancy RAM kits that net them a whopping 1% best case scenario with x3D chips.",AMD,2026-01-23 21:19:21,20
AMD,o1b5kef,It takes literally 60 seconds to google this and come up with an educated responseâ€¦,AMD,2026-01-23 21:06:19,35
AMD,o1bicoa,">But what workflows? For most games, probably not.  Lmao this has been known for almost 4 years now.",AMD,2026-01-23 22:05:58,9
AMD,o1f295h,"I was wondering why I was getting all of these downvotes and just realized it's because my dumb ass misread his post.  I've had a 5800X3D for years, I thought he was saying the opposite.",AMD,2026-01-24 12:53:32,1
AMD,o1bxdgo,"4800CL40 is seriously way slower than 6000CL30, just goes to show how little RAM performance matters",AMD,2026-01-23 23:22:34,18
AMD,o1hwnas,Any data for 4k ?,AMD,2026-01-24 21:06:50,1
AMD,o1b4uvt,"The 1% lows are more important.Â  The difference is significant. The list contains 0 multiplayer games like The Finals(UE5) that actually wakes up both CCDs using up to 11 cores(not threads, cores)...   I regret buying 96GB 5600cl40 memory when I made my system, should have gone for 64 6000+ cl30...   On the other hand that 96GB right now costs a super lot... I wish I had bought the 64GB and just never sold the 96GB ... I'd have printed some money now.",AMD,2026-01-23 21:02:57,-7
AMD,o1bhrcf,Sounds about right. I had a board where every bios update that fixed some other quirk costs about 200mhz on the RAM. By the end I was down to 3000mhz and iffy timings... and honestly in gaming I didn't notice a measurable difference.,AMD,2026-01-23 22:03:08,2
AMD,o1dpq4v,I just got a 5070ti on my 5800x3d and put my 4070ti (non super) onto a 5600x and synthetic bench is just not that different. I have 3600 and hers has 3200 too.,AMD,2026-01-24 05:51:40,1
AMD,o1bjjh0,"that's panci buying because of rumors of discontiuation on nvidia  amd hasn't had any such rumours, even though their prices will at somepoint also be affected",AMD,2026-01-23 22:11:46,3
AMD,o1b46hk,"Lotsa people jealous of you. But really, it's just 2021 all over again.",AMD,2026-01-23 20:59:43,7
AMD,o1bylev,"nice, i got a new work laptop before prices went up, 128gb ram due to running a lot of VM's. Now the ram is worth more than the price of the laptop lol",AMD,2026-01-23 23:29:03,2
AMD,o1ayf2b,"I got 32gb 6000 CL30 during Black Friday 2024, as part of a bundle I think I paid around $90 CAD. The same kit is now $620",AMD,2026-01-23 20:32:32,2
AMD,o1ac5re,The stutters would be noticeable,AMD,2026-01-23 18:48:35,8
AMD,o1aypx9,10% is noticeable.,AMD,2026-01-23 20:33:57,2
AMD,o1blk7l,"10% fps can also be very noticable, depending on breakpoints  for most people, 55 vs 60fps is a noticably worse experience, for me, 100 vs 110 is usually the point where I start to not notice (i get naseuos around 100fps/hz)",AMD,2026-01-23 22:21:42,1
AMD,o1btotm,10% in what workload(s)?,AMD,2026-01-23 23:03:02,1
AMD,o1antyw,Some day I'll spend the time to dial it down further but as I've gotten older I prioritize stability and set-it-and-forget-it over the spend-more-time-tweaking-it-than-using-it behavior of my youth. Optimizing can be fun but it's so much less necessary with AMD's chips in general and X3D chips in particular.,AMD,2026-01-23 19:42:46,2
AMD,o1apci4,"Oh this is a 2x64GB kit, I doubt 4x would run. And it came with a huge caveat and the most anxious drive home from a deal I've ever done. ""My idiot son scraped the stickers off.""  I'll have a helluva time reselling them and I was so nervous about getting scammed.  Turns out my hunch was correct though. He had them pictured over a Gamers Nexus mod mat and for some reason that made me trust him and I was correct to do so. Looking at the prices now I really lucked out.",AMD,2026-01-23 19:49:48,2
AMD,o1e5twb,"In French, this means it's impactful enough to be considered. Replace ""significants"" with ""impactant"" if that works for you. Run the tests with the CapFramex readings using fresh RAM in Expo mode, then after tightening the timings. I have a 9800x3D and I can assure you the results at 1%, and even more so at 0.1% low, are impressive. I build and optimize setups for my sons, one of whom is ranked in Fortnite, and these parameters are very important.   PS: I try to use clear words and sentences after translation, and sometimes you have to read the whole thing before understanding the meaning of a single word. This happens to me all the time when it's translated from English to French, and I can usually understand what I read quite well.",AMD,2026-01-24 08:09:46,1
AMD,o1jpr8j,Ah yeah you are right about DLSS. So in the end it depends if you can find some faster ram for a good price then I guess its worth it.,AMD,2026-01-25 02:42:47,1
AMD,o1do49r,What's JEDEC ram?,AMD,2026-01-24 05:39:18,8
AMD,o1f2g0s,"My dumb ass misread their post, I thought he was saying the opposite.",AMD,2026-01-24 12:54:53,2
AMD,o1c0th5,I mean....1 percent for basic use cases yes. from 1:1 6000cl30 to 2:1 8200cl34 (my current profile) it gives an FPS boost of 20 FPS but a 1 percent LOW boost of 45 FPS in BO7. Even bigger FPS boost in CS:GO benchmark.  So we can atleast say a 10 percent boost in 1 percent lows which can make a difference to how the game feels.  But then again im on the x870e apex with watecooled dimms etc. So im the other end of extreme and im at 1.65v.  best case scenario is not 1 percent... just saying. generally you are correct though.,AMD,2026-01-23 23:41:02,3
AMD,o1d7ynf,"Mem OC still often gives +15% gains, and in a few instances about +30% gains on Zen5 x3d. Notably on Satisfactory and Factorio for example.  On Satisfactory, my mem and interconnect OC improves performance on my test save from 105fps to 150fps, which is +43%. Huge majority of that is memory, and what remains is in part only able to be realised because of the memory overclock being there. A spec 9800x3d with 5600 JEDEC scores 98fps on that scene.  Happy to share test scenes and bench A/B if you don't accept these performance figures.",AMD,2026-01-24 03:48:51,1
AMD,o1b8o6k,> The 1% lows are more important.  11fps when you're already over 144 is not that meaningful.,AMD,2026-01-23 21:20:58,28
AMD,o1bgafb,48GB was the sweet spot as it is the biggest single rank kit you can get. It's still not a popular size and is very expensive but not like 64GB. You could get a nice set and sell your 96GB kit and have cash in your pocket.,AMD,2026-01-23 21:56:13,3
AMD,o1bl4rg,"the fixes had nothing to do with what ram the board could run, you could have manually raise or lower vsoc, vddp and vddg voltages. Bet that setting vsoc to 1.2 would have fixed your issue.",AMD,2026-01-23 22:19:33,1
AMD,o1bl9r7,The DRAM situation isn't going to get better for anyone or anything.,AMD,2026-01-23 22:20:15,5
AMD,o1b8jdo,Well I bought them In september. And build my new PC last month. I thought 200 was a good deal.,AMD,2026-01-23 21:20:21,6
AMD,o1b2t6v,Good price.,AMD,2026-01-23 20:53:16,2
AMD,o1acrim,Shouldn't you only get lower fps?,AMD,2026-01-23 18:51:15,-1
AMD,o1b27fb,Not in daily desktop use.,AMD,2026-01-23 20:50:26,-2
AMD,o1ej2a5,"Itâ€™s only where youâ€™re cpu bound, in 99% of games youâ€™re gpu bound and having faster ram will net you ZERO performance gain.   At 1440p and 4K forget about the ram, at that point thereâ€™s so many pixels that youâ€™re purely gpu bound and having faster ram will make no difference, even a faster cpu like going from a 9700x to 9900x which is a MASSIVELY bigger upgrade compared to nanosecond faster ram will barely be noticeable in fps because your gpu is what matters.",AMD,2026-01-24 10:11:57,2
AMD,o1dzlq6,I really wonder how I was able to play games back when frame rates were somewhere between 30 and 60 fps and 30 was already considered 'very playable'.,AMD,2026-01-24 07:14:22,1
AMD,o1dzxfq,Obviously not gaming.,AMD,2026-01-24 07:17:14,1
AMD,o1drvy1,"JEDEC is the name of the standards committee that defines DRAM specifications.    JEDEC RAM is a short-hand name for RAM which has standards-compliant specifications without any overclocking profiles.  So JEDEC DDR5-6400 would be RAM that has 6400 MT/s speeds in the SPD, and does not achieve those speeds via XMP or EXPO overclocking profiles.  More often than not, JEDEC RAM is of a lower speed, so many people mean the base speed of the spec (DDR5-4800) when saying JEDEC RAM.",AMD,2026-01-24 06:08:49,33
AMD,o1ekcjw,god forbid someone ask a question why yall downvoting him,AMD,2026-01-24 10:23:38,8
AMD,o1c2vod,"Sounds like a special situation really and you're probably trading long-term stability for those current gains at that voltage. Which is fine if you're making that trade-off consciously, but that's not viable or attainable in the majority of circumstances either. That's almost to the level of bringing up an LN2 or chiller overclock when talking performance lol.   These more advanced process nodes, really seem to hate excess power.",AMD,2026-01-23 23:52:06,6
AMD,o1dmdgd,"> Happy to share test scenes and bench A/B if you don't accept these performance figures.  I don't feel strongly enough about it to expend the effort.   Taking what you say at face value I'd either guess cache misses as the factories get bigger and more complex... or the games are highly threaded and you need that RAM perf for the non-x3d CCD (since your flair mentions a 9950x3d).  Things that scale beyond 8 cores will need the RAM perf, especially as only one CCD has the x3d.",AMD,2026-01-24 05:26:14,1
AMD,o1cqhvc,"On top of that, 99% the people who say need all of this performance for multiplayer games are non-pro players who aren't nearly good enough for 1% lows to be a bottleneck on their wins and losses.  It's just something else to blame.",AMD,2026-01-24 02:05:02,10
AMD,o1d0w50,"Don't forget this nugget  > wakes up both CCDs using up to 11 cores  Sounds like /u/windozeFanboi has a 9900X3D/9950X3D and either or doesn't understand that only one CCD has V-cache, or they don't know how to properly disable the non-V-cache CCD for games. It's pretty obvious to anyone who understands how Ryzen and X3D works that performance is going to suffer *severely* when the non-V-cache CCD is trying to access data from the V-cache through the IO die on the other CCD, or from slow memory.   It's the reason I chose a 7800X3D instead of the 7950X3D I originally had in my cart, the extra cores just seem so incredibly pointless when only ~20% of my time is spent compiling or running VMs or rendering video or whatever else and the remaining ~80% is spent in-game. And if I *genuinely* need the extra power, I have servers for that purpose.",AMD,2026-01-24 03:05:36,2
AMD,o1bmuh6,"There are 32 GB single rank DDR5 DIMMs, just so you know. Introduced into consumer DDR5 a bit over a year ago if I remember correctly.",AMD,2026-01-23 22:28:06,3
AMD,o1bpuja,"> the fixes had nothing to do with what ram the board could run  Yes and no. My old board had some craptastic bios support. If I updated to fix problem <A>, I might also no longer be stable at the same clocks when stress testing.  I'm not saying the fix itself cause it, rather just the board had a lot of regressions as they shoehorned in other stuff. It wasn't worth it to me to start upping voltages when the RAM clocks made minimal difference with the 5800x3D I had at the time. I'd rather just run 3000mhz when all is said and done than find out that ""whoops, that voltage wasn't actually safe"".",AMD,2026-01-23 22:43:12,1
AMD,o1ebpra,a lot of the current pricing is more because of panic buying rather than the real market influence rn   at somepoint it'll stabilize but idk when,AMD,2026-01-24 09:03:41,-4
AMD,o1b9474,"Sure is back then. Right now a basic 16x2 6000mhz kit is hovering around 450~500. When DDR4 was new, this sorta thing also happened. Then China launched an investigation against SK Hynix for price fixing(Probably the only time China did something right). Prices dropped, everyone was happy.",AMD,2026-01-23 21:23:01,2
AMD,o1adw21,"X3D chips have significantly better 1% lows, which is what most people perceive as ""stutter"".",AMD,2026-01-23 18:56:17,10
AMD,o1adc29,1% lows would be more affected as it misses frames trying to load something big,AMD,2026-01-23 18:53:49,2
AMD,o1b95xb,"define daily desktop use, my daily desktop use is gaming, other people do rendering, video editing, etc     daily desktop work is different for different people.      so yes, it is very noticeable in ""daily desktop work""",AMD,2026-01-23 21:23:15,2
AMD,o1bliam,for daily desktop use any ram speed is enough ..,AMD,2026-01-23 22:21:26,2
AMD,o1f54gu,"some people play games which are light on gpu, some people prefer high refresh rate display to high pixel count display  and some games are a massive cpu hog (the finals)  if yyou're trying to play cs2 on a 500hz monitor you're almost certainly cpu limited",AMD,2026-01-24 13:13:05,2
AMD,o1ecbu3,"I wasn't I had an LCD but I felt nauseous playing games on it so I got a free crt from someone who wanted to get rid of it hahahah  i was on a crt until like, 2018? when high refresh rate monitors finally started to get affordable, and I had enough money saved for a nice one",AMD,2026-01-24 09:09:22,1
AMD,o1ggr2c,"Or as I like to say, ''Jay Deck Ram.''",AMD,2026-01-24 17:15:16,0
AMD,o1f7eg3,"To add, the hierarchy of timings goes like this:  JEDEC (terrible) < XMP/EXPO (bad) < buildzoid easy timings (decent) < manual overclocking that takes many hours of trial and error (good)",AMD,2026-01-24 13:27:33,-5
AMD,o1exdzv,Because typing JEDEC into google would have answered the question.,AMD,2026-01-24 12:17:15,3
AMD,o1cbgqy,Its 72h karku stable.  24h anta extreme and absolut stable.  prime 95 stable.  vt3 stable  core cycler stable.  & quite alot more.  1.1 SOC is LOW and i could probably go lower. The voltage is on my old Â£130 a-die sticks. The CPU is in 2:1 mode so less IMC stress... if anything my CPU will last longer than 99 percent of people using XMP/DOCP. Since alot of profiles set SOC to 1.3ish. CPU VDDIO is also alot less since again in 2:1.  1.65 vdd and 1.45 vddq.  This ram has had 1.7 daily put through it for years. Heat is a big factor for degradation not just voltage. Depends on the sticks too hynix a-die green sticks can handle it.  Its not a chiller just a simple custom loop with 420 rad with quick release piping.,AMD,2026-01-24 00:38:30,2
AMD,o1drkwt,"It is true specifically in 1ccx mode for zen5 x3d, but yes, the working set is large enough that it doesn't fit well in vcache. It still benefits enormously from it being there, it just needs RAM perf as well. Many cases where the hitrate is like 50-70% or something.  That's also true for e.g. large cities/raids in MMO's for example. If you bench standing alone in elywyn forest then even vcache does nothing because 32MB cache fits well, but if there are hundreds of players or intense combat around then 96MB vcache is overwhelmed and it spills out to RAM.  Since RAM is less than 1/10'th of the L3 speed, stuff really slows down when that happens (or if you have an OC, there is a lot of room to not slow down as much).",AMD,2026-01-24 06:06:20,2
AMD,o1dv3ae,"To expand on last comment, here's an example from FFXIV benchmarking with and without vcache on zen 5  https://files.catbox.moe/hfbf6c.png  If you selectively cut out the first scene, you can make the argument that vcache is virtually useless. However in another scene, the one with the worst performance, vcache is boosting performance by some 80%.  It's much the same with RAM OC, it just happens at larger data sizes (so less often). Even with the same game, systems, settings, all configuration identical it's critically important where and how you benchmark.  I'd generally recommend capturing the times of worst performance AND using a geomean of a variety of scenes for some perspectives. Having a benchmark title of just ""X game"" is usually not going to give enough useful information unless the game has very uniform performance characteristics, which is usually **not** the case for CPU/cache/mem heavy titles.",AMD,2026-01-24 06:35:31,1
AMD,o1cx5my,">On top of that, 99% the people who say need all of this performance for multiplayer games are non-pro players who aren't nearly good enough for 1% lows to be a bottleneck on their wins and losses.  Yeah... when it comes to framerate and performance discourse you'd think everyone online is either a top tier pilot or an esports legend.   >It's just something else to blame.  And obsess over. I question how many people can truly notice a number of things if they don't have an OSD on their screen telling them about it. Heck, sometimes settings can be broken or non-functional and you'll have people swearing by the impact on visuals and performance.",AMD,2026-01-24 02:43:36,5
AMD,o1dmvhq,"> It's the reason I chose a 7800X3D instead of the 7950X3D I originally had in my cart, the extra cores just seem so incredibly pointless when only ~20% of my time is spent compiling or running VMs or rendering video or whatever else and the remaining ~80% is spent in-game. And if I genuinely need the extra power, I have servers for that purpose.  Yeah, having owned a 3900x and a 5800x3D I kind of noped out with the whole one x3D CCD and one regular CCD, especially when the grand solution to gaming perf was to use gamebar to disable a CCD (iirc). Felt like a waste of money, to save a little time on occasion.",AMD,2026-01-24 05:29:55,2
AMD,o1dhe5u,"I understand how it works, I've been messing with CPU affinity since my 4800H in 2020.   Thing is modern anticheats don't let you set CPU affinity , they just don't. Bios/driver/firmware prefer cache lock on 1 CCD doesn't quite work when games require more than 8 cores. it also doesn't quite work if you want to stream for example.   Best FPS results on my favourite game ""the finals"" have been 8 cores and disabled SMT ... I could have that with no compromises if anticheat let me set CPU affinity Â But they don't.   Also , I didn't want a 7800x3d when I could have 2x the cores for just 25% more.   16c was kinda more than enough for me but 8cores was too little. 7900x3d was worse of both worlds.   And yes my point still stands, nobody compares multiplayer games for these comparisons. And yes 1% lows difference of 10% the person above mentioned is significant.",AMD,2026-01-24 04:51:12,0
AMD,o1e973y,Who makes 32Gb modules? I've never heard of them let alone a consumer stick having them.,AMD,2026-01-24 08:40:45,1
AMD,o1guc1p,"It's not going to stabilize unless the AI bubble burst and stops buying up capacity. This isn't a market blip or a short-lived crypto ""goldrush"". Everything needs DRAM from embedded devices to phones to consumer electronics to servers and beyond.   They're not in a rush to build more DRAM production, because the memory makers don't want to be left holding the bag. The supply is going to get worse not better. Even on the business end of things companies are hesitant to commit to pricing on upcoming stuff currently.   It's going to get worse, not better. AI has bought up the bulk of production capacity and materials for the coming **years**.",AMD,2026-01-24 18:14:29,2
AMD,o1gz5d7,2nd post:   https://www.spglobal.com/automotive-insights/en/blogs/2025/12/dram-makers-ai-data-centers-semiconductor-shortage  Everything is screwed for the coming years with the AI bubble.,AMD,2026-01-24 18:35:00,1
AMD,o1bbvkz,I hope the prices will eventually go down again.,AMD,2026-01-23 21:35:50,2
AMD,o1f7udj,"Thatâ€™s why I said 99% of games are gpu bound at 1440p or higher, which a majority of people are playing at these higher resolutions if they have â€œfastâ€ ddr5 ram.   It really doesnâ€™t make much of a difference even then in the few e sports games that are cpu bound, 5-8% gain in 1% lows and averages. Having the better cpu would still net you more fps.  And rlly would you notice if your 1% lows were 200fps instead of 216fps? Like it gets to a point where you rlly wonâ€™t notice in these esport games, fast ram is literally burning money if you are only gaming.",AMD,2026-01-24 13:30:17,1
AMD,o1eeu53,"I found out that playing with the monitor as the only light source doesn't work for me. But with the room illuminated it works just fine, even in the 'Descent' games (full six axis freedom, there is no fixed 'up' or 'down' in those games)",AMD,2026-01-24 09:32:37,1
AMD,o1f46h2,"And kept the information in that answer well apart from this thread. There is value in asking & answering questions in threads rather than just googling, as both the question & following answers can induce enriching & educating discourse. The original intent of the voting system was to encourage discussion and discourage vapid nonsense. Asking a good question is conductive to discussion, and thus should not be downvoted. But of course redditors are ego-obsessed children with a superiority complex, so the votes are like & dislike buttons instead.",AMD,2026-01-24 13:06:50,12
AMD,o1cfwx7,"Fair, but that just further stresses your setup is a phenomenal exception and incredibly far from the norm.",AMD,2026-01-24 01:03:04,5
AMD,o1eat9u,Can you share your zen timings my man. I feel like i am the only one who lost at imc lottery.,AMD,2026-01-24 08:55:22,1
AMD,o1eb47w,Less imc stress is not really right. How much voltage do you push to your VDDIO. If you're running less vddio voltage on 2:1 i bet you can get better 2:1 performance out of that kit. Because as far as my experiences goes IMC is the main limiter.,AMD,2026-01-24 08:58:09,1
AMD,o1gveba,"I think realistically though for the overwhelming bulk of gaming its more than fine to state that it covers for poorer memory, because it does. Yes there are outliers, but do said outliers warrant spending on some insane kit and jumping through the hoops? Unless all you play is factory sims it's probably not worth the headache or effort. Even as someone that does play Satisfactory now and then, I'm just going to stick with the lower voltage lower headache setup I have going since performance is still very good overall. The 3D cache covers incredibly well for the overwhelming bulk of scenarios in gaming. And with crap RAM running about $300 or more and good RAM costing as much as a GPU it's a no brainer.",AMD,2026-01-24 18:18:58,0
AMD,o1hdl55,">his shift has already sparked panic among OEMs and tier 1 suppliers, reminiscent of the rush to secure components during the 2021 crisis.  from your own article  >AI has bought up the bulk of production capacity and materials for the coming years.  AI has bought the right to get them to make what they want if they want for the coming years. If they don't need it, dram manufactures will be able to, and will, use the machines to produce alternative products they can sell  and one of the AI ceos, I think it was microsoft, literally said that the problem isn't chips, it's power and datacenters to put them in",AMD,2026-01-24 19:37:47,1
AMD,o1cs96t,It eventually will. But not anytime soon.,AMD,2026-01-24 02:15:08,2
AMD,o1fau8h,"No.. sorry, itâ€™s fucking laziness. End of",AMD,2026-01-24 13:48:16,-5
AMD,o1cm9lp,You could make the argument my ram is more stressed but that would be all. My CPU has half the stress of a normal CPU using XMP settings in 1:1 mode.  Regardless ram makes a big difference to 1 percent lows which most people CAN feel during gameplay. I recommend everyone does a basic manual ram profile with easy timings.,AMD,2026-01-24 01:40:11,-1
AMD,o1eg65a,"i have 56.6ns latency and im at 1.65v. My timings are as low as they can possibly go. TRFC is at 380. I hold the world record for fastest 98000x3d in geekbench. I spend over a full week just stablity testing and another week dialing my timings in. Its over 200 hours total to get this profile dialed in. My limit is not the IMC, its daily voltages. 1.65v is my limit.  Less IMC stress is exactly right. Its called 2:1 for a reason. Running in 2:1 mode (Memory Controller Frequency, UCLK = Memory Clock/2, MCLK) on AMD AM5 (Ryzen 7000/9000 series) significantly reduces stress on the Integrated Memory Controller (IMC) compared to 1:1 mode. You also get**Lower UCLK:** In 2:1 mode, the UCLK runs at half the memory clock frequency. For instance, at 8000MT/s (4000MHz), the UCLK runs at 2000MHz, which is lower than the 3000MHz required for 6000MT/s in 1:1 mode.  You can find my overclocking on overclock.net. same name there. I do appreciate the comment.",AMD,2026-01-24 09:45:01,1
AMD,o1egcjo,"Also if you need help send me your zen timings and tell me what your looking to achieve and i can give suggestions. Comparing to mine wont help you, im on a binned 9800x3d and the x870e apex with watercooled iceman copper heatsinks and a ram fan etc.",AMD,2026-01-24 09:46:41,1
AMD,o1inz2e,"It does, but to claim only a 1% performance improvement (across a sample of 30 games, which i guess people would read as a representative sample?) is wild. Given the numbers that we have, they're obviously intentionally creating scenarios which are not even CPU bound - using an undisclosed GPU, settings, test scenes and games list - and then using that to make claims about CPU performance differences while specifically excluding all of the data points which disagree. It's disgusting and misleading marketing at your expense.  You can ask yourself: If the diff is 1% between some JEDEC memory speed well below what the 9800x3d supports, then why does AMD refuse to ever test it against their older CPU's or their competition without having unwarrantied and unguaranteed memory overclocks applied, even though it makes them look ridiculous?   They do it because it does matter - even with a terrible automatic OC, and even with vcache. They would rather look ridiculous with a 10% bigger number than report the actual performance of the CPU at its warrantied specifications. They even did this to compare the 9850x3d against the 9800x3d and current gen Intel CPU's.",AMD,2026-01-24 23:19:49,1
AMD,o1hh9fd,"> AI has bought the right to get them to make what they want if they want for the coming years. If they don't need it, dram manufactures will be able to, and will, use the machines to produce alternative products they can sell  Which ignores that OpenAI is trying to choke out the competition's access to supply before it crumbles.   The AI corps that bought up the supply and capacity aren't going to be like ""nah we don't need it you can make whatever""... they want their other competition in the AI space to not have supply.",AMD,2026-01-24 19:54:14,1
AMD,o17yokq,â¤ï¸ðŸŽ¯,AMD,2026-01-23 11:31:28,4
AMD,o15i6yk,"Dunno if this should be considered as ""AMD calls out"" considering the slides were labeled as internal only.",AMD,2026-01-23 00:53:41,114
AMD,o15ofip,But are they as efficient? That's a big factor for portable devices or laptops.,AMD,2026-01-23 01:28:40,48
AMD,o15i26g,Strix Halo adoption is still close to none. What's the point of theoretical wins if you can't ship anything?,AMD,2026-01-23 00:52:58,53
AMD,o17k7jz,"https://i.redd.it/oy0vr67mh2fg1.gif  AMD's department is hitting rock bottom, with all these attempts at coolness and all-caps writing. (This comment is stolen from VC but it's just too pertinent).",AMD,2026-01-23 09:23:46,13
AMD,o15kvu2,Poor Volta ptsd flashbacks ðŸ˜‚,AMD,2026-01-23 01:08:38,14
AMD,o15hdi6,"AMD calls out?  That's pathetic marketing, saying nothing would have been better.    Not only do they make up performance for made up skus, they compare a 25W TDP to Ryzen A 395+ with a TDP of 55-120W.   Wow, a (probably) 120W beats a 25W CPU? Who would have thought?",AMD,2026-01-23 00:49:14,46
AMD,o15n09w,biggest joke I have ever seen.   They had to compare panther lakes efficency to other intel chips bc amd chips aren't even in the same ballpark at this point.   Then they seriously compared the thread count saying that more threads must mean better.,AMD,2026-01-23 01:20:34,16
AMD,o19bj4o,Comparing 368H to Strix Halo is beyond desperate.,AMD,2026-01-23 16:02:59,3
AMD,o17kv09,"Maybe it's time to add FSR 4 to the RDNA 2 APUs?  We know it's possible, because we can do it in Linux already.",AMD,2026-01-23 09:29:59,2
AMD,o198cxz,Strix Halo costs $2400 for 32 gb ram tablet.  A Intel X7 ultrabook can probably be had for less than half that price. Not the same tier of products imo.,AMD,2026-01-23 15:48:48,2
AMD,o1aeiv8,"Not even close. Inefficient APUs is what you are talking about, AMD?",AMD,2026-01-23 18:59:11,2
AMD,o16x4l7,Hahaha first release your halo apu in 1000$ then talk.,AMD,2026-01-23 06:02:07,4
AMD,o17sar2,"Come on. They should make update silicon instead of arguing that the aging mobile chips are still competitive. (Which they really aren't... Intel has caught up in several metrics)  Just to compare the timelines, Lunar Lake was released several months after Strix Point and now its successor, Panther Lake, is being released. Meanwhile all AMD has to offer is a very lukewarm refresh until 2027! And note that the next-gen Medusa Point will STILL have RDNA 3.5. WTF.  It's really annoying. AMD has all the pieces, but it looks like they aren't even trying anymore.",AMD,2026-01-23 10:37:58,2
AMD,o1h4psx,"nope, RPL, ARL, panther lake better than all non 3ds, the only AMD worth buying is 9800x3d.",AMD,2026-01-24 18:58:41,1
AMD,o16sefo,This is so pathetic,AMD,2026-01-23 05:26:50,0
AMD,o17udf9,"Itâ€™s all just wind until products are released and benchmarked by independent testers. Until then, both sides are going to say that they have the faster chip.   What I will say, however, is that Iâ€™m disappointed that AMD's 400 series is just a barely-tweaked 300 series refresh, while Intel is releasing a fully new design.",AMD,2026-01-23 10:55:51,1
AMD,o175ac1,"What a shame, AMD!",AMD,2026-01-23 07:08:17,-2
AMD,o18223k,Intel Kinda isnt relevant for pc users anymore,AMD,2026-01-23 11:57:16,-1
AMD,o16ysaw,From the company that lied about cranking their chip up to match AMD. And from a company witn diabolical level marketing.   Just wait for benchmark ignore these he said she said,AMD,2026-01-23 06:15:02,-4
AMD,o18kdwh,"Like I said there, well, what about the efficiency claims though? Idk if AMD will beat Panther Lake this time in that aspect, but we'll see the reviews. I assume we'll witness a good fight in this generation, but technically is without doubts that's Intel who made the biggest improvement from the previous gen, at least on paper. On the graphic side, no doubts AMD still has an upperhand with that 8060s monster though.",AMD,2026-01-23 13:50:43,0
AMD,o193up1,"The Panther Lake chips with the highest integrated Arc GPUs have more computing power (GPU-wise) than the Strix Halo's 8060s by about 20%. Intel's drivers are still fresh, though, so it may not match up in practice.  Regardless, Intel is starting to get their fabs in order, so it'll be nice if they start to produce some competitive chips. Should help keep prices down for us consumers.",AMD,2026-01-23 15:28:15,0
AMD,o17dlaw,"dont let Intel or Amd know about Apple's M chips and what they are doing in terms of efficiency, and not just for AI slop loads, for ACTUAL WORK",AMD,2026-01-23 08:22:00,-1
AMD,o16zeio,INTEL MY GOAT,AMD,2026-01-23 06:19:55,-6
AMD,o177j09,"AMD has better CPU, GPU, but choose to fail by not using their best designs together. Failure is a choice.",AMD,2026-01-23 07:27:45,-3
AMD,o162yj6,"This isnâ€™t even true, itâ€™s just based on their internal expected performance for upcoming chips according to the disclaimer in the bottom right, the slides were released publicly",AMD,2026-01-23 02:49:31,28
AMD,o179dfv,"Slides say ""**Based on internal estimations**"". Eye check, stat.",AMD,2026-01-23 07:44:04,6
AMD,o1bc15d,"Exactly, this is the big deal with panther lake. It's finally providing efficient x86 chips that run at respectable speeds.  I just want a performance dev laptop that can run Linux natively, and that I can actually run on battery for more than 45 minutes. I'm praying panther lake will make that come true.",AMD,2026-01-23 21:36:32,1
AMD,o15ki3a,"I'm not sure who Strix Halo is even for. I'd imagine those who are keen on high-performance mobile gaming are a bit put off by the lack of FSR 4, and those who wish to run local AI models aren't particularly compelled to pay a premium for a smaller form factor (and would prefer a more cost-effective full-sized desktop). I myself was considering a Strix Halo mini PC for gaming, but I ended up paying roughly the same for a 9800x3d / 9070xt desktop with far better performance (and full fsr 4 support to boot).  It's a really cool APU. But the price point doesn't really make sense for most use cases. It sounds like Intel has a competing product that, while certainly no match in terms of raw performance, supports a more modern feature set that, for better or for worse, seems to be the future of mobile PC gaming.",AMD,2026-01-23 01:06:28,41
AMD,o15w1ie,"Arenâ€™t strix halo laptops generally positioned as highend devices with price tags above $2000? Cmiiw, but the audience at that price point seems relatively limited. Most buyers tend to choose either a solid laptop around $1000 or spend more for a true top tier model. At around $2000, a laptop that doesnâ€™t match the gaming performance of similarly priced gaming laptops can be difficult to justify, and it may feel overpriced for users who donâ€™t specifically need a high performance thin and light system.",AMD,2026-01-23 02:11:37,3
AMD,o15ng0m,"It's available in a lot of different products, unless you have sales numbers to prove otherwise, pretty hard to say what the adoption rate is.",AMD,2026-01-23 01:23:03,-2
AMD,o1c8igk,"Before people start making uninformed statements, this is indeed edited slide used to make fun of AMD's marketing department. Read the comment for yourself on https://videocardz.com/newz/amd-calls-out-intels-panther-lake-ces-claims-says-ryzen-apus-are-still-faster  By Crys86 > This looks as stupid as a slide like this [referring to edited AMD vs Nvidia slide]. The marketing department is hitting rock bottom, with all these attempts at coolness and all-caps writing.  By Blizt_324 > You know what's funny? At least your edited image is more realistic when it comes to comparing hardware. You are pitting the 9070XT against the 5090, literally the best GPU of either company. AMD pits their own flagships against Intel's midrange models and pretends Intel's lineup doesn't have anything better.    With that out of the way, those comments are 100% correct. This ""call out"" is a waste of marketing resources and detached from the reality of the laptop market. Almost no one covered ""Gorgon Point/Gorgon Halo"" during CES. Even when AMD made a big splash with ""Strix Point/Strix Halo"" in mid-2024 there are almost no compelling laptops or products using those chips. Intel caught up with Lunar Lake, Apple continues to consistently put out high quality and good value Macbooks, and Nvidia still dominates mobile GPU.",AMD,2026-01-24 00:22:36,3
AMD,o199mhc,Holy shit that chart is real? ðŸ˜­ðŸ˜­ðŸ˜­,AMD,2026-01-23 15:54:29,2
AMD,o16650o,"The ""25W"" is kinda BS though. Most of Intel's current mobile chips that claim a 25-30W TDP actually have a maximum power draw of 115W. I'm definitely interested in Panther Lake but we need actual benchmarks that look at both performance and actual power draw to know how good it actually is.",AMD,2026-01-23 03:07:07,21
AMD,o1636zi,Also look at that AI column all the way on the right? Panther Lake has better graphics? By AMDâ€™s own admission?   Now Iâ€™m really curious about Panther Lake.,AMD,2026-01-23 02:50:46,14
AMD,o179kht,"Intel used 65W for their X series chips, as per their own slides, not 25W! Stix Halo laptops and tablets run at 65W-80W, depending on the configurations.",AMD,2026-01-23 07:45:48,8
AMD,o17b0gb,Read the article. Its an internal memo. They didn't call out anything or anyone.,AMD,2026-01-23 07:58:44,2
AMD,o1a4gaz,They did say nothing.  This isn't marketing.  These are leaked internal-only slides.,AMD,2026-01-23 18:14:20,1
AMD,o1978su,"the ""25W"" part running at 65W still loses to the 55-120W part running at 65W",AMD,2026-01-23 15:43:45,-3
AMD,o179geu,"Thatâ€™s why they frame it as ""Panther lake barely improved efficiency from lunar lake"" as response to the ""world leading x86 efficiency claim"".  Even lunar lake was far ahead, so Panther lake is still ""world leading""",AMD,2026-01-23 07:44:47,7
AMD,o16agxt,> Then they seriously compared the thread count saying that more threads must mean better.  Welcome back Bulldozer era marketing.,AMD,2026-01-23 03:32:15,13
AMD,o187vhk,They are.  They still hold the majority in market share and sell quite well.,AMD,2026-01-23 12:37:53,8
AMD,o1833jx,"For desktops as of now, absolutely. But when it comes to laptops, especially the newest SKUs, honestly it's a fair fight. Heck, I'd rather have lunar lake than whatever ryzen ai thanks to Intel's upscaling. XMX from Intel is better than FSR3 lmao.",AMD,2026-01-23 12:04:55,6
AMD,o18l5f1,What isn't relevant are DIY sales,AMD,2026-01-23 13:54:45,2
AMD,o19cc5k,This sub is a goldmine for PCMR circlejerk comments,AMD,2026-01-23 16:06:38,1
AMD,o19bw8r,"> Idk if AMD will beat Panther Lake this time in that aspect, but we'll see the reviews.  Strix Point barely beat Arrow (and only in multi thread power scaling while idle draw, media decoding, and various ultrabook workloads it's a wash at best). Straight up lost to Lunar Lake in pretty much everything apart from multi core, which is easily the least important metric in a <30w class ultrabook.",AMD,2026-01-23 16:04:37,0
AMD,o174a0a,"> but I don't need an integrated GPU   In regards to your post down the chain, that's a bit short sighted. A functional iGPU is nice to have. If you've ever had a GPU crap out during a market shortage it's nice to not lose full access to a system. Standby cards are expensive themselves as well.   Having been in that boat before it's always nice to have a functional fallback.",AMD,2026-01-23 06:59:41,5
AMD,o1716ne,They could be if they actually released RDNA4 graphics on their APUs. For the most part they have the best CPU architecture on Desktop and they could also on Laptop if they gave a damn enough about laptops.,AMD,2026-01-23 06:34:20,6
AMD,o179on2,Premium is the price segment. Meaning lots of $$$. Not a premium brand.,AMD,2026-01-23 07:46:50,1
AMD,o1h3w4y,"Oh, they know. They just can't do anything about it because the gap is too big. Plus, Apple is on a yearly cadence versus 15-18 months for Intel and 24+ for AMD, so the gap will only continue to increase.  Zen 5 and Arrow Lake are less efficient than even M2.",AMD,2026-01-24 18:55:08,1
AMD,o19cnt7,So they don't   If their designs aren't scalable and flexible then they're not good for a given segment.,AMD,2026-01-23 16:08:04,1
AMD,o17go3p,"If you click on the first picture and look at the top left, it says ""AMD internal distribution only"". A bit hard to read cuz it's in very deep blue that kinda blends in with the background.",AMD,2026-01-23 08:50:27,14
AMD,o1jvga1,https://i.ibb.co/hRGjHNTY/tmp.png   When would you like your eye check appointment,AMD,2026-01-25 03:14:37,1
AMD,o15nvvj,Excellent take. I want to buy Strix because cool. But thereâ€™s no use case for me.,AMD,2026-01-23 01:25:34,10
AMD,o15r3n3,"I would love Strix Halo in an ITX form factor at a reasonable price.  Don't need lots of RAM, just a good system for use as an HTPC with some light gaming and flight sims.",AMD,2026-01-23 01:43:57,6
AMD,o15m6z5,"Seems like AMD doesnâ€™t really know either.   It seems like you only hear about it from AMD is when they say stuff like here, like ""weâ€™re still on top, we still have the fastest APU"".   They only compare it to products that arenâ€™t really in the same class either.  Apples M4 systems are typically much cheaper and systems with dedicated GPUs perform better.  For local AI they havenâ€™t shown a proper usecase for either yet. Yes, larger models do run on it but theyâ€™re so slow that you wouldnâ€™t really want to use them.  I hope, theyâ€™ll find something for it",AMD,2026-01-23 01:15:56,11
AMD,o18ewxd,"The fact they still produce graphics for so many RDNA 2, 3 and 3.5 laptops, handhelds and consoles is the only reason I still have hope that they officially support FSR4 on the older architecture. It just seems recklessly malfeasant to abandon the stack.",AMD,2026-01-23 13:21:04,3
AMD,o15mcsf,"The 388 one is fun, should've been in the lineup from the start but oh well. Full 8060S and 8 cores, so a single CCD. The release timing is unfortunate. It should've been the FSR4 flagship product, such a perfect fit, oh well. Some day AMD will release the same arch for both APUs and GPUs, instead of this stupid 1 gen delay every time.  Generally I'm the target demographic for it. A person with a single laptop who works on it during the day and plays lighter games in the evening. I lived like that for a long time with a 7945hx + 4070m, it's still my gaming machine.",AMD,2026-01-23 01:16:51,7
AMD,o15m64r,what are the cost effective options with 128gb shared memory?,AMD,2026-01-23 01:15:48,2
AMD,o187tch,Exactly. I never go why APUs need such huge GPUs. it will be a waste of silicon for most and for the ones that need it it's till not enough.,AMD,2026-01-23 12:37:29,1
AMD,o16na7k,I have seen multiple videos of YouTubers who went to CES and had hands on with panther lake. They were allowed to do everything except benchmark the CPU. They were allowed to see the TDP.  I think ETA Prime mentioned 65W max on a laptop he was trying. I don't know what Panther Lake was that.   DigitalFoundry has a whole podcast on the chips too. They aren't running at 115W this time round.,AMD,2026-01-23 04:51:34,18
AMD,o19b0b8,"The leaked Geekbench6 scores are around 15k for multicore, which is really-really good, it is basically HX370 equivalent with a much better GPU. Also will be really curious about power draw, youtubers so far are saying really good things about battery life, so we'll see.",AMD,2026-01-23 16:00:39,2
AMD,o177qmp,"Yes, but only short term. And not on battery power which has been an emphasis with Panther lake. Strix halo will draw a sustained 120W+ all day",AMD,2026-01-23 07:29:36,3
AMD,o16s25p,"And strix halo has a _sustained_ power draw of up to 120w (up to 150w in selected mini pcs, and 70-90w in announced laptops)",AMD,2026-01-23 05:24:24,3
AMD,o16aky9,>Panther Lake has better graphics? By AMDâ€™s own admission?  Given how they've been managing their APUs and iGPUs and the driver stack I feel like that wouldn't be a huge shock?,AMD,2026-01-23 03:32:54,14
AMD,o18q4w0,"That's turbo clocks, if you apply that same logic  Strix Halo uses 120w as that's the max rated wattage.",AMD,2026-01-23 14:20:34,1
AMD,o1a6cz4,It literally says that AMD released that.,AMD,2026-01-23 18:22:46,2
AMD,o1a6adt,">AMD has release a short ""Positioning vs. Intel Panther Lake"" slied deck  Article says AMD released that. Where do you get that info from?",AMD,2026-01-23 18:22:27,1
AMD,o199fkc,Based on what exactly?,AMD,2026-01-23 15:53:36,4
AMD,o19amko,"Strix Point pushed to 80w will still lose, what's your point? A 5080 underclocked to 80w will obliterate it, is that shocking to you? First time learning about how silicon works?",AMD,2026-01-23 15:58:56,1
AMD,o18tv1x,I'm crying. we are so fucking back  https://i.redd.it/y1bvu10024fg1.gif,AMD,2026-01-23 14:39:36,2
AMD,o17vqfy,Lack of RDNA4 reeeeeally kills it for me.,AMD,2026-01-23 11:07:19,1
AMD,o1h8d6c,"the M4 Pro in a mac mini pulls 46w whilst having the CPU power of a 7900X (all core, beats it in single core by a mile) and the GPU power of a RTX 4060 or RX 6700XT   its pretty damn nuts!       if it continues at this pace ill retire my rig for an Apple sillicone mac in 5 years time",AMD,2026-01-24 19:14:40,1
AMD,o19q8cq,"It is a matter of choice. Back when RDNA3 is released they refused to put RDNA3 on APU too. We see it on APU today, so it was not impossible back then. No, it is not a design flexiblity. No technical barrier, they just choose to lag behind. When RDNA2 is out they are still shipping Vega on APU, 2 generations behind.",AMD,2026-01-23 17:09:05,0
AMD,o18t5ms,Wow that is really hard to notice. Had to turn up my brightness to be able to see it,AMD,2026-01-23 14:36:01,4
AMD,o15rxgx,"If those Strix Halo handhelds were like 35% less expensive, I'd be all over it. Do I need it? Hell no. Can I live with playing my PC library at lower framerates on my existing Rog Ally, and save the high-res, high-framerate experience with those same games for my existing desktop? Damn right I can. But am I a nerd who enjoys shiny toys? Yes, yes I am. But I still have my limits with respect to price.",AMD,2026-01-23 01:48:36,10
AMD,o18yuj2,the AI MAX 388 seems to be what we want but I am sure the price will be dumb or flat out not available as itx.,AMD,2026-01-23 15:04:22,2
AMD,o1edgzf,What's more mental is it's out there in the wild. You can fairly easily mod in fsr4 through optiscaler.  And it works nicely. Sure there is a performance hit but it's not too severe.,AMD,2026-01-24 09:19:58,1
AMD,o15mdbm,What do you need 128GB (in reality itâ€™s far less â‚¬ shared memory for?,AMD,2026-01-23 01:16:56,7
AMD,o1h2yt8,"Depends on the definition of cost effective, but an M4 Max Mac Studio with 128GB of RAM costs $3500. It is however, way faster than Max+ 395 while having lower power consumption, and Apple MLX is way better supported than AMD ROCm",AMD,2026-01-24 18:51:12,1
AMD,o19o4vd,Because they're building an all in one system so you can work and game on the same SoC. All while saving on PCIe routing. If you don't need the GPU part just get a 9xxx series CPU.,AMD,2026-01-23 16:59:30,1
AMD,o16bjm0,"Agreed, it's shocking they stuck with RDNA3 for their APUs and iGPUs.  They must have assumed they wouldn't have any competition at that level, and could force anyone who wanted higher end graphics to buy discrete RDNA4 cards.  Miscalculation at this point.",AMD,2026-01-23 03:38:32,14
AMD,o19mekd,"Digital Foundry benchmarked Panther Lake at CES where a Lenovo laptop was set up at 65W with 85W Turbo TDP, stock, against Strix Halo at 65W. Strix Halo wion by 25-35% advantage, depending on the games tested.",AMD,2026-01-23 16:51:39,1
AMD,o1by95q,"""AMD Official Use Only  AMD Internal Distribution Only""  Top left of the first slide.  The fact that the article claims it was released is irrelevant.  That's just sloppy writing on a site known for sloppy writing, though not as bad as that other popular would-be leak site.",AMD,2026-01-23 23:27:14,1
AMD,o1dta6z,"Based on the fact that a small die pushed to way out of its efficiency curve will lose to a big die running at low clocks. Is that black magic sorcery to you? Now you have to ask them, how well does Gorgon Point, the real rival-in-class in terms of fitting into ultrabooks and mainstream laptops, scale at 65w?",AMD,2026-01-24 06:20:15,2
AMD,o1auu9k,DF's video on it,AMD,2026-01-23 20:15:35,0
AMD,o186tt5,consider yourself lucky,AMD,2026-01-23 12:30:56,2
AMD,o18niqg,"I have, multiple even. Both Nvidia and AMD. Having no cards while you RMA that shit is misery. Having no card when there's no cards in retail channels is awful.   Think you also over-estimate the ""savings"".",AMD,2026-01-23 14:07:10,1
AMD,o1hbe82,"And the M4 Max at 55W matches the 9950X at 200+ (again, being way faster for single). And M5 Pro/Max are coming out next week. They've made big improvements to the GPU as well. If the scaling works out, the M5 Max should be around a 4070 Ti Super.  It's such a shame that bootcamp is dead and the Asahi project is facing so many problems. That hardware all-round is just unmatched. It's almost, dare I say, magical.",AMD,2026-01-24 19:27:59,1
AMD,o19rxp0,"Tbf RDNA2 APU came out in 2022 (Rembrandt) and RDNA3 ones immediately followed in CES 2023 (Phoenix). That was only a month after the desktop 7900 GPU was official, actually. RDNA2 APU did come a bit later after RDNA2 desktop gpu, but still nowhere quite like how long we're stuck on RDNA3.5 if even Medusa Point is still RDNA3.5 in 2027.",AMD,2026-01-23 17:17:04,1
AMD,o16n0nk,"Yes, I keep being downvoted by AMD employees but obviously the price is a huge issue. Z1E was an OK deal, Z2E was too expensive, HX370 outrageous, and 395+ AMD is in dreamland.",AMD,2026-01-23 04:49:50,10
AMD,o17v3l8,"Yeah it's a shame, 100% of my gaming is done on my PC Handheld now. Would love a Handheld with something like the Strix Halo for more power, but the price and no FSR4 support makes it kinda a bad purchase.Â    AMD has just sat on its hands and has allowed Intel to actually (at least seemingly), provide an alternative that is competitive.",AMD,2026-01-23 11:01:59,8
AMD,o15ollc,bigger models at higher quants.  like qwen-3-coder-30b or even 72b,AMD,2026-01-23 01:29:38,2
AMD,o15n519,"It's fast (for shared memory), directly accessed by the GPU and has a lot which is what you want for running large models. Though Nvidia also has their equivalent Blackwell based system",AMD,2026-01-23 01:21:18,2
AMD,o1735gt,good point.  with linux i can only allocate 120GB to my vram.,AMD,2026-01-23 06:50:25,1
AMD,o16q1i1,"Itâ€™s mostly that the revenue from APUs doesnâ€™t warrant the cost of better graphics. Itâ€™s a cold calculation, but thatâ€™s what AMD has to do to survive.",AMD,2026-01-23 05:10:19,1
AMD,o1h1wpg,"What I'm interested in is The Phawx's video where tests it at various power levels. 65W for Panther Lake X is the top end. It is not unreasonable to expect that the performance at 45W might be similar. I want to know what's it like in the 20-30W range, where Strix Point plays",AMD,2026-01-24 18:46:44,1
AMD,o1eplyh,"We already ""know"" that because itâ€™s just a refresh",AMD,2026-01-24 11:11:20,2
AMD,o1hifta,makes me wonder if they are going to refresh the Mac Pro (or studio) w/ a M5 Ultra and just blow the lid off the professional editing rig market,AMD,2026-01-24 19:59:37,1
AMD,o172a0s,"> Z1E was an OK deal, Z2E was too expensive, HX370 outrageous, and 395+ AMD is in dreamland.  well said",AMD,2026-01-23 06:43:19,4
AMD,o17wy66,"Yeah, and to my understanding, AMD's roadmap does not look promising with respect to RDNA 4 mobile APUs.  Panther Lake isn't quite enough of a compelling upgrade over the Z1E--it's definitely a better chip, and it's an exciting prospect for people just entering the handheld PC space, but it's not quite enough to make me run out and upgrade from my Z1E handheld.  Once we get a mobile APU with the raster performance of Strix Halo but with a modern ML-driven feature set for under $1000 (in 2025 dollars) in a handheld, I'll be all over it.  Don't care if it's AMD or Intel.",AMD,2026-01-23 11:17:27,4
AMD,o15p0pk,And those run at like 5-10 tokens per second then?,AMD,2026-01-23 01:32:02,3
AMD,o15o3a1,But for 128GB the memory is super slow. What do you need that for? Both Apple and NVIDIA only use 64GB on an Interface like that (and a faster interface for more than 64GB).  The DGX spark is specifically marketed as a dev kit not a desktop system. The DGX is up to like 8 times faster in AI workloads but in all AI related subreddits people donâ€™t really see a point in that much ram that is that small.  Itâ€™s a solution in search of a problem and the problem hasnâ€™t been found yet,AMD,2026-01-23 01:26:44,5
AMD,o173pfw,It'd help their GPU and technology adoption. The jaded business calculations sure haven't done Radeon any favors.,AMD,2026-01-23 06:55:00,7
AMD,o19a86c,"""Survive"" and it's a smol bean indie company with 32 billion projected full year revenue in 2025 (without a fab and much fewer business ventures compared to Intel, who will once again be in the 50-55 billion range)",AMD,2026-01-23 15:57:09,3
AMD,o19lg7f,"Laptop sales are far larger than desktop sales, and can sometimes command a higher margin.  Handheld is a small but growing market.  I think AMD is missing a great opportunity.",AMD,2026-01-23 16:47:24,3
AMD,o1hlp73,"Well they already have most of it. Especially at the prosumer, smaller production level, almost everyone uses a 16 inch MacBook Pro, where even an M1 Max is still viable today.  What they might capture a large chunk of with the M5 Ultra might be the 3D market. The Blender folks. Huge VRAM, big power, big efficiency and actually available to buy",AMD,2026-01-24 20:14:48,1
AMD,o16oznx,about 40,AMD,2026-01-23 05:03:07,3
AMD,o19lelr,"It's 256-bit LPDDR5-8000 with bandwidth amplification provided by 32MB Infinity Cache, but only for iGPU. I mean, it works for what it is.  If you want 128GB HBM3e, you're looking at $32k, minimum, because those are datacenter cards and they lack any sort of graphics engines.",AMD,2026-01-23 16:47:12,1
AMD,o19bl92,"And yet they see very little headway in the mobile space, because they're less competitive (this, incidentally, where Intel focuses to the detriment of their desktop parts usually).  It's not that AMD can't *make* the parts, of course.",AMD,2026-01-23 16:03:14,0
AMD,o19o5kn,"I mean I think theyâ€™re missing out too, but AMDs bean counters see things differently.",AMD,2026-01-23 16:59:35,2
AMD,o1ho65u,"true, almost every dev i know (web, code and UI/UX) operates on apple silicone either on a laptop or studio desktop     the 3d market is what i was thinking of, especially since they have the rack mountable variant of the Mac Pro wich would feel right at home at a CG Studio",AMD,2026-01-24 20:26:25,1
AMD,o19so7n,"No one is talking about HBM memory.   Question is: what is it actually for? Yes, RAM capacity determines how big a model can be but with a 256bit memory interface it's very limited in performance. Even DDR4 based CPU systems come close to that with much higher performance.    A current Threadripper/Xeon W build easily beats it in both performance and memory bandwidth and memory capacity.   Apple offers 128GB too but with a much faster CPU, GPU and a 512bit memory interface at a much higher speed. Yes, it's more expensive but arguably better value.    The M3 Ultra comes with a 1024bit memory interface.   Both base DGX and M4 (Pro) use slightly faster LDDR5X with 8533MT/s RAM for their memory  The Jetson also exists too with faster memory but only 64GB of it.   In terms of performance Strix Halo is left behind all other options.   Question is: Who is it for? What problem does it solve that others don't already (and much better, AMDs software is still a problem).",AMD,2026-01-23 17:20:39,2
AMD,o19dk4d,">this, incidentally, where Intel focuses to the detriment of their desktop parts usually  Laptops began to outsell desktops *twenty years ago*. It was and is clear where the focus *should* be. What Intel really is failing at compared to where they were, is in the data centre.",AMD,2026-01-23 16:12:05,3
AMD,o1by0z2,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-01-23 23:26:03,1
AMD,o1bzbz1,"0.1 mghz increase. Worst troll release ever. Also more AI? Yeah 10 TP more AI, from 50 to 60. That is going to change EVERYTHING.",AMD,2026-01-23 23:33:00,7
AMD,o1camnh,"Bruh, that's 20% more AI in a simple refresh. I swear, there's no winning with you people...",AMD,2026-01-24 00:34:01,6
AMD,o1cvv3e,60 tops on an NPU that is barely supported.  ROCm itself doesn't support the NPU,AMD,2026-01-24 02:36:06,3
AMD,o1ebsn3,"Amuse can use the NPU to do super resolution on GPU AI generated images. That's a start, I guess.  Software really is lagging at this point. Noise Suppression should automatically be offloaded onto NPU, as should live stream super resolution to deblock low bitrate camera streams. Could also adaptively tune each individual CPU/iGPU AVFS by learning over time. Wonder if it could handle spatial audio for headphones. There's so much it could do and it's just wasted silicon for most people.",AMD,2026-01-24 09:04:24,1
AMD,o18ivjj,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-01-23 13:42:41,1
AMD,o18jnix,Only 100mhz more on the CPU and 100mhz on the GPU. Why even release a refresh?,AMD,2026-01-23 13:46:51,78
AMD,o18k3xr,"OK, a 2% increase in CPU boost clock, 3% in GPU boost clock vs. Strix Halo (MAX 395). Whatever.  But will it actually make it into a laptop? If yes, maybe even multiple? Or will these CPUs just all go into mini PCs?",AMD,2026-01-23 13:49:16,28
AMD,o18vc7g,Sadly those chips STILL use RDNA3.5 so no official FSR4.,AMD,2026-01-23 14:47:00,21
AMD,o18p2oy,"I swear, finding a Ryzen AI MAX equipped laptop with the integrated 8060s is bloody difficult.  Hopefully the 400 series will be more readily available.",AMD,2026-01-23 14:15:08,16
AMD,o18nu8n,amd used to release on paper only.. we will see nothing in real life hehe.,AMD,2026-01-23 14:08:50,5
AMD,o18rd5c,No FOMO from my Max+395 - Medusa Halo or bust!,AMD,2026-01-23 14:26:48,5
AMD,o18neq6,"for the low low price of 2 kidneys, I'm sure",AMD,2026-01-23 14:06:35,3
AMD,o19b4il,"I know these CPUs are for laptops, but it would be interesting to see motherboards sold with these CPUs pre-installed, with 32GB to 128GB of LPDDR5 RAM already installed.    2 NVMe slots  1 PCIe 5.0 slot",AMD,2026-01-23 16:01:10,2
AMD,o18vzaw,"I assume most of the improvement comes from an upgraded NPU? But those leaked specs say nothing of it.   Either that, or this is a cost-saving revision on AMD's end. But nothing about this seems very exciting, was definitely expecting more.",AMD,2026-01-23 14:50:11,1
AMD,o1flvl9,I wouldnâ€™t hold my breath for magical improvement on niche products which also happen to offer twice as bad price to performance compared to a normal system - gaming wise. If AMD wants market share then innovation shouldnâ€™t come at 2x the cost. (8060S vs 4060/5060 system).,AMD,2026-01-24 14:50:37,1
AMD,o1gtu52,i really hope the price is gonna be significantly more affordable than the ryzen ai max 300s cause if not intel is definitely gonna sweep. Panther lake is shaping up to be better and consume less power. On top of that panther lake laptops are already looking cheaper. Maybe should've focused more on the consumer instead of just AI at ces to give people some confidence,AMD,2026-01-24 18:12:22,1
AMD,o18o0ta,"What a leak, bravo!",AMD,2026-01-23 14:09:45,0
AMD,o1954z2,This is so exciting /s,AMD,2026-01-23 15:34:11,-1
AMD,o18ru3j,Obligation to system integrators.,AMD,2026-01-23 14:29:13,28
AMD,o18moue,"They don't have a new product, so they'll keep selling the old. But they were able to improve the old one a bit, enough to make a new revision. Those 100 MHz might be what we see now. There is probably a slightly bigger improvement on the NPU, and most importantly,but not immediately visible, probably some fixes that increase realibility, improve yield, reduce power consumtion, maybe fix some bug that would otherwise need a microcode workaround, tc. Lots of small improvements, none of which really matters by itself, but combined enough to make it worth it. After all, with this being small changes, the risk is low, so not much R&D cost, so essentially just the mask costs to pay (which is still a lot at these nodes).",AMD,2026-01-23 14:02:47,33
AMD,o18lc5j,Because release every year make stonks line go up.,AMD,2026-01-23 13:55:43,12
AMD,o196tth,And it takes another 3 quarters to get that extra 100mhz (on selected skus) btw it's not even right now,AMD,2026-01-23 15:41:52,1
AMD,o19x3tk,"> Only 100mhz more on the CPU and 100mhz on the GPU. Why even release a refresh?  Because this one has ""AI"" in the name?",AMD,2026-01-23 17:41:19,1
AMD,o1bzecb,"They don't have Zen 6 ready for at least the first 3 quarters of 2026, either due to AMD or TSMC, and backporting Zen 6 to N4X is not worth it, and it seems like they're too penny pinching to redo the Soc tile with an RDNA4 iGPU, so here we are.",AMD,2026-01-23 23:33:21,1
AMD,o19wu6y,"> Only 100mhz more on the CPU and 100mhz on the GPU  All for only a mere extra $1,000 USD...",AMD,2026-01-23 17:40:07,0
AMD,o18lgp4,It's fine there's not enough RAM to pair with it in 2026 to make it viable at price points people will pay for it. Probably.,AMD,2026-01-23 13:56:22,12
AMD,o195wp3,"They are stuck with rdna3.5 in mobile until mid 2027 at the soonest (Medusa Halo). Yes, they will likely be unveiling rdna3.5 paired with Zen 6 for the mainstream Medusa Point APU in CES 2027.",AMD,2026-01-23 15:37:43,13
AMD,o19zyl3,"This is my big issue. Rdna 3.5 (basically rdna3) is a pretty outdated architecture, itâ€™s essentially rdna2 with WMMA. Itâ€™s a bit ridiculous to call these products â€œAI MAXâ€ considering they donâ€™t have FP8 support. Itâ€™s not a bad arch, I have a 7900xtx and even a 6900xt still, but the 9070xt feels like real competition. It feels like a close nvidia equal finally. Until rdna4 or rather rdna5 at this point appears in these SoCs, count me out. When rdna5 drops Iâ€™ll be first in line for a â€œAI max 595â€ or whatever they decide to call it. Hopefully they give it a better nameâ€¦",AMD,2026-01-23 17:54:15,5
AMD,o19dygv,"Plenty of MiniPC's with them though, which sort of suggests that there's some reason we're not aware as to why they aren't found in more laptops.  Is it a power budget/cooling issue?",AMD,2026-01-23 16:13:53,3
AMD,o18vsge,Every manufacturer wants to make them.,AMD,2026-01-23 14:49:15,7
AMD,o1k2b0t,They cost as much as a laptop with a 5070 too,AMD,2026-01-25 03:54:48,1
AMD,o1cmxja,"On Newegg, the cheapest Strix Halo laptop is \~$2200 (HP ZBook). I know Strix Halo is supposed to be the best AMD mobile APU, but this is pure profit, low volume part, where AMD has no intention of selling hundreds of thousands of these super-high-end Strix Halo laptops. Strix Halo does great work servicing the mobile high-performance PC gaming market.  [https://www.newegg.com/hp-14-0-touch-screen-amd-ryzen-ai-max-pro-380-radeon-graphics-16gb-memory-1-tb-ssd/p/1TS-000D-1P9V8?Item=1TS-000D-1P9V8&SoldByNewegg=1](https://www.newegg.com/hp-14-0-touch-screen-amd-ryzen-ai-max-pro-380-radeon-graphics-16gb-memory-1-tb-ssd/p/1TS-000D-1P9V8?Item=1TS-000D-1P9V8&SoldByNewegg=1)  This Gorgon Halo will be more of the same if not more expensive than Strix Halo from TSMC's 2026 wafer price increases.",AMD,2026-01-24 01:44:09,1
AMD,o1a1xxt,"You can buy motherboards with these chips integrated, the problem is the pcie slot is only gen4 x4. So if you want to use these chips for their zen 5 cores and new packaging (quad channel ram) to power a dGPU, youâ€™re basically out of luck. For AI workloads it might be ok, but gaming will take a hit, as will most tasks. I run a dual gpu setup, Iâ€™m hoping they make these with rdna5 and more pcie lanes down the road, itâ€™d make a neat setup.",AMD,2026-01-23 18:03:07,3
AMD,o19jhd9,"They are being sold in pre-installed in desktop motherboards, Framework has some ITX boards with them. (Edit; to be more specific, Framework is selling the Strix Halo chips in motherboards with RAM.  These Gorgon Halo chips are simply rebradged Strix Halo ones, only difference being slightly higher clock speeds).",AMD,2026-01-23 16:38:38,2
AMD,o1960cz,From 50TOPS to 55TOPS. It will be the same old from the mainstream lineup.,AMD,2026-01-23 15:38:11,2
AMD,o19a0i3,Never heard someone buy a laptop for a â€œNPUâ€,AMD,2026-01-23 15:56:12,24
AMD,o1bdpaj,"> There is probably a slightly bigger improvement on the NPU  LOL. The NPU that no one uses. The NPU that people can't use on Linux because AMD doesn't support it on Linux. That NPU? Yeah, making something better that's not used because AMD doesn't really support it will make a huge difference.",AMD,2026-01-23 21:44:09,6
AMD,o195br9,"In another word, a Kaby Lake aka something amd fans made fun of even after a decade",AMD,2026-01-23 15:35:03,7
AMD,o1bf0zd,If only they added more PCIe lanes,AMD,2026-01-23 21:50:19,1
AMD,o1iv2ms,rarely coherent comment,AMD,2026-01-24 23:56:54,1
AMD,o1aqke6,imho - 2026 for AMD is all about execution on their AI chips deliveries to meet openAI demands.   This as a small iteration for integrators is fine,AMD,2026-01-23 19:55:25,1
AMD,o1cjazd,"Actually, this â€œGorgon Haloâ€ rebrand is coming Q4 2026. So you might have to bump your timing by another 12 months.",AMD,2026-01-24 01:22:41,2
AMD,o19zf8a,it was already hard sell as is before ram price went boom ðŸ˜­,AMD,2026-01-23 17:51:51,1
AMD,o1dddny,"Not saying the naming isnâ€™t stupid, but arenâ€™t they called AI Max because of the NPU?",AMD,2026-01-24 04:24:10,2
AMD,o19isev,Power budget/cooling shouldn't be an issue; at most a Ryzen Max+ chip will pull \~150 watts or so.  That pretty much the same amount as Nvidia's mobile RTX 5070 TI & 5080 chips.  Laptop makers have few problems cooling those chips & including a 2nd cooler for the CPU as well.,AMD,2026-01-23 16:35:32,4
AMD,o1a4d1q,It's in handhelds. Probably just too expensive compared to other options in laptops.,AMD,2026-01-23 18:13:56,2
AMD,o195h8k,And none actually does!,AMD,2026-01-23 15:35:45,9
AMD,o1dckwr,">Strix Halo does great work servicing the mobile high-performance PC gaming market.  It really doesn't, a $1600 laptop with mobile 5070Ti obliterates it in gaming. There were only ever 2 laptops (Zbook and Flow Z13) with Strix Halo in 2025, and only adding 2 more in 2026 (Tuf A14 and Proart P13). The Strix Halo Tuf A14 will be a tough sell vs the cheaper variant with 5060.",AMD,2026-01-24 04:18:48,1
AMD,o1am8f8,So many people are buying these max chips for AI. 395 + 128gb of ram is really popular.,AMD,2026-01-23 19:35:14,-3
AMD,o198xhu,Poor choice to make fun of considering it was the first production generation bringing native x265 10 bit encode to the igpu.  Overnight changed how efficient my plex server ran. I ran a cheap as hell pentium g4560 for years  The little things matter if you know what they are and use them.  Edit: specified 10bit,AMD,2026-01-23 15:51:21,14
AMD,o1dbldo,It's identical silicon so nope nothing at all,AMD,2026-01-24 04:12:17,1
AMD,o1cj59z,2020 - 2100 for Nvidia is all about AI and it still doesnâ€™t mean we canâ€™t talk about and criticise their sidelined focus on the Client,AMD,2026-01-24 01:21:47,3
AMD,o19nzn2,"I'm purely trying to engage in constructive debate here with the following response.   The dGPU laptop that I have, and have physically seen in stores, are all rather thick and heavy, at least in comparison to most iGPU based laptops.  I guess my question was really more about the packaging logistics.  People tend to buy iGPU based laptops cos they want something thin and light. An iGPU laptop that draws as much power as a dGPU laptop, and requires just as much cooling and weight to go along with that cooling, but still performs below that of a dGPU laptop for games is likely the issue here.  If you want a thicker, hotter, heavier gaming laptop, then you're going to want a dGPU as the tradeoff for that packaging.  The AI Max+ 395 asks for the same tradeoffs, but with gaming performance about on par with a 4060.  If I were making a laptop with it, that would feel like a rather difficult sell to potential buyers, unless the costs can be brought down to less than $1000.",AMD,2026-01-23 16:58:51,1
AMD,o1a8a4e,"I wasn't aware.  I did a quick Google search and found this:  [https://gpdstore.net/product/gpd-win-5/](https://gpdstore.net/product/gpd-win-5/)  Maybe I'm out of touch, but \~$2350 for a hand-held sort of strikes me as being out of the budget range of most who would be interested in a hand-held gaming device.  I'd more imagined hand-helds to be more targetted around the $300-500 mark, but maybe my perception is out-dated?  $2350 seems to be more of a high-end laptop price as it is.",AMD,2026-01-23 18:31:20,2
AMD,o1b4jto,What do you think is the reason why that's the case?,AMD,2026-01-23 21:01:28,1
AMD,o1dnqkf,"Yeah, I should have qualified that Strix Halo is great at servicing mobile ""power-efficient"" high-performance PC gaming market. Strix Halo laptops are powered by 140-watt power adapters while some RTX 5070 mobile laptops use 180-watt power adapters, and some RTX 5070 Ti mobile laptops use 330-watt power adapters.   [https://www.newegg.com/msi-cyborg-15-15-6-geforce-rtx-5070-laptop-gpu-intel-core-7-240h-1-80-5-20ghz-fhd-16gb-8gb-2-ddr5-5600mhz-memory-1tb-nvme-ssd-gen4x4-ssd/p/N82E16834156834?Item=N82E16834156834&SoldByNewegg=1](https://www.newegg.com/msi-cyborg-15-15-6-geforce-rtx-5070-laptop-gpu-intel-core-7-240h-1-80-5-20ghz-fhd-16gb-8gb-2-ddr5-5600mhz-memory-1tb-nvme-ssd-gen4x4-ssd/p/N82E16834156834?Item=N82E16834156834&SoldByNewegg=1)  [https://www.newegg.com/msi-16-geforce-rtx-5070-ti-laptop-gpu-intel-core-ultra-7-255hx-16gb-memory-512-gb-ssd/p/N82E16834156740?Item=N82E16834156740&SoldByNewegg=1](https://www.newegg.com/msi-16-geforce-rtx-5070-ti-laptop-gpu-intel-core-ultra-7-255hx-16gb-memory-512-gb-ssd/p/N82E16834156740?Item=N82E16834156740&SoldByNewegg=1)  Power efficiency is probably not a priority for laptop gamers, who want max performance unless they are away from a power outlet. A dedicated NVIDIA mobile GPU would destroy Strix Halo at higher power. At the end of the day, the cost of Strix Halo is absurd, where they are used in $2200 workstation class laptops.",AMD,2026-01-24 05:36:23,2
AMD,o1at6ai,Yeah but itâ€™s not for the NPU. They are buying it for large unified memory which enables them to have 96Gb VRAM for a model.,AMD,2026-01-23 20:07:41,14
AMD,o1ankrk,"Itâ€™s not popular for the NPU, since the NPU is impossible to use on Linux, and barely usable on Windows with anything other than Lemonade.",AMD,2026-01-23 19:41:34,11
AMD,o19bpfz,"I have pentinum g5400, it's uhd 610 igpu handle everything so easily in my jellyfin server",AMD,2026-01-23 16:03:45,3
AMD,o1dbj6b,"So it's official - RDNA3.5+ is WORSE than 14nm+++ (the first + is Kaby Lake, while the second plus they added with Coffee was a core count increase).   AMD did neither.",AMD,2026-01-24 04:11:52,0
AMD,o1dbr75,I'm aware,AMD,2026-01-24 04:13:20,1
AMD,o1fpb7m,"Not disagreeing on any criticisms - my comments is just an observation that AMD's chance to join the big boys AI club is the successful rollouts in '26. Right now, AMD is an afterthought",AMD,2026-01-24 15:08:33,1
AMD,o1a9m4y,"The legion go 2 is literally impossible to buy and it's 1350-1500. Was available for like a few hours in Canada for pre-order before selling out, and constanly out of stock in the US.  And that has the performance of much older handhelds. Should be about half the speed.  The only PC handheld anywhere near 350-500 is the steam deck.",AMD,2026-01-23 18:37:15,3
AMD,o1e1a4y,"Strinx Point handhelds are available at $1000-1200.  Strix Halo handhelds are available at $2300+ and **have double the gaming performance of Strix point.** People pay double for double the performance. Strix Point is 230 mm2. Strix Halo is again double at 450 mm2.  **Double the silicon, double the performance, double the price.**",AMD,2026-01-24 07:29:10,1
AMD,o1cc4cc,"Economically unappealing product with its large size; Â a solution in search for a problem (large amount of memory but slow and unproven software stack for its alleged use case); mediocre performance for gaming use case with outdated software stack; and most importantly historically poor commitment with OEMs (poor supply in the past, lack of technical support as voiced by oems in the past, lack of cooperation with OEM to co-design laptops similar to intelâ€™s Evo platform)",AMD,2026-01-24 00:42:01,2
AMD,o1apd71,Ah wow youâ€™re right thatâ€™s disappointing.,AMD,2026-01-23 19:49:53,2
AMD,o19d2u7,I ran mine all the way until Alder lake when I moved to an I3 12100 that was originally the kids gaming computer (he got an upgrade). The efficiency of the new chip is staggering.,AMD,2026-01-23 16:09:56,4
AMD,o1fs6bb,It's just so damn annoying they can't ever focus on more than one thing at a time.,AMD,2026-01-24 15:22:57,1
AMD,o1dc2m0,The reality is the PC Handheld is an incredibly niche thing irl (estimated worldwide sales not even close to 10m and that's including all models including steam deck) despite what it seems on reddit.,AMD,2026-01-24 04:15:26,1
AMD,o1eizx3,"I just can't imagine that the market for $2300 handhelds is particularly large though.  People have been asking for Strix Halo laptops pretty much since their release.  People are clearly hungry for them.  Why aren't there any (aside from the three Asus 13/14"" models)?",AMD,2026-01-24 10:11:21,2
AMD,o19dqyh,"I have exactly same upgrade path planned!!! I really want to get my hands on i3 12100 , it's uhd 710 finally includes  av1 decoding.  But i3 12100 seem to be very expensive in my country unfortunately.  Other option is arc a310 gpu , it's also insanely good value for media servers but this card don't even exist in my country",AMD,2026-01-23 16:12:56,2
AMD,o1fuuog,"Because Strix Halo doesn't make sense in a laptop for gaming, just for AI.",AMD,2026-01-24 15:35:58,1
AMD,o1g1afa,"I have the GMKTec Evo X2.  The Strix Halo is so much more than ""just AI"".  It's a productivity monster.  It's basically a 9950x that boosts to ~95% of desktop speed, the quad channel memory controller almost closes the performance gap entirely against a dual-channel desktop 9950x, and the iGPU runs pretty much any game I've thrown at it at native 1440p with maxed settings (excluding ray/path tracing).  The ""AI"" aspect of it is more of a marketing check-box.  Sure. It has a dedicated NPU as well that helps with AI TOPS, but even if we ignored that, it'd be a compelling laptop in its own right.  This is why so many people keep asking for it in a larger laptop format.  I.e. the market is there, the capabilities are there, why isn't it being offered in a 15 or 16"" chassis with a 1440p screen is what many people want to know.",AMD,2026-01-24 16:06:04,1
AMD,o1g2zvu,Huh? A productivity monster? What's stopping people buying a laptop with a 16 core Ryzen and a discrete GPU?!,AMD,2026-01-24 16:13:54,1
AMD,o1g65us,"Nothing is stopping them doing so.  A Strix Halo laptop would ideally be positioned that it's offering the productivity of a top-end laptop CPU, the gaming performance of a mid-range laptop GPU, and be able to be more energy efficient and a bit thinner and lighter than a dGPU based laptop.  Maybe my personal preferences are biased by my desires. I travel internationally for work.  I'd love a high productivity low-medium profile 16"" laptop that weighs no more about 2.0kgs that can also game acceptably when on the move.  I bought a gaming laptop a while back, and I really dislike it's weight, heat, and poor battery life. I know I'm not alone in those desires either.  For mobile productivity with a moderate gaming capability it just feels like the Strix Halo is pretty much perfectly positioned for that, and yet it gets treated like it's little more than a beefy handheld unit. IMO, it's a great product saddled with a marketing team that doesn't know how to position and package it in a way that people have been calling for.",AMD,2026-01-24 16:28:15,1
AMD,o0kwia1,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",AMD,2026-01-20 00:45:49,1
AMD,o0l0ngw,I remember when 5GHz all core required liquid nitrogen.   This is insane. Technology is cool.,AMD,2026-01-20 01:08:37,303
AMD,o0llniy,That doesn't seem too surprising since the 9800X3D was able to hit 5.5 all core.  The whole point of the 9850X3D was that it was just a 9800X3D that tested better so it ran at higher clocks.,AMD,2026-01-20 03:03:51,59
AMD,o0md1ww,"But does Crysis run on it? /s  It was a notoriously hard game to run because developers thought that the clock rate will keep improving. Instead, we went multi-core.  I'm still happy to see almost 6 GHz in a consumer product.",AMD,2026-01-20 05:57:18,19
AMD,o0mism4,9990X3D 6GHz all cores on water?,AMD,2026-01-20 06:43:45,11
AMD,o0mpnkw,"Asrock: I heard you been pumping iron, let see...",AMD,2026-01-20 07:43:11,3
AMD,o0r0348,Nice! I can't wait to be unable to get it!,AMD,2026-01-20 22:21:18,3
AMD,o0llh0t,Will this get cooked by motherboards like the 9800x3d too?,AMD,2026-01-20 03:02:50,11
AMD,o0mh2t2,"As a 9800x3d owner, this doesn't tickle my fancy.",AMD,2026-01-20 06:29:32,3
AMD,o0mmzg3,Itâ€™s got some hype sure but Iâ€™m also sure itâ€™s not gonna be showing results that are worth the price tag,AMD,2026-01-20 07:19:22,2
AMD,o0m9fe2,When is it launching bros,AMD,2026-01-20 05:29:46,1
AMD,o0minjm,**NICE**,AMD,2026-01-20 06:42:34,1
AMD,o0rgdz7,Should I upgrade my 8700x3d to this? I already upgraded my 4080 Super to a 5090 today and already have 64 gigs of DDR5 RAM.,AMD,2026-01-20 23:47:31,1
AMD,o0tpnrp,1MHz 6502 is enough for everyone.,AMD,2026-01-21 09:00:27,1
AMD,o0x5x0p,"id like to get one too, but my 5080 is the bottleneck at 1440p. The 9800x3d is an absolute monster of a CPU",AMD,2026-01-21 20:28:04,1
AMD,o11lfg0,When is it being released?,AMD,2026-01-22 13:36:43,1
AMD,o17dej1,Ich Besitze bereits einen,AMD,2026-01-23 08:20:19,1
AMD,o1bbbmv,Stock clocks of 6GHz with normal cooling when?,AMD,2026-01-23 21:33:15,1
AMD,o0m0xgk,I personally find that impossible to believe but hey.,AMD,2026-01-20 04:32:11,1
AMD,o0p2d5p,Its just binning and an artificial limitation on 9800x3D preventing the same clocks. I can do all core and single core boosts much higher than 5450 with ECLK. 5750+,AMD,2026-01-20 17:00:19,1
AMD,o0mnne6,This gamers nexus video is about to hit different,AMD,2026-01-20 07:25:14,-1
AMD,o0mwws0,"Should I run as fast as I can, give my money to upgrade my 9800x3d?",AMD,2026-01-20 08:50:10,0
AMD,o0oh9su,Does it stand against time and not die suddenly :D  Guess its best binned and all the leftovers going to lower SKU's that die on various MB's especially X3D,AMD,2026-01-20 15:22:00,0
AMD,o0q07kb,Hmm so if mine hits 5225 on all cores in full load. - 25 pbo mobo air cooler. This means i have a golden sample or i just overthink?,AMD,2026-01-20 19:34:33,-1
AMD,o0l0ytv,I remember when to hit 40MHz you had to add a heatsink!,AMD,2026-01-20 01:10:22,121
AMD,o0mz3ih,I remember the turbo button on my 8088  That button yielded 10MHz operation   â™¥ï¸,AMD,2026-01-20 09:10:50,9
AMD,o0nh2zj,I see what you did there.,AMD,2026-01-20 11:50:50,0
AMD,o0n4hve,"It's amazing. But it is suspected that the reason why some x3d chips are dying, tho rarely, on some motherboards is because they are run redlined from the get go from factory.",AMD,2026-01-20 10:02:05,-2
AMD,o0lq4j5,"If the rumors of it being a new stepping are true, it means the silicon has improved enough since 9800X3D launch to call it improved silicon, not just overclocked 9800X3Ds.  These will hit higher frequencies on average at the same wattage as the previous stepping.",AMD,2026-01-20 03:29:06,40
AMD,o0o6ddc,"With manual OC? PBO +200 is 5.425, so rather 5.4",AMD,2026-01-20 14:26:58,0
AMD,o0myylc,"didn't intel hit 6ghz with 14th gen? I know it has it's problems, but it's really amazing what they managed to pull off",AMD,2026-01-20 09:09:32,6
AMD,o0rnrhc,I'm waiting for that 12 core single CCD X3D Zen 6 chip that should hit 6Ghz :),AMD,2026-01-21 00:27:35,2
AMD,o0ly9t2,Well itâ€™s a binned 9800x3d soâ€¦,AMD,2026-01-20 04:16:06,13
AMD,o0oeir4,You mean by some asrock motherboards?,AMD,2026-01-20 15:08:30,5
AMD,o0mwaxs,The 9800x3d is officialy outdated and dead bro,AMD,2026-01-20 08:44:27,23
AMD,o0o25ma,"As a 9800x3D owner I turned on eco mode, because the gains from that last bit of power are underwhelming. Made the thermals a hell of a lot nicer too. I can't imagine the extra clocks make that much of a difference outside of like CS2 or R6S.",AMD,2026-01-20 14:04:22,5
AMD,o0ro2p5,The shopper for this is on a 7800X3D chip or lower your upgrade is a 12 core Zen 6 X3D chip.,AMD,2026-01-21 00:29:15,1
AMD,o0tnw8o,"I'd say 99% of gamers couldn't tell the difference between a bunch of modern CPUs if they were all running the same game without any kind of fps/ frame time monitoring counter. I've been gaming since I was a child and I did the test myself, put the CPU in eco mode and played my usual games (space marine 2, hell divers 2, remnant 2, monster hunter wilds...) without the Afterburner layout, I didn't notice anything. Then I enabled the layout and benchmarked both the average and 1% low fps... and there was indeed a performance degradation, more significant than the difference between a 9800X3D and a 9850X3D (according to AMD it's between 2% and 5%, 3% in most cases in CPU limited scenarios).",AMD,2026-01-21 08:43:40,1
AMD,o0mb2m7,January 29th,AMD,2026-01-20 05:42:02,3
AMD,o0mj61f,Why? 9800X3Ds manually oc'd can reach pretty close to that or even hit the same.  With the 9850X3D all it's gonna take is +200 on PBO.  Obviously it's not gonna hit that all core on every workload.,AMD,2026-01-20 06:46:51,11
AMD,o0tm495,My 9800x3d can hit 5.5 all core without even cracking 70 degrees under full load so it's not surprising that processing improvements and binning can get to 5.7 (likely with a bit more wattage that I'm using),AMD,2026-01-21 08:26:30,1
AMD,o0qohfs,"launch OCCT, CPU stability test, extreme - steady, instruction set AVX 512, fixed number of threads: 16, monitor your effective CPU clocks with HWiNFO64 (and enable Ryzen Snapshot CPU polling), then come back here.",AMD,2026-01-20 21:26:48,2
AMD,o0l1fuj,Oldest I am is that you couldnâ€™t use the phone and the internet at the same time haha.,AMD,2026-01-20 01:13:04,48
AMD,o0mt51v,"I remember when those mad lads drove the 8086 at 8 mhz instead of the standard 6. Insane. That was a 20% performance uplift.   For reasons nobody at that time got including myself, I liked PCs more then c64/128/MSX and later the Amiga)",AMD,2026-01-20 08:14:49,9
AMD,o0mk4oe,"I remember when to hit 40 MHz you didn't need a heatsink, but you did have to physically replace the crystal oscillator on the motherboard.",AMD,2026-01-20 06:54:53,7
AMD,o0npmrs,I remember hitting the turbo button to go from 2 MHz to 4.3 Mhz,AMD,2026-01-20 12:51:34,5
AMD,o0lbdid,40Mhz is witchcraft.,AMD,2026-01-20 02:07:28,6
AMD,o0m5o1b,I have a scar from on my knuckle from when I learned computer parts get hot. It was from the surface of a 386dx40 that did not require a heatsink.,AMD,2026-01-20 05:03:18,3
AMD,o0penqz,"The 486DX2/66 did not require a heat sink at all.  I still added one to mine, but it was a tiny chunk of aluminum with a tiny fan, and no TIM.",AMD,2026-01-20 17:57:11,3
AMD,o0x4erl,And press the Turbo button.,AMD,2026-01-21 20:21:16,2
AMD,o13hcto,66MHz was it for me :),AMD,2026-01-22 18:54:10,2
AMD,o0n1ewg,"Had an 8088 (clone?) with a turbo, every game was unplayable unless i turned it back down to 4.77 Mhz. Good times.",AMD,2026-01-20 09:32:59,6
AMD,o0lrclk,"You know, part of me really wants to get one.  Then the smarter part of me says ""You know, your GPU is running at 100% when you're gaming already, while your 7700X GPU sits at 45-55%.""  So I decide to wait for Zen 6.  That's what I keep telling myself, anyway.",AMD,2026-01-20 03:35:58,17
AMD,o0oisyo,"True, although they'll also be making 9800X3D on the new stepping.",AMD,2026-01-20 15:29:22,2
AMD,o0rz3m0,"Async eclk. 5.5 is a pretty bad chip, the worst i've seen was actually about 5.6ghz (and the worst 9950x3d vcache ccd, 5.7)",AMD,2026-01-21 01:31:15,0
AMD,o0ovrm3,And Asus with smaller reports on MSI and Gigabyte.,AMD,2026-01-20 16:29:49,2
AMD,o0todak,You put a 7800X3D and a 9850X3D side by side running the same game at 1440p high setting without a frame counter and very few gamers if any could tell the difference.,AMD,2026-01-21 08:48:10,1
AMD,o0mcnqh,Fml and wallet,AMD,2026-01-20 05:54:13,3
AMD,o0tmf9t,"To be honest, with what my 9800x3d can do at 5.5 all core. I wouldn't be surprised if it did 5.7 all core in everything but AVX 512",AMD,2026-01-21 08:29:29,1
AMD,o0tffo8,Well to be fair that's not realistic workload in any shape and form. AVX2 instead of AVX512 imo,AMD,2026-01-21 07:24:31,1
AMD,o0lbvvn,Awaiting for the teletype and arpanet dude to rock up,AMD,2026-01-20 02:10:13,24
AMD,o0nin20,Damn that takes me back. Crazy that we can play games online in the palm of our hands now.,AMD,2026-01-20 12:02:36,2
AMD,o0ur97z,Who needs mouse. You can do all on keyboard.,AMD,2026-01-21 13:49:16,1
AMD,o0pr201,Why would you challenge a CPU to a fist fight,AMD,2026-01-20 18:52:51,9
AMD,o13hj8k,Mine came with a small heat sink.,AMD,2026-01-22 18:54:56,1
AMD,o13hstt,"Which was actually making your CPU run half slower. I never figured out, what was the purpose. Maybe some crude power/heat management?",AMD,2026-01-22 18:56:06,2
AMD,o0na5oi,slowdown.com,AMD,2026-01-20 10:53:20,3
AMD,o13i3vg,"You can get the same effect in DOSBox with various ""MHz"" settings.",AMD,2026-01-22 18:57:27,2
AMD,o0o1n3l,"> while your 7700X GPU sits at 45-55%.  CPU usage is basically a worthless statistic. It's a holdover from when systems were far simpler and the CPU was literally just an integer unit. You can have high ""usage"" and have tons of unused CPU resources, and you can have low usage and be completely tapped out. All it tells you is thread scheduling which itself doesn't tell you much because a thread stuck waiting will report higher usage.",AMD,2026-01-20 14:01:32,9
AMD,o0miyn5,"3D v cache would make a huge difference in a lot of games, but I would wait for Zen 6 because of better memory controllers anyway",AMD,2026-01-20 06:45:09,8
AMD,o0oqw4s,FPS minimums and 0.1% lows are good reasons to upgrade a CPU.,AMD,2026-01-20 16:07:21,4
AMD,o0v9lir,With the arch changes from Zen 4 to Zen 5 plus more clock speed on the newer part I think we may see gains in the 15% range when cpu bound.  But the primary reason I would do that change is resale value.  Alot of people are waiting for the 12 core Zen 6 X3D chip so I would rather sell my Zen 4 chip now and go 9850X3D while resale value is still high for the 7800X3D then wait for the Zen 6 chip to drop in a year from now then try to sell it will get less money for it.,AMD,2026-01-21 15:22:08,2
AMD,o0tm444,"The same way testmem5 can expose RAM instability after 30 minutes but you might game for decades without a single RAM error. AVX2 stability test is also unrealistic, most users buy a 9800X3D for gaming and no game uses 16 threads at full usage and 130+ watts. The sole purpose of pushing the CPU like this is exposing PBO instability / clock stretching as well as assessing your cooling setup.",AMD,2026-01-21 08:26:28,1
AMD,o0lc795,Dot matrix printer is in my wheelhouse as well haha,AMD,2026-01-20 02:11:58,10
AMD,o0pxqru,It was more of a Michelangelo finger touch,AMD,2026-01-20 19:23:01,7
AMD,o17gueu,I think how fast games ran was based on CPU speed. To make them slower and playable you made your CPU slower.,AMD,2026-01-23 08:52:05,2
AMD,o0mm912,"But when youâ€™re GPU limited there is 0 difference (avg, 1%, 0.1% lows) between 110$ 7500F and 9800x3d. 3dv cache cpu only make sense when youâ€™re on 5090, playing sim games or youâ€™re chasing 600+ fps in competitive titles with 600hz screen. For avg aaa gamer with midrange gpu itâ€™s rather pointless, better to get cheaper cpu and put those money into stronger gpu",AMD,2026-01-20 07:12:55,-9
AMD,o0swqp3,"stupid question here, I bought a used am5 system with a 9900x, and was going to swap to this new cpu, but if my 5090 maxes out the main game i play ( with 2x frame gen ) is there any reason to swap cpus?",AMD,2026-01-21 04:54:43,1
AMD,o0uez7u,"> no game uses 16 threads at full usage and 130+ watts  Cities Skylines 2 I guess, and other simulation games.  However there isn't anything relevant to personal computing that uses AVX512 instructions under full load.",AMD,2026-01-21 12:35:39,1
AMD,o0nagg9,Shoot we have at least 100 dot matrix printers in my office right now that get daily use.,AMD,2026-01-20 10:55:58,3
AMD,o0rx0en,Like touching the old cigarette lighters that were in cars once they heated up,AMD,2026-01-21 01:19:21,1
AMD,o0mq9aw,"Not exactly, your CPU may sits at 50% but one of the cores could reach it's limit and that thread could not offload to other core. Same thing goes to my old 10900K, it never hit 100% when gaming maybe not even 60%, but when I swiched to 9950X3D I can see that FPS boost immediatly also much more stable 0.1%.",AMD,2026-01-20 07:48:36,10
AMD,o0moaa2,"That's cope. Even in my case where I would be ""GPU limited"" in MH Wilds with my 5080 I'd still get another 20% or so FPS going from 7500F to 9800X3D. It's not only due to the bigger L3 cache, but also the improved IPC + clock speeds. I actually do have the 7500F and I get significantly worse performance than others with 9800X3D. Same thing in Cyberpunk 2077.   EDIT: For anyone who actually believes this there are showcases in 1440p and 4k where you can see gaps between 7500F and 9800X3D:     https://youtu.be/K8pPJZUGHJI?si=HRLgZHVdgiMo8hXy   Is it always a *huge* difference at 4K avg? No, but it is there, and it is huge at 1% lows and avg 1440p in games like Black Myth Wukong for example.",AMD,2026-01-20 07:30:52,11
AMD,o0uact3,Depends on the CPU you have now.,AMD,2026-01-21 12:02:50,2
AMD,o0utyp2,Maybe some emulator,AMD,2026-01-21 14:03:42,1
AMD,o0rzprz,"*I* wasn't messing with a lighter, but also I have a scar on my face from my cousins messing with the door lighter in the back of grandparents Audi. This almost 40 years ago.",AMD,2026-01-21 01:34:49,2
AMD,o0ms081,Did you even read my post?,AMD,2026-01-20 08:04:22,-1
AMD,o0mr79d,">Even in my case where I would be ""GPU limited"" in MH Wilds  Are you sure you are GPU limited in wilds? I have a 5080 and even then my 7800X3D is the bottleneck.",AMD,2026-01-20 07:57:02,5
AMD,o0mogc3,"Thats not cope, you not gonna gain any fps while having gpu pegged at 100%. Itâ€™s funny how people donâ€™t understand that",AMD,2026-01-20 07:32:21,-4
AMD,o0n6q9c,"It's 99% L3 cache, there was hardly any IPC improvement with zen 5 and 5% faster clocks translates to maybe 2% better performance, but only at like 1080p with a 4090",AMD,2026-01-20 10:22:41,0
AMD,o0uum1t,a 9900x,AMD,2026-01-21 14:07:10,1
AMD,o0w58b4,"Console emulators don't load all cores. Well, unless you run a handful at the same time, but that's pushing it",AMD,2026-01-21 17:44:40,1
AMD,o0s0kjz,"Yeah been there. Thought the coil was cool because it wasnâ€™t glowing and wanted to see what it felt like, turns out my dad had used it like 5 mins prior and it just wasnâ€™t still glowing hot.",AMD,2026-01-21 01:39:46,1
AMD,o0mt50f,Did you even read my first line lol,AMD,2026-01-20 08:14:49,5
AMD,o0mre6j,"Wilds might be an exception due to the whole DLC checking thing, it's not purely GPU limited. Fundamentally though, in a lot of GPU limited scenarios it would help to upgrade CPU unless it's 4K in certain games, and even in those it should help with L3 cache for 1% lows. For cyberpunk it's still true though, I am definitely not getting the same perf on my 7500F as others eith 9800X3D",AMD,2026-01-20 07:58:46,0
AMD,o0mpq28,"It is cope. A better CPU would improve 1% lows due to handling game logic faster so it won't risk stalling while the GPU is doing it's work. GPU utilization as a metric doesn't tell everything going on, the GPU can still be bottlenecked by the CPU making it wait for logic calcs or fetching mem. The extra L3 cache is the big difference maker in efficiency in these scenarios. Also GPU driver overhead is taxing on the CPU",AMD,2026-01-20 07:43:49,10
AMD,o0v9sgt,"Given all the benches I've seen, the 9900x will def have boosts in FPS minimums and maximums.  Several benches show 9800X3D having a near 50% FPS boost in minimums for BF6, I'd wager 9900X is around 40%-ish in a bump.",AMD,2026-01-21 15:23:03,2
AMD,o0w7g03,I mean the PS3 emulator that uses AVX 512,AMD,2026-01-21 17:54:18,1
AMD,o0mtijv,"Well, i guess you didnâ€™t",AMD,2026-01-20 08:18:17,-1
AMD,o0mrv9m,">due to the whole DLC checking thing  Nah, thats not the reason.  1. I have the check disabled with a mod  2. The check only runs near the DLC palico, its irrelevant in combat  Wilds is a heavily CPU limited game, 9800X3D can reach like 100-120 FPS...",AMD,2026-01-20 08:03:07,6
AMD,o0mpu4c,There is simply no data that supports that :),AMD,2026-01-20 07:44:49,-2
AMD,o0w8rxx,I know. RPCS3 doesn't load all 8c16t with AVX512 instructions.,AMD,2026-01-21 18:00:05,1
AMD,o0mvb79,"OK, I will give you some new things to prevent you continue spreading false information. You assume a GPU at 100% means it already reached 100% of what it can do but that's simply not true. There are many aspects of a GPU that can affect performance like core, vram, 3d engine, output refresher. But whenever one part is at 100%, GPU report will say it is 100%. You want proof? If it is really 100% and no more headroom, why is the power consumption still fluctuating not steady? Now, when a GPU hit 100%, it can still improve FPS if given some extra kick such as better CPU or better Ram to ultilized other parts which were bottlenecked before.",AMD,2026-01-20 08:35:02,9
AMD,o0mt0va,"You're probably right it uses more CPU than some other examples I could have provided. It's just one of those scenarios where I'm gaming at 1440p and my CPU is sitting at maybe 40-50% max while the GPU is at 100%, so it looks like the GPU is the major bottleneck.",AMD,2026-01-20 08:13:42,3
AMD,o0p8qc2,Can confirm,AMD,2026-01-20 17:29:59,1
AMD,o0mpzmw,Just keep coping then,AMD,2026-01-20 07:46:10,2
AMD,o0wlm3n,"then you're right, the most realistic use case would be running 10-12 threads or so of a variable AVX2 load",AMD,2026-01-21 18:56:15,1
AMD,o0xa88h,No mention of how many zones it has. No mention of if you are allowed the honor and privilege of using local dimming in SDR mode without needing to enable HDR as most mini led laptops do not allow this.,AMD,2026-01-21 20:47:33,6
AMD,o1h9pzc,"Feels like it should be $1400, not $2400",AMD,2026-01-24 19:20:44,1
AMD,o148i1n,"Fucking stupid, so in SDR it's a TV from 2009",AMD,2026-01-22 20:58:51,3
AMD,o148vvy,"I don't know, the article didn't indicate. But going by most laptops, that's probably the case. The main windows laptop that actually allows you to toggle it on in SDR is gonna be the Asus ROG scar. Almost none of the others allow it unless you have HDR enabled.",AMD,2026-01-22 21:00:39,2
AMD,o1j6h6o,"Oh! The 32 mb was my first ever GPU, but back in like 2007. I found it in a box of PC parts my uncle gave me, and I put it in a Dell Office tower with a Pentium 3 800mhz and 512 MB of RAM. It wasn't great but I started learning Blender on it.",AMD,2026-01-25 00:56:16,5
AMD,o1jtqvm,"Made in Canada? Biggest shocker there, what manufacturing facility do/did they have?",AMD,2026-01-25 03:04:50,1
AMD,o1jh31r,"Your comment gave me a lot of nostalgia, lol. Thinking back to cobbling together a PC from hand me down parts was such a neat childhood experience, it's hard to believe how much more powerful things have become since then!",AMD,2026-01-25 01:55:05,2
AMD,o1k1k58,ATI was Canadian.,AMD,2026-01-25 03:50:20,2
AMD,o1jqp0w,"You're telling me! I have a studio in a box now, the ability to do the things that computers can do now easily seemed too fantastical to me back then.",AMD,2026-01-25 02:48:00,2
AMD,nz79v91,"The fact that it can even compare to AMDs halo product, which the avg consumer canâ€™t afford is a win for Intel. Intel has plenty on leg room to expand the GPU too.",Intel,2026-01-12 18:02:37,11
AMD,nz5c8tz,This suffers from bandwidth bottleneck. Strix halo is Quad channel while panther lake is dual. An igpu would benifit greatly with a quad channel,Intel,2026-01-12 11:58:24,12
AMD,nzgoxuf,"This thing is absolutely nuts  AMD BTFO unironically, I'm floored. I never, ever would have considered an Intel chip before 2025, now this is the most obvious laptop part ever. AMD is surely sorely regretting recycling the same 780M and 890M chips for another entire gen, betting that Intel would continue stagnating.  This thing is gonna be a monster in handhelds.  I really, really wanted a Strix Halo laptop, but the lack of SKUs, price and the inflexibility with RAM kind of make it unappealing to say the least, not to mention the power draw compared to Panther Lake is unwelcome. These laptops are gonna be probably the best x86 in mobile has eaten in a very long time.  On top of that, it's almost making the 5050 look like a stupid part in a laptop. Why bother when you have a vastly more power efficient iGPU that will handle every desktop workload on top of being viable for gaming?",Intel,2026-01-14 01:47:11,2
AMD,nz4rv7t,"â€œTakes on strict haloâ€ at about half the performance (:  Title aside, this looks pretty great.",Intel,2026-01-12 08:54:08,3
AMD,nzi9w6l,"Amd hasnâ€™t even reached 30% market share mobile yet (oscillating between 20% and 26% since 2020) and are about to be almost wiped from existence again save some low end designs using Ryzen â€œAI 7â€ 445 (6 core, 2+4, 4CU iGP).",Intel,2026-01-14 08:34:10,1
AMD,nznrlo6,"The performance looks fantastic for high end $1k handhelds.    But the ""80% faster than AMD's 890M"" claim is absolute bullshit.  They tested against the HX370 with LPDDR5 5600.  That said, against an 890M that *hasn't* been crippled, it should still be 40-50% faster which is great.",Intel,2026-01-15 02:38:40,1
AMD,nz7d64j,"Will intel make it affordable for consumers though, or price it like LNL (2000+ USD laptops and up)",Intel,2026-01-12 18:17:35,7
AMD,nz7r7or,"""compare"", it is half the performance. Still good for what it is, assuming it is priced right",Intel,2026-01-12 19:20:44,3
AMD,nz5loww,"Well it's not just memory bandwidth. It's got about as much bandwidth as it needs to feed the Xe3 cores.   Panther Lake's GPU tile size is only 54mm^2 while Strix Halo's GPU is 308mm^2. For Panther Lake to compete with Strix Halo it would need 2-4Â times as many Xe3 cores probably. That'd be expensive. There's a reason Strix Halo is so expensive and kind of low volume, bigger CPU more RAM and more expensive motherboard aside.",Intel,2026-01-12 13:04:48,14
AMD,nzbn738,DDR6 can't come too soon for igpus too. But in reality memory bandwidth will stay an issue for a long time. Of course cramping enough compute power in such a format is an issue too,Intel,2026-01-13 09:02:19,1
AMD,nzwms44,Framework desktop motherboard?  https://frame.work/products/framework-desktop-mainboard-amd-ryzen-ai-max-300-series?v=FRAFMK0004,Intel,2026-01-16 12:02:44,1
AMD,nz5dj5h,"Half the performance, half the power, (more than) half the price.",Intel,2026-01-12 12:08:11,16
AMD,nzd3th0,I preordered X7 358H laptop for 1300,Intel,2026-01-13 15:11:48,3
AMD,nzgpiya,Bro nothing's going to be affordable in computer hardware at this rate,Intel,2026-01-14 01:50:29,2
AMD,nzi9j83,There are plenty of LNL laptops around 1000 what you on about,Intel,2026-01-14 08:30:40,1
AMD,nzoqfoa,Lunar lake is in sub 800$ laptops now and 12xe cpus are available for sale 1300$ despite the ram and cpu shortage.   The comparison is good even before likely price hikes for strix halo,Intel,2026-01-15 06:39:34,1
AMD,nz9efnf,"Weâ€™re talking mobile chipsets here, strix halo is what happens when you throw efficiency out the window, with Power (TDP) range, typically from 55W up to 120W. The ultra H 300 has default TDP of 25W, with Maximum Turbo Power (MTP) going up to 65W-80W. Intel has a better design, if they threw 40 XeSS3 cores on it, it would prolly run circles around Strix.",Intel,2026-01-13 00:08:25,3
AMD,nzhjs1h,"Strix Halo is double the die size, this should be compared to Strix Point.  But price will tell everything.",Intel,2026-01-14 04:53:30,1
AMD,nz5xoxm,"> It's got about as much bandwidth as it needs to feed the Xe3 cores.  GPU's will take all the bandwidth you can feed them. It won't help EVERY benchmark, but it will help many.  I'd rather see 256-bit bus on something like this. maybe 192 since you can do that with LPDDR5X etc.",Intel,2026-01-12 14:13:42,6
AMD,nz7ptl5,">Panther Lake's GPU tile size is only 54mm^(2)  is this confirmed for the bigger tile?  edit: also, Halo has all the IO, en-/decoders, etc. in the ""GPU"" tile, so the comparison isn't quite valid",Intel,2026-01-12 19:14:20,2
AMD,nz5j09r,And about four orders of magnitude more availability.,Intel,2026-01-12 12:47:16,12
AMD,nz8m3ma,"> (more than) half the price  Have we seen pricing? Not doubting it, I just haven't seen anything personally but probably missed it.  Strix Halo does seem to be a pretty mythical chip due to its price.",Intel,2026-01-12 21:44:42,1
AMD,nzanyas,Keep in mind that each Xe3 core is about as wide as an AMD WGP. We're looking at 1536 vs 2560 shaders. The B390 is 60% as wide as the 8060S. 20 Xe3 cores would match the 8060S in width. 40x Xe3 is as wide as the 7900GRE.,Intel,2026-01-13 04:15:53,4
AMD,nzgqkco,"Bingo  Strix is also limited by being RDNA 3.5 and no FSR4, so it's rather dependent on raw throughput, and it can't possibly fit in a comfortable handheld that would last for more than an hour and a half under load.  I really, really appreciate what AMD has done historically in the APU space, but it is genuinely time for vendors to start considering Intel. The strides here are absolutely immense. They went from an iGPU being a thing that can do basic graphics and 2D gaming to something that competes against lower end NVIDIA parts at less power draw and can actually legitimately game. It's bonkers. In mobile it's a no brainer.  Of course, it's going to be interesting seeing AMD's next UDNA architecture and what they can pull off, but competition never hurt nobody, and it was sad seeing AMD stagnate in the APU space of all things, their bread and butter that gave them pretty much the entire console market plus the Steamdeck. The entire Windows and Linux handheld market has been nothing but AMD for years. This is even better than Lunar Lake.  We're getting to the point where Intel could legitimately compete in the home console space and make a really great product, but realistically they can't undermine AMD's relationship with vendors at this stage. I hope they keep it up, it would really be cool to see an AMD vs Intel APU console war generation.",Intel,2026-01-14 01:56:20,2
AMD,nzixqmc,It's about 50% bigger die with 1 CCD (which seems comparable CPU performance),Intel,2026-01-14 12:07:34,1
AMD,nzbnge8,"This is a big of exaggeration, as you can see with Nvidia moving to gddr7. While bandwidth has increased substantially, performance is clearly limited by lack of compute power",Intel,2026-01-13 09:04:52,1
AMD,nz610q1,">GPU's will take all the bandwidth you can feed them.Â   Didn't deny that. But 12 Xe cores is presumably considered the sweet spot, that's all I'm saying. And Strix Halo only has twice as much bandwidth to feed a GPU die 6 times the size of Panther. I'm sure it has more cache, but still. I think Intel would consider triple or quad channel memory not worth the costs. It would require new i/o, new pins, new motherboard, more RAM, and all, for what's essentially the lowest volume product.  Besides, Intel already has Nova Lake AX in the backlog, or whatever it's going to be called. Practically intel's strix halo. It'll have Xe3P cores, more powerful than Xe3, thus deemed more worthy of the halo treatment.",Intel,2026-01-12 14:31:23,1
AMD,nza15gk,"No official confirmation yet, but JayKihn leaked the tile size for the 12Xe SKU last year. Another user somewhere else said the 4Xe GPU is 33mm2.Â Â  https://x.com/jaykihn0/status/1812898063502938260   And even without the PHYs and NPU, from what I see, Halo's GPU tile is still like almost 3 times as big. So yeah, it's on another class, that's my whole point.",Intel,2026-01-13 02:11:11,2
AMD,nz68s35,Yep,Intel,2026-01-12 15:11:22,2
AMD,nzao8ks,"I have a pre-order in for an MSI 14"" at B&H for $1300. 358H, 32GB LPDDR5X-9600, 2TB, 1200p OLED. I've seen some lower-end PTL laptops rumored around the $900-$1k starting range, but those are likely the 4Xe chips. Wildcat lake with its tiny 2Xe GPU is probably going directly into the budget sector.",Intel,2026-01-13 04:17:34,2
AMD,nzdgaj0,Good catch.,Intel,2026-01-13 16:10:17,1
AMD,nzhkchb,AMD is dormant on the APU space since it had basically the monopoly for x86 because Intel was just bad.  They are taking one of the old Intel's book by releasing rebrands and reashes,Intel,2026-01-14 04:57:28,2
AMD,nz657hj,"> But 12 Xe cores is presumably considered the sweet spot  By what? much larger Xe3 GPU's exist.  We have nothing to compare against in Intel-iGPU-land that has 256bit memory.  Strix Halo die size isn't the metric you want either. It's only 2x the fps (and who knows, panther lake could be 2x its own fps with doubled memory bus, but we'll never know, because Intel won't release a strix halo competitor)",Intel,2026-01-12 14:53:19,0
AMD,nze3jni,"Halo's die is still quite a bit bigger, but from the Intel side, you need to include IO, GPU and about half of the compute die which has the MCs, encoders / decoders, etc. to match the ""GPU"" die of Halo, so it is more like 200mmÂ² to 300mmÂ² when compared",Intel,2026-01-13 18:08:18,1
AMD,nzkoj06,AMD is paying more attention to NVIDIA for sure. ESP on the data center side.,Intel,2026-01-14 17:33:11,1
AMD,nz696vi,">much larger Xe3 GPU's exist.  The biggest one for the moment is on Panther Lake X CPUs. I wouldn't know if there's something bigger tbh.  >Strix Halo die size isn't the metric you want either.\\  Sure you can't compare two different architectures. But all I need to know is it's faaar bigger.  >panther lake could be 2x its own fps with doubled memory bus  That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.",Intel,2026-01-12 15:13:24,1
AMD,nzgcub2,"Well Panther uses mixed processes, and hybrid tiles are bound to be a bit less space efficient than putting everything on a single die. And to be fair, Halo GPU uses N4P process while Panther GPU uses N3E So, still not directly comparable.   Gotta say though, Arc's PPA has improved a lot since Alchemist and Battlemage.",Intel,2026-01-14 00:38:45,2
AMD,nz753k2,"> That would be within Strix Halo territory. I highly doubt it. A GPU with bigger bandwidth will access things and perform raster operations faster, but it won't get much faster at actually processing vectors and other calculations. Gotta need more cores for that.  Yeah, it doesn't matter how much memory bandwidth you have if the GPU doesn't have the raster performance to keep up with the flow of data. Case and point, AMD's R9 Fury X. Released with 4096bit bus HBM. Had a total memory bandwidth of 512GB/s. Yet the GTX 980 Ti released with a 384bit bus and 336GB/s memory bandwidth and it out performed the Fury X in pretty much everything.   That said, I have no idea how close the iGPU is to being bandwidth bottlenecked at 1080p. But I very much doubt doubling it would also double the frame rate.",Intel,2026-01-12 17:41:14,4
AMD,nziy0qm,"Yeah, I think Intel has done a great job with the improvements, I just don't want to overhype things.",Intel,2026-01-14 12:09:36,1
AMD,nxy9s8q,"Hopefully they deliver. Amd needs a shake up in the APU market, mostly the GPU side of it.",Intel,2026-01-06 04:25:51,35
AMD,nxy85c1,That article claims it on par with the 4050 laptop. Jesus christ,Intel,2026-01-06 04:15:26,35
AMD,nxygoos,this is nice but the handheld market could use less ultra 9 and 7s and more ultra 3s.  the closer they can get to the nintendo 2DS XL in size while being under $400 the better.,Intel,2026-01-06 05:12:20,31
AMD,nxywtjt,I am definitely looking to get a gaming handheld PC with PTL in it. Gonna cost a fortune probably but it's my first and intend to stick with it for a long time  The only thing that would stop me is if they skimp on ram... Which might be a very real problem,Intel,2026-01-06 07:22:44,3
AMD,nxz3n59,77% faster while using 80% more power.  I rather see power matched benchmarks.,Intel,2026-01-06 08:25:58,10
AMD,nxzqm0j,"happy about more handheld focus, genuinely have put in more hours on my steam deck than my pc setup this year. i have my eyes on ARM going forward as well",Intel,2026-01-06 11:52:57,2
AMD,ny14mg9,fun to see them tout XeSS MFG on mobile gpus while the B580 still doesnt have it....,Intel,2026-01-06 16:28:10,2
AMD,nxzumfv,Really excited to see these chips on handhelds.,Intel,2026-01-06 12:22:50,1
AMD,ny3a7m6,"Iâ€™ll always want a really good handheld besides my PC. Currently own a Legion Go S and the Switch 2 so this is good for everyone. AMD stays on their toes and if intel is good and gives us a SteamOS native device, Iâ€™ll definitely try them next upgrade. The",Intel,2026-01-06 22:21:45,1
AMD,ny52cw8,"I'm looking forward to this, especially for a gaming handheld/mini pc device, having a gpu that is nearly capable of a RTX 4050 with that power profile could be game changing.  Plus all the existing Intel XESS features are icing on the cake, although support for that scaler is flakey. I'm just hopeful that more games will support XESS.",Intel,2026-01-07 04:03:23,1
AMD,ny5mck0,"Can anyone say real life performance diff, and how much increase in battery life in real laptops, Because I feel many times those ppt numbers don't nearly match real usage (especially when ppt numbers are huge).   Can I get Mac like battery(or atleast 7hrs with no performance drop) and how much does it compare to Mac m1/a18 air performance with them on a $600 laptop.(Assuming fedora/mint as OS)",Intel,2026-01-07 06:22:50,1
AMD,ny68bvw,"I don't understand, we need to see the price of this thing, because otherwise we have to compare it to the AMD 8060s which will be more powerful, but even the cheapest machine with that costs â‚¬1500/â‚¬2000. We just need to see what price point this chip will be offered at.",Intel,2026-01-07 09:39:36,1
AMD,ny87s84,"On just 2 channel / 128 bit RAM, well done Intel!",Intel,2026-01-07 16:51:19,1
AMD,nyaqr0z,An ancient Chinese proverb (roughly) states: *'Talk...* does not make rice...' ðŸ¤”,Intel,2026-01-07 23:35:58,1
AMD,nybohxz,Steam deck 2??? Take my money gaben.,Intel,2026-01-08 02:30:03,1
AMD,nzlao1v,It does look like an amazing performer. I hope the Linux drivers are up to the taskâ€¦ Windows based handhelds have been pretty bad because of Windows.,Intel,2026-01-14 19:11:22,1
AMD,nzmn683,Waiting for this since AMD 890M was disappointing,Intel,2026-01-14 22:54:25,1
AMD,nxyuq77,"Who cares, gives cheaper powerful GPUs for 2k, 4k gaming",Intel,2026-01-06 07:04:00,-18
AMD,nxznnuh,"Yeah, RDNA 3.5 lasted way too long. Admittedly, RDNA4 was a special case where they gave up on a mobile version in favour of getting UDNA ready but that's their own fault. Hopefully, this pushes them to make UDNA a mobile focused architecture as well and perhaps push more cores in igpus to take back the integrated graphics crown. Competition is very good for the consumer.",Intel,2026-01-06 11:29:47,12
AMD,ny16bso,agreed. these rehashed mobile chips with bad upscaling are well beyond their lifespan.,Intel,2026-01-06 16:36:00,4
AMD,nxyagzo,"I looked at the benchmark scores they put out and it looks pretty promising, apparently the 12XE core variant can score somewhere around 6300 on Time Spy graphics (https://www.notebookcheck.net/Early-Intel-Panther-Lake-iGPU-benchmark-impresses-with-50-faster-performance-vs-Lunar-Lake.1138923.0.html).  Intel is comparing a 4050 with low wattage (60W system TDP IIRC) so it's not as good as the full powered 4050 which scores around 8000 on Time Spy. On low powered 4050s though like the one in the XPS and other thin and lights it will compare pretty evenly. It also outscores basically any 3050 on the market since the highest powered ones get around 4500-5000 on Time Spy (which was already matched pretty decently by the old 8XE core GPUs)",Intel,2026-01-06 04:30:18,23
AMD,nxzdq9g,"It will depend on game to game basis. Some perform well on iGPUs, some tank hard due to memory bandwidth or whatever issue they have with it.",Intel,2026-01-06 10:02:48,3
AMD,nylh0bq,"A quick Google says 9 TFLOPS or the equivalent to an RTX 1080, 2070, 3060, 4050 give or take.",Intel,2026-01-09 13:55:48,1
AMD,ny086nv,They did against the 285h and it's a similar margin. Lunar lake has a power burst max wattage below panther lakes max sustain power here so they can't compare 1:1 properly,Intel,2026-01-06 13:47:07,6
AMD,ny7zxvb,>77% faster while using 80% more power.  Are you following CES at all?  The top feature of that architecture so far has been power reduction,Intel,2026-01-07 16:16:03,2
AMD,ny0hpkm,82% faster than 890M with 30% more power draw with native resolutions,Intel,2026-01-06 14:38:55,4
AMD,ny0v0jb,rdna? dude their vega lasted too long they got very complacent in their igpu department,Intel,2026-01-06 15:44:10,4
AMD,ny2t2go,"At this point, we'll be lucky if they even care about consumer cards at all.  It's AI all the way these days.",Intel,2026-01-06 21:02:33,2
AMD,ny2qsag,"And then UDNA has been nowhere to be found, probably coming next year. AMD completely blew their lead in the APU space.",Intel,2026-01-06 20:52:10,1
AMD,nxyitdu,Panther Lake with an iGPU being able to play the newest games on medium/high settings in a thin notebook is pretty crazy,Intel,2026-01-06 05:27:44,14
AMD,ny6mhyg,Mfg on or off? The article wasnâ€™t clear on that.,Intel,2026-01-07 11:42:22,1
AMD,ny5jwr8,their Zen 5 desktop iGPU still use RDNA2; a 5 yr old architecture let that sink in...,Intel,2026-01-07 06:03:30,6
AMD,ny89039,"AMD didn't have money when they were using Vega iGPUs, and they were still the best iGPUs around",Intel,2026-01-07 16:56:45,2
AMD,ny2tvmr,"Unlike Nvidia they actually can make a lot of money relative to what they do right now if they get consumer marketshare. Iirc, Nvidias gaming revenue still beats AMDs enterprise earnings.",Intel,2026-01-06 21:06:18,2
AMD,ny2rjyw,"Eh, they will still have the best igpus on the market for a while. If they price the 388 well there is hope for them. But it's never going to sell the volumes Intel will.  UDNA is a major architecture overhaul on par with the the introduction of Ryzen and RDNA. A year is a long time but AMD only really needs a single gen to recover this gap if they so wish. But UDNA will need to be made with versatility and low power application in mind.",Intel,2026-01-06 20:55:40,0
AMD,ny55lgh,"Low to medium , not high",Intel,2026-01-07 04:23:33,4
AMD,ny8apbn,native rendering,Intel,2026-01-07 17:04:31,1
AMD,nz2dezh,Those weren't good though.  They didn't get close to the 1050ish equivalent that's a decent min spec card until the steam deck.,Intel,2026-01-11 23:40:06,1
AMD,nya3a7m,"High with XESS maybe at â€˜okayâ€™ frame rates. Still, crazy.",Intel,2026-01-07 21:45:51,1
AMD,nylfre3,"While I agree their naming scheme is a mess, yours is far worse.",Intel,2026-01-09 13:49:14,17
AMD,nykrwmo,"TBH, as long as the Ultra 5 338H is actually called an Ultra X5, it'd make the entire thing a lot more consistent  as in now X always means ""the one with the good GPU""",Intel,2026-01-09 11:10:20,19
AMD,nykqc9r,"You have no understanding of Intel's business and thus are not qualified to advise them what to do   Intel doesn't sell these CPUs to the end consumer, they sell them to their customers - the PC manufacturers. And that is the reason why there is so much choice, because the PC manufacturers want it.   Also, you clearly have never heard of vPro.",Intel,2026-01-09 10:57:07,37
AMD,nylcqp0,"The SKU count is roughly doubled because you have each step with/without vPro - these get a 100MHz max turbo frequency bump, but the main benefit is you can run the corporate firmware with vPro support, so you get additional security and manageability features. Exception is the Ultra 9 where they just do it with vPro support as standard.  These have a higher cost because you are getting more features.  You could make it so you just have one CPU and then the manufacturer pays a license for corporate firmware per device, but that's more work to then ensure manufacturers are licensing machines correctly, and more confusing for end-users where now if you're buying a corporate device with vPro support you know you are looking at Core Ultra 236V and 268V for vPro support whilst 226V and 258V don't have it.",Intel,2026-01-09 13:32:44,6
AMD,nykrcxa,"Because intelâ€™s customers work with thin margins and want the wide product stack with lots of performance and price steps. For them it matters if they get 4.4ghz for $300 or 4.6ghz for $350. You are not Intelâ€™s customer unless you ordered a pallet of 1000 CPUs, which I doubt.",Intel,2026-01-09 11:05:44,10
AMD,nyo7z62,Apple really isn't better. They leave out lots of the important performance information. They just don't tell you at all.,Intel,2026-01-09 21:31:03,3
AMD,nykykh6,I agree for those cpu that have no alphabet denomination at the back as that just looks like how desktop cpu is.  But for X7 and X9 is just even easier CPU differentiation,Intel,2026-01-09 12:02:34,2
AMD,nywwcvg,I only agree with the title. Your naming scheme is much worse lol,Intel,2026-01-11 03:59:55,2
AMD,nylenad,"At this point even S3, S3 Pro and S3 Pro Max would be a great improvement.",Intel,2026-01-09 13:43:16,1
AMD,nylwbke,"Totally donâ€™t get it, miss the 13900k 14600k type names.",Intel,2026-01-09 15:11:59,1
AMD,nyneqru,"Brah I don't care about the naming conventions, it is what it is.  It is petty to argue about all of this.  I need the B770 and C880 to be released.  I need more Intel Arc Pro cards to be released, there is no hope for humanity otherwise.  I need Battlemage and Battlematrix everywhere but TSMC is the bottleneck.  Hopefully there is more for 2026 where Intel IFS shines.  God help us all!",Intel,2026-01-09 19:16:03,1
AMD,nyo686v,Are you saying they are all locked???,Intel,2026-01-09 21:22:59,1
AMD,nyqfh80,I only care about the top level sku so the names don't matter.,Intel,2026-01-10 04:43:17,1
AMD,nykt7v1,Samaung Galaxy S3,Intel,2026-01-09 11:21:10,1
AMD,nyky6jo,Intel to $100 guaranteed,Intel,2026-01-09 11:59:46,1
AMD,nymfe1x,Just Josh presses them hard on this issue at about 7:30 in this videoÂ https://youtu.be/AzGFbkKZE7A?si=yq1pmpRv7exQ-7i5,Intel,2026-01-09 16:38:15,0
AMD,nyo3uhv,"Check the Just Josh interview! Dude criticizes exactly that to an intel executive... For me, they should drop the ultra naming scheme altogether... it hasn't stuck yet... they should go back to de i3/i5/i7/i9...use the X for the B390... and an S for the 16 core variants...",Intel,2026-01-09 21:11:54,0
AMD,nyps0n6,Apple always nails the small stuff,Intel,2026-01-10 02:21:54,0
AMD,nyn9p1s,"Just because it sounds similar to Apple does not make it ""bad"".  I chose ""S3"" because they literally market them as ""Series 3"" (sounds awfully similar to M-series ðŸ¤¨). Could fiddle with it but my idea stands:  Different core count = different name, Better gpu = add X",Intel,2026-01-09 18:53:22,-5
AMD,nyoegef,"Nope, its the Ultra 5 338H: [https://www.intel.com/content/www/us/en/products/sku/245531/intel-core-ultra-5-processor-338h-18m-cache-up-to-4-70-ghz/specifications.html](https://www.intel.com/content/www/us/en/products/sku/245531/intel-core-ultra-5-processor-338h-18m-cache-up-to-4-70-ghz/specifications.html)  The Ultra X7 and X9 are listed as such on Ark: [https://www.intel.com/content/www/us/en/ark/products/series/245528/intel-core-ultra-series-3-processors.html](https://www.intel.com/content/www/us/en/ark/products/series/245528/intel-core-ultra-series-3-processors.html)",Intel,2026-01-09 22:00:46,3
AMD,nz4b2zo,"the 338H doesn't have ""the"" good GPU, it has a B370, with 10 cores",Intel,2026-01-12 06:22:28,2
AMD,nylk5nn,"Btw, OEMs love the fact that the Ultra 5 336H and 338H are vastly different products with hugely different performance when it comes to graphics. Why? Cause they can market the 338H to you, and then sell you the 336H at a fraction of the cost, and if you are not very tech-savvy, well, that's too bad for you.",Intel,2026-01-09 14:12:07,16
AMD,nynb0an,"Yes because PC manufacturers want a ""choice"" to get a CPU with 100MHz higher clock speed as if that will make a difference in a mobile device at all.  If you think ""vPro"" is so important, processors with it should have entirely unique names. You and many others in the comments made an effort to point this out more than intel's own naming scheme does.",Intel,2026-01-09 18:59:08,-4
AMD,nyn93or,"I see, that does complicate things.",Intel,2026-01-09 18:50:47,-2
AMD,nz8mprz,"It still should have the X IMO. They're still bothering to call it a B3xx chip rather than just ""Intel Graphics"" like the <=4 Xe chips. The handheld chips running downclocked GPUs get the B360 and B380 names as well. Those should all be Core Ultra X.  As of right now only the ending 8 differentiates the 338H from the small-GPU SKUs, which IMO is not clear enough. Also, if the X became the standard for all big-GPU chips, that ending digit can be used for something else, such as noting the actual GPU performance within the stack. Perhaps just using the 6-9 from B360-390 or something like that.",Intel,2026-01-12 21:47:30,2
AMD,nynhyn2,"You are placing way more importance into the naming than any normal customer would.  The names that matter to normal customers are Core Ultra 5, Core Ultra 7, Core Ultra X7, which is what you will also find on the stickers that Intel has the PC manufacturers put on the device. The model numbers are just for the PC manufacturers and customers who want the exact SKU.  Seriously, your obsession with this is weird. Just accept that the naming is not meant for you and move on. Not everything has to be like Apple.  > If you think ""vPro"" is so important, processors with it should have entirely unique names. You and many others in the comments made an effort to point this out more than intel's own naming scheme does.  Yes, because vPro doesn't matter for normal consumers, but only for big enterprise customers.",Intel,2026-01-09 19:30:44,4
AMD,nymf8n3,Itâ€™s too hard to be a smart consumer and Google the names of the processor(s) and compare?,Intel,2026-01-09 16:37:35,4
AMD,nyo54xn,"""Obsession"" as if multiple YouTubers, some with millions of subscribers, haven't said the exact same thing I did",Intel,2026-01-09 21:17:55,-2
AMD,nynq0up,If you're a smart consumer you aren't buying windows laptops.,Intel,2026-01-09 20:07:44,-3
AMD,nyou0lx,"Right, those guys are surely the ultimate authority on anything and not just engagement driven outrage machines /s",Intel,2026-01-09 23:18:05,5
AMD,nyviuhi,That's some serious credentials you're bringing up,Intel,2026-01-10 23:32:07,5
AMD,nypm046,Ah I should instead buy Apple laptops and/or Arm laptops that don't work with my programs. Genius!,Intel,2026-01-10 01:49:37,6
AMD,nykpkf2,Probs the same reason tire companies donâ€™t make their own cars. Itâ€™s a lot of work. Plus thereâ€™s plenty of competition  What youâ€™re missing is companies like intel have initiatives such as the ultrabook initiative to help manufacturers make better laptops.    Besides itâ€™s way more profitable for them to just make the cpu and not deal directly with consumers as much as possible.,Intel,2026-01-09 10:50:30,40
AMD,nykpm6e,">Another common complaint I hear with non Apple laptops is battery life on suspend  This is still a problem, and it's because of windows. Microsoft claim to have fixed it ONLY for snapdragon laptops, but I have heard some still having the issue. Ever since apple made the M chips the blame has shifted to X86 processors being the problem when it was and still is windows the whole time. This whole thing makes x86 seem worse than it actually is.",Intel,2026-01-09 10:50:55,21
AMD,nykulsp,"They did. Intel produced NUC desktop and laptops for quite a while, then they sold the business to Asus. The desktops are absolutely excellent.",Intel,2026-01-09 11:32:25,23
AMD,nymeqg2,Because then theyre essentially *competing* against their own customers. In a market that would require a lot of effort for small margins.   Their effort is better spent horizontally expanding rather than vertically at this point.,Intel,2026-01-09 16:35:22,5
AMD,nym7ev6,"As someone who owns the Intel nuc 12 enthusiast, the engineering on them is phenomenal, Intel would do an extremely good job resulting in low margins and financially it doesnâ€™t make sense to make laptops stick and perfect the chips is probably the best financial decision seeing as neither Qualcomm, amd, nvidia make laptops themselves. Even reference designed are outsourced. I worked on intels Arc program those Intel branded cards are expensive AF as they donâ€™t have the bargaining power of getting low cost components like Asus or Lenovo would.",Intel,2026-01-09 16:02:39,3
AMD,nykpdew,"They used to, NUC was acquired by Asus.",Intel,2026-01-09 10:48:51,5
AMD,nykquc9,Making and selling CPUs is more profitable. Apple has a different business model.,Intel,2026-01-09 11:01:21,2
AMD,nymjurr,I really like Intel NUC. Too bad they sold it to Asus,Intel,2026-01-09 16:58:00,2
AMD,nykrk5a,"Why don't tire companies make their own cars? Why don't window companies build their own houses or office buildings?  Why don't the semiconductor tooling companies like ASML, LAM, Applied Materials just build their own fabs and make their own chips? Hell why can't they just build their own laptops too??   Do you really think that Apple manufactures their own M series chips? Or really any of the components in their products? Because the truth might shock you.   I recommend you look up supply chains and maybe learn a lil something :)",Intel,2026-01-09 11:07:26,4
AMD,nyq0x6r,"You are not Intelâ€™s or AMDâ€™s customer  Dell, Asus, Lenovo, MSI, etc are all customers of Intel and AMD in that they are the ones who actually buy chips  If Intel or AMD go into the laptop business then they are competing with their customers which tends to make their customers upset  The same problem exists with Microsoft and their Surface line which is one of the reasons the Surface line often seems to struggle - Microsoft has to be careful walking the line between demonstrating what they would like the hardware companies to do vs actually competing with them  As for differences in performance that can often come down to what price point the manufacturer is aiming for with the laptop. Higher price can often mean better quality components (which in a laptop can mean lower power consumption)",Intel,2026-01-10 03:12:36,1
AMD,nyszed9,A mid ground solution would be to make a reference laptop and mandate manufacturers to meet or exceed the criteria like battery life and thermals.  Intel did have an initiative called Ultrabook serving a similar purpose before.,Intel,2026-01-10 16:05:13,1
AMD,nyvag38,"They did, but got out of it  https://www.intel.com/content/www/us/en/ark/products/series/196845/intel-nuc-laptop-kits.html  They used to make mobos too, but got out of that",Intel,2026-01-10 22:47:52,1
AMD,nzmt7q4,"low margin, and competing with customers is no good",Intel,2026-01-14 23:25:58,1
AMD,nyljl73,"Please, don't.  Once, a long time ago, when dial-up was a thing and used by business, I've purchased an Intel modem. 9600 bps connection speed. It was *the* *worst modem I've ever witnessed* \[despite the fact that it was expensive\], because the firmware was expected to be run in sterile laboratory conditions. The real world with unperfect landlines drove this child into confusion and madness, it was repeatedly entering renegotiation on every disturbance, often ending in just dropped connection.  I assume that Intel entrusted the design to talented engineers who had no idea about real-world operating conditions and did not bother to study them. Even in the US at that time, analog telephone lines did not always have negligible levels of interference.  \* 9600bps was the bleeding edge at the time; the old modem at the company where I worked had a speed of 2400 bps.",Intel,2026-01-09 14:09:12,-3
AMD,nyl7kr6,"\> Itâ€™s a lot of work.  Smaller companies like Framework are able to come up with new designs from scratch, though. Slimbook too I think?  \> Besides itâ€™s way more profitable for them to just make the cpu  That pretty much sums it up & answers my question.",Intel,2026-01-09 13:02:37,-20
AMD,nyrb7mi,It's both.,Intel,2026-01-10 09:04:04,0
AMD,nzbd8ey,I have an Intel NUC 9 Extreme [LAPQC71A](https://www.intel.com/content/www/us/en/products/sku/196641/intel-nuc-9-extreme-laptop-kit-lapqc71a/specifications.html). Itâ€™s built like a tank (magnesium alloy chassis) and Iâ€™ve always been very happy with it. Itâ€™s still my main laptop.,Intel,2026-01-13 07:27:53,2
AMD,nys69j0,It's not cause SDXE also suffers from this issue Intel CPU Didn't have the issue with MacOS,Intel,2026-01-10 13:25:30,1
AMD,nyt7d08,"There are issues, but SDXE still did better on battery life than comparable Intel chips, and that's with a number of its own issues. Only with LNL/PTL has Intel meaningfully started to close that gap, the first such push since HSW-ULT.",Intel,2026-01-10 16:42:50,2
AMD,nyt7wqy,Some software on windows will break it you are one update away from Things breaking in windows be it WoA or X86_64,Intel,2026-01-10 16:45:26,3
AMD,nxtihn4,The biggest issue was that it was crippled by the ported meteor lake memory controller dies its that simple,Intel,2026-01-05 14:12:18,29
AMD,nxtjtfv,"Very good explanation, I own a 285k and I can say the stock experience is average, but the platform is great and coming from 14900k, the temps and power efficiency are impressive. Once fully tuned, 9000c38 A-die, 36 d2d and 34 ngu, gaming is on par with 14900k, but more efficient. I think nova lake will be amazing.",Intel,2026-01-05 14:19:43,30
AMD,nxvnqr5,">if you judge Arrow Lake solely by the frame rate counter in Cyberpunk 2077 at 1080p  Am I allowed to take into account that Intel went all the way from ""7"" to ""3"" lithography which is more than 2x improvement to achieve almost nothing?",Intel,2026-01-05 20:16:32,9
AMD,nxti8go,Itâ€™s not necessary for consumers to buy an inferior product from a multibillion dollar company now backed by the global superpowerâ€™s government.,Intel,2026-01-05 14:10:52,38
AMD,nxugtl3,">We need to stop looking at the Core Ultra 9 285K through the lens of a typical generational refresh  That's all what consumers care about. They don't care if ARL on paper or on theory is some great reset. Perf, power, and cost is what's important.   >he 285K is suffering from the acute growing pains of decoupling the compute complex from the uncore in a way that creates a distinct latency penalty that enthusiasts are mistaking for regression.  It's not being ""mistaken"" for a regression, it quite literally is one.   The problem is also that AMD also has disaggregated their compute from their IMC, and yet has *better* latency on *less advanced* packaging.   >The controversy here isn't that Intel failed to push frequency; it is that they deliberately chose to execute a hard pivot away from the monolithic brute force strategy of Raptor Lake to a disaggregated chiplet design that prioritizes area efficiency and performance-per-watt over raw, latency-sensitive throughput.  Nothing about ARL's current design prioritizes ppw or area efficiency over RPL's design from a chiplets vs monolithic perspective. ARL isn't enabling higher core counts from going chiplets, that seems to be left to NVL according to rumors. And chiplets carries an area penalty over monolithic designs anyway.   >The removal of Hyper-Threading from the Lion Cove P-cores is the most contentious yet logically sound decision engineers could have made given the thermal constraints of modern silicon.Â   This makes no sense   >By removing the simultaneous multithreading logic, specifically the duplication of architectural state and the complexity required in the reorder buffers and schedulers to handle two threads, Intel was able to physically widen the core and increase the L2 cache per core to 3MB without blowing up the die size  SMT costs Zen 5 less than 5% in area btw. Just throwing that out there.   >The result is a P-core with significantly higher IPC than Raptor Cove  It's not though. This has been a significantly worse ""tock"" in terms of IPC uplift compared to something like SNC or GLC.   >but this raw single-threaded throughput is being masked by the interconnect latency.  Maybe gaming or some benchmarks are, but for the large part, no.   You can see this two ways, LNC's structural gains (core width, ROB capacity, etc etc) have smaller gains, percentage wise, over their predecessor versus something you would see in GLC vs SNC, or SNC vs SKL.  And also LNL's uncore is dramatically improved over ARL, and yet you see the same unimpressive IPC gains over MTL/RWC (which is the basis for ARL's mid mem fabric).   >the architectural overhead of the Foveros packaging means that ring bus latency is higher.  No? The ring runs at a different frequency than the D2D?",Intel,2026-01-05 17:00:52,13
AMD,nxtflnr,"https://chipsandcheese.com/p/skymont-in-gaming-workloads  None of the youtubers mentioned about core-to-core latency, improvements on the schedulers and execution ports setup.  The e-cores are really great in a 4 group cluster.",Intel,2026-01-05 13:55:51,17
AMD,nxv9k2w,Designing a consumer product line around the niche of top of the line workstation is not a good decision for the average consumer.  I only once met a person with video production workstation that has more than an I7 or R7.  Previous gen I5 were amazing combo of multi core and single core . They rivaled amd r7 in preformance . Now a person wanting mid level cpu's would pay preformance tax due to it being made for the few people needing extreme amount of cores since those people would not buy dies that actually were designed for it like Xeon.,Intel,2026-01-05 19:10:59,2
AMD,nxtg5aw,"this is a lot of words, being honest this writing feels like ai (but in good sense, right to point without a bunch of bs)   i would agree this architecture is very much limited by d2d and without 200s or just pushing d2d can be kinda underwhelming in performance but for sure as first gen product is very solid and makes me personally excited for nova lake as it seems they plan to fix and improve on their current architecture  also i believe clockspeed difference was merely responsible to 13/14th gen failures which were caused by excessive idle voltage.  i would say adding DLVR was kinda smart as well as it reduces your power consumption significantly at idle especially with proper tuning.  Edit: fixed typos, autocorrect being silly with me",Intel,2026-01-05 13:58:56,10
AMD,nxtm1ov,as a person who made an upgrade from lga1700 13950hx ES laptop mutant to 265kf this cpu feels soooo smooth in win 11 despite on a huge 75 NS the ram latency and not ability to reduce latency this new E core with 800 score at 5 ghz in cpuz single core make using the pc so nice and ofc in single core game like cs2 I've loose a lot of performance with 13950hx at 5.6ghz I've had 980fps in dust 2 fps benchmark and on 265kf only 800... BUT in a multi core load game I got fps improvement but any way this upgrade is worth it for ppl who is not a pc enthusiast and don't want to tune the lga1700 CPUs,Intel,2026-01-05 14:32:00,3
AMD,ny3llfj,That's one big wall of excuses,Intel,2026-01-06 23:17:53,3
AMD,nxto7vg,"It smells like slop in here. Shit post rather than a shitpost, congratulations.  I like my 265K. It performs well for my purposes and has reduced my personal power consumption considerably vs AM4. It is behind AM5 in my testing for broad term ""gaming"" when specifically chasing framerates, but compared to a 9700X or 9800X3D it is absolutely stellar at doing stuff while doing other stuff.",Intel,2026-01-05 14:43:46,4
AMD,nxtkrwq,Yea with a z bord and the 200s boost and fast ram 15th gen is finally matching or supasing 14th gen,Intel,2026-01-05 14:25:03,5
AMD,nxwaygs,Wall of slop.  Make your points in a more concise manner.,Intel,2026-01-05 22:04:26,4
AMD,nxtzro3,"AMD had a very similar experience with their first generation of Ryzen CPUs. One of the differences here is Intel had a competitive product prior to their architectural shakeup. Had Intel totally croaked for years and not been competitive in the CPU space the narrative would be completely different and everyone would be singing their praises.  Aside from that I think the biggest problem with this new architecture is simply there's little reason to buy in. It was only recently (if memory serves, I could be wrong on this) announced that the next generation of CPUs will share this platform and later ones will be on a new socket. When AM4 was announced we knew that it would persist for multiple generations and now with AM5 - why would you buy a board that will be obsolete when its time to upgrade when you could buy into a platform that will support your next 1-2 CPUs? Especially with the old intel socket performing just as well on a more mature platform, for most end users this first core ultra series just isn't worth investing.",Intel,2026-01-05 15:41:28,2
AMD,nxua32l,"""they deliberately chose to execute a hard pivot away from the monolithic brute force strategy of Raptor Lake to a disaggregated chiplet design that prioritizes area efficiency and performance-per-watt over raw, latency-sensitive throughput.""  That good for them, but we don't want that. I would take better latency over improvements that pretty much only save money to the companies.",Intel,2026-01-05 16:29:42,4
AMD,nxu3n2t,Love my 285K rig.,Intel,2026-01-05 15:59:37,3
AMD,nxu27tg,"IMO corrupt tech sites and tech YouTubers are behind ARLâ€™s failure for two reasons.   The first: AMD MCM processors without 3D cache are bad for gaming, and itâ€™s hard to find this information in 90% of charts to warn intel about the consequences of going down this path, even though they should have done their own tests and experiments.   The second: in my own tests, the 265K was 15% faster than the 9700X on launch BIOS. Through BIOS updates, that gap extended to 20%, making it only 7%-10% slower than the 14700K. Yet on some very questionable charts, the 265K is shown as slower than the 9700X.   For experienced users, RPL CPUsâ€™ temperatures can be lowered by 20Â°C by turning Hyper-Threading off and undervolting. For inexperienced users, ARL is better, as it runs 20Â°C cooler out of the box.   I believe that in 2026 we deserve raw, unedited video benchmarks that start from the desktop, show full system specs, then enter game-by-game benchmarkingâ€”no more charts with zero evidence to back them up.   When I tried to confront HUB with my benchmarks, I got blocked to cover up their lies and corruption. If they truly cared about which CPU is faster, they would share their in-game benchmark scores and discuss them in a scientific way, but thatâ€™s not their goal. The numbers go up and down for the highest bidders.   Lastly, I believe Nova Lake, with its expected 144 MB cache, will be faster than Zen 6 X3D by 10â€“20% as 9700x in real world is 20% slower than 265k and if both got the same IPC uplift NL will end up on top.   My own testsÂ    14700k vs 9700x 30% faster https://youtu.be/1f6W6nkDS4o?si=chFUAeBWzybQopaL   265k vs 9700x 23% faster https://youtu.be/PuB0Dg-Jvyk?si=SmGJUFtYj-OjjpQh   Tech sites got same results that clearly show RPL and ARL CPUs are only slower than 9800x3d and faster than everything else from AMDÂ    https://www.pcgameshardware.de/Ryzen-9-9950X3D-CPU-281025/Tests/Benchmark-Release-Preis-vs-9800X3D-1467485/2/     https://www.purepc.pl/amd-ryzen-9-9950x3d-test-recenzja-opinia-cena-wydajnosc-gry-programy?page=0,55",Intel,2026-01-05 15:52:58,6
AMD,nxts3mv,"For me.   I will go with Intel because of reliability (I know about chip degradation) but for me I bought 13900K from first day I undervolted using offset and but Max turbo frequency to 5.5Ghz . so my chip never tried to boost 5.9Ghz with crazy voltage.  Next is the most important is Everything just works. The boot is faster. wakes from sleep. its a more mature. I have another amd pc with r5 2600, where I found some stability issue.  Another one that is important to me. is idle power consumption. My 13900K can idle at 6 watts. Imagine 24 cores idling at 6 watts. where as 6 core zen idle at 15-20watts.(and that is low side many user reported 25+watt). its all because of chiplet.  I didn't test the new Arrow Lake as this uses tile. so i cant comment on idle power draw. if anybody has test, let me know.",Intel,2026-01-05 15:03:58,4
AMD,nxwilo7,"Intel's latency problems have been around for a while now. Arrow Lake just threw gasoline on a fire that was already burning. [The 14900K had a latency of about 90ns for memory access, which is awful compared to the 10900K's 66ns and the 3950X's 73ns latency.](https://chipsandcheese.com/p/examining-intels-arrow-lake-at-the). The 285K sits at 106ns.  The 9900X sits at 82.43ns, so it seems like latency is going up across the board in general.",Intel,2026-01-05 22:41:53,2
AMD,nxmkdsp,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2026-01-04 14:07:07,1
AMD,nxxn31i,I compare Arrow Lake to Zen 1 or the first iterations of Ryzen. It is quite the techinal hurdle but it allows for future generations to bake very well. Intel was just playing catchup from having 14NM^6 nodes lmao.,Intel,2026-01-06 02:15:07,1
AMD,nxyburv,the problem for me the platform cost value over lga1700 isn't where it needs to be. namely I'm not interested in trading 32GB of DDR4 for just 8GB of DDR5.,Intel,2026-01-06 04:39:27,1
AMD,ny7t3re,"Since you are talking about latencies, best intel gaming cpu, so far, is 14700k. Excellent gaming performance and also great cpu for productivity.  Second best, is 14900k, same as 14700k, but cause of price it gets to second place.  Third place, you name it.  Period.  PS: I'm with intel since ..... Pentium 3 at 800MHz. Also have AMD cpu.",Intel,2026-01-07 15:44:56,1
AMD,nyabjt5,"End-user workloads at midsized businesses are almost entirely single-threaded. Even when the applications are multi-threaded, they often become single-threaded when they get shuffled through corporate antimalware. While AMD chases gamers and Apple chases artists, Intel is hitting a sweet spot here not perfectly served by any other single-user processor.",Intel,2026-01-07 22:22:37,1
AMD,nyduqy4,"I've been saying similar in a couple of PCMR threads.  Arrow lakes problem is chiplet to chiplet latency. Which is understandable as this was a major departure from monolithic dies this generation. This explains both why games particularly suffer, and why very high speed memory can mitigate the issue somewhat.  Does this mean they aren't bad chips? No, they are (or at least for gaming performance they are). But it's quite exciting in the sense that it's a specific issue holding them back that can be solved. The rest of the architecture has a lot of potential.  I think of these chips as in a similar place to AMD Zen 1 technology wise. A huge shift, not actually delivering much performance boost yet, but with a whole lot of potential for updates to make big improvements.",Intel,2026-01-08 12:08:37,1
AMD,nyf32oe,"Yes yes, I remember saying similar things about my FX-8350.",Intel,2026-01-08 16:03:36,1
AMD,nysqs8m,"285K seems to have fantastic workstation performance, I didn't think much before selecting Intel over AMD parts for work to be honest, considering the efficiency gains. We landed on 265K, it benches very favorably compared to AM5 parts in nearly every workload except for gaming, and even then, that's mostly the X3D parts, and even then, that's when there is virtually no GPU bottleneck.   Most initial ""gaming"" reviews were done exclusively with 5090s, which is extremely unrealistic for most gamers and gave an extremely false impression that you would see a massive uplift by buying this part compared with the alternative CPU parts in gaming.   A lot of reviewers such as HUB have done a lot of work to correct this impression with a variety of testing scenarios and explaining their reasoning behind removing the GPU limiting factor as much as possible in benchmarks. Incidentally, these reviews typically showed that anything less than a 5080 sees virtually no benefit with a 9800X3D versus, say, a 9700X, and incidentally the two parts that saw no discernible benefits were the Radeon 9070XT and the 9070.  Obviously, if you have a 5090 and an unlimited budget with price being no factor, your best bet for top tier performance in all categories is a 9950X3D, but that just isn't what most people are looking at.  I myself bought into the future proofing mentality with my CPU and bought a 9800X3D, and ironically bought another AMD Radeon 9070XT to pair it with, a GPU that seems literally no performance gain compared to a 9700X, let alone a 285K. The same 285K that takes a dump all over the 9800X3D in nearly every other productivity benchmark.  I guess the good news is that when I upgrade my GPU in 2-4 years, at least I'll take advantage of the 9800X3D in games, lol. But considering it cost me over $900 for CPU, RAM, and motherboard, that's cold comfort. Ironically I probably would have made out a lot better buying a 5700X3D or a 5800X3D and the 9070XT and keeping my AM4 board and DDR4 RAM and saving $600.",Intel,2026-01-10 15:22:09,1
AMD,nxtna7j,"You are correct that they needed to make a big architectural change as the 14th gen was clearly having issues and was being pushed too hard to make up for them, and ARL is the first step to that reset.  From their engineering perspective, it makes sense.  I wouldn't buy it though.  NVL... different story.",Intel,2026-01-05 14:38:45,1
AMD,nxvbwve,"No 285k is not shaving off 80-100W from 14900k, Set 100W PL1=PL2 on both and then compare. 285k is not even that much better than 9950x in MT. No amount of words can explain ARL flop, it took intel 3 gens after rocket lake fiasco to catch up, just to waste all that effort on ARL. Show me any other silicon design with advantage of 2 nodes and a new architecture just to be slower than the predcessor.",Intel,2026-01-05 19:21:42,1
AMD,nxwb6x7,"""If you are buying a 285K solely for (1080P) gaming, you are buying the wrong product for the wrong reason.Â ""  Fixed it for you. I game in 4K and I didn't buy a Ultra 9 285K to play games in 1080P to get high(er) framerate at the expense of stuttering. The ultras are outstanding CPUs.",Intel,2026-01-05 22:05:33,1
AMD,nxtfm4y,I guess.,Intel,2026-01-05 13:55:55,-1
AMD,nxte2yd,No one has time to read all that,Intel,2026-01-05 13:47:04,-10
AMD,nxvicu7,This is a very long way of saying Intel has chose to prioritize competing with Appleâ€™s SoC and ignoring gamerâ€™s and PC enthusiastâ€™s wants/needs.,Intel,2026-01-05 19:51:27,0
AMD,nxuwzmt,"Iâ€™m just waiting for Nova Lake and if it gets delayed into 2027 and if TSMC does the packaging for some chips there may be issues  with supply. TSMC does not like that Intel has IFS and they play dirty, real dirty.  Nova Lake flagship could be at or above $1000, I may go with Ultra 9 285K with the fastest pcie5 nvme and ram, by then hopefully ddr5 is accessible.",Intel,2026-01-05 18:14:53,0
AMD,nxtrgru,My same experience with both of my 265k systems. They have been extremely stable for me and very efficient. Never have to worry about temps and they perform well with a 5080 and 9070XT. Contrary to all the media rhetoric I enjoy gaming with them.,Intel,2026-01-05 15:00:40,15
AMD,nxu6pg9,Basically the same or even less FPS than a 9800X3D consuming 80 watts,Intel,2026-01-05 16:13:55,3
AMD,nxvj13w,"Hi I'm sorry to ask but what does this mean?     9000c38 A-die, 36 d2d and 34 ngu     is that an app or something?",Intel,2026-01-05 19:54:33,2
AMD,nxuzcbk,"Exactly, tuned 285k is just 2-3% away from max tuned 14900KS + DDR4 at 4300MHz CL16.  All Z890 boards are so cheap so I grabbed an APEX with a 265K. Only costed me $600",Intel,2026-01-05 18:25:21,2
AMD,nxvtdhm,"that is exactly the point, if it was just architecture we could live with it and consider it an scurve of innovation, but the process proves that this design is going nowhere",Intel,2026-01-05 20:42:56,7
AMD,nxwwv5w,"New process nodes donâ€™t improve  performance on desktop processors when clock speeds and core counts stay the same. A 10nm monolithic ARL could have performed better and cost them less, although the newer process does improve power consumption which is essential for laptops.",Intel,2026-01-05 23:55:52,1
AMD,nxum4ri,"And nobody was saying you're required to. The entire point of the post was to say regardless of your thoughts and purchasing habits, ARL was deliberate, and even smart. Those who look for single metrics by which to judge ARL are missing the point.   Yes, if that single metric is all you care about, by all means go spend your money elsewhere, but ARL is a step sideways so future generations can take leaps forward.",Intel,2026-01-05 17:25:38,6
AMD,nxtkrhd,"This is way too true. I want a healthy Intel and AMD, but I'm also not going to act like Intel being now backed by the US Government and MAGA, as well as securing a well funded partnership with nVidia, doesn't make me feel a lot less like they need consumers to pity buy things from them to encourage innovation.",Intel,2026-01-05 14:24:59,8
AMD,nxuiree,">In highly parallelized rendering workloads like Blender or Cinebench, the 24-thread Arrow Lake design is often matching or beating the 32-thread Raptor Lake parts, which proves that the removal of Hyper-Threading was not a net loss for total throughput  So matching perf with a last gen part, after you hit a double node shrink **and** a massive E-core IPC gain and a P-core tock too is fine?   >The ""rent"" paid in silicon area for HT was no longer worth the ""yield"" in multithreaded performance,  This was a mistake according to LBT himself.   >This implies that Intelâ€™s next step must be an aggressive overhaul of the interconnect topology, perhaps moving towards a mesh or a more direct active interposer solution for desktop parts if they want to reclaim the gaming crown from AMDâ€™s X3D parts  Moving to a mesh wouldn't help much, and Intel's mesh's have a reputation for being insanely slow on their Xeon parts.   How much more advanced packaging does Intel have to use to match the latency of AMD using iFOP?   >Â But if you analyze the architecture, the Lion Cove P-core is a marvel of width and prediction capability that is simply being strangled by the packaging logistics  It's not. LNC is both not that all that wide, all the ARM cores beat it in that metric, and the prediction capabilities of LNC is bad- it's a literal regression vs RWC (last gen) in accuracy. It's worse than the E-cores branch prediction accuracy. And it's much worse than Zen 5's as well.   >and the floating-point performance is stellar.  This specifically is not the case. While in previous generations Intel was very competitive with AMD in spec 2017 FP, with ARL vs Zen 5 we see an almost 15% gap.   >The 285K is the cooler, more efficient, strictly professional grown-up in the room that unfortunately forgot how to play games because itâ€™s too busy trying to figure out how to talk to its own memory controller across a microscopic bridge.  Idk why you are trying to trivialize gaming when it pushes a huge percentage of the market, and is why Intel has been repeatedly telling investors they have lost the high end DT market.",Intel,2026-01-05 17:09:57,9
AMD,nxtic59,Yes because they do core to cores transfers via their shared l2 rather than the l3/ring. It's sick. Skymont in general is so underrated for how enormous of a performance gain it was. They literally fixed all the ecore issues it's the pcores that flopped,Intel,2026-01-05 14:11:26,18
AMD,nxu08ua,"As someone who â€œwrites like AIâ€ in part because of a learning disability that made it hard for me to write, I tend to organize my thoughts very deliberately. Using lots of punctuation, dashes, etc is now often interpreted as having used AI, although I have no idea whether OP did or didnâ€™t.",Intel,2026-01-05 15:43:44,8
AMD,nxviton,">this is a lot of words, being honest this writing feels like ai (but in good sense, right to point without a bunch of bs)  It doesn't point out a bunch of BS, it introduces a bunch of new BS that is just straight up, factually wrong.   >i would agree this architecture is very much limited by d2d  Not single core performance like he is implying it is.   Just look at ARL-H vs MTL-H for example.",Intel,2026-01-05 19:53:36,3
AMD,nxv2i2x,"Soon you will not be able to tell what is AI and what was before, where you were living in the Stone Age. Fooz better buckle their seat belts and get rekt, itâ€™s about to get a Bit-Funky.",Intel,2026-01-05 18:39:29,2
AMD,nyi3r20,"\> For experienced users, RPL CPUsâ€™ temperatures can be lowered by 20Â°C by turning Hyper-Threading off     \> 265k vs 9700x 23% faster   \> 5 games    You are cute.",Intel,2026-01-09 00:12:42,1
AMD,o0ie5bv,at least you have proved that i did make the right choice on my 265k,Intel,2026-01-19 17:29:31,1
AMD,nxzan7c,AMD Unboxed are not serious and should be ignored at every opportunity. They have a long history of doing what you've said they've done to you. They get in childish arguments on twitter when they aren't blocking people who have data that disagrees with their clear bias.,Intel,2026-01-06 09:33:39,1
AMD,nxu6lhk,I have one 13900k since day one... But like all enthusiastic guys I did some benchmarks and the voltages were a concern. So I tuned the bios after just a few hours of working.  3 years have passed and I have 0 problems. It's like a rocket ðŸš€ very fast and reliable,Intel,2026-01-05 16:13:24,7
AMD,nxzllb5,"Very nice, do you also use a contact frame?",Intel,2026-01-06 11:12:32,1
AMD,nyi50z0,> so my chip never tried to boost 5.9Ghz with crazy voltage.  You have no idea what cause degradation and undervolting does not save CPU.,Intel,2026-01-09 00:19:10,-1
AMD,nxwmvqc,For sure.  I still use 10900k which benefits massively from memory frequency. When running 4500 cl16 it sits in 33-36ns territory. Lowest latency cpu around was 2020.  Doesnt hold up vs today's cpus but the user experience is great.,Intel,2026-01-05 23:03:28,1
AMD,nxxq0wj,"Really, 90ns on DDR5-3600, what else can go wrong here...",Intel,2026-01-06 02:31:08,1
AMD,nxx7vlh,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:53:14,1
AMD,nyf7a0y,"""Wait till games leverage multicore"".",Intel,2026-01-08 16:22:24,1
AMD,nxz46az,"Could also add 1440p into the mix, at max settings with ray tracing, the CPU is going to matter less and less.",Intel,2026-01-06 08:31:05,1
AMD,nydtdxm,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-08 11:58:45,1
AMD,nxuz1x1,"Are you using a B580, Iâ€™m wondering how it performs.  I have a 5070 OC and an army of Alchemist and Battlemage cards.  I may try to get a 9070 this year and OC / solder it to XT performance. There is a great deal of fake hype around the 9070 XT, Lisa and her army of gawkers really pulled out the hype train for the 9070 XT.  Many influencers (somebody who can post a video to snoozetube) received 9070 XTs for free so they could gimp primp them out to the masses. It has left a stale, rotting smell in my loins.",Intel,2026-01-05 18:24:04,2
AMD,nxwegrj,"Did you used just ""stock"" profile ie NGU and D2D at auto and Intels default profile? Or Intel 200s boost?",Intel,2026-01-05 22:21:22,1
AMD,nxuzm9f,"Both 14900K and 285K won't get frametime spike when the L3 Cache is maxed and no random stutter/issue.  If I want a good gaming experience, I would go for the 14600k instead of the 9800X3D. Way cheaper and works way better.",Intel,2026-01-05 18:26:34,1
AMD,nxvwkhz,"These are the bios overclocking settings, ram speed is 9000mhz, d2d die2die is 3600mhz, ngu chip fabric 3400mhz fully manually tuned",Intel,2026-01-05 20:57:48,6
AMD,ny1yrwf,"> New process nodes donâ€™t improve desktop performance when clock speeds and core counts stay the same.  This is correct. The node shrink itself does not give IPC gains, what it gives is energy efficiency and area. The reason why people associate node shrinks with better performance is because when they switch a processor to a newer node they typically increase cache sizes and/or improve base/turbo clocks.",Intel,2026-01-06 18:43:59,2
AMD,ny0mjxi,> New process nodes donâ€™t improve desktop performance when clock speeds and core counts stay the same.  Netburst was awesome!,Intel,2026-01-06 15:03:34,1
AMD,nxvi8ry,The problem is that MTL for all means and purposes should have been that test bed product. Or even lakefield tbh.,Intel,2026-01-05 19:50:56,5
AMD,nxxqpo4,"Nothing smart about it.. they just couldn't do any better than release a half unfinished product because the company is corrupt as hell, fully relying on US gvt injecting  tons of money that ends up in a few top manager pockets instead of being used for restarting the core architecture from scratch.",Intel,2026-01-06 02:34:57,1
AMD,nxubozd,"We were backed strongly by Biden admin too, the CHIPS act money was what Trump gave us, but demanded the stock in return instead.   Which I think is ultimately good for us American citizens. Too many times companies just got hand outs, even Bernie Sanders approved the Intel stock deal. https://www.reuters.com/world/us/us-senator-sanders-favors-trump-plan-take-stake-intel-other-chipmakers-2025-08-20/",Intel,2026-01-05 16:37:08,15
AMD,nxu3xx9,Brotha everyone is tryina get a piece of that maga pie. Or vice versa. It would probably be unlawful to go against maga on fiduciary responsibility alone,Intel,2026-01-05 16:01:00,2
AMD,nxtmb1f,This!,Intel,2026-01-05 14:33:25,5
AMD,nxv74ph,"On my 13600k I also see a shared L2 for each 4-core E-core cluster. Was there a regression between then and now that they've resolved? Or is there some hidden behavior where this shared L2 couldn't actually be used to core-to-core communication without going through the ring?  If you have a source with more info, I'd greatly appreciate it.",Intel,2026-01-05 19:00:02,2
AMD,nxvzfjz,"AI is the aggregation of all the rules and examples fed to it.  You write according to proper ""rules"" or clear organization (which is very much not ""vernacular"" level writing), then boom, you and AI aren't all that different.  It's bloody annoying to try organizing thoughts or points to be easily digested instead of a wall-of-text like you've been doing since Mavis Beacon taught you typing only to have people bitch about the style and ignore the content.",Intel,2026-01-05 21:11:12,3
AMD,nxv1yn0,There are so many of us lol,Intel,2026-01-05 18:37:05,0
AMD,nylpv7p,5 CPU bound games enough.,Intel,2026-01-09 14:41:06,1
AMD,nxzllwo,Do you use contact frame for the cpu?,Intel,2026-01-06 11:12:41,1
AMD,nxzluhl,"yes, i do",Intel,2026-01-06 11:14:44,1
AMD,nyi5ufb,"Please, High life form, what causes degradation? enlighten us, mere mortal",Intel,2026-01-09 00:23:20,2
AMD,nxv14y0,I do have a B580 but haven't paired it with the 265k cpus yet. The B580 is currently in a i5-13600 system.,Intel,2026-01-05 18:33:23,2
AMD,nya2yya,I have a 265k and b580. Had it about 6 months. First desktop I've had in about 20 years. Works flawless for everything I need. Very stable. Very fast for my needs.. I pretty much only play warcraft though.,Intel,2026-01-07 21:44:30,1
AMD,nxwoxfl,"Stock. One system uses a Z890 mb with 8200mhz cudimm and the other uses a B860 MB with 8000mhz cudimm. I tried 200S boost on the Z890 and it didn't benchmark much faster at all for the games I play, plus I had occasional lockups. It wasn't worth leaving it on for me so everything is at the Intel default profile.",Intel,2026-01-05 23:13:58,1
AMD,nxzc32p,Lmao,Intel,2026-01-06 09:47:24,6
AMD,nxv0qyc,"Lol, Userbenchmark guy making things up.",Intel,2026-01-05 18:31:40,13
AMD,nxw9ntc,thank you,Intel,2026-01-05 21:58:18,2
AMD,ny41irt,"Clock speeds havenâ€™t meaningfully improved since 32 nm Sandy Bridge. The 2500K and 2600K could overclock to around 5.0 GHz, and modern CPUs still run at roughly the same frequencies. Core counts are also similarâ€”Intelâ€™s 14 nm 7980XE had 18 Skylake cores. Cache increases are possible on older nodes as well, so newer process nodes mainly improve efficiency these days, which is contrary to what most expect of them.  A 10nm monolithic ARL could have performed better at least in gaming on desktop.",Intel,2026-01-07 00:40:20,1
AMD,nydvpul,Apart from straight up increasing frequency improved nodes also make improvements to CPU possible even at the same clock.,Intel,2026-01-08 12:15:24,1
AMD,nxvyiba,"I suspect they just didn't have enough time to change anything. MTL releasing in Dec 2023, internal testing I'm sure yielding some set of data, and external reviews giving other feedback, even by the release of MTL, ARL has probably been in the pipeline for years and probably locked into certain design choices regardless of the feedback and testing.  Additionally I suspect that on mobile chips/laptop gaming rigs there's less focus on each individual part because  a) few people hardcore game on laptops b) the latency can be blamed on other things since laptops are a prepackaged consortium of parts and it's harder to isolate, and  c) therefore laptops tend to be evaluated as a whole rather than the individual pieces that comprise them.   Therefore the ""latency issues"" only became a massive kerfuffle when the offending cpu could be isolated and tested alone, and reviewers needed something to complain about.   That's even if Intel was considering latency as the issue everyone made it out to be. Intel could have looked at it and considered the latency worth the cost to improve in other areas and serve as intels seminal desktop entry utilizing disaggregated silicon. Then gamers came and lost their shit that the newest Intel chip deprioritized something that impacted their precious fps- even though the impact was ""the new one is approximately the same to maybe a little worse as last gen in most games"".   Notwithstanding the fact that the 200 series chips retain healthy gen-to-gen perf uplift and a _massive efficiency improvement_ in productivity and general computing, and boosting the NGU and D2D clocks (which are _very_ low from the factory, and can be done with the app that Intel _has specifically designed for tuning their chips_ and is freely available (XTU)) brings the gaming performance to ""approximately the same to a little better in most games"". Contrary to what some people may think these chips are not solely or even majority used for gaming and there are other use cases Intel can to think about/chose to prioritize.   To be fair, gamers and tech enthusiasts are the ones who will care the most and therefore have a disproportionately large impact on the kind of publicity the chip will get. So, was this intels smartest PR decision? Maybe not. But I think it was still a sound engineering decision, regardless of whether or not they had feedback or data on the issue, even if it didn't go over very well with their loudest customer base.  And this is all overshadowed by the fact that if you stuck even the most hardcore of gamers on a blind trial and told them to identify what kind of chip they were gaming on, I have a hard time believing any of them would be able to tell with any certainty which is which.   Once you get to 60 fps, the vast majority of people stop caring. Whats five fps going to do to radically change your gaming experience so much that it's unplayable? Which brings me back to one of my original points: If you care _that much_, you can go spend your money _elsewhere_.",Intel,2026-01-05 21:06:54,3
AMD,nxun1ug,"I'm aware of CHIPS.  Like I said, I want Intel to be competitive. I hope if their new layouts mature into something that can retake marketshare that the same care will be taken to preserve AMD. AFAIK, no one came to lend them money or sign large multifaceted partnerships at that time. This ""must save Intel,"" movement is something that is borne out of how big Intel is. They're too big to fail, it would have too many implications for the economy. It's not tit for tat, but as they are both American companies and AMD has significantly more invested in my country, I don't feel particularly motivated to help bail out Intel when they have the equivalent of the iron rice bowl rolled out for them at the moment. I'll buy what works best for my needs at the time I'm buying, as always, and if Intel can make a better gaming focused, with some imagery stitching on the side, processor than AMD can at the time I'm buying, that's the direction I'll go.  Ultimately the 200 series is what the Ryzen 1000 series was, but more stable as Intel has always had better support both internally and from vendors. I have no doubt Intel may catch AMD. My worry will be, what happens when they put their boots on AMDs throat, especially now that nVidia has more control over the situation than ever before?",Intel,2026-01-05 17:29:53,1
AMD,nxxrp83,Pouring free gvt money into Intel is only making the company high managers less interested in restructuring and improving the company when instead they can just keep going at snails pace while cashing in. Gvt money is only supporting growth of corruption inside the company and stalling progress.,Intel,2026-01-06 02:40:23,1
AMD,nxv6i3j,"It's so strange to me that we've hit a spot in consumer and enterprise computing where politics is now a factor. It's a different game when companies have to worry about that side, far too many consumers make decisions based on their politics when all that does is cause other issues. Obviously I won't go farther than that in a tech focused discussion lol, but I will say again that it's a different game and I don't think anyone wins if it becomes the norm.",Intel,2026-01-05 18:57:13,1
AMD,nxty6pw,I upped the tREFI of my memory in my MSI Vector with Intel Ultra 9 275HX and got some fps gains in Fortnite/Valorant/Hogwarts Legacy at 1200p,Intel,2026-01-05 15:33:56,2
AMD,nxv7w1f,Yeah pre arrow lake it was particularly shit as it would instead go through BOTH l2 and then to l3 to communicate between cores like a shitty ISP route to a game server   That is obviously worse than normal but core to core communication being done through even shared l2 is very rare so even without that quirk it's not expected  Go to chips and cheese.com they have a ton of information about this stuff. Like their skymont article in this case it details all the huge upgrades,Intel,2026-01-05 19:03:27,3
AMD,nxx7gwg,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:51:03,1
AMD,nylunna,Every respectable benchmark is CPU-bound (including HUB whom you try to diss) because every sane reviewer uses a fat 5090 and 1080p to show CPU differences.  Even if you did not cherry-pick the games the review with fast GPU and more games is much more indicative of CPU performance than your benchmarks.,Intel,2026-01-09 15:04:11,1
AMD,ny47bmi,No I don't need it... You need a 360mm AIO that's all... After that it's all about bios settings.,Intel,2026-01-07 01:11:04,1
AMD,nxzm8r1,which brand do you use?,Intel,2026-01-06 11:18:02,1
AMD,nyi87lr,"1. tvb working incorrectly 2. unlimited current set by motherboards which gets only higher when undervolted because same power limit at lower voltage necessarily means more current   Motherboards were undervolting CPUs with wrong LLC calibration intentionally   4. ~~motherboards having wrong LLC calibration~~ EDIT: Intel CPUs themselves requesting abnormal voltage in anticipation of frequency boost resulting in abnormal idle voltage (just remembered it correctly) 5. motherboards having wrong LLC calibration resulting in abnormal load voltage  Not a single smart ass on the planet could have predicted that all of the above can happen with ""stock"" settings and foresee all of this.  No frequency limit will save CPU when it dies in idle state.",Intel,2026-01-09 00:35:27,-1
AMD,nxyej0v,"Thanks so in short:  You use only ""intel default profile"" and enabled XMP on your CUDIMM's right?   No further manual tweaks under NGU or D2D and RING values or any other critical tweaks pertaining to voltages no?",Intel,2026-01-06 04:57:20,2
AMD,ny5j3g0,"Bruh my 2600k couldn't hit 5ghz. I struggled to get to 4.4. same with my 5820k, which would do 4.2, both with voltage bumps and good cooling.   Modern processors definitely run faster. The 12900k in my media pc will happily sit at 5.2 on lightly threaded loads   Tho you are right about the core counts. High core numbers have been around for eons, they were just far too expensive for mainstream use",Intel,2026-01-07 05:57:02,1
AMD,nxuzom9,"It's because AMD doesn't manufacture chips, we do. That's what the funding was for, to revive manufacturing leadership in the US not to save failing architectures.  And yes as a consumer buy what works best for your needs and budget. That's the best thing about Ryzen and AMD's resurgence.  I don't get your ""boots on throat"" comment, but to think AMD hasn't done anything mischievous in the past is, well a lot has happened between the two companies in 40 years.",Intel,2026-01-05 18:26:52,4
AMD,nxv19d7,"A strong Intel IFS is a strong US. Many people get disillusioned and deceived through all the cognitive dissonance on social media, especially gamers. Unfortunately they are easy to manipulate.  Many people are buying INTC in the US to retire on, we will see this more and more as we approach 2030. INTEL IFS has to succeed otherwise the US will be doomed in this century. Even India is getting into the semiconductor industry now and Intel is working with them. We need IFS to be on top, cream of the crop, I need a taste.  You are defeating yourself by getting wrapped up with all the geopolitical propaganda, go take a walk in the woods and get away from it all.  Checks are in the mail.",Intel,2026-01-05 18:33:57,-1
AMD,nxypf2c,"What? The deal under Trump was for 10% of the stake in shares. The government can sell the shares at any time to get a return. Since the stock has doubled since, it looks like it was a great deal for tax payers.",Intel,2026-01-06 06:19:05,2
AMD,nxv1ubv,Which way is up...?,Intel,2026-01-05 18:36:33,1
AMD,nxw36zg,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.  Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2026-01-05 21:28:35,1
AMD,nymqb9s,"Here are few more games  [https://youtu.be/XZ6JJNdMW4g?si=mkuKutnT1tc7k9U\_](https://youtu.be/XZ6JJNdMW4g?si=mkuKutnT1tc7k9U_)  [https://youtu.be/Ah6izQnylsM?si=s2iqbkLFHc83jHgI](https://youtu.be/Ah6izQnylsM?si=s2iqbkLFHc83jHgI)  [https://youtu.be/mQ80rNg0k3c?si=sgXQJ\_kh0Nb22BIM](https://youtu.be/mQ80rNg0k3c?si=sgXQJ_kh0Nb22BIM)  [https://youtu.be/fDdwwx4vYrs?si=QzJ7jz5H6oFWHwuU](https://youtu.be/fDdwwx4vYrs?si=QzJ7jz5H6oFWHwuU)  Non 3ds are bad for gaming, thats a fact.  Here is my latest test for 7800x3d vs 14700k  [https://youtu.be/ZTNE0EWtA1Y?si=zoujDFCzvCSj-UE2](https://youtu.be/ZTNE0EWtA1Y?si=zoujDFCzvCSj-UE2)",Intel,2026-01-09 17:27:05,1
AMD,ny1tgj8,thermalright.   Any frame will do. get whatever is cheap,Intel,2026-01-06 18:20:04,1
AMD,nyimhf2,"Let me explain why it will not degrade my CPU.  1. **TVB (Thermal Velocity Boost) is disabled when I set the turbo limit.** 2. **Current is not the issue.**Â First, even Intel chips were failing at idle. Second, you stated that ""the same power limit at lower voltage necessarily means more current."" That is incorrect, if it drew more current, then why does lowering voltage reduce power consumption? Have you missed basic physics? 3. **CPUs follow a voltage-frequency (V-F) curve.**Â The main issue was that either the CPU or the motherboard was supplying excessive voltage, or the CPU was requesting too much. Higher voltage is required for extreme single-core boosts, such as 5.9 GHz. This is why i3 and i5 CPUs were not affected. When I limit my boost and apply an undervolt, the CPU no longer requests high voltage. It might request slightly higher voltage for 5.5 GHz, but that is nowhere near the voltage required for a 5.9 GHz boost. 4. **The same principle applies.**  Its seems you are a failure of high life form.   Maybe read here more. it was already to high voltage   [https://community.intel.com/t5/Blogs/Tech-Innovation/Client/Intel-Core-13th-and-14th-Gen-Desktop-Instability-Root-Cause/post/1633239](https://community.intel.com/t5/Blogs/Tech-Innovation/Client/Intel-Core-13th-and-14th-Gen-Desktop-Instability-Root-Cause/post/1633239)  Maybe you havent read much. if you undervolted only using AC/DC loadline and didnt limit max frequency. Then your CPU could have degraded.   While I used offset and set the max turbo limit.  I know how silicon works. I tune every CPU and GPU I buy.",Intel,2026-01-09 01:50:53,2
AMD,nyx4wv5,"Yeah, I do use XMP to get 8000/8200hz but no tweaks and everything is Intel default.",Intel,2026-01-11 04:51:00,1
AMD,nxx7x5q,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Intel,2026-01-06 00:53:28,1
AMD,ny6b0of,"Probably just bad chips. In almost every review, 4.7â€“4.8 GHz was achievable on the 2600K, and 4.6â€“5.0 GHz on Haswell-E. 5.0ghz wasn't hard to do on 8700k-9900k-10900k. Also, ARL runs at lower clock speeds than RPL, so in some cases newer process nodes actually clock lower.   this happened many times before like 32nm SNB OC better than 22nm IVB and haswell or even 14nm 6000 series skylake, so no in most cases new process nodes don't improve clock speeds or at least not on desktop.",Intel,2026-01-07 10:04:24,0
AMD,nxv4op7,"Okay, so we've gone right off the rails of a logical discussion. I fail to see how we went from talking about a failing architecture, which I don't think of Intel's product line as, to the whole bailout discussion. Maybe because I said AMD didn't get bailed out? I think maybe you forget that AMD had to offload global foundries, I don't think anyone even blinked when that happened and it's likely because that was a different era.   My post had nothing to do with the bailout versus architecture (or saving it) , apart from mentioning that I don't believe as a consumer I should feel motivated to buy Intel over AMD, or any other American firm, at the moment.   To be very clear, as this is personal for you, I do own Intel equipment including an Arc graphics setup for one of my children. I'm not anti-Intel or anti-American at all.   I know the history between AMD and Intel fairly well. Oversimplifying, AMD as it is doesn't exist without Intel. AMD only grew because they were the most successful second source producer for Intel, and the most successful at riding the thin grey line between patient infringement and unique implementation of similar IP that kept them alive while everyone else in the X86 space either died or became irrelevant to the consumer or enterprise space. After Itanium, the story levels out with both companies becoming effectively unwilling AMD64 codependents, and I'm saying that humorously.   Intel would gladly own the X86 market outright. So would AMD. At the end of the day, we need the two driving innovation through competition. Even if the CHIPS Act, the government stock acquisition, and the nVidia partnership are solely aimed at bringing more, needed, chip manufacturing to the US, it could create a situation where that amount of leverage puts Intel into a hyper dominant position again in the near future. Honestly, I hope I'm wrong.",Intel,2026-01-05 18:49:13,2
AMD,nxwdvf9,"Hey sorry to interject like that, can you ask around what ppl in the team think the safe VCCSA voltage for raptor lake is beyond standard spec? You can dm the answer if u want. I'm from Russia so it's not like I'll go run RMA'ing this stuff just because you told me that info",Intel,2026-01-05 22:18:29,1
AMD,nxv1r9g,"Did you even read my post, or are you a bot? Seriously asking. I didn't bring up geopolitics. I live in Canada my dude. AMD has their graphics office still right beside the TO airport, and that's only a side comment. AMD is American too. My concern isn't about anything you just said lol.",Intel,2026-01-05 18:36:10,0
AMD,nxv9flb,65535,Intel,2026-01-05 19:10:25,1
AMD,nyk009z,"\>then why does lowering voltage reduce power consumption?  I have no idea what is the workload you are using which is guaranteed to never hit the limits. Solitaire?   Undervolting does not guarantee lower power usage with modern boosting CPUs. It very obviously (to any sane person) depends on workload.  \>Maybe read here more.   \>Thomas\_Hannaford, Employee   â€Ž You are so cute.  \>Motherboard power delivery settings exceeding Intel power guidance.  Literally what I said in 2, 3, 4 but in stupid terms.  \>Microcode and BIOS code requesting elevated core voltages which can cause Vmin shift especially during periods of idle and/or light activity.  Literally what I said.  \>IntelÂ® reaffirms that both **IntelÂ® Coreâ„¢ 13th and 14th Gen mobile processors** and future client product families â€“ including the codename Lunar Lake and Arrow Lake families - are unaffected by the Vmin Shift Instability issue.  This is verifiably bullshit because nothing about mobile processors protects them from wrong voltage received because of wrong LLC settings.  \>CPUs follow a voltage-frequency (V-F) curve  If you do not know that Intel is requesting high voltage to avoid insufficient voltage before frequency boost you do not know jack shit. It is literally what happens under description ""**Microcode and BIOS code requesting elevated core voltages which can cause Vmin shift especially during periods of idle and/or light activity**"" and there is a video proof of that with an oscilloscope.  No settings which you mentioned are preventing irreversible damage.   Claiming that you use Intel for reliability after you did not wish to use your own Intel at stock settings is laughable.",Intel,2026-01-09 07:01:30,0
AMD,nyjz7xj,"You skipped the 8600K, it also did 5ghz",Intel,2026-01-09 06:54:55,1
AMD,nzt2k76,"Anything over 4.7ghz on Haswell-E was NOT very common. I would say 4.4-4.5 on those chips was more realistic. 5 GHz we are talking golden godly chip.  I had a 5960X for 8 years, one of the better ones manufactured in Costa Rica and not Malaysia which was better for OC, and mine did 4.7 ghz.",Intel,2026-01-15 21:49:25,1
AMD,nxv805h,"My initial comments were on, ""I'm also not going to act like Intel being now backed by the US Government and MAGA"" and what the funding was for.  Edit: and my response was about who also backed the funding.",Intel,2026-01-05 19:03:57,1
AMD,nxwhiw8,"All I can recommend and say is follow whatever guidelines you're given officially and make sure your BIOS is updated. All those teams work to make sure it's delivered to the customer. Otherwise it's all random, some parts can handle higher voltage, some can't.  The term silicon lottery is real and just a nature of small scale manufacturing, EM and quantum effects these days.  I will say, with our new CEO a lot of these customer issues are now streamlined internally. Used to be layers between engineering and customer interaction, so I expect better responses and reliability than before.",Intel,2026-01-05 22:36:31,1
AMD,nxv9vyk,"Ah, so you mean *lower* the TREFI then, from what JEDEC or XMP specified?",Intel,2026-01-05 19:12:28,1
AMD,nzv5tob,"I believe that refined process nodes (the â€œ+++â€ stages) are what really improve clock speeds, not the first generation of a new node, or at least this is the case for the last 15 years.   Here are some examples:   -Intelâ€™s 32nm Sandy Bridge came after the early 32nm CPUs (dual-core i3/i5 300/500 series and the 6-core 980X), and Sandy Bridge turned out to be an excellent overclocker.   -22nm Ivy Bridge and Haswell were mediocre in that regard, while Haswell-E (a more mature implementation) overclocked better. Broadwell-E and early Skylake also werenâ€™t great for OC.   -Once the node matured, things improved again. Coffee Lake (refined 14nm) made 5 GHz relatively easy. The same pattern repeats with 10nm: Alder Lake (12th gen) came after Tiger Lake and could already reach ~5 GHz, while Raptor Lake pushed clocks even higher, up to ~5.7 GHz.   Overall, higher clock speeds tend to come from node maturity and refinement, not from the first use of a new manufacturing process, and I believe 5.8ghz could have been possible if intel used its own 10nm process node on ARL considering its HyperThreadingless.",Intel,2026-01-16 04:43:25,1
AMD,nxv944s,"On that side, it'll depend on what the US does with the stock it has. Canada bought GM stock during the 2008 crash, and then quietly sold it off as GM recovered. If the US does that, I don't really see an issue.  The MAGA part was a half hearted comment, made to follow the weirdness of the whole situation and comment I was replying on as well. As I said in a separate post:  ""It's so strange to me that we've hit a spot in consumer and enterprise computing where politics is now a factor (speaking about how decisions beyond national security are now driven by politics and trying to be on the right side at any moment when it matters). It's a different game when companies have to worry about that side, far too many consumers make decisions based on their politics when all that does is cause other issues. Obviously I won't go farther than that in a tech focused discussion lol, but I will say again that it's a different game and I don't think anyone wins if it becomes the norm.""  I'm going to add, light heartedly, I do hope we see AMD/nVidia/Qualcomm/etc manufactured and final packaged products coming out of Intel Foundries one day. I'm not sure how or if it will work, but the situation seems dead set not to allow significant further nodes beyond 2 nm or packaging to occur outside of North America. If the US wants viable national chip production, Intel is the better option, I hope it works out in a way that maintains design level competition while meeting national security goals.",Intel,2026-01-05 19:08:59,1
AMD,nxwo9wf,"I get that. But itâ€™d be nice to have a â€œthis voltage is safe for 99% of the cpus and this voltage is the LD50 for the cpuâ€. That would probably improve customer relations but your legal team might be very, very unhappy with that lol.   Anyway cheers for the response, with the way things are looking up for intel, i might be buying some stocks soon",Intel,2026-01-05 23:10:36,1
AMD,nxvnm9c,JEDEC 5600 cl 40 kingston fury sodimm 2x 16 gb,Intel,2026-01-05 20:15:57,1
AMD,nxws02u,"I mean, you can just figure that out yourself and buy new CPUs till you get one working :P",Intel,2026-01-05 23:30:06,1
AMD,nxvpz7b,"Yeah, but what was TREFI before you lowered it?",Intel,2026-01-05 20:27:01,1
AMD,nxvvs3t,10000 ish,Intel,2026-01-05 20:54:09,1
AMD,nxiuczi,"Hi everyone if I'm upgrading my Dell vostro 3670 i5 8400 @32gb ram to an i7 9700, would I be able to upgrade the RAM it's still being ddr4? To 64 or 128?",Intel,2026-01-03 22:49:37,1
AMD,nxrm6ic,"Hi there I have an xps 15 9530 laptop with two gpus: one is an arc a370m and the other is an iris xe graphics and in the Intel system it says I can use rebar, but I've tried and searched everywhere in the BIOS and followed countless guides and can't seem to find the setting. Can someone help me with enabling it please. I've searched the bios and done everything and can't seem to find it",Intel,2026-01-05 05:08:11,1
AMD,ny2u31x,"With the crazy RAM prices, I'm looking to move to a 13600K or 14600K to keep using the 64GB of DDR4 from my ancient 7700K build. Do we users generally consider Vmin Shift Instability to be fixed at this point through the series of BIOS and microcode updates?  Related: Should I expect something in the range of a 10% performance drop from any of the reputable reviews, due to the fixes? Also, are efficiency-cores pretty much working as intended at this point, or is thread scheduling still a concern on them where your high performance thread ends up on an e-core?  Thanks all!  Note: This question is not for Intel\_Support. The answer from your side would obviously be ""Yes!"". :)",Intel,2026-01-06 21:07:15,1
AMD,ny85o2z,Is Tiber cloud gone forever?  https://console.cloud.intel.com/ just gives a DNS error now.,Intel,2026-01-07 16:41:57,1
AMD,o04fx6u,"I have installed new Intel Wi-Fi 6 AX210.NGWG.NV in my ASUS laptop, bcz the old one died and couldnt connect to bluetooth since, WIFI works perfectly fine tho, so i dont know if problem is with drivers or not. Also i would just instal them from Intel, but i live in russia and i dont know any trustworthy sites, so if anybody knows, i would be really gratefull",Intel,2026-01-17 15:56:24,1
AMD,o07e6d2,"How do I know the legitimacy of an Intel wifi card, model AX210? I've been searching for it in Amazon and most are manufactured in Vietnam and China, with varying prices.",Intel,2026-01-18 00:38:33,1
AMD,o0uehap,"I keep seeing mentions of TPM in our system requirements and I'm honestly a bit lost on what it actually does for our security, so who is the best person in the org to chat with to get the full rundown?",Intel,2026-01-21 12:32:18,1
AMD,nxwkozf,"u/Chelostyles Thank you for your inquiry regarding the CPU and RAM upgrade for your Dell Vostro 3670. As much as I'd like to provide my technical insights on this upgrade path, I'm not in a position to provide specific suggestions since this involves hardware modifications to an OEM system.  For the best compatibility outcome and to ensure optimal system performance, I strongly recommend reaching out to your system manufacturer directly. They can provide definitive guidance on supported CPU upgrades (i5-8400 to i7-9700) and maximum RAM configurations for your specific model. We don't want to inadvertently bypass any warranty terms and conditions on your system by providing modification recommendations that might affect your coverage.  Your system manufacturer's technical support team will have access to the exact specifications, BIOS compatibility matrices, and supported hardware configurations for your Vostro 3670 model. They can confirm whether the motherboard supports the i7-9700, the maximum RAM capacity (64GB vs 128GB), and any potential limitations or requirements for these upgrades.  This approach ensures you get accurate, manufacturer-validated information while maintaining your system's warranty protection.",Intel,2026-01-05 22:52:24,1
AMD,nxwjdkt,"u/I_like_carsyay  XPS 15 9530 hardware does support Resizable BAR, which is why Intel's system detection shows it as available for both your Arc A370M and Iris Xe graphics. However, the system manufacturer has designed their BIOS interface to prioritize stability and user-friendliness, often managing advanced PCIe features like ReBAR automatically in the background rather than exposing manual configuration options. This approach ensures optimal system performance while reducing complexity for users. I recommend checking for the latest BIOS updates from your OEM's support site and contacting their technical support team, as they would have the most current information about how ReBAR is implemented on your specific model and whether any additional configuration steps are needed to fully utilize this feature.     I've posted an article below in case you haven't yet come across it:  **Helpful Resources:**  *  [What Is Resizable BAR and How Do I Enable It?](https://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html)",Intel,2026-01-05 22:45:46,1
AMD,ny3upu3,"u/QunatumLeader Hi, thanks for your interest!Â  You can find and apply for all of our jobs online atÂ [http://](http://jobs.intel.com/)[j](http://jobs.intel.com/)[obs.intel.com](http://jobs.intel.com/). We donâ€™t currently accept submissions via social.Â  Good luck!",Intel,2026-01-07 00:05:20,2
AMD,o0l3yzt,"Late to this, but I'm a 13900K owner. I have not had any issues with stability since applying the BIOS update and haven't noticed any performance loss, so I think this is fine. I did not thoroughly benchmark before and after though, partially because of how high peak temperatures were before the update. I am using a Noctua NH-D15 and a contact frame to reduce CPU temperatures.  Up until a few days ago I would have said that thread scheduling isn't an issue, but then I played the game Maneater and it's basically unplayable unless you use launch options to force the game to only P-cores. There's the Intel ""Application Optimizer (APO)"" utility but it seems abandoned and you can't add your own games if Intel hasn't added a profile. I was a big proponent of E-cores but honestly it seems like a half-baked technology that Intel never put the effort in to support properly. That said I guess I could just entirely disable them if I cared so much, but that's a non-trivial amount of performance to just give up.",Intel,2026-01-20 01:26:55,1
AMD,nya3rq0,Hi u/ConspiracyPhD **Post**Â a question onÂ [IntelÂ® Tiber Developer Cloud Community](https://community.intel.com/t5/Intel-Developer-Cloud/bd-p/developer-cloud)Â forum for further investigation.,Intel,2026-01-07 21:48:00,1
AMD,o0e1nqe,"u/Far-Common2207 In this case, we suggest buying the wireless module from authorized Distributors to mitigate the legit concerns. Other than that, the OEM module warranty is not covered by Intel. For more details, you need to work with the Distributor or place of purchase for support to further verify if the wireless card is legitimate.  Check this article: [Where to find the Serial Number for IntelÂ® Wireless Cards](https://www.intel.com/content/www/us/en/support/articles/000092302/wireless.html)",Intel,2026-01-19 00:35:58,1
AMD,o0ztjvw,"[**Plenty-Solution-3692**](https://www.reddit.com/user/Plenty-Solution-3692/)**, TPM (Trusted Platform Module)** is builtâ€‘in security hardware that helps protect important data on your PC using encryption**. Intel PTT** is Intelâ€™s TPM that lives in the system firmware instead of being a separate chip, but it works the same way. Most PCs from the last few years already have TPM 2.0, sometimes it just needs to be turned on in the system settings. . If youâ€™re not sure how to do that, your motherboard or PC manufacturer should be able to help.  You can check this article for more information: [What Is Trusted Platform Model (TPM) and Its Relation to IntelÂ® Platform Trust Technology (IntelÂ® Pâ€¦](https://www.intel.com/content/www/us/en/support/articles/000094205/processors/intel-core-processors.html)",Intel,2026-01-22 05:04:22,1
AMD,nyarzrn,Forum doesn't exist or access denied.  I guess Tiber is just gone now.,Intel,2026-01-07 23:42:22,1
AMD,o0fgizr,Do you know any authorized distributors here in the Philippines?,Intel,2026-01-19 05:37:43,1
AMD,o0zyc8d,"I see, all good thanks for your support!",Intel,2026-01-22 05:39:13,1
AMD,nz1jsfl,u/ConspiracyPhD I just checked the forum and it looks like itâ€™s up and running. Could you try accessing it again using your Intel account?  [IntelÂ® Tiber Developer Cloud - Intel Community](https://community.intel.com/t5/Intel-Tiber-Developer-Cloud/bd-p/developer-cloud)  [](javascript:void(0);),Intel,2026-01-11 21:15:16,1
AMD,o0jlcxj,"u/Far-Common2207 According to the directory, these are the distributors in the Philippines. [Distributor Partners](https://www.intel.com/content/www/us/en/partner/showcase/partner-directory/distributor.html#sort=relevancy&f:@sfdisticountry_en=[Philippine,Philippines,Phillippines])",Intel,2026-01-19 20:45:26,1
AMD,nz301xe,"Nope.  https://imgur.com/a/tYRhYoV  Access denied and a nice ""This content is no longer available.""  Guess it's a completely dead project and should be removed from Intel's website.  http://console.cloud.intel.com/ is not accessible.",Intel,2026-01-12 01:35:48,1
AMD,nz3b0gd,"u/ConspiracyPhD Please check your inbox, Iâ€™ve sent you a personal message. Iâ€™ve already coordinated your concern with the respective team, and as per their instructions, youâ€™ll need to email them directly.  [](javascript:void(0);)",Intel,2026-01-12 02:33:45,1
AMD,nw3e1uz,that is the most non-descript render of a laptop possible,Intel,2025-12-26 22:13:56,5
AMD,nw638sa,So light it visibly doesn't have any ports?,Intel,2025-12-27 09:58:34,2
AMD,nur68kw,"Does intel 10A still come out as scheduled in 2027? I googled it and found out intel said the 10A will come out in 2027, but this was old news in 2024.",Intel,2025-12-18 21:37:42,16
AMD,nuu5n9y,I wonder how intel and other companies are going to manage for next year? Prices for memory and SSDâ€™s are predicted to go even higher putting off many buyers from getting a new PC build or laptop.   This makes me concerned Nova Lake wonâ€™t sell as well because of this.,Intel,2025-12-19 09:41:00,6
AMD,nuthq66,It's shameful to see LBT posing with 14A wafers when all the groundwork for this was setup by Pat Gelsinger. The entire Intel board should have been sacked instead of Pat.,Intel,2025-12-19 05:59:22,12
AMD,nur0ojq,"GFHK also has 14a for Razor and Coral Rapids in 2H 2027, so I'm taking what they are saying with very little credibility.   Plus, we had very similar rumors during 18A, and that went nowhere. Fool me once...",Intel,2025-12-18 21:10:09,12
AMD,nutolrl,Unbelievable till official announcement,Intel,2025-12-19 06:56:47,2
AMD,nusrcmh,can't they use it to make more ram ?,Intel,2025-12-19 03:04:41,3
AMD,nur9juo,good news,Intel,2025-12-18 21:54:21,2
AMD,nvzjgd1,They can't even sell 18A to NVDA what are they doing on 14A really ?,Intel,2025-12-26 06:29:30,1
AMD,nym6t8e,"Hm, we will see what happens with the stock price soon, but so far so good",Intel,2026-01-09 15:59:57,1
AMD,nur0nvy,"Lisa So Sue Me wants a taste of the Lip? Am I living in a different dimension? I callled out So Sue Me on X, is she jumping on Big Blueâ€™s Back?  Is anyone Dollar Cost Averaging INTC? It will still be awhile before IFS is firing on all cylinders. The Lip said he would stop high end chip production for external customers (If No One Took A Byte) in order to get $$$ to build out Ohio Fab.   Letâ€™s get it done. Iâ€™m driving distance from the Ohio Fab, any chance Intel will give me a tour?",Intel,2025-12-18 21:10:03,-17
AMD,nusksfh,"If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  [https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots](https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots)  *""Intel's previously-unannounced Intel 10A (analogous to 1nm) will enter production/development in late 2027, marking the arrival of the company's first 1nm node, and its 14A (1.4nm) node will enter production in 2026.* Â ***\[Edit:Â to be clear, this means 10A is beginning development, not entering high volume manufacturing, in 2027\]*** *The company is also working to create fully autonomous AI-powered fabs in the future.""*",Intel,2025-12-19 02:26:13,11
AMD,nur6gwd,"14A probably won't be ready for 2027, much less 10A.",Intel,2025-12-18 21:38:53,17
AMD,nutent6,10A & 7A are in R&D phase,Intel,2025-12-19 05:35:17,3
AMD,nurn23y,It's gonna be 2026 soon and 18A is launching at the very start of 2026. A double node shrink in like 2 years doesn't exactly sound very possible.,Intel,2025-12-18 23:07:17,6
AMD,nuu144l,"Remember, these are just names/nicknames. 10A? The difference between 14A and 10A is probably equivalent to the difference between 14nm and 14nm+",Intel,2025-12-19 08:55:31,2
AMD,nurpt6m,And yet here you are.,Intel,2025-12-18 23:23:22,10
AMD,nutpnod,"Brother, don't hint at your place of employment when you have your full face in your profile as well as you commenting in NSFW subs.",Intel,2025-12-19 07:06:02,3
AMD,nuteqb3,There will probably still be another of layoffs next month ðŸ˜‚,Intel,2025-12-19 05:35:49,2
AMD,nutezdb,"Yes, perhaps itâ€™s better if you post it on the r/intelstock subreddit instead ðŸ¤ª",Intel,2025-12-19 05:37:45,1
AMD,nv78q7z,"Ram should be at a more reasonable price in 2027 according to Moores Law is Dead. Maybe not $100 for 32GBs, but maybe below $200 ðŸ¤ž",Intel,2025-12-21 14:18:36,2
AMD,nvi0fpp,They have contract.,Intel,2025-12-23 05:47:11,1
AMD,nw3qq9x,"TBH I feel LBT is doing a good job. I was hesitant at first, but he's making a lot more sense than Pat's crazy descent into spending crazy amount of cash with no business in sight.  Speaking as a shareholder.",Intel,2025-12-26 23:27:31,3
AMD,nutoo6g,"The entire Intel board probably should have been sacked, but Gelsinger as well. He failed at his main mission and drove the company into a crisis. That kind of thing should have consequences.",Intel,2025-12-19 06:57:22,3
AMD,nuv6jd0,Who was it that decided to exit the SSD business.  They sold off a cash cow for pennies on the dollar.,Intel,2025-12-19 14:18:21,0
AMD,nutu4bu,Nvidia is at least some what believable. AMD though?,Intel,2025-12-19 07:47:24,5
AMD,nuu5f18,"I thought that too. At least they'd have some money coming in. But apparently it takes years to rejig the plants to churn out RAM instead of CPUs. And they're heavily invested in getting the next gen CPU fabs working.   Pivoting to RAM just doesn't make sense, unless they magic'd up a new type of RAM that's cheap to make and has super low latency - which is one thing I've always thought they ought to do.   Imagine if external RAM ran with super low latencies like CL1 or CL2 or something. You wouldn't even need branch prediction and prefetch and massive caches in the CPUs.",Intel,2025-12-19 09:38:44,3
AMD,nutophj,"""news"" needs a lot of quotes around it...",Intel,2025-12-19 06:57:41,1
AMD,nuuzsz6,This isn't wallstreetbets. We don't talk like that here.,Intel,2025-12-19 13:40:01,4
AMD,nusti41,">If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  Yup, and to make it even more obvious, the same graph also has Intel 14A showing up early 2026, and 20/18A showing up at the start of 2023*,* so clearly it's not the date of when the node is going to come out (or even start HVM).",Intel,2025-12-19 03:17:42,7
AMD,nurmeyo,"Dunno why this is being downvoted, the CEO of Intel himself said that 14A is a 28-29 node in the Q2 2025 earnings call.",Intel,2025-12-18 23:03:37,13
AMD,nuy09wl,enough info about how intel names products exists to know. if it didn't increase in transistor density per mm it would not be called 10A.,Intel,2025-12-19 23:07:36,6
AMD,nutpt6n,"I think everyone knows there will be continued Q1 and possibly Q2 layoffs.   Return to office didn't lead to enough voluntary attrition. Leadership wants to hit a magic number which sounds good for financial reports, not what is actually viable to run things.",Intel,2025-12-19 07:07:25,6
AMD,nw3rzlk,That crazy amount of cash being spent by Pat is what enabled 18A and 14A. They HAD to buy multuple $250M Litho machines from ASML in order to make that possible. Pat was playing catch up after years of under-investment by Swan and Krzanich. It was necessary and LBT is getting the credit. You don't appear to understand the lead times required in the semi industry. Pat understood that. The mistakes Pat made were trying to build a fab in Ohio and not cutting headcount and getting rid of dead weight sooner.,Intel,2025-12-26 23:35:12,3
AMD,nuuu28f,The thing intel is doing rn is literally pat's groundwork isn't it?,Intel,2025-12-19 13:04:50,10
AMD,nutv81y,Still a tall order imo unless it's some defense chip for RAMP-C,Intel,2025-12-19 07:57:50,1
AMD,nv0hjyu,"If they're following industry standards I'd say it depends on how good AMD's next gen is. Intel doesn't need direct access to AMD designs to etch chips for them, and designers make way more than per wafer than foundries do.  If AMD has superior designs to intel again they could finally ship out some damn chips for laptop OEMs. It would hurt intel more than the revenue would benefit them imo since client has really been carrying intel for the last six years and demand for AMD chips has been high despite the drip feed of strix chips. honestly I'm considering an AIO/NUC/whatever the new name is with strix halo and unified LPDDR5 to upscale old footage without having to use my daily desktop. imagine if it was available at scale.",Intel,2025-12-20 10:33:05,1
AMD,nuu642g,"they don't have to make faster ram, just make it, right now, some ppl don't really care about speed",Intel,2025-12-19 09:45:35,2
AMD,nuu0o0x,"So, risk production in late 27/early 28 and HVM in 2029 I suppose?",Intel,2025-12-19 08:51:11,2
AMD,nuvsda6,YEs it is. He did make mistakes. He was hiring like crazy at the beginning of his term. And he should have started cutting sooner. But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.,Intel,2025-12-19 16:10:45,3
AMD,nuwuwrt,"Nothing they're doing *right now* is a success story. Remember that they don't actually have customers, and that is first and foremost what got Gelsinger fired. As things stand, the foundry as a whole is a failure. If things turn around, that will have to be under Lip Bu.",Intel,2025-12-19 19:20:39,1
AMD,nuu5jf7,I wasnâ€™t aware 14A is part of the RAMP-C initiative. I thought it was only Intel 16 & 18A that are currently covered by RAMP-C?,Intel,2025-12-19 09:39:58,1
AMD,nuubzhq,I think so. Maybe optimistically we see a 14A product in late 28'.,Intel,2025-12-19 10:41:56,4
AMD,nuwv4b8,"> But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.  No, that was just more wasted money. 18A doesn't even use the high-NA machines Intel bragged so much about. It seems they tried blaming their struggles in foundry on the equipment instead of the broader org culture and talent.",Intel,2025-12-19 19:21:43,-1
AMD,nv78ydu,"As much as I hate to say it, Intel arc was also a mistake.",Intel,2025-12-21 14:20:00,0
AMD,nvl276b,get out of here with your sensable comments. we only circle jerk on this sub,Intel,2025-12-23 18:22:21,1
AMD,nuu6whm,It can expand in future ? My point is how can we believe such stuff at face value without actual proof.,Intel,2025-12-19 09:53:22,1
AMD,nuzuyhs,14A does use the High-NA machines. They didn't buy them with no plan to use them That would be stupid.,Intel,2025-12-20 06:44:37,3
AMD,nvl210i,"no it wasnt. GPU's are surpassing cpu's eventually if not now.  a major part of amds success  was buying radeon all those years ago. when intel realized how utterly shortsighted they had been, they pushed arc heavy even though it wasnt going to succeed that well.  this was the right choice, as otherwise they would look like a dinosaur.",Intel,2025-12-23 18:21:32,2
AMD,nuu7dr5,"It can expand in the future but this is a trial, itâ€™s not yet a long term commitment until the outcome of the project is known (final evaluation wonâ€™t be until 2026/2027). 14A is not part of RAMP-C, itâ€™s still in phase III trial with 18A. Thereâ€™s been no additional RAMP-C design calls via NSTXL that Iâ€™m aware of",Intel,2025-12-19 09:58:04,1
AMD,nv05iea,"> 14A does use the High-NA machines. They didn't buy them with no plan to use them  They bought the very first high-NA machines, claiming it was for 18A. Now they won't be used until a node that hits volume in '28/'29, by which point TSMC will have (or rather, already has) much better machines. So what exactly was the point?  > That would be stupid.  Is that not a perfectly apt description for Intel's foundry strategy in recent years? It sounds like they really drunk the coolaid with their attempts to blame the 10nm failures on the lack of EUV.",Intel,2025-12-20 08:28:08,2
AMD,nvl37xc,Yeah. The real mistake was LBT and the other Intel board members nerfing the r&d budget.,Intel,2025-12-23 18:27:21,2
AMD,nv074kj,14A will have volume production in 2027.,Intel,2025-12-20 08:44:44,3
AMD,nv63f2d,Didn't Intel say in a presentation that 2027 is risk production for 14A? https://www.techspot.com/news/107736-intel-doubles-down-foundry-ambitions-unveils-18a-14a.html  https://www.youtube.com/watch?v=5Jbj4RQBXbo&t=818s,Intel,2025-12-21 08:18:24,1
AMD,nv088jg,"Lip Bu himself is saying '28-'29. At this point, there isn't a chance in hell it's ready for volume in '27.",Intel,2025-12-20 08:56:04,0
AMD,nvqcqv3,I just wanted arc to succeed ðŸ˜”,Intel,2025-12-24 15:43:21,2
AMD,nybmp7b,did you buy one? I have owned two. A A750 and now a B580. They are great cards for the price paid. I'd like to upgrade to a B60 PRO. 24GB VRAM sounds amazing especially for $600. but I can't find one in stock anywhere.,Intel,2026-01-08 02:20:39,1
AMD,nsnn38a,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Intel,2025-12-06 21:19:54,57
AMD,nsyszxy,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Intel,2025-12-08 17:21:35,1
AMD,nt7hr1e,I hope it comes to desktop CPUs,Intel,2025-12-10 00:33:29,1
AMD,nspltzy,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Intel,2025-12-07 04:30:47,0
AMD,nsphwfn,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Intel,2025-12-07 04:04:39,11
AMD,nsokucv,Yeah this headline doesn't add up based on my own testing,Intel,2025-12-07 00:33:14,9
AMD,nsoke0g,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Intel,2025-12-07 00:30:33,5
AMD,nsr4t91,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Intel,2025-12-07 12:46:36,4
AMD,nsohcjh,concur  some benchmarks are biased,Intel,2025-12-07 00:12:56,3
AMD,nsyvkq6,lateral,Intel,2025-12-08 17:34:23,1
AMD,nspzeik,If itâ€™s just â€œ16% faster than 890mâ€ itâ€™s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Intel,2025-12-07 06:11:11,6
AMD,nsr2kyn,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Intel,2025-12-07 12:28:18,2
AMD,nsurw77,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Intel,2025-12-08 00:23:50,3
AMD,nsvascy,Answer to strix halo was the partnership with nvidia,Intel,2025-12-08 02:16:25,1
AMD,nspzssn,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Intel,2025-12-07 06:14:26,3
AMD,nssnwlk,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Intel,2025-12-07 17:57:13,7
AMD,nsvn3ok,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.Â      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Intel,2025-12-08 03:32:00,2
AMD,nsv64t7,"I mean no offense, but Passmark is irrelevant.Â  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.Â  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Intel,2025-12-08 01:48:49,4
AMD,nsy50hx,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Intel,2025-12-08 15:22:58,2
AMD,nsyymwg,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Intel,2025-12-08 17:49:21,1
AMD,nsyv727,"I guess we'll see more when we get actual info about the potential devices.Â  Right now, I haven't read about a device coming to market.",Intel,2025-12-08 17:32:30,0
AMD,ntimkr9,Back in the day you could overclock a 2600k from 3.4Ghz to 4.5Ghz on a $25 Hyper212 cooler. The performance gains were incredible as Sandy Bridge scaled very well at higher clocks.   Now days CPUs come overclocked already.,Intel,2025-12-11 19:25:58,29
AMD,ntifd3j,"""It's crazy to think that a cpu from 2009 can be easily overclocked.. 2.9Ghz to 4.1Ghz is crazy !""  You could overclock huge amounts on earlier generations - I used to run Pentium 4 1.6GHz chips at 3.2GHz on air-cooling, more on phase-change cooling.",Intel,2025-12-11 18:50:27,28
AMD,ntikk9s,"I ran my i5 750 2.67Ghz for years at 4Ghz without any issues. I benched it some at 4.2Ghz even, but it was not fully stable.  The X58 CPU are even better tho. And even if you had insane OC potential back in the days it was not as good as it sounds, since the turboboost was higher than the stock frequency that is listed.",Intel,2025-12-11 19:15:50,8
AMD,ntjwvoj,"Lol a 15 year old computer running Windows 11, meanwhile Microsoft telling people to upgrade 5 year old laptops for win10 being EOL.",Intel,2025-12-11 23:27:29,8
AMD,ntivoqo,X5690@4.6GHz on Rampage III Extreme ðŸ˜˜,Intel,2025-12-11 20:12:02,7
AMD,ntj26xa,it is crazy that intel sold you same technology at downclocked speeds to make a nice model range with different prices.,Intel,2025-12-11 20:45:29,4
AMD,ntiq1p0,Sick stuff. I still got my i7 930 at 4.2Ghz running just fine. These types of chips overclock like crazy.,Intel,2025-12-11 19:43:28,3
AMD,ntkwagl,Be nice. Give it another stick of ram!,Intel,2025-12-12 02:58:53,3
AMD,ntoxmhs,Q6600 G0,Intel,2025-12-12 19:02:37,3
AMD,ntiqlci,"Cool. Glad it worked for you. I have dual xeon server, maybe i should try it. But its production server dont wana break my apps. Lol",Intel,2025-12-11 19:46:15,2
AMD,ntjb06t,My 2500k did ~4.8 ghz and my 6950x did 5.2 ghz. Its base clock was like 3.2ghz and this was using 128GB of quad channel DDR4.  It was â€œstableâ€,Intel,2025-12-11 21:29:41,2
AMD,ntkz9ut,45nm is crazy in 2025,Intel,2025-12-12 03:16:12,2
AMD,ntosqvz,500W power draw when,Intel,2025-12-12 18:38:31,2
AMD,nthl3mn,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Intel,2025-12-11 16:23:16,1
AMD,ntk2ims,"I used to run my i3-540 at 4.2GHz, air cooled on what is effectively worse than a Hyper 212 Evo. I miss the old days when I could overclock the snot out of them. These days I guess they're binned to almost their max potential out of the factory so most of the time I'm undervolting them.",Intel,2025-12-12 00:01:05,1
AMD,ntk9tcx,Well done. Still using two H55m machines with OC (x3450 and i5 661).  They also OC decently at stock voltage keeping turbo and all power savings. My X3450 does 2.6 -> 3.3Ghz(3.8 turbo). The advantage is that it idles quite low at 50-60W.   But for gaming and rendering it's better to go all in as you did. Most chips can do anywhere from 3.8 to 4.2 all cores IME.,Intel,2025-12-12 00:44:00,1
AMD,ntl2d0n,nah my 40 logical processors would smash through it all  x2 xeon e5-2680 v2,Intel,2025-12-12 03:34:33,1
AMD,ntk4sw2,"is that better? I dont need to dive into setting anymore, the CPU maker do it for me with warranty.",Intel,2025-12-12 00:14:36,4
AMD,ntsgvaj,"There is still more to work with, especially if one does not fossilize on static all core OC, but does 2-step TVB fueled dynamic OC, Ecores are Aldo the source of much happiness on arrow",Intel,2025-12-13 09:30:18,1
AMD,ntja1e8,I miss overclocking. Felt like you were getting a bargain. Now I donâ€™t even try.,Intel,2025-12-11 21:24:52,14
AMD,ntjnkj4,"Not as big an OC as yours, but I had a pre-built from FutureShop.  It was their home brand name.  Found a BIOS for the board that wasnâ€™t theirs.  Managed to get 3.2GHz out of a 2.4GHz Pentium 4 on pre-built from FutureShop cooling.",Intel,2025-12-11 22:34:57,3
AMD,ntihjuu,"Wow, soo cool",Intel,2025-12-11 19:00:59,1
AMD,ntnudso,The motherboard doesn't accept other stick of ram. Only my corsair ram work,Intel,2025-12-12 15:49:11,1
AMD,ntmrk4p,How did you even get a 6950x to boot at 5.2ghz? Most of them hit a wall around 4.3ghz,Intel,2025-12-12 12:03:28,3
AMD,ntkag8u,"Has its ups and downs. Now that I'm older and have less time to tweak things and mostly just want shit to be stable, I see ""pre-overclocked with maybe 5% performance left on the table"" as a pro. The con is that chipmakers just jam a ton of power through it to make it happen, and the option of buying a half-price chip and spending an entire sleepless weekend tweaking it yourself to get 95% of the more expensive chip's performance is gone.",Intel,2025-12-12 00:47:48,5
AMD,ntjl0xd,"Same - the complexity and heat rose a lot and the gains because less significant - with multi-core chips and turbo frequencies there just isn't much headroom in them.  That and I work fixing issues with computers all day, I just want my own PC to work.",Intel,2025-12-11 22:21:04,6
AMD,ntlnjst,"Thats because these older CPUs were surprisingly energy efficient. Also mostly because now modern CPUs are powerful enough where overclocking is pointless. Even my i3-12100 being overclocked would be pointless, even if its only a 80 watt CPU",Intel,2025-12-12 05:56:24,2
AMD,ntwqjex,He couldn't without LN2.,Intel,2025-12-14 01:36:27,2
AMD,ntnxj8h,"It was short lived, over ~7 years I had to pull back the multiplier from 52 to 44 to keep it stable.  I retired the system this year.  It was a full open loop from EK.  2x Pascal Titan X in SLi",Intel,2025-12-12 16:04:28,0
AMD,ntmp3kk,"I used to overclock everything, now I undervolt everything lol",Intel,2025-12-12 11:44:00,2
AMD,nonhxk4,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is â€œslowâ€ is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already â€œbeatingâ€ AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Intel,2025-11-13 16:13:13,52
AMD,nootzxi,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Intel,2025-11-13 20:08:27,9
AMD,nrd4uj2,"I've gotten one and honestly it's amazing, easily the best laptop I've ever used so far.   I was skeptical about the battery life claims but I've genuinely found that using it for about 8 hours straight for coding, only drains the battery maybe 50%.  I've set it to only charge up to 80% max for battery health conservation, and I've regularly coded for 12 hours straight on the medium performance profile and haven't needed to charge until I got back home.  (This is for the Ultra 7 258v cpu variant btw)  Also this is while running Fedora with KDE Plasma which makes the battery life even more impressive as it's one of the heavier distros running cutting edge hardware and I've heard that Linux has less battery optimization compared to windows.    Screen isn't anything to write home about but the 100% srgb one looks good enough and is bright, 60hz looks kind of bad but I know that it saves a lot on battery.   Keyboard feels very nice as far as laptop keyboards go, having it be easily swappable is lovely as I wore out the keys on my old laptop, and I want this thing to last.   Linux hardware compatibility is perfect so far, even the fingerprint sensor works out of the box on fedora.   My only real complaint is that the plastic it is made out of is a major grease magnet and if I touch it without having immediately washed my hands, even if my hands weren't dirty, it'll leave dark patches from oils. Also it would be nice to have swappable RAM but I think 32gb ought to last a very long time anyway.   Genuinely seems like arguably one of the, if not the, best laptops for actually getting work done. Maybe it's not as fancy or sleek, but it just works. It's like the 2001 Toyota of the laptop world, it's not winning prizes for looks, but it'll never die, gets good mileage (battery life), and is easily repairable. Maybe not the laptop you want, but definitely the one you need (excluding people who need something like a dedicated GPU or really need super high CPU performance).",Intel,2025-11-29 11:19:13,3
AMD,noon81e,"Intel beats AMD in software (drivers, firmware) â€¦ I got think pad 780M laptop by company I work for. Randomly display wonâ€™t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Intel,2025-11-13 19:34:25,11
AMD,non7ozt,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Intel,2025-11-13 15:23:02,7
AMD,nom9a0s,lol. Even in the cons it says weaker multicore than AMDâ€¦.?   This article seems like AI wrote it,Intel,2025-11-13 11:53:59,1
AMD,nr8651t,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Intel,2025-11-28 15:11:41,1
AMD,nopvuqn,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Intel,2025-11-13 23:25:31,0
AMD,nonh9ew,Suck at gaming.,Intel,2025-11-13 16:09:54,-15
AMD,notgml4,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Intel,2025-11-14 15:00:36,9
AMD,noy6f36,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Intel,2025-11-15 08:22:42,9
AMD,npap4we,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Intel,2025-11-17 10:42:37,3
AMD,ntz6joo,Isnâ€™t the keyboard one of the most important characteristics?,Intel,2025-12-14 13:39:49,1
AMD,nopn323,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Intel,2025-11-13 22:36:30,-1
AMD,ntz6z28,"Hey Iâ€™m looking at the exact same laptop that you have. Can you tell me about the build quality and if thereâ€™s any keyboard flex when pressing down on it? Please tell me. Iâ€™m going to use it for word, excel, reading lots of pdf files and ebooks and watch movies. Will it be enough for that?",Intel,2025-12-14 13:42:38,1
AMD,nop3ehp,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Intel,2025-11-13 20:56:10,5
AMD,nongtqn,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Intel,2025-11-13 16:07:47,4
AMD,nousjfs,Soldered ram is a lot faster. So no.,Intel,2025-11-14 18:58:54,3
AMD,noniq16,Yes but now RAM costs a ton of money,Intel,2025-11-13 16:17:05,-1
AMD,nomcmkj,Is multicore performance the only consideration when buying a laptop?,Intel,2025-11-13 12:19:56,27
AMD,nqyoc8i,What kind of issues?,Intel,2025-11-26 23:00:59,2
AMD,ntz73fr,What kind of issues with Intel? I thought it was the AMD that had tons of issues,Intel,2025-12-14 13:43:27,1
AMD,nonhqb5,It is not a gaming laptop,Intel,2025-11-13 16:12:13,17
AMD,np9o16h,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Intel,2025-11-17 04:44:52,2
AMD,np3siex,It's an enterprise grade product you buffoon.,Intel,2025-11-16 06:03:41,4
AMD,npd9987,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Intel,2025-11-17 19:32:27,2
AMD,np8gg6z,Build quality.Â    Thinkpads are solid machines that are easy to fix.Â    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Intel,2025-11-17 00:13:54,1
AMD,ntzg92j,"So build quality will be subjective, from what I can tell, it's got very good build quality in terms of ""real"" factors such as durability. But it definitely feels less ""premium"" than similarly priced consumer grade laptops. The plastic is plastic so it will flex a little bit, but the parts all seem very well put together and it does feel ""solid"" overall.   I haven't really noticed keyboard flex, but I have noticed a slight amount of flex where my palms rest, particularly on the right side, where the smart card reader is, which makes sense as it is just a big hole in the side of the laptop. I plan on getting a dummy smart card to fill the gap and hopefully that should reduce it.   Overall whilst the internal chassis is metal, the outside is just plastic. I imagine that is good for durability, as it ought to be able to absorb shocks, but, as I said, it definitely makes it feel less ""premium"". They key press feel of the keyboard definitely does feel very nice as far as laptops go though. Obviously it's still nothing compared to a good mechanical keyboard but for a laptop it's very nice.   I bought this laptop for longevity and durability, so given that It's only just come out, I can't really say much about that, but the prestige of thinkpads of previous generations kind of speaks to their reliability. Plus it's apparent that they are still quite easy to repair and Lenovo has video guides on replacing loads of the parts.   And for your use case the battery life should be very good. It seems the Intel chip was designed to be very efficient during periods of downtime and something like viewing a PDF or editing a document has a LOT of downtime for the CPU",Intel,2025-12-14 14:40:19,2
AMD,norwnxs,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Intel,2025-11-14 07:46:09,5
AMD,noruygl,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Intel,2025-11-14 07:29:24,3
AMD,noxc6wn,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Intel,2025-11-15 03:57:11,2
AMD,nop71cl,All the more reason to make it upgradable,Intel,2025-11-13 21:14:31,-2
AMD,nomhldl,Lunar Lake isnâ€™t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Intel,2025-11-13 12:55:02,15
AMD,nomp84g,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Intel,2025-11-13 13:42:02,4
AMD,nu0g7o7,This was back when the 14th generation were having issues.,Intel,2025-12-14 17:47:28,2
AMD,nonivqb,"It's $2,000 so no excuse.",Intel,2025-11-13 16:17:52,-10
AMD,nu6fuik,Thank you so much for this valuable and comprehensive information! I really appreciate it:),Intel,2025-12-15 16:29:10,1
AMD,npd8o9g,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Intel,2025-11-17 19:29:33,3
AMD,nomwgxm,"Lunar Lake already beat AMD, nobody buys AMD laptops",Intel,2025-11-13 14:23:36,19
AMD,non5ael,i stopped at $2100 for a Thinkpad T14,Intel,2025-11-13 15:10:49,1
AMD,nu0kv9z,"Ah okay, got it thanks.",Intel,2025-12-14 18:10:16,1
AMD,nowos5a,wrong  Nobody Supply AMD laptop     There fixed for u,Intel,2025-11-15 01:22:32,2
AMD,nov79aa,"I do, and many of the people I know do.",Intel,2025-11-14 20:14:59,-1
AMD,nonhh1g,Nobody pays that much.,Intel,2025-11-13 16:10:57,6
AMD,np79214,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Intel,2025-11-16 20:19:32,-1
AMD,nmdn09e,Try the shunt mod,Intel,2025-10-31 14:42:59,12
AMD,nmef8nt,"Cool, errr...  icy",Intel,2025-10-31 17:01:07,5
AMD,nmg9eah,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Intel,2025-10-31 23:03:20,2
AMD,nmi6hcl,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Intel,2025-11-01 08:35:35,2
AMD,nn1205o,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Intel,2025-11-04 08:46:02,2
AMD,nmicxp5,did you use dry ice? how did you hit sub-ambient?,Intel,2025-11-01 09:46:46,1
AMD,np6680l,I ordered a steel legend B580 and Iâ€™m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Intel,2025-11-16 17:04:39,1
AMD,nmfa81q,Are you in the US? If so how were you able to get Maxsun?,Intel,2025-10-31 19:39:09,1
AMD,nmg20dw,Oh... for sure ðŸ˜,Intel,2025-10-31 22:15:08,3
AMD,nmi9zg0,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Intel,2025-11-01 09:14:35,1
AMD,nn1h3l3,Great work dude! Only 200MHz to go ðŸ˜‰,Intel,2025-11-04 11:15:21,2
AMD,nmilk0q,Car coolant in the freezer ðŸ˜,Intel,2025-11-01 11:12:18,2
AMD,np782zx,That's the way! Let us all know the results.,Intel,2025-11-16 20:14:39,1
AMD,nmg21z3,I am in Australia.,Intel,2025-10-31 22:15:25,3
AMD,nmg24cm,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Intel,2025-10-31 22:15:50,2
AMD,nmiujle,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Intel,2025-11-01 12:26:53,1
AMD,np7d3w5,Should be here in a few days and Iâ€™ll tear it down and prep it. Iâ€™ll see how it goes when itâ€™s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. Iâ€™ll let you know.,Intel,2025-11-16 20:40:17,1
AMD,nmtio7j,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Intel,2025-11-03 03:07:52,1
AMD,nmj43zx,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Intel,2025-11-01 13:30:54,2
AMD,npa5wyd,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Intel,2025-11-17 07:21:57,1
AMD,nmj9sh7,Oh! You put the car coolant to run through a freezer? Wow! Nice,Intel,2025-11-01 14:05:32,1
AMD,ngieos7,And largely against the non-x3d lmfao.,Intel,2025-09-27 17:21:03,78
AMD,ngif1q6,Aren't they just showing that AMDs CPUs are better for gaming?,Intel,2025-09-27 17:22:52,30
AMD,ngmmadi,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores theyâ€™re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K arenâ€™t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointingÂ  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Intel,2025-09-28 10:33:19,2
AMD,ngiqxv3,"I havenâ€™t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30â€“40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Intel,2025-09-27 18:23:57,-17
AMD,ngp4a4b,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Intel,2025-09-28 18:56:21,0
AMD,nglqard,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Intel,2025-09-28 05:26:07,32
AMD,ngiw9gz,I assume they compared with CPUs in a similar price range,Intel,2025-09-27 18:52:05,35
AMD,ngl774g,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Intel,2025-09-28 02:59:30,6
AMD,ngj2d2a,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Intel,2025-09-27 19:24:33,53
AMD,ngmt8qo,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Intel,2025-09-28 11:35:30,10
AMD,ngn0xy1,">asking others what scores theyâ€™re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K arenâ€™t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Intel,2025-09-28 12:33:09,3
AMD,ngix2qg,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Intel,2025-09-27 18:56:22,32
AMD,ngiy5wa,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Intel,2025-09-27 19:02:07,9
AMD,ngir4n9,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Intel,2025-09-27 18:24:57,3
AMD,ngj6exq,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Intel,2025-09-27 19:46:01,2
AMD,nhi1lee,"Now install windows 11, lol",Intel,2025-10-03 06:08:42,1
AMD,nglqeum,"> I havenâ€™t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Intel,2025-09-28 05:27:08,1
AMD,ngkj8l9,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Intel,2025-09-28 00:25:26,12
AMD,ngkwq9d,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Intel,2025-09-28 01:51:37,19
AMD,ngmy2qz,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Intel,2025-09-28 12:12:58,4
AMD,ngmx0bu,"I sell PC hardware and get to test a wide range of components. Iâ€™ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. Iâ€™ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but Iâ€™m still not sure. Since Iâ€™m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesnâ€™t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30â€“40% slower than the 14700K. Why does it feel like Iâ€™m the only one who knows this?  As for proof, Iâ€™ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. Iâ€™ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Intel,2025-09-28 12:05:06,2
AMD,ngn1g9m,"Ive given my evidence by videos recorded by phone and obs,Â  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Intel,2025-09-28 12:36:42,5
AMD,ngiyobq,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Intel,2025-09-27 19:04:51,-5
AMD,ngizavn,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Intel,2025-09-27 19:08:11,3
AMD,ngjh5f6,"I only tested 5 games, and the two processors were very close. Iâ€™d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Intel,2025-09-27 20:42:11,5
AMD,nhk7a3s,I did same performance on all processors.,Intel,2025-10-03 15:24:53,1
AMD,ngm7i8q,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Intel,2025-09-28 08:05:53,4
AMD,ngkykmi,That sounds like an AMD Stan argument circa 2020,Intel,2025-09-28 02:03:12,17
AMD,nh261w2,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Intel,2025-09-30 19:23:27,1
AMD,ngn25ws,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Intel,2025-09-28 12:41:35,0
AMD,ngjdprn,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Intel,2025-09-27 20:24:13,11
AMD,ngj1pgu,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Intel,2025-09-27 19:21:01,8
AMD,ngkqt0z,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Intel,2025-09-28 01:12:59,1
AMD,ngj49y7,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Intel,2025-09-27 19:34:44,3
AMD,ngk1b4v,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Intel,2025-09-27 22:35:49,3
AMD,nhka12e,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Intel,2025-10-03 15:38:08,1
AMD,ngmkwf8,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Intel,2025-09-28 10:19:51,9
AMD,ngnw7zu,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Intel,2025-09-28 15:28:44,4
AMD,ngl05xq,Expand ?,Intel,2025-09-28 02:13:17,-2
AMD,nh27g09,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Intel,2025-09-30 19:30:10,1
AMD,ngtvgdv,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Intel,2025-09-29 14:03:26,5
AMD,ngw45vk,"I spammed my score screenshots on every popular tech tuberâ€™s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores theyâ€™re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isnâ€™t that the case in other tech site charts?",Intel,2025-09-29 20:34:34,0
AMD,ngjg69y,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Intel,2025-09-27 20:37:03,6
AMD,ngqfmrw,"Did you get any more FPS when you tried it? Iâ€™m getting good frames without it, so I donâ€™t think itâ€™s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there werenâ€™t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Intel,2025-09-28 22:53:49,1
AMD,ngk26op,did you do it,Intel,2025-09-27 22:41:09,1
AMD,ngmi765,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Intel,2025-09-28 09:53:01,1
AMD,ngk22ke,can you reset settings then choose ray tracing ultra preset.,Intel,2025-09-27 22:40:28,2
AMD,nhle3iw,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Intel,2025-10-03 18:52:42,1
AMD,ngnx0bd,because they exclusively exist in DIY build your pc enthusiast bubble,Intel,2025-09-28 15:32:30,3
AMD,nhvyteo,Pricing was aggressive. A 12 core 3900x was 400 usd.,Intel,2025-10-05 13:26:56,1
AMD,ngl3zfu,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Intel,2025-09-28 02:38:06,14
AMD,nh2848m,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Intel,2025-09-30 19:33:29,1
AMD,ngtxj1d,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.Â   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Intel,2025-09-29 14:14:15,4
AMD,ngmif3t,"Okay, I did it",Intel,2025-09-28 09:55:13,2
AMD,ngmglra,"No, I didnâ€™t remember good",Intel,2025-09-28 09:37:10,1
AMD,ngk41a9,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Intel,2025-09-27 22:52:22,2
AMD,nhp053x,Thanks for solidifying opinion that your benchmarks are fake,Intel,2025-10-04 10:05:07,1
AMD,nh2a34y,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Intel,2025-09-30 19:43:15,1
AMD,ngk5zrq,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Intel,2025-09-27 23:04:17,2
AMD,nhp2exv,"Whatâ€™s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If youâ€™re comparing score screenshots to OBS-recorded videos, you shouldnâ€™t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. Iâ€™ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think theyâ€™d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since Iâ€™ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Intel,2025-10-04 10:27:16,1
AMD,ngbpsza,Cam someone confirm or is this gas lighting?,Intel,2025-09-26 15:51:04,19
AMD,ngbym0c,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Intel,2025-09-26 16:33:40,16
AMD,nghesqk,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Intel,2025-09-27 14:17:43,3
AMD,ngc7w1b,Intel comeback real?,Intel,2025-09-26 17:18:16,5
AMD,ngf1ik5,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Intel,2025-09-27 02:43:51,2
AMD,ngbsck8,3D v-cache has entered the chat.,Intel,2025-09-26 16:04:06,7
AMD,ngbr9eb,Take it as a grain of salt. Intel marketing LOL,Intel,2025-09-26 15:58:30,2
AMD,nh5ixeo,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Intel,2025-10-01 08:33:03,1
AMD,ngfguk7,Thats cool ...but lets talk about better pricing.,Intel,2025-09-27 04:36:12,1
AMD,ngfrla1,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Intel,2025-09-27 06:09:56,2
AMD,ngc573e,Tech Jesus has entered chat :).,Intel,2025-09-26 17:05:31,-10
AMD,ngcjbbq,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Intel,2025-09-26 18:13:09,-12
AMD,ngbqjhe,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.Â   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform thatâ€™s already halfway out the door.",Intel,2025-09-26 15:54:51,39
AMD,ngdvx9l,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Intel,2025-09-26 22:23:43,12
AMD,nge3sfi,What do you mean by gaslighting in this case?,Intel,2025-09-26 23:10:51,3
AMD,ngcf9aj,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Intel,2025-09-26 17:53:19,1
AMD,ngcutw5,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Intel,2025-09-26 19:10:03,1
AMD,ngl3adb,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Intel,2025-09-28 02:33:31,1
AMD,ngdfut5,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Intel,2025-09-26 20:55:42,1
AMD,ngbxbws,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Intel,2025-09-26 16:27:28,-9
AMD,ngfqkoh,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Intel,2025-09-27 06:00:33,2
AMD,ngfebe1,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Intel,2025-09-27 04:16:03,2
AMD,ngfqbry,Nova Lake bLLC about to ruin Amd X3D party.,Intel,2025-09-27 05:58:18,1
AMD,ngc2ju0,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Intel,2025-09-26 16:52:57,18
AMD,ngc2czl,I always wondered if Intel marketing budget is higher than the R&D budget,Intel,2025-09-26 16:52:01,-7
AMD,ngfrkpn,Intel Arrow Lake is much cheaper than Amd Zen 5.,Intel,2025-09-27 06:09:47,3
AMD,ngemp1j,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Intel,2025-09-27 01:07:45,6
AMD,ngeo8em,only an AMD fan would worry about replacing their shit CPUs under 3 years,Intel,2025-09-27 01:17:36,0
AMD,ngbzwzr,Hardware unboxed isn't a reliable source.,Intel,2025-09-26 16:40:06,9
AMD,ngf1ob8,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  Itâ€™s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, itâ€™s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Intel,2025-09-27 02:44:54,9
AMD,ngealuz,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Intel,2025-09-26 23:51:44,1
AMD,nge8xbh,Telling people that its performance is better than it actually is?,Intel,2025-09-26 23:41:39,2
AMD,ngca7el,The ones with similar pricing not performance,Intel,2025-09-26 17:29:11,6
AMD,ngigkrj,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what â€œcheaperâ€ is seen as.",Intel,2025-09-27 17:30:41,2
AMD,ngfrgqn,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.Â    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Intel,2025-09-27 06:08:45,0
AMD,ngerbdr,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Intel,2025-09-27 01:37:32,1
AMD,ngezf04,Quite common for AM4 in my experience.,Intel,2025-09-27 02:29:45,2
AMD,ngihhii,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe Iâ€™ll just wait for next ones unsure.",Intel,2025-09-27 17:35:17,1
AMD,ngerg53,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Intel,2025-09-27 01:38:23,0
AMD,nggftxh,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Intel,2025-09-27 10:08:25,-1
AMD,ngeozwu,I originally had a Ryzen 5 3600 and am now on a 5700x3d so itâ€™s pretty relevant Iâ€™d say. Of course itâ€™s anecdotal but I donâ€™t think itâ€™s too uncommon,Intel,2025-09-27 01:22:31,-2
AMD,ngg1fuo,"I agree with you, itâ€™s daft, no doubt about it. But youâ€™ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and itâ€™s still holding its own. Is it on par with the 9800X3D? Of course not. But itâ€™s still a very capable bit of kit. Next time I upgrade, Iâ€™ll go top shelf again.",Intel,2025-09-27 07:42:27,-1
AMD,ngerrz8,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Intel,2025-09-27 01:40:28,10
AMD,ngeao9a,Sooo they are in the YouTube space for the money not for the love of tech,Intel,2025-09-26 23:52:08,6
AMD,ngfq1bg,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Intel,2025-09-27 05:55:39,2
AMD,ngdp9bd,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Intel,2025-09-26 21:45:51,-1
AMD,ngc0yus,"Even accepting that as true, I donâ€™t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. Itâ€™s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet itâ€™s probably not worth their money and time to test it.Â   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Intel,2025-09-26 16:45:15,10
AMD,ngeb3z7,Isnâ€™t that just either lying or exaggerating?   I know those words donâ€™t â€œgo hardâ€ and are â€œlow keyâ€ boring.Â   But I think some internet buzzwords are just overused or badly applied.,Intel,2025-09-26 23:54:48,8
AMD,ngmlzg8,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Intel,2025-09-28 10:30:22,1
AMD,ngtp3t3,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Intel,2025-09-29 13:28:22,1
AMD,nh5i8gf,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Intel,2025-10-01 08:25:38,1
AMD,nh5il7n,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Intel,2025-10-01 08:29:26,1
AMD,ngc36bx,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Intel,2025-09-26 16:55:54,4
AMD,ngcbde9,Sure but charts seem about right to me,Intel,2025-09-26 17:34:45,1
AMD,ngcxbac,APO is game specific. I'm referring to what has changed overall.,Intel,2025-09-26 19:22:34,3
AMD,nil3hc6,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Intel,2025-10-09 12:28:52,2
AMD,nimk9vp,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Intel,2025-10-09 17:05:36,2
AMD,njlc3cu,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Intel,2025-10-15 09:13:26,1
AMD,nkp7gzk,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Intel,2025-10-22 00:53:17,1
AMD,nksv7pa,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Intel,2025-10-22 16:17:48,1
AMD,nm887uc,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Intel,2025-10-30 17:38:48,1
AMD,nmj83z4,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Intel,2025-11-01 13:55:37,1
AMD,nnc1z1l,"Hi, Iâ€™m at my wit's end with my build and would really appreciate some advice.  My PC has been plagued by random crashes, CRC errors, and installation failures for months:  - Random application crashes, often citing `KERNELBASE.dll`, `ntdll.dll`, or `ucrtbase.dll`. - Frequent CRC errors when extracting large ZIP or RAR files. Retrying usually works. - Software installations fail with data corruption or unpacking errors, only to succeed when I try again. - Games crash or stutter randomly. - Very rare BSODs.  Specs:  - **CPU**: Intel Core i9-13900K - **Motherboard**: ASUS ROG STRIX Z790-H GAMING WIFI - **RAM**: 32GB (2x16GB) G.Skill Trident Z5 RGB (DDR5-6400 CL32, Model: F5-6400J3239G16GX) - **GPU**: MSI RTX 4090 Gaming X Trio - **Storage (OS Drive)**: Crucial P5 Plus 2TB NVMe SSD - **OS**: Windows 11   This is what I've already tried (everything passed):  *  **Memtest86:** Completed multiple passes with no errors. *  **Prime95 & Intel XTU:** Seems to be stable. *  **FurMark:** GPU stress test is stable. *  **Storage Health:** All drives pass SMART and manufacturer-specific self-tests. *  **System Integrity:** `sfc /scannow` and `DISM /RestoreHealth` complete successfully. *  **Updates:** All drivers, firmware, BIOS, and Windows are fully up-to-date. *  **Physical:** Cleaned the case, re-seated components, and replaced thermal paste. * **XMP**: I've disabled/re-enabled XMP multiple times, doesn't make a difference.  I have trouble finding a consistent way of reproducing the issue. Today I tried a 7-Zip benchmark which failed once with a ""decoding error"", but I wasn't able to reproduce it afterwards. I couldn't get Intel Processor Diagnostic Tool to fail after multiple hours. So...  *   **Is this a memory instability issue?** Could the RAM be faulty?  *   **Or, could this be a faulty CPU core?** I found [this post](https://www.reddit.com/r/intel/comments/15mflva/tech_support13900k_problems_when_using_multiple/) where a user had identical symptoms (CRC/7-Zip errors) that were only resolved by replacing a faulty 13900K, even with XMP off.  Thanks in advance for any help.",Intel,2025-11-06 00:15:38,1
AMD,noppamh,"# Alienware 34 - AW3425DWM resolution issues  [](https://www.reddit.com/r/ultrawidemasterrace/?f=flair_name%3A%22Tech%20Support%22)  I just got the AW3425DWM, and my laptop is a Dell Inspiron 15 5510, which is not a gaming laptop. I'm not a gamer.  When connecting through the HDMI port on the laptop and monitor, I can't set the resolution to 3440x1440; I can go up to 3840x2160, but not the monitor's native resolution. However, when I connect through the USB-C port on the laptop to the DisplayPort on the monitor, I can set the resolution to 3440x1440 without any issues. The downside is that I've lost the only Thunderbolt port I have available.  Is there a workaround for this issue? If I use an HDMI to DisplayPort adapter, will I be able to set the resolution to 3440x1440?  I understand that the HDMI 1.4 port usually can handle 21:9 resolutions, but with the latest Intel drivers, it isn't giving me the option for 3440x1440",Intel,2025-11-13 22:48:26,1
AMD,noqvatt,"Hi, can someone help me please? I'm trying to generate a video on AI playground after installing it but it just keeps loading for videos and images don't show up. That's my it it'll specs:   Processor: Intel(R) N95 (1.70 GHz) Installed RAM: 32.0 GB (31.7 GB usable) System Type: 64-bit operating system, x64-based processor Graphics Card: Intel(R) UHD Graphics   -That's a part of what's in the console:   Â No key found for setting negativePrompt. Stopping generation oa @ index-fOc02QH8.js:61 w @ index-fOc02QH8.js:61 await in w V @ index-fOc02QH8.js:22 pt @ index-fOc02QH8.js:61 await in pt V @ index-fOc02QH8.js:22 c @ index-fOc02QH8.js:61 await in c Il @ index-fOc02QH8.js:14 Sr @ index-fOc02QH8.js:14 n @ index-fOc02QH8.js:18 index-fOc02QH8.js:61 uploadImageName b99fb28ea4440a2f51ce53cd5c529554e5965b66f5f5a8506b9b973b66e754bd.png index-fOc02QH8.js:251 [comfyui-backend] got prompt (anonymous) @ index-fOc02QH8.js:251 (anonymous) @ VM5:2 emit @ VM4 sandbox_bundle:2 onMessage @ VM4 sandbox_bundle:2 index-fOc02QH8.js:61 updating image {id: '6eb88f1b-f433-4541-a421-40619ac9fdc2', imageUrl: 'data:image/svg+xml,%3C%2Fpath%3E%3C%2Fsvg%3E', state: 'generating', settings: {â€¦}, dynamicSettings: Array(3)}       With: RuntimeError: UR error     (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:251 [comfyui-backend] Prompt executed in 116.86 seconds   (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:61 executing Object",Intel,2025-11-14 02:55:16,1
AMD,nq4im71,Will XeSS 3 and Intel multi framegen be available for Iris xe graphics igpus?,Intel,2025-11-22 01:57:26,1
AMD,nq9ltvl,"Putting together a 4k gaming 5090 machine, deciding between the 285k or 265k. Does the extra L3 cache of the 285k make any difference, or does the ~5ns less latency of the 265k make more of a difference?   I plan on a 2dimm board to push memory and OC slightly, but nothing crazy.",Intel,2025-11-22 22:42:16,1
AMD,nqoo97z,"Hi all,  I ordered a contact frame from Thermal Grizzly because the temperature of my CPU wasnâ€™t great, and I was tired of the fan noise.   I installed the contact frame in August, and everything was fine until two weeks ago, when the desktop suddenly stopped booting. The fans were spinning, but there was no POST, no debug LED â€” nothing. I thought the motherboard was dead.   However, when I removed the AIO head and tried to boot again, it worked!  Since then, Iâ€™ve been experiencing intermittent no-POST issues, especially after gaming sessions.   To get it to boot, I always have to adjust the screws on the AIO head. If itâ€™s too tight: no POST. If itâ€™s too loose: CPU temperatures are high. For example, in Hogwarts Legacy, I get around 60 FPS with an RTX 4090, while the CPU runs at around 18% usage @ 85Â°C. Which is not normal at all, as I used to run the game at around 100â€“110 FPS.  I tightened the screws on the contact frame using my thumb and index fingers.   Iâ€™m looking for advice from anyone who has encountered a similar issue, because at this point my only idea is to remove the contact frame and reinstall the original one.  For context, my setup is from November 2022, and I installed the contact frame in August 2025. I have never removed the CPU from the socket since the first installation.  Thank you all.",Intel,2025-11-25 10:53:08,1
AMD,nr6yiuz,Anyone know why the performance of my 13700k is so much better when using a pre micro code bios on a z790 board? This even after applying the latest micro code update through windows,Intel,2025-11-28 09:31:06,1
AMD,nsz9e66,"My hp elitebook 830 g8 notebook, had some problems booting up and it shows blinking lights (caps key) and it keeps trying to boot up without success, but i after getting it repaired now it boots up but sometimes it still does the same thing and boots up after awhile  I was already using the latest versions of the BIOS and ME firmware, and trying to download and update them again did nothing. However, avter updating the BIOS (to the same version), it showed a message saying ""HP Sure Start detected that the Intel Management Engine Firmware is corrupted"", but it only did so once.  What it does consistently is it restarts once or twice when i boot it. It shows the logo, then turns off, and then boots normally.  In the BIOS, it shows in system information that ""ME Firmware Mode: Recovery Mode"".  Windows also takes much longer to boot than usual, can take up to a full minute.  I can recall that the whole system was super slow at some point, but that was before I started diagnosing any of these details. It works fine now after boot.  Tried all sorts of things, installing ME drivers doesnt seem to do anything, and IDK what to do now.  help",Intel,2025-12-08 18:41:34,1
AMD,ntcs1o8,"My processor with integrated 11th Generation Intel graphics (Intel Core i3-1115G4) completely lost Vulkan support after installing the latest version, 7080 (coming directly from the previously installed version, 6987). I'm testing the drivers one by one, but it looks like Vulkan support has been completely removed. Did this only happen with my processor, or with all 11th Generation processors?",Intel,2025-12-10 21:00:34,1
AMD,ntn8xto,"I'm on Win11 with Killer WiFi 7 BE1750x 320MHz Wireless Network Adapter. PC keeps crashing/restarting. Here is an error:  The computer has rebooted from a bugcheck.  The bugcheck was: 0x00020001 (0x0000000000000011, 0x0000000000210720, 0x0000000000001005, 0xffffe700010059a0). A dump was saved in: C:\\WINDOWS\\Minidump\\121225-18625-01.dmp. Report Id: 189a9fba-5969-4b6c-8199-d8b6a03a1a34.  Copilot had me disable all Killer services except Killer Network Service, but the issue persists. Now it tells me to uninstall the drivers and block them from reinstalling and just use the generic drivers.",Intel,2025-12-12 13:56:20,1
AMD,ntoe22w,"I recently bought a 2025 LG Gram 17 (Intel 258V + 140V). Great laptop for productivity tasks which is its primary use for me. I might occasionally game on it if I'm traveling. I only really play Final Fantasy XI these days which was released 2003 so this processor has no problem running it at max settings and \~60 FPS even in Silent mode.  That's when it is plugged in though.  As soon as it is unplugged the performance drops like 80%. I don't mind the performance hit when I'm doing productivity tasks, but would like control over it when I want.  I've tried various things to improve this while on battery:  * Making sure it is designated as ""High Performance"" in Windows. * Making sure Advanced Power settings are the same between plugged-in and battery. * Making sure in Intel Graphics Command Center that Display Power Savings are off. * Making sure in My gram that performance is set to high. * Changing anything anywhere I can find that might be limiting performance on battery.  I haven't started digging through the BIOS yet and don't know that I will. At that point it is too much of a hassle as opposed to some quick toggle(s) in the OS.  Any other places I should be checking or suggestions to address this related to the CPU itself?",Intel,2025-12-12 17:25:39,1
AMD,nttrt4h,"Hey everyone, looking for second opinions because this behavior doesnâ€™t seem normal.  Specs:  CPU: Intel i5-14600KF  Cooler: Scythe Fuma 2 (dual tower, dual fan) with oficial LGA 1700 mounting kit.  Motherboard: ASUS TUF Gaming B760M-PLUS WiFi II  Case airflow is fine.  Darkflash DLX case with 9 fans.  Ambient temp \~25â€“30Â°C (Brazil)  Before this build I had a Ryzen 7 5800X on an ASUS TUF board with the same case and airflow, and temps were completely normal. No overheating issues like this.  but now, I'm facing the following:  Idle temps sit around 60â€“75Â°C  Any moderate load (opening anything, even the bios) causes instant spikes to 95â€“100Â°C. When acessing the Bios, the temp shown is 78-88Â°C  CPU thermal throttles immediately  Heatsink stays barely warm, even when CPU reports 95â€“100Â°C  Fans ramp up correctly  Iâ€™ve tried:  Re-mounted the Scythe Fuma 2  3 times (check [https://imgur.com/a/ehBnxLn](https://imgur.com/a/ehBnxLn) for how the termal paste was when I remounted  the last time)  Re-mounted the CPU.  Checked power limits  Limited PL1/PL2 to 65W â†’ temps drop but CPU becomes extremely slow, 1Ëœ3GHZ, but still hitting 50-60ÂºC on idle and 80-90ÂºC on the rest, 88 on the bios as well.  Undervolted â€“0.06V, didn't solve.  Has anyone else seen this behavior with 14600KF + ASUS B760 TUF boards?  My friend (chatgpt) recommended a contact frame and said that it would fix it. What do you think about this?  Any input appreciated. Thanks!",Intel,2025-12-13 15:34:32,1
AMD,nupcpsu,"This isn't tech support, but my thread got locked and the mods said to post it here.  My 14700k is starting to give instability symptoms, so I opened a ticket with Intel.  The next day, I got an email asking for more information and was told they would call yesterday if they hadnâ€™t received it. I replied to the email with information and my order confirmation, but as of 4:00 yesterday, I didnâ€™t get a reply.  So, I went into the ticketing system and didnâ€™t see my reply. I added a new comment with the information from my email and received almost an immediate email from the support person granting the RMA.  He asked for shipping information and if Iâ€™d choose option 1 or 2. For option 2, he asked for my agreement to the process and my billing address, name on the credit card, and expiration. He said heâ€™d call to get the credit card number.  I replied to the email and pasted the information in my ticket. Oddly enough, I didnâ€™t see his second response in the ticket.  Is that really how this goes down? Thereâ€™s not an order system where I can input my credit card information? I have to wait for a call from some random support person and give him my credit card number over the phone?",Intel,2025-12-18 16:06:09,1
AMD,nv6u3b9,"Hi,  Iâ€™m trying to register my account for claiming master game key card, but when i put my phone number to complete registration it always â€œphone number unreachableâ€.   I already submitted the ticket for support, i worry about not get any replies because of xmast and new year holiday.   Can you help me to complete my registration, i just want to claim the game and play in peace.   I tried to look for solution but nothing have same problem with me ? Is there any possibilities that on weekend the system canâ€™t use A2P SMS for Indonesia Region",Intel,2025-12-21 12:36:51,1
AMD,nvbb6wv,"Intel ARC b580 apparently unable to run games in the snowdrop engine, but in such a way that it's almost impossible for benchmarkers/testers to catch.  I've been trying to run Avatar Frontiers of Pandora on my arc b580 for over a year now, and I keep having the same extremely unusual crash.  If I freshly reinstall the game, or change my operating system, or switch arc driver version, I can play the game normally, at regular framerates, with no issues, for around an hour. MAYBE two if I'm lucky.   The game then crashes, with seemingly no trigger. No thermal issues, no unusual resource usage, no memory leak, no framerate issues, no stuttering, not even a crash report. The game will just close as if I had closed it manually.   I don't crash in the same location every time, it crashes with different game settings, xess enabled/disabled, different monitors, different ram configurations. I can have the framerate capped or uncapped. GPU maxed out or not, power draw high or low. I can't consistently recreate the crash in the same way every time. The only certainty is that it WILL crash eventually.  After crashing once, It will then crash on game launch, every single time. The game opens, the epilepsy warning flashes up for a second or two, and then the game crashes in the same way, with no freeze, no crash report, just the game closing.  Since the game runs fine for an hour or so after I first install it, any benchmarker testing the gpu will likely never see the crash, because they install the game, run benchmarks, do their tests, and then close it before seeing anything wrong.  The following are fixes that I have tried, and have NOT worked.  * Updating graphics drivers (to both the latest absolute driver, and latest WHQL certified driver). * Downgrading graphics drivers (going all the way back to the first game ready drivers for the b580). * Updating windows (latest windows 10 and latest windows 11 versions). * Running a memory diagnostic * Enabling/disabling XMP. * Verifying integrity of game files. * Forcing the game to run in the Vulkan engine instead of snowdrop. * Performing a complete reinstall of the game. * Replacing one of the .dll files with a ""repaired"" one. (this apparently fixed a crash for some people). * Forcing the game to boot in dx11 instead of dx12.  The following are fixes that I have tried, and HAVE worked.  * Running the game on my AMD integrated graphics.  Obviously running the game on an igpu doesn't offer playable performance, but the game will run, and I've yet to find any crash, even leaving the game open for several hours.  Due to this issue, I'm on the verge of selling my b580 and replacing it with a different card from another manufacturer that hopefully won't have the same issue.  Has anyone had a similar issue and knows the fix? Although I suspect the issue has to be resolved with a driver update.",Intel,2025-12-22 03:46:16,1
AMD,nw4g1zt,VR Support for ARC GPU? :),Intel,2025-12-27 02:03:19,1
AMD,nwenzma,"Hello, I'm experiencing an intermittent but severe system hard freezes on a prebuilt PowerSpec G441 desktop purchased from Micro Center 2 years ago. The freezes occur during gaming, mixed workloads (gaming + video playback), remote desktop usage (TeamViewer), and occasionally shortly after logging into Windows with no workload running.  When my PC hard freezes:  \- Mouse is unresponsive  \- Keyboard is unresponsive  \- No audio sound  \- No BSOD or error message     My PC does not recover and requires a manual power button reboot.  After a forced reboot, Windows loads normally and allows me to log in but the system may freeze again within 1â€“2 minutes, even at idle. Other times, the system may run normally for days before the issue reappears.  No relevant error logs are generated at the time of the freezes. Event Viewer only shows Kernel Power events related to the forced shutdown.  So far I've tried all possible solutions to resolve my issue:  \- Updated motherboard BIOS to the latest available version  \- Updated Windows 11 fully and also reinstalled Windows 11 through USB  \- Performed clean GPU driver reinstallations  \- Verified CPU and GPU temperatures (normal under load)  \- Ran hardware stress tests and diagnostics (CPU, GPU, memory, storage) and no failures detected  \- Tested each monitor individually (dual monitor setup)  \- Swapped HDMI/DisplayPort cables and ports  \- Tested my PC without background applications  \- Returned my PC to stock settings (no overclocks, no undervolts, no XMP changes beyond default)  I've also took my pc to a local repair shop, which they could not reproduce the issue but suggested it may be CPU or PSU related issue. However, when I took my PC to Micro Center, technicians ran similar diagnostics and stress tests but were also unable to reproduce the freezes. They have also tested the PSU voltages and it passed. Despite extensive testing, the issue remains unresolved and continues to occur consistently now.  Specs:  Operating System: Windows 11 Pro 25H2 (Build 26200.7462)  CPU: Intel Core i7-13700KF (stock settings)  GPU: NVIDIA GeForce RTX 3070 Ti 8GB (stock)  Motherboard: MSI PRO Z690-A WIFI (MS-7D25), BIOS v5.32  RAM: 32GB (2Ã—16GB) DDR5-5600 (G.Skill)  Storage: WD Blue SN570 1TB NVMe SSD  Cooling: NZXT Kraken 240mm AIO liquid CPU cooler  PSU: 750W 80+ Gold (PowerSpec OEM, included with prebuilt)  Display: Dual monitor setup, tested individually  I gave a call to Intel and now waiting for a call back. Thank you and I appreciate any guidance!",Intel,2025-12-28 18:50:36,1
AMD,nwjovjk,"Does my 12700f support TME?  In specs, there's no single line about TME. 12700's specs declare TME support. Processor Identification Utility does not mention TME, `hwinfo64` shows TME in gray, and `cpuid -1 | rg TME` prints `TME: Total Memory Encryption = false`. There can i check whether my cpu support TME or not?",Intel,2025-12-29 14:08:35,1
AMD,nwkfoww,"I have an i5-13600kf and a asus strix b760-i motherboard and mostly it runs fine but in specific games I get random shutdowns. Most commonly hell divers and now expedition 33.  I found the vmin instability issue and patched my bios to the latest version now but I still get it happening. Is there any suggested settings changes that have been found to help or is the only course to look at an RMA? Iâ€™d rather not be out of a PC for a while so trying to seek help to avoid that if possible.  Whatâ€™s I find odd is I never get any errors or BSOD. Just the monitor goes black and the system locks up, the fans are still running but it has to be shutdown by holding the power button.",Intel,2025-12-29 16:25:47,1
AMD,niosucw,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with ourÂ [Software Advantage Program](https://softwareoffer.intel.com/)Â team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with yourÂ Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [IntelÂ® Software Advantage Program â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-10 00:13:23,1
AMD,niowtna,"Hi u/afyaff For this kind of inquiry, please contact directly ourÂ [Software Advantage Program](https://softwareoffer.intel.com/)Â team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [IntelÂ® Software Advantage Program â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-10-10 00:37:02,1
AMD,nicof1i,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [IntelÂ® Software Advantage Program â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Intel,2025-10-08 01:46:19,2
AMD,njq2uvf,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [IntelÂ® Driver & Support Assistant (IntelÂ® DSA) Results in â€œSorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Intel,2025-10-16 01:31:17,1
AMD,nkq87np,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Intel,2025-10-22 04:59:31,1
AMD,nkxufey,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Intel,2025-10-23 11:35:54,2
AMD,nkwr1mw,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Intel,2025-10-23 05:27:23,1
AMD,nmj7ctn,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Intel,2025-11-01 13:51:07,1
AMD,nmsah2r,"u/vincococka ,Yes you can use that memory kit safely as long as you donâ€™t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of IntelÂ® Coreâ„¢ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Intel,2025-11-02 22:47:21,1
AMD,nnd464j,"u/SuperV1234 If the motherboard BIOS allows,Â disable Turbo and run the system to see if the instability continues. Â If the instability ceases with Turbo disabled, it is likely that the processor  need a replacement.",Intel,2025-11-06 04:10:00,1
AMD,np77ndt,"u/triptoasturias Before I share any recommendations, could you confirm if this is your exact system-[Inspiron 15 5510 Setup and Specifications | Dell](https://www.dell.com/support/manuals/en-my/inspiron-15-5510-laptop/inspiron-5510-setup-and-specifications/specifications-of-inspiron-15-5510?guid=guid-7c9f07ce-626e-44ca-be3a-a1fb036413f9&lang=en-us)? Also, may I know which driver version youâ€™re using and where you downloaded it fromâ€”was it from Dell or the Intel Download Center?",Intel,2025-11-16 20:12:25,1
AMD,np8jpxo,"u/triptoasturias Your concern is related to **port and bandwidth limitations**. Most Inspiron models only support **HDMI 1.4**, which is limited to 4K at 30Hz or 2560Ã—1440 at 60Hz. Ultra-wide resolutions like 3440Ã—1440 often arenâ€™t exposed because theyâ€™re outside the standard HDMI 1.4 spec. **USB-C to DisplayPort**: This works because DisplayPort has much higher bandwidth and supports ultra-wide resolutions natively. HDMI-to-DisplayPort Adapter, Unfortunately, **passive adapters wonâ€™t work** because HDMI and DisplayPort use different signaling. Youâ€™d need an **active HDMI-to-DisplayPort converter**, but even then It will still be limited by the HDMI 1.4 bandwidth from your laptop. So, you likely **wonâ€™t get 3440Ã—1440 at 144Hz,** maybe 3440Ã—1440 at 30Hz or 50Hz at best.",Intel,2025-11-17 00:32:25,1
AMD,np8oqxv,"u/mercurianbrat This spec can run **basic image generation workflows** (CPU mode or lightweight models), but **video generation and heavy diffusion models will struggle or fail** on this setup. AI Playgroundâ€™s minimum requirements are currently tied to Intel Arc GPUs with 8GB or more of allocated VRAM. Â Currently you can download the installer for discrete GPUs. Â We will also publish an installer that will run on Intel Core Ultra-H with built-in Intel Arc GPU (please keep in mind thatÂ [Windows allocates half of the system RAM as VRAM](https://www.intel.com/content/www/us/en/support/articles/000020962/graphics.html)Â for integrated GPUs, so 16GB or more of system RAM are required) Â and Intel Arc GPU discrete add in cards with 8GB or more of memory. AI Playground takes up 8GBs of Â HDD/SDD requirements: 8GB w/o models, Â \~50GB with all models installed.",Intel,2025-11-17 01:00:59,1
AMD,nqg4c7m,"u/mano109 As a general corporate policy, Intel Support does not comment on information about products that have not been released yet.  **Visit**Â ourÂ [Newsroom](https://newsroom.intel.com/)Â for the most recent announcements and news releases.",Intel,2025-11-24 00:10:02,1
AMD,nqg5tvg,"**265K all the way.** At 4K with a 5090, you're GPU-bound anyway. The 265K's lower memory latency beats the 285K's extra cache at that resolution, plus you save money for better RAM or cooling.",Intel,2025-11-24 00:18:36,1
AMD,nqry4gw,"u/hus1030  The mounting pressure from your AIO cooler can directly affect whether the system successfully completes POST. When the cooler is tightened too much, it can cause the CPU or motherboard to bend slightly, which may lead to poor or lost contact between the CPU and the socket pins. This prevents the processor from initializing properly, resulting in a no-POST condition. Installing a contact frame changes the pressure distribution compared to the stock retention mechanism, so overtightening the AIO screws can amplify this issue. On the other hand, if the screws are too loose, the CPU temperatures will rise because the cooler is not making proper thermal contact. To avoid these problems, ensure the AIO screws are tightened evenly in a cross pattern and do not exceed the manufacturerâ€™s torque specifications. If the issue persists, you may need to verify that the contact frame is correctly installed or temporarily revert to the original retention bracket to rule out pressure-related problems.",Intel,2025-11-25 21:43:16,1
AMD,nrmm6fq,"u/BudgetPractical8748 Â Â Intel Default Settings may impact system performance in certain workloads as compared to unlocked or overclocked settings. Â As always, system performance is dependent on configuration and several other factors.",Intel,2025-12-01 00:19:07,1
AMD,ns3syx7,Nope got cash back,Intel,2025-12-03 18:08:27,1
AMD,nt12wk5,"u/Any_Information429 Your HP EliteBook 830 G8 is experiencing boot issues due to corrupted Intel Management Engine (ME) firmware, which is a critical low-level system component that manages hardware initialization. This corruption is causing the blinking caps lock light, multiple restart attempts before successful boot, and the extended Windows startup times you've been experiencing. The BIOS showing ""ME Firmware Mode: Recovery Mode"" confirms this diagnosis. Since these issues began after your recent repair, it's likely that the Management Engine chip connections were disturbed or the firmware became corrupted during the service process.  To resolve this, you need to perform a forced recovery of the ME firmware by downloading the specific firmware version for your EliteBook model from HP's support website-[HP EliteBook 830 G8 Notebook PC Software and Driver Downloads | HPÂ® Support](https://support.hp.com/us-en/drivers/hp-elitebook-830-g8-notebook-pc/38216726) and using specialized recovery tools to reflash the Management Engine. You also have check BIOS settings to ensure proper ME configuration and temporarily disable fast boot to allow complete initialization. If the firmware recovery doesn't resolve the issue, this may indicate hardware-level damage to the ME controller that occurred during the previous repair, which would require professional chip-level service or potentially warranty coverage since the problem originated after authorized service work. The good news is that once the ME firmware is properly restored, your system should return to normal boot times and eliminate the restart cycles you're currently experiencing.  USB flash recovery method is definitely worth trying first - it's designed specifically for these types of firmware corruption issues and should get your laptop back to normal boot times without all those frustrating restarts. Check here: [Support Search Results | HPÂ®ï¸ Support](https://support.hp.com/us-en/search/videos?q=BIOS) BIOS Videos",Intel,2025-12-09 00:22:32,1
AMD,ntk9thw,"Hi @[Content\_Magician51](https://www.reddit.com/user/Content_Magician51/) Upon checking, there is a new driver version available which is 32.0.101.7082. You may try this and use DDU method to make sure that you performed a clean driver installation. Here are the links of the latest driver and the steps on how to perform DDU.  [IntelÂ® 11th â€“ 14th Gen Processor Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/864990/intel-11th-14th-gen-processor-graphics-windows.html)  [How to Use the Display Driver Uninstaller (DDU) to Uninstall a...](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)",Intel,2025-12-12 00:44:01,1
AMD,ntyyo2o,"u/MISINFORMEDDNA Â I took a look at your crash error, and here's what's going on. That error code you're seeing (0x00020001) is actually what's called a ""hypervisor error,"" which basically means it's a problem with Windows' virtualization stuff rather than directly being caused by your WiFi drivers.  The real culprits are more likely things like memory issues, BIOS problems, or conflicts with virtualization features like Hyper-V. I'd suggest running a memory test first (just search for ""Windows Memory Diagnostic"" in your start menu), and if you have any virtual machines or Docker running, try shutting those down temporarily, we'll probably need to dig deeper into hardware or system-level issues to really fix this one.",Intel,2025-12-14 12:43:17,1
AMD,ntz1is6,"u/strumpystrudel So what you're experiencing is actually pretty normal behavior for your laptop when it's unplugged - that 80% performance drop is totally expected and here's why. When your laptop is plugged into the wall, your CPU can run at much higher power levels (probably around 28W or more), but when you switch to battery, it gets severely limited to maybe 8-15W to preserve battery life. This is especially true for ultrabooks like the Gram that prioritize being thin and light over raw performance. The thing is, a lot of this power management happens at the hardware level with Intel's built-in systems, which is why all those Windows power settings you tweaked aren't really making a difference - the CPU is basically ignoring them and doing its own thing to save battery.  Now, for a game like Final Fantasy XI, you should still be able to get it running decently on battery with some tweaks, but expecting that same smooth 60 FPS at max settings is probably unrealistic given the fundamental power constraints of ultrabook design. Most ultrabooks see this kind of 60-80% performance hit on battery for any sustained workload, so you're definitely not alone in this.Â   But honestly, this is just how these thin and light laptops are designed to work - they're amazing when plugged in, but they have to make compromises when running on battery to actually give you decent battery life.",Intel,2025-12-14 13:04:59,1
AMD,ntz4fy4,"u/Aggravating_Gap_203 I'd recommend running Intel's Processor Diagnostic Tool first to rule out any hardware defects with the CPU itself. Just download it from Intel's website, run the test, and let us know if it passes or fails. While you're at it, try loading your BIOS defaults and make sure your power settings are at Intel's recommended specs - PL1 should be around 125W and PL2 around 181W for your 14600KF-[IntelÂ® Coreâ„¢ i5 processor 14600KF](https://www.intel.com/content/www/us/en/products/sku/236778/intel-core-i5-processor-14600kf-24m-cache-up-to-5-30-ghz/specifications.html)  [IntelÂ® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html)  Your friend is actually spot on about the contact frame recommendation.Â Before you buy one though, try remounting your cooler one more time - make sure you're tightening the screws in an X-pattern and that everything is perfectly aligned. Sometimes it just takes that perfect mount to get things working right. Let us know what the Intel diagnostic shows and we can go from there!",Intel,2025-12-14 13:25:40,1
AMD,nurwhxx,Hi [RadioFr33Europe](https://www.reddit.com/user/RadioFr33Europe/) I sent a direct message to gather more details for me to review the case and check the status of your replacement request.,Intel,2025-12-19 00:03:08,1
AMD,nvbnrfy,"Hi [Designer-Let-7867](https://www.reddit.com/user/Designer-Let-7867/) For issues related to game bundles and how to claim it, please contact directly ourÂ [Software Advantage Program](https://softwareoffer.intel.com/)Â team. They are the one who handles bundle related inquiry/issues/error message during claiming.   You may also check this link for additional information:  [IntelÂ® Software Advantage Program â€“ Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Intel,2025-12-22 05:14:39,1
AMD,nvfrjth,"u/earwig2000 Let me check this internally. From what I see, youâ€™ve already tried a lot of steps to address the game crash issue. Iâ€™ll share an update here as soon as I have more details, and I might need to collect some info from you for further analysis.",Intel,2025-12-22 21:29:55,1
AMD,nvmgqub,"u/earwig2000 Have you tried doing a clean installation of the graphics driver usingÂ [DDUÂ ](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)and then installing the latest version from ourÂ [download center](https://www.intel.com/content/www/us/en/download/785597/intel-arc-graphics-windows.html)? After that, please retest the game. If the issue still persists, could you share your PSU make, model, and wattage? Also, check if Resizable BAR (ReBAR) is enabled in your BIOS. Finally, review the Event Viewer for any error messages or crash-related events, this will help us determine whether the problem is driver-level or application-related.",Intel,2025-12-23 22:44:39,1
AMD,nwglbda,u/outlander94 Kindly check this article: [Is Virtual Reality (VR) supported on IntelÂ® Arcâ„¢ A-Series and...](https://www.intel.com/content/www/us/en/support/articles/000093024/graphics.html),Intel,2025-12-29 00:38:16,1
AMD,nwgmhqt,"u/Kai-juu We trust the technicianâ€™s diagnosis of the system. However, since the unit is a prebuilt, it is likely to have a tray processor. Based on our warranty terms and conditions, we can only replace boxed processors. For a faster turnaround time, please first verify whether the processor is tray or boxed using our website. Once you confirm the type, I can guide you through the next steps.  [Warranty Information](https://supporttickets.intel.com/s/warrantyinfo?language=en_US)  [Where to Find IntelÂ® Boxed Processor Serial Numbers (FPO and ATPO)...](https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html)  [Warranty Policy for IntelÂ® Boxed and Tray Processors](https://www.intel.com/content/www/us/en/support/articles/000024255/processors.html)",Intel,2025-12-29 00:44:27,1
AMD,nwohvpq,"u/Sk7Str1p3 I can see you've already tried several tools and are getting mixed results, which is definitely frustrating.  To help you out better, could you share what's driving this inquiry? Are you working on a security project, dealing with compliance requirements, or troubleshooting a specific feature? Understanding your use case will help me point you toward the right verification methods or suggest alternatives if needed. The more context you can provide, the better I can assist you!",Intel,2025-12-30 05:04:29,1
AMD,nwvjztq,"u/pheoxs you may try to follow this article [Computer Randomly Reboots or Shuts Down](https://www.intel.com/content/www/us/en/support/articles/000035903/processors.html) if the issue persist, disable Turbo boost If the motherboard BIOS allows and run the system to see if the instability continues. Â If the instability ceases with Turbo disabled, it is likely that the processor is affected and need a replacement.",Intel,2025-12-31 06:58:32,1
AMD,nktqa25,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Intel,2025-10-22 18:45:49,2
AMD,nmk5tv3,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Intel,2025-11-01 16:57:33,1
AMD,nnel13c,"Hi, I've tested disabling turbo and I still experienced issues. I've created a support request, case number 06728608, please take a look ASAP.",Intel,2025-11-06 12:05:51,1
AMD,np8oivj,"Actually, this is not the case. After spending hours on this issue, I've finally found the 31.0.101.4502\_A14 drive at Dell's website that is working without any problems with the 3440x1440. So your claim that HDMI 1.4 is limited to 2560x1440 is false. HDMI 1.4 can go further to 3840 x 2160 without any problems. It's unacceptable the lack of support for UWM from Intel graphics cards and drives. Many previous drivers let you choose 3440X1440 resolution. Why did Intel stop supporting this resolution in the last graphics drivers?",Intel,2025-11-17 00:59:39,1
AMD,nqkt81n,I was going with the 265K over the 9800X3D since the Intel stuff seems to get better 1% lows and smoother experience at 4k and above. But does DLSS change that? Does DLSS lowering the render resolution push the 9800X3D back into the lead?,Intel,2025-11-24 19:16:09,1
AMD,nz1bddl,"The issue turned out to be Bitdefender (which uses hyper-v), which I believe Killer services depend on. After uninstalling all Killer stuff, I was able to delete the Bitdefender driver and my computer is now stable.",Intel,2026-01-11 20:37:01,1
AMD,nvmmo42,"Tried running a clean driver reinstall using DDU. (I'm pretty sure I did this last install too but did it again to double check) and that didn't fix the issue.  ReBAR is enabled, it was on by default.  My PSU is the [Thermaltake Toughpower 650W Gold](https://www.thermaltake.com/toughpower-650w-gold-modular.html)  Windows event viewer did pick up the crash [(imgur link)](https://imgur.com/NhS5e6l), not sure what to make of it though.",Intel,2025-12-23 23:18:45,1
AMD,nwgvzmr,"u/Intel_Support Thank you for your guidance. My system is a prebuilt, so the processor never came in a box and is likely a tray CPU. Micro Center told me to reach out to Intel for warranty support, so I just want to confirm, should I be working with Intel directly or do you recommend contacting PowerSpec/Micro Center for the RMA? Thank you again, I look forward for the next steps!",Intel,2025-12-29 01:38:35,1
AMD,nwr1jrz,"Yes, I am information security hobbyist, I have a research project on countering attackers with physical access to the machines. Unfortunately, my current hardware is average gaming PC - i7 12700f with h670 motherboard and I cannot update hardware.",Intel,2025-12-30 16:12:50,1
AMD,nkvsdop,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued IntelÂ® Killerâ„¢ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.  Â   Based on your initial description, here's what I recommend:  Â   First, please review this article-[Customer Support Options for Discontinued IntelÂ® Killerâ„¢ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html)Â and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html).Â Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.  Â   If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver  Â   For additional troubleshooting, here's another helpful articleÂ that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most IntelÂ® Killerâ„¢...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)  Â   If you've tried these steps and the issue persists, I'd recommend coordinating directly withÂ [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Intel,2025-10-23 01:24:29,1
AMD,nnh8tyn,"u/SuperV1234 Hi, thanks for the update. Iâ€™ve reviewed case number and confirmed that itâ€™s currently being handled by our **warranty team** for replacement. You should be receiving further instructions from them shortly.     [Guide to pack your faulty CPU](https://www.intel.com/content/www/us/en/content-details/841997/guide-to-pack-your-faulty-cpu.html)",Intel,2025-11-06 20:19:42,1
AMD,np8q866,"u/triptoasturias this explains, The generic IntelÂ® driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing Intel generic graphics driver will overwrite your handpicked OEM graphics driver (in your case Dell driver). Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see thisÂ [article.](https://www.intel.com/content/www/us/en/support/articles/000096252/graphics.html)",Intel,2025-11-17 01:09:54,1
AMD,nvz4tef,"u/earwig2000 ï»¿Thank you for sharing this information. I will begin investigating the issue and attempt to replicate it on our end. I'll post an update here or notify you directly once there are any developments. If I need further details, I'll reach out to you here. I appreciate your patience as I work on this matter.Â   [](javascript:void(0);)",Intel,2025-12-26 04:28:47,1
AMD,nwnx92g,"u/Kai-juu According to our warranty policy, RMAs for tray processors must be handled by the original place of purchase, as clearly stated in the article I referenced.",Intel,2025-12-30 02:59:10,1
AMD,nwvn4dm,"u/Sk7Str1p3 I see,Â let me loop in our product support engineer on this one and dig into it a bit more. I'll circle back with you once I have a clearer picture on the TME support situation.",Intel,2025-12-31 07:26:19,2
AMD,nnd9ral,Unfortunately my Dell warranty support has ended and so far the forum there has not been able to help either. The removal tool works but Killer just keeps coming back.,Intel,2025-11-06 04:50:53,1
AMD,nnd9w3h,"I also went to Dell, typed in my service tag # and cant find any other ethernet drivers.",Intel,2025-11-06 04:51:52,1
AMD,nvz6y15,"If it helps, I'm also using   CPU - Ryzen 5 7600   RAM - Corsair Vengeance 32gb DDR5 6000mhz cl36  SSD - Crucial p3 plus 1tb  Motherboard - MSI b650m-a",Intel,2025-12-26 04:44:54,1
AMD,nx27aj0,"Hello, have you received a reply yet?",Intel,2026-01-01 10:17:28,1
AMD,nwgjs6p,u/earwig2000 Please check your inbox; Iâ€™ve sent you a message.,Intel,2025-12-29 00:30:28,1
AMD,nx7bu85,u/Sk7Str1p3 ï»¿Iâ€™m still reviewing the details to ensure I provide you with the most accurate information. Iâ€™ll update you as soon as I have a clear answer.  [](javascript:void(0);),Intel,2026-01-02 04:55:46,1
AMD,nxygejg,"u/Sk7Str1p3 I'm pleased to confirm thatÂ TME is indeed supported on your i7-12700F processor. TME is a security feature that encrypts the entire system memory using AES encryption with a hardware-generated key. The i7-12700F is part of Intel's 12th generation Alder Lake family, which includes TME support as a standard security feature.  However, TME functionality requires both processor support and proper system configuration:  Verification methods:  * BIOS/UEFI security settings * System diagnostic tools that enumerate processor security features  Important considerations:Â TME enablement depends on the motherboard BIOS implementation and may be disabled by default on many systems.  To assist you further, I need the following information:  * Make and model of your motherboard, as the TME feature may be disabled in the BIOS settings  For your reference, please find the official documentation:Â 12th Generation IntelÂ® Coreâ„¢ Processors Datasheet, Volume 1 of 2 Â [655258](https://cdrdv2.intel.com/v1/dl/getContent/655258)Â (Page 40) - This document contains the specific technical details about TME support for your processor.  Once you provide your motherboard information, I can guide you on how to check and potentially enable TME in your system's BIOS settings.  Please let me know if you have any additional questions.",Intel,2026-01-06 05:10:19,1
AMD,nxzgti7,My motherboard is Asrock H670-M Pro RS. firmware version: 18.01,Intel,2026-01-06 10:30:53,1
AMD,nz0ylbi,Hello. I want to tell you i7-12700f does NOT support TME. Thank you for your help anyway.,Intel,2026-01-11 19:38:08,1
AMD,ny5a7jq,"Your Intel i7-12700F processor does support TME (Total Memory Encryption), but the issue lies with your motherboard implementation rather than CPU capability. The ASRock H670M Pro RS with firmware 18.01 appears to either disable TME at the platform level or doesn't expose the necessary BIOS controls for consumer users. This explains why your diagnostic tools show ""TME = false"" and why HWiNFO64 displays it in gray. While you could check for TME-related options in your BIOS under Advanced â†’ CPU Configuration or Security sections, consumer-grade H670 motherboards often omit these enterprise security features from their interfaces. Your best course of action is to contact ASRock technical support directly, providing your exact motherboard model, BIOS version, and specifically asking about TME support and whether future firmware updates might include these controls.",Intel,2026-01-07 04:53:34,1
